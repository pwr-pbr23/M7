[{"number": 36159, "title": "Enhance SlurmClusterResolver", "body": "This enhances the SlurmClusterResolver and fixes its bugs, see #36094\r\n\r\nThe design goal is that the user simply creates an instance of the class when the python script is run by `srun` (the `mpirun` wrapper on SLURM). As all information is available in the environment this is certainly possible.\r\n\r\n- Default all arguments to something reasonable\r\n- Use Slurm step env variables to fill in data\r\n\r\nThis has been tested (kinda) successfully on a SLURM cluster with TF 2.1.0 and MultiWorkerMirroredStrategy. It works with artificial data. I couldn't test it fully with MNIST due to https://github.com/tensorflow/tensorflow/issues/36153\r\n\r\nWhat should be discussed besides the code:\r\n- Usage of `nvidia-smi` to query the number of GPUs. I couldn't use `context.num_gpus()` at this point because this initializes the context including collectives which is not wanted as collectives might be overwritten later when creating a strategy", "comments": ["cc @steven-chien ", "Thank you very much for the work @Flamefire and apologies for the late reply. I made the resolver originally for a work that needs a simple pattern of one/two GPUs per process so support for advanced resource distribution is not implemented. I always wanted to extend it but then other works came along...\r\n\r\nWith the complete shift to Slurm env variables, one problem that I see is things might behave differently when (different implementation of) mpirun is used instead of srun, i.e. OMPI_COMM_WORLD_SIZE is used instead, etc, so this also should be tested.\r\n\r\nFor expand_hostlist:\r\n- My original plan was to use expand_hostlist as well but eventually skipped it due to license and dependency issues. Thus the scontrol solution that you saw.\r\n\r\nFor auto query of GPUs:\r\n- The auto query of the number of GPUs on a node is a very nice feature. However, the use of nvidia-smi reports all the GPUs used in a node.\r\n- One problem with my original implementation is that the exclusive use of a node is assumed. In case of non-exclusive access of node, SLURM uses the environment variable CUDA_VISIBLE_DEVICES to assign GPUs, whereas nvidia-smi reports all GPUs present on a system.\r\n- Some thought should be given on how to figure out how to take this into account in the future. (i.e. extract info from more Slurm variables)\r\n\r\nApart from that, I don't see any major issues with the code. Since Slurm is mostly used in HPC community I also suggest that we can improve the documentation, so people who wants to use TF on HPC clusters can use this easily.", "> With the complete shift to Slurm env variables, one problem that I see is things might behave differently when (different implementation of) mpirun is used instead of srun, i.e. OMPI_COMM_WORLD_SIZE is used instead, etc, so this also should be tested.\r\n\r\nIIRC when SLURM is used then you'd better use srun too which also takes care of job steps. So I'd remove OMPI variable references to avoid people thinking this is for MPI and focus on correct work with SLURM. If someone wants/needs MPI support, they can implement another class or derive from this or use the ctor parameters.\r\n\r\n>  My original plan was to use expand_hostlist as well but eventually skipped it due to license and dependency issues.\r\n\r\nOk so that needs to be rewritten. I'll look into that. scontrol is not really an option as it may not handle job steps correctly. But I'll check that first. Job step handling is crucial!\r\n\r\n> However, the use of nvidia-smi reports all the GPUs used in a node.\r\n\r\nThat's what it is used for: To set gpus_per_node\r\n\r\n> One problem with my original implementation is that the exclusive use of a node is assumed. In case of non-exclusive access of node, SLURM uses the environment variable CUDA_VISIBLE_DEVICES to assign GPUs,\r\n\r\nThat's actually a good idea: If CUDA_VISIBLE_DEVICES is set, then that is used instead of the nvidia-smi solution. This can (and should) be mentioned as a caveat though.\r\n\r\n> Some thought should be given on how to figure out how to take this into account in the future. (i.e. extract info from more Slurm variables)\r\n\r\nI did. There are no standard SLURM variables to use. Hence nvidia-smi to figure out the number of GPUs/node is the only thing you got. Defaulting the gpus_per_task to gpus_per_node//tasks_per_node is the most reasonable thing to do here.\r\n\r\n> Since Slurm is mostly used in HPC community I also suggest that we can improve the documentation, so people who wants to use TF on HPC clusters can use this easily.\r\n\r\nYes this is what I intend: Currently TF on HPC is hard to awkward to use. Figuring out what to set how is difficult. And then try to explain that to users who just want to \"make it work\". The default arguments allow exactly this use case. I assume 90% of the situations are covered by that and then you can always use the ctor args to overwrite values that are gotten wrong or are non-standard. ", "> > With the complete shift to Slurm env variables, one problem that I see is things might behave differently when (different implementation of) mpirun is used instead of srun, i.e. OMPI_COMM_WORLD_SIZE is used instead, etc, so this also should be tested.\r\n> \r\n> IIRC when SLURM is used then you'd better use srun too which also takes care of job steps. So I'd remove OMPI variable references to avoid people thinking this is for MPI and focus on correct work with SLURM. If someone wants/needs MPI support, they can implement another class or derive from this or use the ctor parameters.\r\n> \r\nThis one is tricky on generality. As some centers use neither srun or mpirun, but some other wrappers to optimize node to process mapping. We might initially try to explicitly document that a user should only use this Slurm with srun that is built with PMI.\r\n\r\n> > Some thought should be given on how to figure out how to take this into account in the future. (i.e. extract info from more Slurm variables)\r\n> \r\n> I did. There are no standard SLURM variables to use. Hence nvidia-smi to figure out the number of GPUs/node is the only thing you got. Defaulting the gpus_per_task to gpus_per_node//tasks_per_node is the most reasonable thing to do here.\r\n> \r\nOne thing to be careful here is that if the user passes --gres during srun instead of salloc, the environment variable might be overwritte. see GPU management in https://slurm.schedmd.com/gres.html\r\n `This environment variable is only set when tasks are launched on a specific compute node (no global environment variable is set for the salloc command and the environment variable set for the sbatch command only reflects the GPUs allocated to that job on that node, node zero of the allocation).`", "Ok. I think we can document the environment variables used so when users are not using standard SLURM tools, they can check if they still are able to use this. Maybe someone from such a center could help with compatibility.\r\n\r\nI'm purposely writing this for execution inside `srun`. Only then is the final configuration known. So salloc/sbatch settings are not covered here.", "Thanks for your comments @steven-chien! It would be good to have documentation to cover standard Slurm use cases for this written.", "What is the preferred way of writing test assertions? pytest style `assert foo == bar` or unittest style `self.assertEqual(foo, bar)`? I couldn't find this in the docu and at least this file seems to be inconsistent. I'd prefer the pytest style for improved clarity", "@Flamefire The current preferred usage is in the `unittest` style with `assertEqual`, `assertNotEqual`, etc...", "I rebased to master and addressed the comments:\r\n- Fix line lengths to 80 (as far as I could find, is there a style check for TF?)\r\n- Implement `expand_hostlist` and `expand_tasks_per_node` to drop dependency on python-hostlist\r\n- Improve some error reporting\r\n- Prefer `CUDA_VISIBLE_DEVICES` over `nvidia-smi`\r\n- Add `_resolve_*` as member functions so they can be overwritten for e.g. MPI environments\r\n- Only keep  functions in the public \"namespace\" that might be useful for others\r\n- Improve documentation (I'll probably have to update `Readme.slurm` too but I'd prefer to rename it to `Slurm.md` or `ReadmeSlurm.md` to get syntax support from Github)\r\n- Fix existing tests and add own ones for the hostlist stuff and a default constructed SlurmClusterResolver", "Thanks for doing this @Flamefire! I've added some minor comments, and we should be good to go once those are fixed. If the internal linter catches anything else, I'll fix it then.", "Done. For completeness: There is `scontrol show hostnames=n[1-3]` which does the hostlist expansion. For an invalid hostlist it shows \"Invalid hostlist: <input>\" but exits with no error code. It would significantly reduce the code as `expand_hostlist` would become a single call and split of output potentially with a check for \"invalid hostlist\".   \r\nDownside: It is harder to test as it'd need to be mocked and I don't know how long this was available and if the \"invalid\" output changed or is the same for all versions. The current implementations is more independent.", "@Flamefire You may have to run the API compatibility test/updater at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/api_compatibility_test.py#L23 ", "Ah yes, of course the defaults have changed to something more reasonable:\r\n\r\n```\r\n    member_method {\r\n      name: \"__init__\"\r\n-     argspec: \"args=[\\'self\\', \\'jobs\\', \\'port_base\\', \\'gpus_per_node\\', \\'gpus_per_task\\', \\'tasks_per_node\\', \\'auto_set_gpu\\', \\'rpc_layer\\'], varargs=None, keywords=None, defaults=[\\'8888\\', \\'1\\', \\'1\\', \\'None\\', \\'True\\', \\'grpc\\'], \"\r\n?                                                                                                                                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n+     argspec: \"args=[\\'self\\', \\'jobs\\', \\'port_base\\', \\'gpus_per_node\\', \\'gpus_per_task\\', \\'tasks_per_node\\', \\'auto_set_gpu\\', \\'rpc_layer\\'], varargs=None, keywords=None, defaults=[\\'None\\', \\'8888\\', \\'None\\', \\'None\\', \\'None\\', \\'True\\', \\'grpc\\'], \"\r\n?                                                                                                                                                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n```\r\n\r\nCan I just copy the new defaults to tensorflow/tools/api/golden/v2/tensorflow.distribute.cluster_resolver.-slurm-cluster-resolver.pbtxt? I guess that would make it pass but: \r\n> You will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nI added a commit where I applied the reported diff. It seems I have to compile all of TF to run the test/update script?", "@frankchn This is currently still not easily usable due to https://github.com/tensorflow/tensorflow/issues/36550\r\n\r\nI have some code locally for that:\r\n```\r\ndef create_tf_config(num_worker=None, num_ps=0, trial=1, job=None, **kwargs):\r\n    \"\"\"Create a TensorFlow config suitable for TF_CONFIG\r\n    Args:\r\n      num_worker: Number of worker tasks, defaults to available tasks in current SLURM job (step)\r\n      num_ps: Number of parameter server tasks\r\n      trial: The identifier of the hyperparameter tuning trial currently running.\r\n      job: The job parameters you used when you initiated the job.\r\n    \"\"\"\r\n\r\n    if num_worker is None:\r\n        num_worker = get_num_slurm_tasks() - num_ps\r\n    assert num_worker > 0\r\n    assert num_ps >= 0\r\n\r\n    jobs = {'worker': num_worker}\r\n    if num_ps:\r\n        jobs['ps'] = num_ps\r\n    resolver_args = {'jobs': jobs}\r\n    resolver_args.update(kwargs)\r\n    resolver = SlurmClusterResolver(**resolver_args)\r\n    return {\r\n        'cluster': resolver.cluster_spec().as_dict(),\r\n        'task': {\r\n            'type': resolver.get_task_info()[0],\r\n            'index': resolver.get_task_info()[1],\r\n            'trial': trial\r\n        },\r\n        'rpc_layer': resolver.rpc_layer,\r\n        'job': job\r\n    }\r\n```\r\n\r\nI think it would make sense to set `TF_CONFIG` in the cluster resolver, at least until #36550 is resolved. Or provide some function for setting the TF_CONFIG variable from a cluster resolver (`return` part of the code). What do you think? How and where would you suggest to add this?\r\n\r\nEdit: Refined function based on v2.1.0 usages:\r\n```\r\ndef set_tf_config(resolver, environment=None):\r\n    \"\"\"Set the TF_CONFIG env variable from the given cluster resolver\"\"\"\r\n    cfg = {\r\n        'cluster': resolver.cluster_spec().as_dict(),\r\n        'task': {\r\n            'type': resolver.get_task_info()[0],\r\n            'index': resolver.get_task_info()[1],\r\n        },\r\n        'rpc_layer': resolver.rpc_layer,\r\n    }\r\n    if environment:\r\n        cfg['environment'] = environment\r\n    os.environ['TF_CONFIG'] = json.dumps(cfg)\r\n```", "@Flamefire Thanks for filing the bug! I would prefer the current changes you have go in first (it is undergoing internal review now) and let's see how #36550 shakes out since I am not that familiar with the current usages there.", "@frankchn What about the documentation update? https://github.com/tensorflow/tensorflow/blob/25d2d106cbe84969809fe546507a09c2e7f653f9/tensorflow/python/distribute/cluster_resolver/README.slurm is out of date now. I can update that with the description of \"my\" SlurmClusterResolver and how to use.\r\n\r\nRelated: I recently wrote an MPIClusterResolver inheriting from SlurmClusterResolver which uses mpi4py (i believe already a transitive dependency of TF) to fill in the template methods created by this. I can contribute that too if wanted.", "@Flamefire Sure, both of those will be super helpful! Thanks for contributing to TensorFlow :)", "Just create PRs and assign me as the reviewer.", "Ok, will do. Is there some auto-formatter that can be used? autopep8? yapf? Haven't seen a style definition file in the repo. \r\n\r\nEdit: There is a link to https://github.com/google/styleguide/blob/gh-pages/pyguide.md but that mentions 4 spaces indent while everything in TF seems to use 2 spaces", "According to the [Google Python style guide](http://google.github.io/styleguide/pyguide.html), yapf with `--style google` should work.", "The indentation is a major difference between our internal style guide and the public one, so you should use 2 spaces."]}, {"number": 36158, "title": "Fix hip-clang build", "body": "- Add clang 11.0 support.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36158) for more info**.\n\n<!-- need_sender_cla -->", "@whchung @sunway513 ", "@darkbuck thanks for submitting this. please sign individual CLA.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36158) for more info**.\n\n<!-- ok -->", "@chsigg may I ask for your help to expedite the merge of this PR? we are testing new toolchain with tip of TensorFlow + LLVM + Clang and this is breaking our CI."]}, {"number": 36157, "title": "Repair broken links", "body": "Fixes #36099 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36157) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36157) for more info**.\n\n<!-- ok -->", "> Don't think we should point to the old docs. Perhaps just point to the website: https://www.tensorflow.org/lite/convert\r\n> \r\n> Or these pages in GitHub: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/g3doc/convert\r\n\r\nI think this is the README for the old converter. The new convert is based off of this: https://github.com/tensorflow/tensorflow/blob/3cfbf14e80a4b5feb9e1a786e02ff705b42f83ef/tensorflow/lite/g3doc/convert/index.md"]}, {"number": 36156, "title": "Output from TFLite model converted using experimental converter does not match old converter", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): pip install tf-nightly\r\n- TensorFlow version (or github SHA if from source): 2.2.0-dev20200123\r\n\r\nEDIT: I am experimenting with converting the SSD MobileNet V2 models. Using the old converter, the model seems to work without any issues. However, the model converted with the new experimental converter does not match the results. I also tried a Float16 quantized version of the model and it also doesn't match the output of any the two float32 models.\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(frozen_model, input_arrays, output_arrays, input_shapes)\r\nconverter.allow_custom_ops = True\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen('ssd_mobilenet_v2_float32_experimental.tflite', \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\nComparison of model output with old and new experimental converter.\r\n```\r\nAssertionError: \r\nArrays are not almost equal to 5 decimals\r\n\r\nMismatched elements: 40 / 40 (100%)\r\nMax absolute difference: 11.113381\r\nMax relative difference: 3501.543\r\n x: array([[ -8.2944 , -10.18906,  -9.16781,  -8.85916],\r\n       [ -8.63264,  -9.66667,  -8.13753, -11.32026],\r\n       [ -7.67533,  -9.24478,  -9.12119, -11.07866],...\r\n y: array([[ 2.46333e-01, -4.78567e-01, -7.21600e-01,  4.30845e-02],\r\n       [ 3.55053e-01,  1.12653e+00, -3.39829e+00, -5.85306e+00],\r\n       [ 7.73718e-01,  4.47561e-01,  1.09934e+00, -1.17987e-02],...\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n[Colab notebook with model](https://colab.research.google.com/drive/1J4As3g3BY1zkdRdHHXySz_SgvfuThqHF)\r\n\r\n**Failure details**\r\nThe output produced by models using various options all output different results as demonstrated in notebook. I also created a Float16 Quantized version of the SSD MobileNet V2 model however that doesn't produce a similar precision output to the two F32 models.", "comments": ["Ashwin can you please check\r\n\r\nThanks", "We investigated the model and found it's actually working as intended. \r\n\r\nHere's the detail: The model outputs up to 10 results, but the actual number of results is stored in an output tensor (output 3). \r\nThe other outputs always allocate enough memory to handle 10 results, but only the first `num_results` rows are meaningful. The rest of output buffer is uninitialized. \r\n\r\nBelow is the code snippet to fix the testing code:\r\n```\r\n# Get the number of results from the 2 models, and check if it matches. \r\ntflite_num_results_1 = interpreter_1.get_tensor(output_details_1[3]['index'])[0]\r\ntflite_num_results_2 = interpreter_2.get_tensor(output_details_2[3]['index'])[0]\r\nnp.testing.assert_equal(tflite_num_results_1, tflite_num_results_2)\r\nnum_results = int(tflite_num_results_1)\r\n\r\n# Get the actual results (bonding boxes), while slicing the first num_results rows.\r\ntflite_results_1 = interpreter_1.get_tensor(output_details_1[0]['index'])[0, :num_results, :]\r\ntflite_results_2 = interpreter_2.get_tensor(output_details_2[0]['index'])[0, :num_results, :]\r\n\r\n# Compare the result.\r\nfor result_1, result_2 in zip(tflite_results_1, tflite_results_2):\r\n  np.testing.assert_almost_equal(result_1, result_2, decimal=5)\r\n```\r\n\r\nThanks for trying out the experimental converter. Let us know if you have further questions!"]}, {"number": 36155, "title": "Memory leak in Tensorflow Graph on CPU", "body": "I have a Face Detector that I'm trying to use for inference in Golang via official [Tensorflow bindings](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go). However, I faced a stepwise memory leak that causes the killing of an application due to OOM.\r\n\r\n\r\n# Screenshot with the leak:\r\n![image](https://user-images.githubusercontent.com/2982775/72971506-d8f01900-3dda-11ea-8b7e-8057879f15de.png)\r\n\r\nHeap samples from pprof does not show anything interesting in Go code. It looks like the leak on C++ backend side:\r\n```\r\ngo tool pprof http://localhost:8200/debug/pprof/heap\r\nFetching profile over HTTP from http://localhost:8200/debug/pprof/heap\r\nSaved profile in /pprof.facedetector.alloc_objects.alloc_space.inuse_objects.inuse_space.073.pb.gz\r\nFile: facedetector\r\nBuild ID: c4dba901e690718468bdd4fa7d1a631daca9e65a\r\nType: inuse_space\r\nTime: Jan 22, 2020 at 11:44pm (MSK)\r\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\r\n(pprof)\r\n(pprof) top20\r\nShowing nodes accounting for 1861.64kB, 100% of 1861.64kB total\r\n      flat  flat%   sum%        cum   cum%\r\n  930.82kB 50.00% 50.00%   930.82kB 50.00%  bytes.makeSlice\r\n  930.82kB 50.00%   100%   930.82kB 50.00%  main.(*FaceDetector).FindFaces\r\n         0     0%   100%   930.82kB 50.00%  bytes.(*Buffer).Grow\r\n         0     0%   100%   930.82kB 50.00%  bytes.(*Buffer).grow\r\n         0     0%   100%   930.82kB 50.00%  io/ioutil.ReadFile\r\n         0     0%   100%   930.82kB 50.00%  io/ioutil.readAll\r\n         0     0%   100%  1861.64kB   100%  main.main\r\n         0     0%   100%  1861.64kB   100%  runtime.main\r\n```\r\n\r\n\r\n# Code to reproduce the issue\r\nThis code just reading an image from a file system in a for-loop and feed it to the model.\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"io/ioutil\"\r\n\t\"log\"\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"net/http\"\r\n\t_ \"net/http/pprof\"\r\n)\r\n\r\ntype FaceDetector struct {\r\n\tsession         *tf.Session\r\n\tgraph           *tf.Graph\r\n\r\n\tinputOp  tf.Output\r\n\tbboxesOp tf.Output\r\n\tscoresOp tf.Output\r\n}\r\n\r\nfunc NewFaceDetector(frozenGraphPath string) (*FaceDetector, error) {\r\n\tfd := &FaceDetector{}\r\n\tmodel, err := ioutil.ReadFile(frozenGraphPath)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tfd.graph = tf.NewGraph()\r\n\tif err := fd.graph.Import(model, \"\"); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tfd.session, err = tf.NewSession(fd.graph, nil)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tfd.inputOp = fd.graph.Operation(\"input_image\").Output(0)\r\n\tfd.bboxesOp = fd.graph.Operation(\"bboxes\").Output(0)\r\n\tfd.scoresOp = fd.graph.Operation(\"scores_1/GatherV2\").Output(0)\r\n\r\n\treturn fd, nil\r\n}\r\n\r\nfunc (fd *FaceDetector) Close() error {\r\n\tif err := fd.session.Close(); err != nil {\r\n\t\treturn err\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc (fd *FaceDetector) FindFaces(image []byte) ([]float32, [][]int32, error) {\r\n\timageTensor, err := tf.NewTensor(string(image))\r\n\tif err != nil {\r\n\t\treturn nil, nil, err\r\n\t}\r\n\r\n\toutput, err := fd.session.Run(\r\n\t\tmap[tf.Output]*tf.Tensor{\r\n\t\t\tfd.inputOp: imageTensor,\r\n\t\t},\r\n\t\t[]tf.Output{\r\n\t\t\tfd.bboxesOp,\r\n\t\t\tfd.scoresOp,\r\n\t\t},\r\n\t\tnil)\r\n\r\n\tif err != nil {\r\n\t\treturn nil, nil, err\r\n\t}\r\n\r\n\tbboxes := output[0].Value().([][]int32)\r\n\tscores := output[1].Value().([]float32)\r\n\r\n\treturn scores, bboxes, nil\r\n}\r\n\r\nfunc main() {\r\n\tlog.SetFlags(log.LstdFlags | log.Lshortfile | log.Lmicroseconds)\r\n\r\n\tgo func() {\r\n\t\tlog.Println(http.ListenAndServe(\"0.0.0.0:8200\", nil))\r\n\t}()\r\n\r\n\tfd, err := NewFaceDetector(\"OptimizedGraph.pb\")\r\n\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tdefer fd.Close()\r\n\r\n\tfor {\r\n\t\timg, err := ioutil.ReadFile(\"photo.jpg\")\r\n\r\n\t\tif err != nil {\r\n\t\t\tlog.Println(\"Fail to read image:\", err)\r\n\t\t}\r\n\r\n\t\tscores, boxes, err := fd.FindFaces(img)\r\n\r\n\t\tif err != nil {\r\n\t\t\tlog.Println(\"Fail to infer:\", err)\r\n\t\t}\r\n\r\n\t\tlog.Println(scores, boxes)\r\n\t}\r\n}\r\n\r\n```\r\n\r\n\r\n# System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu-based Docker images**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Not tested**\r\n- TensorFlow installed from (source or binary): **Binary/Pre-compiled** (https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz)\r\n- TensorFlow version (use command below): **1.15.0**\r\n- Python version: -\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n- Golang version: **1.13**\r\n\r\n**photo.jpg:** https://user-images.githubusercontent.com/2982775/72982648-96393b80-3df0-11ea-807f-c4979b4c3af2.jpg\r\n**TF Graph:** [graph.zip](https://github.com/tensorflow/tensorflow/files/4102930/graph.zip)\r\n", "comments": ["Upd. \r\n\r\nI tested this on Python 3.7 (TF 1.15.0) and got a memory leak again.\r\n\r\nLook at the spike at the end:\r\n![image](https://user-images.githubusercontent.com/2982775/72993142-eec70380-3e05-11ea-9f6f-49f5b1c231a4.png)\r\n\r\nPython code:\r\n```Python\r\nimport tensorflow as tf\r\n\r\nmodel_path = \"./OptimizedGraph.pb\"\r\n\r\nwith tf.gfile.GFile(model_path, 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    tf.import_graph_def(graph_def, name=\"\")\r\n\r\ninput_image = graph.get_tensor_by_name('input_image:0')\r\noutput_ops = [\r\n    graph.get_tensor_by_name('bboxes:0'),\r\n    graph.get_tensor_by_name('scores_1/GatherV2:0'),\r\n]\r\n\r\nsess = tf.Session(graph=graph)\r\n\r\nwhile True:\r\n    with open(\"./photo.jpg\", \"rb\") as f:\r\n        image = f.read()\r\n\r\n    boxes, scores = sess.run(output_ops, feed_dict={input_image: image})\r\n    print(scores)\r\n\r\nsess.close()\r\n```\r\n", "Tested yet another [model](https://yadi.sk/d/AYypBO3ke2c2ig) (NSFW detector). There is a memory leak too.\r\n\r\n<img width=\"967\" alt=\"Screenshot 2020-01-24 at 11 36 55\" src=\"https://user-images.githubusercontent.com/2982775/73055428-d4d8ff80-3e9d-11ea-9919-aea207e1cf8c.png\">\r\n\r\nCode:\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"io/ioutil\"\r\n\t\"math/rand\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n)\r\n\r\nfunc main() {\r\n\tgraphFile, err := ioutil.ReadFile(\"nsfw_core_tf/nsfw_model.pb\")\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tgraph := tf.NewGraph()\r\n\tif err := graph.Import(graphFile, \"\"); err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tsession, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tinputOp := graph.Operation(\"DecodeJpeg/contents\").Output(0)\r\n\tresultOp := graph.Operation(\"final_result\").Output(0)\r\n\r\n\timages := make([]string, 0)\r\n\r\n\terr = filepath.Walk(\"nsfw_core_tf/imgs/\", func(path string, info os.FileInfo, err error) error {\r\n\t\tif !info.IsDir() && filepath.Ext(path) == \".jpg\" {\r\n\t\t\timages = append(images, path)\r\n\t\t}\r\n\r\n\t\treturn nil\r\n\t})\r\n\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\r\n\tfor {\r\n\t\timgIdx := rand.Int() % len(images)\r\n\r\n\t\timg, err := ioutil.ReadFile(images[imgIdx])\r\n\r\n\t\tif err != nil {\r\n\t\t\tpanic(err)\r\n\t\t}\r\n\r\n\t\timageTensor, err := tf.NewTensor(string(img))\r\n\t\tif err != nil {\r\n\t\t\tpanic(err)\r\n\t\t}\r\n\r\n\t\toutput, err := session.Run(\r\n\t\t\tmap[tf.Output]*tf.Tensor{inputOp: imageTensor},\r\n\t\t\t[]tf.Output{resultOp},\r\n\t\t\tnil)\r\n\r\n\t\tif err != nil {\r\n\t\t\tpanic(err)\r\n\t\t}\r\n\r\n\t\tscores := output[0].Value().([][]float32)\r\n\t\tpornoScore := scores[0][1]\r\n\r\n\t\tfmt.Println(images[imgIdx], pornoScore)\r\n\t}\r\n}\r\n```\r\n", "Short output of Valgrind:\r\n```\r\n$ valgrind ./facedetector\r\n==90015== Memcheck, a memory error detector\r\n==90015== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==90015== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info\r\n==90015== Command: ./facedetector\r\n==90015==\r\n==90015== Warning: ignored attempt to set SIGRT32 handler in sigaction();\r\n==90015==          the SIGRT32 signal is used internally by Valgrind\r\n==90015== Warning: ignored attempt to set SIGRT32 handler in sigaction();\r\n==90015==          the SIGRT32 signal is used internally by Valgrind\r\n==90015== Warning: client switching stacks?  SP change: 0x1fff000348 --> 0xc0000427d8\r\n==90015==          to suppress, use: --max-stackframe=687211816080 or greater\r\n==90015== Warning: client switching stacks?  SP change: 0xc000042758 --> 0x1fff000400\r\n==90015==          to suppress, use: --max-stackframe=687211815768 or greater\r\n==90015== Warning: client switching stacks?  SP change: 0x1fff000400 --> 0xc000042758\r\n==90015==          to suppress, use: --max-stackframe=687211815768 or greater\r\n==90015==          further instances of this message will not be shown.\r\n==90015== Conditional jump or move depends on uninitialised value(s)\r\n==90015==    at 0x44BCE7: runtime.adjustframe (/usr/local/go/src/runtime/stack.go:537)\r\n==90015==    by 0x45779C: runtime.gentraceback (/usr/local/go/src/runtime/traceback.go:334)\r\n==90015==    by 0x44C4C9: runtime.copystack (/usr/local/go/src/runtime/stack.go:886)\r\n==90015==    by 0x44C8CA: runtime.newstack (/usr/local/go/src/runtime/stack.go:1055)\r\n==90015==    by 0x4616FE: runtime.morestack (/usr/local/go/src/runtime/asm_amd64.s:449)\r\n==90015==    by 0x4614B3: runtime.rt0_go (/usr/local/go/src/runtime/asm_amd64.s:220)\r\n==90015==    by 0x772C0F: ??? (in /root/src/general-face-detector/facedetector)\r\n==90015==\r\n==90015== Conditional jump or move depends on uninitialised value(s)\r\n==90015==    at 0x44BCED: runtime.adjustframe (/usr/local/go/src/runtime/stack.go:537)\r\n==90015==    by 0x45779C: runtime.gentraceback (/usr/local/go/src/runtime/traceback.go:334)\r\n==90015==    by 0x44C4C9: runtime.copystack (/usr/local/go/src/runtime/stack.go:886)\r\n==90015==    by 0x44C8CA: runtime.newstack (/usr/local/go/src/runtime/stack.go:1055)\r\n==90015==    by 0x4616FE: runtime.morestack (/usr/local/go/src/runtime/asm_amd64.s:449)\r\n==90015==    by 0x4614B3: runtime.rt0_go (/usr/local/go/src/runtime/asm_amd64.s:220)\r\n==90015==    by 0x772C0F: ??? (in /root/src/general-face-detector/facedetector)\r\n==90015==\r\n2020-01-27 16:32:04.623138: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: saved\r\n2020-01-27 16:32:05.035236: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-01-27 16:32:05.261579: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-27 16:32:05.698834: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2099995000 Hz\r\n2020-01-27 16:32:05.745237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x20454e70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-27 16:32:05.750507: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-27 16:32:06.728451: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.\r\n2020-01-27 16:32:06.733516: I tensorflow/cc/saved_model/loader.cc:212] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: saved/variables/variables.index\r\n2020-01-27 16:32:06.749386: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 2129638 microseconds.\r\n==90015== Conditional jump or move depends on uninitialised value(s)\r\n==90015==    at 0x4C938C: reflect.typesByString (/usr/local/go/src/reflect/type.go:1738)\r\n==90015==\r\n2020/01/27 16:34:48.645185 main.go:95: [0.94583106 0.9434663 0.93685865 0.93612325 0.9334286 0.9332391 0.92946804 0.9264806 0.9199306 0.9178868 0.91611177 0.91398704 0.90572965 0.9011878 0.89952284 0.89777416 0.897482 0.8922111 0.8867197 0.8847141 0.8702864 0.8671061 0.86256355 0.85984224 0.84040093 0.8260047 0.80469215] [[1425 551 1471 614] [1677 736 1731 811] [993 661 1045 734] [1148 545 1200 617] [642 592 692 662] [1764 543 1818 617] [1831 762 1882 834] [545 682 598 754] [1584 552 1633 617] [237 736 289 815] [1569 679 1622 757] [355 693 409 771] [1119 729 1174 814] [1752 684 1807 762] [1314 727 1368 808] [538 543 584 611] [1284 556 1332 627] [1364 671 1418 750] [649 747 699 817] [442 754 494 834] [1696 488 1741 555] [813 555 863 622] [1174 658 1226 725] [390 492 440 574] [176 682 224 755] [761 677 812 746] [1464 732 1513 807]]\r\n==90015==\r\n==90015== HEAP SUMMARY:\r\n==90015==     in use at exit: 18,332,179 bytes in 249,733 blocks\r\n==90015==   total heap usage: 2,326,773 allocs, 2,077,040 frees, 697,890,067 bytes allocated\r\n==90015==\r\n==90015== LEAK SUMMARY:\r\n==90015==    definitely lost: 0 bytes in 0 blocks\r\n==90015==    indirectly lost: 0 bytes in 0 blocks\r\n==90015==      possibly lost: 780,432 bytes in 11,899 blocks\r\n==90015==    still reachable: 17,551,747 bytes in 237,834 blocks\r\n==90015==                       of which reachable via heuristic:\r\n==90015==                         stdstring          : 6,673,450 bytes in 67,087 blocks\r\n==90015==                         newarray           : 512,992 bytes in 2,410 blocks\r\n==90015==         suppressed: 0 bytes in 0 blocks\r\n==90015== Rerun with --leak-check=full to see details of leaked memory\r\n==90015==\r\n==90015== Use --track-origins=yes to see where uninitialised values come from\r\n==90015== For lists of detected and suppressed errors, rerun with: -s\r\n==90015== ERROR SUMMARY: 31 errors from 3 contexts (suppressed: 0 from 0)\r\n```\r\n", "Full output of `valgrind -s --leak-check=full --log-file=valgrind-out.txt ./facedetector`\r\n[valgrind-out.txt](https://github.com/tensorflow/tensorflow/files/4117365/valgrind-out.txt)\r\n", "I have the same issue, but I use tf.LoadSavedModel, I tried assign model to nil, defer session.close(), golang garbage collection, none of them works, go pprof or cmd top both show the same result, loadsavedmodel did not release memory.\r\n\r\nin my case, i had to choose one of hundreds model file and load it to inference, I know there's a mechanism stop my session.close or model release due to I'm sending request frequently\r\n\r\nis there anything i can do? I choose golang and try prevent from python memory leak nightmare, now I put myself into another one, sight", "Hello everyone !\r\n\r\nSince we switch our prediction platform to Tensorflow we also experienced a memory leak.\r\n\r\nAfter finding that out, we used **go tool pprof** in order to found where leaks where happening but it was useless. Impossible to find the leak using pprof. In fact, **pprof**'s _inuse_space_ sum output was completely different from **top**'s. Everything seemed to be fine on **pprof** but **top** was showing a constant rise in memory usage.\r\n\r\nWe tried to figured out if our code could be the issue and it is not. No leak at all without any call to TF and still nothing of a memory leak shown by **pprof**.\r\n\r\nWe tried to abstract as much as possible and finished with this piece of code. \r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"runtime\"\r\n\t\"time\"\r\n)\r\n\r\nfunc printStats(mem runtime.MemStats) {\r\n\r\n\truntime.ReadMemStats(&mem)\r\n\r\n\tfmt.Println(\"mem.Alloc:\", mem.Alloc)\r\n\r\n\tfmt.Println(\"mem.TotalAlloc:\", mem.TotalAlloc)\r\n\r\n\tfmt.Println(\"mem.HeapAlloc:\", mem.HeapAlloc)\r\n\r\n\tfmt.Println(\"mem.NumGC:\", mem.NumGC)\r\n\r\n\tfmt.Println(\"-----\")\r\n\r\n}\r\n\r\nfunc loadModel(modelPath string, tags []string) {\r\n\tmodel, err := tf.LoadSavedModel(modelPath, tags, nil)\r\n\tif err != nil {\r\n\t\tfmt.Println(\"error OPENING model\")\r\n\t\treturn\r\n\t}\r\n\terr = model.Session.Close()\r\n\tif err != nil {\r\n\t\tfmt.Println(\"error CLOSING model\")\r\n\t\treturn\r\n\t}\r\n}\r\n\r\nfunc main() {\r\n\tvar mem runtime.MemStats\r\n\r\n\tprintStats(mem)\r\n\ttags := []string{\"serve\"}\r\n\tfor i := 0; i < 300; i++ {\r\n\t\tloadModel(\"./path_to_model/\", tags)\r\n\t\tfmt.Println(\"loading model: => \", i)\r\n\t\tprintStats(mem)\r\n\t}\r\n\tfmt.Println(\"done\")\r\n\tprintStats(mem)\r\n\tfmt.Println(\"call to garbage collector\")\r\n\truntime.GC()\r\n\tprintStats(mem)\r\n\tfmt.Println(\"sleeping 10 minutes\")\r\n\ttime.Sleep(time.Minute * 10)\r\n\tfmt.Println(\"complete done\")\r\n}\r\n```\r\n\r\nWe are basically loading a model 300 times, closing them right after and then calling the Garbage Collector at the end of the process to force the memory pointed by useless pointers to be freed. However, whether it be with TF 1.15.2 or 2.0.1 we are experiencing something weird. The models are, as weird as it is, being freed *some time to times*. I let you run this program several times and see what top says on the process's memory.\r\n\r\nWe are expecting it to decrease as soon as the GC is called. On both 1.15.2 & 2.0.1, sometimes it decreases, sometimes it does not and we think it is linked with some useless pointers still used by TF somewhere, preventing the GC to free them. Note that with our recents tests we can point that this issue seems to happens more often with TF 2.0.1 than with 1.15.2.\r\n\r\nIn our case and with our model, the program start at around 100-200Mo, reach 1.5Go at the end of the 300 models loading and then when the GC is called, the inuse memory shown by top drops back to 150Mo or sometimes doesn't and stays at 1.5Go.\r\n\r\nAll of this lead us to believe that the issue is from the C++ code OR from the CGo bindings of Tensorflow. We couldn't figured out anything else. An investigation would be appreciated \ud83d\ude80 ", "Execute the following code, a memory leak will occur\r\n\r\n```go\r\nfor i := 0; i < 100; i++ {\r\n\ttf.LoadSavedModel(\"/model/fm_model_v1\", []string{\"serve\"}, nil)\r\n\ttime.sleep(10 * time.Second)\r\n}\r\n\r\n", "![Image 2020-03-10 22-39-17](https://user-images.githubusercontent.com/20059289/76325130-f09b4680-6321-11ea-831f-107d4ea167b6.png)\r\n", "any update on this ? ", "I have the same issue; pprof did not detected any heap memory leak;\r\nany update on this?", "same issue ; need help", "I meet a similar issue on inference pb files, hope for help.", "I have narrow it down to NewTensor also creating memory leak. \r\nIt seems the golang garbage collector does not clear the whole unused Tensor.", "It seems that tensorflow has no limit on cpu utilization by default\uff0cmemory usage will increase until OOM if you don't add limit. Try add ` tf_config = tf.ConfigProto(inter_op_parallelism_threads=1,intra_op_parallelism_threads=1))  sess=tf.Session(config=tf_config)` and it works for me.", "I think am observing the same. @jerklee I tried your approach of setting the configproto in my golang implementation but that has no effect. I see a constant memory increase whenever a new model file gets loaded. I also tried dereferencing the session and graph pointers so that the runtime finalizers kick in and release the memory but there's been no effect doing that. I'd like to know how the allocated memory from the previous model load can be unallocated safely to stop OOM. Am using TF 1.15 ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 36154, "title": "Error while converting to quantized tflite.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.1.0\r\n- Python version : 3.5.6 anaconda\r\n\r\n**Describe the Problem**\r\nI implemented lite-weight custom model based on SSD Paper to detect just one Object, Person.\r\nI want to run that model on Coral EdgeTpu which is connected to Raspberry Pi 4 B.\r\nI follow this doc (https://coral.ai/docs/accelerator/get-started/)  to set up coral on rpi and \r\nfollow this doc (https://coral.ai/docs/edgetpu/compiler/) to install EdgeTpu Compiler on my laptop.\r\nWant to convert tflite to edgetpu_tflite, but tflite need to be quantized so follow that doc\r\n(https://www.tensorflow.org/lite/performance/post_training_integer_quant) to do that.\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport cv2\r\nimport numpy as np\r\nimport os\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./frozen_models/ssd_tf/')\r\n# fitst set the optimizations flag to optimize for size\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n\r\ntrain_data_dir = '/home/shashank/Documents/SSD7_tf/Training-data/test/'\r\n\r\nimages = []\r\nfor each_img in os.listdir(train_data_dir):\r\n    read_img = cv2.imread(train_data_dir + each_img).astype(np.float32)\r\n    norm_img = (read_img - np.min(read_img, axis=(0,1,2), keepdims=True))/ (np.max(read_img, axis=(0,1,2), keepdims=True) - np.min(read_img, axis=(0,1,2), keepdims=True))\r\n    images.append(norm_img)\r\n\r\nimages = np.array(images)\r\nimages_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n\r\ndef representative_data_gen():\r\n    for input_value in images_ds.take(100):\r\n        yield [input_value]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model_quant = converter.convert()\r\ntflite_model_quant_file = 'model_quant_io.tflite'\r\ntflite_model_quant_file.write_bytes(tflite_model_quant)\r\nprint('Done!')\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-01-23 15:15:34.464864: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-01-23 15:15:34.464958: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-01-23 15:15:34.464974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-01-23 15:15:35.849036: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-01-23 15:15:35.849060: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-01-23 15:15:35.849095: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (shashank): /proc/driver/nvidia/version does not exist\r\n2020-01-23 15:15:35.849245: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-23 15:15:35.873039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\r\n2020-01-23 15:15:35.873694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559e284c8740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-23 15:15:35.873733: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-23 15:15:38.481817: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 544320000 exceeds 10% of system memory.\r\n2020-01-23 15:15:38.675897: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-01-23 15:15:38.676034: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-23 15:15:38.717217: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-01-23 15:15:38.717254: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 543 nodes (474), 995 edges (926), time = 12.469ms.\r\n2020-01-23 15:15:38.717260: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 543 nodes (0), 995 edges (0), time = 8.12ms.\r\n2020-01-23 15:15:38.717264: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_true_2404_6083\r\n2020-01-23 15:15:38.717287: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-23 15:15:38.717290: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_false_2596_2599\r\n2020-01-23 15:15:38.717298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717301: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717305: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_false_2547_5852\r\n2020-01-23 15:15:38.717308: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-23 15:15:38.717312: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717315: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_true_2546_1326\r\n2020-01-23 15:15:38.717319: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-23 15:15:38.717323: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_cond_2488_1300\r\n2020-01-23 15:15:38.717331: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717335: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717339: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_false_2652_4375\r\n2020-01-23 15:15:38.717343: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717346: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-23 15:15:38.717350: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_body_2489_5925\r\n2020-01-23 15:15:38.717354: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 80 nodes (0), 85 edges (0), time = 1.227ms.\r\n2020-01-23 15:15:38.717357: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 80 nodes (0), 85 edges (0), time = 1.357ms.\r\n2020-01-23 15:15:38.717361: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_true_2651_2758\r\n2020-01-23 15:15:38.717365: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-23 15:15:38.717369: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717372: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_true_2595_4150\r\n2020-01-23 15:15:38.717377: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717381: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:38.717384: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_false_2405_759\r\n2020-01-23 15:15:38.717388: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2020-01-23 15:15:38.717392: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-01-23 15:15:39.008345: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2020-01-23 15:15:39.008521: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-01-23 15:15:39.071925: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2020-01-23 15:15:39.071961: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 435 nodes (-60), 804 edges (-126), time = 21.02ms.\r\n2020-01-23 15:15:39.071983: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 435 nodes (0), 804 edges (0), time = 11.699ms.\r\n2020-01-23 15:15:39.071987: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_true_2404_6083_frozen\r\n2020-01-23 15:15:39.071992: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 26 nodes (0), 24 edges (0), time = 0.477ms.\r\n2020-01-23 15:15:39.072011: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 26 nodes (0), 24 edges (0), time = 0.382ms.\r\n2020-01-23 15:15:39.072016: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_cond_false_2405_759_frozen\r\n2020-01-23 15:15:39.072020: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 16 edges (0), time = 0.366ms.\r\n2020-01-23 15:15:39.072023: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 16 edges (0), time = 0.293ms.\r\n2020-01-23 15:15:39.072026: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_false_2652_4375_frozen\r\n2020-01-23 15:15:39.072030: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 24 edges (0), time = 0.473ms.\r\n2020-01-23 15:15:39.072033: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 22 nodes (0), 24 edges (0), time = 0.391ms.\r\n2020-01-23 15:15:39.072037: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_true_2546_1326_frozen\r\n2020-01-23 15:15:39.072040: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (-1), 1 edges (-1), time = 0.226ms.\r\n2020-01-23 15:15:39.072044: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (0), 1 edges (0), time = 0.066ms.\r\n2020-01-23 15:15:39.072047: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_cond_2488_1300_frozen\r\n2020-01-23 15:15:39.072051: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 8 edges (0), time = 0.255ms.\r\n2020-01-23 15:15:39.072055: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 8 edges (0), time = 0.179ms.\r\n2020-01-23 15:15:39.072059: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_2_true_2651_2758_frozen\r\n2020-01-23 15:15:39.072062: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 11 nodes (0), 11 edges (0), time = 0.284ms.\r\n2020-01-23 15:15:39.072066: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 11 nodes (0), 11 edges (0), time = 0.188ms.\r\n2020-01-23 15:15:39.072069: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_true_2595_4150_frozen\r\n2020-01-23 15:15:39.072073: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (-1), 1 edges (-1), time = 0.21ms.\r\n2020-01-23 15:15:39.072077: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 4 nodes (0), 1 edges (0), time = 0.078ms.\r\n2020-01-23 15:15:39.072081: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_1_false_2596_2599_frozen\r\n2020-01-23 15:15:39.072084: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 39 nodes (0), 43 edges (0), time = 0.726ms.\r\n2020-01-23 15:15:39.072088: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 39 nodes (0), 43 edges (0), time = 0.745ms.\r\n2020-01-23 15:15:39.072092: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_model_1_decoded_predictions_loop_over_batch_while_body_2489_5925_frozen\r\n2020-01-23 15:15:39.072096: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 81 nodes (1), 87 edges (2), time = 2.628ms.\r\n2020-01-23 15:15:39.072100: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 81 nodes (0), 87 edges (0), time = 1.5ms.\r\n2020-01-23 15:15:39.072103: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_cond_false_2547_5852_frozen\r\n2020-01-23 15:15:39.072107: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 37 nodes (1), 41 edges (2), time = 1.256ms.\r\n2020-01-23 15:15:39.072111: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 37 nodes (0), 41 edges (0), time = 0.653ms.\r\nTraceback (most recent call last):\r\n  File \"generate_quant_tflite.py\", line 30, in <module>\r\n    tflite_model_quant = converter.convert()\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 464, in convert\r\n    **converter_kwargs)\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 457, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/convert.py\", line 203, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-01-23 15:15:40.457472: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-01-23 15:15:40.457547: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-01-23 15:15:40.457557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2020-01-23 15:15:41.229352: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-01-23 15:15:41.253051: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1800000000 Hz\r\n2020-01-23 15:15:41.253841: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f574b1d500 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-23 15:15:41.253867: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-23 15:15:41.255187: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-01-23 15:15:41.255200: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-01-23 15:15:41.255215: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (shashank): /proc/driver/nvidia/version does not exist\r\n2020-01-23 15:15:41.266040: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: StatelessIf\r\n2020-01-23 15:15:41.266100: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x00007f4c7a502740 (most recent call first):\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56 in execute\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/absl/app.py\", line 250 in _run_main\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/absl/app.py\", line 299 in run\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/python/platform/app.py\", line 40 in run\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93 in main\r\n  File \"/home/shashank/ENTER/envs/dl_py35/bin/toco_from_protos\", line 8 in <module>\r\nAborted (core dumped)\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nPlease copy and paste this link to browser.\r\n(https://github.com/tensorflow/tensorflow/files/4102761/tflite_bug_related_folder.zip)\r\nThis folder contains source code to generate the saved model folder.\r\nIt also contain saved model folder also.\r\nrun python ssd7_inference.py\r\n```", "comments": ["@haozha111 Could you please take a look of this issue.", "Have you tried using the new MLIR tflite converter?\r\n\r\nadd this line after you construct the converter object:\r\nconverter.experimental_new_converter = True ", "@haozha111  Thanks for quick reply.\r\n\r\n**Now i'm getting this error.**\r\nTraceback (most recent call last):\r\n  File \"generate_quant_tflite.py\", line 38, in <module>\r\n    tflite_model_quant = converter.convert()\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 469, in convert\r\n    self.experimental_new_quantizer)\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/lite.py\", line 243, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, enable_mlir_quantizer)\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 81, in calibrate_and_quantize\r\n    enable_mlir_quantizer)\r\n  File \"/home/shashank/ENTER/envs/dl_py35/lib/python3.5/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, *args)\r\n\r\nRuntimeError: Quantization not yet supported for op: REDUCE_MAX\r\n\r\nIf i comment this line then this error get disappear.\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\nIt mean the converted model is not fully quantize.\r\nI don't understand where is REDUCE_MAX op in my model.\r\n\r\n**my model is. **\r\n    x = Input(shape=(img_height, img_width, img_channels))\r\n\r\n    conv1 = Conv2D(48, (5, 5), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv1')(x)\r\n    conv1 = BatchNormalization(axis=3, momentum=0.99, name='bn1')(conv1) # Tensorflow uses filter format [filter_height, filter_width, in_channels, out_channels], hence axis = 3\r\n    conv1 = ReLU(name='relu1')(conv1)\r\n    pool1 = MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1) #150x150\r\n\r\n    conv2 = Conv2D(48, (5, 5), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv2')(pool1)\r\n    conv2 = BatchNormalization(axis=3, momentum=0.99, name='bn2')(conv2) # Tensorflow uses filter format [filter_height, filter_width, in_channels, out_channels], hence axis = 3\r\n    conv2 = ReLU(name='relu2')(conv2)\r\n\r\n    conv3 = Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv3')(conv2)\r\n    conv3 = BatchNormalization(axis=3, momentum=0.99, name='bn3')(conv3)\r\n    conv3 = ReLU(name='relu3')(conv3)\r\n    pool2 = MaxPooling2D(pool_size=(2, 2), name='pool2')(conv3) #75x75\r\n\r\n    conv4 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv4')(pool2)\r\n    conv4 = BatchNormalization(axis=3, momentum=0.99, name='bn4')(conv4)\r\n    conv4 = ReLU(name='relu4')(conv4)\r\n\r\n    conv5 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv5')(conv4)\r\n    conv5 = BatchNormalization(axis=3, momentum=0.99, name='bn5')(conv5)\r\n    conv5 = ReLU(name='relu5')(conv5)\r\n    pool3 = MaxPooling2D(pool_size=(2, 2), name='pool3')(conv5) #37x37\r\n\r\n    conv6 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv6')(pool3)\r\n    conv6 = BatchNormalization(axis=3, momentum=0.99, name='bn6')(conv6)\r\n    conv6 = ReLU(name='relu6')(conv6)\r\n\r\n    conv7 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv7')(conv6)\r\n    conv7 = BatchNormalization(axis=3, momentum=0.99, name='bn7')(conv7)\r\n    conv7 = ReLU(name='relu7')(conv7)\r\n    pool4 = MaxPooling2D(pool_size=(2, 2), name='pool4')(conv7) #18x18\r\n\r\n    conv8 = Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='conv8')(pool4)\r\n    conv8 = BatchNormalization(axis=3, momentum=0.99, name='bn8')(conv8)\r\n    conv8 = ReLU(name='relu8')(conv8)\r\n\r\n    classes6 = Conv2D(n_boxes[0] * n_classes, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='classes6')(conv6)\r\n    classes7 = Conv2D(n_boxes[1] * n_classes, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='classes7')(conv7)\r\n    classes8 = Conv2D(n_boxes[2] * n_classes, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='classes8')(conv8)\r\n    # Output shape of `boxes`: `(batch, height, width, n_boxes * 4)`\r\n    boxes6 = Conv2D(n_boxes[0] * 4, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='boxes6')(conv6)\r\n    boxes7 = Conv2D(n_boxes[1] * 4, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='boxes7')(conv7)\r\n    boxes8 = Conv2D(n_boxes[2] * 4, (3, 3), strides=(1, 1), padding=\"same\", kernel_regularizer=l2(l2_reg), name='boxes8')(conv8)\r\n\r\n    def tile_op(anchor_and_box):\r\n      return tf.tile(anchor_and_box[0], (tf.shape(anchor_and_box[1])[0], 1, 1, 1, 1))\r\n\r\n    load_anchor6 = pickle.load(open('anchor6.pkl', 'rb')).astype(np.float32)\r\n    anchors6 = Lambda(lambda x : tile_op(x))([load_anchor6, boxes6])\r\n\r\n    load_anchor7 = pickle.load(open('anchor7.pkl', 'rb')).astype(np.float32)\r\n    anchors7 = Lambda(lambda x : tile_op(x))([load_anchor7, boxes7])\r\n\r\n    load_anchor8 = pickle.load(open('anchor8.pkl', 'rb')).astype(np.float32)\r\n    anchors8 = Lambda(lambda x : tile_op(x))([load_anchor8, boxes8])\r\n    \r\n    classes6_reshaped = Reshape((-1, n_classes), name='classes6_reshape')(classes6)\r\n    classes7_reshaped = Reshape((-1, n_classes), name='classes7_reshape')(classes7)\r\n    classes8_reshaped = Reshape((-1, n_classes), name='classes8_reshape')(classes8)\r\n    # Reshape the box coordinate predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\r\n    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\r\n    \r\n    boxes6_reshaped = Reshape((-1, 4), name='boxes6_reshape')(boxes6)\r\n    boxes7_reshaped = Reshape((-1, 4), name='boxes7_reshape')(boxes7)\r\n    boxes8_reshaped = Reshape((-1, 4), name='boxes8_reshape')(boxes8)\r\n    # Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\r\n    \r\n    anchors6_reshaped = Reshape((-1, 8), name='anchors6_reshape')(anchors6)\r\n    anchors7_reshaped = Reshape((-1, 8), name='anchors7_reshape')(anchors7)\r\n    anchors8_reshaped = Reshape((-1, 8), name='anchors8_reshape')(anchors8)\r\n\r\n    # Concatenate the predictions from the different layers and the assosciated anchor box tensors\r\n    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\r\n    # so we want to concatenate along axis 1\r\n    # Output shape of `classes_concat`: (batch, n_boxes_total, n_classes)\r\n    classes_concat = Concatenate(axis=1, name='classes_concat')([classes6_reshaped,\r\n                                                                 classes7_reshaped,\r\n                                                                 classes8_reshaped])\r\n\r\n    # Output shape of `boxes_concat`: (batch, n_boxes_total, 4)\r\n    boxes_concat = Concatenate(axis=1, name='boxes_concat')([boxes6_reshaped,\r\n                                                             boxes7_reshaped,\r\n                                                             boxes8_reshaped])\r\n\r\n    # Output shape of `anchors_concat`: (batch, n_boxes_total, 8)\r\n    anchors_concat = Concatenate(axis=1, name='anchors_concat')([anchors6_reshaped,\r\n                                                                 anchors7_reshaped,\r\n                                                                 anchors8_reshaped])\r\n\r\n    # The box coordinate predictions will go into the loss function just the way they are,\r\n    # but for the class predictions, we'll apply a softmax activation layer first\r\n    classes_softmax = Activation('softmax', name='classes_softmax')(classes_concat)\r\n\r\n    # Concatenate the class and box coordinate predictions and the anchors to one large predictions tensor\r\n    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\r\n    predictions = Concatenate(axis=2, name='predictions')([classes_softmax, boxes_concat, anchors_concat])\r\n    \r\n    model = Model(inputs=x, outputs=predictions)", "Suharsh, could you take a look at this issue? Thanks!", "@suharshs , I'm waiting for your response.Thanks!", "Have you tried converting the model without any optimizations specified? Want to make sure first that your model can convert successfully for the float version before trying to add quantization.\r\n\r\nThanks!\r\n-Suharsh", "Hi @suharshs, Thanks for quick reply.\r\nThis is the result. I tried with TensorFlow 1.14\r\n```\r\npb_file = frozen_model.pb'\r\ninput_arrays = ['input_1']\r\noutput_arrays = ['predictions/concat']\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(pb_file, input_arrays, output_arrays)\r\n\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nYes, it does convert to float version.\r\nWhen i comment this line then it also successfully convert to tflite, but i don't know which OP is\r\nremains in float32 format.\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n\r\n**This is Converter Code**\r\nTried with Tensorflow Version 2.1.0\r\n```\r\ntflite_models_dir = pathlib.Path(\"./frozen_models/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('./frozen_models/ssd_tf/')\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n\r\ntrain_data_dir = '/home/shashank/Documents/SSD7_tf/Training-data/test/'\r\n\r\nimages = []\r\nfor each_img in os.listdir(train_data_dir):\r\n    read_img = cv2.imread(train_data_dir + each_img).astype(np.float32)\r\n    norm_img = (read_img - np.min(read_img, axis=(0,1,2), keepdims=True))/ (np.max(read_img, axis=(0,1,2), keepdims=True) - np.min(read_img, axis=(0,1,2), keepdims=True))\r\n    images.append(norm_img)\r\n\r\nimages = np.array(images)\r\nimages_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n\r\ndef representative_data_gen():\r\n    for input_value in images_ds.take(100):\r\n        yield [input_value]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_model_quant = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/'model_quant_io.tflite'\r\ntflite_model_quant_file.write_bytes(tflite_model_quant)\r\nprint('Done!')\r\n```", "Hmm, yes it seems that REDUCE_MAX doesn't yet have a quantized implementation, can you print all ops in your tensorflow graph and see is Max is in there for some reason?", "In case anybody wonders. It's keras' softmax activation which uses the tf.reduce_max."]}, {"number": 36153, "title": "MultiWorkerMirroredStrategy training does not work with multiple epochs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Minor adaption of https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: K80\r\n\r\n**Describe the current behavior**\r\n\r\nExecuting training over multiple epochs and workers as per the example fails with \"Empty training data\" and\r\n> WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1407 batches). You may need to use the repeat() function when building your dataset.\r\n\r\n**Describe the expected behavior**\r\n\r\nRunning training over multiple epochs automatically repeats the training data\r\n\r\n**Code to reproduce the issue**\r\n\r\nPretty much the exact example code from the MultiWorkerDistributed tutorial:\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport os\r\nimport json\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nfrom slurm_utils import create_tf_config\r\ntfds.disable_progress_bar()\r\n\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\n\r\n\r\ndef make_datasets_unbatched():\r\n    # Scaling MNIST data from (0, 255] to (0., 1.]\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n        return image, label\r\n\r\n    datasets, info = tfds.load(name='mnist',\r\n                               with_info=True,\r\n                               as_supervised=True)\r\n\r\n    return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32,\r\n                               3,\r\n                               activation='relu',\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\n\r\ntfConfig = create_tf_config(gpus_per_task=2)\r\nprint('Used Config: {}'.format(tfConfig))\r\nos.environ['TF_CONFIG'] = json.dumps(tfConfig)\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nprint('Number of workers: {}\\nParameter devices: {}\\nWorkers: {}'.format(\r\n    strategy.num_replicas_in_sync, strategy.extended.parameter_devices,\r\n    strategy.extended.worker_devices))\r\n\r\n# Here the batch size scales up by number of workers since\r\n# `tf.data.Dataset.batch` expects the global batch size.\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\r\nwith strategy.scope():\r\n    # Creation of dataset, and model building/compiling need to be within\r\n    # `strategy.scope()`.\r\n    train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n    multi_worker_model = build_and_compile_cnn_model()\r\n\r\nmulti_worker_model.fit(x=train_datasets, epochs=3)\r\n```\r\n\r\n**Other info / logs**\r\nThe above `create_tf_config` creates the following config from SLURM environment: `{'cluster': {'worker': ['taurusa9:8888']}, 'task': {'type': 'worker', 'index': 0, 'trial': 1}, 'job': None}`\r\n\r\n\r\nRunning with 1 epoch works without any other code changes", "comments": ["I may have found the cause to this issue: After iterating over a `Dataset` provided by `tfds` the iterator for that dataset is exhausted. There is some code here: https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/data_adapter.py#L203-L208\r\n\r\nThe size is always unknown because a `DatasetAdapter` is used. The `steps_per_epoch` is not None, because this is required for `MultiWorkerMirroredStrategy`. Hence the iterator is never recreated leading to the mentioned issue.\r\n\r\nI've seen that there were some changes, however https://github.com/tensorflow/tensorflow/commit/6be131d0860559954c42685a87c63f16cebb2185 does not fix the issue because `cardinality` returns `UNKNOWN` leading to NOT recreating the iterator.\r\n\r\nSide note: Why is a `Dataset` implemented in a way so it does not know its size? The information is often (always?) available. See e.g. https://github.com/tensorflow/datasets/issues/1403", "I met up with the same problem. But in a little different scenario. I used the for loop as \r\n``` \r\nfor epoch in range(FLAGS.epoch):\r\n        for train_data in train_dataset:\r\n           ...\r\n```\r\nI log on every train batch. But the train stops before the last step of the first epoch. I guess that dataset can't fetch from the second epoch to fill the shuffle queue.", "@372046933 That is a different problem. The issue I describe is with the default training loop. Can you open another issue with a full example?\r\n>  But the train stops before the last step of the first epoch.\r\n\r\nIn case you are using MultiWorkerMirroredStrategy: The docs explain that this can NOT handle the trailing batch and you need to use `repeat` or explicit step/batch counts.", "@Flamefire  I was using custom training loop, i.e. there is no model.fit. Also, I just checked the failing point, it's far from the last batch. Anyway, it seems a different problem", "> I met up with the same problem. But in a little different scenario. I used the for loop as\r\n> \r\n> ```\r\n> for epoch in range(FLAGS.epoch):\r\n>         for train_data in train_dataset:\r\n>            ...\r\n> ```\r\n> \r\n> I log on every train batch. But the train stops before the last step of the first epoch. I guess that dataset can't fetch from the second epoch to fill the shuffle queue.\r\n\r\nI logged `batch_size` on every iteration, and found that the stopped position have smaller batch_size than `FLAGS.batch_size` @Flamefire , Do you think this is a duplicate issue", "Your issue is different as I wrote in https://github.com/tensorflow/tensorflow/issues/36153#issuecomment-590736823 because you use a custom training loop. Please open another issue after reading that comment again and verifying that you are NOT using your loop inside MultiWorkerMirroredStrategy scope which is documented not to work if the last batch is smaller than the others. In case of doubt open a new issue with ALL information instead of adding half-information to an existing issue as this makes it MUCH easier to reason about your problem.", "@Flamefire I have just opened an issue https://github.com/tensorflow/tensorflow/issues/37112 . Thanks for your help.", "This is a known issue that we are working on - MultiWorkerMirroredStrategy doesn't handle last partial batch correctly. To remedy the error, please pass steps_per_epoch argument (and use repeat in your dataset accordingly for multiple epochs). We've updated the tutorial with these details. \r\n\r\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy\r\n\r\nWe expect to have this fixed by TF 2.3", "Updating this thread as the `steps_per_epoch` requirement with MWMS has been lifted and the tutorial has been updated again.\r\nFor more info on distributed input, see this [guide.](https://www.tensorflow.org/tutorials/distribute/input#distributed_datasets)\r\n\r\nI'll close the issue now, but feel free to reopen if something is still unanswered. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36153\">No</a>\n"]}, {"number": 36152, "title": "Error while converting to tflite", "body": "Hi I am using Tensorflow 2.0 installed through anaconda\r\nI was trying to convert an RNN model to TFlite and I got an error.\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-9-d0e813d03e06> in <module>\r\n      1 converter=tf.lite.TFLiteConverter.from_keras_model(model)\r\n      2 converter.experimental_new_converter = True\r\n----> 3 tflite=converter.convert()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py in convert(self)\r\n    444         input_tensors=input_tensors,\r\n    445         output_tensors=output_tensors,\r\n--> 446         **converter_kwargs)\r\n    447 \r\n    448     if self._is_calibration_quantize():\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    447       input_data.SerializeToString(),\r\n    448       debug_info_str=debug_info_str,\r\n--> 449       enable_mlir_converter=enable_mlir_converter)\r\n    450   return data\r\n    451 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198       stdout = _try_convert_to_unicode(stdout)\r\n    199       stderr = _try_convert_to_unicode(stderr)\r\n--> 200       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    201   finally:\r\n    202     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2020-01-23 15:09:32.712287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2020-01-23 15:10:13.660078: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-01-23 15:10:13.667155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-23 15:10:14.122950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce 940M major: 5 minor: 0 memoryClockRate(GHz): 1.176\r\npciBusID: 0000:01:00.0\r\n2020-01-23 15:10:14.125064: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2020-01-23 15:10:14.131581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-23 15:10:17.047944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-23 15:10:17.049761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n2020-01-23 15:10:17.050571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n2020-01-23 15:10:17.052847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2020-01-23 15:10:17.115704: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListFromTensor\r\n2020-01-23 15:10:17.116879: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-23 15:10:17.118307: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListReserve\r\n2020-01-23 15:10:17.119396: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-23 15:10:17.120452: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: While\r\n2020-01-23 15:10:17.121448: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-23 15:10:17.122442: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 21\r\n2020-01-23 15:10:17.123502: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorListStack\r\n2020-01-23 15:10:17.147899: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 18 operators, 54 arrays (0 quantized)\r\n2020-01-23 15:10:17.149816: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 18 operators, 54 arrays (0 quantized)\r\n2020-01-23 15:10:17.157346: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 6 operators, 34 arrays (0 quantized)\r\n2020-01-23 15:10:17.158946: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 6 operators, 34 arrays (0 quantized)\r\n2020-01-23 15:10:17.161212: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 6 operators, 34 arrays (0 quantized)\r\n2020-01-23 15:10:17.172833: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.\r\n2020-01-23 15:10:17.214437: E tensorflow/lite/toco/toco_tooling.cc:466] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 89, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 52, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, STRIDED_SLICE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.\r\n\r\n", "comments": ["@Sapnil98 m\r\nPlease try including the code before tflite=converter.convert() \r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\nFor more information refer #35590 \r\nIf the issue is still not resolved please provide us the complete code snippet used.Thanks!\r\n\r\n", "Based on comment in issue #36160. Looks like issue resolved.\r\nClosing issue"]}, {"number": 36151, "title": "Error in pywrap_tensorflow.py after installing tensorflow", "body": "Hi!\r\nI have this error after installing Tensor Flow. I downgraded my python version to python 3.6.9 and it still doesn't work. \r\n\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Anaconda3_x64\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3_x64\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\nI also tried to uninstall it and after installing  tf-nightly and it gives the same error when importing tensorflow.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n> \r\n> Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n> \r\n> We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n\r\n\r\nI am using Windows 10 and I have installed TensorFlow ( Version: 2.2.0.dev20200121) using the command 'pip install tf-nightly' in the Anaconda Prompt . The error appears when I import tensor flow:\r\n\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\imp.py\", line 243, in load_module\r\nreturn load_dynamic(name, filename, file)\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\n\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\nFile \"\", line 1, in\r\nimport tensorflow\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_init_.py\", line 101, in\r\nfrom tensorflow_core import *\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core_init_.py\", line 40, in\r\nfrom tensorflow.python.tools import module_util as _module_util\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_init_.py\", line 50, in getattr\r\nmodule = self._load()\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_init_.py\", line 44, in _load\r\nmodule = _importlib.import_module(self.name)\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\importlib_init_.py\", line 126, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python_init_.py\", line 50, in\r\nfrom tensorflow.python import pywrap_tensorflow\r\n\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in\r\nraise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in\r\nfrom tensorflow.python.pywrap_tensorflow_internal import *\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in\r\n_pywrap_tensorflow_internal = swig_import_helper()\r\nFile \"C:\\Anaconda3_x64\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nFile \"C:\\Anaconda3_x64\\lib\\imp.py\", line 243, in load_module\r\nreturn load_dynamic(name, filename, file)\r\nFile \"C:\\Anaconda3_x64\\lib\\imp.py\", line 343, in load_dynamic\r\nreturn _load(spec)\r\nImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nError in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x000002C0F6ABC0F0>> (for post_execute):\r\n", "What is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements).Make sure to download the [latest microsoft visual c++ redistributable from here.](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).Make sure you update environment path for cuda.Make sure if there is a library that is in a different location/not installed on your system that cannot be loaded.Also, please follow the instructions from [Tensorflow website]( https://www.tensorflow.org/install/gpu#windows_setup).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36151\">No</a>\n", "---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     63   try:\r\n---> 64     from tensorflow.python._pywrap_tensorflow_internal import *\r\n     65   # This try catch logic is because there is no bazel equivalent for py_extension.\r\n\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-4-5e619db9b532> in <module>\r\n      2 import seaborn as sns\r\n      3 \r\n----> 4 from tensorflow.keras.preprocessing.text import Tokenizer\r\n      5 from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n      6 from sklearn.preprocessing import StandardScaler\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\r\n     39 \r\n---> 40 from tensorflow.python.eager import context\r\n     41 \r\n     42 # pylint: enable=wildcard-import\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py in <module>\r\n     33 from tensorflow.core.protobuf import config_pb2\r\n     34 from tensorflow.core.protobuf import rewriter_config_pb2\r\n---> 35 from tensorflow.python import pywrap_tfe\r\n     36 from tensorflow.python import tf2\r\n     37 from tensorflow.python.client import pywrap_tf_session\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py in <module>\r\n     26 \r\n     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\r\n---> 28 from tensorflow.python import pywrap_tensorflow\r\n     29 from tensorflow.python._pywrap_tfe import *\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     81 for some common reasons and solutions.  Include the entire stack trace\r\n     82 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 83   raise ImportError(msg)\r\n     84 \r\n     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Gaganu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "getting these errors while importing tensorflow..using python 3-8-3 and Tensorflow==2.2", "same error as above I am getting", "any suggestions or solution will greatly help, thank you in advance"]}, {"number": 36150, "title": "Cause: No module named 'tensorflow_core.estimator'", "body": "WARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (/home/local/ZOHOCORP/dharani-pt3410/.local/lib/python3.7/site-packages/tensorflow_core/python/feature_column/__init__.py)\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (/home/local/ZOHOCORP/dharani-pt3410/.local/lib/python3.7/site-packages/tensorflow_core/python/feature_column/__init__.py)\r\nWARNING: Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: cannot import name 'dense_features' from 'tensorflow.python.feature_column' (/home/local/ZOHOCORP/dharani-pt3410/.local/lib/python3.7/site-packages/tensorflow_core/python/feature_column/__init__.py)\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nWARNING: Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nWARNING:tensorflow:Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\nWARNING: Entity <bound method TopLevelFeature.decode_example of Translation({\r\n    'en': Text(shape=(), dtype=tf.string),\r\n    'pt': Text(shape=(), dtype=tf.string),\r\n})> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I'm using Ubuntu 18.04 64bit OS, tensorflow version is 1.15.0-dev20190730 and the place where I got the error is \r\n\r\nexamples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\r\n                               as_supervised=True)\r\ntrain_examples, val_examples = examples['train'], examples['validation']\r\n\r\ntokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\r\n    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\r\n\r\n\r\n@ravikyram ", "@dk7998 \r\n\r\nWill it be possible to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36149, "title": "Fix invalid XLA_FLAGS documentation", "body": "Setting `XLA_FLAGS=\"--dump_hlo_as_text\"` is invalid and causes a catastrophic error; `--xla_dump_hlo_as_text` is the correct flag.", "comments": ["Let see", "Thanks!", "Going even further, `dump_hlo_as_text` is not even required, since that's the default. For some reason, the system is having difficulties importing this commit. I've committed the change internally, referencing your name and this pull request.", "> Going even further, dump_hlo_as_text is not even required, since that's the default\r\n\r\nEven better!\r\n\r\nThanks, @cheshire."]}, {"number": 36148, "title": "Compatibility issue with cudnn 7.0.5 and cuda 9.0", "body": "My source code required tensorflow 1.7.0 with cuda 9.0 and cudnn 7.0.5 version. However when i install cudnn 7.0.5 then my cuda 9.0 version downgraded to 8.0 that arise error during training. While when i install cuda 9.0 and cudnn 7.1.2 then i got following error:\r\n\r\n**E tensorflow/stream_executor/cuda/cuda_dnn.cc:396] Loaded runtime CuDNN library: 7102 (compatibility version 7100) but source was compiled with 7005 (compatibility version 7000).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.**", "comments": ["Please provide details about what platform you are using (operating system, architecture).\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@Vishal2188 \r\n\r\nCan you please try with cudnn 7.0.Can you please see tested build configuration from [here](https://www.tensorflow.org/install/source#gpu).Thanks!", "@Vishal2188 \r\n\r\nAny update on this issue please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36148\">No</a>\n"]}, {"number": 36147, "title": "What factors influence training time through distributed strategy?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version (use command below):\r\nTF2.1.0\r\n- Python version:\r\nPython3.5\r\n- CUDA/cuDNN version:\r\ncuda10.1\r\n- GPU model and memory:\r\nv100 \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI tried to get the training time / epoch under [1, 2, 4, 8] GPUs with tf.distribute.MirroredStrategy. However, I got a very interesting result: the time cost is not only linear to the size fo dataset or GPU num, but also influenced by the training loop style, custom training loop or Keras API.\r\nHere is the result table I make. It's not a  serious experiment but hopefully, we could get some inspiration.\r\n![image](https://user-images.githubusercontent.com/33815430/72955745-25862500-3dd8-11ea-9e0f-ea8b00ab28db.png)\r\n\r\n\r\nSome key points should know beforehand\r\n1. I accumulate and average last 4 epochs out of total 5 epochs, since the first epoch always takes longer than the rest of epochs. \r\n2. loading styles are two: tfds.load() and the TFrecords loading recommended by tensorflow.\r\n3. For TFrecords loading style, I first converted original images into TFrecords files, and then using TFRecords loading.\r\n4. I promise shards number of TFrecords, global_total_batch_size, loading style don't affect training time greatly.\r\n5. I downloaded the original datasets of tf_flowers to my disk. The meaning of _n_ in tf_flowers * n are the times I copied and pasted.  The only purpose is to enlarge the size of the dataset.\r\n6. Please look at the rows of tf_fowers*n. With the size of the tf_flowers doubles, training time doubles with all GPU numbers. It makes sense and is same with our expectation.\r\n7. However, changing the dataset from tf_flowers to 10k dogs(my datasets including 10k dogs of 10 classes), but keep an equivalent size. The time doesn't change as expects. \r\ntf_flowers*8 and 10k dogs are in similar size but tf_flowers*8 takes 3 times longer than 10k dogs\r\ntf_flowers*20 and 10k dogs*2 are in similar size but tf_flowers*20 takes 3 times longer than 10k dogs*2\r\n8. Keras API performs strangely as well. GPU number doesn't affect the training time linearly\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nKeras API\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport time\r\nimport datetime\r\n\r\nimport argparse\r\nimport sys\r\nimport shutil\r\nimport time\r\nimport os\r\nimport numpy as np\r\n\r\nfrom functools import partial\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.datasets.fashion_mnist import load_data\r\nimport tensorflow_datasets as tfds\r\nimport read_params\r\nfrom train_config import configure_model, configure_optimizer, configure_lossfunc\r\nfrom datasets.readtf_utils.dataset import get_dataset \r\nfrom datasets.readtf_utils.dataset import _parse_fn\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n\r\nnum_epochs = 5\r\nbatch_size_per_replica = 128\r\nlearning_rate = 0.001\r\nsetting_GPUs_num = 8\r\ndevices = [\"/gpu:\"+str(i) for i in range(setting_GPUs_num)]\r\n\r\nstrategy = tf.distribute.MirroredStrategy(devices)\r\nGPUs_num = strategy.num_replicas_in_sync\r\nprint('Number of devices: %d' % GPUs_num)  # \u8f93\u51fa\u8bbe\u5907\u6570\u91cf\r\n\r\nbatch_size = batch_size_per_replica\r\n\r\n# \u8f7d\u5165\u6570\u636e\u96c6\u5e76\u9884\u5904\u7406\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\n\r\n\r\nclass MyCustomCallback(tf.keras.callbacks.Callback):\r\n    def on_train_begin(self, logs={}):\r\n        self.train_begin = time.time()\r\n        self.times = []\r\n        # print('trian begins at {}'.format(self.train_begin))\r\n        # d\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        self.epoch_begin = time.time()\r\n        # print('epoch: {} begins at {}'.format(epoch, self.epoch_begin))\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        self.epoch_end = time.time()\r\n        self.epoch = epoch\r\n        # print('epoch: {} ends at {}'.format(epoch, self.epoch_end))\r\n        self.times.append(self.epoch_end-self.epoch_begin)\r\n        print(\" epoch: {} takes: {}\".format(epoch, self.epoch_end-self.epoch_begin))\r\n\r\n    def on_train_end(self, epoch, logs=None):\r\n        self.train_end = time.time()\r\n        # print('training takes {} secs/epoch: '.format((self.train_end - self.train_begin)/self.epoch))\r\n        print('training takes average {:.2f} secs/epoch'.format(sum(self.times[1::]) / (self.epoch)))\r\n\r\n\r\n\r\ntfrecords_dir = \"/data121/lijiayuan/test/classify_flowers/datasets/\"\r\ndataset, _ = get_dataset(tfrecords_dir, subset=\"train\", batch_size=batch_size)\r\n\r\n\r\nif GPUs_num == 1:\r\n    model = tf.keras.applications.MobileNetV2()\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n    )\r\nelse:\r\n    with strategy.scope():\r\n        model = tf.keras.applications.MobileNetV2()\r\n        model.compile(\r\n            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\r\n            loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n            metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n            # run_eagerly=False\r\n        )\r\n\r\n\r\nstart = time.time()\r\nmodel.fit(dataset, epochs=num_epochs, callbacks=[MyCustomCallback()])\r\nend = time.time()\r\n\r\nprint(\"{} GPUs takes {:.2f} secs/epoch = {:.2f} mins/epoch\".format(strategy.num_replicas_in_sync, \r\n                                                                    (end-start)/num_epochs, \r\n                                                                    (end-start)/60/num_epochs))\r\n\r\n```\r\n\r\nCustom training loop\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n# Import TensorFlow\r\nimport tensorflow as tf\r\n\r\n# Helper libraries\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport tensorflow_datasets as tfds\r\nfrom datasets.readtf_utils.dataset import get_dataset \r\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D\r\n\r\n\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n  try:\r\n    # Currently, memory growth needs to be the same across GPUs\r\n    for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n  except RuntimeError as e:\r\n    # Memory growth must be set before GPUs have been initialized\r\n    print(e)\r\n\r\n\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [224, 224]) / 255.0\r\n    return image, label\r\n\r\ndef assemble_model(num_classes, model_name='MobileNetV2'):\r\n    import tensorflow as tf \r\n    base_model = tf.keras.applications.ResNet50(input_shape=(224,224,3),\r\n                                                    weights='imagenet',\r\n                                                    include_top=False)\r\n    model = tf.keras.Sequential([\r\n                                base_model,\r\n                                GlobalAveragePooling2D(),\r\n                                Dense(num_classes, activation='softmax')\r\n                                ])\r\n    model.trainable = True\r\n    return model\r\n\r\n\r\nprint(tf.__version__)\r\n\r\nsetting_GPUs_num = 8\r\ndevices = [\"/gpu:\"+str(i) for i in range(setting_GPUs_num)]\r\n\r\nstrategy = tf.distribute.MirroredStrategy(devices)\r\nprint ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBATCH_SIZE_PER_REPLICA = 256\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\nEPOCHS = 5\r\n\r\n\r\n##-----dataset------##\r\ntfrecords_dir = \"/data121/lijiayuan/test/classify_flowers/datasets\"\r\ntrain_ds, classes_num = get_dataset(tfrecords_dir, subset=\"train\", batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n\r\ntrain_ds = strategy.experimental_distribute_dataset(train_ds)\r\n\r\n\r\n\r\n\r\n\r\n\r\nwith strategy.scope():\r\n  # Set reduction to `none` so we can do the reduction afterwards and divide by\r\n  # global batch size.\r\n  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n      reduction=tf.keras.losses.Reduction.NONE)\r\n  # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\r\n  def compute_loss(labels, predictions):\r\n    per_example_loss = loss_object(labels, predictions)\r\n    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\r\n\r\n\r\nwith strategy.scope():\r\n\r\n  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\r\n      name='train_accuracy')\r\n\r\n# model and optimizer must be created under `strategy.scope`.\r\nwith strategy.scope():\r\n  model = assemble_model(num_classes=classes_num)\r\n  optimizer = tf.keras.optimizers.Adam()\r\n  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\r\n\r\nwith strategy.scope():\r\n  def train_step(inputs):\r\n    images, labels = inputs\r\n\r\n    with tf.GradientTape() as tape:\r\n      predictions = model(images, training=True)\r\n      loss = compute_loss(labels, predictions)\r\n\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n    train_accuracy.update_state(labels, predictions)\r\n    return loss \r\n\r\n\r\n\r\n\r\nwith strategy.scope():\r\n\r\n  @tf.function\r\n  def distributed_train_step(dataset_inputs):\r\n    per_replica_losses = strategy.experimental_run_v2(train_step,\r\n                                                      args=(dataset_inputs,))\r\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                           axis=None)\r\n \r\n \r\n  times = []\r\n  for epoch in range(EPOCHS):\r\n    # TRAIN LOOP\r\n\r\n    total_loss = 0.0\r\n    num_batches = 0\r\n    epoch_start = time.time()\r\n    for x in train_ds:\r\n      total_loss += distributed_train_step(x)\r\n      num_batches += 1\r\n    train_loss = total_loss / num_batches\r\n    epoch_end = time.time()\r\n    \r\n    if epoch != 0:\r\n      times.append(epoch_end-epoch_start)\r\n    \r\n\r\n\r\n\r\n    template = (\"Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, \"\r\n                \" Takes: {:.2f}\")\r\n    print (template.format(epoch+1, train_loss,\r\n                           train_accuracy.result()*100, \r\n                           epoch_end-epoch_start))\r\n\r\n    train_accuracy.reset_states()\r\n  print(\"{} GPUs takes average {:.2f} secs\".format(setting_GPUs_num, \r\n                                                    sum(times)/(EPOCHS-1)))\r\n\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["hi, thank you for the analysis you've done. a couple of quick observations/questions:\r\n\r\n1. batch size. in the keras example, the batch size seems \"per replica\" batch size, but in the custom training loop example, the batch size is \"global\", i.e. \"per replica batch size * num replicas\". Is that accurate? If so, the scaling you expect in those cases will be different. You should use \"per replica batch size * num replicas\" in both cases. I think this might address your point #8? \r\n\r\n2. Re point #7: Regarding your comparison of flowers vs dogs dataset, setting aside distribution, a number of factors can affect the performance observed in those 2 cases (for e.g. the input processing can take different amounts of time based on the exact input). When you say their sizes are \"same\", do you mean they have same number of images, or do they have the same total bytes? Why are you trying to compare the time taken for different datasets? ", "Hi, sorry for my late reply.\r\n1. I indeed did the experiments on batch_size on total or per replica, but it doesn't show a great difference so I didn't mention it\r\n2. The same is the same input size. The most weird thing is that Keras performs best on 2 GPUs while equvalent to 8 GPUs with the custom training loop:\r\n![image](https://user-images.githubusercontent.com/33815430/73808710-d8f50d80-480b-11ea-92eb-bbd37198ffc7.png)\r\nplease take a look at the results marked by green", "> hi, thank you for the analysis you've done. a couple of quick observations/questions:\r\n> \r\n> 1. batch size. in the keras example, the batch size seems \"per replica\" batch size, but in the custom training loop example, the batch size is \"global\", i.e. \"per replica batch size * num replicas\". Is that accurate? If so, the scaling you expect in those cases will be different. You should use \"per replica batch size * num replicas\" in both cases. I think this might address your point #8?\r\n> 2. Re point #7: Regarding your comparison of flowers vs dogs dataset, setting aside distribution, a number of factors can affect the performance observed in those 2 cases (for e.g. the input processing can take different amounts of time based on the exact input). When you say their sizes are \"same\", do you mean they have same number of images, or do they have the same total bytes? Why are you trying to compare the time taken for different datasets?\r\nPlease take a look at the above reply", "Hi @ProNoobLi, we recently published a TensorFlow [performance guide](https://www.tensorflow.org/guide/gpu_performance_analysis) that details different factors that can influence training time for single and multi gpu use cases. You might want to follow the steps in the guide and use the TF Profiler to get a better insight into your progam.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 36146, "title": "tf.function and tf.nest break for valid Mapping instances", "body": "`tf.function` makes invalid assumptions about arguments that are `Mapping` instances. In general, there are no requirements for `Mapping` instances to have constructors that accept `[(key, value)] ` initializers, [as assumed here](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/util/nest.py#L145).\r\n\r\nThis leads to cryptic exceptions when used with perfectly valid `Mapping` subclasses such as this one:\r\n```python\r\nclass CustomMapping(Mapping):\r\n\r\n  def __init__(self, **kwargs):\r\n    self.mapping = kwargs\r\n  \r\n  def __getitem__(self, key):\r\n    return self.mapping[key]\r\n  \r\n  def __iter__(self):\r\n    return iter(self.mapping)\r\n\r\n  def __len__(self):\r\n    return len(self.mapping)\r\n```\r\n\r\nSee this [Colab notebook for an example](https://colab.research.google.com/drive/1sIbvyPVtQexCWqVO7QD3KlY7PoLdPeKq).\r\n", "comments": ["Issue replicating in [tf-nightly.](https://colab.research.google.com/gist/Saduf2019/52d668ef566842781e293594fabf27ef/36146.ipynb) Thanks.", "It looks like `tf.nest` is overly permissive with `Mapping` subclasses, because the contract of `Mapping` does not seem to specify an interface for constructors. Users may accept key-value pairs, **kwargs or just about anything else and they'd still have a valid Mapping. Moreover, detecting how to properly use that constructor is extremely difficult in Python.\r\n\r\nThe correct solution is either to only support `dict` and `OrderedDict` (treating everything else as opaque objects), or to explicitly require a certain signature for constructor (right now, that is: \"it must accept key-value pairs\").\r\n\r\nAt any rate, this limitation should be documented and a best-effort to detect errors could be made by catching anything exception that the constructor raises and adding a hint about this.\r\n\r\nFor future readers, the workaround is to provide a constructor that accepts key-value pairs, similar to [dict](https://docs.python.org/3/library/stdtypes.html#dict)'s.", "Hey! I would like to work on this. Could you guide me? ", "Sure, here are a few pointers -\r\n\r\nFirst, the documentation of all `@tf_export` methods inside [tf.nest](https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/util/nest.py) needs to be brought to date and clarify what is a \"structure\". Some entries mention that you can have dicts, others do not. I think the module documentation should clarify that. The docstrings in there are used to generate the [public docs](https://www.tensorflow.org/api_docs/python/tf/nest).\r\n\r\nSecond, [IsMappingHelper](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/util.cc#L216) that verifies for supported types should check for `MutableMapping` rather than just `Mapping`. It should probably be renamed to `IsNestCompatibleMapping` on this occasion. The reason for that is because we need to know how to construct new mappings of the same type and the Mapping interface doesn't standardize that. See [the Python reference](https://docs.python.org/3/library/collections.abc.html) for more details.\r\n\r\nThird, the error messages need to be improved. [the code of _sequence_like](https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/util/nest.py#L145) that attempts to reconstruct `dict` objects should catch exceptions and raise a more informative one (something like \"`could not reconstruct object of type X`\"). Also, [the code of func_graph_from_py_func](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/func_graph.py#L897) should again catch exceptions and add more information (something like \"`could not handlearguments of <function name>: <message of caught error>`\").\r\n\r\nI hope this helps! As a general guidance, I recommend sending multiple small PRs instead of one big change, to make them easier to review.", "Hey! Thanks for the input. \r\n\r\n> First, the documentation of all @tf_export methods inside tf.nest needs to be brought to date and clarify what is a \"structure\". Some entries mention that you can have dicts, others do not. I think the module documentation should clarify that. The docstrings in there are used to generate the public docs.\r\n\r\nRegarding this if am not wrong a couple of the function's documentation need to updated for which I should create a separate PR is what you are suggesting and to do the same for the remaining two broken down problems?", "I'd start with everything in one PR (docstring changes and small code changes), and if you end up changing lots of code, break it down into more pieces.", "@mdanatg I've create a PR #36186 addressing this. Could you please review?", "I would like to test tensorflow locally from the github repo, how do I set that up for python?", "@namedtoaster see the guide: https://www.tensorflow.org/install/source; you may want to skip to the [Docker section](https://www.tensorflow.org/install/source#docker_linux_builds) for a ready-made setup.", "@mdanatg I downloaded tensorflow from the most current repo and built using the instructions you linked to. When I try to run the code from the notebook (or really any simple tensorflow command), I get the following error.\r\n\r\n`Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA`\r\n\r\nIs there a way to suppress those features from my CPU so I can run tf? Or do I have to rebuild a new binary that disables those features?\r\n\r\nEdit: I think I found some possible answers. I'll come back when I've exhausted my attempts", "@namedtoaster You can safely ignore that warning, but otherwise you want a binary that is built with those features enabled. Try to see if you can rebuild it with these bazel options: `--copt=-mavx2 --copt=-msse4 --copt=-mfma`", "@mdanatg thanks for the info. I stand corrected on my last post, I am able to do a simple tensorflow command with the original binary installed. When I tried to re-install with the optimizations I got some compiler errors that I'll try to fix at a later time. I did what you said but I get this output:\r\n\r\n`clang: error: unsupported option '--config=opt'`\r\n\r\nMy bigger confusion right now is that when I try to run the code from the notebook I get an error about using the wrong number of arguments:\r\n\r\n`2020-02-12 15:50:38.623392: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/Users/----/PycharmProjects/tensorflow_dev/test.py\", line 27, in <module>\r\n    test_tf_function(CustomMapping(theta=3.1415))\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 623, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 505, in _initialize\r\n    *args, **kwds))\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2440, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2771, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/eager/function.py\", line 2661, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 898, in func_graph_from_py_func\r\n    args, arg_names, flat_shapes=arg_shapes)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 1132, in _get_defun_inputs_from_args\r\n    args, names, structure=args, flat_shapes=flat_shapes)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 1188, in _get_defun_inputs\r\n    arg_value = nest.map_structure(_get_composite_tensor_spec, arg_value)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/util/nest.py\", line 600, in map_structure\r\n    expand_composites=expand_composites)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/util/nest.py\", line 535, in pack_sequence_as\r\n    return _pack_sequence_as(structure, flat_sequence, expand_composites)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/util/nest.py\", line 499, in _pack_sequence_as\r\n    return sequence_fn(structure, packed)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow_core/python/util/nest.py\", line 145, in _sequence_like\r\n    return instance_type((key, result[key]) for key in instance)\r\nTypeError: __init__() takes exactly 1 argument (2 given)`\r\n\r\nI'm looking into the error right now. I may have an incorrect understanding of kwargs", "I think the error you see is expected - it's the bug described in the OP.", "For the instruction set warning, try specifying `-c opt` as well. I came across this page where the author seemed to successfully use these options: https://gist.github.com/Brainiarc7/6d6c3f23ea057775b72c52817759b25c", "@mdanatg that makes sense. I should've noticed that -- I will check out that link and give it a try. Thanks", "Hello I thought this issue was solved but it's been open for quite some time.\r\nI would like to contribute if contributions are still being accepted.", "Hey @Joey155 ! PR #36186 has been opened regarding the same.", "@ethereon I guess the issue can be closed since the PR #36186 has been merged.", "I believe the described issue still exists. With PR #36186 , it produces a warning (with a typo) before failing with the same exception as before\r\n\r\n```\r\nWARNING:tensorflow:Mapping types may not work well with tf.nest. Preferusing MutableMapping for <class '__main__.CustomMapping'>\r\n...\r\nTypeError: __init__() takes 1 positional argument but 2 were given\r\n```\r\n\r\nIndeed, even as a workaround, it's not immediately clear that the recommendation should be to adopt `MutableMapping`, which seems like a stronger requirement than just accepting `(key, value)` pairs in the constructor...", "Oops. My bad didn't notice that. ", "@punndcoder28 will you update your fix as per @ethereon's feedback or shall I? ", "@ethereon I'm not clear on what the desired behavior would be that you would consider a \"fix\"? Is it a more clear error message on the exception that says something like \"Map must have a (key, value) constructor\" instead of \"TypeError: __init__() takes 1 positional argument but 2 were given\"?\r\n\r\nIt seems the example code in the notebook should indeed fail because the custom map doesn't meet the tf.nest's requirement of having a (key, value) constructor. We just need a better error message. Is that correct? Or does tf.nest need to change to accommodate that type of map? ", "I will get on it today and try to fix. If I am unable to do so, you could take over. Also, I had the same doubt as @tonytonev. ", "The `**kwargs` mechanics only work with maps that have string keys. Since `tf.nest` cannot make any assumption about the type of the keys, I think it requires a constructor with on positional argument. So, improving the error message seems like the right path forward.", "@mdanatg How can I improve the error message? Any suggestions would be helpful.", "@punndcoder28 How about \"Mapping must have a (key, value) constructor\". This should be an actual exception rather than just a warning, since it is a fatal error that will throw \"TypeError: init() takes 1 positional argument but 2 were given\" anyway. @mdanatg do you agree?", "The error message SGTM, but the warning message is separate. I do want to discourage using `Mapping` subclasses in favor of `MutableMapping`, for the reason that follows. It's unfortunate that `Mapping` subtypes cannot be robustly supported.\r\n\r\nTo actually raise an error, more checks need to be added: first you need to find the constructor of the type, then you need to check its argspec and count the positional arguments. Even so, we can only check that the constructor takes a positional arg and will have no idea whether that constructor expects it to be a key-value list (it might expect it to be a dict instead). So the error will not be 100% robust.\r\n\r\nAlternatively, you could catch exceptions raised while calling the constructor and re-raise them with a helper message, something like: \"Error rebuilding custom mapping. Note that it must accept a single positional argument representing an iterable of key-value pairs, in addition to self. Cause: <original error>\".", "@punndcoder28 Do you mind if I submit this as my first pull request? You already had your changes accepted \ud83d\ude04 .", "Sure. Go ahead @tonytonev.\n\nSent from my iPhone\n\n> On 03-Apr-2020, at 10:16 PM, Tony Tonev <notifications@github.com> wrote:\n> \n> \ufeff\n> @punndcoder28 Do you mind if I submit this as my first pull request? You already had your changes accepted \ud83d\ude04 .\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "Thanks, I'm working on getting it to build locally so I can run tests, and I'll submit a PR soon. ", "@mdanatg I sent a pull request https://github.com/tensorflow/tensorflow/pull/38283#pullrequestreview-388593812", "The fix has been merged. I think this can be closed. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36146\">No</a>\n"]}, {"number": 36145, "title": "Fixed line formatting", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F36145) for more info**.\n\n<!-- need_sender_cla -->", "@settle thank you for your contribution, please sign CLA.", "@settle , If you are signing CLA [(here)](https://cla.developers.google.com/) then please sign CLA for all the email IDs which you are already logged in in your browser. I faced the same problem once but I got solution after posting the question to stackoverflow [here](https://stackoverflow.com/questions/57214178/how-should-i-troubleshoot-my-google-contributor-licence-agreement-issue-on-my-pu/57228116#57228116).", "@settle without signing CLA we cannot merge in your change", "> @settle without signing CLA we cannot merge in your change\r\n\r\nSorry, but I can't sign the CLA at this time.  I guess someone else will have to fix this missing newline character in the docs.", "In that case, closing the PR. Sorry for this.", "@settle, Your approval is necessary on #36479."]}, {"number": 36144, "title": "makefile fails to execute a.out file", "body": "I am new to TensorFlow. At the moment, I am learning TensorFlow for C code on my Macbook. I follow the instructions by Google and manage to run the 'hello world' in C.\r\n\r\nI set the paths on my bash_profile as follows:\r\n\r\nexport LIBRARY_PATH=$LIBRARY_PATH:/usr/local/tensorflow/lib\r\n\r\nexport DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/usr/local/tensorflow/lib.\r\n\r\nMy makefile is as follow.\r\n\r\ntensorlib = -L/usr/local/tensorflow-1.15.0/lib/\r\n\r\ntensorlnk = -I/usr/local/tensorflow-1.15.0/include/\r\n\r\ntask:\r\n\r\nmpicc -c hello.c $(tensorlnk)\r\n\r\nmpicc    hello.o $(tensorlib)\r\nrun:\r\n\r\nmpirun -np 4 ./a.out\r\nI can compile the program and generate the executable, a.out. In my terminal, I can run the program using 'mpirun -np 4 ./a.out' to execute. But the command 'make run' fails. Here is the error message\r\n\r\ndyld: Library not loaded: @rpath/libtensorflow.1.dylib Referenced from: /Users/usr/Documents/code/dev/TensorFlow/c/./a.out Reason: image not found\r\n\r\nCould anyone tell me what the problem is?\r\n\r\nMany thanks in advance.\r\n", "comments": ["@applied-math-K \r\nJust to verify did you followed the instructions from [Tensorflow site](https://www.tensorflow.org/install/lang_c). Thanks!", "Yes! I followed the instructions from Tensorflow site.", "What is your macOS version?\r\nSee https://www.tensorflow.org/install/lang_c#supported_platforms", "I am running Catalina 10.15.3. Tensorflow runs on my Mac. But I am unable to run it using 'make run'.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 36143, "title": "Memory leak at tflite::Interpreter::Invoke()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): tflite example to inference with mobilenetV2\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): tflite 2.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): 1.1.0\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: NO\r\n- GPU model and memory: NO\r\n\r\n\r\n**Describe the current behavior**\r\nWe are running and tflite example using the tensorflow-lite.a library generated by `./tensorflow/lite/tools/make/build_lib.sh`  this command.\r\nThe example works but it has a memory leak\r\n**Other info / logs**\r\nThis is the log that I get using heapusage, as you can see in bold the problem is with tflite::Interpreter::Invoke() \r\n\r\n`\r\n==1359== 1216 bytes in 4 block(s) are lost, originally allocated at:\r\n==1359==    at 0x00007fd0cce977f2: calloc + 68\r\n==1359==    at 0x00007fd0cd0ba4a7: _dl_allocate_tls + 39\r\n==1359==    at 0x00007fd0cbdc1228: pthread_create + 2168\r\n==1359==    at 0x00007fd0cc64b925: std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) + 21\r\n==1359==    at 0x00007fd0ccb488f5: tflite::eigen_support::GetThreadPoolDevice(TfLiteContext*) + 2005\r\n==1359==    at 0x00007fd0ccb1d9d2: void tflite::ops::builtin::conv::EvalFloat<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*, TfLiteConvParams*, tflite::ops::builtin::conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) + 1250\r\n==1359==    at 0x00007fd0ccb1e515: TfLiteStatus tflite::ops::builtin::conv::Eval<(tflite::ops::builtin::conv::KernelType)2>(TfLiteContext*, TfLiteNode*) + 501\r\n==1359==    at 0x00007fd0cca71ba7: tflite::Subgraph::Invoke() + 775\r\n**==1359==    at 0x00007fd0cc9d7b89: tflite::Interpreter::Invoke() + 25\r\n==1359==    at 0x00007fd0cc9caa44: r2i::tflite::Engine::Predict(std::shared_ptr<r2i::IFrame>, r2i::RuntimeError&) + 1122**\r\n==1359==    at 0x000055f42ee148eb: \r\n==1359==    at 0x00007fd0cb9e9b97: __libc_start_main + 231\r\n==1359==    at 0x000055f42edf2bda: \r\n==1359== \r\n\r\n`\r\n", "comments": ["Could you please provide us with simple standalone code to reproduce the issue in our environment, Thanks.", "Hi @Saduf2019,\r\nActually if you run the label_image example you get this memory leak as follow:\r\n\r\n\r\n==12072== 912 bytes in 3 block(s) are lost, originally allocated at:\r\n**==12072==    at 0x00007f0854dc27f2: calloc + 68\r\n==12072==    at 0x00007f0854fe54a7: _dl_allocate_tls + 39\r\n==12072==    at 0x00007f0854b95228: pthread_create + 2168\r\n==12072==    at 0x00007f08548c1925: std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)()) + 21**\r\n==12072==    at 0x0000559277c8ba65: \r\n==12072==    at 0x0000559277c60b42: \r\n==12072==    at 0x0000559277c61685: \r\n==12072==    at 0x0000559277bb4d17: \r\n==12072==    at 0x0000559277b1a789: \r\n==12072==    at 0x0000559277b1127b: \r\n==12072==    at 0x0000559277b11bc5: \r\n==12072==    at 0x0000559277b11ed8: \r\n==12072==    at 0x00007f0853e7eb97: __libc_start_main + 231\r\n==12072==    at 0x0000559277aef26a: \r\n\r\n\r\nAs you can see in bold this message is the same. You can try with label_image example to reproduce the problem.\r\n\r\nThanks", "Hi Lenin, \r\nCan you provide the reproduce steps ?\r\n\r\nThanks", "Yes sure,\r\nI'm using a tool called [heapusage](https://github.com/d99kris/heapusage) that internally use Valgrind to detects memory leaks.\r\nI follow the instructions to build the label_image example provided by you and get the output above.\r\n\r\nThis is how I execute the example:\r\n\r\n`heapusage ./label_image   --tflite_model mobilenet_v1_1.0_224.tflite   --labels labels.txt   --image testdata/grace_hopper.bmp\r\n`\r\n\r\n", "Hi Lenin,\r\n\r\nI can see valgrind reporting a leak when i run the label_image example, but it is not actually a leak, it's from initializing the absl flags.\r\n\r\nI still can't see the stack in your first post which is from the interpreter.\r\nDo you have reproduce steps ?\r\n\r\nThanks", "No activity. Closing.\r\nPlease reopen if you have more data.\r\nThanks", "@karimnosseir i have same issues, [my code](https://drive.google.com/drive/folders/1gOzL0mBLREZ-QI5Yrbc4uTKszx-ghwTO?usp=sharing), please, check my code and tell me why memory leak/\r\nthank you so much", "@tpppppub \r\nAs this issue is in closed status can you please create a new issue, with the details.", "@Saduf2019 \r\nThanks for your quick reply. I found that's my fault so I deleted my comment."]}, {"number": 36142, "title": "TF2.0 to 2.1 breaking for tf.image.random_jpeg_quality", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\nI upgraded to tensorflow 2.1 and everything in my project is working fine except for one thing. Calling `tf.image.random_jpeg_quality` throws a ValueError: `as_list() is not defined on an unknown TensorShape`. The full error is pasted at the bottom of this issue.\r\n\r\nThe code works after removing the call to random_jpeg_quality. That works for me at this point. Just raising this issue in case it hasn't been noted yet.\r\n\r\n**Other info / logs**\r\n```\r\nValueError: in converted code:\r\n\r\n    /data.py:261 _parse_example_augment  *\r\n        image = tf.image.random_jpeg_quality(image=image, min_jpeg_quality=60, max_jpeg_quality=100)\r\n    /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/ops/image_ops_impl.py:2031 random_jpeg_quality\r\n        return adjust_jpeg_quality(image, jpeg_quality)\r\n    /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/ops/image_ops_impl.py:2064 adjust_jpeg_quality\r\n        channels = image.shape.as_list()[-1]\r\n    /root/miniconda3/lib/python3.5/site-packages/tensorflow_core/python/framework/tensor_shape.py:1166 as_list\r\n        raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\n\r\n    ValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n", "comments": ["@klanderson  Could you provide a piece of code which to reproduce this issue?\r\nThe problem may relates to the input image, since the error message remind the input images is an unknown TensorShape.", "This commit id\r\nhttps://github.com/tensorflow/tensorflow/commit/ef3508020e0af36bbe9f5088222778ef3678bc6f\r\nmakes the changes.\r\n@klanderson I suspect you need to pass the None in the last shape parameter", "Thanks, I just wanted to bring up the issue as it was something that changed between 2.0 and 2.1.", "@klanderson ,\r\nPlease feel free to close the issue if its resolved.Thanks!", "I use the docker image tensorflow/tensorflow:latest-gpu-py3, but the problem have not resolved. Why this issue closed?"]}, {"number": 36141, "title": "HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory", "body": "**System information**\r\n- Linux tutkain 4.19.93-v7l+ #1290 SMP Fri Jan 10 16:45:11 GMT 2020 armv7l GNU/Linux\r\n- Debian 10.2 on Raspberry PI 4+ 4GB\r\n\r\n**Describe the problem**\r\n(venv) tas@tutkain:~ $ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2020-01-22 20:21:52.312444: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\nTensor(\"Sum:0\", shape=(), dtype=float32)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- Installation exactly as described https://www.tensorflow.org/install/pip\r\n\r\n**Any other info / logs**\r\n(venv) tas@tutkain:~ $ pip list\r\nPackage              Version\r\n-------------------- ---------\r\nabsl-py              0.9.0\r\nasn1crypto           0.24.0\r\nastor                0.8.1\r\ncachetools           4.0.0\r\ncertifi              2018.8.24\r\nchardet              3.0.4\r\ncryptography         2.6.1\r\ndistro-info          0.21\r\nentrypoints          0.3\r\ngast                 0.2.2\r\ngoogle-auth          1.10.2\r\ngoogle-auth-oauthlib 0.4.1\r\ngoogle-pasta         0.1.8\r\ngrpcio               1.26.0\r\nh5py                 2.8.0\r\nidna                 2.6\r\nKeras-Applications   1.0.8\r\nKeras-Preprocessing  1.1.0\r\nkeyring              17.1.1\r\nkeyrings.alt         3.1.1\r\nMarkdown             3.1.1\r\nnumpy                1.16.2\r\noauthlib             3.1.0\r\nopt-einsum           3.1.0\r\npip                  20.0.1\r\nprotobuf             3.11.2\r\npyasn1               0.4.8\r\npyasn1-modules       0.2.8\r\npycrypto             2.6.1\r\npycurl               7.43.0.2\r\nPyGObject            3.30.4\r\npython-apt           1.8.4\r\npyxdg                0.25\r\nrequests             2.21.0\r\nrequests-oauthlib    1.3.0\r\nrsa                  4.0\r\nSecretStorage        2.3.1\r\nsetuptools           45.1.0\r\nsix                  1.12.0\r\nssh-import-id        5.7\r\ntensorboard          2.0.2\r\ntensorflow           1.14.0\r\ntensorflow-estimator 1.14.0\r\ntermcolor            1.1.0\r\nunattended-upgrades  0.1\r\nurllib3              1.24.1\r\nvirtualenv           16.7.9\r\nWerkzeug             0.16.0\r\nwheel                0.33.6\r\nwrapt                1.11.2\r\n", "comments": ["Do you have `libhdf.so` in a searchable location? Is it at the right version?", "> Do you have `libhdf.so` in a searchable location? Is it at the right version?\r\n\r\nI do not think so. If that would be the case, it would work as expected, obviously.\r\n\r\nThe point being, installation instructions at https://www.tensorflow.org/install/pip **do not mention anything** about `libhdf.so`. \r\n\r\nProblem with the package, should work out-of-the box without manually installing a missing dependency. Before this is fixed, a **validated** workaround is appreciated. \r\n\r\nThere's a duplicate issue #35462 which was closed without resolution. \r\n\r\n\r\n", "Hadoop file system requires additional setup on your system, which can be found on [Hadoop project's page](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)\r\n\r\nWe cannot document how to install all third party libraries in all operating systems and all environments in the world. The world is not just a \"install this one thing and everything must work now\" type of thing.\r\n\r\nYou only need `libhdf.so` if you use the Hadoop file system and if you want to use that you have to install it properly.\r\n\r\nI am closing the issue but please reopen if you still get an error in TF after installing Hadoop and confirming that Hadoop works outside of TF.", "Reopening, as it seems your error is related to #32885, not to wanting to use the Hadoop filesystem", "> We cannot document how to install all third party libraries in all operating systems and all environments in the world.\r\n> The world is not just a \"install this one thing and everything must work now\" type of thing.\r\n\r\nStraw man. It's **your** application that requires the 3rd party library by default, out-of-the-box. Not **my** code, system nor environment.\r\n\r\nAnd hell yeah. The world is full of crappy second class open source software with incorrect documentation, non-working sample code and tons of open critical issues.\r\nResume the tradition, keep up the good work. \r\n \r\n> You only need `libhdf.so` if you use the Hadoop file system and if you want to use that you have to install it properly.\r\n\r\nFinally we're getting _somewhere_. Not using Hadoop file system. \r\nObviously you didn't read the [installation instructions](https://www.tensorflow.org/install/pip).\r\n\r\n_Verify the install:_\r\n```\r\n(venv) tas@tutkain:~ $ python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n2020-01-22 20:21:52.312444: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] HadoopFileSystem load error: libhdfs.so: cannot open shared object file: No such file or directory\r\nTensor(\"Sum:0\", shape=(), dtype=float32)\r\n```\r\n\r\n", "Can you use TF 1.15 or TF 2.1 (or TF 2.0) instead of TF 1.14?\r\n\r\nSee #32885.", "Same problem here, is there a resolution by any chance? Thanks", "@tuomassiren \r\n\r\nCan you please confirm if @mihaimaruseac's suggestion is working for you.Thanks!", "@ravikyram I am using TF > 2.0 installed with \"pip install --upgrade tensorflow\" but I got the same error. I am trying TF on a RaspPi 4 with Raspbian.", "@MarcoLeti can you post output of `pip freeze`?", "I checked and I don't know why I have version 1.14, could you please advise? (pip version 20.0.2)\r\n\r\n```\r\nabsl-py==0.9.0\r\nastor==0.8.1\r\ncachetools==4.0.0\r\ncertifi==2019.11.28\r\nchardet==3.0.4\r\ngast==0.2.2\r\ngoogle-auth==1.11.0\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.1.8\r\ngrpcio==1.26.0\r\nh5py==2.10.0\r\nidna==2.8\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nMarkdown==3.1.1\r\nnumpy==1.18.1\r\noauthlib==3.1.0\r\nopencv-python==3.4.6.27\r\nopt-einsum==3.1.0\r\npkg-resources==0.0.0\r\nprotobuf==3.11.2\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\nrequests==2.22.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.0\r\nsix==1.14.0\r\ntensorboard==2.0.2\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntermcolor==1.1.0\r\nurllib3==1.25.8\r\nWerkzeug==0.16.0\r\nwrapt==1.11.2\r\n```", "@MarcoLeti please post the output of running `pip uninstall tensorflow tensorflow-estimator && pip install tensorflow==2.1`. Please use ` ``` ` before and after the output so the output is properly formatted.", "```\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.1 (from versions: 0.11.0, 1.12.0, 1.13.1, 1.14.0)\r\nERROR: No matching distribution found for tensorflow==2.1\r\n```", "What is the output of `pip version` and `pip debug verbose`?", "```\r\npip3 --version\r\npip 20.0.2 from /home/pi/Git/computer-vision/comp-vision/lib/python3.7/site-packages/pip (python 3.7)\r\n```\r\n```\r\npip3 debug --verbose\r\nWARNING: This command is only meant for debugging. Do not use this with automation for parsing and getting these details, since the output and options of this command may change without notice.\r\npip version: pip 20.0.2 from /home/pi/Git/computer-vision/comp-vision/lib/python3.7/site-packages/pip (python 3.7)\r\nsys.version: 3.7.3 (default, Apr  3 2019, 05:39:12)\r\n[GCC 8.2.0]\r\nsys.executable: /home/pi/Git/computer-vision/comp-vision/bin/python3\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: iso8859-1\r\nlocale.getpreferredencoding: ISO-8859-1\r\nsys.platform: linux\r\nsys.implementation:\r\n  name: cpython\r\n'cert' config value: global\r\nREQUESTS_CA_BUNDLE: None\r\nCURL_CA_BUNDLE: None\r\npip._vendor.certifi.where(): /home/pi/Git/computer-vision/comp-vision/lib/python3.7/site-packages/pip/_vendor/certifi/cacert.pem\r\nCompatible tags: 44\r\n  cp37-cp37m-manylinux2014_armv7l\r\n  cp37-cp37m-linux_armv7l\r\n  cp37-abi3-manylinux2014_armv7l\r\n  cp37-abi3-linux_armv7l\r\n  cp37-none-manylinux2014_armv7l\r\n  cp37-none-linux_armv7l\r\n  cp36-abi3-manylinux2014_armv7l\r\n  cp36-abi3-linux_armv7l\r\n  cp35-abi3-manylinux2014_armv7l\r\n  cp35-abi3-linux_armv7l\r\n  cp34-abi3-manylinux2014_armv7l\r\n  cp34-abi3-linux_armv7l\r\n  cp33-abi3-manylinux2014_armv7l\r\n  cp33-abi3-linux_armv7l\r\n  cp32-abi3-manylinux2014_armv7l\r\n  cp32-abi3-linux_armv7l\r\n  py37-none-manylinux2014_armv7l\r\n  py37-none-linux_armv7l\r\n  py3-none-manylinux2014_armv7l\r\n  py3-none-linux_armv7l\r\n  py36-none-manylinux2014_armv7l\r\n  py36-none-linux_armv7l\r\n  py35-none-manylinux2014_armv7l\r\n  py35-none-linux_armv7l\r\n  py34-none-manylinux2014_armv7l\r\n  py34-none-linux_armv7l\r\n  py33-none-manylinux2014_armv7l\r\n  py33-none-linux_armv7l\r\n  py32-none-manylinux2014_armv7l\r\n  py32-none-linux_armv7l\r\n  py31-none-manylinux2014_armv7l\r\n  py31-none-linux_armv7l\r\n  py30-none-manylinux2014_armv7l\r\n  py30-none-linux_armv7l\r\n  cp37-none-any\r\n  py37-none-any\r\n  py3-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```\r\n", "Can you upgrade setuptools and try again? It seems you cannot get the manylinux2010 compliant pips (after TF1.14 we switched to producing those)", "Unfortunately still the same error and setuptools seems to be upgrade\r\n```\r\npip3 install --upgrade pip setuptools\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nRequirement already up-to-date: pip in ./comp-vision/lib/python3.7/site-packages (20.0.2)\r\nRequirement already up-to-date: setuptools in ./comp-vision/lib/python3.7/site-packages (45.1.0)\r\n```\r\n", "One more attempt at getting more information: `pip install --no-cache-dir -vvv tensorflow==2.1`. Please attach the full log, no matter how long it is", "Also try adding `--force-reinstall ` flag on the above command too", "```\r\npip3 install --no-cache-dir -vvv tensorflow==2.1\r\n```\r\n[output.txt](https://github.com/tensorflow/tensorflow/files/4130585/output.txt)\r\n", "pip3 install --no-cache-dir -vvv tensorflow==2.1 --force-reinstall\r\n[output2.txt](https://github.com/tensorflow/tensorflow/files/4130590/output2.txt)\r\n", "None of the wheels that we publish on PyPi (https://pypi.org/project/tensorflow/2.1.0/#files) match the tags supported by your system.\r\n\r\nYou were able to install 0.11.0, 1.12.0, 1.13.1 and 1.14.0 only because they are on piwheels: https://www.piwheels.org/project/tensorflow/ But note that not all TF versions are there.\r\n\r\nI think the only options left are to either get piwheels to upload the new pips (it seems they cannot upload pips after we switched to be manylinux2010 compliant) or to build them yourself from source.", "Ok, thank you. So basically TF > 1.4 cannot be installed on RaspberryPi through pip and must be built from source? This is not very clear and I also found a lot of people on the web who install it from pip...\r\nAnyway I found this issue that may be useful: #29704", "That issue is related to the install issue.\r\n\r\nYes, you are right, you have to build it from source, as we are not managing it ourselves. If people are installing from pip, **on RaspberryPi**, then the pip they use must come from a different source (i.e., someone compiled from source and made the pip public)", "> ... you have to build it from source, ... on RaspberryPi,\r\n\r\nA sentence to this requirement would save many people many hours. Please don't get _testy_ about a feedback on documentation.\r\n\r\nKind regards.", "This issue still persists. Any luck with this?", "It seems the issue is with version 1.14, you can install version a different version and I think you won't have this problem. Unfortunately you cannot install from pip versions > 1.14. I can advice this repository where you can find already prepared wheels and you can avoid to create your own: https://github.com/PINTO0309/Tensorflow-bin\r\nI installed version 2.1 and it worked for me!", "@MarcoLeti Thx for the suggestion. I will definitely give it a try. Much appreciated.\r\n\r\nKind regards.", "I guess I'll drop **arm6l** from consideration and focus more on arm7l, armv8 and aarch64 for my deployments. My mistake in not reading the \"_fine print._\"", "I am still getting this error on my raspberry pi 4, even after installing tensorflow 2.1.0 using the instructions in https://github.com/PINTO0309/Tensorflow-bin. I used the instructions for \"Example of Python 3.x + Tensorflow v2 series\" since I am using Python 3.7.3", "I'm still getting the message however, i can do all my TensorFlow related tasks without issues. So i am guessing that your application should work normally even if this message shows up. ", "Thanks so much @SirPhemmiey! You're right!", "@SirPhemmiey Ty yes if you print tf.sin(2.2) for example, it shows you 0 but if you load a simple model it works fine :)", "So there's no way to find the wheels file of tensorflow >1.14 for arm6l? I have a RaspberryPI 2, I have the same problem of asking me libhdfs when I try to import tensor flow 1.14 and I don't know what to do...", "rpi builds are community driven effort at the moment, so unfortunately this means that someone from the community has to build and release the pips.", "@Gabriv93 after it shows the error, what else did it say? I don't know about RPi 2 but it works fine in higher models (RPi 3 and 4) even after showing this error message. ", "@SirPhemmiey actually it's true, after the error message it takes some seconds to actually import the package but then it seems to work normally... I will see if there will be issues with some scripts, thanks", "![image](https://user-images.githubusercontent.com/6893075/75901613-c3f6b300-5e79-11ea-915c-3c83376c036e.png)\r\nI got the same result.\r\nenv:  Raspbian buster \r\n\r\n```\r\n(pp) pi@raspberrypi:~/pp $ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tRaspbian\r\nDescription:\tRaspbian GNU/Linux 10 (buster)\r\nRelease:\t10\r\nCodename:\tbuster\r\n(pp) pi@raspberrypi:~/pp $ pip freeze\r\nabsl-py==0.9.0\r\nastor==0.8.1\r\nattrs==19.3.0\r\nbackcall==0.1.0\r\nbleach==3.1.0\r\ncachetools==4.0.0\r\ncertifi==2019.11.28\r\nchardet==3.0.4\r\ndecorator==4.4.1\r\ndefusedxml==0.6.0\r\ndlib==19.19.0\r\nentrypoints==0.3\r\ngast==0.2.2\r\ngoogle-auth==1.11.2\r\ngoogle-auth-oauthlib==0.4.1\r\ngoogle-pasta==0.1.8\r\ngrpcio==1.27.2\r\nh5py==2.10.0\r\nidna==2.9\r\nimportlib-metadata==1.3.0\r\nipykernel==5.1.3\r\nipython==7.10.2\r\nipython-genutils==0.2.0\r\njedi==0.15.2\r\nJinja2==2.10.3\r\njson5==0.8.5\r\njsonschema==3.2.0\r\njupyter-client==5.3.4\r\njupyter-core==4.6.1\r\njupyterlab==1.2.4\r\njupyterlab-server==1.0.6\r\nKeras-Applications==1.0.8\r\nKeras-Preprocessing==1.1.0\r\nMarkdown==3.2.1\r\nMarkupSafe==1.1.1\r\nmistune==0.8.4\r\nmore-itertools==8.0.2\r\nnbconvert==5.6.1\r\nnbformat==4.4.0\r\nnotebook==6.0.2\r\nnumpy==1.17.4\r\noauthlib==3.1.0\r\nopencv-python==4.1.1.26\r\nopt-einsum==3.1.0\r\npandocfilters==1.4.2\r\nparso==0.5.2\r\npexpect==4.7.0\r\npickleshare==0.7.5\r\npkg-resources==0.0.0\r\nprometheus-client==0.7.1\r\nprompt-toolkit==3.0.2\r\nprotobuf==3.11.3\r\nptyprocess==0.6.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\nPygments==2.5.2\r\npyqtgraph==0.10.0\r\npyrsistent==0.15.6\r\npyserial==3.4\r\npython-dateutil==2.8.1\r\npyzmq==18.1.1\r\nrequests==2.23.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.0\r\nSend2Trash==1.5.0\r\nsix==1.13.0\r\ntensorboard==2.0.2\r\ntensorflow==1.14.0\r\ntensorflow-estimator==1.14.0\r\ntermcolor==1.1.0\r\nterminado==0.8.3\r\ntestpath==0.4.4\r\ntornado==6.0.3\r\ntraitlets==4.3.3\r\nurllib3==1.25.7\r\nwcwidth==0.1.7\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.0\r\nwrapt==1.12.0\r\nzipp==0.6.0\r\n```", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "**Solution:** Don't use `tensorflow==1.14.0`. Install version `2.X.X` or higher.\r\n \r\nThat's how I stopped getting this error on the Raspberry Pi 4 running Buster v10.\r\n\r\nIf you want community precompiled wheels, you can get them [here](https://github.com/lhelontra/tensorflow-on-arm/releases).", "@lclibardi Thanks a lot, my environment has been reset , I am going to try version 2.x.x as well .", "In this case, can we close the issue as it has been solved?", "I have no objection to closing the issue owing to any concern that I raised\npreviously.\n\nKind regards.\n\nOn Thu, Mar 26, 2020, 13:02 Mihai Maruseac <notifications@github.com> wrote:\n\n> In this case, can we close the issue as it has been solved?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36141#issuecomment-604586138>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABV6MF32WD2A447XR6V4IHTRJOKCRANCNFSM4KKL6DEQ>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36141\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36141\">No</a>\n", "I was installing TensorFlow 1.14.0 on my Raspberry 3B, and I countered the same problem when I ask python3 to `import tensorflow`\r\nand then things get magical:\r\nI repeat the same order, and it made no complains, I even take a easy test and it nailed that as well.\r\n![Uploading 12.gif\u2026]()\r\n\r\n", "Can you try the same with RPi0, please (if practical without too much extra\neffort)? Thanks.\n\nOn Thu, Apr 30, 2020, 08:32 Ian <notifications@github.com> wrote:\n\n> I was installing TensorFlow on my Raspberry 3B, and I countered the same\n> problem when I ask python3 to import tensorflow\n> and then things get magical:\n> I repeat the same order, and it made no complains, I even take a easy test\n> and it nailed that as well.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36141#issuecomment-621843008>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABV6MF2IEG32ZBM7XQG4SXTRPF4WPANCNFSM4KKL6DEQ>\n> .\n>\n", "@IanGe2000 it is just a warning, so something like this should not print warning the second time:\r\n\r\n```\r\n$ python\r\n> import tensorflow as tf\r\n... prints warnings and stuff ...\r\n> import tensorflow as tf\r\n>\r\n```\r\n\r\nThat is because all these warnings are printed only once, mostly", "> **Solution:** Don't use `tensorflow==1.14.0`. Install version `2.X.X` or higher.\r\n> \r\n> That's how I stopped getting this error on the Raspberry Pi 4 running Buster v10.\r\n> \r\n> If you want community precompiled wheels, you can get them [here](https://github.com/lhelontra/tensorflow-on-arm/releases).\r\n\r\nThanks to @lclibardi for this solution!\r\n\r\nFor the record, I executed \"pip3 install https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.1.0/tensorflow-2.1.0-cp37-none-linux_armv7l.whl\" on my Pi 4 and my code ran...HDFS error gone!", "\r\n> For the record, I executed \"pip3 install https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.1.0/tensorflow-2.1.0-cp37-none-linux_armv7l.whl\" on my Pi 4 and my code ran...HDFS error gone!\r\n\r\nThanks @ckcampbell248 - worked for me rebuilding my Pi3B with Pi OS\r\n```Linux deskpi 5.4.51-v7+ #1333 SMP Mon Aug 10 16:45:19 BST 2020 armv7l GNU/Linux ```", "> I executed \"pip3 install https://github.com/lhelontra/tensorflow-on-arm/releases/download/v2.1.0/tensorflow-2.1.0-cp37-none-linux_armv7l.whl\" on my Pi 4 and my code ran...HDFS error gone!\r\n\r\nThis solved my issues as well."]}, {"number": 36140, "title": "ValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.", "body": "### System information\r\n2.0.0 Tensorflow version\r\nIDE Spyder(Python 3.7)\r\n\r\n### Source code / logs\r\nimport sys\r\nimport os\r\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.python.keras import optimizers\r\nfrom tensorflow.python.keras import Sequential\r\nfrom tensorflow.python.keras.layers import Flatten, Dense, Activation\r\nfrom tensorflow.python.keras.layers import Convolution2D, MaxPooling2D\r\nfrom tensorflow.python.keras.layers import Dropout\r\nfrom tensorflow.python.keras.optimizers import Adam\r\nfrom tensorflow.python.keras import backend as K\r\n\r\nK.clear_session()\r\ndata_entrenamiento='./data/entrenamiento'\r\ndata_validacion= './data/validacion'\r\n\r\nepocas=10\r\naltura, longitud= 100, 100\r\nbatch_size=10 \r\npasos= 50 \r\npasos_validacion= 20 \r\n\r\nfiltrosConv1=32\r\nfiltrosConv2=64\r\n\r\ntama\u00f1o_filtro1=(3,3)#Conv1\r\ntama\u00f1o_filtro2=(2,2)#Conv2\r\n\r\ntama\u00f1o_pool=(2,2)#Maxpooling\r\nclases=2\r\nlr=0.005\r\n\r\nentrenamiento_datagen= ImageDataGenerator(rescale=1./255, #255 a 1\r\n                                          shear_range=0.3,#Inclina la imageb\r\n                                          zoom_range=0.3, #Realiza un zoom en la imagen\r\n                                          horizontal_flip=True #Inversi\u00f3n de la imagen\r\n                                          )\r\n\r\nvalidacion_datagen=ImageDataGenerator(\r\n                                        rescale=1./255\r\n                                      )\r\nimagen_entrenamiento=entrenamiento_datagen.flow_from_directory(\r\n        data_entrenamiento,\r\n        target_size=(altura,longitud),\r\n        batch_size=batch_size,\r\n        class_mode='categorical'\r\n        )\r\n\r\nimagen_validacion=validacion_datagen.flow_from_directory(\r\n        data_validacion,\r\n        target_size=(altura,longitud),\r\n        batch_size=batch_size,\r\n        class_mode='categorical'\r\n        )\r\n\r\ncnn=Sequential()\r\n\r\ncnn.add(Convolution2D(filtrosConv1, tama\u00f1o_filtro1, padding='same', input_shape=(altura, longitud, 3), activation='relu'))\r\ncnn.add(MaxPooling2D(pool_size=tama\u00f1o_pool))\r\n\r\ncnn.add(Convolution2D(filtrosConv2, tama\u00f1o_filtro2, padding='same', activation='relu'))\r\ncnn.add(MaxPooling2D(pool_size=tama\u00f1o_pool))\r\n\r\ncnn.add(Flatten())\r\ncnn.add(Dense(256, activation= 'relu'))\r\ncnn.add(Dropout(0.5))\r\ncnn.add(Dense(clases, activation='softmax'))\r\n\r\ncnn.compile(loss='categorical_crossentropy', optimizer = optimizers.Adam(lr=lr), metrics=['accuracy'])\r\n\r\ncnn.fit(imagen_entrenamiento, steps_per_epoch= pasos, epochs=epocas, validation_data =imagen_validacion, validation_steps=pasos_validacion)\r\n\r\n### Problem complete\r\nFound 4001 images belonging to 2 classes.\r\nFound 2000 images belonging to 2 classes.\r\nEpoch 1/10\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-11-083720e5df85>\", line 1, in <module>\r\n    runfile('C:/Users/Acer/Documents/Resp_roja/PROGRAMAS TESIS/python/practica4_TF.py', wdir='C:/Users/Acer/Documents/Resp_roja/PROGRAMAS TESIS/python')\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"C:/Users/Acer/Documents/Resp_roja/PROGRAMAS TESIS/python/practica4_TF.py\", line 84, in <module>\r\n    cnn.fit(imagen_entrenamiento, steps_per_epoch= pasos, epochs=epocas, validation_data =imagen_validacion, validation_steps=pasos_validacion)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 603, in fit\r\n    steps_name='steps_per_epoch')\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 265, in model_iteration\r\n    batch_outs = batch_function(*batch_data)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1017, in train_on_batch\r\n    self._make_train_function()\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2116, in _make_train_function\r\n    params=self._collected_trainable_weights, loss=self.total_loss)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\", line 476, in get_updates\r\n    grads = self.get_gradients(loss, params)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizers.py\", line 92, in get_gradients\r\n    if None in grads:\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\", line 1336, in tensor_equals\r\n    return gen_math_ops.equal(self, other)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\", line 3626, in equal\r\n    name=name)\r\n\r\n  File \"C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 545, in _apply_op_helper\r\n    (input_name, err))\r\n\r\nValueError: Tried to convert 'y' to a tensor and failed. Error: None values not supported.\r\n\r\n### Please help me", "comments": ["@Chosen-ones ,\r\nCan you refer solutions in the links and let us know if it helped [link1](https://stackoverflow.com/questions/58261348/valueerror-tried-to-convert-y-to-a-tensor-and-failed-error-none-values-not) and [link2](https://stackoverflow.com/questions/58279628/what-is-the-difference-between-tf-keras-and-tf-python-keras). Thanks!", "> @Chosen-ones ,\r\n> Can you refer solutions in the links and let us know if it helped [link1](https://stackoverflow.com/questions/58261348/valueerror-tried-to-convert-y-to-a-tensor-and-failed-error-none-values-not) and [link2](https://stackoverflow.com/questions/58279628/what-is-the-difference-between-tf-keras-and-tf-python-keras). Thanks!\r\n\r\nThank you, but I removed \".python.\" to the imports and now It work. However, the accuracy don't rise from 0.5 in the validation phase. Do you know what it happen?", "Thank you, @Chosen-ones! I had the same error. Why \"remove .python\" worked? Please, I still don't get why this problem happened.", "@rosanag24 \r\nI hope this [comment](https://github.com/tensorflow/models/issues/3705#issuecomment-375563179) helps\r\n\r\n@Chosen-ones please confirm if we may proceed to move this issue to resolved status.", "@Chosen-ones ,\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 36139, "title": "Inconsistent crashing behavior with CuDNN, tensorflow RNNs, and padding", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0\r\n- Python version: Python 3.6.8\r\n- CUDA/cuDNN version: CUDA Version: 10.1/cuDNN v7.6.5\r\n- GPU model and memory: GTX Titan X\r\n\r\n**Describe the current behavior**\r\n\r\nCuDNN has inconsistent behavior with RNNs and fully masked sequences. In some cases, handing CuDNN a fully masked sequence works fine --- it happily returns an all zero vector without error, even if the input sequence has no valid tokens in it. This is the behavior that occurs on CPU as well. However --- in other cases, you get a CuDNN error (`CUDNN_STATUS_BAD_PARAM`) when handing fully padded sequences. Strangely, the thing that seems to control whether or not CuDNN error is thrown is: whether or not any of the non-fully-padded sequences in the batch have any padding.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe RNNs should return all-zero vectors for fully padded sequences on CuDNN even if none of the non-fully-padded sequences have padding. (see the below code example)\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nseq_len = 5\r\nvocab_size = 10\r\nbatch_size = 2\r\nhidden_size = 20\r\n\r\nseq_input = tf.keras.layers.Input(seq_len, dtype=tf.int32)\r\nembs = tf.keras.layers.Embedding(vocab_size, hidden_size, mask_zero=True)\r\nrnn = tf.keras.layers.GRU(hidden_size)\r\nout = rnn(embs(seq_input))\r\n\r\nkeras_model = tf.keras.Model(\r\n    inputs=seq_input,\r\n    outputs=out)\r\n\r\nworking_input = np.array(\r\n    [[0, 1, 2, 3, 4], # if this sequence has a pad, CuDNN is happy\r\n     [0, 0, 0, 0, 0]])\r\n\r\nbroken_input = np.array(\r\n    [[1, 2, 3, 4, 5], # if this sequence doesn't have a pad, CuDNN is sad\r\n     [0, 0, 0, 0, 0]])\r\n\r\nprint('Computing the working input:')\r\nres1 = keras_model.predict(working_input)\r\nprint(res1.shape)\r\nprint(res1[1])\r\nprint('Computing the broken input:')\r\n# this line causes the CuDNN error if running on GPU\r\nres2 = keras_model.predict(broken_input)\r\nprint(res2.shape)\r\nprint(res2[1])\r\n```", "comments": ["@jmhessel \r\nI have tried on colab with TF version 2.1.0-rc2 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/6065ae0d608ed737aa8e373daad06a0b/untitled590.ipynb). Thanks!", "One of the requirement to use cuDNN implementation of GRU layer is that `Inputs are not masked`\r\nSee https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\r\nTherefore you may want to set `mask_zero=False` or use CPU implementation.\r\nThanks!", "Thanks for the reply! Is it really the case that CuDNN RNNs don't support masking in any capacity? It seems like they behave properly in some cases (e.g., the first sequence in the example I gave) but not in others. But, even if they don't support masking at all, then shouldn't there be a check in tf.keras that disables CuDNN's RNNs if you are using masking, rather than failing silently? Am I understanding properly?", "#23269 seems to suggest that there is CuDNN functionality for masking --- it looks like one just needs to convert the padding tensors into sequence lengths. I suspect this is being done internally given that the all-mask sequence does return an all-zero vector for the working example.", "This also seems to imply that you can use padding with cudnn, but only post-padding?\r\n\r\nhttps://www.tensorflow.org/guide/keras/masking_and_padding\r\n\r\n```python\r\n# By default, this will pad using 0s; it is configurable via the\r\n# \"value\" parameter.\r\n# Note that you could \"pre\" padding (at the beginning) or\r\n# \"post\" padding (at the end).\r\n# We recommend using \"post\" padding when working with RNN layers\r\n# (in order to be able to use the \r\n# CuDNN implementation of the layers).\r\npadded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,\r\n                                                              padding='post')\r\n```", "Hi @jmhessel \r\n\r\nThanks for reporting the issue.\r\n\r\n1. cuDNN kernel only support right padded input sequence. We will fallback to use generic kernel when input is left padded (pre-padding) or mix-masked. See docstring for cuDNN kernel criteria in the LSTM/GRU layer. For the working case, since the data is pre-padded, it goes to generic kernel, and nothing breaks (it doesn't invoke cuDNN kernel at all).\r\n\r\n2. There is a known bug that when the whole sequence is fully masked, cuDNN kernel will break and raise an error. This is what you see for input [0, 0, 0, 0, 0]. We are tracking the bug in https://github.com/tensorflow/tensorflow/issues/33148. You can also verify this by changing the input to [1, 0, 0, 0, 0] and it should work fine.\r\n\r\nI am closing this bug since the issue need to be address is in a different bug.", "Gotcha --- that makes sense. If you use pre-padded/left-padded sequences, a non CuDNN kernel is used. Otherwise, a CuDNN kernel is used. So --- if all your sequences are either full, i.e., don't require padding or fully-padded, the CuDNN kernel is used (because it appears that everything is right-padded) but then it fails because of fully empty sequences.\r\n\r\nBTW --- is their an easy way to check to see which kernel is being used?\r\n\r\n(Also --- on the off-chance someone else encountering this problem still wants to use CuDNN: in my case, all-mask sequences are not included in the loss function, so I just converted my all-mask sequences to sequences that contain a single <UNK>; this doesn't affect the loss or the gradient if these sequences are masked out later.)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36139\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36139\">No</a>\n", "The actual kernel is only data dependent. It is possible that one batch run with cuDNN kernel since it meets the criteria, and the following one doesn't. The kernel is assigned at ops runtime, which is too far from python API level to retrieve. ", "Gotcha --- very cool :) I think I am now successfully using the cudnn kernel, because my training steps are over 2x faster 0.0"]}, {"number": 36138, "title": "Tensorflow 2.0.0 cpu, import error: DLL load failed", "body": "**System information**\r\n- OS Platform and Distribution: x86_64-w64-mingw32/x64 (64-bit) under Win10 x64 (build 18363),\r\n  Bios V1.08\r\n- Processor: Intel(R) Pentium(R) CPU N3710 @ 1,6GHz, RAM 4GB (DDR), 4 cores, L1 Cache 56kB, \r\n  L2 Cache 1024kB\r\n- Maker & Model: Acer Notebook TravelMate B117-M Signature Edition\r\n- TensorFlow installed from (source or binary): binary \r\n  i. under Rstudio 1.2.5033 with R 3.6.2 (20191212) & Miniconda3 4.7.12 (Py 3.7.4-64bit)\r\n  ii. under conda (since i. failed, see below)\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.10 (under conda)\r\n- Installed using virtualenv? pip? conda?: conda=>pip install \r\n\r\n**Describe the problem**\r\ni. Since I used keras (including reticulate & tensorflow) for the last 3 yrs under RStudio, but had various problems with my very old installation, I definitely want to continue with tensorflow + keras under RStudio and tried to install tensorflow 2.0.0. [under RStudio](https://tensorflow.rstudio.com/installation/):\r\n```\r\ninstall.packages(\"tensorflow\", version = \"2.0.0.\")   \r\nlibrary(tensorflow)\r\ninstall_tensorflow(method = \"conda\")\r\n```\r\nthen test sequence - is tensorflow installed?\r\n```\r\nlibrary(reticulate)\r\nuse_condaenv(condaenv = \"r-reticulate\", required = TRUE)\r\nlibrary(tensorflow)\r\n```\r\n```\r\n> reticulate::py_config()\r\npython:         C:/.../Miniconda3/envs/r-reticulate/python.exe\r\nlibpython:      C:/.../Miniconda3/envs/r-reticulate/python36.dll\r\npythonhome:     C:/.../Miniconda3/envs/r-reticulate\r\nversion:        3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)]\r\nArchitecture:   64bit\r\nnumpy:          C:/.../Miniconda3/envs/r-reticulate/Lib/site-packages/numpy\r\nnumpy_version:  1.18.1\r\ntensorflow:     C:\\...\\MINICO~1\\envs\\R-RETI~1\\lib\\site-packages\\tensorflow\\__init__.p\r\npython versions found: \r\n C:/.../Miniconda3/envs/r-reticulate/python.exe\r\n C:/.../Miniconda3/python.exe\r\n```\r\n```\r\n> tensorflow::tf_config()\r\n**Installation of TensorFlow not found.**\r\nPython environments searched for 'tensorflow' package:\r\n C:\\...\\Miniconda3\\envs\\r-reticulate\\python.exe\r\n C:\\...Miniconda3\\python.exe\r\n**You can install TensorFlow using the install_tensorflow() function.**\r\n```\r\nThus, although installation was terminated correctly, tensorflow was not found, i.e. not working.\r\n\r\nii. After forwarding this issue under [Installation w/ Miniconda, Reticulate 1.14, Tensorflow 2.0.0 & Keras 2.2.5.0 failing #964](https://github.com/rstudio/keras/issues/964) to RStudio/keras, tests were made to get tensorflow 2.0.0 pip installed directly under Miniconda3 (also under Anaconda3 and starting with R3.5.3 and R3.6.1), all with the same result: \r\n```\r\nconda activate r-reticulate\r\npip install tensorflow==2.0\r\npython \r\nimport tensorflow as tf\r\n```\r\nThis produced the following error log (sorry, I'm unable to copy text from the python shell window), indicating `ImportError: DLL load failed` twice:\r\n\r\n![import_tensorflow_as_tf_20200122-1800](https://user-images.githubusercontent.com/22592465/72921761-2a04fc00-3d4c-11ea-9fe5-57ab7aaa1545.PNG)\r\n\r\n**Any other info**\r\nUnfortunately, I am an absolute newbie as regards Python - taking that into account, please help...\r\n", "comments": ["If I may add: This issue - DLL not found - looks very similar to the recently closed\r\n\r\ntensorflow/tensorflow#35749 and\r\ntensorflow/tensorflow#35618\r\n\r\nwhich were solved by either\r\n\r\n- reverting from 2.1 to 2.0\r\n- installing `tensorflow-cpu` explicitly\r\n- installing _Microsoft Visual C++ Redistributable for Visual Studio_\r\n\r\n@faltinl tried all of those but none worked. The above screenshot shows the \"DLL not found\" error using TF 2.0.", "There are many DLLs that are loaded and depending on which one fails to load the solution is different. It would be great if we have the DLL name listed in the issue.", "Yeah! Do you know a good way of getting it, apart from adding a print statement to line 342 of `imp.py` (which might be a way though; and shouldn't require admin rights I guess [[but not sure as myself I'm not on Windows]])", "What is your CPU make and model?\r\nAlso I see that you installed through anaconda, adding @jjhelmus to see if there are any known issues there.", "> What is your CPU make and model? 0> data added to system information", "From what I can tell, this CPU does not have AVX instruction set. That is\nthe cause of your problem. You can build TF from source to use it, however\nprebuilt binaries will not work for you.\n\nOn Thu, Jan 23, 2020, 10:27 PM Leo Faltin <notifications@github.com> wrote:\n\n> What is your CPU make and model? 0> data added to system information\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36138?email_source=notifications&email_token=AB4UEOOVKL67T4UI326UAGDQ7KC5HA5CNFSM4KKKPVE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJZ3I4I#issuecomment-578008177>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AB4UEOIFJE4LVKJA7WJV263Q7KC5HANCNFSM4KKKPVEQ>\n> .\n>\n", "> You can build TF from source to use it, however prebuilt binaries will not work for you.\r\n\r\nTnx. That's raising several questions for me:\r\n- From which TF versions onwards has the AVX instruction set been used? I have used TF 1.8, \r\n  Reticulate 1.10 & Keras 2.1.6 until 2 weeks ago without any (for me) noticable restrictions. \r\n- Which ones are the 'prebuilt binaries', i.e. in which way does that restrict my future work with NNs? \r\n- How would I install TF from source? How could I circumvent the semi-automatic installation process\r\n  as it is now used in TF2.0 in connection with Reticulate 1.14 and Keras 2.2.5.0?", "thanks @gunan! (I would just be wondering why 1.8 has worked for @faltinl then, since AVX builds should have happened since 1.6)?\r\n\r\n@faltinl on the build-from-source page (which also contains the instructions how to build from source on Windows)\r\n\r\nhttps://www.tensorflow.org/install/source\r\n\r\nit says \r\n\r\n> Note: Starting with TensorFlow 1.6, binaries use AVX instructions which may not run on older CPUs.\r\n\r\nso in theory, even 1.8 should not have worked? Anyways, since that was a version that you say worked for you, I would suggest reverting to that one:\r\n\r\n```\r\nconda activate r-reticulate\r\npip install tensorflow==1.8\r\n```\r\n\r\n", "Tnx for replies, [@gunan](https://github.com/gunan) and [@skeydan](https://github.com/skeydan) ! I tried `pip install tensorflow==1.8` as suggested, installation went smoothly, but import failed again, though with different error messages: \r\n\r\n![import_tensorflow1 8 as tf_20200124a](https://user-images.githubusercontent.com/22592465/73063245-ac053a00-3e9e-11ea-8c70-c2c91e0d3ff3.PNG)\r\n![import_tensorflow1 8 as tf_20200124b](https://user-images.githubusercontent.com/22592465/73063251-af98c100-3e9e-11ea-994b-4594a2bb7f8c.PNG)\r\n\r\nSince TF1.8 possibly also required AVX, I tried `pip install tensorflow==1.5` too, presumably the last version before AVX has been required. As you can see...\r\n\r\n![import_tensorflow1 5 as tf_20200124](https://user-images.githubusercontent.com/22592465/73063415-030b0f00-3e9f-11ea-82cb-ffe921d64000.PNG)\r\n\r\n... a completely different reaction. Again no success message either but 6 \"future warnings\". I suspect this might be related to my Miniconda3, which didn't yet exist at the time of TF1.5. I still have the version Anaconda3-5.1.0-Windows-x86_64.exe, which I used in spring 2018 for my then working installation. What do you think - should I try to replace my present Miniconda3 with this old version? The configuration would then correspond entirely to what I had in working condition at that time, except for my then R3.5.1. That would then be the very last step...  \r\n\r\nRegardless of all these attempts, I would still like to know, [@gunan](https://github.com/gunan), which restrictions I would have to face using tensorflow 2.0.0 installed from source without AVX in my cpu and how exactly I should carry out the installation under these preconditions. ", "> Again no success message either but 6 \"future warnings\". I\r\n\r\nThese are just warnings... Can you try running a simple program against that installation?\r\n\r\nAlthough you'd need to make sure you don't use any newer than 1.5 features ...", "No, unfortunately not - I even don't come that far. Using the installation sequence I was used to apply earlier (with tensorflow 1.5, reticulate 1.10 and keras 2.1.6 (also 2.2.0), with a `restart R` for every new attempt), i.e.\r\n\r\n```\r\nlibrary(tensorflow)\r\n#install_tensorflow(method = \"conda\", conda = \"auto\", version = \"1.5.0\") #, envname = \"r-tensorflow\")\r\n# the attribute envname=... is seen as an \"unused argument\"\r\nlibrary(reticulate)\r\nuse_condaenv(\"r-reticulate\", required = TRUE) \r\n# result for \"r-tensorflow\" instead of \"r-reticulate\": Unable to locate conda environment 'r-tensorflow'.\r\nlibrary(keras) \r\n#install_keras(method = \"conda\", conda = \"auto\") #, tensorflow = \"1.5.0\")\r\nis_keras_available() # => FALSE\r\n```\r\nWhichever \"install_...\" command I actually use, I get the error message\r\n```\r\nCreating r-tensorflow conda environment for TensorFlow installation...\r\nCollecting package metadata (current_repodata.json): ...working... failed\r\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://repo.anaconda.com/pkgs/main/win-64/current_repodata.json>\r\n'https://repo.anaconda.com/pkgs/main/win-64'\r\nFehler: Error 1 occurred creating conda environment r-tensorflow\r\n```\r\nWithout any of the `install_` commands nothing happens at all, except that keras is not available. That's the reason why I suspect Miniconda3 to cause (compatibility) problems. If I run the [test programme](https://github.com/rstudio/keras/issues/548#issuecomment-425333539 ) (you might know it...!), error messages start to rain down right from the first keras-specific commands:\r\n```\r\n> input <- layer_input(shape = 3)\r\nFehler: ImportError: cannot import name 'tf_utils'\r\n> dense <- layer_dense(input, units = 32)\r\nFehler: Python module keras was not found.\r\n```\r\nObviously, there is no reticulation between the 3 packages.", "Please don't use R in this issue, this one's purpose is just to get your TensorFlow installation working from the Python side. I meant pasting a simple example in Python, just to see if 1.5 is working on your machine.\r\n\r\nHonestly though, I don't know how much sense it makes using TF 1.5. (Our current R wrappers might not be compatible with 1.5 either.)\r\n\r\nAs to compiling 2.0 from source on Windows, I certainly leave all recommendations to @Gunan - but given that building from source on a current laptop, on linux, takes hours I don't know...\r\n\r\nPerhaps it's worthwhile thinking about working in the cloud somewhere.\r\n\r\n", "Well, I don't have an example in Python - and if I had, I simply wouldn't know how to get it running there. Except for typing in the few commands forwarded in your posts I have never before used Python. \r\n\r\nMy entry to Keras and all that happened by reading [@jjallaire](https://github.com/jjallaire)'s and [@fchollet](https://github.com/fchollet)'s book \"Deep Learning with R\" which had me so excited to start working in that field and learning everything needed while going along, starting with R. It's really extremely disappointing to feel that all that should now come to such a sudden end, simply because I concluded from [this article](https://blog.rstudio.com/2019/12/20/reticulate-1-14/) that it could be the right opportunity to update my existing  - and working - Keras-setup. If I look at my (at the moment) [only available resource](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX), just finding a machine on the market exhibiting the required properties seems already to be a problem.\r\n\r\nImportant experience for me: In order to avoid the frustrating experience with unexpected requirements like this AVX thing, I have to be much more cautious with my decisions regarding updates.\r\n\r\nSorry about molesting you with all that. You are the one who deserves it the least, taking into account all the positive support I received and appreciated from your side over these years...", "One potential difference could be, if you are getting TF binaries through conda/anaconda.\r\nWe are not building them, so I am not sure which version started to require AVX. @jjhelmus may know that.\r\n\r\nWhen it comes to `pip install tensorflow`, looks like 1.5 is where we switched?\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v1.5.1\r\nSo the reason 1.8 worked for you may be because you got them through a different source, or through anaconda?\r\n\r\nTo test TF 1.x, here is a simple snippet you can try this example:\r\nhttps://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/helloworld.py\r\n", "AFAIK all the conda packages of Tensorflow released by Anaconda, Inc do not require AVX, our target CPU minimum is nocona (x86_64 with SSE3).  ", "to [@gunan](https://github.com/gunan):\r\n> So the reason 1.8 worked for you may be because you got them through a different source, or through anaconda?\r\n\r\nNo, certainly not (at least not that I noticed it!-), but what might have happened is that I downloaded v1.8, but overlooked that the version was automatically downgraded, e.g. during installation. (I must confess that, for me, Anaconda is a necessary evil - my aim is to work with Keras for R within RStudio).\r\n\r\n> To test TF 1.x, here is a simple snippet you can try this example:\r\n\r\nYeah, I'm using that snippet for tests already. However, since TF2.0.0 the command `import tensorflow as tf` never succeded on my notebook - neither using the conda prompt nor the command `tf <- import(tensorflow)` within RStudio where I prefer to work. Both produce the error messages I reported here earlier. \r\n\r\nto [@jjhelmus](https://github.com/jjhelmus):\r\n\r\nIs SSE / SSE3 a feature which is usually offered in addition to AVX or rather as an alternative, i.e. does one replace the other or are they independent features?", "@faltinl Could you please let us know if you still need help on this ? if it is resolved then please feel free to move this issue to close status ? Thanks!", "Sorry for keeping that issue open without further need. I am actually using TF2.6.0.9000. And since recently, when initializing my R programs, I get the notice\r\n\r\n`2021-10-25 17:22:41.359220: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2`\r\n\r\nSo apparently the problem has been solved (or solved itself) by updating Tensorflow regularly...\r\n\r\nThank you all for your continued interest and support! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36138\">No</a>\n"]}, {"number": 36137, "title": "add attribute '_estimator_type' to KerasRegressor and KerasClassifier", "body": "resolves: #36074 ", "comments": ["I tried locally. These changes fix the original problem.\r\nso running,\r\n\r\n```\r\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\r\nfrom sklearn.ensemble import VotingRegressor\r\na = VotingRegressor([('x', KerasRegressor(lambda: None))])\r\na.fit([[1,2]], [3])\r\n```\r\ndoesn't give `ValueError: The estimator KerasRegressor should be a regressor.`.\r\n\r\n\r\nBut it generates,\r\n```\r\n~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/wrappers/scikit_learn.py in fit(self, x, y, **kwargs)\r\n    157       self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\r\n    158 \r\n--> 159     if (losses.is_categorical_crossentropy(self.model.loss) and\r\n    160         len(y.shape) != 2):\r\n    161       y = to_categorical(y)\r\n\r\nAttributeError: 'NoneType' object has no attribute 'loss'\r\n```\r\n\r\nIs it a problem of our inputs or there is another bug?", "Can you review this, please? @fchollet ", "We are no longer maintaining the sklearn wrapper utilities and we will be removing them from the API in a future release. Please use one of the 3rd party implementations instead."]}, {"number": 36136, "title": "TPU Socket Closed", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nPlatform: Colab TPU\r\n- TensorFlow version (use command below): 2.1.0rc1\r\n\r\n**Describe the current behavior**\r\n[TPU_Socket_Traceback.pdf](https://github.com/tensorflow/tensorflow/files/4098637/TPU_Socket_Traceback.pdf)\r\n\r\n\r\nUsing model.fit raises an error which does not occur with GPU backend.\r\n\r\n`UnavailableError: Socket closed\r\nAdditional GRPC error information:\r\n{\"created\":\"@1579711911.023658059\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}`\r\n\r\n**Describe the expected behavior**\r\nCode should run the same in both instances.\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/156Q2BsM9gVS3cvDytovQzEGKmjJpP4s8\r\n\r\n**Other info / logs**\r\nAttached.\r\n", "comments": ["same error here when I use tpu on colab, and  any update?", "Same issue here", "I'm encountering the same issue\r\n", "Same issue here", "Same here on non-Colab TPU", "> Same here on non-Colab TPU\r\n\r\nAnswering my own question, turns out there was a bug with my host_call function. So this error message strongly suggests that there's something wrong in the model implementation, especially the parts that invokes host machine functions.", "Any chance we can get an update on this? It seems like a pretty fundamental problem for using TPUs, and support was introduced a while ago suggesting it should work in simple cases at the least. Just to know that some progress is being made, or someone is working on this, would be great.", "> Any chance we can get an update on this? It seems like a pretty fundamental problem for using TPUs, and support was introduced a while ago suggesting it should work in simple cases at the least. Just to know that some progress is being made, or someone is working on this, would be great.\r\n\r\nHi @Ryandry1st ,\r\nTook a quick look at your code and this seems to be the same problem as Issue #36996 .\r\nI changed your code to\r\n`x = np.random.rand(10000, 5, 2).astype(np.float32)`\r\n`y = np.random.rand(10000, 1).astype(np.float32)`\r\nand everything works as expected:\r\n\r\n> Train on 10000 samples\r\n> Epoch 1/10\r\n> 10000/10000 [==============================] - 5s 547us/sample - loss: 0.1424 - mse: 0.1419\r\n> Epoch 2/10\r\n> 10000/10000 [==============================] - 2s 164us/sample - loss: 0.0872 - mse: 0.0872\r\n> ...\r\n\r\nAlso there's a small typo, it should be `metrics=['mse']`.\r\n\r\nHope that helps!", "Yes, it appears that issue is in reference to the same problem. It is interesting that the issue seems to be solvable by simply casting the data to float32. \r\n\r\nAlso trying to cast the data into other types causes it to break in a different way as well: \r\n- float64 leads to the same socket closing error\r\n- float16 produces a warning about missing an Identity for the OpKernel for TPU for DT_HALF, because it appears to be missing from the list.\r\n- int16 does the same as float16\r\n- int32 does the same as int16 and float16\r\n- unsigned integers have the same output as signed integers\r\n- complex64 has the same missing OpKernel\r\n- float128 does not have an associated TF data type\r\n- booleans also have the missing OpKernel\r\n\r\nSo it appears that we are limited to float32 representations for everything.\r\n\r\nAlso the typo was intentional, I was desiring to see the mean absolute error as well. \r\n\r\n\r\nHere is the full error for other types besides float32:\r\nNo registered 'Identity' OpKernel for 'TPU' devices compatible with node {{node Identity}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_HALF\r\n\t.  Registered:  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_HALF, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]\r\n  device='XLA_TPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_BFLOAT16, DT_UINT32, DT_UINT64, DT_RESOURCE, DT_VARIANT]\r\n  device='XLA_CPU'; T in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, ..., DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='TPU'; T in [DT_INT32, DT_UINT32, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE, DT_BOOL, DT_COMPLEX64, DT_INT64, DT_UINT64]\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_INT64]\r\n  device='GPU'; T in [DT_UINT16]\r\n  device='GPU'; T in [DT_INT16]\r\n  device='GPU'; T in [DT_UINT8]\r\n  device='GPU'; T in [DT_INT8]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_VARIANT]\r\n  device='DEFAULT'; T in [DT_STRING]\r\n  device='DEFAULT'; T in [DT_VARIANT]\r\n  device='DEFAULT'; T in [DT_RESOURCE]\r\n  device='CPU'\r\n\r\n\t [[Identity]] [Op:PrefetchDataset]", "Is there any new progress here? I attempted to run the gist after updating tensorflow to 2.2rc0, however, this fails in tpu initialization, so I cannot test newer versions for a fix.\r\n\r\nIt has been 2 months though and this encompasses all tpu use on colab that does not use purely 32-bit floating point data, so it is a pretty widespread issue. ", "I am facing a similar issue, for a different script, I am beginning to think that when I use TPU's through google colab, the underlying engine that manages TPU instances just cuts out one of the TPU workers, in an attempt to manage loads, I am not a premium colab user, and colab preimum isnt available in my location yet... and as wild guess this issue would not appear for colab preium users, since they are promised dedicated instances for longer.\r\n\r\nThe same script works on Kaggle kernels though", "I suspect this issue is a memory leak.\r\nIn my experience I'm seeing a constant slow rise in TPU memory usage while training, until the TPU crashes:\r\n![image](https://user-images.githubusercontent.com/6796648/80746558-fc350d80-8b19-11ea-84a6-daabbd67c667.png)", "This appears to be an exception raised in a variety of different situations, so it can be quite difficult to debug. Things to try that've solved issues for me thus far: \r\n\r\n* Cast everything to `tf.float32` (i.e. if working with images, using `tf.uint8` is a non-starter)\r\n* Be careful using [tf.dataset.Dataset.cache](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) with large datasets ( @mxbi thanks for the inspiration - this solved my most recent issue).\r\n* Use `from_logits=True` when using a categorical crossentropy loss.\r\n* Upgrade to Colab Pro for less disconnects. They do disconnect runtimes from time to time.\r\n\r\nI'm sure I'll be back to add more. Would be incredible to have insightful error messages on these.", "I reran the gist using tf2.2 and found that it runs. I also tried the various data types and it seems to be running now under most cases. The cases that I found it **did not** work were for:\r\n\r\n1. uint8, uint16, uint64\r\n2. int8, int16\r\n\r\nAll other cases worked including booleans and half/double precision floats. When an error occurs it is for a missing OpKernel  type as mentioned earlier. \r\n\r\nSo it seems like the issue is resolved. An additional issue could (should) be raised to handle the missing datatypes especially since uint8 is pretty fundamental to image processing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36136\">No</a>\n", "I have the same error when using TPU on TF2.2. It is not an error caused by model implementation because, trying on CPU, it works fine. I'm currently trying to run the code with the following data preparation steps:\r\n```\r\nbs = 128\r\n\r\ntrain_data = tf.data.Dataset.from_tensor_slices((X_data_training.astype('float32'), y_data_training.astype('float32')))\r\ntrain_data = train_data.shuffle(len(X_data_training)).repeat()\r\ntrain_data = train_data.batch(bs)\r\n\r\nval_data = tf.data.Dataset.from_tensor_slices((X_data_validation.astype('float32'), y_data_validation.astype('float32')))\r\nval_data = val_data.shuffle(len(X_data_validation))\r\nval_data = val_data.batch(bs)\r\n```\r\n\r\nI've also tried to change data types but without any success. I get the same error even feeding to the fit function the data without using tf.data.Dataset.from_tensor_slices. Any idea about how to quick solve this issue?", "I am not sure, I have not run into that problem since updating to TF 2.2, although I do use the rc4 which may have a difference. Looking at the snippet you provided, I cannot see any reason to get the error. It also looks like it is something solved, or believed to be solved, in TF 2.3 coming to release candidate soon? ", "Hi, unfortunately, I've just tried the same piece of code after upgrading both my GCP VM and the TPU to TF2.3-rc2 and I am getting the same kind of error, even if with some more details (which are certainly not insightful from my perspective):\r\n\r\n> UnavailableError: Socket closed\r\n> Additional GRPC error information from remote target /job:worker/replica:0/task:0:\r\n> :{\"created\":\"@X.X\",\"description\":\"Error received from peer ipv4:X.X.X.X:X\",\"file\":\"external/com_github_grpc_grpc/src/core/lib/surface/call.cc\",\"file_line\":1056,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\r\n\r\nEDIT:\r\nWe discovered that the issue disappear if we replace LayerNormalization with BatchNormalization. Therefore, it might be a bug related to LayerNormalization implementation when using TPU.", "I hit the same issue with 2.2, 2.3 and 2.4 nightly, however it was due to me trying to cache a large dataset into memory, so please do check if you are trying to cache a large dataset!\r\n", "> I hit the same issue with 2.2, 2.3 and 2.4 night ly, however it was due to me trying to cache a large dataset into memory, so please do check if you are trying to cache a large dataset!\r\n\r\nHow can I check and mitigate this? Apologies if I am new to this."]}, {"number": 36135, "title": "model.save() Error: object has no attribute '_compile_metrics'", "body": "<em>With python3 model saving stopped to work.</em>\r\n\r\nAfter switching from python2 to python3 I started to get for my not compiled model the following error:\r\nError: 'Model' object has no attribute '_compile_metrics' \r\nLater I switched to TF 2 - right now running with 2.0.1 - but nothing has changed.\r\n\r\n**System information**\r\n- Ubuntu, Debian\r\n- TensorFlow installed from binary\r\n- TensorFlow version from 1.x -> 2.0.1\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 8 -> 10.1  based on TF needs\r\n- GPU model and memory: Various.\r\n\r\n", "comments": ["@dodiak, Can you provide the complete code to reproduce the reported issue. Thanks!", "@gadagashwini \r\nThanks and sorry for the late reply. Some significant minimization was needed.\r\nHere it is:\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.keras import models\r\nimport numpy as np\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 0:default all | 1:no info | 2:no warning | 3:no error\r\n\r\nx=np.ndarray(shape=(8,6),dtype=int,buffer=np.array([[0,2,3,4,5,1],[0,3,3,1,5,1],[0,2,5,4,5,1],[0,2,5,4,5,1],[0,2,3,4,5,1],[0,2,3,1,1,1],[0,2,1,4,5,1],[0,2,3,4,5,1]]))\r\ny=np.ndarray(shape=(8,6,1),dtype=int,buffer=np.array([[[2],[3],[4],[5],[1],[1]],[[3],[3],[1],[5],[1],[1]],[[2],[3],[4],[5],[1],[1]],[[3],[3],[1],[5],[1],[1]],[[2],[5],[4],[5],[1],[1]],[[2],[3],[1],[1],[1],[1]],[[2],[1],[4],[5],[1],[1]],[[2],[3],[4],[5],[1],[1]]]))\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nkwargs = dict()\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  batch_size = tf.compat.v1.placeholder(tf.int64, shape=[])\r\n  batch_x = tf.compat.v1.placeholder(tf.int64, shape=(None,None))\r\n  batch_y = tf.compat.v1.placeholder(tf.int64, shape=(None,None,1))\r\n  sample_weights = tf.compat.v1.placeholder(tf.float32, shape=(None,None))\r\n  inputs = [batch_x, batch_y, sample_weights, batch_size]\r\n  inputs += [K.learning_phase()]\r\n  model_input = tf.keras.layers.Input(tensor=batch_x)\r\n  optimizer = tf.keras.optimizers.Adagrad(**kwargs)\r\n  lastOutput=tf.keras.layers.Embedding( 6, 300, name='embedding', embeddings_regularizer = None )(model_input)\r\n  lastOutput=tf.compat.v1.keras.layers.CuDNNLSTM( 256, kernel_regularizer = None, recurrent_regularizer = None, bias_regularizer = None, return_sequences = True, stateful = False, **kwargs )(lastOutput)\r\n  last_hidden = lastOutput\r\n  model_output = tf.keras.layers.Dense( 6, activation = 'softmax', name='output_L', kernel_regularizer = None, bias_regularizer = None )(lastOutput)\r\n  model = tf.keras.models.Model( inputs=model_input, outputs=model_output )\r\n  # config model without compile\r\n  model.optimizer = optimizer\r\n  model.loss = 'sparse_categorical_crossentropy'\r\n  #self.model.metrics = []\r\n  labels = tf.reshape(batch_y, (-1,1))\r\n  sample_weights = tf.reshape(sample_weights, (-1,1))\r\n  last_hidden = tf.reshape(last_hidden, (-1,last_hidden.shape[-1]))\r\n  w,b = model.layers[-1].weights\r\n  logits = tf.matmul( last_hidden, w ) + b\r\n  total_loss = tf.compat.v1.losses.sparse_softmax_cross_entropy( labels=tf.reshape(labels, (-1,)), logits=logits, weights=sample_weights )\r\n  # batch training \r\n  with K.name_scope('training'):\r\n    with K.name_scope(optimizer.__class__.__name__):\r\n      training_updates = optimizer.get_updates(params=model.trainable_weights, loss=total_loss)\r\n    updates = model.updates + training_updates\r\n    model.train_function = K.function( inputs, [total_loss], updates = updates )\r\n  model.predict_function = K.function( [batch_x, K.learning_phase()], [model_output], updates=model.state_updates )\r\n\r\n  print(\"## Train Model ##\")\r\n  inputs = [x, y, y.shape[0]*y.shape[1], 1.]\r\n  loss = model.train_function(inputs)[0]\r\n  print(\"## Save Model ##\")\r\n  model.save('/tmp/model.hdf5')\r\n\r\n\r\n\r\n", "@dodiak, Tried replicating the reported issue but getting different error. Please take a look at [gist](https://colab.research.google.com/gist/gadagashwini/51289f56f062db7ca715e11ce5964596/untitled370.ipynb) and confirm. Thanks!", "ok, copy/paste error. I pasted at gist one more times the full code and it\ngive the same error as I see here.\nthanks\n\nOn Mon, Feb 3, 2020 at 7:30 AM gadagashwini <notifications@github.com>\nwrote:\n\n> @dodiak <https://github.com/dodiak>, Tried replicating the reported issue\n> but getting different error. Please take a look at gist\n> <https://colab.research.google.com/gist/gadagashwini/51289f56f062db7ca715e11ce5964596/untitled370.ipynb>\n> and confirm. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36135?email_source=notifications&email_token=AAIZB6AEXJ2INTLG6PKOLWTRA62ZHA5CNFSM4KKH7VLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKSVDJY#issuecomment-581259687>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIZB6DO7UDN4KSPO4OUL5LRA62ZHANCNFSM4KKH7VLA>\n> .\n>\n", "@dodiak, I don't see the gist in the comment. Can you include the gist in comment box. Thanks!", "Sorry, first time using gist, wo one more times:\nhttps://colab.research.google.com/gist/dodiak/cf98f0eb5c0ecf7daad735ac05102f4d/untitled370.ipynb\n\nOn Thu, Feb 6, 2020 at 7:28 AM gadagashwini <notifications@github.com>\nwrote:\n\n> @dodiak <https://github.com/dodiak>, I don't see the gist in the comment.\n> Can you include the gist in comment box. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/36135?email_source=notifications&email_token=AAIZB6H2XCKR32PGFF4L3LTRBOUZRA5CNFSM4KKH7VLKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEK6CZJQ#issuecomment-582757542>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAIZB6EI46GKFRU6J6KJGLLRBOUZRANCNFSM4KKH7VLA>\n> .\n>\n", "@dodiak I have ran your code using tf-nightly and I am not running into any errors. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/4683486c67976a2accce02830a29685c/untitled370.ipynb#scrollTo=p3-KfWu3sUap). Thanks!", "Great, thanks a lot. I can confirm that with:\r\ntf-nightly             2.2.0.dev20200218  \r\ntf-nightly-gpu         2.2.0.dev20200227  \r\nthe simple example works. Give me a few more days please to test with a full version.", "Sure. I am closing this issue now. If you have any more questions please open a new issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36135\">No</a>\n", "So I can confirm that it runs as we expected. Thanks a lot."]}, {"number": 36134, "title": "Cannot build TensorFlow from source on Mac OS Catalina", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: miniconda\r\n- Bazel version (if compiling from source): Using Bazelisk, so I presume 0.12.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nTrying to install TensorFlow from source for CPU only, I encounter this issue after following the install instructions precisely and then using the following code after installing bazelisk with Go.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazelisk clean --expunge\r\nsudo xcode-select -s /Applications/Xcode.app/Contents/Developer\r\nsudo xcodebuild -license\r\nbazelisk clean --expunge\r\nbazelisk build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: /Users/harrisonwilde/python/tensorflow/tensorflow/lite/tools/optimize/calibration/BUILD:106:1: undeclared inclusion(s) in rule '//tensorflow/lite/tools/optimize/calibration:calibration_reader':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/tools/optimize/calibration/calibration_reader.cc':\r\n  'external/com_google_absl/absl/strings/string_view.h'\r\n  'external/com_google_absl/absl/base/internal/throw_delegate.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /Users/harrisonwilde/python/tensorflow/tensorflow/python/tools/BUILD:80:1 undeclared inclusion(s) in rule '//tensorflow/lite/tools/optimize/calibration:calibration_reader':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/lite/tools/optimize/calibration/calibration_reader.cc':\r\n  'external/com_google_absl/absl/strings/string_view.h'\r\n  'external/com_google_absl/absl/base/internal/throw_delegate.h'\r\nINFO: Elapsed time: 325.470s, Critical Path: 117.47s\r\nINFO: 1821 processes: 1821 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Since you are installing from source, can you post the commit hash/branch you are building from?", "Apologies, I actually got this working after trying with the latest release branch, I was initially trying to build master but your comment made me realise I didn't want to do that.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36134\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36134\">No</a>\n", "how to solve this problem ?\r\n", "@zhaozheng09 please open a new issue and fill in issue template. This issue has been resolved already."]}, {"number": 36133, "title": "initializer() method has no attribute run()", "body": "**System information**\r\n- Windows 10(x64)\r\n- TensorFlow 2.1.0\r\n- Python 3.7\r\n- CUDA 10.0:\r\n- GPU - NVIDIA GeForce GTX 1660 Ti:\r\n__________________________________________________________________________________________________________\r\n\r\nI wanted to repeat some code from book named \"Machine Learning with TensorFlow\" of Nishant Shukla (@BinRoot)\r\n\r\n\r\n![\u0421\u043d\u0438\u043c\u043e\u043a](https://user-images.githubusercontent.com/43477533/72905924-83285c00-3d5b-11ea-81f7-dd8b37468814.PNG)\r\n\r\nHowever there's an error:\r\n`File \"C:/Users/PC2/PycharmProjects/Adil/newTensor/Main.py\", line 6, in <module>`\r\n`spikes.initializer.run()`\r\n`AttributeError: 'NoneType' object has no attribute 'run'`\r\n__________________________________________________________________________________________________________\r\n**And I would like to know if somebody has faced that before. I tried to reinstall tensorforce but it didn't help. I think it's because of the version update. However I don't know to fix that. I would really appreciate if somebody could help me! Thanks in advance!**", "comments": ["@adekability, Issue looks like Tensorflow version incompatibility. \r\nPlease provide the standalone code to replicate the reported issue. Thanks!\r\n", "@adekability, Provide the minimal code snippet to replicate the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 36132, "title": "Build Tensorflow 2.0 error (CUDA 10.1, Windows 10, conda)", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.7.6 (conda)\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1/ 7.6\r\n- GPU model and memory: GeForce GTX 1660 Ti (6GB)\r\n- Procesor: Intel Core i7 9th Generation (AVX2 support)\r\n\r\n\r\n**Describe the problem**\r\nI need tensorflow==2.0.0 to use with https://github.com/matterport/Mask_RCNN/pull/1896. (Or fix that pull request to run on my computer).\r\nI couldn't use the official binaries due to CUDA version (I have 10.1 but tf 2.0 needs CUDA 10.0). I can't downgrade it (GPU driver is 10.1 or 10.2). I found https://github.com/tensorflow/tensorflow/issues/33026 issue and there was advice about building tf from source (https://github.com/tensorflow/tensorflow/issues/33026#issuecomment-539915647)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI was running comands from https://medium.com/@amsokol.com/how-to-build-and-install-tensorflow-2-0-1aab4eaa4c1a which is based on official documentation.\r\n\r\nDiffs:\r\n* Git for Windows and MSYS2 are installed on C:/ drive directly (so I set system variables properly).\r\n* Python is installed from conda (3.7.6) (path is set correctly)\r\n* instead of venv I'm using conda. Due to recent pip update I don't run pip update (I have 19.3.1 version)\r\n* I set compute capability as 3.5 (there is no official info on nvidia website https://developer.nvidia.com/cuda-gpus though it's Turing series and should be 7.x)\r\n\r\nAfter executing ```bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package``` it throws the errors:\r\n\r\n```ERROR: An error occurred during the fetch of repository \u2018io_bazel_rules_docker\u2019: Traceback (most recent call last): File \u201cC:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl\u201d, line 234 _clone_or_update(ctx) File \u201cC:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl\u201d, line 74, in _clone_or_update fail((\u201cerror cloning %s:\\n%s\u201d % (ctx\u2026.))) error cloning io_bazel_rules_docker: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(\u201cC:\\msys64\\usr\\bin\u201d -c \u201c cd C:/users/konta/_bazel_konta/cs3mthex/external set -ex ( cd C:/users/konta/_bazel_konta/cs3mthex/external && if ! ( cd \u2018C:/users/konta/_bazel_konta/cs3mthex/externa(\u2026)): Odmowa dost?pu. ERROR: error loading package \u2018\u2019: Encountered error while reading extension file \u2018repositories/repositories.bzl\u2019: no such package \u2018@io_bazel_rules_docker//repositories\u2019: Traceback (most recent call last): File \u201cC:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl\u201d, line 234 _clone_or_update(ctx) File \u201cC:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl\u201d, line 74, in _clone_or_update fail((\u201cerror cloning %s:\\n%s\u201d % (ctx\u2026.))) error cloning io_bazel_rules_docker: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(\u201cC:\\msys64\\usr\\bin\u201d -c \u201c cd C:/users/konta/_bazel_konta/cs3mthex/external set -ex ( cd C:/users/konta/_bazel_konta/cs3mthex/external && if ! ( cd \u2018C:/users/konta/_bazel_konta/cs3mthex/externa(\u2026)): Odmowa dost?pu. ERROR: error loading package \u2018\u2019: Encountered error while reading extension file \u2018repositories/repositories.bzl\u2019: no such package \u2018@io_bazel_rules_docker//repositories\u2019: Traceback (most recent call last): File \u201cC:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl\u201d, line 234 _clone_or_update(ctx) File \u201cC:/users/konta/_bazel_konta/cs3mthex/external/bazel_tools/tools/build_defs/repo/git.bzl\u201d, line 74, in _clone_or_update fail((\u201cerror cloning %s:\\n%s\u201d % (ctx\u2026.))) error cloning io_bazel_rules_docker: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(\u201cC:\\msys64\\usr\\bin\u201d -c \u201c cd C:/users/konta/_bazel_konta/cs3mthex/external set -ex ( cd C:/users/konta/_bazel_konta/cs3mthex/external && if ! ( cd \u2018C:/users/konta/_bazel_konta/cs3mthex/externa(\u2026)): Odmowa dost?pu. INFO: Elapsed time: 0.103s INFO: 0 processes. FAILED: Build did NOT complete successfully (0 packages loaded)```\r\n\r\nI found the solution of this here: https://github.com/tensorflow/tensorflow/issues/28824#issuecomment-536669038 - I added these lines to WORKSPACE file in tensorflow directory (tensorflow repo).\r\n\r\nIt gave me the next errors, so I needed to add also \r\n\r\n```\r\nhttp_archive(\r\n    name = \"com_google_protobuf\",\r\n#    build_file = \"//:third_party/protobuf.BUILD\",\r\n    sha256 = \"b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59\",\r\n    # This protobuf release is based on protobuf 3.8.0.\r\n    strip_prefix = \"protobuf-310ba5ee72661c081129eb878c1bbcec936b20f0\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\",\r\n        \"https://github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\",\r\n    ],\r\n)\r\n```\r\n to my WORKSPACE file. This fix was working. Now it can't fetch of repository 'png_archive' (see log at the bottom). I tried to comment it in BUILD files but it was causing the next lacks (missing flatbuffers). I have no idea how it can be fixed.\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```Odmowa dost?pu.``` means 'Permission/access denied'. I thought it's caused by Windows Defender or antivirus but it's turned off. I'm using terminal with admin privileges and user access control is set as 'never notify'. Probably there is problem with repos (when I added newer config of com_google_protobuf and  io_bazel_rules_docker there wasn't previous errors).\r\n\r\nFull log after running build:\r\n```(tf2.0) C:\\Users\\konta\\Documents\\Repos\\tf-2.0\\tensorflow>bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package                                                                          Starting local Bazel server and connecting to it...                                                                     INFO: Options provided by the client:                                                                                     Inherited 'common' options: --isatty=1 --terminal_columns=120                                                         INFO: Options provided by the client:                                                                                     'build' options: --python_path=C:/Users/konta/Miniconda3/envs/tf2.0/python.exe                                        INFO: Reading rc options for 'build' from c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.bazelrc:                      'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --config=v2                                                                                       INFO: Reading rc options for 'build' from c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.tf_configure.bazelrc:         'build' options: --action_env PYTHON_BIN_PATH=C:/Users/konta/Miniconda3/envs/tf2.0/python.exe --action_env PYTHON_LIB_PATH=C:/Users/konta/Miniconda3/envs/tf2.0/lib/site-packages --python_path=C:/Users/konta/Miniconda3/envs/tf2.0/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5 --config=cuda --config monolithic --copt=-w --host_copt=-w --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --verbose_failures --distinct_host_configuration=false --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0                                                          INFO: Found applicable config definition build:v2 in file c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.bazelrc: --define=tf_api_version=2                                                                                                  INFO: Found applicable config definition build:cuda in file c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true                                                                        INFO: Found applicable config definition build:using_cuda in file c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain       INFO: Found applicable config definition build:monolithic in file c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.bazelrc: --define framework_shared_object=false                                                                             INFO: Found applicable config definition build:opt in file c:\\users\\konta\\documents\\repos\\tf-2.0\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --define with_default_optimizations=true                                                   INFO: Call stack for the definition of repository 'png_archive' which is a tf_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:124:19):                                                    - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:260:5                                       - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'png_archive' used the following cache hits instead of downloading the corresponding file.              * Hash 'ca74a0dace179a8422187671aee97dd3892b53e168627145271cad5b5ac81307' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/glennrp/libpng/archive/v1.6.37.tar.gz                                                            If the definition of 'png_archive' was updated, verify that the hashes were also updated.                               ERROR: An error occurred during the fetch of repository 'png_archive':                                                     Traceback (most recent call last):                                                                                           File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 104                                          _apply_patch(ctx, ctx.attr.patch_file)                                                                          File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 71, in _apply_patch                          _execute_and_check_ret_code(ctx, cmd)                                                                           File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code                                                                                                                                   fail(\"Non-zero return code({1}) when ...))                                                              Non-zero return code(256) when executing 'C:\\msys64\\usr\\bin -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\" \"-i\" \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi.patch\"':        Stdout:                                                                                                                 Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(\"C:\\msys64\\usr\\bin\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\\\" \\\"-i\\\" \\\"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi(...)): Odmowa dost?pu.                                                  INFO: Call stack for the definition of repository 'flatbuffers' which is a third_party_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:204:28):                                           - C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/flatbuffers/workspace.bzl:6:5                            - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:39:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:82:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'flatbuffers' used the following cache hits instead of downloading the corresponding file.              * Hash '3f4a286642094f45b1b77228656fbd7ea123964f19502f9ecfd29933fd23a50b' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz                                                        If the definition of 'flatbuffers' was updated, verify that the hashes were also updated.                               INFO: Call stack for the definition of repository 'cython' which is a tf_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:124:19):                                                         - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:754:5                                       - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'cython' used the following cache hits instead of downloading the corresponding file.                   * Hash 'bccc9aa050ea02595b2440188813b936eaf345e85fb9692790cecfe095cf91aa' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/cython/cython/archive/0.28.4.tar.gz                                                              If the definition of 'cython' was updated, verify that the hashes were also updated.                                    INFO: Call stack for the definition of repository 'eigen_archive' which is a tf_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:124:19):                                                  - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:168:5                                       - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'eigen_archive' used the following cache hits instead of downloading the corresponding file.            * Hash 'f3d69ac773ecaf3602cb940040390d4e71a501bb145ca9e01ce5464cf6d4eb68' for https://storage.googleapis.com/mirror.tensorflow.org/bitbucket.org/eigen/eigen/get/049af2f56331.tar.gz                                                           If the definition of 'eigen_archive' was updated, verify that the hashes were also updated.                             INFO: Call stack for the definition of repository 'icu' which is a third_party_http_archive (rule definition at C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl:204:28):                                                   - C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/icu/workspace.bzl:11:5                                   - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:42:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/workspace.bzl:82:5                                        - C:/users/konta/documents/repos/tf-2.0/tensorflow/WORKSPACE:38:1                                                      INFO: Repository 'icu' used the following cache hits instead of downloading the corresponding file.                      * Hash 'e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-62-1.tar.gz                                                      If the definition of 'icu' was updated, verify that the hashes were also updated.                                       ERROR: C:/users/konta/documents/repos/tf-2.0/tensorflow/tensorflow/tools/pip_package/BUILD:157:1: no such package '@png_archive//': Traceback (most recent call last):                                                                                  File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 104                                          _apply_patch(ctx, ctx.attr.patch_file)                                                                          File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 71, in _apply_patch                          _execute_and_check_ret_code(ctx, cmd)                                                                           File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code                                                                                                                                   fail(\"Non-zero return code({1}) when ...))                                                              Non-zero return code(256) when executing 'C:\\msys64\\usr\\bin -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\" \"-i\" \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi.patch\"':        Stdout:                                                                                                                 Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(\"C:\\msys64\\usr\\bin\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\\\" \\\"-i\\\" \\\"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi(...)): Odmowa dost?pu.                                                   and referenced by '//tensorflow/tools/pip_package:licenses'                                                            ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@png_archive//': Traceback (most recent call last):                                                                                File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 104                                          _apply_patch(ctx, ctx.attr.patch_file)                                                                          File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 71, in _apply_patch                          _execute_and_check_ret_code(ctx, cmd)                                                                           File \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/repo.bzl\", line 52, in _execute_and_check_ret_code                                                                                                                                   fail(\"Non-zero return code({1}) when ...))                                                              Non-zero return code(256) when executing 'C:\\msys64\\usr\\bin -l -c \"patch\" \"-p1\" \"-d\" \"C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\" \"-i\" \"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi.patch\"':        Stdout:                                                                                                                 Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(179): CreateProcessW(\"C:\\msys64\\usr\\bin\" -l -c \"\\\"patch\\\" \\\"-p1\\\" \\\"-d\\\" \\\"C:/users/konta/_bazel_konta/cs3mthex/external/png_archive\\\" \\\"-i\\\" \\\"C:/users/konta/documents/repos/tf-2.0/tensorflow/third_party/png_fix_rpi(...)): Odmowa dost?pu.                                                  INFO: Elapsed time: 92.788s                                                                                             INFO: 0 processes.                                                                                                      FAILED: Build did NOT complete successfully (174 packages loaded, 4346 targets configured)```\r\n", "comments": ["I installed tf 2.1.0rc0 and run in python console:\r\n\r\n```\r\n>>> import tensorflow as tf \r\n>>> tf.test.is_built_with_cuda()\r\nTrue\r\n>>> tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\r\n2020-01-22 15:51:56.474875: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-01-22 15:51:56.497346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-01-22 15:51:57.671307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 1660 Ti computeCapability: 7.5\r\ncoreClock: 1.455GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s\r\n2020-01-22 15:51:58.574146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/device:GPU:0 with 4625 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nTrue\r\n```\r\nso __GeForce GTX 1660 Ti computeCapability: 7.5__", "Reported compute capability of 7.5 for 1660 Ti is correct.\r\n\r\nGeForce GTX 1660 Ti is built on Turing TU116-400-A1 and CUDA reports it's compute capability to be 7.5, although it's one of the very few GPU-s with 7.5 that *does not* have Tensor Cores, go figure... :-)\r\n\r\nYou can also safely switch now to tf 2.1.0 final, it's built with CUDA 10.1.\r\n\r\nIf you still want to have fun building from source in Win10, make sure the proper VC version is installed and detected (incl make sure to set TF_VC_VERSION!). At the moment builds should work both with CUDA 10.1 and CUDA 10.0. Couple of hints can be found at https://github.com/tensorflow/tensorflow/issues/33544#issuecomment-577678541", "Thanks for answer and hints.\r\nI just tried tf 2.1.0rc0 and it works with MaskRCNN (in contrast to 2.1.0) so building from source is not necessary for me now.", "@juskuz, Can we close if the issue resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36132\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36132\">No</a>\n", "I think so. I closed the issue. You can change the stat label"]}, {"number": 36131, "title": "[TFLite int16] 16-bit reference kernel FULLY_CONNECTED", "body": "This PR is one of steps to extend 8-bit quantization to support symmetric 16-bit activations.\r\n\r\nEach activation is of type int16 and symmetric around zero. The weight tensor precision remains at 8-bit signed values. The bias is set to int64 precision.\r\n\r\nIn this PR we introduce implementation and tests for FULLY_CONNECTED kernel reference function.\r\n\r\nFULLY_CONNECTED \r\n\u202f Input 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Input 1 (Weight): \r\n\u202f \u202f data_type \u202f: int8 \r\n\u202f \u202f range \u202f \u202f \u202f: [-127, 127] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Input 2 (Bias): \r\n\u202f \u202f data_type \u202f: int64 \r\n\u202f \u202f range \u202f \u202f \u202f: [-(1<<39), (1<<39)-1] \r\n\u202f \u202f granularity: per-tensor \r\n\u202f \u202f restriction: (scale, zero_point) = (input0_scale * input1_scale[...], 0) \r\n\u202f Output 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n", "comments": []}, {"number": 36130, "title": "[TFLite int16] 16-bit reference kernel operators MAX_POOL_2D and AVERAGE_POOL_2D", "body": "This PR is one of steps to extend 8-bit quantization to support symmetric 16-bit activations.\r\n\r\nEach activation is of type int16 and symmetric around zero. The weight tensor precision remains at 8-bit signed values. The bias is set to int64 precision.\r\n\r\nIn this PR we introduce implementation and tests for MAX_POOL_2D and AVERAGE_POOL_2D kernel reference functions.\r\n\r\nAVERAGE_POOL_2D \r\n\u202f Input 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Output 0: \r\n\u202f \u202f data_type \u202f: int8 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f restriction: Input and outputs must all have same scale\r\n\r\nMAX_POOL_2D \r\n\u202f Input 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f Output 0: \r\n\u202f \u202f data_type \u202f: int16 \r\n\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n\u202f \u202f granularity: per-tensor, zero_point=0 \r\n\u202f restriction: Input and outputs must all have same scale\r\n", "comments": ["@renjie-liu Hi, Sorry to bother, but could you please review this PR ?", "`third_party/tensorflow/lite/kernels/pooling_test.cc:441:44: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n              ElementsAreArray({(44 - 128) << 8, (92 - 128) << 8}));\r\n                                ~~~~~~~~~~ ^\r\nthird_party/googletest/googlemock/include/gmock/gmock-matchers.h:4815:60: note: expanded from macro 'EXPECT_THAT'\r\n    ::testing::internal::MakePredicateFormatterFromMatcher(matcher), value)\r\n                                                           ^~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:117:23: note: expanded from macro 'EXPECT_PRED_FORMAT1'\r\n  GTEST_PRED_FORMAT1_(pred_format, v1, GTEST_NONFATAL_FAILURE_)\r\n                      ^~~~~~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:104:17: note: expanded from macro 'GTEST_PRED_FORMAT1_'\r\n  GTEST_ASSERT_(pred_format(#v1, v1), \\\r\n                ^~~~~~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:79:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\nthird_party/tensorflow/lite/kernels/pooling_test.cc:441:61: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n              ElementsAreArray({(44 - 128) << 8, (92 - 128) << 8}));\r\n                                                 ~~~~~~~~~~ ^\r\nthird_party/googletest/googlemock/include/gmock/gmock-matchers.h:4815:60: note: expanded from macro 'EXPECT_THAT'\r\n    ::testing::internal::MakePredicateFormatterFromMatcher(matcher), value)\r\n                                                           ^~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:117:23: note: expanded from macro 'EXPECT_PRED_FORMAT1'\r\n  GTEST_PRED_FORMAT1_(pred_format, v1, GTEST_NONFATAL_FAILURE_)\r\n                      ^~~~~~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:104:17: note: expanded from macro 'GTEST_PRED_FORMAT1_'\r\n  GTEST_ASSERT_(pred_format(#v1, v1), \\\r\n                ^~~~~~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:79:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\\r\n                                                   ^~~~~~~~~~\r\nthird_party/tensorflow/lite/kernels/pooling_test.cc:923:44: error: shifting a negative signed value is undefined [-Werror,-Wshift-negative-value]\r\n              ElementsAreArray({(96 - 128) << 8, (160 - 128) << 8}));\r\n                                ~~~~~~~~~~ ^\r\nthird_party/googletest/googlemock/include/gmock/gmock-matchers.h:4815:60: note: expanded from macro 'EXPECT_THAT'\r\n    ::testing::internal::MakePredicateFormatterFromMatcher(matcher), value)\r\n                                                           ^~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:117:23: note: expanded from macro 'EXPECT_PRED_FORMAT1'\r\n  GTEST_PRED_FORMAT1_(pred_format, v1, GTEST_NONFATAL_FAILURE_)\r\n                      ^~~~~~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:104:17: note: expanded from macro 'GTEST_PRED_FORMAT1_'\r\n  GTEST_ASSERT_(pred_format(#v1, v1), \\\r\n                ^~~~~~~~~~~\r\nthird_party/googletest/googletest/include/gtest/gtest_pred_impl.h:79:52: note: expanded from macro 'GTEST_ASSERT_'\r\n  if (const ::testing::AssertionResult gtest_ar = (expression)) \\`", "@wwwind can you please fix above error ?", "Hi @renjie-liu Could you please re-approve this PR ? There was an internal failure due to the warning Wshift-negative-value, which I have corrected. Thanks!", "Hi @renjie-liu Could you please re-approve this PR ? I had to push a small fix for the failure in some internal tests. Thanks!", "Hi @rthadur I pushed a fix. Could you please take a look and re-approve this PR ? \r\nThanks", "@gbaned Can this PR be merged ? Thanks", "@wwwind getting this error internally , can you please check \r\n\r\n`third_party/tensorflow/lite/kernels/test_util.cc:331\r\nExpected equality of these values:\r\n  CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate())\r\n    Which is: 0\r\n  1\r\nExpecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate`", "Hi @renjie-liu Could you please check and re-approve this PR ? I had to put a fix for some internal failures. I added versioning to these operators - I believe this will fix this failure. \r\nThanks!", "@wwwind can you please resolve conflicts ", "Hi @renjie-liu Could you please re-approve this PR ? I had to push a change to fix the conflict.\r\nThanks!", "Hi @liufengdb ! Could you please re-approve this PR ? I had to push a fix for the merge conflict.\r\nThanks", "@liufengdb  could you please help merge this PR internally ?", "Hi @rthadur , @liufengdb ! I had to push a small fix for the CI failure. Could you you please re-approve ? Thanks!", "@wwwind Can you please check build failures. Thanks!", "@gbaned I pushed a fix. \r\nHi @renjie-liu ! Could you please re-approve it again ? Thank you!", "@wwwind here is the internal error : \r\n`third_party/tensorflow/lite/kernels/test_util.cc:330\r\nExpected equality of these values:\r\n  CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate())\r\n    Which is: 0\r\n  1\r\nExpecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate` can you please check ?", "Hi @jdduke ! Sorry to bother you, but could you please help with this error:\r\n\r\n> third_party/tensorflow/lite/kernels/test_util.cc:330 Expected equality of these values: CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate()) Which is: 0 1 Expecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate\r\n\r\nI added version at this [commit ](https://github.com/tensorflow/tensorflow/pull/36130/commits/0a8dc639e244181a393314e174333f8aed00347d)\r\nSo, if input_types is TensorType_INT8, it should return the version 2 as it was before.\r\nMaybe, input_types is not set in the test ? \r\nAre these tests public in another repo ? Where can I find them to run and debug ?\r\nThanks!\r\nElena", "Hi @rthadur Could you please help me with this reported error ?\r\nIt looks that the test lives in another repo, because tensorflow is included as the third_party library.\r\nIs this repo public ? Can I reproduce this test locally ?\r\nThanks a lot !\r\n\r\n> third_party/tensorflow/lite/kernels/test_util.cc:330 Expected equality of these values: CountPartitionsDelegatedTo(interpreter_.get(), TestNnApiDelegate()) Which is: 0 1 Expecting operation to be accelerated but cannot find a partition associated to the NNAPI delegate\r\n\r\n", "The failing CI test looks unrelated, can you rebase your CL and we can try again?", "Hi @jdduke Could you please re-approve this PR ? I rebased it. Thanks!", "Hi @rthadur Could you please re-approve this PR ? I had to rebase it. Thanks!", "@wwwind Could you please resolve the conflicts? Thanks!", "Hi @rthadur I have resolved the conflict on this PR. Could you please re-approve it ? Thanks!", "@renjie-liu can you please approve this ?thank you", "Hi @jdduke, import/copybara is red on this PR. Is this due to the internal error ?\r\n\r\nThanks\r\n", "I'll try manually merging internally, thanks for flagging.", "Will you please stop sending me emails about I have to reset my phone every\ntime you do thank you\n\nOn Fri, Apr 3, 2020, 3:09 AM Elena Zhelezina <notifications@github.com>\nwrote:\n\n> Hi @jdduke <https://github.com/jdduke> Could you please re-approve this\n> PR ? I rebased it\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/36130#issuecomment-608347613>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKXF6BUHGHZHODHNKBJPJUTRKWYT5ANCNFSM4KKGZ4KQ>\n> .\n>\n"]}]