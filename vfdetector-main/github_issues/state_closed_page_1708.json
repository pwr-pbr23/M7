[{"number": 1660, "title": "Possible memory leak with TensorArray", "body": "I'm not sure if the usage is incorrect (see below), but I am seeing a memory leak in cpu ram when using the `TensorArray`. Seems to be consistent regardless of device placement. I noticed this because I've been using the `dynamic_rnn`.  Happy to provide further info if needed.\n### Environment info\n\nOperating System:\nUbuntu 14.04\n\nIf installed from sources, provide the commit hash:\n\n`35c3f16f81b6a1d39a004c97160c22b73d3798e5`\n### Steps to reproduce\n\nRun this in a script  (I had to add `from tensorflow.python.ops import tensor_array_ops` to `tensorflow/python/__init__.py`)\n\n```\nimport tensorflow as tf\n\ndef ta_mem_leak():\n\n    ta = tf.python.tensor_array_ops.TensorArray(\n                        dtype=tf.float32, size=10000)\n    ta = ta.write(0, [0.])\n    final_outputs = ta.read(0)\n    return [final_outputs]\n\nif __name__ == \"__main__\":\n    with tf.Graph().as_default(), tf.Session() as session:\n        ops = ta_mem_leak()\n        for i in xrange(200000):\n            res = session.run(ops)                          \n```\n", "comments": ["FYI `Unref`ing the `tensor_array` object after getting them out of the `resource_mgr` seemed to do the trick for me - https://github.com/awni/tensorflow/commit/af8bef11151ff98137c9b5ce1a27e543f311e7d9\n\nI can PR this if it looks like the right fix.\n", "This does look correct.  Send a PR.  Make sure the rnn tests continue to pass.\n", "@ebrevdo Sounds good, I submitted the PR #1691. Please review.\n", "I'm assuming this can be closed since #1691 is merged now.\n"]}, {"number": 1659, "title": "Updated year on License", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it! \n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks for the contribution!  IANAL, but I don't think this is needed -- you have to maintain at least the copyright year of the first distribution (2015).  Some resources online say it is okay to update on major updates / releases though, but I'd rather just keep the original year unless lawyercats suggest otherwise.   git history is a more accurate source of truth than what we put in this file anyway ;)\n\n@martinwicke might know some lawyercats who say otherwise, so we can re-open if needed.\n"]}, {"number": 1658, "title": "Tensorflow with GPU-support crashes when opening many CPU-only sessions in parallel", "body": "The following code works fine in a CPU-only Tensorflow, but crashes on a GPU-enabled Tensorflow installation (0.7.1, with a Titan X, NVidia driver 352.79) when run many times in parallel:\n\n``` python\n# test_tensorflow_session.py\nimport tensorflow as tf\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\nsess = tf.Session()\n```\n\nBash command to run it in parallel:\n\n``` bash\nfor i in {1..48}; do ((python ./test_tensorflow_session.py 2> output$i.err 1> output$i.out ) &) ; done\n```\n\nIf you then look into the `output*.err` files, you will see that most of the processes crashed. The output will look like this:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:481] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:114] retrieving CUDA diagnostic information for host: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:121] hostname: gpu1\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:146] libcuda reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:257] driver version file contents: \"\"\"NVRM version: NVIDIA UNIX x86_64 Kernel Module  352.79  Wed Jan 13 16:17:53 PST 2016\nGCC version:  gcc version 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC) \n\"\"\"\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:150] kernel reported version is: 352.79\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:226] kernel version seems to match DSO: 352.79\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: \nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\n```\n\nFor the sake of completeness, here is the use case: applying a Tensorflow model is just a small part in processing large amounts of data; processing is parallelised simply by using multiple processes.\n\nI also tried wrapping the `tf.Session()` call in `with tf.device('/cpu:0'):`, but that didn't change anything. I assume that it is trying to get exclusive access to the GPU, and if it can't do that, it crashes. This is a bit annoying, since by setting `os.environ['CUDA_VISIBLE_DEVICES'] = ''`, I am actively trying to disable the GPU.\n\nIdeas?\n", "comments": ["Hm, that is indeed weird.  Not sure why CUDA_VISIBLE_DEVICES isn't helping you.\n\nCan you try the following in the meantime?\n\n```\nconfig=tf.ConfigProto(device_count={\"GPU\": 0, \"CPU\": 1})\ntf.Session(config=config) \n```\n", "Thanks for the amazingly quick reply! Unfortunately, explicitly setting the `device_count` doesn't change the situation.\n\nI would love to gather more debug information. Right now, all I can see is that a `std::system_error` is thrown somewhere, although it looks like it is thrown somewhere in the CUDA code, not in Tensorflow (I couldn't find any matching line in Tensorflow's codebase). Do you have any advice on how I could get a more meaningful stack trace?\n", "I think it would be nice to figure out what's causing the actual failure at the cuda level, but it's surprising to me that this code is being reached at all -- I think that's the bug.  Basically, we need to figure out why \"GPUMachineManager()\" is ever being called when no GPU devices are requested -- that's the entry point that tries to do GPU initilization.  I'll try to find time to debug this.\n", "Okay I partially take this back.  It is weird that the stream executor code is called at all, but I don't think that's the cause of the problem.  I tried reproducing your problem at HEAD, spawning 48 processes with CUDA_VISIBLE_DEVICES=\"\" and I didn't get any std::system_error.\n\nMy guess from looking up the error is that each session is creating a number of std::threads, and at some point one of them blows up.\n\nAnother thing to try for debugging is to set the intra_op_parallelism_threads and intra_op_parallelism_threads in the ConfigProto to smaller values (they default to the number of cores on the machine).\n\nAs for why this causes problems only with the GPU pip, I'm not entirely sure yet :)\n", "Oh gosh, turns out the problem was somewhere else completely. I hit the user process limit. Just to recap, I have two servers, one with a GPU and one without, otherwise nearly identical. However, on the CPU-only server, I increased the number of maximum processes per user in `/etc/security/limits.conf` a while ago, and when setting up the GPU server, I forgot to do that. It simply crashed because it wasn't allowed to spawn a thread anymore.\n\nI think I simply got mislead by Tensorflow's `stderr` output. For the future, it would be great if there was an easier way to see when exactly Tensorflow crashes - that way I would have seen it had nothing to do with CUDA.\n\n@vrv, thank you so much for your really fast replies!\n", "Hi \r\nI have exactly the same issue, I am training resNet50 and always after 3 epochs,i get this error\r\n i use ubuntu 16.04,tensorflow 1.3,and gtx 1080 gpu on a shared server with 3 gpus \r\nand also i have used new dataset tensorflow api,i suspect something is write aboute feeding data to network \r\nthis is my training code :+1: \r\n[github.txt](https://github.com/tensorflow/tensorflow/files/1503372/github.txt)\r\n"]}, {"number": 1657, "title": "Upstream changes from internal", "body": "", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please.\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1656, "title": "any way to build train and save pd file on Android?", "body": "It seems no API of C++ to train and save graph,is there any way to run python API of Tensroflow on Android?\n", "comments": ["Is the C++ API described here [1] not sufficient to you for some reason?\n\n[1] https://www.tensorflow.org/versions/r0.7/api_docs/cc/index.html\n", "This kind of question probably belongs on StackOverflow, unless it is a concrete feature request.\n"]}, {"number": 1655, "title": "How to search similiar images?", "body": "This is not an issue, but i don't know where to ask this. I just get started with tensor flow and it's interesting to me. I know tensorflow will help classifying images, but in my case, i need to search similiar images. I read that google using tensorflow for search image in photos app. So how to make tensorflow return similiar image?\nThank you very much\n", "comments": ["https://github.com/AKSHAYUBHAT/VisualSearchServer might be a good example to look at. (StackOverflow would be a better location for this question, in the future).\n"]}, {"number": 1654, "title": "replace libpng from google source to github source", "body": "since google source will be blocked for people behind the wall, we could use the official release of libpng in github\n", "comments": ["Can one of the admins verify this patch?\n", "@girving: glennrp maintains libpng so this seems like a trustworthy github dependency -- what do you think?\n", "Seems fine to me.  Note that the only risk of attack occurs when we change the hash.\n", "Jenkins, test this please.\n"]}, {"number": 1653, "title": "Anyone implemented a Deconvolutional layer combined the Keras and Tensorflow?", "body": "I am a little confused by the output shape in conv_transpose( ) function. How to calculate it?\n\n``` python\nclass Convolution2D_Transpose(Layer):\n    input_ndim = 4\n\n    def __init__(self, nb_filter, nb_row, nb_col,\n                 init='glorot_uniform', activation='linear', weights=None,\n                 border_mode='valid', subsample=(1, 1),deconv_shape = (),\n                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                 W_constraint=None, b_constraint=None, **kwargs):\n\n        if border_mode not in {'valid', 'same'}:\n            raise Exception('Invalid border mode for Convolution2D:', border_mode)\n        self.nb_filter = nb_filter\n        self.nb_row = nb_row\n        self.nb_col = nb_col\n        self.deconv_shape = deconv_shape\n        self.init = initializations.get(init)\n        self.activation = activations.get(activation)\n        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n        self.border_mode = border_mode\n        self.subsample = tuple(subsample)\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.constraints = [self.W_constraint, self.b_constraint]\n\n        self.initial_weights = weights\n        self.input = K.placeholder(ndim=4)\n        super(Convolution2D_Transpose, self).__init__(**kwargs)\n\n    def build(self):\n        stack_size = self.input_shape[1]\n        #self.W_shape = (self.nb_filter, stack_size, self.nb_row, self.nb_col)\n        self.W_shape = ( self.nb_col, self.nb_row, stack_size, self.nb_filter)\n        print self.W_shape\n        self.W = self.init(self.W_shape)\n        self.b = K.zeros((self.nb_filter,))\n        self.trainable_weights = [self.W, self.b]\n        self.regularizers = []\n\n        if self.W_regularizer:\n            self.W_regularizer.set_param(self.W)\n            self.regularizers.append(self.W_regularizer)\n\n        if self.b_regularizer:\n            self.b_regularizer.set_param(self.b)\n            self.regularizers.append(self.b_regularizer)\n\n        if self.activity_regularizer:\n            self.activity_regularizer.set_layer(self)\n            self.regularizers.append(self.activity_regularizer)\n\n        if self.initial_weights is not None:\n            self.set_weights(self.initial_weights)\n            del self.initial_weights\n\n    @property\n    def output_shape(self):\n        input_shape = self.input_shape\n        rows = input_shape[2]\n        cols = input_shape[3]\n        self.deconv_shape = (self.nb_filter, input_shape[0], rows+2, cols+2)\n        return self.deconv_shape\n\n    def get_output(self, train=False):\n        X = self.get_input(train)\n        X = K.permute_dimensions(X,(0,2,3,1))\n        #batch_size = tf.shape(X)[0]\n        #deconv_shape = tf.pack([batch_size, 40, 40, 32])\n        print 'deconv_shape: {0}'.format(self.deconv_shape)\n        print 'value: {0}'.format(X)\n        print 'W shape: {0}'.format(K.eval(self.W).shape)\n        conv_out = tf.nn.conv2d_transpose(X, self.W, strides=self.subsample,\n                            padding='VALID',\n                            output_shape=self.deconv_shape)\n\n        output = conv_out + K.reshape(self.b, (1, 1, 1,self.nb_filter))\n        return output\n\n    def get_config(self):\n        config = {'name': self.__class__.__name__,\n                  'nb_filter': self.nb_filter,\n                  'nb_row': self.nb_row,\n                  'nb_col': self.nb_col,\n                  'init': self.init.__name__,\n                  'activation': self.activation.__name__,\n                  'border_mode': self.border_mode,\n                  'subsample': self.subsample,\n                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None}\n        base_config = super(Convolution2D_Transpose, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n```\n", "comments": ["@fchollet, can you please take this?\n", "Closing for now, since this is not a bug or feature request. If you haven't resolved this question, please re-ask on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) \n"]}, {"number": 1652, "title": "RNN Cell: different device placement for variables and ops", "body": "I want to use RNN cell on multiple GPUs. For maximum performance, I want to place RNN cell variables on CPU and operations (matmul, tanh, etc) on GPU, just like in cifar10 multi gpu example. However, the current implementation of RNN cell seems to only allow same device placement for both variables and ops. Could you enable different device placement?\n", "comments": ["The existing implementation focuses equally on being readable/understandable/hackable, and being performant. Please see the documentation [here](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph.device) for how to place variables/operations on specific devices. Closing this issue as the functionality to do what you want is already present in TensorFlow.\n", "Thanks, I just want to confirm one thing: using the raw RNNCell as it is, I cannot place vars and ops in different devices, is this correct? I understand that I can easily make changes to it myself to give different placement.\n", "That's correct.\n", "@seominjoon Hi, How did you seperate vars and ops in different devices for an RNN cell? I know changing source code can implement it. Is there a better way?", "\r\nI am also having this issue where I want to place RNN vars on CPU but execute the cell ops on GPU. having to hack around source code is subprime.  it would be nice if vars and ops had a kwarg for soft device placement. then we could do:\r\n\r\nto create a cell\r\n`cell = tf.contrib.rnn.BasicRNNCell(celldim,device='/cpu:')`\r\n\r\nto run the cell ops\r\n`output,state = cell(x_batch[:,time_step,:], cell_state,device='/gpu:0')`\r\n\r\n\r\n"]}, {"number": 1651, "title": "Support for inter-example-dependent labels?", "body": "This is likely the wrong place for this, but I've run into a dead end on other websites.\n\nI have a somewhat unusual learning task: the \"labels\" for my examples (i.e., images) are actually relations between specific images. For instance, suppose each batch consists of 2*n images, where the pair (i, j) has label 1 while the pair (j, i) has label -1. Suppose most (i.e., the vast majority) pairs possible from the set of images lack any label, and as such random sampling will only rarely yield even a single relation to train on. Therefore, in this example, each batch consists of a set 2N images, N pairs of which are known to have some relation.\n\nTensorFlow is so flexible it's easy to design a net that can compute the appropriate loss function. However, creating the input is problematic,requiring many queue objects (why do readers only accept queues in TensorFlow instead of outputs from previous nodes?!). Assuming that I have some set of files which are serialized examples, each a list of the form {file1: <...>, file2: <...>, label: <...>}, is there a simple way to serve them to the net as a tensor of shape 2N \\* W \\* H \\* C (with image i and image i + 1 belonging to a single relation, where i is (0, 2, 4, 6, ...) i.e., [relation_n_image_1, relation_n_image_2, relation_m_image_1, ...])? My current creation, individual components of which work but works as a whole only in theory, employs 5 queue objects of various kinds and does not shuffle perfectly.\n\nWill more flexible readers be implemented in the future, or is there something I'm missing?\n", "comments": ["Closing issue; all the data preprocessing will be done with python, and we will leverage the fact that the enqueue op blocks while the queue is full.\n"]}, {"number": 1650, "title": "fix trailing whitespace in edge.ts", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 1649, "title": "Fix building android_tensorflow_lib.", "body": "This commit fixes a build time failure of the Android TF library that is\nmissing some dependencies since they have been made more fine-grained.\nFor reference, the relevant errors (one per required dep) look something\nlike:\n\n```\nERROR: tensorflow/tensorflow/core/BUILD:663:1: Couldn't build file tensorflow/core/_objs/android_tensorflow_lib/tensorflow/core/kernels/softsign_op.o: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/softsign_op.cc':\n\nERROR: tensorflow/tensorflow/core/BUILD:663:1: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/maxpooling_op.cc':\n\nERROR: tensorflow/tensorflow/core/BUILD:663:1: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/ctc_decoder_ops.cc':\n\nERROR: tensorflow/tensorflow/core/BUILD:663:1: undeclared inclusion(s) in rule '//tensorflow/core:android_tensorflow_lib':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/avgpooling_op.cc':\n```\n", "comments": ["Can one of the admins verify this patch?\n", "@andrewharp since he's been dutifully maintaining this target\n\n@damienmg because I don't understand why our Android test target is fine but according to Jenkins but users still run into this problem :)\n", "I'd add a patch to make this show up in your default CI builds but I'm not sure what your build steps are. tools/ci_build doesn't seem to be the right place to be looking.\n", "@darrengarvey Hi, are you building tensorflow/core:tensorflow_android_lib directly (as opposed to //tensorflow/examples/android:tensorflow_demo)? If so, you need to tell Bazel to actually build for Android, e.g. with the flag --fat_apk_cpu=armeabi-v7a\n\n@vrv There's a select in android_tensorflow_lib_lite that nullifies attempts to build for non-Android, but not for android_tensorflow_lib. This is probably what's causing the misleading errors people are seeing. Jenkins always builds in an Android configuration so it's not catching this. I'll send a CL to fix this.\n", "@darrengarvey You'll need a couple more flags in addition to fat_apk_cpu if you want to build the library by itself. Here's what worked for me:\n`bazel build -c opt tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain`\n", "Note that if you are building this for an app you'll probably want to wrap the cc_library in a cc_binary (see tensorflow/examples/android/BUILD for an example) so that unused code can be stripped out. Otherwise you'll end up with over 200mb of unnecessary size. Just make sure you export the symbols you want to reference so they don't get stripped as well.\n", "@andrewharp - Thanks for the tips. I can build it using the command line above.\n\nI replicated the switch in `srcs` from `android_tensorflow_lib_lite` and that also prevented the errors I was seeing while building correctly for android when I asked for that. I can raise a PR for that if you want / haven't already?\n\nWhile this change makes it better, it might be more intuitive for users (like me) if this target failed to build with a message saying it should be built with android tools explicitly. Does it makes sense to build `android_tensorflow_lib` for anything except an android target? Then again I'm only new to bazel so it may well be just right as it is given the trade-off between build system complexity and user-friendliness.\n\nBtw. I didn't have a problem building an app for Android using [the docs](http://bazel.io/docs/tutorial/android-app.html) and the `android/BUILD` example. I came across this error when trying to understand what steps bazel was doing internally in the `android_binary` command, just for curiosity's sake really.\n\nI'm happy to close the PR if we can replace it with another as suggested by @andrewharp.\n", "I've submitted an internal patch that will nullify the src for android_tensorflow_lib in the same way as android_tensorflow_lib_lite. I agree that an informative error message might be ideal, but I'm worried about erroneous failures in automated builds and tests (not everything we run TF through seems to respect manual and notap at this point). As a compromise I added a comment on the targets specifying the targets must be built for Android, and how to do so.\n", "Sounds good to me. Thanks for the help @andrewharp and @vrv.\n"]}, {"number": 1648, "title": "Fix wrong parenthesis.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for the fix!  Would it be possible to introduce a test that exercises this codepath, so we can prevent it from regressing?\n", "@tensorflow-jenkins: test this please\n\n(well, add a test later if you can, this is better than nothing)\n"]}, {"number": 1647, "title": "Upstream changes from 2016/03/25", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins: test this please.\n", "@tensorflow-jenkins: test this please\n", "Hey, the links in the README.md for the distributed_runtime are now broken.\n\nThey link out to https://github.com/tensorflow/tensorflow/tree/r0.7/tensorflow/g3doc/how_tos/distributed/index.md and https://www.tensorflow.org/versions/r0.7/how_tos/distributed/index.html. The \"distributed\" directory doesn't exist.\n"]}, {"number": 1646, "title": "Get tensorflow/core link down to 2.2G", "body": "Most tests now depend on only the kernels they desperately need.\n", "comments": ["Jenkins, test this please.\n"]}, {"number": 1645, "title": "Error on SummaryWriter creation", "body": "I'm loading a graph from a file and I'd like to see what the topology is. As there doesn't seem to be a way to do this in python (the best I can do is guess the tensor names and use `graph.get_tensor_by_name`), I'm trying to produce a Summary which I can then view with TensorBoard. Unfortunately, instantiating the SummaryWriter object gives me an error:\n\n```\nAttributeError                            Traceback (most recent call last)\n/home/vlad/Repos/vision/Hand/simple_imagenet.py in <module>()\n      9 sess = tf.Session()\n     10 \n---> 11 writer = tf.train.SummaryWriter('./', sess.graph_def)\n     12 \n     13 #pool = sess.graph.get_tensor_by_name('pool_3:0')\n\n/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/summary_io.py in __init__(self, logdir, graph_def, max_queue, flush_secs)\n     95     if not gfile.IsDirectory(self._logdir):\n     96       gfile.MakeDirs(self._logdir)\n---> 97     self._event_queue = six.moves.queue.Queue(max_queue)\n     98     self._ev_writer = pywrap_tensorflow.EventsWriter(\n     99         compat.as_bytes(os.path.join(self._logdir, \"events\")))\n\nAttributeError: 'module' object has no attribute 'Queue'\n```\n\nI'm using the pip3 install of tensorflow 7.1 on Ubuntu 15.10.\n", "comments": ["This sounds like a version error with the `six` library: can you please run the following code and reply with the result?\n\n``` python\nimport six\nprint(six.__version__)\n```\n", "My `six` version is `1.9.0`.\n", "Can you also try the following?\n\n``` python\nprint(dir(six.moves.queue))\nprint(six.moves.queue.__file__)\n```\n\nI have tried `six` versions `1.4.1`, `1.5.2` and `1.10.0` (in Python 2.7 and 3.4) and these all have `six.moves.queue.Queue`.\n", "It looks like I've identified the problem:\n\n```\n> print(six.moves.queue.__file__)\n/home/vlad/Repos/vision/Hand/queue.py\n```\n\nFor some reason my local `queue.py` file is being picked up! Any idea why this is the case?\n", "I believe this is related to Python's confusing import semantics, which make problems likely if a local module shadows the name of a standard library one. The easiest fix is probably to rename your local `queue.py`, or move it into a submodule (I can't tell from the path if your `queue` module would be imported as `queue` or `Hand.queue` in your program; if the former then I think you'll need to move it).\n", "(Closing this since the issue seems to be related to local configuration.)\n", "I get the same error with six v1.10.0 and Tensorflow v0.12.0rc1 .", "Same Here we are six v1.10"]}, {"number": 1644, "title": "Support AWS cfncluster definition in distributed_runtime", "body": "", "comments": ["This seems like a reasonable feature to add in `contrib/...`.\n", "Automatically closing due to lack of recent activity. Thanks!"]}, {"number": 1643, "title": "Improve core/kernels/BUILD so that no test is bigger than 25M", "body": "Tests now depend on a much finer grain set of kernels.\n", "comments": ["Jenkins, test this please.\n", "Jenkins, test this please.\n"]}, {"number": 1642, "title": "Poor performance of Android camera demo on arm64", "body": "I've been running the Android camera demo on both a Nexus 5 (armeabi-v7a) and Nexus 5X (arm64-v8a).  The older Nexus 5 is able to perform a single recognition in about ~350ms.  The Nexus 5X however performs at around ~750ms.  I wouldn't expect a newer device to perform worse (especially not that much worse) than the older one.\n\nI've tried running the resulting apk from \n`bazel build //tensorflow/examples/android:tensorflow_demo` \nas well as\n`bazel build //tensorflow/examples/android:tensorflow_demo --fat_apk_cpu=arm64-v8a`\n\nThere's no change in performance for either option on the Nexus 5X.\n\nI know that the `-mfpu=neon` flag seems to be a problem when building for arm64-v8a so I've had to remove it for those builds.  I suspect the performance issue might be related to the performance benefits of neon.  Is there a different flag that needs to be specified to enable neon support for the arm64-v8a architecture?  Or am I completely off base on the performance expectations on Nexus 5X?\n\nThis is related to #1019 \n", "comments": ["I've done some tests of this, and it seems like the Nexus 5X is capable of Nexus 5-comparable performance when the 2 A57 cores help out with the processing. Which, unfortunately, is rarely. The majority of the time the bulk of the processing is done by the slower (but more energy-efficient) A53 cores.\n\nYou can see this behavior if you run a systrace while the demo is running and look for the \"Recognize\" calls. e.g. `python $SDK_ROOT/platform-tools/systrace/systrace.py --time=5 -o nexus5x.html sched gfx view wm --app=org.tensorflow.demo`\n\nOver 25 runs I saw a min demo inference time of 285ms, a max of 642ms, with an average of 491ms.\n\nI also looked at the per-operation performance (by compiling with -DLOG_DETAILED_STATS), and found the worst offenders for lower performance by far were the convolutions. For example,                    conv2d2_pre_relu/conv took an average of 40ms on the Nexus 5, but a whopping 106ms on the Nexus 5x.\n", "@andrewharp Does that mean there isn't much that can be done to improve the performance on the Nexus 5X?  I haven't run it on the 6P or any other 64-bit device to be able to compare to Nexus 5.\n", "If you can figure out how to force all the cores to activate it should be possible to increase the speed. Though I expect thermal throttling will kick in fairly quickly and it will be impossible to maintain the performance over a sustained duration.\n", "@maciekcc may have more insight into this.\n", "I don't have a Nexus 5x on hand, but I have a suspicion that tensorflow tries to use all the 6 cores and spawns 6 threads for computation. Because of the non-homogeneous setup in 5X this might lead to thrashing. Could you please try using just 1-4 cores and checking how this scales. You can do that in tensorflow_jni.cc set intra_op_parallelism_threads explicitly to 1, 2, 3 or 4 in the ConfigProto.\n\nIf this is the case, it means that our current multithreading scheme doesn't handle on demand non-homogeneous cores well.\n\nIIRC neon is turned on by default for Arm64 builds.\n", "average recognition time without setting explicit intra_op_parallelism_threads: **743ms**\n\nResults of testing `config.set_intra_op_parallelism_threads(size)`:\n`size = 1` average recognition time: **1919ms**\n`size = 2` average recognition time: **1064ms**\n`size = 4` average recognition time: **742 ms**\n`size = 6` average recognition time: **704 ms**\n\nSeems like it's running with 4 when not setting thread count explicitly.  \n", "This seems to be the culprit: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/posix/port.cc#L43\n", "When I ran a standalone benchmark of the same model (coming soon), both devices improved their performance considerably.\n5 standalone benchmark: 238ms\n5x standalone benchmark: 315ms\n\nFor reference, here's the performance I saw in my last run of the standard TF Android camera demo:\n5: 330ms\n5x: 565ms\n\nThe standalone runs perform much better than the app version in both cases, but considerably moreso on the 5x. This implies the overhead of running the user-facing Android app differentially affects the 5x, in addition to or magnifying the speed discrepancy caused by the CPU setup. \n", "I just tried setting kDefaultCores to 6 on the 5x, and got 501ms/run in the TF demo app. Surprisingly there was no improvement at all in the standalone benchmark, as it ran in 316ms/run.\n", "Further testing using the standalone benchmark tool reveals that adding a delay between inference passes of 1 seconds gives an per-inference average of 427ms over 50 runs.\n\nThis is compared to a 321ms result on an otherwise identical test with the inter-inference delay set to 0.\n\nI'd speculate that in the Android camera demo, the delay between receiving frames to process is significant enough that the processor scales down, and needs to ramp back up once inference starts again. A busy-loop in the inference thread between frames might help with that behavior.\n", "Not sure it's related, but we had a similar difference in performance between standalone benchmark binary and test app on the Nexus 5X for our neural networks, and we tracked it down to the compiler inlining some functions in the binary and not in the .so library. We fixed it by adding linker-time optimization (LD_FLAGS += -flto -O3), that fixed the issue and also reduced the library size.\n", "@andrewharp @petewarden What's the status of this? \n", "The standalone benchmark target tensorflow/tools/benchmark:benchmark_model has been added which will allow users to repeat their own experiments.\n\nWe're still evaluating compilation flags to get the best performance out of mobile builds. As far as the 5x in particular, I think we have viable explanations for the performance difference (asymmetric cores, etc) but no simple solution to improve it right now.\n", "@andrewharp Should I leave it assigned to you or Pete, or mark it contributions welcome if there is something specific we could ask others to contribute?  We're trying to avoid leaving nonspecific bugs open. \n", "I don't think we have any specific plans at the moment to improve 5X (relative) performance, so this should probably be closed. Happy to test out any contributions, though.\n", "The native OCR code that I work with runs painfully slow without compiler optimizations on arm64-v8a\r\n\r\nRecognizing a default test image was taking ~19 sec on the arm64-v8a Nexus 5x, but only ~7 sec on the considerably weaker armeabi-v7a Moto E2.\r\n\r\nAs @bissacc0 said, be sure to use:\r\n\r\nAPP_OPTIM := release\r\nAPP_CFLAGS += -O3\r\nLOCAL_LDFLAGS += -flto\r\n\r\nIn your make files. Using these flags reduced recognition time from 19 s to 4 s (!!!) on the Nexus 5x\r\n", "@jf-rce   whice file to add the build configure ???   \r\nAPP_OPTIM := release\r\nAPP_CFLAGS += -O3\r\nLOCAL_LDFLAGS += -flto", "Hi there - I'm trying to test the demo app as well. One thing I noticed is that only 4 cores of my 8-core phone were used, even if I changed the number of threads by ProtoConfig. I also modified the function NumSchedulableCPUs in port.cc to set kDefaultCores to 8.\r\nAny advice?", "@jf-rce Currently our Makefile only targets armeabi-v7a, so it's not surprising there could be improvements there, but do you know if these flags have any affect on the Bazel build as well?\r\n\r\n@zhangpao17 You should add them in [tensorflow/contrib/makefile/Makefile] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/Makefile#L213) under the Android section.\r\n\r\n@kindloaf #8653 may help with this.", "Hi @andrewharp ,\r\nThanks for the reply. I modified the code in port.cc, and logged the return value of NumSchedulableCPUs, which is 8. Still, my phone is using only 4 cores.\r\nAre there other restrictions on tensorflow framework? Or, maybe 4 cores are the most that the framework can use - the bottleneck is not on computation?"]}, {"number": 1641, "title": "gradient calc description change it text cell", "body": "I have found the previous explanation of manual gradient calculation a\nlittle confusing because we compute a vector of gradients with the size\nof number of neurons (with bias) and in the explanation it looks like we\ncompute a scalar.\n\nPropose more detailed variant if community finds it's better. Excuse me\nif  it is convenient to write x_i even for bias unit, just discard this\nthen.\n\nAttach old and new variant of just last notebook text cell saved to pdf (cannot attach ipynb here). The differences in the description of second line of code.\n\n[Old.pdf](https://github.com/tensorflow/tensorflow/files/189573/Old.pdf)\n[New.pdf](https://github.com/tensorflow/tensorflow/files/189574/New.pdf)\n", "comments": ["Can one of the admins verify this patch?\n", "So that I understand, how does the old description imply the gradient is a scalar?\n", "Jenkins, test this please\n", "@tensorflow-jenkins: test this please\n", "@girving: I'm sorry, not a scalar, but a vector of size of number of neurons (without bias unit), and in the case of one neuron - a scalar. It was says that partial derivatives of loss function are _SUM( y_hat - y ) \\* x_i_ . And then I look at loss function and see only one _x_: 1/2_SUM( w2_x + w1 - y)^2. So then the _SUM( y_hat - y ) \\* x_i_ becomes _SUM( y_hat - y ) \\* x_ which looks like one number for me. Then I look at the code and see that the result is two numbers as it should be. But there is no partial derivative with respect to w1 in description\n\nI understand that maybe it is convenient to just omit this dedicated equation for the bias unit thinking of it like it is an _x_whichisalwaysone_ and thus _SUM( y_hat - y ) \\* x_i_ will return two values here (with bias). But for a newbie like me it was a little confusing at start so I humbly propose more detailed description. If you found it bad, pls just discard it.\n", "The line `x_with_bias = np.array([(1., a) for a in x]).astype(np.float32)` makes the simpler formula correct. Your new description is inconsistent with the simplified formulas above.  If you want to clarify that `x` contains a 1 entry corresponding to the bias, just add a parenthetical comment like \"(Note that the first entry of `x` is 1 corresponding to the bias)`.\n", "Yes,I see your logic now! But in this python code string the name of variable is `x_with_bias` which means \"x with something else\", or \"not only x.\" And, according to that string, x does not contains a 1 entry, it's `x_with_bias`. It's all just a matter of taste of course and not that important :)\n", "Very true!  Let's use `x_with_bias` in the formula; that even avoids the need for an extra comment.\n"]}, {"number": 1640, "title": "Fix mktemp issue on Mac", "body": "PR #1605 caused silent failure related to mktemp on Mac. This PR fixes that. \n", "comments": []}, {"number": 1639, "title": "Typo descient to descent", "body": "Typo in text cell\n", "comments": ["Can one of the admins verify this patch?\n", "Can you squash these commits to get rid of the merge commit?  Then I'll test and merge.  Thank you for the fix!\n", "@girving: merge commits don't show up in our history, so you can ignore those.  Only real commits show up in the final tree.\n\n@tensorflow-jenkins: test this please\n"]}, {"number": 1638, "title": "mu to learning_rate", "body": "New var `mu` defined as _Learning rate_:\n`mu = 0.002`\n but later in the same code cell var `learning_rate` is used instead:\n`update_weights = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)`\n, which was defined in previous cell:\n\nSuggest to rename because cell uses var which is not defined in it, but\ndefines var which is not used.\n\nSuggest to rename `mu` to `learning_rate` (not `learning_rate` to `mu` later in\ncode) because this code cell is supposed to be like previous but with\ncomments. And in previous cell the name `learning_rate` was used.\n\nCommit also change var name in comment to reflect changes in var\ndefinition.\n", "comments": ["Can one of the admins verify this patch?\n", "Looks like you left a couple of `mu` references in the file.  Can you get rid of those too?  Also, please rebase and squash to remove the merge commit from the pull request.\n", "@girving: Yes, i have left them in the last code cell. Thought that `mu`was written there intentionally to distinguish manual calculations of weights update. And later, at the bottom text cell there are references to `mu` too. And I understand that in text and formulas `mu` looks more compact then `learning_rate.`\n\nIf you think that it is better to replace `mu` with `learning_rate` there too, I will do it.\n", "Yeah, we should be uniform throughout the page.  `learning_rate` is more verbose but obviously less confusing as a name.\n", "Okay, done.\n", "Thanks!  Jenkins, test this please.\n", "I should have double checked the merge commit - it was bogus.  You can send this PR in again with the right content!\n", "I'm very sorry, somehow some waste lines leaked in the file while I was trying to update my brach from master. Made a new PR: #1706. Hope it will be ok now.\n"]}, {"number": 1637, "title": "Pushing changes from internal for March 24", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1636, "title": "dynamic_rnn got  an error", "body": "I want to test dynamic_rnn based on ptb_word_lm.py.\n\nwhen I use rnn,it's ok!\n\n```\ninputs = [tf.squeeze(input_, [1])\n          for input_ in tf.split(1, num_steps, inputs)]\noutputs, state = rnn.rnn(cell, inputs, initial_state=self._initial_state)\n```\n\nBut,when I use dynamic_rnn,like this:\n\n```\noutputs,state=rnn.dynamic_rnn(cell,inputs,initial_state=self._initial_state)\n```\n\nIt's got an error:\n\n```\nTraceback (most recent call last):\n  File \"ptb_word_lm.py\", line 391, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"ptb_word_lm.py\", line 368, in main\n    m = PTBModel(is_training=True, config=config)\n  File \"ptb_word_lm.py\", line 207, in __init__\n    self._train_op=tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 190, in minimize\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 241, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py\", line 137, in _TensorArrayPackGrad\n    grad_source = _GetGradSource(grad)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_grad.py\", line 62, in _GetGradSource\n    % op_or_tensor.name)\nValueError: Expected op/tensor name to start with gradients, got: model/gradients/model/RNN/transpose_grad/transpose:0\n```\n", "comments": ["It's a bug in _GetGradSource.  we have a fix incoming, probably a few days till it shows on github.\n", "Can you verify this is fixed for you at HEAD?\n", "it's OK!\n"]}, {"number": 1635, "title": "replace xrange with range for python3 compatibility", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks!  Jenkins, test this please.\n", "The failure looks like a flaky test on our end.  Investigating.\n", "Jenkins, test this please.\n", "@keveman: Do you know might be causing this protobuf pip test failure? \n", "@vrv apparently knows the issue.\n", "@tensorflow-jenkins: test this please\n", "@tensorflow-jenkins: test this please  (PLEASE)\n"]}, {"number": 1634, "title": "KeyboardInterrupt make aws g2.2xlarge sys  no response 'cuda_gpu_executor.cc:624] Deallocating stream'", "body": "### Environment info\n\nOperating System:\naws \nhttps://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LaunchInstanceWizard:ami=ami-e191b38b\n\nKeyboardInterrupt\nE tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:624] Deallocating stream with pending work\n^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n\nhttp://ramhiser.com/2016/01/05/installing-tensorflow-on-an-aws-ec2-instance-with-gpu-support/  \nhe installed\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["/var/log/\\*   files no info about sys down\n", "This isn't enough information.  From what you've written so far, maybe you typed ^C to kill a process?\n", "Automatically closing because there was no response. Please reopen if it is still an issue.\n"]}, {"number": 1633, "title": "TensorBoard Graph: Put edge label in the middle of edge only if it is thick enough", "body": "Put edge label in the middle of edge only if it is thick enough.\n\n`2.5` looks like the correct stroke width threshold.  \n\nSee example from CIFAR\n\n<img width=\"551\" alt=\"tf-graph_demo\" src=\"https://cloud.githubusercontent.com/assets/111269/14037174/1160d4a0-f1fe-11e5-8378-c8fba63d4306.png\">\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks! Just 1) a tiny comment and 2) make sure your branch is up-to-date with master and you are good to go.\n", "@dsmilkov  Fixed & Rebased.  :) \n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1632, "title": "how to set subtensor on Tensor objects", "body": "As a naive example, in numpy: \n\n```\nx = numpy.zeros((2, 3, 4))\ny = numpy.ones((2, 3, 4))\nx[1:] = y[:-1]\narray([[[ 0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.]],\n\n       [[ 1.,  1.,  1.,  1.],\n        [ 1.,  1.,  1.,  1.],\n        [ 1.,  1.,  1.,  1.]]])\n```\n\nHow to achieve this in tensorflow? I've looked into tf.scatter_update(), but it only works on Variable objects, not on Tensors. \n", "comments": ["I'd suggest `tf.pack` or `tf.concat`.  If you want more details, please ask questions like this on stackoverflow; issues are for bugs in tensorflow.\n"]}, {"number": 1631, "title": "Initial iOS Support and Example", "body": "Compiling this will require the top-of-tree version of Bazel, until it makes it into the released version in about a week.\n\nTo compile the iOS example, run the following command:\n`bazel build -c opt -s //tensorflow/examples/ios:ios-app-binary-cc --ios_sdk_version=9.2 --ios_cpu=\"arm64\"`\n\nYou will need to put your mobile provisioning certificate in tools/objc/default_provisioning_profile.mobileprovision, and the Inception v3 model file and labels in a data directory inside tensorflow/examples/ios/data, e.g.\n`ls tensorflow/examples/ios/data/`\n`grace_hopper.jpg  imagenet_comp_graph_label_strings.txt tensorflow_inception_graph.pb`\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "@petewarden The failures still show bazel 0.2, are the new docker files not pushed out yet? Or did something go wrong with the bazel update? Or is this just a too old test?\n", "@tensorflow-jenkins: test this please\n", "This was a recent test, I need to debug what's going wrong on the Jenkins server. The install script does appear to be updated:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_bazel.sh\n", "@tensorflow-jenkins: test this please\n", "I just checked, and at least on the Mac, bazel 0.2.1 is definitely installed\n", "You have the same error in linux build with 0.2.1 as well as on mac where is 0.2.0.\n\n@martinwicke you have checked only mac0-slave, right?\n\n@caisq can you upgrade bazel on mac1-slave, please?\n", "Oh, right, I have only updated mac0-slave to 0.2.1.\n", "I've installed the 0.2.1 binaries locally, and it seems like the fix we need didn't make it in, despite being in top-of-tree ahead of time! :( I'm following up with the Bazel team to find out what happened, and what we can do to fix this.\n", "any updates on this patch? \n", "We're currently blocked because the top-of-tree protobuf is broken, and we need that for iOS support. I'm working on fixing that right now.\n", "I've been attempting to use this patch locally to see if I could manually work around the issues in protobuf. I've run into issues with Xcode 7 where object files built using the bazel cc_library rule can't be linked into an iOS simulator application due to metadata that's now being written into the object files. Going back to the bazel iOS example (http://bazel.io/docs/tutorial/ios-app.html) and adding some C++ code, I found that I was only able to get it to build properly if the C++ was added using an objc_library rule, not a cc_library rule.\n\nI note that this patch currently only defines a cc_library rule for the tensorflow code. Does it also need to define an objc_library rule for the benefit of the simulator?\n\nEdit: Explanation of Xcode 7's new object file metadata: https://karp.id.au/a/2015/09/15/xcode-7s-new-linker-rules/\n", "@petewarden is this PR still active or can it be closed?\n"]}]