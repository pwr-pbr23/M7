[{"number": 2533, "title": "The docker image of gcr.io/tensorflow/tensorflow-full does not exist", "body": "We follow the [official docs](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/docker) and try to pull this image but it doesn't exist.\n\n```\nroot@do:~# docker pull gcr.io/tensorflow/tensorflow-full\nUsing default tag: latest\nPulling repository gcr.io/tensorflow/tensorflow-full\nTag latest not found in repository gcr.io/tensorflow/tensorflow-full\n```\n\nAfter searching in StackOverflow and we found `b.gcr.io/tensorflow/tensorflow-full` works.\n### Environment info\n\nOperating System: Ubuntu 16.10\n### Steps to reproduce\n1. docker pull gcr.io/tensorflow/tensorflow-full(not work)\n### What have you tried?\n1. docker pull b.gcr.io/tensorflow/tensorflow-full(work)\n", "comments": ["(Which link should we use?  gcr.io/tensorflow/tensorflow or b.gcr.io/tensorflow/tensorflow-full ?\n", "Which version should I use, if I would like to practice with \"inception-v3\" model, as described in this tutorial?\ntutorial link:https://www.tensorflow.org/versions/r0.8/tutorials/image_recognition/index.html#image-recognition\n\nI pulled b.gcr.io/tensorflow/tensorflow-full, but couldn't find the source code \"classify_image.py\" under this directory \"tensorflow/models/image/imagenet\". In fact, there is no such a folder named \"imagenet\".\n", "I will update the documentation. It has to be very old. I don't think \"full\" was even in the original release 0.5.0.\n\nThere are basically two images:\n1) gcr.io/tensorflow/tensorflow - smaller but cpu only\n2) gcr.io/tensorflow/tensorflow:0.8.0-gpu - bigger but supports nvidia cuda\n\nIf you have nvidia gpu use the second otherwise the first.\n\nBTW: Do you really have ubuntu 16.10? :-)\n", "Thanks @jendap for fixing this. We use Ubuntu 16.10 for dev environment, not production \ud83d\ude03 \n"]}, {"number": 2532, "title": "Tensorflow Anaconda Import Module Error", "body": "1. I installed Anaconda Environment Installation with Python 3.5 for 64-bit on Ubuntu 16.04 LTS\n2. I created the conda environment for tensorflow in 3.4 and installed tensorflow 0.8.0 version for python 3. \n3. After activating tensorflow source, I typed in python and imported sklearn & skflow. But it gives me an import error. \n   When I install both sklearn & skflow using pip within activated tensorflow environment. It does work for that session only. After I deactivate and activate again, it starts giving an error again. \n", "comments": ["I solved this problem using \nconda install -c https://conda.anaconda.org/jjhelmus tensorflow\nBasically I installed tensorflow as conda package instead of creating a tensorflow environment. this way I did not had to change to root and didnt get any import errors. I can also use tensorflow on spyder. \n"]}, {"number": 2531, "title": "Running model failed: executing graph using C++ API", "body": "Hi,\n\nI trained a model using Python API, and saved a standalone GraphDef model using freeze_graph.\nAnd then I execute the graph in C++ enviornment\n\nHowever, after building the binary file using bazel, it's cannot running correctly:\n`E tensorflow/examples/facenet/main.cc:309] Running model failed: Invalid argument: Input 0 of node incept5b/in4_conv1x1_55/batch_norm/cond/ExponentialMovingAverage/AssignMovingAvg_1/Switch was passed float from incept5b/in4_conv1x1_55/batch_norm/cond/incept5b/in4_conv1x1_55/batch_norm/moments/moments_1/variance/ExponentialMovingAverage:0 incompatible with expected float_ref.`\n\nIs there any reason for this failure? Does anyone know how to fix this?\nMaybe should add some detailed explanation in C++ API documentation....\nThanks,\n\nThe structure of the graph (in python) can be simplified as: (it's used for face recognition).\n\n```\nimages_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, FLAGS.image_size, FLAGS.image_size, 3), name='input')\n\nphase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n\nembeddings = faceRecog.inference_nn4_max_pool_96(images_placeholder, phase_train=phase_train_placeholder)\n\n\n\ndef inference_nn4_max_pool_96(images, phase_train=True):\n\n  conv1 = _conv(images, 3, 64, 7, 7, 2, 2, 'SAME', 'conv1_7x7', phase_train=phase_train, use_batch_norm=True)\n  pool1 = _mpool(conv1,  3, 3, 2, 2, 'SAME')\n  conv2 = _conv(pool1,  64, 64, 1, 1, 1, 1, 'SAME', 'conv2_1x1', phase_train=phase_train, use_batch_norm=True)\n  conv3 = _conv(conv2,  64, 192, 3, 3, 1, 1, 'SAME', 'conv3_3x3', phase_train=phase_train, use_batch_norm=True)\n  pool3 = _mpool(conv3,  3, 3, 2, 2, 'SAME')\n\n  incept3a = _inception(pool3,    192, 1, 64, 96, 128, 16, 32, 3, 32, 1, 'max', 'incept3a', phase_train=phase_train, use_batch_norm=True)\n  incept3b = _inception(incept3a, 256, 1, 64, 96, 128, 32, 64, 3, 64, 1, 'max', 'incept3b', phase_train=phase_train, use_batch_norm=True)\n  incept3c = _inception(incept3b, 320, 2, 0, 128, 256, 32, 64, 3, 0, 2, 'max', 'incept3c', phase_train=phase_train, use_batch_norm=True)\n\n  incept4a = _inception(incept3c, 640, 1, 256, 96, 192, 32, 64, 3, 128, 1, 'max', 'incept4a', phase_train=phase_train, use_batch_norm=True)\n  incept4b = _inception(incept4a, 640, 1, 224, 112, 224, 32, 64, 3, 128, 1, 'max', 'incept4b', phase_train=phase_train, use_batch_norm=True)\n  incept4c = _inception(incept4b, 640, 1, 192, 128, 256, 32, 64, 3, 128, 1, 'max', 'incept4c', phase_train=phase_train, use_batch_norm=True)\n  incept4d = _inception(incept4c, 640, 1, 160, 144, 288, 32, 64, 3, 128, 1, 'max', 'incept4d', phase_train=phase_train, use_batch_norm=True)\n  incept4e = _inception(incept4d, 640, 2, 0, 160, 256, 64, 128, 3, 0, 2, 'max', 'incept4e', phase_train=phase_train, use_batch_norm=True)\n\n  incept5a = _inception(incept4e,    1024, 1, 384, 192, 384, 0, 0, 3, 128, 1, 'max', 'incept5a', phase_train=phase_train, use_batch_norm=True)\n  incept5b = _inception(incept5a, 896, 1, 384, 192, 384, 0, 0, 3, 128, 1, 'max', 'incept5b', phase_train=phase_train, use_batch_norm=True)\n  pool6 = _apool(incept5b,  3, 3, 1, 1, 'VALID')\n\n  resh1 = tf.reshape(pool6, [-1, 896])\n  affn1 = _affine(resh1, 896, 128)\n  if FLAGS.keep_probability<1.0:\n    affn1 = control_flow_ops.cond(phase_train,\n                                  lambda: tf.nn.dropout(affn1, FLAGS.keep_probability), lambda: affn1)\n  norm = tf.nn.l2_normalize(affn1, 1, 1e-10, name='embeddings')\n\n  return norm\n\n```\n\nAnd in C++, I edit the \"lable_image\" example in tensorflow, and here's the code:\n\n```\n#include <fstream>\n\n#include \"tensorflow/cc/ops/const_op.h\"\n#include \"tensorflow/cc/ops/image_ops.h\"\n#include \"tensorflow/cc/ops/standard_ops.h\"\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/graph/default_device.h\"\n#include \"tensorflow/core/graph/graph_def_builder.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/stringpiece.h\"\n#include \"tensorflow/core/lib/core/threadpool.h\"\n#include \"tensorflow/core/lib/io/path.h\"\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\n#include \"tensorflow/core/platform/init_main.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/public/session.h\"\n#include \"tensorflow/core/util/command_line_flags.h\"\n\n// These are all common classes it's handy to reference with no namespace.\nusing tensorflow::Flag;\nusing tensorflow::Tensor;\nusing tensorflow::Status;\nusing tensorflow::string;\nusing tensorflow::int32;\n\n// Given an image file name, read in the data, try to decode it as an image,\n// resize it to the requested size, and then scale the values as desired.\nStatus ReadTensorFromImageFile(string file_name, const int input_height,\n                               const int input_width, const float input_mean,\n                               const float input_std,\n                               std::vector<Tensor>* out_tensors) {\n  tensorflow::GraphDefBuilder b;\n  string input_name = \"file_reader\";\n  string output_name = \"normalized\";\n  tensorflow::Node* file_reader =\n      tensorflow::ops::ReadFile(tensorflow::ops::Const(file_name, b.opts()),\n                                b.opts().WithName(input_name));\n  // Now try to figure out what kind of file it is and decode it.\n  const int wanted_channels = 3;\n  tensorflow::Node* image_reader;\n  if (tensorflow::StringPiece(file_name).ends_with(\".png\")) {\n    image_reader = tensorflow::ops::DecodePng(\n        file_reader,\n        b.opts().WithAttr(\"channels\", wanted_channels).WithName(\"png_reader\"));\n  } else {\n    // Assume if it's not a PNG then it must be a JPEG.\n    image_reader = tensorflow::ops::DecodeJpeg(\n        file_reader,\n        b.opts().WithAttr(\"channels\", wanted_channels).WithName(\"jpeg_reader\"));\n  }\n  // Now cast the image data to float so we can do normal math on it.\n  tensorflow::Node* float_caster = tensorflow::ops::Cast(\n      image_reader, tensorflow::DT_FLOAT, b.opts().WithName(\"float_caster\"));\n  // The convention for image ops in TensorFlow is that all images are expected\n  // to be in batches, so that they're four-dimensional arrays with indices of\n  // [batch, height, width, channel]. Because we only have a single image, we\n  // have to add a batch dimension of 1 to the start with ExpandDims().\n  tensorflow::Node* dims_expander = tensorflow::ops::ExpandDims(\n      float_caster, tensorflow::ops::Const(0, b.opts()), b.opts());\n  // Bilinearly resize the image to fit the required dimensions.\n  tensorflow::Node* resized = tensorflow::ops::ResizeBilinear(\n      dims_expander, tensorflow::ops::Const({input_height, input_width},\n                                            b.opts().WithName(\"size\")),\n      b.opts());\n  // Subtract the mean and divide by the scale.\n  tensorflow::ops::Div(\n      tensorflow::ops::Sub(\n          resized, tensorflow::ops::Const({input_mean}, b.opts()), b.opts()),\n      tensorflow::ops::Const({input_std}, b.opts()),\n      b.opts().WithName(output_name));\n\n  // This runs the GraphDef network definition that we've just constructed, and\n  // returns the results in the output tensor.\n  tensorflow::GraphDef graph;\n  TF_RETURN_IF_ERROR(b.ToGraphDef(&graph));\n  std::unique_ptr<tensorflow::Session> session(\n      tensorflow::NewSession(tensorflow::SessionOptions()));\n  TF_RETURN_IF_ERROR(session->Create(graph));\n  TF_RETURN_IF_ERROR(session->Run({}, {output_name}, {}, out_tensors));\n  return Status::OK();\n}\n\n// Reads a model graph definition from disk, and creates a session object you\n// can use to run it.\nStatus LoadGraph(string graph_file_name,\n                 std::unique_ptr<tensorflow::Session>* session) {\n  tensorflow::GraphDef graph_def;\n  Status load_graph_status =\n      ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\n  if (!load_graph_status.ok()) {\n    return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\n                                        graph_file_name, \"'\");\n  }\n  session->reset(tensorflow::NewSession(tensorflow::SessionOptions()));\n  Status session_create_status = (*session)->Create(graph_def);\n  if (!session_create_status.ok()) {\n    return session_create_status;\n  }\n  return Status::OK();\n}\n\n\nint main(int argc, char* argv[]) {\n  // These are the command-line flags the program can understand.\n  // They define where the graph and input data is located, and what kind of\n  // input the model expects. If you train your own model, or use something\n  // other than GoogLeNet you'll need to update these.\n  string image = \"tensorflow/examples/facenet/data/img.png\";\n // string image = \"tensorflow/examples/label_image/data/grace_hopper.jpg\";\n  string graph =\n      \"tensorflow/examples/facenet/data/\"\n      \"FaceNet.pb\";\n  // not necessary for facenet\n  // string labels =\n  //     \"tensorflow/examples/label_image/data/\"\n  //     \"imagenet_comp_graph_label_strings.txt\";\n  int32 input_width = 96;\n  int32 input_height = 96;\n  int32 input_mean = 0;\n  int32 input_std = 1;\n  string input_layer = \"input\";\n  string output_layer = \"embeddings\";\n  bool self_test = false;\n  string root_dir = \"\";\n  const bool parse_result = tensorflow::ParseFlags(\n      &argc, argv, {Flag(\"image\", &image),                //\n                    Flag(\"graph\", &graph),                //\n                    // Flag(\"labels\", &labels),              //\n                    Flag(\"input_width\", &input_width),    //\n                    Flag(\"input_height\", &input_height),  //\n                    Flag(\"input_mean\", &input_mean),      //\n                    Flag(\"input_std\", &input_std),        //\n                    Flag(\"input_layer\", &input_layer),    //\n                    Flag(\"output_layer\", &output_layer),  //\n                    Flag(\"self_test\", &self_test),        //\n                    Flag(\"root_dir\", &root_dir)});\n  if (!parse_result) {\n    LOG(ERROR) << \"Error parsing command-line flags.\";\n    return -1;\n  }\n\n  // We need to call this to set up global state for TensorFlow.\n  tensorflow::port::InitMain(argv[0], &argc, &argv);\n  if (argc > 1) {\n    LOG(ERROR) << \"Unknown argument \" << argv[1];\n    return -1;\n  }\n\n  // First we load and initialize the model.\n  std::unique_ptr<tensorflow::Session> session;\n  string graph_path = tensorflow::io::JoinPath(root_dir, graph);\n  Status load_graph_status = LoadGraph(graph_path, &session);\n  if (!load_graph_status.ok()) {\n    LOG(ERROR) << load_graph_status;\n    return -1;\n  }\n\n  // Get the image from disk as a float array of numbers, resized and normalized\n  // to the specifications the main graph expects.\n  std::vector<Tensor> resized_tensors;\n  string image_path = tensorflow::io::JoinPath(root_dir, image);\n  Status read_tensor_status =\n      ReadTensorFromImageFile(image_path, input_height, input_width, input_mean,\n                              input_std, &resized_tensors);\n  if (!read_tensor_status.ok()) {\n    LOG(ERROR) << read_tensor_status;\n    return -1;\n  }\n  const Tensor& resized_tensor = resized_tensors[0];\n\n  // Actually run the image through the model.\n  std::vector<Tensor> outputs;\n  Status run_status = session->Run({{\"input\", resized_tensor}},\n                                    {\"embeddings\"}, {}, &outputs);\n                                   // {output_layer}, {}, &outputs);\n  if (!run_status.ok()) {\n    LOG(ERROR) << \"Running model failed: \" << run_status;\n    return -1;\n  }\n\n\n  return 0;\n}\n\n```\n\nThanks!\n", "comments": ["I'm not entirely sure -- it's possible there's a bug with freeze graph in not preserving the type\n", "@vrv thanks.\nSo, what method do you often use to save a graph def file?\nIs \"tf.train.write_graph\" good ?\n", "I don't have an update on this yet. From the node names, it sounds like there's something that's expecting to update a variable, and it's getting a constant?\n", "@petewarden \nYes, I save the graphdef using freez_graph. However, when making inference using the graphdef, it requires float_ref. The freez_graph seems just store the variables to variables, not really \"freeze\" the graph and change the variable values to constant...\n\nSo, I just use tf.train.write_graph to save a graphdef file.\n\n```\n    for v in tf.trainable_variables():\n        vc = tf.constant(v.eval())\n        tf.assign(v, vc, name=\"tf_weights\")\n    tf.train.write_graph(sess.graph_def, export_path, 'graph_def.pb', as_text=False)\n```\n\nIs this a good way to store graph for using in c++?\n\nThanks\n", "That looks like a reasonable approach for your case. I'm not sure what the bigger issue with refs is here, but I'm closing this one since you've found a workaround.\n", "@TianweiXing I met the same problem with you when use facenet. Have you solved the problem yet? \nI used the method you metioned but it did not work.\nCould you tell me how to solve the problem?\n", "Using tf.train.write_graph is not a workaround at all...\r\nIt just save the graph but not the weights. I found no way to restore weights of graph in C++, which makes me sad.", "Is there still no way to restore weights of graph in C++?", " Have you solved the problem yet?"]}, {"number": 2530, "title": "Cannot run Tensorflow example for Android on Windows ", "body": "I am trying to run Tensorflow example for Android on an ARM emulator (Android 5.0.1, API 21), but it does not run. What can be the reason for that?\n\n> 05-25 17:14:31.340: E/AndroidRuntime(1189): FATAL EXCEPTION: main 05-25 17:14:31.340: E/AndroidRuntime(1189): Process: org.tensorflow.demo, PID: 1189 05-25 17:14:31.340: E/AndroidRuntime(1189): java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.demo-2/base.apk\"],nativeLibraryDirectories=[/vendor/lib, /system/lib]]] couldn't find \"libtensorflow_demo.so\" 05-25 17:14:31.340: E/AndroidRuntime(1189): at java.lang.Runtime.loadLibrary(Runtime.java:366) 05-25 17:14:31.340: E/AndroidRuntime(1189): at java.lang.System.loadLibrary(System.java:989) 05-25 17:14:31.340: E/AndroidRuntime(1189): at org.tensorflow.demo.TensorflowClassifier.(TensorflowClassifier.java:47) 05-25 17:14:31.340: E/AndroidRuntime(1189): at org.tensorflow.demo.TensorflowImageListener.(TensorflowImageListener.java:55) 05-25 17:14:31.340: E/AndroidRuntime(1189): at org.tensorflow.demo.CameraConnectionFragment.(CameraConnectionFragment.java:452) 05-25 17:14:31.340: E/AndroidRuntime(1189): at org.tensorflow.demo.CameraConnectionFragment.newInstance(CameraConnectionFragment.java:271) 05-25 17:14:31.340: E/AndroidRuntime(1189): at org.tensorflow.demo.CameraActivity.onCreate(CameraActivity.java:33) 05-25 17:14:31.340: E/AndroidRuntime(1189): at android.app.Activity.performCreate(Activity.java:5937) 05-25 17:14:31.340: E/AndroidRuntime(1189): at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1105)\n", "comments": ["Hi, how are you building the Tensorflow demo? Are you building the native library as well? Can you check in your .apk (it's a zip file) to see if the .so file is actually there? \n", "Hi, I am using Eclipse 4.2.1 to import and build the demo app. I am not building the native library. Also, there's no .SO file in the .APK.\n", "Right, the .so file is the native library that contains the actual Tensorflow code. Eclipse can't do all the building by itself (unless you configure extra build steps to handle the native libs).\n\nThere are instructions for building in the Android example's [README.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md) file which will cover the native compilation as well.\n", "Closing, please reopen if you need specific help building the demo (though currently we don't officially support building on Windows).\n"]}, {"number": 2529, "title": "[tf.learn] logic for pandas import and fixed test", "body": "1. pandas should be imported by users when needed\n2. fixed test_categorical.py when pandas is installed\n", "comments": ["@martinwicke Seems like tests are running automatically for me now?\n", "Someone may have fixed auth, if that's the case, thank you, @jendap?\nOn Thu, May 26, 2016 at 19:57 Yuan (Terry) Tang notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke Seems like tests are\n> running automatically for me now?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2529#issuecomment-222048852,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_a5UB--qjqR1NkImeBtlhVWUPIKYks5qFl2ugaJpZM4IoIRE\n> .\n", "I have not changed anything. But the email address in the commits has domain @users.noreply.github.com so maybe the \"pull request builder plugin\" will then resolve properly the user belonging to tensorflow org.\n\nActually I believe the domain does not matter. I guess it will take the user name before domain and match that agains the list of users ion tensorflow org. Hence if someone has martinwicke on github and commit as wicke@ then it will not recognize. (but of course wicke is admin so for that user it does not matter whether he is resolved to be in the org or not)\n\nThat would also explain why @zheng-xq had to be added to admins. He has different user name in email then on github.\n", "Ok, Gunan has looked up the source code of the jenkins plugin. It is not cheating with user names. It asks github api. All public members of tensorflow org should build automatically. Hence this has to return 2xx\n\ncurl -s -o /dev/null -w \"%{http_code}\" https://api.github.com/organizations/15658638/public_members/terrytangyuan\n", "Thanks for looking into it!\n"]}, {"number": 2528, "title": "Remove --recurse-submodules from docs", "body": "Now that https://github.com/tensorflow/tensorflow/pull/1289 has been merged, the documentation for master doesn't need the `--recurse-submodules` argument to `git clone` unless cloning earlier releases.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!  I think there's a few more in the codebase that can be removed, can we do that in the same PR?\n", "Yeah, I'll have a look for them.\n", "Are there any more instances?\n", "LGTM, thanks!\n"]}, {"number": 2527, "title": "fix missing cudnn dependencies", "body": "This is the simplest solution to fix missing cudnn dependencies.  Just use the devel image from nvidia/cuda\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please  (not sure if our tests require docker)\n", "I'm optimistically merging this.  We can rollback if there are problems.\n", "But the library is **already** present in the `runtime` image, see my comment:\nhttps://github.com/tensorflow/tensorflow/issues/2525#issuecomment-223170383\n", "wait, why? that would make the image large. we should change it back\n"]}, {"number": 2526, "title": "Upstream changes from internal", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2525, "title": "the cudnn lib directory is not in the tensorflow gpu docker image's LD_LIBRARY_PATH", "body": "libcudnn4 is installed in the gcr.io/tensorflow/tensorflow:latest-gpu docker image, but I get the following errors from the jupyter tensorflow tutorial:\n\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:204] could not find cudnnCreate in cudnn DSO; dlerror: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate\n\nrunning env shows that LD_LIBRARY_PATH does not have a reference to the libcudnn.so location \n\n```\nLD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n```\n", "comments": ["More detail here: https://github.com/NVIDIA/nvidia-docker/issues/45\n", "That's because `LD_LIBRARY_PATH` is not the only path used for lookup.\nThe library is present in `/usr/lib/x86_64-linux-gnu/libcudnn.so.4`, it's just that the `dlopen` should be using the `SONAME`.\n", "Probably an issue with the configuration script again.\n\n`GetCudnnVersion` needs to return `4`, see:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L57\nhttps://github.com/tensorflow/tensorflow/blob/14ac2235699509f512b44b71160239c153ab413d/configure#L243\n\nAnd you might want to hardcode `\"1\"` for the CUDA library too\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/dso_loader.cc#L75\n", "It does not return 4. It will return empty string. It should (probably) look for /usr/lib/x86_64-linux-gnu/libcudnn.so\n\n@3XX0 what waas the story why the container doe not contain a symlink from `/usr/lib/x86_64-linux-gnu/libcudnn.so.4` to `/usr/lib/x86_64-linux-gnu/libcudnn.so`?\n\nIf we really have to have the number there we should modify the configure script to find the latest number by default. Or maybe even better - we could create a static constant set to CUDNN_MAJOR (macro in cudnn.h). That should work. But the link would be much better if cuda is somewhat backward compatible. We could compile for 4 and it would run even on 5. That will not happen if we look for exact filename containing the version.\n\n@3XX0 what is the nvidia oppinion? We should finally fix this. It is getting back too often.\n", "Symlinks like those are only useful for development purposes and we actually include them in our `devel` images (e.g. `cuda:7.5-cudnn4-devel`). When compiling, the linker will look up the soname of the library pointed by the symlink (in this case it will be `libcudnn.so.4`) and use it to build the binary dependencies.\n\nThe [soname](https://en.wikipedia.org/wiki/Soname) of a library defines its ABI compatibility, if it changes it means the ABI broke. With `dlopen` you are asking the dynamic linking loader to load a specific file. If you use the symlink, you are effectively giving up on enforcing a specific ABI. This can lead to undefined behaviors if the symlink points to something that you don't expect.\n\nNow the cuDNN versionning scheme is confusing and they broke the ABI every single time (hence the bump in the soname). I raised the issue internally a while ago and they said they will probably provide backward compatibility in the future.\nSo for example, we might see cuDNN 4.5 with the same soname (`libcudnn.so.4`).\n\nUsing the constant is a good idea, hopefully `CUDNN_MAJOR` is akin to the soname and will only change if the ABI breaks (I will warn the cuDNN team about that). For CUDA, I think it is safe to just hardcode \"1\" because libcuda has always provided backward compatibilty.\n", "@jendap What is the status on this? Has it been resolved?\n", "I think with the latest updates to our docker images, this issue is now resolved. Please let me know if it is still broken at head.\n"]}, {"number": 2524, "title": "Merge changes from internal", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2523, "title": "Incorrect error message", "body": "This error message is out of date:\n\n```\nValueError: Cannot execute operation using Run(): No default session is registered. Use 'with default_session(sess)' or pass an explicit session to Run(session=sess)\n```\n\nAs far as I can tell, there is no longer any `default_session` function. Instead, one must call `sess.as_default()`.\n", "comments": []}, {"number": 2522, "title": "Install instructions for OS X GPU", "body": "With the wonderful addition of #664, installation installation instructions for OS X with GPU support would be very helpful.\n", "comments": []}, {"number": 2521, "title": "Retrain.py with multiple GPUs", "body": "I am trying to run **retrain.py** on an AWS GPU instance with 4 K520's. When running **retrain.py** and watching the GPU activity using `nvidia-smi`, it seems like all of the calculations are happening on one GPU. The output looks like:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      2833    C   python                                        3801MiB |\n|    1      2833    C   python                                          37MiB |\n|    2      2833    C   python                                          37MiB |\n|    3      2833    C   python                                          37MiB |\n+-----------------------------------------------------------------------------+\n\n```\n\nHowever, when I run **cifar10_multi_gpu_train.py** and set `--num_gpus` to 4, the computations are distributed evenly as follows:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     14226    C   python                                        3841MiB |\n|    1     14226    C   python                                        3841MiB |\n|    2     14226    C   python                                        3841MiB |\n|    3     14226    C   python                                        3841MiB |\n+-----------------------------------------------------------------------------+\n```\n\nIs there a way to run **retrain.py** on multiple GPUs like on the cifar10_multi_gpu_train.py? If not, is there an easy way to use my own custom images for the **cifar10_multi_gpu_train.py**?\n", "comments": ["- @petewarden \n\nretrain.py is, as far as I know, an illustrative example.  You would have to modify it to support multiple GPUs like is done with cifar10_multi_gpu_train.py, I believe.\n\nFor information about how to do this, StackOverflow might be a better resource -- issues are mostly for bug reports and installation issues.\n"]}, {"number": 2520, "title": "Error for using tensorflow on Ubuntu 14:04 with TITAN x", "body": "Operating System:\n   ubuntu 14.04\n ======================================================================  \nInstalled version of CUDA and cuDNN: \n cuDNN v4 , CUDA 7.5\n\ntensorflow)sal@sal-All-Series:~/tensorflow$ ls -l /home/sal/cuda/lib64/libcud*\nlrwxrwxrwx 1 sal sal       15 Aug 21  2015 /home/sal/cuda/lib64/libcudnn.so -> libcudnn.so.7.0\nlrwxrwxrwx 1 sal sal       17 Mar  1 18:27 /home/sal/cuda/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rw-rw-r-- 1 sal sal 61453024 Mar  1 18:27 /home/sal/cuda/lib64/libcudnn.so.4.0.7\nlrwxrwxrwx 1 sal sal       18 Aug 21  2015 /home/sal/cuda/lib64/libcudnn.so.7.0 -> libcudnn.so.7.0.64\n-rwxrwxr-x 1 sal sal 48217000 Aug 21  2015 /home/sal/cuda/lib64/libcudnn.so.7.0.64\n-rw-rw-r-- 1 sal sal 62025862 Mar  1 18:27 /home/sal/cuda/lib64/libcudnn_static.a\n(tensorflow)sal@sal-All-Series:~/tensorflow$ \n# \n1. Virtualenv installation # I am not sure if this the answer\n2. (tensorflow)sal@sal-All-Series:~/tensorflow$ python -c \"import tensorflow; print(tensorflow.**version**)\n   \"\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64/:/home/sal/torch/install/lib::/usr/local/cuda-7.5/lib64:/opt/OpenBLAS-no-openmp/lib\n   I tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n   0.8.0\n   (tensorflow)sal@sal-All-Series:~/tensorflow$ \n# \n\nI am trying to import tensorflow and use it with my TITAN X GPU and I\nI think I have cuda installed as I use it with caffe and Torch7 even though I tried to install it from the beginning but same message I kept receiving. Any help please.\n\n> > > import tensorflow\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcudnn.so. LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64/:/home/sal/torch/install/lib::/usr/local/cuda-7.5/lib64:/opt/OpenBLAS-no-openmp/lib\n> > > I tensorflow/stream_executor/cuda/cuda_dnn.cc:1562] Unable to load cuDNN DSO\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n> > > I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n", "comments": ["You erased all the information we ask in our issue template, can you please include all that information so we can provide more help?\n", "Sorry for that I have updated it\n", "It looks like your symlinks aren't set up properly -- you have both v3 and v4 installed, libcudnn.so points to so.7.0, which is v3\n\nSecondly, your LD_LIBRARY_PATH doesn't have /home/sal/cuda/lib64 in it\n", "Could you please tell me how I can sort them out?\n", "Remove the existing libcudnn files and recopy libcudnn (just for v4).  You can Google for better instructions about how to set LD_LIBRARY_PATH.\n", "After untar the cudnn\r\n\r\n```\r\n[root@localhost cudnn]# cd include/\r\n[root@localhost include]# mv cudnn.h /usr/local/cuda/include/\r\n[root@localhost include]# cd ../lib64/\r\n[root@localhost lib64]# mv * /usr/local/cuda/lib\r\n```\r\nAnd it is ok\r\n\r\n```\r\n[root@localhost ~]# python\r\nPython 2.7.5 (default, Sep 15 2016, 22:37:39) \r\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as f\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n```\r\n"]}, {"number": 2519, "title": "Remove unnecessary args from gpu small test suite", "body": "", "comments": []}, {"number": 2518, "title": "Upstream changes from internal", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2517, "title": "d", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 2516, "title": "tf.test.compute_gradient returns unexpected analytical Jacobian for complex polynomial", "body": "### Environment info\n\nOperating System: Ubuntu 14.04.4 LTS\nTitan X GPU\nPython 2.7.11\n\nInstalled version of CUDA and cuDNN: Cuda 7.5, cuDNN 5.0.5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n(tensorflow)sarroff@eltopo:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 59909104 May 22 13:27 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 59909104 May 22 13:27 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 59909104 May 22 13:27 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 58775484 May 22 13:27 /usr/local/cuda/lib64/libcudnn_static.a\n```\n1. Which pip package you installed.\n   N/A (Installed from source)\n2. The output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\n```\n(tensorflow)sarroff@eltopo:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\n```\n\nIf installed from sources, provide the commit hash:\n4b7bc3174ed67b4a0eb1803537c9d00f132e9ae7\n### Steps to reproduce\n\nI have a complex variable z = x+iy and complex polynomial function f(z) = z^2. I am computing df/dz. `tf.gradients` returns the expected value but the returned analytical value of `tf.test.compute_gradient` does not:\n\n(In Python shell, logging messages snipped.)\n\n``` python\n>>> import numpy as np\n>>> z = np.asarray(np.random.randn() + 1j*np.random.randn(), dtype=np.complex64)\n>>> # Value of z\n... print z\n(-1.15801084042+0.433556616306j)\n>>> z_tf = tf.constant(z)\n>>> f = z_tf**2\n>>> with tf.Session() as sess:\n...     analytical, numerical = tf.test.compute_gradient(z_tf, (), f, (), x_init_value=z)\n...     df_dz = sess.run(tf.gradients(f, z_tf))\n... \n>>> # Expected derivative\n... 2*z\n(-2.3160216808319092+0.86711323261260986j)\n>>> \n>>> # Automatically computed derivative:\n... print df_dz\n[(-2.3160217+0.86711359j)]\n>>> \n>>> # analytical derivative from compute_gradient\n... print analytical\n[[-2.31602168 -0.86711359]\n [ 0.86711359 -2.31602168]]\n>>> \n>>> # numerical derivative from compute_gradient\n... print numerical\n[[-2.31617689  0.86712837]\n [-0.86700916 -2.31587887]]\n```\n\nAccording to the [TensorFlow API documentation](https://www.tensorflow.org/versions/r0.8/api_docs/python/test.html#compute_gradient), `compute_gradient` returns the analytical and numerical Jacobians\n\n```\nJ[:m, :n] = d(Re y)/d(Re x)\nJ[:m, n:] = d(Im y)/d(Re x)\nJ[m:, :n] = d(Re y)/d(Im x)\nJ[m:, n:] = d(Im y)/d(Im x)\n```\n\nIt appears that the analytical Jacobian is incorrect, and more specifically, that `J[:m, n:] = d(Im y)/d(Re x)` and `J[m:, :n] = d(Re y)/d(Im x)` may be swapped.\n", "comments": ["The real gradient of a complex analytic function is the conjugate of the complex derivative.  Derivation left as an exercise for the reader. :)\n"]}, {"number": 2515, "title": "examples/udacity: Ignore data", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 2514, "title": "Feature Request: multi-epoch alternative to tf.QueueBase.close()", "body": "Examples on the web demonstrate signaling the end of queue data by calling queue.close() in the data producer and then catching the tf.errors.OutOfRangeError exception in the data consumer.\n\nThis works fine for a single epoch, but I do multiple epochs, alternating between training data and testing data, and I can't reuse the queue after calling queue.close().\n\nThe two solutions that I have thought of using the existing code are:\n1) enqueue() some sentinel at the end of an epoch in the data producer and then tf.Assert() against the sentinel and catch the tf.errors.InvalidArgumentError in the data consumer.\n2) know the number of enqueue's for the epoch and only dequeue that number.\nBoth seem a little hacky.\n\nMulti-epoch use of queues might be simplified by adding one of the following:\n1) A queue.reset(), that throws one tf.errors.OutOfRangeError on dequeue() or some other exception.\n2) A queue.close(reset=True), that only throws one tf.errors.OutOfRangeError on dequeue() or some other exception.\n\nexample usage of 1):\n\n```\nq = tf.FIFOQueue(...)\nplaceholder = ...\nenqueue_op = q.enqueue(placeholder)\n....\n\ndef producer(data_dir, sess, q, enqueue_op, placeholder):\n  for ...:\n    sess.run(enqueue_op, {placeholder:...})\n  sess.run(q.reset())\n\ndef do_epoch(data_dir, learn):\n  threading.Thread(target=producer, args=(data_dir, sess, q, enqueue_op, placeholder)).start()\n  while True:\n    try:\n      sess.run(...)\n    exception tf.errors.OutOfRangeError:\n      break\n\nfor epoch in range(NUM_EPOCHS):\n  ... = do_epoch(TRAIN_DIR, learn=True)\n  ... = do_epoch(TEST_DIR, learn=False)\n```\n", "comments": ["cc @ebrevdo @mrry @josh11b \n", "Why not build a separate graph with its own queue for eval?  You'll also get the flexibility of being able to add additional flexibility to append the eval graph for eval purposes.\n", "Thank you for the suggestion. I guess I stayed away from two graphs to avoid code duplication between train and eval graphs and the accompanying maintenance issues, and because it would require updating the eval model weights with the trained model weights. I could define them both within one graph and share the weights, but that feels messy and still has the duplication. \n\nWith a single model, I can just run `sess.run([loss, train_op],...)` for training, or leave off train_op for eval `sess.run([loss],...)`. It seems clean; only one place to modify the model and weights are ready from the previous training invocation.\n", "If this isn't how others are structuring their code, then there is no urgency/necessity. I have been pre-computing the number of enqueue()'s that will be performed in an epoch, and then running the consumer the appropriate number of times, with asserts that the producer is not alive and the queue is empty before starting the next epoch.\n", "People usually use one method to create the core of the code, and a\nseparate method to take its output and generate the loss and gradients. A\nthird to generate the eval losses.  This way you can share code and\nvariables between the two graphs.  See tf.contrib.learn.\nOn May 26, 2016 3:05 PM, \"Mark Woodward\" notifications@github.com wrote:\n\n> If this isn't how others are structuring their code, then there is no\n> urgency/necessity. I have been pre-computing the number of enqueue()'s that\n> will be performed in an epoch, and then running the consumer the\n> appropriate number of times, with asserts that the producer is not alive\n> and the queue is empty before starting the next epoch.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-222008578\n", "I think I see what you are saying. Just to be certain, do you by chance have a specific example? I looked through tensorflow/contrib/learn/python/learn but did not find an example of this on my first scan. No worries if not, thank you.\n", "I have exactly the issue that markpwoodward does. If you have tensorflow v0.8 you might be able to use this workaround where by default you stream directly from the train queue except sometimes you pass test data in through a feed_dict. To be fair, I haven't tried it, and I came here looking for a better solution. I imagine it would look like this:\n\n```\n##Within Model \ninput= tf.place_holder_with_default( training_dequeue, shape)\n\n###Do training until end of epoch\nsess.run( model.update)\n\n###Do test/validate epoch\ntest_batch=sess.run( test_dequeue)\nsess.run( model.accuracy, feed_dict={input: test_batch} )\n\n\n```\n", "Picking up this old thread, a question: why don't you use `make_template` and have three queues feeding into three automatically shared model graphs? True, you still need to keep track when an epoch ends and switch over to the e.g. validation queue, but this can be done rather simple by either counting or have the queues return an additional element (additional wrt inputs and targets) that indicates one full pass through the data (training or validation data) is done. Or is this approach considered harmful in some way?\n", "I ended up doing the switching between validation and train in a python\nproducer thread, and counting the number of elements processed. But I have\nbeen thinking about this a bit, and I am leaning towards a similar (maybe\nsame) idea as yours, where there are two queues (train and validation),\nwith a tf.cond switching between queues, controlled by a placeholder. This\nwould allow for a single model graph (no need for multiple instantiations\nwith shared weights). Funny enough, Rohit Girdhar just emailed the list\nwith a question, and it seems that this is the setup he is using. I copied\nthe relevant portion below.\n\n\"I'm training a deep network with two data input pipelines, one for\ntraining and one for validation. They use `shuffle_batch_join` and\n`batch_join` respectively for parallel data reading. The data stream that\nis used in the network is decided using a `tf.cond` operation on top of\nthese two pipelines, which is controlled by a `is_training` placeholder\nthat is set to true for a training iteration and false when doing\nvalidation. I have 4 threads for reading training data and 1 thread for\nvalidation...\"\n\nOn Mon, Jul 18, 2016 at 12:21 PM, Christian notifications@github.com\nwrote:\n\n> Picking up this old thread, a question: why don't you use make_template\n> and have three queues feeding into three automatically shared model graphs?\n> True, you still need to keep track when an epoch ends and switch over to\n> the e.g. validation queue, but this can be done rather simple by either\n> counting or have the queues return a third element that indicates one full\n> pass through the data (training or validation data) is done. Or is this\n> approach considered harmful in some way?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233430140,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGgTpS5Jyv_NEidwew9thKg9x5KTJAiRks5qW9IhgaJpZM4Inwby\n> .\n", "Oops, tf.cond may be the wrong choice, as ebrevdo mentions in the parallel\nthread which I will stop referencing now. From the documentation, it seems\nthat tf.cond executes both paths up to, but not including, the final\noperation, so it wouldn't work as I had hoped.\n\nPerhaps it could still work if what was passed to tf.cond() was the\nqueue.dequeue(), or queue.dequeue_many(), operations for the train_queue\nand validation_queue. But those may themselves be the end of other tf\noperations, so it might still not work.\n\nOn Mon, Jul 18, 2016 at 5:59 PM, Mark Woodward mwoodward@cs.stanford.edu\nwrote:\n\n> I ended up doing the switching between validation and train in a python\n> producer thread, and counting the number of elements processed. But I have\n> been thinking about this a bit, and I am leaning towards a similar (maybe\n> same) idea as yours, where there are two queues (train and validation),\n> with a tf.cond switching between queues, controlled by a placeholder. This\n> would allow for a single model graph (no need for multiple instantiations\n> with shared weights). Funny enough, Rohit Girdhar just emailed the list\n> with a question, and it seems that this is the setup he is using. I copied\n> the relevant portion below.\n> \n> \"I'm training a deep network with two data input pipelines, one for\n> training and one for validation. They use `shuffle_batch_join` and\n> `batch_join` respectively for parallel data reading. The data stream that\n> is used in the network is decided using a `tf.cond` operation on top of\n> these two pipelines, which is controlled by a `is_training` placeholder\n> that is set to true for a training iteration and false when doing\n> validation. I have 4 threads for reading training data and 1 thread for\n> validation...\"\n> \n> On Mon, Jul 18, 2016 at 12:21 PM, Christian notifications@github.com\n> wrote:\n> \n> > Picking up this old thread, a question: why don't you use make_template\n> > and have three queues feeding into three automatically shared model graphs?\n> > True, you still need to keep track when an epoch ends and switch over to\n> > the e.g. validation queue, but this can be done rather simple by either\n> > counting or have the queues return a third element that indicates one full\n> > pass through the data (training or validation data) is done. Or is this\n> > approach considered harmful in some way?\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233430140,\n> > or mute the thread\n> > https://github.com/notifications/unsubscribe-auth/AGgTpS5Jyv_NEidwew9thKg9x5KTJAiRks5qW9IhgaJpZM4Inwby\n> > .\n", "What can work is a cond(filter_predicate, lambda: queue.enqueue(..),\ntf.no_op).  This allows controlling what goes into a queue.  Most other\nways of combining queues and tf.cond usually don't have the behavior you\nwould expect.\n\nOn Jul 18, 2016 8:20 PM, \"Mark Woodward\" notifications@github.com wrote:\n\n> Oops, tf.cond may be the wrong choice, as ebrevdo mentions in the parallel\n> thread which I will stop referencing now. From the documentation, it seems\n> that tf.cond executes both paths up to, but not including, the final\n> operation, so it wouldn't work as I had hoped.\n> \n> Perhaps it could still work if what was passed to tf.cond() was the\n> queue.dequeue(), or queue.dequeue_many(), operations for the train_queue\n> and validation_queue. But those may themselves be the end of other tf\n> operations, so it might still not work.\n> \n> On Mon, Jul 18, 2016 at 5:59 PM, Mark Woodward mwoodward@cs.stanford.edu\n> wrote:\n> \n> > I ended up doing the switching between validation and train in a python\n> > producer thread, and counting the number of elements processed. But I\n> > have\n> > been thinking about this a bit, and I am leaning towards a similar (maybe\n> > same) idea as yours, where there are two queues (train and validation),\n> > with a tf.cond switching between queues, controlled by a placeholder.\n> > This\n> > would allow for a single model graph (no need for multiple instantiations\n> > with shared weights). Funny enough, Rohit Girdhar just emailed the list\n> > with a question, and it seems that this is the setup he is using. I\n> > copied\n> > the relevant portion below.\n> > \n> > \"I'm training a deep network with two data input pipelines, one for\n> > training and one for validation. They use `shuffle_batch_join` and\n> > `batch_join` respectively for parallel data reading. The data stream that\n> > is used in the network is decided using a `tf.cond` operation on top of\n> > these two pipelines, which is controlled by a `is_training` placeholder\n> > that is set to true for a training iteration and false when doing\n> > validation. I have 4 threads for reading training data and 1 thread for\n> > validation...\"\n> > \n> > On Mon, Jul 18, 2016 at 12:21 PM, Christian notifications@github.com\n> > wrote:\n> > \n> > > Picking up this old thread, a question: why don't you use make_template\n> > > and have three queues feeding into three automatically shared model\n> > > graphs?\n> > > True, you still need to keep track when an epoch ends and switch over to\n> > > the e.g. validation queue, but this can be done rather simple by either\n> > > counting or have the queues return a third element that indicates one\n> > > full\n> > > pass through the data (training or validation data) is done. Or is this\n> > > approach considered harmful in some way?\n> > > \n> > > \u2014\n> > > You are receiving this because you authored the thread.\n> > > Reply to this email directly, view it on GitHub\n> > > <\n> > > https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233430140\n> > > ,\n> > > or mute the thread\n> > > <\n> > > https://github.com/notifications/unsubscribe-auth/AGgTpS5Jyv_NEidwew9thKg9x5KTJAiRks5qW9IhgaJpZM4Inwby\n> > > \n> > > .\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233519829,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim7VModT2soQ10NwRw9Zf7-umSE5Rks5qXEKFgaJpZM4Inwby\n> .\n", "With `make_template` there won't be multiple copies of the graph. Tensorboard nicely shows that the two queues (one for training data, the other for validation data) share one graph with the weights (True, you need to have two different smaller pieces of loss expressions that are associated with the queue and are placed on top of the one graph that has the trainable parameters). Deciding which queue is used is done on the outer training iteration loop, no tf.cond or any other expression-based logic necessary, you just sess.run the respective loss expression and the underlying queue is polled.\n", "Christian, just a clarification, doesn't calling make_template twice create\nduplicate operation paths in the graph, one for training and one for\nvalidation? That duplication is probably harmless, it just seems messy when\nall we want to do is switch between two input sources (although I don't\nhave an alternative to propose).\n\nOn Tue, Jul 19, 2016 at 12:05 AM, Christian notifications@github.com\nwrote:\n\n> With make_template there won't be multiple copies of the graph.\n> Tensorboard nicely shows that the two queues (one for training data, the\n> other for validation data) share one graph with the weights (True, you need\n> to have two different smaller pieces of loss expressions that are\n> associated with the queue and are placed on top of the one graph that has\n> the trainable parameters). Deciding which queue is used is done on the\n> outer training iteration loop, no tf.cond or any other expression-based\n> logic necessary, you just sess.run the respective loss expression and the\n> underlying queue is polled.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233548788,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGgTpRXoUlDAdmhT6zbDrHRd5_ReufOaks5qXHdXgaJpZM4Inwby\n> .\n", "@markpwoodward hmm, i don't know what an _operation path_ is. Say you call it twice, with two different input tensors (e.g. coming from a deque op on your training/validation set respectively). What is happening is that in the first call, the graph with the parameters is constructed. In the second call, this graph is simply reused. If you open up tensorboard, you see exactly this view, sort of a bottlenecked picture (if your shared graph is followed by some additional, loss-connected ops, which you need to have duplicated, that's true): At the bottom the two queues and their nodes, both feeding into the network expression that is made up of shared parameters, then going splitting up again to two loss-connected paths. With respect to messiness, I thought it the least messy solution, as I only had to add a for loop over the queues and kept the rest the same, make_template taking care of the rest. But messiness is probably quite subjective :-).\n", "@osdf, thank you for the response. Would you mind include the picture from\ntensorboard? I certainly may be missing something about how tensorflow\nexecution happens. Also, maybe brief pseudo code for your usage of\nmake_template(); where the goal of the pseudo code is just inference (no\nloss or training), but on two different queues.\n\nOn Tue, Jul 19, 2016 at 11:16 AM, Christian notifications@github.com\nwrote:\n\n> @markpwoodward https://github.com/markpwoodward hmm, i don't know what\n> an _operation path_ is. Say you call it twice, with two different input\n> tensors (e.g. coming from a deque op on your training/validation set\n> respectively). What is happening is that in the first call, the graph with\n> the parameters is constructed. In the second call, this graph is simply\n> reused. If you open up tensorboard, you see exactly this view, sort of a\n> bottlenecked picture (if your shared graph is followed by some additional,\n> loss-connected ops, which you need to have duplicated, that's true): At the\n> bottom the two queues and their nodes, both feeding into the network\n> expression that is made up of shared parameters, then going splitting up\n> again to two loss-connected paths. With respect to messiness, I thought it\n> the least messy solution, as I only had to add a for loop over the queues\n> and kept the rest the same, make_template taking care of the rest. But\n> messiness is probably quite subjective :-).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-233719206,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AGgTpRXQZW9sXwpodVkpt3Joe28JgAnrks5qXRRygaJpZM4Inwby\n> .\n", "You may be able to use QueueBase.from_list to dynamically select which queue to dequeue from, see:\nhttps://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#QueueBase.from_list\n", "I don't know how I missed that! Thank you. I just tested it. It works great; two FIFOQueues, one placeholder to select the queue.\n\nThis feature request was side tracked a bit, I will leave it open as my original request of a way to signal the last dequeue of an epoch, without needing to count dequeue's, still stands. Low priority, since it is easy enough to count dequeue's and this feature is likely less relevant for larger datasets, where people don't usually do things on epoch boundaries.\n", "@markpwoodward Can you explain how to use the feature pointed out by @josh11b? A code snippet would be great.\n", "Hi, I am using the same solution that was mentioned here, having a boolean placeholder `is_training`, and combining the training and evaluation queues using `batch = tf.cond(is_training, lambda: training_batch, lambda: testing_batch)`. Why would this not work? It seems to work for me, but I didn't check if it always dequeues both batches.\n", "@danijar Assuming `training_batch` and `testing_batch` are the results of calls to `tf.train.batch()` that occur before the `tf.cond()`, it will always dequeue both batches. See [here](http://stackoverflow.com/a/37064128) for an explanation (and substitute \"dequeue operation\" for \"assignment\").\n", "Thanks. So I basically need to pass functions into `tf.cond` that do a `tf.identity` and perform the desired calls within the `tf.control_dependencies` of the identity, correct? What about something like this instead:\n\n``` python\nqueue = tf.cond(is_training, train_queue, test_queue)\nbatch = queue.dequeue_many(batch_size)\n```\n", "The issues that I have with the two original proposals are:\nThe two solutions that I have thought of using the existing code are:\n\n> 1) enqueue() some sentinel at the end of an epoch in the data producer and then tf.Assert() against the sentinel and catch the tf.errors.InvalidArgumentError in the data consumer.\n\nWhat to do with batches? ie the single rows are handled by a buffering batch creator\n\n> 2) know the number of enqueue's for the epoch and only dequeue that number.\n> Both seem a little hacky.\n\nAgain with batching, If the epoch size is not a multiple of the batch size, then part of the next epoch ends up on the last batch.\n\nPerhaps an alternative solution to the OP would be a queues that raises `OutOfRangeError` but stays open. That way the local variable for `epochs` can just be reset.\n", "One solution that I have found is to create a new `input_producer` each epoch and then to `initialize_variables` only the new local variable (the `epoch` count). This works but yields warnings for the old queues:\n\n```\n[[Node: input_producer_18/fraction_of_32_full_Dequeue = QueueDequeue[_class=[\"loc:@input_producer_18/input_producer/fraction_of_32_full/fraction_of_32_full\"], component_types=[DT_INT32], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer_18/input_producer/fraction_of_32_full/fraction_of_32_full)]]\nW tensorflow/core/framework/op_kernel.cc:940] Out of range: FIFOQueue '_37_input_producer_18/input_producer/fraction_of_32_full/fraction_of_32_full' is closed and has insufficient elements (requested 1, current size 0)\n```\n", "Regarding the side track, not the original request.\n@muhammadzaheer, here is an example usage of `QueueBase.from_list()` that you asked for.\n@danijar, I would recommend using `QueueBase.from_list()` over `tf.cond()`.\n\n```\nimport tensorflow as tf\n\nq1 = tf.FIFOQueue(capacity=100, dtypes=[tf.int32])\ninput1 = tf.placeholder(tf.int32, [])\nenq1 = q1.enqueue(input1)\n\nq2 = tf.FIFOQueue(capacity=100, dtypes=[tf.int32])\ninput2 = tf.placeholder(tf.int32, [])\nenq2 = q2.enqueue(input2)\n\nselect_q = tf.placeholder(tf.int32, [])\nq = tf.QueueBase.from_list(select_q, [q1, q2])\ndata = q.dequeue()\n\nwith tf.Session() as sess:\n  # enqueue values (these would typically be in their own thread(s))\n  q1_vals = [1,2,3]\n  print \"q1_vals = \" + str(q1_vals)\n  for v in q1_vals:\n    sess.run(enq1, { input1: v })\n\n  q2_vals = [4,5,6]\n  print \"q2_vals = \" + str(q2_vals)\n  for v in q2_vals:\n    sess.run(enq2, { input2: v })\n\n  # run an op that pulls from the queue, specifying which queue\n  for i in range(3):\n    print \"q1.dequeue = \" + str(sess.run(data, {select_q: 0}))\n    print \"q2.dequeue = \" + str(sess.run(data, {select_q: 1}))\n```\n\noutputs\n\n```\nq1_vals = [1, 2, 3]\nq2_vals = [4, 5, 6]\nq1.dequeue = 1\nq2.dequeue = 4\nq1.dequeue = 2\nq2.dequeue = 5\nq1.dequeue = 3\nq2.dequeue = 6\n```\n", "QueueBase.from_list may be deprecated soon.  +@mrry\n\nOn Sep 9, 2016 1:14 PM, \"Mark Woodward\" notifications@github.com wrote:\n\n> Regarding the side track, not the original request.\n> @muhammadzaheer https://github.com/muhammadzaheer, here is an example\n> usage of QueueBase.from_list() that you asked for.\n> @danijar https://github.com/danijar, I would recommend using\n> QueueBase.from_list() over tf.cond().\n> \n> import tensorflow as tf\n> \n> q1 = tf.FIFOQueue(capacity=100, dtypes=[tf.int32])\n> input1 = tf.placeholder(tf.int32, [])\n> enq1 = q1.enqueue(input1)\n> \n> q2 = tf.FIFOQueue(capacity=100, dtypes=[tf.int32])\n> input2 = tf.placeholder(tf.int32, [])\n> enq2 = q2.enqueue(input2)\n> \n> select_q = tf.placeholder(tf.int32, [])\n> q = tf.QueueBase.from_list(select_q, [q1, q2])\n> data = q.dequeue()\n> \n> with tf.Session() as sess:\n>   # enqueue values (these would typically be in their own thread(s))\n>   q1_vals = [1,2,3]\n>   print \"q1_vals = \" + str(q1_vals)\n>   for v in q1_vals:\n>     sess.run(enq1, { input1: int(v) })\n> \n>   q2_vals = [4,5,6]\n>   print \"q2_vals = \" + str(q2_vals)\n>   for v in q2_vals:\n>     sess.run(enq2, { input2: int(v) })\n> \n>   # run an op that pulls from the queue, specifying which queue\n>   for batch in range(3):\n>     print \"q1.dequeue = \" + str(sess.run(data, {select_q: 0}))\n>     print \"q2.dequeue = \" + str(sess.run(data, {select_q: 1}))\n> \n> outputs\n> \n> q1_vals = [1, 2, 3]\n> q2_vals = [4, 5, 6]\n> q1.dequeue = 1\n> q2.dequeue = 4\n> q1.dequeue = 2\n> q2.dequeue = 5\n> q1.dequeue = 3\n> q2.dequeue = 6\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-246026683,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim4OS5APEuGuTpj9-uSnIFfE0qZ4Lks5qob4vgaJpZM4Inwby\n> .\n", "My current suggestion is to \n\n`group(*[cond(..., lambda: q.enqueue(....), tf.no_op) for q in queues])`\n", "See the new tf.contrib.training.bucket function for an example.\n", "For me, @osdf 's `make_template` funcionality was key. @markpwoodward here is a picture of tensorboard you asked for:\n<img width=\"911\" alt=\"tb_visualisation_templating\" src=\"https://cloud.githubusercontent.com/assets/7721540/18722724/79784626-803e-11e6-8ee3-004f7ec2aff4.png\">\n", "Thanks @TimZaman for the image, I overread the request by @markpwoodward (apologies for that!).\n", "Note that anyone using it must be extremely careful. Running something simple like a 'summary operation' that has not been seperated well enough would already pull both graphs through and dequeue both. Just fixed that issue in my implementation.\n", "Many thanks to all contributors & to @markpwoodward , @ebrevdo, @danijar.\r\nSince this thread is around 4 months old, curious to know the final solution you settled on- any updates on dong this more efficient/simpler?\r\nVery grateful for any inputs.", "There is now a set of methods in tf.train that allow conditional enqueue.  E.g. tf.train.maybe_batch.  the conditional may be a boolean Tensor. This is probably the appropriate solution from now on.", "Many thanks @ebrevdo for the prompt response. My question was around the \"side tracked\" question in this thread: how do you efficiently execute training & val testing using queues in your training loop. \r\nI was unable to get the intuition of your comments in this context (is it relevant, please)?\r\n\r\nCurrently i'm using tf.place_holder_with_default( training_dequeue, shape) method - ugly.\r\nDetails earlier in this thread [link](https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-223447983)\r\n\r\nMost thankful for any pointers.", "@rsethur you can use `make_template` to generate two pairs of inputs and outputs, hence two independent queues\r\n\r\ntraining_outputs = make_template(\"model\", training_inputs)\r\ntesting_outputs = make_template(\"model\", testing_inputs)\r\n<img src=https://cloud.githubusercontent.com/assets/23068/22184884/9bc3e6de-e08f-11e6-819b-43965ab32af8.png width=152 height=252>\r\n", "Thank you @yaroslavvb!\r\nI implemented this and it works.\r\nThe challenge is that now model construction time is doubled! Is this expected?\r\n\r\nThis could be a deal breaker for complex models(for some reason my model construction time in GPU is more than CPU)\r\n\r\nAny advise is appreciated.", "@rsethur Using the `make_template()` method constrained me too much (also implemented trn and val models with different feeds. You can achieve the same thing with scoping. I.e. something like this where `Model` is an object setting up your model. Just make sure they share the `variable`(!) name scope to share their variables.\r\nThe clear advantage of this is that this will look _very_ neat in the Tensorboard graph.\r\n```py\r\nwith tf.name_scope(const.STAGE_TRAIN) as stage_scope:\r\n    train_model = Model(const.STAGE_TRAIN, FLAGS.croplen, nclasses, FLAGS.optimization, FLAGS.momentum)\r\nwith tf.name_scope(const.STAGE_VAL) as stage_scope:\r\n    val_model = Model(const.STAGE_VAL, FLAGS.croplen, nclasses)\r\n```\r\nYou can clearly see they use the same model and its weights, but they have their own data sources:\r\n![image](https://cloud.githubusercontent.com/assets/7721540/22203055/c281f7f4-e173-11e6-9869-2c9613339179.png)\r\n", "@rsethur `make_template` will double the number of non-variable ops in the graph. I'm guessing if you have 50k ops, you'll see an extra 20 seconds added to your graph construction or so. If you wanted to shave that time off, sounds like ` tf.train.maybe_batch` would work better. Also, you could cut down graph construction time further by reducing number of Python node creating calls by using functions. IE group together repeating parts of the graph into function nodes, and create those.", "@yaroslavvb I would be very thankful if you can share some intuition/pseudo code on how to leverage `tf.train.maybe_batch` for this problem. Thank you! ", "Many thanks to @yaroslavvb : The time taken to add the operators was just 90 seconds in my case - so `make_template` is a reasonable approach and works great for me.\r\n\r\n@TimZaman : Thanks a LOT for taking the time to share your approach using scopes instead of `make_template` - this works fine as well in my case. The latter throws an error incase of tf.Variable is used - that seems to be an advantage.\r\n", "@yaroslavvb, I'd also greatly appreciate some sample code on how to use tf.train.maybe_batch. I imagine that keep_input needs to be a placeholder passed in during graph eval. \r\n\r\n@TimZaman, I also implemented the train and val with different feeds, with two different models in different name_scopes but still reusing variables across the graphs obviously. I found that if I created the train graph first and then the val graph, evaluating the val graph still caused the train input pipeline to dequeue, implying that the graphs were not separated well enough. I see you mentioned that issue earlier in this feed. How do you ensure graph separation using different input feeds? \r\n\r\nI created a google groups [discussion](https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/rmGu1HAyPw4) to share different tensorflow workflows. I think it would be very helpful to me and other more inexperienced users if some of you guys could maybe share your code designs. Thanks!  ", "Does anyone by now have a minimum working example using maybe_batch to switch between train and test queues? I'd greatly appreciate it!", "We're planning to move away from queues and provide first-class support for multi-epoch processing in the redesigned input pipeline API. Please feel free to comment on #7951 if there are particular features that you'd particularly like to see in the new API!", "in the meantime, what is the standard approach for running multiple training epochs using a queue?  I've searched around for a while, and the closest thing I could find was http://stackoverflow.com/a/39209186/212731 \"http://stackoverflow.com/a/39209186/212731\", for which @mrry provides a workaround. Is this the standard technique we should follow for now? or ... ?\r\n\r\n(basically, I have a bunch of examples, which I'm happy to store in a file as tfrecords, but I need to run indefinitely; specifying the number of epochs at the start is not really ideal for me. Having to guess how many steps per epoch is also not ideal).", "How would I use `tf.train.maybe_batch()` to select between training and testing batches?", "does someone used tf.train.maybe_batch() and wants to share the way he used it ? It would be awersone", "@mrry?", "In my opinion, `tf.train.maybe_batch()` is not working properly yet. By looking at the code, it is exactly the same method as `tf.train.batch()`. The latter simply uses _keep_input=True_.\r\n\r\nAccording to the API documentation, _keep_input_ is a bool Tensor, so it should accept a `tf.placeholder(tf.bool)` as well, right? But when I use a placeholder, and feed it with the value _True_, the queue is simply blocking and nothing is happening.\r\n\r\nI tried something like that:\r\n\r\n```\r\nis_training = tf.placeholder(tf.bool, shape=[])\r\n\r\n...\r\n\r\nimage_batch, label_batch = tf.cond(is_training,\r\n                                   true_fn=lambda: tf.train.maybe_batch([train_image, train_label],\r\n                                                                        keep_input=is_training,\r\n                                                                        batch_size=BATCH_SIZE),\r\n                                   false_fn=lambda: tf.train.maybe_batch([test_image, test_label],\r\n                                                                         keep_input=tf.logical_not(is_training),\r\n                                                                         batch_size=BATCH_SIZE))\r\nwith tf.Session() as sess:\r\n    # initialize the variables\r\n    sess.run(tf.local_variables_initializer())\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    # initialize the queue threads to start to shovel data\r\n    threads = tf.train.start_queue_runners(coord=tf.train.Coordinator())\r\n\r\n    for i in range(10):\r\n        print sess.run(label_batch, feed_dict={is_training: True})\r\n...\r\n```\r\nI thought of `tf.train.maybe_batch()` to be similar to the approach to use `tf.cond()` to switch between input-queue (for training) and feeding (for validation), but probably without having the [downside described here](https://groups.google.com/a/tensorflow.org/forum/#!msg/discuss/mLrt5qc9_uU/sGNbC7GpAwAJ). But it's also possible that I got this wrong...", "Hi,\r\nSo any1 found a way to use tf.train.maybe_batch() appropriately ?\r\nI am not finding out ways to switch between train/test data queues while training or eval phase.\r\n\r\nThanks", "Here is a hacky solution using `tf.FIFOQueue.from_list()`:\r\n\r\n```python\r\ndef select_batch(batches, index):\r\n  \"\"\"\r\n  Select a batch based on the current value of the index. Only the active batch\r\n  will be consumed. Each batch can be an arbitrarily nested tuple or list.\r\n  \"\"\"\r\n\r\n  def _get_dtypes(tensors):\r\n    if isinstance(tensors, (list, tuple)):\r\n      return type(tensors)(_get_dtypes(tensor) for tensor in tensors)\r\n    return tensors.dtype\r\n\r\n  def _get_shapes(tensors):\r\n    if isinstance(tensors, (list, tuple)):\r\n      return type(tensors)(_get_shapes(tensor) for tensor in tensors)\r\n    return tensors.shape\r\n\r\n  def _flatten(collection):\r\n    if isinstance(collection, (list, tuple)):\r\n      return sum([_flatten(element) for element in collection], [])\r\n    return [collection]\r\n\r\n  def _unflatten(iterator, shapes):\r\n    if isinstance(shapes, (list, tuple)):\r\n      return type(shapes)(_unflatten(iterator, shape) for shape in shapes)\r\n    return next(iterator)\r\n\r\n  queues = []\r\n  for batch in batches:\r\n    dtypes, shapes = _get_dtypes(batch), _get_shapes(batch)\r\n    queue = tf.FIFOQueue(10, _flatten(dtypes), _flatten(shapes))\r\n    runner = tf.train.QueueRunner(queue, (queue.enqueue(_flatten(batch)),))\r\n    tf.train.add_queue_runner(runner)\r\n    queues.append(queue)\r\n  batch = tf.FIFOQueue.from_list(index, queues).dequeue()\r\n  return _unflatten(iter(batch), shapes)\r\n```", "@tpatel0409 @LucasMahieu @danijar Have you found an example of how to use tf.train.maybe_batch in the mean time? Or did you end up switching to the new dataset api?", "Personally I decided to use the \"TF SLIM \u201c API juste before the 1.2 release : where the dataset API was introduced.\nI don\u2019t know the best solution right know.\n\n\n> On 29 Jun 2017, at 12:01, Julien Siems <notifications@github.com <mailto:notifications@github.com>> wrote:\n> \n> @tpatel0409 <https://github.com/tpatel0409> @LucasMahieu <https://github.com/lucasmahieu> @danijar <https://github.com/danijar> Have you found an example of how to use tf.train.maybe_batch in the mean time? Or did you end up switching to the new dataset api?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-311920475>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ALc1oOEpyP9ufuOliv8aUAR7PipI2IgQks5sI3XpgaJpZM4Inwby>.\n> \n\n", "@LucasMahieu Thank you for your quick response. Does the tf slim api provide any way of solving the problem? I am using tf slim as well but just for using predefined layers. I am currently looking into tf.train.maybe_batch and will post here if I come up with any example.", "Maybe @ebrevdo ? Since you suggested it. It would be great if you could post a snippet", "Also curious and will be reading through TF Slim data API today. I will post if I come across a solution.", "https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets\r\n\r\nLooks like the dataset api is the recommended way to do it (See for example tfrecords reader). Since I am using the input pipeline I ended up evaluating batches of testdata and then feeding them back into the training queue.\r\nAs suggested by: https://github.com/tensorflow/tensorflow/issues/2514#issuecomment-223447983", "How about if the labels for train and test are different? For example, If I have two lables for train, but only one label for test.", "If you are using tf.estimator.Estimator, which I think is the current best way to go (https://stackoverflow.com/questions/46925196/does-tf-estimator-estimator-train-maintain-input-fn-state), you can pass a different input_fn for train and evaluate, and you can use the mode passed to model_fn to create the appropriate graph.", "I just closed this. Feel free to re-open it. In my opinion, `tf.estimator` and `tf.dataset` are the way to go, and solve my original issue.\r\n\r\nIf you need to do things at the end of each epoch then just run `estimator.train()` with a `Dataset` that does not repeat. Alternate this with `estimator.evaluate()` or whatever you need.\r\n\r\nOften times the dataset is too big to wait for epoch boundaries, so use a `Dataset` that repeats and pass a listener to `estimator.train()` that runs `estimator.eval()` whenever a checkpoint is saved. See the link in my last comment above.\r\n\r\nI initially had a problem with this approach because of the perceived overhead of creating the graph on each call to `estimator.train()` or `estimator.evaluate()`. But, for me, the overhead has been negligible, and  having checkpoints be the transfer of information between `train()` and `evaluate()` feels like the right approach.", "@TimZaman Is there a example(make_template or share weights by sope) of try do so such things(diffrent feed for train and test). I try to use share weights method but end result that the testing data is always using the wights initialized. But the wights for traing have been updated. \r\nHere is my code:\r\n```\r\nwith tf.name_scope('train') as scope:\r\n    dist_train = network_out(network, x_train, keep_prob= FLAGS.keep_prob, phase_train=True, batch_size=FLAGS.batch_size)\r\n```\r\n```\r\n with tf.name_scope('eval') as scope:\r\n    with tf.variable_scope(tf.get_variable_scope(), reuse=True) as vscope:\r\n        dist_test = network_out(network, x_test, keep_prob= FLAGS.keep_prob, phase_train=False, batch_size=FLAGS.eval_batch_size)\r\n```\r\nAny one can help me?", "@markpwoodward Many thanks for your update and stackoverflow [link ](https://stackoverflow.com/questions/46925196/does-tf-estimator-estimator-train-maintain-input-fn-state)- I would like to migrate to estimator API as well. I still have a challenge - can you share your thoughts please?\r\n\r\nI would like to run evaluation before my epoch ends (as stated in your post as well because dataset is huge). Earlier I used to run evaluation and then save a checkpoint if score is better than the old one. Is this logical?\r\n\r\nFrom your post i understand that evaluation (by a listener) is run whenever a checkpoint is saved. \r\nWhat triggers a checkpoint save? How to implement the above logic?\r\nThank you!", "@rsethur Saving only the best checkpoint would be efficient, but I am not sure how to do that with this setup. Also, evaluation loss may not be the thing you want to optimize, you might want to review a number of evaluation metrics and pick the checkpoint that looks best in a general sense. I just keep all checkpoints, and visually inspect if I need to train more.\r\n\r\nAs for setting the frequency that estimator.train() saves checkpoints, there are probably other ways to do this, but I do it in a RunConfig object passed to Estimator's constructor.\r\n\r\n```\r\nestimator = tf.estimator.Estimator(\r\n  model_fn=...,\r\n  config=tf.estimator.RunConfig().replace( # I'm not sure why I use replace here and not the constructor\r\n    save_checkpoints_steps=1000, # or whatever you want\r\n    keep_checkpoint_max=None, # defaults to last 5, but I keep all\r\n  ),\r\n  ...\r\n)\r\n```\r\n\r\nAlso, take a look at the new Estimator.train_and_evaluate(), you may prefer it. I still prefer my proposed approach as I actually run evaluation on a fixed subset of my training data in addition to running evaluation on my validation set. I haven't been able to get train_and_evaluate() to support this (e.g. multiple EvalSpec's)  ", "@markpwoodward Many thanks for your detailed response - much appreciated!"]}, {"number": 2513, "title": "connect trained inception model to classify_image.py file cannot find pbtxt file", "body": "I have tried the training on flowers, it generated 2 files in /tmp a output_graph.pb and output_labels.txt file, but when i tried connecting these files to classify_image.py file I found the code is reading a .pbtxt apart from .pb and .txt file. How do I use the python code without the pbtxt file. \n\nfind the classify_image code here\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/imagenet/classify_image.py\n", "comments": ["@wolffg has a solution for this upcoming I believe.\n", "Same question. Waiting for a suggestion.\n", "@DKP-90\nHave you already solve this problem\uff1f\n", "No haven't found a solution yet.\n", "You can look at this gist:\n\nhttps://gist.github.com/wolffg/541c97a74dfc0d77c4b8fd2a946a5b41\n\nwhich is referred to here:\n\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5\n\nwhich shows using Python to classify with the output of tensorflow/examples/image_retraining/retrain.py.  It uses hardcoded pathnames, but you can get the idea.\n\nDoes that help?\n", "Thank you for your help. I'll try it.\n\nOn Wednesday, June 22, 2016, Wolff Dobson notifications@github.com wrote:\n\n> You can look at this gist:\n> \n> https://gist.github.com/wolffg/541c97a74dfc0d77c4b8fd2a946a5b41\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__gist.github.com_wolffg_541c97a74dfc0d77c4b8fd2a946a5b41&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=9uipfns-LMvzyI9sUZVflUuAD8ANgFkOooA2PNf36j4&e=\n> \n> which is referred to here:\n> \n> https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__codelabs.developers.google.com_codelabs_tensorflow-2Dfor-2Dpoets_-235&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=2GGLbOQ6wrVli14INo5OpShKcteO2368b_jrrbWuxSA&e=\n> \n> which shows using Python to classify with the output of\n> tensorflow/examples/image_retraining/retrain.py. It uses hardcoded\n> pathnames, but you can get the idea.\n> \n> Does that help?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_2513-23issuecomment-2D227651707&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=v-zu2WEEXP1fxSzxUM-UmfkLnlVrV4OyWIz8CWNtBhs&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQP8uxJoc8EduJE-5FXhZPDZbtSktohks5qONCfgaJpZM4Int0U&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=vvFCujyi5d-u0PXCfyBrKRGJRUUO11Umj75nGHHG4Ro&e=\n> .\n", "I copied the code and save it as a py file. Then I revised the path of the\npb and txt file and run it using python retrain_test.py PATHTOIMAGE. It\ngives me the error below:\n\npi@raspberrypi:~/robotimages $ python retrain_test.py\n/home/pi/robotimages/Robot1/w/1.jpg\nTraceback (most recent call last):\n  File \"retrain_test.py\", line 15, in <module>\n    graph_def.ParseFromString(f.read())\n  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/message.py\",\nline 185, in ParseFromString\n    self.MergeFromString(serialized)\n  File\n\"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\",\nline 1091, in MergeFromString\n    if self._InternalParse(serialized, 0, length) != length:\n  File\n\"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.py\",\nline 1127, in InternalParse\n    pos = field_decoder(buffer, new_pos, end, self, field_dict)\n  File\n\"/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/decoder.py\",\nline 610, in DecodeRepeatedField\n    raise _DecodeError('Truncated message.')\ngoogle.protobuf.message.DecodeError: Truncated message.\npi@raspberrypi:~/robotimages $\n\nOn Wed, Jun 22, 2016 at 6:05 AM, Wolff Dobson notifications@github.com\nwrote:\n\n> You can look at this gist:\n> \n> https://gist.github.com/wolffg/541c97a74dfc0d77c4b8fd2a946a5b41\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__gist.github.com_wolffg_541c97a74dfc0d77c4b8fd2a946a5b41&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=9uipfns-LMvzyI9sUZVflUuAD8ANgFkOooA2PNf36j4&e=\n> \n> which is referred to here:\n> \n> https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#5\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__codelabs.developers.google.com_codelabs_tensorflow-2Dfor-2Dpoets_-235&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=2GGLbOQ6wrVli14INo5OpShKcteO2368b_jrrbWuxSA&e=\n> \n> which shows using Python to classify with the output of\n> tensorflow/examples/image_retraining/retrain.py. It uses hardcoded\n> pathnames, but you can get the idea.\n> \n> Does that help?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_2513-23issuecomment-2D227651707&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=v-zu2WEEXP1fxSzxUM-UmfkLnlVrV4OyWIz8CWNtBhs&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQP8uxJoc8EduJE-5FXhZPDZbtSktohks5qONCfgaJpZM4Int0U&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=QtgmY051M6RcaSowhdL5AIxb01ntULZ0Pm2Bnlj8L80&s=vvFCujyi5d-u0PXCfyBrKRGJRUUO11Umj75nGHHG4Ro&e=\n> .\n", "That error message indicates that there's something wrong with the model file you're trying to load. It's hard to tell what the problem is there though.\n", "Closing this issue as it has been idle for some time. Please reopen or create new open if you get more information.\n", "can you tell how did you train for flower data set to generate .pb and .txt file. i tried from here https://github.com/tensorflow/models/tree/master/slim to \"fine tune a model from an existing checkpoint\", and its generating\r\n1. graph.pbtxt\r\n2. model.ckpt-1197.data-00000-of-00001\r\n3. model.ckpt-1197.index\r\n4. model.ckpt-1197.meta \r\nfiles. I generated .pb file and not able to generate .txt file. "]}, {"number": 2512, "title": "Problem about 'placeholder' function", "body": "I installed Tensorflow using Pip in Vmware Simulation Machine Ubuntu Operation System with only CPU Evn.\n\nWhen I used tensorflow in python 2.7:\n\n> > > import tensorflow as tf\n> > > x = tf.placeholder(name=\"x\")\n\nI met one error:\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: placeholder() takes at least 1 argument (1 given)\n\nThis code is an instance in http://download.tensorflow.org/paper/whitepaper2015.pdf\n", "comments": ["The whitepaper is probably out of date / incorrect.  See https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#placeholder for up to date documentation.\n", "> > > x = tf.placeholder(tf.string, name=\"x\")\n> > > Thanks!\n"]}, {"number": 2511, "title": "Shape inference crashes in SpaceToBatch when padding input is a persistent tensor", "body": "TLDR; is there a way to turn off shape inference, or allow support SpaceToBatch getting padding from GetSessionTensor op, which has unknown shape?\n\nTo reproduce\n\n```\n(holder1, input) = tf.get_session_tensor(tf.float32)\n(holder2, paddings) = tf.get_session_tensor(tf.int32)\ntf.space_to_batch(input, paddings, 2)\n\n\nTypeError: 'NoneType' object has no attribute '__getitem__'\n\n```\n1. SpaceToBatch inputs are GetSessionTensor ops.\n2. `_SpaceToBatchShape` calls tensor_util.constant_value\n3. that goes to _ConstantValue which checks op.type, and returns None since GetSessionTensor not recognized\n4. That goes back to `_SpaceToBatchShape` and crashes with\n   TypeError: 'NoneType' object has no attribute `'__getitem__'`\n   in this line because paddings is None\n\n```\n  paddings = tensor_util.constant_value(op.inputs[1])\n  if (paddings[0, 0] < 0 or paddings[0, 1] < 0 or\n      paddings[1, 0] < 0 or paddings[1, 1] < 0):\n    raise ValueError(\"paddings cannot be negative.\")\n\n```\n\nI got this when trying to use atrous convolution\n\n```\nFile \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/contrib/immediate/python/immediate/ops/nn_test.py\", line 304, in testAtrousSequence\n    y1 = tf.nn.atrous_conv2d(x, f, rate, padding=padding)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/python/ops/nn_ops.py\", line 205, in atrous_conv2d\n    block_size=rate)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/python/ops/gen_array_ops.py\", line 1471, in space_to_batch\n    block_size=block_size, name=name)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/contrib/immediate/python/immediate/op.py\", line 315, in apply_op\n    **keywords)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/python/ops/op_def_library.py\", line 702, in apply_op\n    op_def=op_def)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/python/framework/ops.py\", line 2188, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/python/framework/ops.py\", line 1635, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/private/var/tmp/_bazel_yaroslavvb/ea642be0829ea15e6d2092d22e894d3d/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/contrib/immediate/nn_test.runfiles/tensorflow/python/ops/array_ops.py\", line 1632, in _SpaceToBatchShape\n    if (paddings[0, 0] < 0 or paddings[0, 1] < 0 or\nTypeError: 'NoneType' object has no attribute '__getitem__'\n\n```\n", "comments": ["PS, a work-around is to modify to `@ops.RegisterShape(\"SpaceToBatch\")` and `@ops.RegisterShape(\"BatchToSpace\")` in `tensorflow/python/ops/array_ops.py` with\n\n```\n\n@ops.RegisterShape(\"BatchToSpace\")\ndef dummy(op):\n  return tensor_shape.TensorShape(None).with_rank(1)\n\n@ops.RegisterShape(\"SpaceToBatch\")\ndef dummy(op):\n  return tensor_shape.TensorShape(None).with_rank(1)\n\n```\n", "This has the same underlying issue as #2473, and we have a fix incoming, so I'll close this as a duplicate.\n"]}, {"number": 2510, "title": "Tensorboard feature request - Text summary", "body": "Would it be reasonable to add a basic text summary feature to Tensorboard? Personally I've run my network a few dozen times with really minor changes between them for testing and it would be really useful if there was a field where I could put some arbitrary text where I just wrote the key differences in my runs.\n\nFor example, on the Events page (or somewhere else) there would be a dropdown, similar to the summaries on the Events and Histograms page, with text I added (either hardcoded or as a script argument) that says what I did differently this run. Maybe I would print out the argument values for each run as well, that would be pretty useful, but basically something where I can say \"What did I do with this run again? Why was it different than the one before? Oh yeah I changed the batch size\" or \"Oh yeah I used my other dataset instead.\"\n\nObviously if it's arbitrary text you could maybe use it to write up a description of the network or whatever you want. \n", "comments": ["@danmane: Unless you're planning to work on this soon, let's mark as contributions welcome. \n", "We now have a text summary. See https://www.tensorflow.org/versions/master/api_docs/python/tf/summary/text"]}, {"number": 2509, "title": "Upstream changes for 5/25/2016", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2508, "title": "Can't place tf.nn.moments() on GPU ", "body": "While trying to optimize training of a network by moving most work to the GPU, I encountered the following error below.  Although this references a variable defined in the batch_normalization helper class I've used, removing that class (replacing all calls to it with an identify function) results in a similar error elsewhere.\n\n```\nTraceback (most recent call last):\n  File \"train_resnet.py\", line 106, in <module>\n    tf.app.run()\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"train_resnet.py\", line 103, in main\n    run_training()\n  File \"train_resnet.py\", line 97, in run_training\n    sess.run(tf.initialize_all_variables())\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 333, in run\n    run_metadata_ptr)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 573, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 648, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 668, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/resnet_module_1/bn/moments/moments/mean_ss_grad/Maximum/y': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n         [[Node: gradients/resnet_module_1/bn/moments/moments/mean_ss_grad/Maximum/y = Const[_class=[\"loc:@resnet_module_1/bn/moments/moments/mean_ss\"], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: 1>, _device=\"/device:GPU:0\"]()]]\nCaused by op 'gradients/resnet_module_1/bn/moments/moments/mean_ss_grad/Maximum/y', defined at:\n  File \"train_resnet.py\", line 106, in <module>\n    tf.app.run()\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"train_resnet.py\", line 103, in main\n    run_training()\n  File \"train_resnet.py\", line 89, in run_training\n    train_op = opt.minimize(loss, colocate_gradients_with_ops=True, aggregation_method=2)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 193, in minimize\n    grad_loss=grad_loss)\n  File \"/data/Ray/policy_network/gpu/clipopt.py\", line 9, in compute_gradients\n    *args, **kwargs)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\", line 481, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 41, in _SumGrad\n    tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 33, in _safe_shape_div\n    return x // math_ops.maximum(y, 1)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1173, in maximum\n    result = _op_def_lib.apply_op(\"Maximum\", x=x, y=y, name=name)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py\", line 455, in apply_op\n    as_ref=input_arg.is_ref)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 620, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/constant_op.py\", line 179, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/constant_op.py\", line 166, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1224, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'resnet_module_1/bn/moments/moments/mean_ss', defined at:\n  File \"train_resnet.py\", line 106, in <module>\n    tf.app.run()\n[elided 1 identical lines from previous traceback]\n  File \"train_resnet.py\", line 103, in main\n    run_training()\n  File \"train_resnet.py\", line 77, in run_training\n    depth=FLAGS.num_features)\n  File \"train_resnet.py\", line 45, in model\n    model = res_3x3_pair(model, depth, \"resnet_module_{}\".format(idx + 1), training_switch, keep_prob_var)\n  File \"train_resnet.py\", line 29, in res_3x3_pair\n    normed = tf.nn.relu(batch_norm(data, depth, training_switch))\n  File \"/data/Ray/policy_network/gpu/batchnorm.py\", line 22, in batch_norm\n    batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/nn.py\", line 712, in moments\n    name=name)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/nn.py\", line 649, in sufficient_statistics\n    m_ss = math_ops.reduce_sum(m_ss, axes, keep_dims=keep_dims, name=\"mean_ss\")\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 909, in reduce_sum\n    keep_dims, name=name)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2087, in _sum\n    keep_dims=keep_dims, name=name)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/thouis/VENV35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1224, in __init__\n    self._traceback = _extract_stack()\n```\n### Environment info\n\nOperating System: Ubuntu 16.06\n\nInstalled version of CUDA and cuDNN: \n\n```\n-rw-r--r-- 1 root root 189170 May 24 11:22 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 May 24 11:22 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 May 24 11:22 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 May 24 11:22 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 May 24 11:22 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n\npip installed from May 25 nightly build of Linux GPU version.\n\n```\npython -c \"import tensorflow; print(tensorflow.__version__)\"\n0.8.0\n```\n### Steps to reproduce\n- grab python files from https://gist.github.com/thouis/9bbd330a153b1f553fd58743a3ae4c9a\n- `python -u -i train_resnet.py --learning_rate 0.005  --num_modules 16 --num_features 192 --summary_dir Workspace --checkpoint_dir Workspace --batch_size 64`\n### What have you tried?\n1. I tried to cast the values in _safe_shape_div() to int64 in case that was the underlying cause, but that seemed to cause different problems.\n### Logs or other output that would be helpful\n\nAll files (python, run log) at:\nhttps://gist.github.com/thouis/9bbd330a153b1f553fd58743a3ae4c9a\n", "comments": ["I think this is actually a duplicate of https://github.com/tensorflow/tensorflow/issues/2505  -- the error message is misleading and I'll try to fix that.  But the problem has been fixed at HEAD.\n\nUnfortunately last night's pip binary didn't build properly, so it's still on the previous night's version, which doesn't have the fix.\n\n@caisq do you know why the release binaries keep failing?\n", "OK.  I saw that fix, but didn't realize the nightlies were failing.  I'll install from source and test again.\n", "I confirmed that this is fixed in HEAD.\n"]}, {"number": 2507, "title": "Build Model in Python and Run in Java Example", "body": "How would one go about doing this, and are there any examples? \n", "comments": ["There's no Java API for Tensorflow yet, so this is not possible to do directly.\n\nHowever, in the Android example project we run inference from Java via JNI: see [tensorflow_jni.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/jni/tensorflow_jni.cc) and [TensorflowClassifier.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorflowClassifier.java)\n\nYou'll likely want to use [freeze_graph](https://www.tensorflow.org/versions/r0.8/how_tos/tool_developers/index.html#freezing) on your GraphDef first to merge your checkpoint values in, letting you load it all as a self-contained file.\n"]}, {"number": 2506, "title": "Enable GPU for Matmul float64", "body": "Enabled GPU registration for MatMul operations of type double. This partially addresses #1140.\n", "comments": ["Can one of the admins verify this patch?\n", "@vrv Here is the new pull request.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2505, "title": "No mod for int32 no GPU kernel support", "body": "I apologize if this is not actually a issue. When trying to put tf.reduce_mean() on the gpu, I get the following error:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/Mean_grad/mod': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n         [[Node: gradients/Mean_grad/mod = Mod[T=DT_INT32, _device=\"/device:GPU:0\"](gradients/Mean_grad/add, gradients/Mean_grad/Size)]]\n```\n\nI am on commit 43a8c49f04a485ffc33b510c27eda77f8f38377c from Tue 5/24/2016.\n\nYou can reproduce this by modifying the mnist/convolutional.py file by putting the tf.reduce_mean() and optimizer.minimize() inside \"with tf.device('/gpu:0'):\" statements.\n\nI am trying to do gradient calculations on their respective gpu towers, but compute_gradients() gets hung up on tf.reduce_mean().\n\nThank you,\nMark\n", "comments": ["the error message actually says there's no \"mod\" support for int32 on GPU.\n\nThankfully, this was fixed yesterday at https://github.com/tensorflow/tensorflow/commit/249106f652591afd0fc36ee4e560c3989287f699 -- can you sync to head and try again?\n", "Fix confirmed. thank you!\n"]}, {"number": 2504, "title": "Add genrule_strategy=standalone to bazelrc", "body": "Fixes #2499 (hopefully)\n", "comments": []}]