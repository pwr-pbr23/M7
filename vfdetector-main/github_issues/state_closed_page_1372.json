[{"number": 11908, "title": "Query in lower-level versions of Get/Put involved in EncodeFixed/DecodeFixed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 'v1.2.1-0-gb4957ff', '1.2.1'\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: No GPU\r\n- **GPU model and memory**: No GPU\r\n- **Exact command to reproduce**: Run tests from core module.\r\n\r\n### The problem\r\nWhile checking [coding_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/coding_test.cc) , came across a sub test - `TEST(Coding, EncodingOutput)` which tests that encoding routines generate little-endian encodings. \r\nThis test is passing on a big endian machine. So while debugging realized that the `EncodeFixed/DecodeFixed` functions from [coding.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/coding.cc) and [raw_coding.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/core/raw_coding.h) encode/decode `buf` values on big endian incorrectly i.e. the character buffer writing happens in the same way as on a little-endian machine.\r\n\r\n\r\nAfter I made the required changes to correct the same, although 1 test of my interest(`TEST(TensorBundleTest, Checksum)`) now passes, I could see many others failing on big endian with errors like:\r\n1. `Data loss: block checksum mismatch: perhaps your file is in a different file format and you need to use a different restore operator?`\r\n2. `\"Data loss: corrupted record at 19\"`\r\n3. `Segmentation fault`\r\n4. `Invalid argument: sample_rate must be in (0, 2^32), got: 0`\r\n\r\nSo looks like this change is breaking too many things here. \r\n\r\nCan anyone help in understanding why the correction is causing so many issues? \r\nWill this change involve too many changes in TensorFlow code to support big endian?\r\n", "comments": ["The routines in coding.{h,cc} are meant to be agnostic of the endian-ness of the machine on which they're executing.  They should only be used for encoding/decoding values used by tensorflow itself, not for values produced or consumed by some other program.  Hence they pick one convention (little-endian) as confirmed in coding_test.cc and use it independently of the machine architecture, which should not cause problems.", "@poxvoculi, Thank you for the clarification.\r\nI now need to see how the functionality related to TEST(TensorBundleTest, Checksum) is to be corrected with the LE convention on big endian.\r\n\r\n"]}, {"number": 11907, "title": "[XLA] Any unrecognised fusion type should be described as kCustom", "body": "@majnemer @vrv \r\n\r\nHi,  \r\n\r\nthis is the knock on from cancelling having a fusion sub-type.  if I use (kCustom+N) as types of fusion then I need the printing routine to not fail when given a type that is not recognised.\r\n\r\nThe cancelled PR was https://github.com/tensorflow/tensorflow/pull/11719\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@DavidNorman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @tensorflower-gardener and @meheffernan to be potential reviewers.", "@DavidNorman Can you please explain, in more detail, the motivation behind this custom tag? I'd like to better understand the limitations in the current scheme.", "Sure.  The limitations of the current scheme are that there are only 6 explicit types.  One of them is  'kConvBackwardFilter', which maps to a special backwards conv operation specifically for the GPU hardware.\r\n\r\nIn our hardware we have, for instance, an operation is which like parts of an average_pool.  It is like the reduce_window<add> followed by the divide.  I am using the fusion mechanism to fuse these two things into one thing, just like the kConvBackwardFilter does for the GPU hardware.\r\n\r\nTo label this fusion, I could introduce a new value into the enumeration that describes the operation.  However, I have been asked not to put things into the public repo that are specific to our hardware.  Notwithstanding the fact that other companies are doing exactly that, I think it is a good idea to separate out 3rd party devices.\r\n\r\nWhich means that the public repo must provide a mechanism for extending all enumerations cleanly.   Adding a 'custom' upper bound is one way of doing this - probably the simplest way.\r\n\r\nHope this helps.\r\n\r\nCheers\r\n\r\n", "@majnemer \r\n\r\nHi.\r\n\r\nI wonder if you have had a chance to think about this?\r\n\r\nCheers\r\n\r\nDavid\r\n\r\n", "Hi,\r\n\r\nI believe I can make your example of ReduceWindow followed by Divide work within the existing framework of fusion.\r\n\r\nYou could represent it as a kOutput fusion node where the kDivide follows the kReduceWindow.", "Thanks.  Nevertheless I think that there should be the option to extend the enumeration.  I do not want to have several of my fusions labelled with the same token.\r\n\r\nFor instance, I have the reduce_window+divide, and also a convolution+add.  If I fuse them both to something called 'kOutput' then when I am lowering the graph later, how do I tell one from the other?\r\n\r\nAlso - if this is the case, why does GPU backward convolution get to have a specially named op?  It is merely a conv followed by an optional transpose...\r\n\r\n```\r\n  //        activations  gradients\r\n  //              \\         /\r\n  //               v       v\r\n  //              Convolution\r\n  //                 conv\r\n  //                   |\r\n  //                   v\r\n  //               Transpose (optional if identity transposition)\r\n```\r\n\r\n", "Here is one approach for identifying what sort of operation you are looking at for a kOutput node: do a reverse post order traversal on the instructions in the fused computation. The first operation which is not a parameter is the operation of interest. In the case of reduce_window+divide, it would give you the reduce_window. In the case of convolution+add, it would give you the add.", "Again, I wonder why the GPU reverse convolution is allowed a special label, but 3rd party devices are not?\r\n\r\nHaving gone to the trouble of identifying large combinations of ops, I do not really want to have to go back through them again at elaboration time, in order to figure out what I knew before.\r\n\r\n```\r\n  // Sigmoid\r\n  {{HloOpcode::kAdd, true, nullptr, {4, 1}},\r\n   {HloOpcode::kMultiply, true, nullptr, {4, 2}},\r\n   {HloOpcode::kTanh, true, nullptr, {3}},\r\n   {HloOpcode::kMultiply, true, nullptr, {4, -1}},\r\n   {HloOpcode::kConstant, true, IsConstantHalf, {}}},\r\n\r\n  // BiasAdd on convolution (explicit broadcast)\r\n  {{HloOpcode::kAdd, true, nullptr, {1, 2}},\r\n   {HloOpcode::kCall, false, IsPoplarConvolution, {-1, -1}},\r\n   {HloOpcode::kBroadcast, true, nullptr, {-1}}},\r\n\r\n  // Random truncated normal with post scale and add\r\n  {{HloOpcode::kAdd, true, nullptr, {2, 1}},\r\n   {HloOpcode::kConstant, true, nullptr, {}},\r\n   {HloOpcode::kMultiply, true, nullptr, {4, 3}},\r\n   {HloOpcode::kConstant, true, nullptr, {}},\r\n   {HloOpcode::kWhile, true, IsTruncatedNormalWhile, {5}},\r\n   {HloOpcode::kRng, true, nullptr, {6, 7}},\r\n   {HloOpcode::kConstant, true, nullptr, {}},\r\n   {HloOpcode::kConstant, true, nullptr, {}}},\r\n```\r\n\r\n", "The GPU convolution fusions are things we would remove given the time to do so, I see them as warts on fusion.\r\n\r\nFusion nodes are mostly intended for rather generic computations. If your hardware only has limited support for particular fusions, it makes sense to use something like CustomCall to represent the specific thing you have already pattern matched.", "Thanks for the suggestion.  I think I will shelve this for the time being and discuss this in the XLA forum to get some clarity.\r\n\r\nCheers\r\n\r\n", "Ok.  It's all good.  Thanks for the input.  Closing this now, and using kCall until I can see what happens to the GPU fusion.\r\n\r\nThanks\r\n\r\n:)\r\n"]}, {"number": 11906, "title": "Wrong conv2d outputs for tensors of certain sizes", "body": "### System information\r\n- **Custom code**: yes\r\n- **OS Platform and Distribution**: Windows 7 Professional, SP1\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: tensorflow-gpu 1.3.0rc0\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Nvidia Titan X (Pascal), 12GB\r\n- **Exact command to reproduce**: conv2d\r\n\r\nImages (tensors produced from numpy 2d arrays) of certain sizes fail to be properly convolved with conv2d function. Only the stripe on top of the image is processed correctly. The rest appears empty or contains a kind of structured noise. Sometimes it also shifted and wrapped around the image borders.\r\n\r\nhttps://nbviewer.jupyter.org/github/adibrov/jupyterNotebooks/blob/master/bug_tf.ipynb", "comments": ["@adibrov,\r\n\r\nI cannot reproduce this behaviour on both on TensorFlow 1.0.0/GTX1080/Win10/cuDNN 5.1 and TensorFlow 1.3.0/GTX1070/Win10/cuDNN 6.0. Maybe your issue was corrected after rc0.", "Not reproducible on TitanX (different instance) under Linux Mint 18.", "On the affected instance, are CUDA and associated drivers up-to-date?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11905, "title": "No registered 'ResizeBilinear' OpKernel for XLA_CPU_JIT", "body": "Similar to https://github.com/tensorflow/tensorflow/issues/11890, `tf.image.resize_images` and its siblings haven't been made available for XLA yet. Is there a timeline for when core ops will be supported by XLA? Is there a short instruction somewhere on how to implement ops kernels for the XLA bridge so we could do pull requests as needed to speed up development?\r\n\r\nRelated: https://github.com/tensorflow/tensorflow/issues/11275", "comments": ["Sure, I'd be happy to use bilinear resize as the subject of a writeup.", "Sweet! Looking forward to it!", "@learyg, ETA on a short guide (just a rough draft at least)? Would be nice to see ResizeBilinear XLA support on master anyway.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "hawkinsp@ added an implementation in https://github.com/tensorflow/tensorflow/commit/cddf9415564b16c2bc234df68d3eb44fc8689dae#diff-e46056ad1b86847e16150ab7906bab3b -- does that work for your use case?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @learyg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Wow, I completely missed this issue. Yes, @learyg that has been very useful! Closing this issue. \r\n\r\n(I'd still love to see a tutorial on how to extend XLA so we in the open source community can help out more \ud83d\ude03)"]}, {"number": 11904, "title": "Unclear about how to integrate AttentionWrapper with BeamSearchDecoder", "body": "TF Version: 1.2.1\r\n\r\nI cannot find information about how to integrate AttentionWrapper with BeamSearchDecoder on website / nmt tutorial, in particular how to feed the beam-search-tiled (```tf.contrib.seq2seq.tile_batch```) states to the attention-cloned (```zero_state(...).clone(...)```) states.\r\n\r\nIn my attempt, there seems to be an inconsistency between the requirement of the ```batch_size``` of the zero states generated by ```AttentionWrapper```  in the training stage and predicting stage. In training stage, initial state of decoder requires batch size, however in predicting stage it requires (batch_size * beam_width).\r\nthe minimal code to demonstrate this problem is:\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\nBEAM_WIDTH = 5\r\nBATCH_SIZE = 128\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# ATTENTION\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\n# TRAINING DECODER\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# PREDICTING_DECODER\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\r\n    end_token = 2,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(\r\n                    cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, BEAM_WIDTH)),\r\n    beam_width = BEAM_WIDTH,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = False,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\r\n```\r\nthe error message is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 58, in <module>\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE*BEAM_WIDTH,tf.float32).clone(\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 659, in zero_state\r\n    message=error_message)]):\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 317, in assert_equal\r\n    _assert_static(condition_static, data)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/check_ops.py\", line 100, in _assert_static\r\n    raise ValueError('\\n'.join(data_static))\r\nValueError: When calling zero_state of AttentionWrapper attention_wrapper: Non-matching batch sizes between the memory (encoder output) and the requested batch size.  Are you using the BeamSearchDecoder?  If so, make sure your encoder output has been tiled to beam_width via tf.contrib.seq2seq.tile_batch, and the batch_size= argument passed to zero_state is batch_size * beam_width.\r\nCondition x == y did not hold element-wise:\r\nx (AttentionWrapperZeroState_1/assert_equal/x:0) = \r\n640\r\ny (AttentionWrapperZeroState_1/assert_equal/y:0) = \r\n128\r\n```", "comments": ["@ebrevdo any insight about this?", "@JerrikEph @Songweiping @pplantinga\r\nany insight about this?", "The error message seems to suggest a fix: tile the encoder output", "hi @pplantinga \r\nIt is already tiled here:\r\n```python\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(\r\n                    cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, BEAM_WIDTH)),\r\n```\r\nI use clone method because of attention wrapper", "I see:\r\n\r\nEncoder out -> attention -> decoder cell -> beam search(cell=decoder cell)\r\n\r\nWithout ever getting tiled. I'm not sure of the correct place to tile, however. ", "I finally get the graph successfully compiled by the following code,  I will further test the session running and then close the issue if solved\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n\r\nBEAM_WIDTH = 5\r\nBATCH_SIZE = 128\r\n\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\n\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\n\r\n# ATTENTION (TRAINING)\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (TRAINING)\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n\r\n# BEAM SEARCH TILE\r\nencoder_out = tf.contrib.seq2seq.tile_batch(encoder_out, multiplier=BEAM_WIDTH)\r\nX_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\r\nencoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=BEAM_WIDTH)\r\n\r\n\r\n# ATTENTION (PREDICTING)\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (PREDICTING)\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\r\n    end_token = 2,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(cell_state=encoder_state),\r\n    beam_width = BEAM_WIDTH,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = False,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\r\n\r\nprint('successful')\r\n```", "session running is passed, this is closed, thanks.", "One question about the code: you use two attention mechanism during training and predicting phase. Do they share the same parameters?", "@applenob no they don't, here i just do a minimal example to demonstrate the workflow. You are right, we need to control variable scopes in practice.", "Thanks for your reply. Do you have any code sharing? Since the two attention as two inputs with different shape, I don't think these two variables can be shared.", "Hi, @applenob If you want to have a look at more complex and complete code, you can have a look on my repo (https://github.com/zhedongzheng/finch).", "Cool project !", "@zhedongzheng, Your complete code can actually work and make sense to me now. Thanks.", "\u60a8\u597d\uff0c\u6211\u4e5f\u9047\u5230\u4e86\u540c\u6837\u7684\u95ee\u9898\uff0c\u4e3a\u4ec0\u4e48\u9700\u8981\u8fd9\u4e2atile\u5462\uff1f", "@zhedongzheng ", "@lmolhw5252  \u5047\u8bbebeam_width = 5\uff0c\u90a3\u4e48\u6bcf\u6b21decode\u90fd\u9700\u8981\u4fdd\u63015\u4e2a\u6700\u4f73state\uff0c\u6240\u4ee5\u4e00\u5f00\u59cb\u5c31\u76f4\u63a5\u628a\u521d\u59cb\u5316state\u590d\u52365\u4efd\uff0c\u8fd9\u5c31\u662ftile\u7684\u4f5c\u7528\u3002", "This is the minimal fix of code by @zhedongzheng that makes parameters of attention mechanism shared for train and predict decoders:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n\r\nBEAM_WIDTH = 5\r\nBATCH_SIZE = 128\r\n\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nY = tf.placeholder(tf.int32, [BATCH_SIZE, None])\r\nX_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\nY_seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])\r\n\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\n\r\n# ATTENTION (TRAINING)\r\nwith tf.variable_scope('shared_attention_mechanism'):\r\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        num_units = 128, \r\n        memory = encoder_out,\r\n        memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (TRAINING)\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE,tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\nwith tf.variable_scope('decode_with_shared_attention'):\r\n    training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder = training_decoder,\r\n        impute_finished = True,\r\n        maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n\r\n# BEAM SEARCH TILE\r\nencoder_out = tf.contrib.seq2seq.tile_batch(encoder_out, multiplier=BEAM_WIDTH)\r\nX_seq_len = tf.contrib.seq2seq.tile_batch(X_seq_len, multiplier=BEAM_WIDTH)\r\nencoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=BEAM_WIDTH)\r\n\r\n\r\n# ATTENTION (PREDICTING)\r\nwith tf.variable_scope('shared_attention_mechanism', reuse=True):\r\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n        num_units = 128, \r\n        memory = encoder_out,\r\n        memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n\r\n# DECODER (PREDICTING)\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [BATCH_SIZE]),\r\n    end_token = 2,\r\n    initial_state = decoder_cell.zero_state(BATCH_SIZE * BEAM_WIDTH,tf.float32).clone(cell_state=encoder_state),\r\n    beam_width = BEAM_WIDTH,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\nwith tf.variable_scope('decode_with_shared_attention', reuse=True):\r\n    predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n        decoder = predicting_decoder,\r\n        impute_finished = False,\r\n        maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.predicted_ids[:, :, 0]\r\n\r\nprint('successful')\r\n```\r\n\r\nTo check sharing of trainable parameters execute the following code:\r\n```python\r\nfor var in tf.trainable_variables():\r\n    print(var)\r\n```\r\n\r\nFor the code above it will output:\r\n```bash\r\n<tf.Variable 'EmbedSequence/embeddings:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'shared_attention_mechanism/memory_layer/kernel:0' shape=(128, 128) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/dense/kernel:0' shape=(128, 10000) dtype=float32_ref>\r\n<tf.Variable 'decode_with_shared_attention/decoder/dense/bias:0' shape=(10000,) dtype=float32_ref>\r\n```\r\n But without wrapping of `tf.contrib.seq2seq.LuongAttention` and `tf.contrib.seq2seq.BeamSearchDecoder` into variable scopes it outputs:\r\n```bash\r\n<tf.Variable 'EmbedSequence/embeddings:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\r\n<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'Variable:0' shape=(10000, 128) dtype=float32_ref>\r\n<tf.Variable 'memory_layer/kernel:0' shape=(128, 128) dtype=float32_ref>\r\n<tf.Variable 'decoder/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\r\n<tf.Variable 'decoder/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'decoder/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\r\n<tf.Variable 'decoder/dense/kernel:0' shape=(128, 10000) dtype=float32_ref>\r\n<tf.Variable 'decoder/dense/bias:0' shape=(10000,) dtype=float32_ref>\r\n<tf.Variable 'memory_layer_1/kernel:0' shape=(128, 128) dtype=float32_ref>\r\n<tf.Variable 'decoder_1/attention_wrapper/basic_lstm_cell/kernel:0' shape=(384, 512) dtype=float32_ref>\r\n<tf.Variable 'decoder_1/attention_wrapper/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\r\n<tf.Variable 'decoder_1/attention_wrapper/attention_layer/kernel:0' shape=(256, 128) dtype=float32_ref>\r\n```\r\n where there are extra memory_layer_1/ and decoder_1/attention_wrapper/ being trained.\r\n"]}, {"number": 11903, "title": "Missing input file mpi:mpio.h", "body": "### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution: **Linux Ubuntu 16.04)**\r\n- TensorFlow installed from: **source**\r\n- TensorFlow version: **master**\r\n- Python version: **3.5.2**\r\n- Bazel version (if compiling from source): **0.5.2**\r\n- CUDA/cuDNN version: **8.0**\r\n- GPU model and memory: **K80**\r\n- Exact command to reproduce:\r\n\r\n```bash\r\n#!/usr/bin/env bash\r\n# Only the compilation step for tensorflow is in this script, for clarity.\r\n\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd ./tensorflow\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-8.0/targets/x86_64-linux/lib:/usr/lib/x86_64-linux-gnu/\r\nexport PYTHON_BIN_PATH=\"/home/ubuntu/anaconda3/bin/python\"\r\nexport PYTHON_LIB_PATH=\"/home/ubuntu/anaconda3/lib/python3.6/site-packages\"\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=0\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_CLANG=0\r\nexport TF_NEED_MPI=1\r\nexport MPI_HOME=\"/usr/lib/openmpi\"\r\nexport GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nexport CUDA_VERSION='8.0'\r\nexport CUDNN_VERSION='6'\r\nexport CUDNN_INSTALL_PATH=/usr/local/cuda\r\nexport CUDA_COMPUTE_CAPABILITIES='3.7'\r\nexport CUDA_PATH='/usr/local/cuda'\r\nexport CUDA_PATH_LINUX='/opt/cuda'\r\nyes \"\" | ./configure\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package && \\\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen turning on `mpi`, the compile fails with the following error:\r\n\r\n```error\r\nINFO: Found 1 target...\r\nERROR: missing input file '//third_party/mpi:mpicxx.h'.\r\nERROR: missing input file '//third_party/mpi:mpi.h'.\r\nERROR: missing input file '//third_party/mpi:mpio.h'.\r\nERROR: /home/ubuntu/scripts/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpicxx.h'.\r\nERROR: /home/ubuntu/scripts/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpio.h'.\r\nERROR: /home/ubuntu/scripts/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpi.h'.\r\n```\r\n\r\n### Source code / logs\r\nbefore running the build script I install bazel 0.5.2 (0.5.3 breaks the build)", "comments": ["I assume you're trying to use the contrib/mpi extension to TF.  Did you follow the configuration instructions to select this library?   See tensorflow/contrib/mpi/README.md    I have not used this package myself.  I'm suspicious that you're setting TF_NEED_MPI=0 in your env.  Is that correct?", "Hi Paul, @poxvoculi \r\n\r\nI'm using `TF_NEED_MPI=1` and `MPI_HOME=/usr/bin/openmpi`. The three header files the config needs are `mpi.h`, `mpio.h`, and `mpicxx.h`. `mpio.h` is not available in ubuntu16.04 but the other two are available in different folders.\r\n\r\nI updated the env parameters above to reflect this.\r\n\r\nThe README here doesn't say much about the setup. The default given during manual`./configure` for `MPI_HOME` is `/usr` but it also doesn't work.\r\n\r\nAny help would be greatly appreciated! I have looked through a whole bunch of CI build scripts in this repo but no prevail.\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/mpi\r\n\r\nThanks!", "I'm afraid I can't build with this package in the Google environment, and you'll need to find someone in the open-source community to help you.  Perhaps you can get some help on stackoverflow.   It's a contrib package contributed by @jbedorf.  Perhaps you don't have a full installation of MPI, or it's not an installation with which this package was tested.", "@poxvoculi Right. @jbedorf Hey @jeroen, which version of mpi did you build with? I'm on ubuntu 16.04.\r\n\r\nThanks!", "@episodeyang  \r\nI'm using the latest OpenMPI and MVAPICH versions, built from source. I'm not sure what the Ubuntu pre-built packages do and do not install. In this case it looks like the configuration script can not find the 'mpi_portable_platform.h' file and incorrectly assumes you are using MVAPICH. \r\nI would get the latest MVAPICH version which is confirmed to work (http://mvapich.cse.ohio-state.edu/downloads/  )\r\n\r\nNote that there has been a recent change which breaks the compilation until pull request #11935  is merged. ", "@episodeyang \r\nhello,I have the same problem,could you please tell me how to solve it?", "I am using OpenMPI 2.1.1 and getting the following error: \r\n\r\n`./third_party/mpi/mpi.h:2704:41: fatal error: openmpi/ompi/mpi/cxx/mpicxx.h: No such file or directory`\r\n\r\nIt seems like OpenMPI also needs the mpicxx.h header file but this is only generated with MVAPICH. ", "@ahaider3 Looks like you are building with C++ bindings, if you don't need those you can set this environment variable `OMPI_SKIP_MPICXX=1`. If you do need them you have to modify the TensorFlow configure file to add the mpicxx.h file. ", "Hi,\r\n\r\nI'm trying to do the same but I can't get it build. I'm not sure what I should do but I modified configure script to link mpicxx.h in third_party/mpi but that does not help.\r\n\r\nHere the content on the openmpi stuff on Debian:\r\n```\r\n/usr/lib/x86_64-linux-gnu/openmpi/\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_ext.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmca_common_sm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so.20.10.1\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_usempi_ignore_tkr.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libopen-rte.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_java.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmca_common_verbs.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_java.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libompitrace.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmca_common_sm.so.20.10.1\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/liboshmem.so.20.10.1\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmca_common_verbs.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmca_common_libfabric.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_iof_hnp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_slurm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_errmgr_default_hnp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_io_ompio.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_resilient.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_topo_basic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_dfs_app.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_staged_orted.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_mtl_ofi.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_routed_radix.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_errmgr_default_orted.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_iof_orted.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_shmem_mmap.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_io_romio314.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_vprotocol_pessimist.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ras_gridengine.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fbtl_posix.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_iof_tool.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_iof_mr_orted.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_hnp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_libnbc.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_errmgr_default_tool.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/libompi_dbg_msgq.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_self.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_osc_rdma.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_btl_sm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_round_robin.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_btl_tcp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_pml_ob1.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_ppr.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_spml_yoda.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fs_ufs.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_seq.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_dfs_orted.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fcoll_dynamic_gen2.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_osc_pt2pt.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_schizo_ompi.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_errmgr_default_app.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fcoll_dynamic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_memheap_buddy.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_sm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_oob_usock.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_pmi.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_routed_direct.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_plm_isolated.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_plm_slurm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_orted.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ras_simulator.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_pmix_pmix112.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_scoll_mpi.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_hnp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fcoll_static.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_atomic_basic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_sec_basic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_iof_mr_hnp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_odls_default.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rcache_grdma.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_dvm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_allocator_basic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rml_oob.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_pml_cm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_routed_binomial.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_patcher_overwrite.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_basic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fcoll_two_phase.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_oob_tcp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ras_loadleveler.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rtc_freq.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_sshmem_mmap.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_oob_ud.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_rank_file.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_grpcomm_direct.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_sharedfp_individual.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_plm_rsh.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_staged.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_sync.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_staged_hnp.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_tuned.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rtc_hwloc.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_shmem_posix.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_btl_self.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_sshmem_sysv.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_shmem_sysv.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_sharedfp_sm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_scoll_basic.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_notifier_syslog.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_mtl_psm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_coll_inter.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_mpool_hugepage.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_tool.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_allocator_bucket.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_filem_raw.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_tool.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_fcoll_individual.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_dfs_test.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_novm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_env.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_pstat_linux.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_rmaps_mindist.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_state_app.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_btl_openib.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_memheap_ptmalloc.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_bml_r2.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_osc_sm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ras_slurm.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_ess_singleton.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_routed_debruijn.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_btl_vader.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/openmpi/mca_sharedfp_lockedfile.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_usempif08.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_f08_interfaces.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libopen-pal.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmca_common_libfabric.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_f08_types.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_f08_interfaces_callbacks.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_f08_callbacks.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/ompi-cxx.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/opal.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/orte.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/ompi.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/ompi-f77.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/ompi-c.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/ompi-f90.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pkgconfig/ompi-fort.pc\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/pmpi_f08_interfaces.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_usempif08.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_mpifh.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_mpifh.so.20.11.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_usempi_ignore_tkr.so.20.10.0\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libopen-rte.so.20.10.1\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_f08_ext.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi_f08.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libompitrace.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libopen-pal.so.20.10.1\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/liboshmem.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/mpi.mod\r\n/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so\r\n/usr/lib/x86_64-linux-gnu/openmpi/include\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-io-handles.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pshmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-sentinels.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/shmem-compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal_config_top.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte_config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/communicator\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/communicator/comm_helpers.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/communicator/comm_request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/communicator/communicator.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/io\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/io/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/io/base/io_base_request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/io/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/io/io.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/osc\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/osc/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/osc/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/osc/base/osc_base_obj_convert.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/osc/osc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fs\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fs/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fs/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fs/fs.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/rte\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/rte/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/rte/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/rte/rte.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/rte/orte\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/rte/orte/rte_orte.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fbtl\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fbtl/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fbtl/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fbtl/fbtl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/vprotocol\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/vprotocol/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/vprotocol/base/vprotocol_base_request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/vprotocol/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/vprotocol/vprotocol.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/mca.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/mtl\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/mtl/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/mtl/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/mtl/base/mtl_base_datatype.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/mtl/mtl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/base/coll_tags.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/base/coll_base_util.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/base/coll_base_topo.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/base/coll_base_functions.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/coll/coll.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/topo\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/topo/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/topo/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/topo/topo.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base/pml_base_request_dbg.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base/pml_base_request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base/pml_base_bsend.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base/pml_base_sendreq.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/base/pml_base_recvreq.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/pml.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/pml/pml_constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fcoll\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fcoll/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fcoll/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/fcoll/fcoll.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/op\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/op/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/op/base/functions.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/op/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/op/op.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/bml\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/bml/bml.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/bml/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/bml/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/bml/base/bml_base_btl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/sharedfp\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/sharedfp/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/sharedfp/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mca/sharedfp/sharedfp.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/class\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/class/ompi_seq_tracker.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/runtime\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/runtime/ompi_info_support.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/runtime/mpiruntime.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/runtime/params.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/net\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/net/netpatterns_knomial_tree.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/net/netpatterns.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/net/coll_ops.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/comm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/comm/coll_ops.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/patterns/comm/commpatterns.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/memchecker.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/debuggers\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/debuggers/msgq_interface.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/debuggers/ompi_msgq_dll_defs.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/debuggers/debuggers.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/debuggers/ompi_common_dll_defs.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/datatype\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/datatype/ompi_datatype.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/datatype/ompi_datatype_internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/frameworks.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/info\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/info/info.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/file\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/file/file.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/totalview.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/request\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/request/request_dbg.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/request/grequest.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/request/request_default.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/request/request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/win\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/win/win.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/errhandler\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/errhandler/errcode.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/errhandler/errhandler.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/errhandler/errhandler_predefined.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/errhandler/errcode-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/affinity\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/affinity/c\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/affinity/c/mpiext_affinity_c.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/mpiext.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/cuda\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/cuda/c\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpiext/cuda/c/mpiext_cuda_c.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/dpm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/dpm/dpm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/proc\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/proc/proc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/peruse\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/peruse/peruse-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/version.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/op\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/op/op.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_GraphComm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Message.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Datatype.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Prequest.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Intercomm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Op.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_MPI.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_File.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Intracomm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Version.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_CartComm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Constant.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_GraphParms.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Group.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Comm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_CartParms.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Status.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_ShiftParms.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Count.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Win.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Errhandler.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/java/mpi_Info.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/java/mpiJava.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/tool\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/tool/mpit-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/tool/profile\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/tool/profile/defines.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/c\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/c/bindings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/fortran\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/fortran/mpif-h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/fortran/mpif-h/status-conversion.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/fortran/mpif-h/bindings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/fortran/mpif-h/prototypes_mpi.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/status.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/functions.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/op_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/cxx_glue.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/intracomm_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/info_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/file.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/win.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/exception.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/comm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/topology.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/mpicxx.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/functions_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/comm_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/topology_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/info.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/errhandler.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/datatype.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/group_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/win_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/intercomm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/intracomm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/status_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/datatype_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/op.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/errhandler_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/intercomm_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/request_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/group.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/mpi/cxx/file_inln.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/message\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/message/message.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/attribute\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/attribute/attribute.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/group\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/group/group_dbg.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi/group/group.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_framework.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_component_repository.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_var_enum.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_pvar.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_var.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_var_group.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/base/mca_base_vari.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool/base/mpool_base_tree.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool/mpool.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool/hugepage\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mpool/hugepage/mpool_hugepage.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/hwloc\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/hwloc/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/hwloc/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/hwloc/hwloc-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/if\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/if/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/if/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/if/if.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/changelist-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/WIN32-Code\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/WIN32-Code/event2\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/WIN32-Code/event2/event-config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/WIN32-Code/tree.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evrpc-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/ipv6-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/ratelim-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/tag_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/event-config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/keyvalq_struct.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/http_struct.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/bufferevent_struct.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/buffer_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/event_struct.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/bufferevent.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/dns_struct.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/tag.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/dns_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/buffer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/thread.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/listener.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/bufferevent_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/rpc_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/http_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/bufferevent_ssl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/rpc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/http.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/rpc_struct.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/event_compat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/util.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/event2/event.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evsignal-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/http-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evrpc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evutil.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/event-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/mm-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evhttp.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evbuffer-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/defer-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/compat\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/compat/sys\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/compat/sys/queue.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evmap-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/ht-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/iocp-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/opal_rename.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/strlcpy-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/minheap-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/util-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evthread-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/evdns.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/bufferevent-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/log-internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/event.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent2022.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/event.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memory\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memory/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memory/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memory/base/empty.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memory/memory.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pstat\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pstat/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pstat/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pstat/pstat.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/timer\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/timer/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/timer/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/timer/base/timer_base_null.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/timer/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/mca.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/patcher\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/patcher/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/patcher/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/patcher/patcher.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/allocator\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/allocator/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/allocator/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/allocator/allocator.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/dl\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/dl/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/dl/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/dl/dl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/shmem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/shmem/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/shmem/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/shmem/shmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/shmem/shmem_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/backtrace\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/backtrace/backtrace.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/backtrace/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/backtrace/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/sec\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/sec/sec.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/sec/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/sec/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/base/pmix_base_hash.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/base/pmix_base_fns.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/pmix_server.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/pmix.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/pmix/pmix_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memcpy\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memcpy/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memcpy/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memcpy/base/memcpy_base_default.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memcpy/memcpy.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/base/rcache_base_vma_tree.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/base/rcache_base_vma.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/base/rcache_base_mem_cb.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/grdma\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/grdma/rcache_grdma.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/rcache/rcache.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memchecker\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memchecker/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memchecker/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/memchecker/memchecker.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/installdirs\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/installdirs/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/installdirs/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/installdirs/installdirs.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/btl\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/btl/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/btl/base/btl_base_error.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/btl/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/btl/btl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/libfabric\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/libfabric/common_libfabric.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/sm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/sm/common_sm_mpool.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/sm/common_sm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/verbs\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/common/verbs/common_verbs.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_free_list.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_fifo.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_ring_buffer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_lifo.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_pointer_array.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_tree.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_graph.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_rb_tree.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_value_array.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_hash_table.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_hotel.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_object.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_list.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/class/opal_bitmap.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/prefetch.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/architecture.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/arm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/arm/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/arm/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/ia32\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/ia32/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/ia32/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/gcc_builtin\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/gcc_builtin/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/mips\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/mips/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/mips/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/x86_64\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/x86_64/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/x86_64/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/arm64\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/arm64/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/arm64/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/sparcv9\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/sparcv9/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/sparcv9/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/atomic_impl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/ia64\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/ia64/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/ia64/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/sync_builtin\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/sync_builtin/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/osx\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/osx/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/powerpc\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/powerpc/timer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/powerpc/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/sys/cma.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/runtime\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/runtime/opal_progress.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/runtime/opal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/runtime/opal_info_support.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/runtime/opal_params.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/runtime/opal_progress_threads.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/align.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/hash_string.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_prototypes.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_unpack.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_memcpy.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_convertor.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_copy.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_checksum.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_convertor_internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/datatype/opal_datatype_pack.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/wait_sync.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/condition.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/mutex_unix.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/tsd.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/threads.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/mutex.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/threads/thread_usage.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/frameworks.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/opal_portable_platform.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/dss\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/dss/dss_internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/dss/dss.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/dss/dss_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/errhandler\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/errhandler/opal_errhandler.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/memoryhooks\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/memoryhooks/memory.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/memoryhooks/memory_internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/version.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/opal_getcwd.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/net.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/os_dirpath.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/crc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/malloc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/show_help.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/basename.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/if.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/alfg.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/argv.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/os_path.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/path.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/sys_limits.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/daemon_init.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/printf.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/timings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/bit_ops.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/few.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/arch.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/keyval_parse.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/uri.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/qsort.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/error.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/cmd_line.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/show_help_lex.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/proc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/strncpy.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/fd.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/stacktrace.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/output.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/numtostr.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/opal_pty.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/util/opal_environ.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/opal_socket_errno.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal_stdint.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/errmgr\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/errmgr/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/errmgr/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/errmgr/base/errmgr_private.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/errmgr/errmgr.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/state\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/state/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/state/base/state_private.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/state/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/state/state.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/state/state_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/odls\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/odls/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/odls/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/odls/base/odls_private.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/odls/odls_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/odls/odls.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rmaps\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rmaps/rmaps_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rmaps/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rmaps/base/rmaps_private.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rmaps/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rmaps/rmaps.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/plm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/plm/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/plm/base/plm_private.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/plm/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/plm/plm_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/plm/plm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/schizo\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/schizo/schizo.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/schizo/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/schizo/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/routed\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/routed/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/routed/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/routed/routed_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/routed/routed.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/notifier\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/notifier/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/notifier/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/notifier/notifier.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rml\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rml/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rml/base/rml_contact.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rml/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rml/rml_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rml/rml.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/mca.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/filem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/filem/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/filem/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/filem/filem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/grpcomm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/grpcomm/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/grpcomm/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/grpcomm/grpcomm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ess\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ess/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ess/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ess/ess.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/iof\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/iof/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/iof/base/iof_base_setup.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/iof/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/iof/iof_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/iof/iof.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ras\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ras/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ras/base/ras_private.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ras/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ras/ras.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/ras/ras_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/dfs\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/dfs/dfs_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/dfs/dfs.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/dfs/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/dfs/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rtc\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rtc/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rtc/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/rtc/rtc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/oob\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/oob/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/oob/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/mca/oob/oob.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/orte_data_server.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/runtime_internals.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/orte_locks.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/orte_globals.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/orte_wait.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/orte_info_support.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/orte_quit.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/runtime.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/data_type_support\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/runtime/data_type_support/orte_dt_support.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/orted\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/orted/orted.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/orted/pmix\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/orted/pmix/pmix_server.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/orted/pmix/pmix_server_internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/frameworks.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/version.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/nidmap.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/name_fns.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/show_help.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/pre_condition_transports.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/parse_options.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/dash_host\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/dash_host/dash_host.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/comm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/comm/comm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/error_strings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/hostfile\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/hostfile/hostfile.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/hostfile/hostfile_lex.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/hnp_contact.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/attr.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/listener.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/context_fns.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/regex.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/proc_info.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/orte/util/session_dir.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/atomic\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/atomic/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/atomic/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/atomic/atomic.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/mca.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/memheap\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/memheap/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/memheap/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/memheap/memheap.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/sshmem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/sshmem/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/sshmem/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/sshmem/sshmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/sshmem/sshmem_types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base/spml_base_request_dbg.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base/spml_base_getreq.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base/spml_base_putreq.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base/spml_base_atomicreq.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/base/spml_base_request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/spml/spml.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/scoll\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/scoll/base\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/scoll/base/base.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/mca/scoll/scoll.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/runtime\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/runtime/oshmem_info_support.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/runtime/runtime.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/runtime/oshmem_shmem_preconnect.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/runtime/params.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/info\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/info/info.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/request\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/request/request_dbg.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/request/request.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/proc\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/proc/proc_group_cache.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/proc/proc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/shmem_lock.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/c\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/c/profile\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/c/profile/defines.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/bindings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/shmem_fortran_pointer.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/prototypes_shmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/profile\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/profile/pbindings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/profile/defines.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/fortran/profile/prototypes_pshmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/shmem/shmem_api_logger.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/op\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/op/op.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/util\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/oshmem/util/oshmem_util.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal_config_bottom.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/ompi_config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal_config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpi.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-externals.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-sizeof.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/shmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/shmem.fh\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/shmemx.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pshmemx.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/class\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/class/pmix_list.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/class/pmix_object.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/class/pmix_hash_table.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/class/pmix_value_array.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/class/pmix_pointer_array.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/dstore\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/dstore/pmix_dstore.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/dstore/pmix_esh.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/client\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/client/pmix_client_ops.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/include\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/include/pmix_jobdata.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/include/pmix_globals.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/sm\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/sm/pmix_sm.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/sm/pmix_mmap.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/buffer_ops\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/buffer_ops/types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/buffer_ops/buffer_ops.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/buffer_ops/internal.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/sec\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/sec/pmix_native.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/sec/pmix_sec.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/usock\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/usock/usock.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/crc.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/basename.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/argv.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/os_path.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/printf.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/timings.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/progress_threads.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/pmix_environ.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/error.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/fd.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/output.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/strnlen.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/util/hash.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/server\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/pmix/src/server/pmix_server_ops.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-ext.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpi_portable_platform.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem/oshmem\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem/oshmem/types.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem/oshmem/constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem/oshmem/frameworks.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem/oshmem/version.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/openshmem/oshmem_config.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-c-constants-decl.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-handles.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpp\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpp/shmem.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpp/shmem.fh\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpif-io-constants.h\r\n/usr/lib/x86_64-linux-gnu/openmpi/include/mpi-ext.h\r\n```\r\n\r\nAny hint ?\r\n\r\nThanks !", "@eLvErDe I would try this: https://github.com/tensorflow/tensorflow/pull/12299\r\n\r\nIt uses MPI collective operations for distributed computation. ", "@eLvErDe have you tried setting the following environment variable: `OMPI_SKIP_MPICXX=1` ? Alternatively you can try using an OpenMPI distribution built from source where you do not enable the C++ bindings. ", "@jbedorf @eLvErDe @ahaider3 @xgh45 @episodeyang \r\nI have successfully tweaked the variable OMPI_SKIP_MPICXX for a successful building (for trunk version of tensorflow).\r\nHowever, it seems that we have to use an enviroment variable CC_OPT_FLAGS=\"-DOMPI_SKIP_MPICXX=1 -march=native\" instead. That's because only in this way we can pass the variable into the source code layer and bypass the original definition of the macro OMPI_SKIP_MPICXX in the source code (for this case mpi.h). CC_OPT_FLAGS is the variable taken by configure.py (for trunk version of tensorflow) to assign the values for --copt & --cxxopt in bazel build process.", "Indeed, worked for me. I passed the -D as cflags optimization when being prompted by configure script", "There is no need to pass any extra flags during build. I downloaded OpenMPI 3.0.0 source and compiled it with C++ support although C++ support is deprecated. Then pointed tensorflow to use new installation of OpenMPI. After that it compiled fine.", "I'm encountering ~this same~ a similar error when compiling the tf1.5 release from source. Would someone mind posting the flags or a super obvious command line BASH script that enables MPI correctly works with ubuntu 16.04?\r\n\r\nmy error:\r\n```\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (267 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: missing input file '//third_party/mpi:mpi.h'\r\nERROR: /home/ahundt/src/tensorflow/third_party/mpi/BUILD:18:1: //third_party/mpi:mpi: missing input file '//third_party/mpi:mpi.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/ahundt/src/tensorflow/third_party/mpi/BUILD:18:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 29.868s, Critical Path: 8.44s\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI also tried \r\n`export MPI_HOME=/usr`\r\nwhich got:\r\n```\r\nWARNING: /home/ahundt/src/tensorflow/tensorflow/core/BUILD:1807:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/ahundt/src/tensorflow/tensorflow/tensorflow.bzl:1138:30\r\nWARNING: /home/ahundt/src/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/ahundt/src/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (267 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: missing input file '//third_party/mpi:mpi.h'\r\nERROR: /home/ahundt/src/tensorflow/tensorflow/python/BUILD:3055:1: //tensorflow/python:pywrap_tensorflow_internal_py_wrap: missing input file '//third_party/mpi:mpi.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/ahundt/src/tensorflow/tensorflow/python/BUILD:3055:1 1 input file(s) do not exist\r\nINFO: Elapsed time: 20.864s, Critical Path: 0.04s\r\nFAILED: Build did NOT complete successfully\r\n-> [1]\r\n```\r\nAdditionally the following options for MPI_HOME didn't work:\r\n`/usr/include/openmpi`, `/user/lib/openmpi`", "Did the configure script place a symbolic link from your MPI install folder's `mpi.h`  into the `third_party/mpi` folder? This error would indicate that it even failed to do that step (which is slightly different from the `mpio.h` errors reported by others.)", "looks like it tried:\r\n```\r\n) ls -alh ~/src/tensorflow/third_party/mpi/*\r\n-rw-rw-r-- 1 ahundt ahundt 483 Aug 11 22:15 /home/ahundt/src/tensorflow/third_party/mpi/BUILD\r\nlrwxrwxrwx 1 ahundt ahundt  18 Jan 28 18:37 /home/ahundt/src/tensorflow/third_party/mpi/libmpi.so -> /usr/lib/libmpi.so\r\n-rw-rw-r-- 1 ahundt ahundt 533 Jan 28 18:37 /home/ahundt/src/tensorflow/third_party/mpi/mpi.bzl\r\nlrwxrwxrwx 1 ahundt ahundt  21 Jan 28 18:37 /home/ahundt/src/tensorflow/third_party/mpi/mpicxx.h -> /usr/include/mpicxx.h\r\nlrwxrwxrwx 1 ahundt ahundt  19 Jan 28 18:37 /home/ahundt/src/tensorflow/third_party/mpi/mpi.h -> /usr//include/mpi.h\r\nlrwxrwxrwx 1 ahundt ahundt  19 Jan 28 18:37 /home/ahundt/src/tensorflow/third_party/mpi/mpio.h -> /usr/include/mpio.h\r\nlrwxrwxrwx 1 ahundt ahundt  48 Jan 28 18:23 /home/ahundt/src/tensorflow/third_party/mpi/mpi_portable_platform.h -> /usr/lib/openmpi/include/mpi_portable_platform.h\r\n```\r\nBasically it seems to assume everything would be in:\r\n```\r\n/path/to/mpi/include/\r\n/path/to/mpi/lib/\r\n```\r\n\r\nHmm perhaps `/usr/lib/openmpi` is supposed to be the one that works?\r\n```\r\n) ls /usr/lib/openmpi\r\ninclude  lib\r\nahundt@femur|~/src\r\n) ls /usr/lib/openmpi/lib\r\nopenmpi                            libompitrace.so\r\npkgconfig                          libompitrace.so.0.0.0\r\nlibmca_common_sm.so                libopen-pal.so\r\nlibmca_common_sm.so.4.0.4          libopen-pal.so.13.0.2\r\nlibmca_common_verbs.so             libopen-rte.so\r\nlibmca_common_verbs.so.7.0.0       libopen-rte.so.12.0.2\r\nlibmpi_cxx.so                      liboshmem.so\r\nlibmpi_cxx.so.1.1.3                mpi_ext.mod\r\nlibmpi_mpifh.so                    mpi_f08_ext.mod\r\nlibmpi_mpifh.so.12.0.0             mpi_f08_interfaces_callbacks.mod\r\nlibmpi.so                          mpi_f08_interfaces.mod\r\nlibmpi.so.12.0.2                   mpi_f08.mod\r\nlibmpi_usempif08.so                mpi_f08_types.mod\r\nlibmpi_usempif08.so.11.1.0         mpi.mod\r\nlibmpi_usempi_ignore_tkr.so        pmpi_f08_interfaces.mod\r\nlibmpi_usempi_ignore_tkr.so.6.1.0\r\n) ls /usr/lib/openmpi/include\r\nmpp               mpif-externals.h     mpif-sentinels.h         shmem-compat.h\r\nopenmpi           mpif-ext.h           mpif-sizeof.h            shmem.fh\r\nopenshmem         mpif.h               mpi.h                    shmem.h\r\nmpi-ext.h         mpif-handles.h       mpi_portable_platform.h  shmemx.h\r\nmpif-config.h     mpif-io-constants.h  pshmem.h\r\nmpif-constants.h  mpif-io-handles.h    pshmemx.h\r\n```", "Yes, give it a shot with setting: `export MPI_HOME=/usr/lib/openmpi` that should result in correct symlinks.", "Closing as this is resolved"]}, {"number": 11902, "title": "//tensorflow/python/kernel_tests:denormal_test test failure on ppc64le (error : Arrays are not equal)", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n     Installed from source\r\n- **TensorFlow version (use command below)**:\r\n    ('v1.2.1-0-gb4957ff', '1.2.1')\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n      0.4.5-2017-07-13 (@037b9b9)\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\nbazel test --test_output=errors //tensorflow/python/kernel_tests:denormal_test\r\n\r\n\r\n### Describe the problem\r\nFollowing code returning incorrect results -\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/kernel_tests/denormal_test.py#L36-L44\r\n\r\n```\r\ndef _flushDenormalsTest(self, use_gpu, dtypes):\r\n    with self.test_session(use_gpu=use_gpu):\r\n      array_ops.identity(7).eval()\r\n      for dtype in dtypes:\r\n        tiny = np.finfo(dtype).tiny\r\n        # Small shape to test main thread, large shape to test thread pool\r\n        for shape in (), (1 << 20,):\r\n          flush = 0.1 * constant_op.constant(tiny, shape=shape)\r\n          self.assertAllEqual(flush.eval(), np.zeros(shape))\r\n```\r\n\r\nI have printed the values of `flush.eval() `and `np.zeros(shape)` , see below :\r\n`flush.eval()` o/p =  `1.17549463108e-39`\r\n`np.zeros(shape)` o/p = `0.0`\r\n\r\nGetting failure `1.17549463108e-39` vs expected `0.0`.  I am not able to understand why we are getting flush.eval() o/p `1.17549463108e-39`  and not  `0.0`.  But I tried changing the  comparison function from assertAllEqual to assertAllClose and test is passing successfully. Is it OK to merge this changes or ? I would like to hear comment on this.Thanks!\r\n\r\nThis test passing successfully in TF1.0.1 without any changes, see relevant code : \r\nhttps://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/python/kernel_tests/denormal_test.py#L31-L48\r\n\r\n```\r\n def testPythonHasDenormals(self):\r\n    \"\"\"Non-tf numpy code should treat denormals correctly.\"\"\"\r\n    for dtype in np.float32, np.float64:\r\n      tiny = np.finfo(dtype).tiny\r\n      self.assertEqual(tiny, tiny / 16 * 16)\r\n\r\n  def _flushDenormalsTest(self, use_gpu, dtypes):\r\n    if control_imports.USE_OSS:\r\n      # TODO(irving): Fix denormal flushing for open source.\r\n      return\r\n    with self.test_session(use_gpu=use_gpu):\r\n      array_ops.identity(7).eval()\r\n      for dtype in dtypes:\r\n        tiny = np.finfo(dtype).tiny\r\n        # Small shape to test main thread, large shape to test thread pool\r\n        for shape in (), (1 << 20,):\r\n          flush = 0.1 * constant_op.constant(tiny, shape=shape)\r\n          self.assertAllEqual(flush.eval(), np.zeros(shape))\r\n```\r\nLooks like this test is disabled in TF1.0.1\r\n\r\n### Source code / logs\r\n```\r\n$ bazel test --test_output=errors //tensorflow/python/kernel_tests:denormal_test\r\n\r\n==================== Test output for //tensorflow/python/kernel_tests:denormal_test:\r\n2017-07-31 08:45:48.796363: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-07-31 08:45:48.798909: I tensorflow/compiler/xla/service/service.cc:198] XLA service 0x100386c7120 executing computations on platform Host. Devices:\r\n2017-07-31 08:45:48.798936: I tensorflow/compiler/xla/service/service.cc:206]   StreamExecutor device (0): <undefined>, <undefined>\r\nFF..\r\n======================================================================\r\nFAIL: testFlushDenormalsCPU (__main__.DenormalTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py\", line 50, in testFlushDenormalsCPU\r\n    self._flushDenormalsTest(use_gpu=False, dtypes=(np.float32, np.float64))\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py\", line 44, in _flushDenormalsTest\r\n    self.assertAllEqual(flush.eval(), np.zeros(shape))\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 699, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 871, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 100.0%)\r\n x: array(1.1754946310819804e-39, dtype=float32)\r\n y: array(0.0)\r\n\r\n======================================================================\r\nFAIL: testFlushDenormalsGPU (__main__.DenormalTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py\", line 54, in testFlushDenormalsGPU\r\n    self._flushDenormalsTest(use_gpu=True, dtypes=(np.float32,))\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/denormal_test.py\", line 44, in _flushDenormalsTest\r\n    self.assertAllEqual(flush.eval(), np.zeros(shape))\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/denormal_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 699, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 871, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nArrays are not equal\r\n\r\n(mismatch 100.0%)\r\n x: array(1.1754946310819804e-39, dtype=float32)\r\n y: array(0.0)\r\n\r\n----------------------------------------------------------------------\r\nRan 4 tests in 0.071s\r\n\r\nFAILED (failures=2)\r\nnot equal lhs =  1.17549463108e-39\r\nnot equal rhs =  0.0\r\nnot equal lhs =  1.17549463108e-39\r\nnot equal rhs =  0.0\r\n\r\n```", "comments": ["I have done some investigation on this, looks like we need to add Power specific code in  https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/platform/denormal.cc#L43-L73  \r\n\r\n", "I was discussing about this test failure on power porting mailing list (power-port-tune@lists.linux.ibm.com) , see below- \r\n> Hi,\r\n> \r\n> I am trying to port the TensorFlow code to Power, I'm looking at \r\n> some code which uses certain Intel intrinsics and want to know if \r\n> there are equivalent Power equivalents \r\n> \r\n> The code I'm trying to port uses these intrinsics:\r\n> \r\n> _MM_GET_FLUSH_ZERO_MODE\r\n> _MM_GET_DENORMALS_ZERO_MODE\r\n> _MM_SET_FLUSH_ZERO_MODE \r\n> _MM_SET_DENORMALS_ZERO_MODE\r\n> _MM_SET_FLUSH_ZERO_MODE \r\n> \r\n> Would there be corresponding intrinsics on Power? \r\n> \r\n> Just for reference, the functions I'm trying to port are these : \r\n> https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/core/platform/denormal.cc#L43-L73 \r\n> (ScopedFlushDenormal::ScopedFlushDenormal() andScopedFlushDenormal::~ScopedFlushDenormal() )\r\n> \r\n> Any suggestions would help!\r\n> \r\n> Thanks!\r\n> Sandip__\r\n\r\nAnd I got following reply - \r\n\r\nThis is not an intrinsic, it is mode and a nonstandard (not IEEE754 conforming) one at that. \r\n\r\nSome embedded machines perform slight better in this mode, but POWER servers do not. So for POWER you do not need this and should simply disable it.\r\n\r\n// For now, we flush denormals only on SSE 3. Other architectures such as ARM\r\n\r\n\\// can be added as needed.\r\n\r\n#ifdef DENORM_USE_INTRINSICS \r\n\r\nSo find out where DENORM_USE_INTRINSICS is #define and #undef it.\r\n\r\n@drpngx @gunan @girving  @mrry   -  Is it OK to disable or don't execute this test on power ?", "Sgtm\n\nOn Jul 31, 2017 11:58 PM, \"sandipmgiri\" <notifications@github.com> wrote:\n\n> I was discussing about this test failure on power porting mailing list (\n> power-port-tune@lists.linux.ibm.com) and they mentioned as follows-\n>\n> _MM_GET_FLUSH_ZERO_MODE\n> _MM_GET_DENORMALS_ZERO_MODE\n> _MM_SET_FLUSH_ZERO_MODE\n> _MM_SET_DENORMALS_ZERO_MODE\n>\n> This is not an intrinsic, it is mode and a nonstandard (not IEEE754\n> conforming) one at that.\n>\n> Some embedded machines perform slight better in this mode, but POWER\n> servers do not. So for POWER you do not need this and should simply disable\n> it.\n>\n> @drpngx <https://github.com/drpngx> @irving@naml.us @\n> derek.murray@gmail.com - Is it OK to disable this test on power ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11902#issuecomment-319284836>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbfArRQalyQG2xvf5LBioWPYlLoWuks5sTsx4gaJpZM4OoGhK>\n> .\n>\n", "Also SGTM,\r\nIf I recall correctly, we also have a check in there to conditionally skip this test anyway.", "Fixed in PR https://github.com/tensorflow/tensorflow/pull/11939 "]}, {"number": 11901, "title": "Make Windows Bazel GPU build work again", "body": "This change mainly fixed the Windows GPU build.\r\n\r\nBesides it also has some other fixes and improvments.\r\n\r\n1. print installed Bazel version in configure.py\r\n\r\n2. Remove slice_op_gpu.cu.cc from strided_slice_op because it's already in slice_op\r\n\r\n3. Mark some failing GPU tests as no_windows_gpu\r\n\r\n4. Add /DNOGDI flag for some targets to fix GPU build\r\n\r\n5. Update TF_CUDNN_VERSION to 6.0\r\n\r\n6. add test filters -no_gpu, -no_pip_gpu\r\n\r\n\r\n@gunan ", "comments": ["Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "@meteorcloudy could you sync your branch? then the change is ok to merge.", "Great, It's done!"]}, {"number": 11900, "title": "patch is not installed on some of Windows slaves", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/1335/console\r\nOn win0-slave:\r\n```\r\n16:57:47 Analyzing: target //tensorflow/tools/pip_package:build_pip_package\r\n16:58:28 ERROR: C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/tools/pip_package/BUILD:70:1: error loading package 'tensorflow/contrib/session_bundle': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': Traceback (most recent call last):\r\n16:58:28 \tFile \"C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/workspace.bzl\", line 119\r\n16:58:28 \t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n16:58:28 \tFile \"C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/workspace.bzl\", line 110, in _apply_patch\r\n16:58:28 \t\t_execute_and_check_ret_code(repo_ctx, cmd)\r\n16:58:28 \tFile \"C:/tf_jenkins/home/workspace/tf-master-win-bzl/tensorflow/workspace.bzl\", line 94, in _execute_and_check_ret_code\r\n16:58:28 \t\tfail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\n16:58:28 Non-zero return code(127) when executing 'C:/tools/msys64/usr/bin/bash -c patch -p1 -d C:/tmp/_bazel_system/424zmya1/external/protobuf_archive -i C:/tf_jenkins/home/workspace/tf-master-win-bzl/third_party/protobuf/add_noinlines.patch':\r\n16:58:28 Stdout: \r\n16:58:28 Stderr: /usr/bin/bash: patch: command not found\r\n16:58:28  and referenced by '//tensorflow/tools/pip_package:si\r\n```", "comments": ["@gunan @yifeif ", "Hi Yun, they should be installed now. I rebuild a few machines(0, 1 and 3) last week. Let me know if you see anything else missing. Thanks!", "Hi Yifei, I still see the same error on win3-slave:\r\nhttps://ci.tensorflow.org/job/tf-master-win-bzl/1344/console", "sorry, should be fixed for 3 for reals! Passed the patch part: http://ci.tensorflow.org/job/tf-master-win-bzl/1347/console", "Great, thanks for fixing this! But the Bazel version seems to fall back to 0.4.5, because we didn't update the Windows slave setup script. I've sent a CL to do that. ;)"]}, {"number": 11899, "title": "Where can I find c++ head file \"tensorflow/cc/ops/sparse_ops.h\"", "body": "I want to use the sparse tensor in C++ API(https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/add-sparse-to-tensors-map). \r\nBut when I search in \"https://github.com/tensorflow/tensorflow\", there is no \"tensorflow/cc/ops/sparse_ops.h\"\r\nDoes anyone know how to use the sparse tensor in C++?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(That file is generated by the build system and located in `bazel-genfiles/tensorflow/cc/ops/sparse_ops.h` when you build from source)", "@asimshankar please can specify where bazel-genfiles/ is ? in ubuntu"]}, {"number": 11898, "title": "Update the graph used in quantization tutorial", "body": "_Another fix to quantization tutorial after #11285_\r\n\r\nCurrently, following the steps in the [quantization tutorial](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/performance/quantization.md#how-can-you-quantize-your-models) will lead to weird results as pointed out in #11181. This is because the graph file and the evaluation script are unmatched.\r\n\r\nFrom this [image recognition tutorial](https://www.tensorflow.org/tutorials/image_recognition), there are two pretrained graphs with two evaluation scripts.\r\n1. `classify_image_graph_def.pb` in [`inception-2015-12-05.tgz`](http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz) is to be used with the (older) [`classify_image.py`](https://github.com/tensorflow/models/blob/master/tutorials/image/imagenet/classify_image.py).\r\n1. `inception_v3_2016_08_28_frozen.pb` in [`inception_v3_2016_08_28_frozen.pb.tar.gz`](https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz) is to be used with [`label_image`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image).\r\n\r\nThe weird results came from using `classify_image_graph_def.pb` with `label_image`. The graphs are different (see also [models#1314](https://github.com/tensorflow/models/issues/1314)), especially in the last layer. The two scripts extract information from the last layer in different ways, so it works with its corresponding graph but not the other.\r\n\r\n### Further changes?\r\n1. In the [image recognition tutorial](https://www.tensorflow.org/tutorials/image_recognition) ([source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/image_recognition.md)), the usage of Python API points to option 1 (above) but the C++ API points to option 2. This might cause confusion as the two graphs/scripts are not equivalent. Now that there's a [python implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py) of the newer `label_image` by @freedomtan, should we update the tutorial to use this python implementation instead?\r\n1. Should this [graph transform README](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/README.md) be updated as well? Currently it uses the 2015 pre-trained graph, hence `--inputs='Mul' --outputs='softmax'`. If it is updated to the 2016 graph, these should instead say `--inputs=input --outputs=InceptionV3/Predictions/Reshape_1`.", "comments": ["@korrawat, thanks for your PR! By analyzing the history of the files in this pull request, we identified @petewarden, @martinwicke and @Androbin to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11897, "title": "Run mnist_with_summaries.py error\uff1acan not open shared object libcupti.so.8.o", "body": "error info as below:\r\n`2017-07-31 11:18:47.859639: I tensorflow/stream_executor/dso_loader.cc:129] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: \r\n2017-07-31 11:18:47.859724: F ./tensorflow/stream_executor/lib/statusor.h:205] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.8.0; dlerror: libcupti.so.8.0: cannot open shared object file: No such file or directory\r\n`\r\n\r\n_____________________________________________\r\nI have add the path to libcupti.so.8.0 to LD_LIBRARY_PATH, but no use.\r\n`export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH`\r\n\r\nif you have any ideas to solve this, please leave your message.", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "Note that the error message says that your LD_LIBRARY_PATH is empty. I would double check that environment is set correctly", "What does your current value of $LD_LIBRARY_PATH say and how does it compare to the instructions [here](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/#environment-setup)? Note that depending on your system you might not have an install path that includes the '-8.0', it might just have to be set as '/usr/local/cuda/lib64'.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "Hi!\r\nI have the same problem with tensorflow 1.6 and cuda 9.0. I am using Pycharm editor and LD_LIBRARY_PATH contains /usr/local/cuda-9.0/extras/CUPTI/lib64, I check LD_LIBRARY_PATH in Pycharm terminal.\r\n\r\n2018-03-12 16:23:37.319154: I tensorflow/stream_executor/dso_loader.cc:141] Couldn't open CUDA library libcupti.so.9.0. LD_LIBRARY_PATH: \r\n2018-03-12 16:23:37.319179: F ./tensorflow/stream_executor/lib/statusor.h:212] Non-OK-status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.9.0; dlerror: libcupti.so.9.0: cannot open shared object file: No such file or directory\r\n", "@laetitiaoist  it seems I have the problem like you ,did you solve this problem?", "Sorry yes, I solved by installing it another way:\r\nsudo apt-get install libcupti-dev\r\n\r\nI had the library in my cuda folder, I changed LD_LIBRARY_PATH but only this other install worked!", "@laetitiaoist  Thank you for your kind reply ! I will try it ,hoping it will work for me .", "Hi,\r\nI am having the same problem:\r\n`I tensorflow/stream_executor/dso_loader.cc:141] Couldn't open CUDA library libcupti.so.9.1. LD_LIBRARY_PATH: /usr/local/nccl_2.1.15-1+cuda9.1_x86_64/lib:/usr/local/cuda/extras/CUPTI/lib64`\r\n\r\n`F tensorflow/compiler/xla/statusor.cc:33] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcupti.so.9.1; dlerror: libcupti.so.9.1: cannot open shared object file: No such file or directory`\r\n\r\nI installed tensorflow (version 1.8.0) from source. Seems like tensorflow is attempting to look for `libcupti.so.9.1`, whereas I have `libcupti.so.9.2` installed.\r\n\r\nAny clues what to do in this case? Thanks in advance. ", "I met the same issue.\r\nTry to find where libcupti.so.9.0 (or different version) is and add it into LD_LIBRARY_PATH fixed this issue.\r\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-9.0/lib64:/usr/local/cuda-9.0/extras/CUPTI/lib64\"", "For me what solved it was to sudo cp the file from /usr/local/cuda-9.0/extras/CUPTI/lib64 to /usr/local/cuda-9.0/lib64/, leaving the LD_LIBRARY_PATH unchanged.", "hi, I met the same issus.\r\nI used pycharm with `Run the python file`.  it didn't work.\r\nbut when I use Terminal with `python3 filename.py` .  It  work!\r\nso, if U are same with me. you can try like me"]}, {"number": 11896, "title": "[Feature request] Dynamically add new machines in distributed TensorFlow", "body": "I'm not sure this has been raised before. I did some search on Google and haven't found relevant stuff. If it do exist, please direct me there. Thank you.\r\n\r\nI'm currently experimenting with distributed TensorFlow. When building a distributed cluster, all machines in the cluster should be fed into tf.train.Server as parameters. That is, the disitributed cluster configuration is defined when building the computation graph. Like the example provided in https://github.com/tensorflow/models/blob/master/inception/inception/imagenet_distributed_train.py.\r\n\r\nBut I have also read papers about robust distributed cluster that it would be nice if the framework support dynamically adding or removing machines if the cluster get larger or some machine goes down.\r\n\r\nIs this doable in current version of TensorFlow. If so, is there an example to implement this? If not, is there plans for this?", "comments": ["There's some discussion of this [here](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!msg/discuss/Okb2jKqYwmQ/yxyxXem0BwAJ). Yahoo and TensorflowOnHadoop have said in the past that they are working with TF team to add this", "I went through a little bit of the discussion. Given the current session implementation does not support dynamic partitioning of the graph, following the design of `MonitoredSession`, one proposed solution could be:\r\n\r\n1. Prepare for a scaling by writing a checkpoint to external storage\r\n2. Terminate current session\r\n3. Re-do the partitioning of graph with respect to the new cluster_def\r\n4. Create a new session and recover from the checkpoint", "@mrry, @saeta, care to comment. Are you working on this, should we mark this contributions welcome. Do you have any ideas on the design in case we do.", "We now have some of basics in place (i.e. ClusterSpec propagation, and ClusterResolvers) to allow this sort of thing, however we do not have anyone working on this now as far as I know. The latest tentative designs are pretty much identical to what @byronyi described in his comment above.", "How about just scaling the workers? The variables don't need to be re-partitioned when a new worker comes in and the worker just need to build a same graph and pin the variables to the already existed parameter servers. Could that be a little easier to implement?", "@hongjic for async parameter server architecture\u00a0this is already possible. IE, training begins as soon as all parameter server shards are up, and at at least one worker is up. As more worker come online, the training throughput increases.", "Thank you the info provided above. It seems that dynamic scaling of the cluster is not supported yet. Is there any clear plan for supporting that?\r\n\r\n@yaroslavvb Correct me if my understanding is wrong. What you mentioned for async parameter server is applicable for data parallelism? When more workers come online, the worker simply build same graph and request data to training the model and transfer updates to parameter server. Then, the effectively batchsize of the cluster increases so we can have larger learning rate as described in [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://research.fb.com/publications/imagenet1kin1h/). Do we have a mechanism for dynamically adjust hyperparamters?\r\n\r\nBTW, in [TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems](https://arxiv.org/pdf/1603.04467.pdf), it also mentioned parameter replication among servers for server fault tolerant. I suppose that is also not supported in current version of TensorFlow?", "@qinglintian if you keep your hyper-parameters (ie, learning rate) in shared variable, then you can adjust from any tensorflow client that's part of the cluster. Fault tolerance is achieved by checkpointing and restarting the training on failures. Permanent removal of workers can be handled with backup workers. Permanent removal of parameter servers is more tricky -- you have to repartition your parameter variables", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This is being addressed in https://github.com/tensorflow/tensorflow/issues/14232", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Hi @byronyi \r\n\r\nFollowing your approach: training --->restart training from checkpoint with a different ClusterSpec\r\n> 1 Prepare for a scaling by writing a checkpoint to external storage\r\n 2 Terminate current session\r\n3 Re-do the partitioning of graph with respect to the new cluster_def\r\n4 Create a new session and recover from the checkpoint\r\n\r\n I have experimented with training my model using synchronous between-graph replicas. After terminating the training process(say 2ps + 8 workers), the training process can be restarted from the last checkpoint and proceed with a different number of worker (say 2ps+10workers).  I was wondering if it is possible to adjust the number of ps . If yes, could you illustrate how to scale the number of parameter server nodes ? Thank you! ", "Hi @kzhang28,\r\nHave you test the time cost of restarting the cluster? We are currently schedule tensorflow clusters on k8s. I doubt whether it is worth doing so if we auto scale too often."]}, {"number": 11895, "title": "Fix typos", "body": "This PR fixes some typos: `squeee`, `argumnet`, `succeeeded`, and `pyton`.", "comments": ["Can one of the admins verify this patch?", "@taehoonlee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @gunan, @caisq and @ebrevdo to be potential reviewers."]}, {"number": 11894, "title": "Slow Hessian", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I have written custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.4.0-47-generic #68-Ubuntu and Also macOS Sierra 10.12.3 (CPU only) \r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cuda/8.0 and cudnn/5.1\r\n- **GPU model and memory**: 12GB, memory:100GB\r\n- **Exact command to reproduce**: tf.hessians(ys,xs)\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThe command tf.hessians(ys, xs) is used to add nodes to the graph in order to compute hessians of ys with respect to xs where both ys and xs are list of tensors. Currently computing hessian with respect to a vector is not possible. The trick is to unstack the input tensor (x) into a list of one dimensional tensors (xs) and compute hessian with respect to each of them separately then stack them together. Tensorflow gets stuck in the phase of graph construction even when the dimension of x (length of list xs) is about one hundred. In the implementation of Hessian in gradients_impl.py the second derivative is implemented as the derivative of partial derivative with respect to each member of xs. I guess the slowness of graph construction is due to this line:\r\n\r\n_hess = [gradients(_gradient, x, **kwargs)[0] for _gradient in _gradients]\r\n\r\nwhich may add several unnecessary intermediate nodes with overlapping functionality to the graph due to the for loop. Is there any way other than looping over input dimensions that efficiently constructs the graph in a reasonable time?\r\n\r\n### Source code / logs\r\nThis source code can simulate the problem:\r\nimport tensorflow as tf\r\nimport numpy as np\r\nin_dimension = 256\r\nx = tf.placeholder(tf.float32, shape=(1, in_dimension))\r\nx_list = tf.unstack(x, axis=1)\r\nxx= tf.stack(x_list, axis=1)\r\ny = tf.pow(xx,3)\r\nhess = tf.hessians(y, x_list)\r\nsess = tf.Session()\r\nprint(sess.run(hess, feed_dict={x : np.random.normal(0, 1, size=(1, in_dimension))}))\r\n", "comments": ["@alextp, it seems like slowness is inherent with the current approach to computing Hessians of non-scalars. I don't know of anything else to do. Could eager mode help with this at all?", "@amehrjou one issue is that current`tf.gradients` implementation is quite slow. The runtime grows linearly with size of the graph even when size of the subgraph to differentiate or the output stays constant. https://github.com/tensorflow/tensorflow/issues/9901", "@yaroslavvb Exactly. I also realized that the size of the graph becomes very large when `tf.gradients` is applied for the second time. For example the first `tf.gradients` added around 100 nodes to a graph with 80 nodes. But the second derivative surprisingly added 1000 nodes to the graph for each dimension of the vector with respect to which derivates are computed. This means that even for an input vector of medium size, the graph does not fit in the memory of any GPU. Thus, do you think if there is any chance to compute Hessian diagonal in a feasible way? ", "Oh you want just the diagonal of Hessian? For piecewise linear neural network (ie, ReLU activations) computing diagonal can be done cheaply (ie, for square loss it's the product of corresponding activation and backprop variances). In a general network, there's no fast way (http://www.cs.utoronto.ca/~ilya/pubs/2012/CurvProp.pdf)", "@yaroslavvb I want the diagonal of Hessian but specifically for a smooth network and I want it with respect to inputs not parameters. Second derivative of a piecewise linear function with respect to its input is trivially zero and just for the record Tensorflow returns `None` even though it must return zero.", "`None` result in `tf.gradients` means there's no path between input and output in the computation graph", "@yaroslavvb Yes. I replace `None` values with zero. Thank you for the pointer to the Curve Propagation paper too. ", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 11893, "title": "Branch 163636676", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 11892, "title": "Softplus with parameters", "body": "Should I go on implementing the parametric version of Softplus?\r\n\r\nCurrent implementation: [softplus_impl.py#L35](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/bijectors/softplus_impl.py)\r\n\r\nReference to parametric version: [PyTorch docs](http://pytorch.org/docs/master/nn.html?highlight=softplus#torch.nn.Softplus)", "comments": ["@jvdillon  could you please comment on this.", "Sorry for just noticing this issue.  Im happy to make this change ASAP.", "So we do have the `hinge_softness` parameter. Do you need the `threshold` parameter as well? (I question if `threshold` is useful since the function is anyway almost linear...)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 11891, "title": "'//tensorflow/contrib/verbs:rdma", "body": "Build fails \r\nThanks for help in advance\r\n\r\n1. It must be a **bug** \r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:yes\r\n- **TensorFlow version (use command below)**:Latest git\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:Build label: 0.5.2\r\n- **CUDA/cuDNN version**:\r\nBuilt on Tue_Jan_10_13:22:03_CST_2017\r\nCuda compilation tools, release 8.0, V8.0.61\r\n- **GPU model and memory**:  [GeForce GTX 860M] 4044MiB Driver Version: 375.66\r\n- **Exact command to reproduce**:\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nBuild fails with carefully chosen configs:\r\nPlease specify the location of python. [Default is /home/op/anaconda2/bin/python]:\r\ninput the desired Python library path to use.  Default is /home/op/anaconda2/lib/python2.7/site-packages\r\n\r\ny jemalloc as malloc support\r\nn Google Cloud Platform\r\nn Hadoop File System support\r\ny XLA JIT support\r\n\r\ny  VERBS support\r\nn OpenCL support\r\ny  CUDA support\r\n\r\nCUDA SDK version: 8.0\r\nCUDA 8.0 toolkit is installed /usr/local/cuda-8.0\r\n\r\ncuDNN 6.0\r\n cuDNN 6 library is installed: /usr/local/cuda-8.0\r\n\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 5.0]\r\n\r\nclang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler\r\ngcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]\r\nMPI support? [y/N]: n\r\n\r\nflags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n### Source code / logs\r\nERROR: /home/op/Downloads/tensorflow/tensorflow/contrib/verbs/BUILD:136:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 150 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/contrib/verbs/rdma.cc:18:0:\r\n./tensorflow/contrib/verbs/rdma.h:21:30: fatal error: infiniband/verbs.h: No such file or directory\r\n #include <infiniband/verbs.h>\r\n                              ^\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3203.696s, Critical Path: 206.50s", "comments": ["Hi @pocandraspoc \r\nWhy \"y VERBS support\" ? \r\nIt means you have chosen to compile with RDMA Verbs support, but you don't have the SW stack (libibverbs, etc..), and I guess you didn't meant to use it.\r\nRun configure and choose \"No\" for Verbs support.", "Thanks I will check it!", "Sorry to make false bug report,\r\nI made the mistake wit my configurations.\r\n"]}, {"number": 11890, "title": "No registered 'MirrorPad' OpKernel for XLA_CPU_JIT", "body": "`tf.pad(..., mode='REFLECT')` doesn't work with tfcompile. Will this be supported and when? \r\n\r\nIt's somewhat urgent on my part, and it seems like it should be a simple enough addition unlike control flow as in https://github.com/tensorflow/tensorflow/issues/11275", "comments": ["I think this might be quite simple to add to the tf2xla bridge. I'll take a look.", "This may be able to make it into the merge tomorrow, I'll ping the issue when it's on the merge train.", "I think this should be resolved, take a look at let us know how it goes."]}, {"number": 11889, "title": "correct description of bounding box points", "body": "bbox points are upperleft and bottomright not bottomleft and upperright. Also coordinates were described in idiosyncratic (y,x) instead of (x,y) form. And no, those two corrections don't cancel.", "comments": ["Can one of the admins verify this patch?", "@jeremy-rutman, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "signed CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 11888, "title": "Can't import graph containing MutableHashTable", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 18\r\n- **TensorFlow installed from (source or binary)**: Binary (pip)\r\n- **TensorFlow version (use command below)**: v1.3.0.0rc0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nImport a meta graph containing `MutableHashTable` operations fails. The MutableHashTable in my use case is named and inside a scope. After some digging around I found that the error is a result of a collection `saveable_objects` containing names of `MutableHashTable`s but _without_ the proper scoping.\r\n\r\n### Source code / logs\r\n```\r\ngraph = create_eval_graph()\r\ntf.train.export_meta_graph('eval_model.meta', graph=graph, as_text=True)\r\n\r\n# ...\r\ns = tf.train.import_meta_graph('eval_model.meta') # Fails \r\n```\r\n\r\nThis throws the following\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-238-7de9a7b0d1f2> in <module>()\r\n----> 1 s = tf.train.import_meta_graph('eval_model.meta')\r\n\r\n/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1696                                       clear_devices=clear_devices,\r\n   1697                                       import_scope=import_scope,\r\n-> 1698                                       **kwargs)\r\n   1699   if meta_graph_def.HasField(\"saver_def\"):\r\n   1700     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)\r\n\r\n/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\r\n    690           for value in field.value:\r\n    691             col_op = graph.as_graph_element(\r\n--> 692                 ops.prepend_name_scope(value, scope_to_prepend_to_names))\r\n    693             graph.add_to_collection(key, col_op)\r\n    694         elif kind == \"int64_list\":\r\n\r\n/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   2704 \r\n   2705     with self._lock:\r\n-> 2706       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   2707 \r\n   2708   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\r\n\r\n/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   2764         if name not in self._nodes_by_name:\r\n   2765           raise KeyError(\"The name %s refers to an Operation not in the \"\r\n-> 2766                          \"graph.\" % repr(name))\r\n   2767         return self._nodes_by_name[name]\r\n   2768 \r\n\r\nKeyError: \"The name 'lstm_c_table' refers to an Operation not in the graph.\"\r\n```\r\n\r\nWhere `lstm_c_table` is created as follows\r\n\r\n```\r\ntf.contrib.lookup.MutableHashTable(key_dtype=tf.string, value_dtype=tf.float32, default_value=value, name='lstm_c_table')\r\n```\r\n\r\nI looked at the generated proto and it contains the following node:\r\n\r\n```\r\n  node {\r\n    name: \"input/lstm_c_table\"\r\n    op: \"MutableHashTableOfTensorsV2\"\r\n    attr {\r\n      key: \"_output_shapes\"\r\n      value {\r\n        list {\r\n          shape {\r\n          }\r\n        }\r\n      }\r\n    }\r\n    attr {\r\n      key: \"container\"\r\n      value {\r\n        s: \"\"\r\n      }\r\n    }\r\n    attr {\r\n      key: \"key_dtype\"\r\n      value {\r\n        type: DT_STRING\r\n      }\r\n    }\r\n    attr {\r\n      key: \"shared_name\"\r\n      value {\r\n        s: \"\"\r\n      }\r\n    }\r\n    attr {\r\n      key: \"use_node_name_sharing\"\r\n      value {\r\n        b: true\r\n      }\r\n    }\r\n    attr {\r\n      key: \"value_dtype\"\r\n      value {\r\n        type: DT_FLOAT\r\n      }\r\n    }\r\n    attr {\r\n      key: \"value_shape\"\r\n      value {\r\n        shape {\r\n          dim {\r\n            size: 512\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\n```\r\ncollection_def {\r\n  key: \"saveable_objects\"\r\n  value {\r\n    node_list {\r\n      value: \"lstm_c_table\"\r\n      value: \"lstm_h_table\"\r\n      value: \"history_table\"\r\n      value: \"first_table\"\r\n      value: \"encode_lstm_c_table\"\r\n      value: \"encode_lstm_h_table\"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nEither removing the above collection or prefixing all the values with `input/` results in the following error:\r\n\r\n```\r\nTypeError: Can't convert Operation 'input/lstm_c_table' to Tensor (target dtype=None, name=None, as_ref=True)\r\n```\r\n\r\nThis solution is part of an imitation of `batch_sequences_with_states` so a graph can be exported that does not rely on queues but instead uses placeholders and the `feed_dict` mechanism, for use in interactive model evaluation.", "comments": ["Hello, i got the same problem. tf.train.import_meta_graph when graph contain MutableHashTables. Did you have a solution?", "A terrible, terrible solution, yes. Save all the information you can and\nrebuild the graph. But that requires the code to be the same, it gets\nmessy.\n\nBest of luck.\n\n\nOn Sat, 16 Jun 2018, 02:48 Zhouwei, <notifications@github.com> wrote:\n\n> Hello, i got the same problem. tf.train.import_meta_graph when graph\n> contain MutableHashTables. Did you have a solution?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11888#issuecomment-397778104>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AE-ZKa0RyGplajVc4LbiERntI7CrrqYRks5t9GPygaJpZM4OnvHn>\n> .\n>\n", "Right, that works. But not flexible, i will keep working on it... Thank you.", "The scope-propagation issue is still present in v1.10.0", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=11888\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=11888\">No</a>\n", "> Hello, i got the same problem. tf.train.import_meta_graph when graph contain MutableHashTables. Did you have a solution?\r\n\r\nHello, i also got the same problem. Did you have a solution?"]}, {"number": 11887, "title": "Plan for supporting tf.contrib.seq2seq.prepare_attention in r1.2", "body": "Any plans to support \r\n\r\nhttps://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/prepare_attention\r\n\r\nin r1.2?", "comments": ["I think we deleted all this code in favor of an object oriented API."]}, {"number": 11886, "title": "Inconsistent normalization formula", "body": "Hi there,\r\n\r\nIn the Input Normalization video, you normalize the pixels by dividing by 1/2 of the pixel depth; however, in the code you divide by the pixel depth. This confused me at first, and you may want to consider making the formula the same in both video and code.\r\n\r\nThanks!\r\n\r\nAll the best,\r\nJake", "comments": ["@jacobyeager, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vincentvanhoucke, @mlucool and @Jakobovski to be potential reviewers.", "Can one of the admins verify this patch?", "Yes, it was an oversight. The point is that a factor 2 won't matter. I'd rather not change the behavior at this point since the class has been running for a while and I worry about unnecessarily changing people's baselines that depend on this notebook, and introducing further delta between the documentation produced around the assignments and the assignment itself. Thanks for flagging it though."]}, {"number": 11885, "title": "Command 'graph_transforms/transform_graph' not found.", "body": "i ran the following script since i want to optimise the graph for using in android.\r\n\r\n```\r\nbazel graph_transforms/transform_graph \\\r\n--in_graph=stripped.pb \\\r\n--out_graph=optimized_stripped.pb \\\r\n--inputs='Mul' \\\r\n--outputs='final_result' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,160,160,3\")\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  round_weights(num_steps=256)\r\n```\r\nIm getting the following error..\r\n\r\n`Command 'graph_transforms/transform_graph' not found. Try 'bazel help'.\r\n`", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@Zumbalamambo \r\n1. Clone tensorflow\r\n2. bazel build tensorflow from source\r\n3. bazel build transform_graph\r\n4. profit"]}, {"number": 11884, "title": "Bazel build failure with gpu version", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.2.0, I'm installing from tha master branch\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.3\r\n- **CUDA/cuDNN version**:\r\n5.1\r\n- **GPU model and memory**:\r\nGTX1060 6G\r\n- **Exact command to reproduce**:\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nI followed every step from the installing & configuration page at tf official [website](https://www.tensorflow.org/install/install_sources#ConfigureInstallation). But when I came the bazel build step, I ran into the problem showed in the error logs below.\r\n\r\nI should mention that:\r\n\r\n1. I choosed cuda support in the ` ./configure ` step, you can find the configure details below as well.\r\n2. I previously installed the cpu version of tensorflow on the same computer and it worked fine. Now I've uninstalled it of course.\r\n\r\n### Source code / logs\r\n\r\n\r\nMy configuration:\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nFound possible Python library paths:\r\n/usr/local/lib/python2.7/dist-packages\r\n/usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N]: \r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N]: \r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: \r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: \r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\"Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 5\r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n\r\n```\r\nMy command which introduced the error below:\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nMy error message:\r\n\r\n```\r\n.......\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1039\r\n\t\t_create_local_cuda_repository(repository_ctx)\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 976, in _create_local_cuda_repository\r\n\t\t_host_compiler_includes(repository_ctx, cc)\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 145, in _host_compiler_includes\r\n\t\tget_cxx_inc_directories(repository_ctx, cc)\r\n\tFile \"/home/zzh/tensorflow/third_party/gpus/cuda_configure.bzl\", line 120, in get_cxx_inc_directories\r\n\t\tset(includes_cpp)\r\ndepsets cannot contain mutable items\r\nINFO: Elapsed time: 5.610s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package\r\n\r\n```\r\n", "comments": ["Have a look at this\r\nhttps://github.com/tensorflow/tensorflow/issues/11859\r\n\r\nusing an older version of bazel (0.5.2) should fix the problem", "Solved the problem by rolling back to  bazel (0.5.2) .", "I am glad I found this because i spent too much time trying to fix these related issues when building with bazel and tensorflow(GPU). Rolling back to Bazel (0.5.2) Worked for me! "]}, {"number": 11883, "title": "Error while building android project from tensorflow/examples", "body": "`Error:/Users/sagarsuri/tensorflow/tensorflow/core/BUILD:194:1 Failed to get cached inputs: Could not determine containing package for external/protobuf/src/google/protobuf/stubs/common.h.`\r\n\r\nI am getting the above error while building Android project which is provided in the examples folder.\r\n\r\nI executed this command to pull tensorflow project to my mac:\r\n\r\n`git clone https://github.com/tensorflow/tensorflow.git --depth 1`", "comments": ["Have you edited tensorflow/WORKSPACE to fill in the Android sdk/ndk values?", "```\r\n# Uncomment and update the paths in these entries to build the Android demo.\r\nandroid_sdk_repository(\r\n    name = \"androidsdk\",\r\n    api_level = 25,\r\n    # Ensure that you have the build_tools_version below installed in the\r\n    # SDK manager as it updates periodically.\r\n    build_tools_version = \"25.0.2\",\r\n    # Replace with path to Android SDK on your system\r\n    path = \"/Users/sagarsuri/Library/Android/sdk\",\r\n)\r\n#\r\n# Android NDK r12b is recommended (higher may cause issues with Bazel)\r\nandroid_ndk_repository(\r\n    name=\"androidndk\",\r\n    path=\"/Users/sagarsuri/Documents/android-ndk-r12b\",\r\n    # This needs to be 14 or higher to compile TensorFlow.\r\n    # Please specify API level to >= 21 to build for 64-bit\r\n    # archtectures or the Android NDK will automatically select biggest\r\n    # API level that it supports without notice.\r\n    # Note that the NDK version is not the API level.\r\n    api_level=14)\r\n```\r\nHere is my WORKSPACE. Now I am getting this error:\r\n```\r\nError:/Users/sagarsuri/tensorflow/tensorflow/core/BUILD:194:1 Failed to get cached inputs: Could not determine containing package for external/protobuf/src/google/protobuf/arena.h.\r\nError:Execution failed for task ':buildNativeBazel'.\r\n> Process 'command '/usr/local/bin/bazel'' finished with non-zero exit value 1\r\n```", "I'm guessing it's related to the `--depth` option you're cloning with, which may have [issues](https://stackoverflow.com/questions/23708231/git-shallow-clone-clone-depth-misses-remote-branches) with remote repositories.\r\n\r\nI typically clone with the following command:\r\n`git clone --recursive https://github.com/tensorflow/tensorflow/`\r\n", "I tried that command as well. Same issue.", "Are you still experiencing this issue? If so, can you provide the requested details for a TF issue such as OS, bazel version, etc?", "Have you also tried \"bazel clean\"? Bazel can cache things about your local repo so sometimes that's necessary to make it take another look.", "Closing due to inactivity, please reply with more details if the issue persists."]}, {"number": 11882, "title": "conv2d_transpose produce different results on GPU", "body": "### System information\r\n== cat /etc/issue ===============================================\r\nLinux ST 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"14.04.4 LTS, Trusty Tahr\"\r\nVERSION_ID=\"14.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.9.4-2ubuntu1~14.04.1) 4.9.4\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux ST 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nmsgpack-numpy (0.4.1)\r\nnumpy (1.13.1)\r\nprotobuf (3.2.0)\r\ntensorflow (0.10.0)\r\ntensorflow-gpu (1.0.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.0.0\r\ntf.GIT_VERSION = v1.0.0-rc2-15-g47bba63-dirty\r\ntf.COMPILER_VERSION = v1.0.0-rc2-15-g47bba63-dirty\r\nSanity check: array([1], dtype=int32)\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/abc/torch/install/lib:/usr/lib/x86_64-linux-gnu:/home/abc/torch/install/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib64:/home/abc/torch/install/lib:/home/abc/code/torch/torch/install/lib:/usr/local/cuda/lib64::/usr/local/computecpp/lib:/data/software/gurobi652/linux64/lib\r\nDYLD_LIBRARY_PATH /home/abc/torch/install/lib:/home/abc/torch/install/lib:/home/abc/code/torch/torch/install/lib:\r\n\r\n== nvidia-smi ===================================================\r\nSun Jul 30 17:45:45 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:01:00.0      On |                  N/A |\r\n|  0%   53C    P2    47W / 260W |   7909MiB /  8112MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1308    G   /usr/bin/X                                     357MiB |\r\n|    0      2590    G   compiz                                         229MiB |\r\n|    0      3254    G   ...el-token=CBAE43C38254E155E78826C3F38F0092    99MiB |\r\n|    0      9480    C   python                                        1039MiB |\r\n|    0     10432    C   /usr/bin/python                               5895MiB |\r\n|    0     20408    C   /usr/bin/python                                283MiB |\r\n|    0     28024    G   /usr/local/MATLAB/R2015a/bin/glnxa64/MATLAB      2MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart.so.8.0\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudart.so\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/MATLAB/R2017a/bin/glnxa64/libcudart.so.8.0.44\r\n/usr/local/MATLAB/R2015a/bin/glnxa64/libcudart.so.6.5.14\r\n### Describe the problem\r\nI am trying to use `tf.nn.conv2d_transpose` but it produces different results every time on GPU. However, the result would be the same when switching the device to CPU. It seems like a bug. Please check the toy model below for more details.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\nconv_ = np.random.randn(10, 7, 7, 56)\r\n\r\nwith tf.device('/gpu:0'):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.get_variable(\"weight\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    bias = tf.get_variable(\"bias\", initializer=np.zeros(1, dtype=np.float32))\t\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv = tf.nn.bias_add(conv, bias)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nnp.array_equal(sess.run(conv), sess.run(conv))\r\n```\r\n`Out[2]: False`", "comments": ["@zheng-xq, Do you have any thoughts on this. I made a slightly more elaborate test case that computes relative error between the gpu and cpu version and two runs of the cpu version. @jnjaby's fear that gpu is not deterministic seems due to non-determinism in reduction (probably), because I only see 1e-5 error between two runs on the gpu. But I see a ~5770. relative  error between the gpu vs cpu. Could you please take a look:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\nconv_ = np.random.randn(10, 7, 7, 56)\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.get_variable(\"weight_cpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    bias = tf.get_variable(\"bias_cpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_cpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.get_variable(\"weight_gpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    bias = tf.get_variable(\"bias_gpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_gpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ncpu_a=sess.run(conv_cpu)\r\ngpu_a=sess.run(conv_gpu)\r\ngpu_b=sess.run(conv_gpu)\r\n\r\ndef rel_error(a,ref):\r\n  return np.max(np.abs((ref-a)/ref))\r\nprint ('relerror gpu_a vs cpu %f relerror gpu_b vs cpu 2 %f'%(rel_error(gpu_a, cpu_a), rel_error(gpu_b, cpu_a)))\r\nprint ('relerror gpu_a vs. gpu_b %f '%(rel_error(gpu_a, gpu_b)))\r\n```", "@aselle because the weight initialized by CPU and GPU are different. I try to initialize weights with another `tf.constant`. That really confuses.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\nconv_ = np.random.randn(10, 7, 7, 56)\r\nweight_ = np.random.randn(9, 9, 1, 56)\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.constant(weight_, dtype=tf.float32, name=\"weight_cpu\")\r\n    bias = tf.get_variable(\"bias_cpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_cpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.constant(weight_, dtype=tf.float32, name=\"weight_gpu\")\r\n    bias = tf.get_variable(\"bias_gpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_gpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ncpu_a=sess.run(conv_cpu)\r\ngpu_a=sess.run(conv_gpu)\r\ngpu_b=sess.run(conv_gpu)\r\n\r\ndef rel_error(a,ref):\r\n  return np.max(np.abs((ref-a)/ref))\r\n\r\nprint ('relerror gpu_a vs cpu %f \\nrelerror gpu_b vs cpu  %f'%(rel_error(gpu_a, cpu_a), rel_error(gpu_b, cpu_a)))\r\nprint ('relerror gpu_a vs. gpu_b %f '%(rel_error(gpu_a, gpu_b)))\r\n\r\nprint (np.array_equal(sess.run(conv_cpu), sess.run(conv_cpu)))\r\nprint (np.array_equal(sess.run(conv_gpu), sess.run(conv_gpu)))\r\n```", "I would agree with first making the initialization the same. One common way is to random initialize the weights in numpy, and use that in both paths. ", "This version shows no relative error between any of the implementations, so I am thinking there is no bug\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\nconv_ = np.random.randn(10, 7, 7, 56)\r\n\r\nweight = np.random.uniform(-1.,1., (9,9,1,56)).astype(np.float32)\r\nbias = np.random.uniform(-.1,.1, (1)).astype(np.float32)\r\n\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    #weight = tf.get_variable(\"weight_cpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    #bias = tf.get_variable(\"bias_cpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_cpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    #weight = tf.get_variable(\"weight_gpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    #bias = tf.get_variable(\"bias_gpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_gpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ncpu_a=sess.run(conv_cpu)\r\ngpu_a=sess.run(conv_gpu)\r\ngpu_b=sess.run(conv_gpu)\r\n```", "@aselle The last case is consistent. I make another test case output `True` and `False` respectively, showing different result between `tf.constant` and `tf.Variables`. Commonly, however, the weights should remain the same after initialization. So let's clarify the questions:\r\n- Where is the non-determinism produced, Tensorflow or other libraries of GPU?\r\n- What is the difference between `constant` Tensor and `variable` Tensor?\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\nconv_ = np.random.randn(10, 7, 7, 56)\r\nweight = np.random.uniform(-1.,1., (9,9,1,56)).astype(np.float32)\r\n\r\nwith tf.device('/gpu:0'):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    biases = tf.get_variable(\"bias_gpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    weight_con = tf.constant(weight, dtype=tf.float32)\r\n    weight_var = tf.get_variable(\"weight_gpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n\r\n    conv_con = tf.nn.conv2d_transpose(bottom, weight_con, [10, 19, 19, 1],[1, 3, 3, 1],padding='SAME')\r\n    conv_var = tf.nn.conv2d_transpose(bottom, weight_var, [10, 19, 19, 1],[1, 3, 3, 1],padding='SAME')\r\n\r\n    conv_con = tf.nn.bias_add(conv_con, biases)\r\n    conv_var = tf.nn.bias_add(conv_var, biases)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nprint (np.array_equal(sess.run(conv_con), sess.run(conv_con)))\r\nprint (np.array_equal(sess.run(conv_var), sess.run(conv_var)))\r\n```", "@zheng-xq, are you able to answer @jnjaby's questions above? Thanks!", "There are some known differences between CPU and GPU convolutions. Yang is working with Benoit to resolve them.\r\n", "Any updates here? I keep having to switch back and forth between an old version of TensorFlow and the current one. Is addressing this a priority for the team?", "Please refer to https://github.com/tensorflow/tensorflow/issues/14601#issuecomment-361734250 for updates. Once we are using the updated version of eigen, you can verify if the bug is fixed.", "I could be wrong, but I as of today, I don't think this issue has been addressed in the nightly wheel available over here: http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-mac/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tf_nightly-1.head-py3-none-any.whl", "Sorry for bothering you, but are there any updates now? Thanks very much! @yzhwang  @benoitsteiner ", "Please check with the latest version of TensorFlow. Feel free to reopen if the issues still persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=11882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=11882\">No</a>\n"]}, {"number": 11881, "title": "python ImportError: No module named 'scipy'", "body": "In [1]:from sklearn import linear_model\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-a6ebbebad697> in <module>()\r\n----> 1 from sklearn import linear_model\r\nC:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\__init__.py in <module>()\r\n     55 else:\r\n     56     from . import __check_build\r\n---> 57     from .base import clone\r\n     58     __check_build  # avoid flakes unused variable error\r\n     59 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\base.py in <module>()\r\n      8 \r\n      9 import numpy as np\r\n---> 10 from scipy import sparse\r\n     11 from .externals import six\r\n     12 from .utils.fixes import signature\r\n\r\nImportError: No module named 'scipy'", "comments": ["have you check the scipy installation by issue command \"pip list \"?\r\ncan you import scipy from python originally? \r\nanyway, this issue should report from forum rather than tensorflow issue list", "@tomzhang \r\n\u201dimport scipy from python originally\u201d is done,\r\n but i just want to use \"from sklearn  import linear_model\" in the environment of tensorflow-gpu\uff0cmay i \uff1f", "@tomzhang I work it out by in install scipy-0.19.1-cp35-cp35m-win_amd64.whl\uff0cthank U very much\uff01"]}, {"number": 11880, "title": "fix minor typo", "body": "", "comments": ["Can one of the admins verify this patch?", "@callofdutyops, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @skye to be potential reviewers."]}, {"number": 11879, "title": "Determinant-operation on the GPU", "body": "This pull request implements the computation of the matrix determinant on the GPU using LU-factorization with cuSolver.\r\n\r\nThis is the second part of the use-case presented in pull request #11878 .\r\n\r\nIt uses the LU factorization instead of QR-factorization, because this allows to compute the determinant also for non-symmetric matrices. This is required by the definition of the operation in Tensorflow. Hence, I also had to implement the LU-interface to cuSolver, QR would have already been implemented.\r\n\r\nTwo further notes: \r\nFirst, the precision of this algorithm to compute the determinant is less precise than the Eigen-implementation. Hence I had to relax the allowed error in the unit test. This is especially important for singular or near-singular matrices.\r\nSecond, if this pull request and #11878 will both be accepted, they will probably clash in cuda_solvers.h and/or cuda_solvers.cc. I'll resolve the conflicts if it happens to be so.", "comments": ["Can one of the admins verify this patch?", "@shamanDevel, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @josh11b and @andrewharp to be potential reviewers.", "@shamanDevel Thanks for your submission, but I just implemented this kernel myself. It should be available in the opensource tree later this week."]}]