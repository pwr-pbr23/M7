[{"number": 51602, "title": "RuntimeError: Given shapes, [1, 23], ] and [1, 10], are not broadcastable.Node number 1392 (SELECT_V2) failed to prepare.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu1804\r\n- TensorFlow installation (pip package or built from source): tf-nightly 2.7.0\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): pip\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\nI used dymamic axes when using onnx to convert pytorch model to onnx model, later on, I convert onnx to tensorflow model .pb and then to quantized tflite model.\r\n\r\nHere shows the dynamic axes in my input data\r\n                      dynamic_axes={\r\n                          \"img_feat\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"img_pos_feat\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"obj_masks\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"obj_boxes\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"input_ids\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"position_ids\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"attn_masks\": {0: \"batch_size\", 1: \"sequences\"},\r\n                          \"gather_index\": {0: \"batch_size\", 1: \"sequences\"}\r\n                      },\r\n\r\n\r\n### 3. Failure after conversion\r\nI was able to convert the model successfully to int tflite model. \r\nDuring the inference phase of int tflite model, I resized the input_details to set tensor to match the dynamic axes in the tflite model. \r\n\r\n    shape_0 = list(named_args['img_feat'].shape)\r\n    interpreter.resize_tensor_input(input_details[0]['index'], tuple(shape_0))\r\n\r\n    shape_1 = list(named_args['img_pos_feat'].shape)\r\n    interpreter.resize_tensor_input(input_details[1]['index'], tuple(shape_1))\r\n\r\n    shape_2 = list(named_args['obj_masks'].shape)\r\n    interpreter.resize_tensor_input(input_details[2]['index'], tuple(shape_2))\r\n\r\n    shape_3 = list(named_args['position_ids'].shape)\r\n    interpreter.resize_tensor_input(input_details[3]['index'], tuple(shape_3))\r\n\r\n    shape_4 = list(named_args['attn_masks'].shape)\r\n    interpreter.resize_tensor_input(input_details[4]['index'], tuple(shape_4))\r\n\r\n    shape_5 = list(named_args['gather_index'].shape)\r\n    interpreter.resize_tensor_input(input_details[5]['index'], tuple(shape_5))\r\n\r\n    shape_6 = list(named_args['input_ids'].shape)\r\n    interpreter.resize_tensor_input(input_details[6]['index'], tuple(shape_6))\r\n\r\nAll other input data work well except the obj_masks, and I got the error: \r\n  File \"/home/ubuntu/Projects/Vesta/uniter/scripts/uniter_model_convert_quant_test.py\", line 579, in model_prediction\r\n    out_tflite_quant, pred_box_tflite_quant = run_tflite(interpreter, input_details, output_details, named_args, quant=True, detect_feature=detect_feature)\r\n  File \"/home/ubuntu/Projects/Vesta/uniter/scripts/uniter_model_convert_quant_test.py\", line 436, in run_tflite\r\n    interpreter.invoke()\r\n  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 879, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Given shapes, [1, 23], ] and [1, 10], are not broadcastable.Node number 1392 (SELECT_V2) failed to prepare.\r\n\r\nI'm wondering why the shape is [1,23],], instead of [1,23]? Did I do anything wrong?\r\n\r\n#### Option A: Reference colab notebooks\r\n\r\nHere is the visualization of SELECTV2 node in Netron.\r\n![image](https://user-images.githubusercontent.com/32310519/130302354-ad89e43f-4f8a-4320-a30d-f01c48776f94.png)\r\n\r\n\r\nEven if I donot use dynamic axes, the int tflite model is much slower than the float tflite model.\r\n", "comments": ["Hi @SkylerZheng ,\r\nIn order to expedite the trouble-shooting process, could you please provide a stand alone code code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51602\">No</a>\n"]}, {"number": 51601, "title": "Build failure when building Tensorflow MLIR ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 7221ec4eace8d163954fd391c8f8e5accd3ae771\r\n- Python version: Python 3.6.9\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nA build failure is seen while compiling for MLIR from the current HEAD, using a standard build command that's otherwise always worked. The error appears to be absl usage related, within  tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc\r\n\r\nAssumed this was transient, but it has been present for 4 days now. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n$ bazel build --config=v2 --linkopt=\"-fuse-ld=lld\" tensorflow/compiler/mlir/...  -j 32\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nERROR: {$MYPATH}/tensorflow/compiler/mlir/tfrt/BUILD:136:16: C++ compilation of rule '//tensorflow/compiler/mlir/tfrt:tf_cpurt_kernels' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 419 argument(s) skipped)\r\nIn file included from ./tensorflow/core/profiler/lib/traceme.h:27:0,\r\n                 from tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:32:\r\n./tensorflow/core/profiler/lib/traceme_encode.h: In instantiation of 'tensorflow::profiler::TraceMeArg::TraceMeArg(absl::lts_20210324::string_view, Value) [with Value = llvm::StringRef]':\r\ntensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:285:70:   required from here\r\n./tensorflow/core/profiler/lib/traceme_encode.h:35:61: error: no matching function for call to 'absl::lts_20210324::AlphaNum::AlphaNum(llvm::StringRef&)'\r\n   TraceMeArg(absl::string_view k, Value v) : key(k), value(v) {}\r\n                                                             ^\r\nIn file included from external/com_google_absl/absl/container/internal/layout.h:176:0,\r\n                 from external/com_google_absl/absl/strings/internal/cord_rep_ring.h:25,\r\n                 from external/com_google_absl/absl/strings/cord.h:81,\r\n                 from ./tensorflow/core/platform/default/cord.h:22,\r\n                 from ./tensorflow/core/platform/cord.h:25,\r\n                 from ./tensorflow/core/platform/tstring.h:24,\r\n                 from ./tensorflow/core/platform/types.h:23,\r\n                 from ./tensorflow/core/platform/logging.h:20,\r\n                 from ./tensorflow/core/platform/status.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:19,\r\n                 from ./tensorflow/core/util/device_name_utils.h:21,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_structs.h:28,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_op_interfaces.h:27,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:39,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/dialect_registration.h:24,\r\n                 from tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:24:\r\nexternal/com_google_absl/absl/strings/str_cat.h:283:3: note: candidate: template<class T, typename std::enable_if<(std::is_class<_Tp>::value && (std::is_same<T, std::_Bit_reference>::value || std::is_same<T, bool>::value))>::type* <anonymous> > absl::lts_20210324::AlphaNum::AlphaNum(T)\r\n   AlphaNum(T e) : AlphaNum(static_cast<bool>(e)) {}  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:283:3: note:   template argument deduction/substitution failed:\r\nexternal/com_google_absl/absl/strings/str_cat.h:282:11: error: no type named 'type' in 'struct std::enable_if<false, void>'\r\n           nullptr>\r\n           ^~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:282:11: note: invalid template non-type parameter\r\nexternal/com_google_absl/absl/strings/str_cat.h:271:3: note: candidate: template<class T, class> absl::lts_20210324::AlphaNum::AlphaNum(T)\r\n   AlphaNum(T e)  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:271:3: note:   template argument deduction/substitution failed:\r\nexternal/com_google_absl/absl/strings/str_cat.h:269:13: error: no type named 'type' in 'struct std::enable_if<false, void>'\r\n             typename = typename std::enable_if<\r\n             ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:252:3: note: candidate: template<class Allocator> absl::lts_20210324::AlphaNum::AlphaNum(const std::__cxx11::basic_string<char, std::char_traits<char>, Allocator>&)\r\n   AlphaNum(  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:252:3: note:   template argument deduction/substitution failed:\r\nIn file included from ./tensorflow/core/profiler/lib/traceme.h:27:0,\r\n                 from tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:32:\r\n./tensorflow/core/profiler/lib/traceme_encode.h:35:61: note:   'llvm::StringRef' is not derived from 'const std::__cxx11::basic_string<char, std::char_traits<char>, Allocator>'\r\n   TraceMeArg(absl::string_view k, Value v) : key(k), value(v) {}\r\n                                                             ^\r\nIn file included from external/com_google_absl/absl/container/internal/layout.h:176:0,\r\n                 from external/com_google_absl/absl/strings/internal/cord_rep_ring.h:25,\r\n                 from external/com_google_absl/absl/strings/cord.h:81,\r\n                 from ./tensorflow/core/platform/default/cord.h:22,\r\n                 from ./tensorflow/core/platform/cord.h:25,\r\n                 from ./tensorflow/core/platform/tstring.h:24,\r\n                 from ./tensorflow/core/platform/types.h:23,\r\n                 from ./tensorflow/core/platform/logging.h:20,\r\n                 from ./tensorflow/core/platform/status.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:19,\r\n                 from ./tensorflow/core/util/device_name_utils.h:21,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_structs.h:28,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_op_interfaces.h:27,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:39,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/dialect_registration.h:24,\r\n                 from tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:24:\r\nexternal/com_google_absl/absl/strings/str_cat.h:249:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(absl::lts_20210324::string_view)\r\n   AlphaNum(absl::string_view pc) : piece_(pc) {}  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:249:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'absl::lts_20210324::string_view'\r\nexternal/com_google_absl/absl/strings/str_cat.h:248:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(const char*)\r\n   AlphaNum(const char* c_str) : piece_(c_str) {}  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:248:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'const char*'\r\nexternal/com_google_absl/absl/strings/str_cat.h:244:3: note: candidate: template<long unsigned int size> absl::lts_20210324::AlphaNum::AlphaNum(const absl::lts_20210324::strings_internal::AlphaNumBuffer<size>&)\r\n   AlphaNum(  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:244:3: note:   template argument deduction/substitution failed:\r\nIn file included from ./tensorflow/core/profiler/lib/traceme.h:27:0,\r\n                 from tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:32:\r\n./tensorflow/core/profiler/lib/traceme_encode.h:35:61: note:   'llvm::StringRef' is not derived from 'const absl::lts_20210324::strings_internal::AlphaNumBuffer<size>'\r\n   TraceMeArg(absl::string_view k, Value v) : key(k), value(v) {}\r\n                                                             ^\r\nIn file included from external/com_google_absl/absl/container/internal/layout.h:176:0,\r\n                 from external/com_google_absl/absl/strings/internal/cord_rep_ring.h:25,\r\n                 from external/com_google_absl/absl/strings/cord.h:81,\r\n                 from ./tensorflow/core/platform/default/cord.h:22,\r\n                 from ./tensorflow/core/platform/cord.h:25,\r\n                 from ./tensorflow/core/platform/tstring.h:24,\r\n                 from ./tensorflow/core/platform/types.h:23,\r\n                 from ./tensorflow/core/platform/logging.h:20,\r\n                 from ./tensorflow/core/platform/status.h:25,\r\n                 from ./tensorflow/core/lib/core/status.h:19,\r\n                 from ./tensorflow/core/util/device_name_utils.h:21,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_structs.h:28,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_op_interfaces.h:27,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:39,\r\n                 from ./tensorflow/compiler/mlir/tensorflow/dialect_registration.h:24,\r\n                 from tensorflow/compiler/mlir/tfrt/jit/tf_cpurt_kernels.cc:24:\r\nexternal/com_google_absl/absl/strings/str_cat.h:241:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(absl::lts_20210324::Dec)\r\n   AlphaNum(Dec dec);  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:241:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'absl::lts_20210324::Dec'\r\nexternal/com_google_absl/absl/strings/str_cat.h:240:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(absl::lts_20210324::Hex)\r\n   AlphaNum(Hex hex);  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:240:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'absl::lts_20210324::Hex'\r\nexternal/com_google_absl/absl/strings/str_cat.h:237:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(double)\r\n   AlphaNum(double f)  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:237:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'double'\r\nexternal/com_google_absl/absl/strings/str_cat.h:235:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(float)\r\n   AlphaNum(float f)  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:235:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'float'\r\nexternal/com_google_absl/absl/strings/str_cat.h:231:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(long long unsigned int)\r\n   AlphaNum(unsigned long long x)  // NOLINT(*)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:231:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'long long unsigned int'\r\nexternal/com_google_absl/absl/strings/str_cat.h:228:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(long long int)\r\n   AlphaNum(long long x)  // NOLINT(*)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:228:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'long long int'\r\nexternal/com_google_absl/absl/strings/str_cat.h:225:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(long unsigned int)\r\n   AlphaNum(unsigned long x)  // NOLINT(*)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:225:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'long unsigned int'\r\nexternal/com_google_absl/absl/strings/str_cat.h:222:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(long int)\r\n   AlphaNum(long x)  // NOLINT(*)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:222:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'long int'\r\nexternal/com_google_absl/absl/strings/str_cat.h:219:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(unsigned int)\r\n   AlphaNum(unsigned int x)  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:219:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'unsigned int'\r\nexternal/com_google_absl/absl/strings/str_cat.h:216:3: note: candidate: absl::lts_20210324::AlphaNum::AlphaNum(int)\r\n   AlphaNum(int x)  // NOLINT(runtime/explicit)\r\n   ^~~~~~~~\r\nexternal/com_google_absl/absl/strings/str_cat.h:216:3: note:   no known conversion for argument 1 from 'llvm::StringRef' to 'int'\r\nINFO: Elapsed time: 3705.770s, Critical Path: 377.10s\r\nINFO: 11312 processes: 719 internal, 10593 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["@sjarus \r\nCould you please try on stable tf version as the version chosen could be work in progress, using version that are stable and already tested would give better results. you could try 2.4, 2.5 and verify you have no compatibility issues]", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "I think this should be fixed now. Please reopen if that's not the case.\r\n\r\nCulprit: TFRT introduced C++17 code via MLIR/LLVM but TF is using C++14.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51601\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51601\">No</a>\n"]}, {"number": 51600, "title": "TypeError: 'int' object is not callable", "body": "TF Version: `2.6.0`\r\n\r\nusing `tf.random.set_seed(7) `  produce  TypeError: 'int' object is not callable\r\n\r\n", "comments": ["This one `tf.compat.v1.set_random_seed(7)` worked for me in `TF 2.6.0` in Google Colab but dont know why I cant get the same with `tf.random.set_seed(7)` ", "Hi @arjunskumar,\r\n\r\nI was trying to replicate what you stated above in a Google Colab environment:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)  # 2.6.0\r\ntf.random.set_seed(7)\r\n```\r\n\r\nHowever, this does not throw any errors in my Colab environment.\r\n\r\nCould you please provide a minimal code example where the bug occured?\r\nAlso, could you please copy above code and execute it in a new environment \"https://colab.research.google.com/notebooks/empty.ipynb\" to verify?\r\n\r\nBest,\r\nLudwig\r\n", "Now it's working on my google colab. Dont know the exact reason why the same code behaved strange earlier.\r\n\r\nThanks @LudwigStumpp ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51600\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51600\">No</a>\n"]}, {"number": 51598, "title": "tf.experimental.dlpack.from_dlpack() does not support negative strides", "body": "Issue will arise when passing data from CuPy array with negative strides to TF tensor. Please refer to \r\nhttps://github.com/cupy/cupy/issues/5665\r\n@kmaehashi\r\n@leofang\r\n", "comments": ["Hi! @llodds,We see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "@mohantym \r\n\r\n- TF version: 2.4.1\r\n- CuPy version: 8.0.0\r\n\r\nCode to reproduce the issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport cupy as cp\r\nb = tf.random.uniform((10,))\r\na = cp.arange(10)\r\ncap = a.toDlpack()\r\nb = tf.experimental.dlpack.from_dlpack(cap)\r\nc = cp.flip(a) # now c has negative stride\r\ncap = c.toDlpack()\r\nb = tf.experimental.dlpack.from_dlpack(cap)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-c1e25d8172d8> in <module>\r\n      7 c = cp.flip(a) # now c has negative stride\r\n      8 cap = c.toDlpack()\r\n----> 9 b = tf.experimental.dlpack.from_dlpack(cap)\r\n\r\n/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/dlpack/dlpack.py in from_dlpack(dlcapsule)\r\n     64     A Tensorflow eager tensor\r\n     65   \"\"\"\r\n---> 66   return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)\r\n\r\nInvalidArgumentError: Invalid strides array from DLPack\r\n```\r\n\r\nBoth NumPy and CuPy can handle array with negative strides, but TF can't. ", "Hi @llodds @Saduf2019 ,was able to produce  in 2.3 ,2.4.1 and 2.6  and got different error in TF 2.5 ,providing [gist ](https://colab.research.google.com/gist/mohantym/e8981804fcda98631842ca6784853ae9/github_51598.ipynb#scrollTo=3kQHpzRF7IoE)for reference", "@llodds \r\nthis issue is already reported and tracked at [this link](https://github.com/tensorflow/tensorflow/issues/42660)\r\n[You could try workaround as of now by  making a \"c\" -contiguous copy and let us know also refer to above link for any workarounds meanwhile]", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51598\">No</a>\n"]}, {"number": 51597, "title": "[TF Lite C API] XNNPACK option not available using C API ", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 11.4\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: Pixel 4\r\n-   **TensorFlow installed from (source or binary)**: binary\r\n-   **TensorFlow version (use command below)**: tensorflow-lite-2.5.0  AAR \r\n-   **Python version**: 3.7\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: n/a\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI am using TFlite C API available with `tensorflow-lite-2.5.0`  AAR  binary downloaded from Maven. I can not find any way of setting XNNPACK. \r\n\r\nI am looking for **C API** to enable XNNPACK (not ObjC, or Java), something like this:\r\n```\r\nTFL_CAPI_EXPORT extern void TfLiteInterpreterOptionsSetXNNPACK(TfLiteInterpreterOptions* options, bool enable);\r\n```\r\nunless it is already available in some form \r\n\r\n", "comments": ["@multiverse-tf @Maratyszcza could you take a look?", "I think you may be able to enable XNNPACK in the C API by using the \r\nTfLiteXNNPackDelegateCreate function from\r\ntensorflow/lite/delegates/xnnpack/xnnpack_delegate.h to create a delegate,\r\nand then passing that delegate to the TfLiteInterpreterOptionsAddDelegate function\r\n(or the TfLiteInterpreterModifyGraphWithDelegate function in older versions)?", "@sivers2021 Could you please refer to the above [comment](https://github.com/tensorflow/tensorflow/issues/51597#issuecomment-1054169559) and let us know the outcome? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51596, "title": "Determinism OP bug on TF 2.6.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.7.11\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Tesla K80 / 12MB\r\n\r\n**Describe the current behavior**\r\nI've setup the TF determinism environment like what I usually done on Google Colab with TF 2.4.1\r\n-  Setup all the global seeds\r\n- run ```os.environ['TF_DETERMINISTIC_OPS'] = '1'``` and ```os.environ['TF_CUDNN_DETERMINISTIC'] = '1'```\r\n\r\nIt used to work fine on 2.4.1 but when I run the training on 2.6.0 I got this error\r\n```\r\nUnimplementedError:  Deterministic GPU implementation of SparseSoftmaxXentWithLogitsOp not available.\r\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-25-471bb77471a2>:5) ]] [Op:__inference_train_function_1044]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nNo error occured during training\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): -\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n[Colab link](https://colab.research.google.com/drive/1ogEnSN-E9ZKc09kMWRs8vXgBj9Zlfaoo?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["# Get your gradients after training\r\nloss_value, grads = grad(model, features, labels)\r\n\r\n# Apply some clipping\r\ngrads = [tf.clip_by_norm(g, norm)\r\n             for g in grads]\r\n\r\n# Continue on with training\r\noptimizer.apply_gradients(grads)", "> # Get your gradients after training\r\n> loss_value, grads = grad(model, features, labels)\r\n> \r\n> # Apply some clipping\r\n> grads = [tf.clip_by_norm(g, norm)\r\n> for g in grads]\r\n> \r\n> # Continue on with training\r\n> optimizer.apply_gradients(grads)\r\n\r\nis this the fix to model.fit() implementation? any PR for this fix already?", "@raudipra ,\r\nCan you please try to test your code in tf-nightly version which the code is executed without any error.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/19f1395da404ff0a1abd0565fc303ea6/nightly-51596.ipynb).Thanks\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51596\">No</a>\n"]}, {"number": 51595, "title": "fix __array__ for numpy support", "body": "This change is similar to that of fc0f0e61ca9fe3ca3b9b58f51bcf00e0643ed9e3, when an extra parameter is passed to to `__array__` method, it throws an error, the following code can produce this\r\n```\r\nv = tf.Variable([1.0])\r\nprint(np.array(v, dtype=int))\r\n```\r\nWhich throws\r\n> TypeError: \\_\\_array\\_\\_() takes 1 positional argument but 2 were given\r\n\r\nAfter this change, the `np.array` can behave correctly", "comments": ["What's wrong with the check?", "I re-requested the review mistakenly"]}, {"number": 51594, "title": "AttributeError: module 'keras.engine' has no attribute 'Layer'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): using pip install tensorflow in jupyter\r\n- TensorFlow version: 2.6.0\r\n- Python version: 3.8.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n  While run the code I am getting below error. Kindly on this. \r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-fc46460c6aba> in <module>\r\n      4 import mrcnn.config\r\n      5 import mrcnn.utils\r\n----> 6 from mrcnn.model import MaskRCNN\r\n      7 from pathlib import Path\r\n      8 \r\n\r\n~\\Documents\\Mahe_Juptyer\\Mask_RCNN-master\\mrcnn\\model.py in <module>\r\n    253 \r\n    254 \r\n--> 255 class ProposalLayer(KE.Layer):\r\n    256     \"\"\"Receives anchor scores and selects a subset to pass as proposals\r\n    257     to the second stage. Filtering is done based on anchor scores and\r\n\r\n**AttributeError: module 'keras.engine' has no attribute 'Layer'**\r\n-------------------------------------------------------------------------------------------\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi! @MAHESH47T  ,\r\nIn order to expedite the trouble-shooting process, please provide a  Colab Gist / sample code snippet to reproduce the issue reported here. Thanks!", "Dear @mohantym,\r\n    Thanks for the reply,\r\n    Kindly find the code below which is used and its output generated. Please let me know if anything else is required.\r\n--------------------------------\r\n**Code:**\r\nimport os\r\nimport numpy as np\r\nimport cv2\r\nimport mrcnn.config\r\nimport mrcnn.utils\r\nfrom mrcnn.model import MaskRCNN\r\nfrom pathlib import Path\r\n\r\n**Error Output:**\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-14-f0bd135e195a> in <module>\r\n      4 import mrcnn.config\r\n      5 import mrcnn.utils\r\n----> 6 from mrcnn.model import MaskRCNN\r\n      7 from pathlib import Path\r\n\r\n~\\Documents\\Mahe_Juptyer\\Mask_RCNN-master\\mrcnn\\model.py in <module>\r\n    253 \r\n    254 \r\n--> 255 class ProposalLayer(KE.Layer):\r\n    256     \"\"\"Receives anchor scores and selects a subset to pass as proposals\r\n    257     to the second stage. Filtering is done based on anchor scores and\r\n\r\nAttributeError: module 'keras.engine' has no attribute 'Layer'\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51594\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51594\">No</a>\n", "Mistakenly closed it. ", "Hi @MAHESH47T,\r\n\r\nTaking a look at the Issues of the library `mrcnn` that you are using at https://github.com/matterport/Mask_RCNN/issues/2587 we can find a solution to your issue.\r\n\r\nThe issue is caused by the library not being upward compatible to the latest keras version `2.6.0`.\r\n\r\nPlease take a look at the proposed version as stated in the link above. You can then solve your issue by explicitely installing the specified version of keras to make it work (not the latest one).\r\n\r\nAlso please note that this issue is not caused by the Tensorflow library, but by the mrcnn library not specifying their dependencies correctly. For the future, it would be great if you first take a look at the already reported issues of the library that is throwing the error instead of creating an issue here.\r\n\r\nHope that helps!\r\n\r\nBest,\r\nLudwig", "\r\nHi! @MAHESH47T ,Agree with  @LudwigStumpp's point , you can refere this[ link ](https://stackoverflow.com/questions/67905185/module-keras-engine-has-no-attribute-layer) for answers .\r\nand also  post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi,\r\n  Yeah, I have tried lowering the version of tensorflow, keras. Now it is working.\r\n  \r\n #!pip install tensorflow==1.15\r\n#!pip install keras==2.3.0 \r\n ", "Hi @MAHESH47T,\r\n\r\ngreat to hear! Glad that I could help! :) Please could you upvote my provided solution such that others see it?\r\n\r\nAll the best,\r\nLudwig", "Hi @MAHESH47T , if you can also  replace the `import keras.engine as KE with  import keras.layers as KE` after forking the repo , so the KE.layer will do its work if you want to use TF 2.x versions. Feel free close to close the issue if it work. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51594\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51594\">No</a>\n"]}, {"number": 51593, "title": "about the Dropout", "body": "hi,dear all\r\nwhen use Dropout as the **help** down, I got confused, **what sum is not changed** ??\r\n```\r\n>>> help(tf.keras.layers.Dropout)\r\nclass Dropout(tensorflow.python.keras.engine.base_layer.Layer)\r\n |  Dropout(*args, **kwargs)\r\n |  \r\n |  Applies Dropout to the input.\r\n |  \r\n |  The Dropout layer randomly sets input units to 0 with a frequency of `rate`\r\n |  at each step during training time, which helps prevent overfitting.\r\n |  Inputs not set to 0 are scaled up by 1/(1 - rate) such that the **sum** over\r\n |  all inputs is unchanged.\r\n```\r\n\r\n```\r\n>>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\r\n>>> data = np.arange(10).reshape(5, 2).astype(np.float32)\r\n>>> data\r\narray([[0., 1.],\r\n       [2., 3.],\r\n       [4., 5.],\r\n       [6., 7.],\r\n       [8., 9.]], dtype=float32)\r\n>>> outputs = layer(data, training=True)\r\n>>> outputs\r\n<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\r\narray([[ 0.  ,  1.25],\r\n       [ 2.5 ,  3.75],\r\n       [ 5.  ,  6.25],\r\n       [ 7.5 ,  0.  ],\r\n       [10.  , 11.25]], dtype=float32)>\r\n>>> np.sum(outputs)\r\n47.5\r\n>>> np.sum(data)\r\n45.0\r\n>>> np.sum(data)/10\r\n4.5\r\n>>> np.sum(data)/8\r\n5.625\r\n>>> np.sum(data,axis=1)\r\narray([ 1.,  5.,  9., 13., 17.], dtype=float32)\r\n>>> np.sum(outputs,axis=1)\r\narray([ 1.25,  6.25, 11.25,  7.5 , 21.25], dtype=float32)\r\n>>> np.sum(outputs,axis=0)\r\narray([25. , 22.5], dtype=float32)\r\n>>> np.sum(data,axis=0)\r\narray([20., 25.], dtype=float32)\r\n>>> np.sum(outputs)/8\r\n5.9375\r\n>>> np.sum(outputs)/10\r\n4.75\r\n\r\n```\r\ncould you pls help me ?\r\nthx\r\n", "comments": ["and how to set the para **input_shape** ? if I set the input_shape as input, got similar result, \r\n```\r\n>>> layer = tf.keras.layers.Dropout(.2, input_shape=(5,2))\r\n>>> outputs = layer(data, training=True)\r\n>>> outputs\r\n<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\r\narray([[ 0.  ,  1.25],\r\n       [ 2.5 ,  3.75],\r\n       [ 0.  ,  6.25],\r\n       [ 7.5 ,  0.  ],\r\n       [ 0.  , 11.25]], dtype=float32)>\r\n```\r\nif do not set the para , seems Ok \r\n```\r\n>>> outputs = layer(data, training=True)\r\n>>> outputs\r\n<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\r\narray([[ 0.  ,  1.25],\r\n       [ 2.5 ,  0.  ],\r\n       [ 5.  ,  6.25],\r\n       [ 7.5 ,  8.75],\r\n       [10.  , 11.25]], dtype=float32)>\r\n```\r\n", "@ucasiggcas Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51593\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51593\">No</a>\n"]}, {"number": 51592, "title": "AlreadyExistsError: Another metric with the same name already exists", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): [I used this example](https://keras.io/examples/vision/conv_lstm/)\r\n- OS Platform and Distribution: Windows 10 10.0.19043 Build 19043\r\n- TensorFlow installed from (source or binary): Using pip\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9.6\r\n- CUDA/cuDNN version: V11.4.48\r\n- GPU model and memory: NVIDIA GeForce RTX 3090, 24 GB\r\n\r\n**Describe the current behavior**\r\nI\u2019ve updated Tensorflow and Keras to 2.6.0.\r\n\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n```\r\n\r\nI\u2019ve faced the following issue.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAlreadyExistsError                        Traceback (most recent call last)\r\n~\\AppData\\Local\\Temp/ipykernel_16440/2264619257.py in <module>\r\n      4 import tensorflow as tf\r\n      5 from tensorflow import keras\r\n----> 6 from tensorflow.keras import layers\r\n      7 \r\n      8 import io\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py in <module>\r\n      8 import sys as _sys\r\n      9 \r\n---> 10 from keras import __version__\r\n     11 from keras.api._v2.keras import __internal__\r\n     12 from keras.api._v2.keras import activations\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\__init__.py in <module>\r\n     23 \r\n     24 # See b/110718070#comment18 for more details about this import.\r\n---> 25 from keras import models\r\n     26 \r\n     27 from keras.engine.input_layer import Input\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\models.py in <module>\r\n     18 import tensorflow.compat.v2 as tf\r\n     19 from keras import backend\r\n---> 20 from keras import metrics as metrics_module\r\n     21 from keras import optimizer_v1\r\n     22 from keras.engine import functional\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\metrics.py in <module>\r\n     24 \r\n     25 import numpy as np\r\n---> 26 from keras import activations\r\n     27 from keras import backend\r\n     28 from keras.engine import base_layer\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\activations.py in <module>\r\n     18 \r\n     19 from keras import backend\r\n---> 20 from keras.layers import advanced_activations\r\n     21 from keras.utils.generic_utils import deserialize_keras_object\r\n     22 from keras.utils.generic_utils import serialize_keras_object\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\layers\\__init__.py in <module>\r\n     21 \r\n     22 # Generic layers.\r\n---> 23 from keras.engine.input_layer import Input\r\n     24 from keras.engine.input_layer import InputLayer\r\n     25 from keras.engine.input_spec import InputSpec\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\input_layer.py in <module>\r\n     19 from keras import backend\r\n     20 from keras.distribute import distributed_training_utils\r\n---> 21 from keras.engine import base_layer\r\n     22 from keras.engine import keras_tensor\r\n     23 from keras.engine import node as node_module\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\base_layer.py in <module>\r\n     40 from keras.engine import node as node_module\r\n     41 from keras.mixed_precision import autocast_variable\r\n---> 42 from keras.mixed_precision import loss_scale_optimizer\r\n     43 from keras.mixed_precision import policy\r\n     44 from keras.saving.saved_model import layer_serialization\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\mixed_precision\\loss_scale_optimizer.py in <module>\r\n     16 \r\n     17 from keras import backend\r\n---> 18 from keras import optimizers\r\n     19 from keras.mixed_precision import loss_scale as keras_loss_scale_module\r\n     20 from keras.optimizer_v2 import optimizer_v2\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizers.py in <module>\r\n     24 from keras.optimizer_v1 import Optimizer\r\n     25 from keras.optimizer_v1 import TFOptimizer\r\n---> 26 from keras.optimizer_v2 import adadelta as adadelta_v2\r\n     27 from keras.optimizer_v2 import adagrad as adagrad_v2\r\n     28 from keras.optimizer_v2 import adam as adam_v2\r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizer_v2\\adadelta.py in <module>\r\n     20 import numpy as np\r\n     21 from keras import backend_config\r\n---> 22 from keras.optimizer_v2 import optimizer_v2\r\n     23 from tensorflow.python.util.tf_export import keras_export\r\n     24 \r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py in <module>\r\n     34 \r\n     35 \r\n---> 36 keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\r\n     37     \"/tensorflow/api/keras/optimizers\", \"keras optimizer usage\", \"method\")\r\n     38 \r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py in __init__(self, name, description, *labels)\r\n    358       *labels: The label list of the new metric.\r\n    359     \"\"\"\r\n--> 360     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\r\n    361                                     len(labels), name, description, *labels)\r\n    362 \r\n\r\nc:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\monitoring.py in __init__(self, metric_name, metric_methods, label_length, *args)\r\n    133           self._metric_name, len(self._metric_methods)))\r\n    134 \r\n--> 135     self._metric = self._metric_methods[self._label_length].create(*args)\r\n    136 \r\n    137   def __del__(self):\r\n\r\nAlreadyExistsError: Another metric with the same name already exists.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error should appear\r\n", "comments": ["@takumatix ,\r\n\r\nI was able to execute the code without any issues in tf v2.6.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/11bdd96fc8f57b4413156667aa4709b6/u51592.ipynb).Please close the current seesion and try to execute the same code.Thanks!", "I reinstalled everything, and it's working fine.\r\n\r\nThank you for your prompt response.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51592\">No</a>\n", "I have the same problem: even on a clean system this even more minimized code fails with exactly the error message posted above:\r\n```\r\ndocker run -it python:3.9 /bin/bash\r\npip install tensorflow\r\npython\r\nfrom tensorflow.keras import layers\r\n```\r\n\r\nI have the same error on my Mac with miniconda. This occurs with TF 2.6.1 and 2.6.0, but not with 2.5.2.\r\nI've tested using both Python 3.8.10 and 3.9.7.", "This also just happened for me.\r\n\r\n[This Stack Overflow answer](https://stackoverflow.com/a/69830680/13283808) was the key for me.\r\n\r\nI checked with `pip freeze` and found that I had a mismatch with `tensorflow==2.6.1` but `keras==2.7.0`. Funny, because it was a fresh environment with only `tensorflow-hub` installed prior (besides `srsly`, a small I/O utility library). Perhaps TFHub brought in the bad dependency.\r\n\r\nThere did not appear to be a `keras==2.6.1`, so I downgraded both to `2.6.0` and it started working for me.", "Thanks for the reply. Yes, downgrading to Keras 2.6.0 does the trick.", "> This also just happened for me.\r\n> \r\n> [This Stack Overflow answer](https://stackoverflow.com/a/69830680/13283808) was the key for me.\r\n> \r\n> I checked with `pip freeze` and found that I had a mismatch with `tensorflow==2.6.1` but `keras==2.7.0`. Funny, because it was a fresh environment with only `tensorflow-hub` installed prior (besides `srsly`, a small I/O utility library). Perhaps TFHub brought in the bad dependency.\r\n> \r\n> There did not appear to be a `keras==2.6.1`, so I downgraded both to `2.6.0` and it started working for me.\r\n\r\nIt helped. Thanks.", "> This also just happened for me.\r\n> \r\n> [This Stack Overflow answer](https://stackoverflow.com/a/69830680/13283808) was the key for me.\r\n> \r\n> I checked with `pip freeze` and found that I had a mismatch with `tensorflow==2.6.1` but `keras==2.7.0`. Funny, because it was a fresh environment with only `tensorflow-hub` installed prior (besides `srsly`, a small I/O utility library). Perhaps TFHub brought in the bad dependency.\r\n> \r\n> There did not appear to be a `keras==2.6.1`, so I downgraded both to `2.6.0` and it started working for me.\r\n\r\nit worked, thank you!", "> This also just happened for me.\r\n> \r\n> [This Stack Overflow answer](https://stackoverflow.com/a/69830680/13283808) was the key for me.\r\n> \r\n> I checked with `pip freeze` and found that I had a mismatch with `tensorflow==2.6.1` but `keras==2.7.0`. Funny, because it was a fresh environment with only `tensorflow-hub` installed prior (besides `srsly`, a small I/O utility library). Perhaps TFHub brought in the bad dependency.\r\n> \r\n> There did not appear to be a `keras==2.6.1`, so I downgraded both to `2.6.0` and it started working for me.\r\n\r\nAwesome. Thanks!"]}, {"number": 51590, "title": "TypeError: EndVector() takes 1 positional argument but 2 were given", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ArchLinux\r\n- TensorFlow installation (pip package or built from source):  pip package   \r\n- TensorFlow library (version, if pip package or github SHA, if built from source):  v2.5.0\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pathlib\r\n\r\n\r\ngpus = tf.config.experimental.list_physical_devices(device_type='GPU')\r\nif gpus:\r\n    for gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(device = gpu, enable = True)\r\n\r\n\r\n# Set keras model name\r\nkeras_model = \"weight.h5\"\r\n        \r\n# Load MNIST dataset\r\nmnist = tf.keras.datasets.mnist\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n\r\n# Normalize the input image so that each pixel value is between 0 to 1.\r\ntrain_images = train_images.astype(np.float32) / 255.0\r\ntest_images = test_images.astype(np.float32) / 255.0\r\n\r\n# Define the model architecture\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.InputLayer(input_shape=(28, 28)),\r\n  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\r\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),  \r\n  tf.keras.layers.Flatten(),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\n\r\n# Set training details\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\n# Train model\r\nmodel.fit(\r\n  train_images,\r\n  train_labels,\r\n  epochs=5,\r\n  validation_data=(test_images, test_labels)\r\n)\r\n\r\n# Save model\r\nmodel.save_weights(filepath = keras_model, save_format = 'h5')\r\n\r\n# Load keras model weight\r\nmodel.load_weights(filepath = keras_model)\r\n\r\n\r\n# Define representative dataset\r\ndef representative_data_gen():\r\n  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\r\n    yield [input_value]\r\n\r\n\r\n# Do conversion\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\ntflite_model_quant = converter.convert()   # FAILED HERE\r\n\r\n\r\n# Save the quantized model\r\ntflite_models_dir = pathlib.Path(\"\")\r\ntflite_model_quant_file = tflite_models_dir/tflite_model\r\ntflite_model_quant_file.write_bytes(tflite_model_quant)\r\n```\r\n\r\n\r\n\r\n\r\n### 3. Failure after conversion\r\nNot able to convert\r\n\r\n### 4. (optional) RNN conversion support\r\nN/A\r\n\r\n### 5. (optional) Any other info / logs\r\n2021-08-20 16:27:43.366934: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nfully_quantize: 0, inference_type: 6, input_inference_type: 9, output_inference_type: 9\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/Desktop/TFLite_practice/convert_tflite.py\", line 56, in <module>\r\n    tflite_model_quant = converter.convert()\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/lite/python/lite.py\", line 1057, in convert\r\n    result = super(TFLiteKerasModelConverterV2,\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/lite/python/lite.py\", line 800, in convert\r\n    result = _modify_model_io_type(result, **flags_modify_model_io_type)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/lite/python/util.py\", line 906, in modify_model_io_type\r\n    return _convert_model_from_object_to_bytearray(model_object)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/lite/python/util.py\", line 556, in _convert_model_from_object_to_bytearray\r\n    model_offset = model_object.Pack(builder)\r\n  File \"/usr/lib/python3.9/site-packages/tensorflow/lite/python/schema_py_generated.py\", line 5630, in Pack\r\n    operatorCodes = builder.EndVector(len(self.operatorCodes))\r\nTypeError: EndVector() takes 1 positional argument but 2 were given\r\n\r\n", "comments": ["anyone can help, please?", "I also have this problem with the latest tf-nightly, waiting for the solution!\r\n\r\n  File \"/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/tensorflow/lite/python/schema_py_generated.py\", line 5702, in Pack\r\n    operatorCodes = builder.EndVector(len(self.operatorCodes))\r\nTypeError: EndVector() takes 1 positional argument but 2 were given\r\npython-BaseException\r\n", "Could anyone help?", "This code works with TF 2.6.0. Please use stable TF versions and avoid nightly builds. \r\n\r\nInstallation (commandline):\r\n`pip install tensorflow==2.6.0`\r\n\r\nTest in code (python):\r\n```\r\nimport tensorflow as tf\r\nassert tf.version.VERSION == \"2.5.0\", \"Please install TF 2.6.0, you're currently using TF \" + tf.version.VERSION\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51590\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51590\">No</a>\n", "I got the same error and solved  by downgrading flatbuffer from 2.0.0 to 1.12.0. It looks like a version mismatch between tflite and flatbuffer", "+1. This is because your Flatbuffer version is 2.0.0, while TF 2.5/2.6 expects Flatbuffer 1.12. Reinstall TF will refresh the Flabuffer version. Or you can imply downgrade your Flatbuffer version.", "```\r\npip3 uninstall flatbuffers\r\npip3 install flatbuffers==1.13\r\n```", "We're working on the migration to Flatbuffer 2.0. Hopefully the issue will be resolved soon."]}, {"number": 51589, "title": "google.protobuf.message.DecodeError: Error parsing message when using tf.function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda binary\r\n- TensorFlow version (use command below): tf.2.5\r\n- Python version: python 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 11.4, cudatoolkit 11.0.221,  cudnn 8.2.1.32\r\n- GPU model and memory: NVIDIA TITAN RTX, 24Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nSuccessfully opened dynamic library libcudart.so.11.0 v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n**Describe the current behavior**\r\nI write a custom model involving the tf.gather_nd function. When the 'train_step' function is not decorated   by 'tf.function', the model can be well-trained. But when I use 'tf.function' to decorate the 'train_step' function, I get the error 'google.protobuf.message.DecodeError: Error parsing message'.\r\n\r\n**Describe the expected behavior**\r\nThe model can be trained.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing): no\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass GatherModel(tf.keras.Model):\r\n    def __init__(self,ind1,w1):\r\n        super(GatherModel, self).__init__()\r\n        self.ind1=ind1\r\n        self.w1=tf.cast(w1,tf.float32)\r\n        self.lambda1 = tf.Variable(initial_value=tf.constant(0.1), trainable=True, name='lambda1')\r\n\r\n    def __call__(self, inputs,training=0):\r\n        y=inputs\r\n        for i in range(5):\r\n            y=tf.transpose(y, [1, 2, 3, 0])\r\n            y=tf.gather_nd(y*1.0, self.ind1)\r\n            y=y*self.w1\r\n            y=tf.reduce_sum(y,0)\r\n            y=tf.transpose(y,[3,0,1,2])\r\n            y = self.lambda1*y\r\n        return y\r\n\r\n# @tf.function\r\ndef train_step(model, inputs, labels, Loss, optimizer):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(inputs, training=1)\r\n        loss = Loss(labels, predictions)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss\r\n\r\nif __name__ == '__main__':\r\n    ind1=np.random.randint(0,300,[367, 217, 721, 2])\r\n    w1=np.random.normal(size=[367, 217, 721, 1, 1])\r\n    Model=GatherModel(ind1,w1)\r\n    inputs=tf.random.normal([2,256,256,1])\r\n    labels= tf.random.normal([2,217,721,1])\r\n    loss=tf.keras.losses.MeanSquaredError()\r\n    optimizer=tf.keras.optimizers.Adam(0.001)\r\n    for i in range(10):\r\n        LL=train_step(Model,inputs,labels,loss,optimizer)\r\n        print(LL)\r\n  \r\nIn the google colab https://colab.research.google.com/drive/1TrTctTKjYOIrZ2rvKhQmdTDS77d0gmg9#scrollTo=WMcegEVFWW_M, it says that the program crashed because of  exhausting of RAM.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@wangwei-cmd ,\r\n\r\nCan you please try to execute the in tf v2.6 which it is executing without any errors.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3f3cb42450844c117e42351676c448ef/u51589.ipynb).\r\n\r\nAlso please take a look at this links with similar error.[Link1](\r\nhttps://github.com/tensorflow/tensorflow/issues/23809),[Link2](https://stackoverflow.com/questions/57642264/how-to-fix-google-protobuf-message-decodeerror-error-parsing-message-when-cr).It helps.Thanks\r\n", "Hi, @tilakrayal \uff0c after uncomment @tf.function in the gist provided by your link,  the colab also crashed.", "@Saduf2019 ,\r\nI have tried to uncomment the tf.function from code and executed in colab with TF version 2.5, 2.6 and nightly version and noticed that session is being crashed. Please, find the gist [here](https://colab.research.google.com/gist/tilakrayal/b26597c7d9157d04a35cd7652bdb3108/untitled62.ipynb). Thanks!\r\n\r\n", "Closing since its a Duplicate https://github.com/tensorflow/tensorflow/issues/51588\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51589\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51589\">No</a>\n"]}, {"number": 51588, "title": "google.protobuf.message.DecodeError: Error parsing message when using tf.function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 21.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): conda binary\r\n- TensorFlow version (use command below): tf.2.5\r\n- Python version: python 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 11.4, cudatoolkit 11.0.221,  cudnn 8.2.1.32\r\n- GPU model and memory: NVIDIA TITAN RTX, 24Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\nSuccessfully opened dynamic library libcudart.so.11.0 v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n**Describe the current behavior**\r\nI write a custom model involving the tf.gather_nd function. When the 'train_step' function is not decorated   by 'tf.function', the model can be well-trained. But when I use 'tf.function' to decorate the 'train_step' function, I get the error 'google.protobuf.message.DecodeError: Error parsing message'.\r\n\r\n**Describe the expected behavior**\r\nThe model can be trained.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing): no\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass GatherModel(tf.keras.Model):\r\n    def __init__(self,ind1,w1):\r\n        super(GatherModel, self).__init__()\r\n        self.ind1=ind1\r\n        self.w1=tf.cast(w1,tf.float32)\r\n        self.lambda1 = tf.Variable(initial_value=tf.constant(0.1), trainable=True, name='lambda1')\r\n\r\n    def __call__(self, inputs,training=0):\r\n        y=inputs\r\n        for i in range(5):\r\n            y=tf.transpose(y, [1, 2, 3, 0])\r\n            y=tf.gather_nd(y*1.0, self.ind1)\r\n            y=y*self.w1\r\n            y=tf.reduce_sum(y,0)\r\n            y=tf.transpose(y,[3,0,1,2])\r\n            y = self.lambda1*y\r\n        return y\r\n\r\n# @tf.function\r\ndef train_step(model, inputs, labels, Loss, optimizer):\r\n    with tf.GradientTape() as tape:\r\n        predictions = model(inputs, training=1)\r\n        loss = Loss(labels, predictions)\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss\r\n\r\nif __name__ == '__main__':\r\n    ind1=np.random.randint(0,300,[367, 217, 721, 2])\r\n    w1=np.random.normal(size=[367, 217, 721, 1, 1])\r\n    Model=GatherModel(ind1,w1)\r\n    inputs=tf.random.normal([2,256,256,1])\r\n    labels= tf.random.normal([2,217,721,1])\r\n    loss=tf.keras.losses.MeanSquaredError()\r\n    optimizer=tf.keras.optimizers.Adam(0.001)\r\n    for i in range(10):\r\n        LL=train_step(Model,inputs,labels,loss,optimizer)\r\n        print(LL)\r\n```\r\n\r\n\r\nIn the google colab   [https://colab.research.google.com/drive/1TrTctTKjYOIrZ2rvKhQmdTDS77d0gmg9#scrollTo=WMcegEVFWW_M](url), it says that the program is crashed because of  exhausting of RAM.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi! @wangwei-cmd ,@sanatmpa1 , I was able to replicate this in TF 2.4/2.5/2.6 .providing [gist f](https://colab.research.google.com/gist/mohantym/9e1c815b72b47d979aa3f9f432a23e43/github_51588.ipynb#scrollTo=n-OmNbUkK1XC)or reference . Colab environment crashed when decorated @tf.function", "It looks like the crash is unrelated with the original error message (\"google.protobuf.message.DecodeError: Error parsing message\"). The crash is likely due to the huge tensors ind1 and w1. That might also be the cause of the DecodeError, but let's rule it out by using smaller shapes which fit inside colab's RAM.\r\n\r\n@mohantym can you reproduce the DecoderError in a gist?", "@mdanatg, yes, I think the crash is because the RAM were exhausted. I don't how why @tf.function needs so many RAM. Can someone  explains the reasons. Thanks a lot.", "@wangwei-cmd when you create a value from a numpy array, (like when you call `self.w1 = tf.cast(w1,tf.float32)`), the entire array is encoded inside the graph, taking a lot of space because non-Tensors appear as literal values to the graph. It's more efficient to avoid using numpy altogether, for instance by writing `self.w1 = tf.random.normal(shape=[367, 217, 721, 1, 1])`.", "@mdanatg Thank you. But if self.w1 is a fixed numpy array, how can I modified the code to avoid RAM exhausting.", "You can fix the value by setting it earlier, just like in your original code:\r\n\r\n```\r\nif __name__ == '__main__':\r\n    ind1=tf.random.uniform([367, 217, 721, 2], minval=0, maxval=300, dtype=tf.int32)\r\n    w1=tf.random.normal([367, 217, 721, 1, 1])\r\n    Model=GatherModel(ind1,w1)\r\n```\r\n\r\nThe only trick is making the value a Tensor so that TF doesn't attempt to inline it as a literal.\r\n\r\nTechnically, TF could be smart enough and wrap into an EagerTensor to avoid generating a huge graph, and that does sound like a useful optimization.", "@mdanatg Thank you very much.", "@wangwei-cmd ! Can this issue be closed then?", "Yeah, the problem is solved. please close it.", "Ok @wangwei-cmd ! Thanks for confirming the same. Closing this issue as it seems to resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51588\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51588\">No</a>\n"]}, {"number": 51587, "title": "KeyError: \"Failed to add concrete function b'__inference_train_14141' to object based saved model as it captures tf.Tensor(<unprintable>, shape=(), dtype=resource)  tensor", "body": "**tensorflow 2.5 python 3.7**\r\n\r\nI have totally no idea of what the error is when I'm using `tf.saved_model.save`. So less the hint what causes the error. At least it should tell us which tensor on earth is unreachable.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 65, in serialize_concrete_function\r\n    bound_inputs.append(node_ids[capture])\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/util/object_identity.py\", line 139, in __getitem__\r\n    return self._storage[self._wrap_key(key)]\r\nKeyError: <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main_Semantic3D.py\", line 628, in <module>\r\n    main1()\r\n  File \"main_Semantic3D.py\", line 566, in main1\r\n    tf.saved_model.save(model, modeldir)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1193, in save\r\n    save_and_return_nodes(obj, export_dir, signatures, options)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1228, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1399, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1363, in _build_meta_graph_impl\r\n    saveable_view, asset_info.asset_index)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 937, in _serialize_object_graph\r\n    concrete_function, saveable_view.captured_tensor_node_ids, coder)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 74, in serialize_concrete_function\r\n    % (concrete_function.name, capture))\r\nKeyError: \"Failed to add concrete function b'__inference_train_14141' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n\r\n```", "comments": ["@sjtusmartboy In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "\uff20sushreebarsa I'm afraid it's not possible since it's a large project. I have no idea which part causes this error. If I've known the what causes the error, I would definitely provide the minimum code to reproduce the error. Please find developers who are responsible for saved_model to give us some advice. I'm so feared of this bug since I've been working on this project for months and this is the last step.Please help.", "Any kind of method to debug?", "**saved_model code**\r\n\r\n```\r\n    model = Network(dataset, cfg)\r\n    modeldir = 'model/semantic/%s' % stamp\r\n\r\n    tf.saved_model.save(model, modeldir, signatures=model.inference.get_concrete_function(\r\n        tf.TensorSpec(shape=[cfg.batch_size, cfg.num_points, 3], dtype=tf.float64),\r\n        tf.TensorSpec(shape=[cfg.batch_size, cfg.num_points, 3], dtype=tf.float64)\r\n    ))\r\n```\r\n\r\n**train function**\r\n```\r\n    @tf.function( input_signature=[tf.TensorSpec(shape=[cfg.batch_size, cfg.num_points, 3], dtype=tf.float32),\r\n                                   tf.TensorSpec(shape=[cfg.batch_size, cfg.num_points, 3], dtype=tf.float32),\r\n                                   tf.TensorSpec(shape=(), dtype=tf.int64)])\r\n    def train(self, xyz_batch, color_batch, step):\r\n        t_start = time.time()\r\n\r\n        try:\r\n            self.adjacency_sparse_batch, self.scaledLaplacian_sparse_batch = self.get_graph(xyz_batch, color_batch)\r\n\r\n            self.inputs['xyz_batch'] = xyz_batch\r\n            self.inputs['color_batch'] = color_batch\r\n            self.inputs['adjacency_sparse_batch'] = self.adjacency_sparse_batch\r\n            self.inputs['scaledLaplacian_sparse_batch'] = self.scaledLaplacian_sparse_batch\r\n\r\n            loss_value = self.soft_n_cut_loss(step)\r\n\r\n            self.train_op = self.optimizer.minimize(loss_value, var_list=self.trainable_variables)\r\n\r\n            t_end = time.time()\r\n\r\n        except tf.errors.InvalidArgumentError as e:\r\n            print('Caught a NaN error :')\r\n\r\n        print('finished')\r\n\r\n\r\n        return tf.constant(0)\r\n```\r\n\r\n\r\n**inference function**\r\n```\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[cfg.batch_size, cfg.num_points, 3], dtype=tf.float64),\r\n                                  tf.TensorSpec(shape=[cfg.batch_size, cfg.num_points, 3], dtype=tf.float64)])\r\n    def inference(self, xyz_batch, color_batch):\r\n\r\n        self.adjacency_sparse_batch, self.scaledLaplacian_sparse_batch = self.get_graph(xyz_batch,\r\n                                                                                        color_batch)\r\n\r\n        self.inputs['xyz_batch'] = xyz_batch\r\n        self.inputs['color_batch'] = color_batch\r\n        self.inputs['adjacency_sparse_batch'] = self.adjacency_sparse_batch\r\n        self.inputs['scaledLaplacian_sparse_batch'] = self.scaledLaplacian_sparse_batch\r\n\r\n        return self.gcn_inference(self.inputs, 0)\r\n```\r\n", "I tried to find some hints through `serialize_concrete_function` in `\"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/function_serialization.py\"`\r\n\r\n```\r\ndef serialize_concrete_function(concrete_function, node_ids, coder):\r\n  \"\"\"Build a SavedConcreteFunction.\"\"\"\r\n  bound_inputs = []\r\n  try:\r\n    # import pdb;pdb.set_trace()\r\n    print('-----------------------------------------------------')\r\n    print(\"concrete_function:\", concrete_function)\r\n    print('node_ids:',node_ids)\r\n    for capture in concrete_function.captured_inputs:\r\n      print('capture:', capture)\r\n      bound_inputs.append(node_ids[capture])\r\n      print(\"bound_inputs:\", bound_inputs)\r\n    print(\"bound_inputs:\",bound_inputs)\r\n    print('-----------------------------------------------------')\r\n  except KeyError:\r\n    raise KeyError(\r\n        \"Failed to add concrete function %s to object based saved model as it \"\r\n        \"captures tensor %s which is unsupported or not reachable from root. \"\r\n        \"One reason could be that a stateful object or a variable that the \"\r\n        \"function depends on is not assigned to an attribute of the serialized \"\r\n        \"trackable object \"\r\n        \"(see SaveTest.test_captures_unreachable_variable).\"\r\n        % (concrete_function.name, capture))\r\n  concrete_function_proto = saved_object_graph_pb2.SavedConcreteFunction()\r\n  structured_outputs = func_graph_module.convert_structure_to_signature(\r\n      concrete_function.structured_outputs)\r\n  concrete_function_proto.canonicalized_input_signature.CopyFrom(\r\n      coder.encode_structure(concrete_function.structured_input_signature))\r\n  concrete_function_proto.output_signature.CopyFrom(\r\n      coder.encode_structure(structured_outputs))\r\n  concrete_function_proto.bound_inputs.extend(bound_inputs)\r\n  return concrete_function_proto\r\n```\r\n\r\nThe following is the print output:\r\n```\r\n\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction inference(xyz_batch, color_batch)\r\n  Args:\r\n    xyz_batch: float64 Tensor, shape=(2, 65536, 3)\r\n    color_batch: float64 Tensor, shape=(2, 65536, 3)\r\n  Returns:\r\n    float64 Tensor, shape=(2, 65536, 16)\r\n\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 4, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 5, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 8, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 9, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 13, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 14, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 15, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 16, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 17, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 18, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 19, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 20, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 21, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 22, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 23, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 24, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 25, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 26, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 27, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 28, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 29, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 30, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 31, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 32, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 33, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 34, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 35, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 36, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 37, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 38, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 39, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 40, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 41, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 42, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 43, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 44, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 45, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 46, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 47, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 48, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 49, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 50, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 51, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 52, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 53, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 54, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 55, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 56, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 57, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 58, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 59, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 60, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 61, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 62, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 63, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 64})\r\n\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26, 9]\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26, 9]\r\n-----------------------------------------------------\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction train(xyz_batch, color_batch, step)\r\n  Args:\r\n    xyz_batch: float32 Tensor, shape=(2, 65536, 3)\r\n    color_batch: float32 Tensor, shape=(2, 65536, 3)\r\n    step: int64 Tensor, shape=()\r\n  Returns:\r\n    int32 Tensor, shape=()\r\n\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 4, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 5, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 8, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 9, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 13, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 14, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 15, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 16, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 17, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 18, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 19, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 20, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 21, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 22, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 23, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 24, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 25, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 26, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 27, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 28, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 29, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 30, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 31, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 32, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 33, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 34, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 35, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 36, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 37, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 38, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 39, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 40, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 41, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 42, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 43, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 44, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 45, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 46, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 47, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 48, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 49, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 50, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 51, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 52, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 53, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 54, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 55, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 56, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 57, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 58, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 59, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 60, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 61, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 62, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 63, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 64})\r\n\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26, 9]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nTraceback (most recent call last):\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/function_serialization.py\", line 71, in serialize_concrete_function\r\n    bound_inputs.append(node_ids[capture])\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/util/object_identity.py\", line 139, in __getitem__\r\n    return self._storage[self._wrap_key(key)]\r\nKeyError: <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>\r\n\r\n```", "From the message, it seems `def train(self, xyz_batch, color_batch, step)` causes the error. But in `saved_model` code, the signatures is `model.inference.get_concrete_function`. Why ` train(xyz_batch, color_batch, step)` is captured by `serialize_concrete_function`", "Another sample code to test the print output from `serialize_concrete_function`\r\n```\r\nclass ExampleModel(tf.Module):\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\r\n  def capture_fn(self, x):\r\n    if not hasattr(self, 'weight'):\r\n      self.weight = tf.Variable(5.0, name='weight')\r\n    self.weight.assign_add(x * self.weight)\r\n    return self.weight\r\n\r\n  @tf.function\r\n  def polymorphic_fn(self, x):\r\n    return tf.constant(3.0) * x\r\n\r\nmodel = ExampleModel()\r\n# model.polymorphic_fn(tf.constant(4.0))\r\n# model.polymorphic_fn(tf.constant([1.0, 2.0, 3.0]))\r\ntf.saved_model.save(\r\n    model, \"/tmp/example-model\", signatures={'capture_fn': model.capture_fn})\r\n```\r\n\r\n**Output**\r\n```\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction capture_fn(x)\r\n  Args:\r\n    x: float32 Tensor, shape=()\r\n  Returns:\r\n    float32 Tensor, shape=()\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 1})\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [1]\r\nbound_inputs: [1]\r\n-----------------------------------------------------\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction signature_wrapper(*, x)\r\n  Args:\r\n    x: float32 Tensor, shape=()\r\n  Returns:\r\n    {'output_0': <1>}\r\n      <1>: float32 Tensor, shape=()\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 1})\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [1]\r\nbound_inputs: [1]\r\n-----------------------------------------------------\r\n```\r\nIt seems `capture_fn` is captured by `serialize_concrete_function`, while `polymorphic_fn` are not captured. But what is `signature_wrapper` and why is it captured?\r\n\r\nSo back to the original problem, Why `train(xyz_batch, color_batch, step)` is captured by `serialize_concrete_function` and what causes the key error in serializing `def train(self, xyz_batch, color_batch, step)`?", "```\r\nclass ExampleModel(tf.Module):\r\n  # def __init__(self):\r\n  #   self.polymorphic_fn(tf.constant(4.0))\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\r\n  def capture_fn(self, x):\r\n    if not hasattr(self, 'weight'):\r\n      self.weight = tf.Variable(5.0, name='weight')\r\n      self.bias = tf.Variable(5.0, name='bias')\r\n    self.weight.assign_add(x * self.weight)\r\n    self.bias.assign_add(x * self.weight)\r\n    return self.weight\r\n\r\n  @tf.function\r\n  def polymorphic_fn(self, x):\r\n    return tf.constant(3.0) * x\r\n\r\nmodel = ExampleModel()\r\n# model.polymorphic_fn(tf.constant(4.0))\r\n# model.polymorphic_fn(tf.constant(4.0))\r\nmodel.polymorphic_fn(tf.constant([1.0, 2.0, 3.0]))\r\ntf.saved_model.save(\r\n    model, \"/tmp/example-model\", signatures={'capture_fn': model.capture_fn})\r\n\r\n```\r\n**Output**\r\n```\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction capture_fn(x)\r\n  Args:\r\n    x: float32 Tensor, shape=()\r\n  Returns:\r\n    float32 Tensor, shape=()\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 1, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 2})\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [1]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [1, 2]\r\nbound_inputs: [1, 2]\r\n-----------------------------------------------------\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction polymorphic_fn(x)\r\n  Args:\r\n    x: float32 Tensor, shape=(3,)\r\n  Returns:\r\n    float32 Tensor, shape=(3,)\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 1, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 2})\r\nbound_inputs: []\r\n-----------------------------------------------------\r\n-----------------------------------------------------\r\nconcrete_function: ConcreteFunction signature_wrapper(*, x)\r\n  Args:\r\n    x: float32 Tensor, shape=()\r\n  Returns:\r\n    {'output_0': <1>}\r\n      <1>: float32 Tensor, shape=()\r\nnode_ids: ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 1, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>>: 2})\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [1]\r\ncapture: tf.Tensor(<unprintable>, shape=(), dtype=resource)\r\nbound_inputs: [1, 2]\r\nbound_inputs: [1, 2]\r\n-----------------------------------------------------\r\n\r\n```\r\n\r\nIt seems as long as `polymorphic_fn` is called once, it will be captured by `serialize_concrete_function`", "Back to the original problem, I find bound_inputs of `inference` is [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26, 9], the  length of which is 18, exactly what I expected of the number of weight variables in the model. Also bound_inputs of `train` is also  [13, 14, 15, 16, 4, 17, 18, 19, 5, 20, 21, 22, 23, 8, 24, 25, 26, 9], the same with bound_inputs of `inference` .\r\n\r\n```\r\ndef serialize_concrete_function(concrete_function, node_ids, coder):\r\n  \"\"\"Build a SavedConcreteFunction.\"\"\"\r\n  bound_inputs = []\r\n  try:\r\n    # import pdb;pdb.set_trace()\r\n    print('-----------------------------------------------------')\r\n    print(\"concrete_function:\", concrete_function)\r\n    print('node_ids:',node_ids)\r\n    for capture in concrete_function.captured_inputs:\r\n      print('capture:', capture)\r\n      bound_inputs.append(node_ids[capture])\r\n      print(\"bound_inputs:\", bound_inputs)\r\n    print(\"bound_inputs:\",bound_inputs)\r\n    print('-----------------------------------------------------')\r\n```\r\nFrom this `serialize_concrete_function`, there must be some of the `capture` not in the dict of `node_ids`, causing the error messsage, then the mission is how to find those `capture`?\r\n\r\nThe error message of \r\nKeyError: \"Failed to add concrete function b'__inference_train_11465' to object based saved model as it captures tensor **tf.Tensor(<unprintable>, shape=(), dtype=resource)** which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).\"\r\n\r\nHow to find the name of that **tf.Tensor(<unprintable>, shape=(), dtype=resource)**?", "@sushreebarsa, @Saduf2019 I finally track the problem, but don't know how to solve it, it seems to be the tensorflow internal error. Following is the minimum code to reproduce the error.\r\n\r\n```\r\nimport os\r\nimport datetime\r\nimport tensorflow as tf\r\n\r\n\r\nclass ExampleModel(tf.Module):\r\n  def __init__(self):\r\n    stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n    logdir = 'logs/test11/%s' % stamp\r\n    self.summary_writer = tf.summary.create_file_writer(logdir)\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.float32)])\r\n  def capture_fn(self, x):\r\n    if not hasattr(self, 'weight'):\r\n      self.weight = tf.Variable(5.0, name='weight')\r\n    self.weight.assign_add(x * self.weight)\r\n\r\n    # no error if these two lines are commented\r\n    with self.summary_writer.as_default():\r\n      tf.summary.scalar('loss', tf.constant(0, dtype=tf.int64), step=tf.constant(0, dtype=tf.int64))\r\n\r\n    return self.weight\r\n\r\n  @tf.function\r\n  def polymorphic_fn(self, x):\r\n    return tf.constant(3.0) * x\r\n\r\nmodel = ExampleModel()\r\nmodel.polymorphic_fn(tf.constant([1.0, 2.0, 3.0]))\r\ntf.saved_model.save(\r\n    model, \"/tmp/example-model\", signatures={'capture_fn': model.capture_fn})\r\n\r\n```\r\n**error message**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test11.py\", line 35, in <module>\r\n    model, \"/tmp/example-model\", signatures={'capture_fn': model.capture_fn})\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1195, in save\r\n    save_and_return_nodes(obj, export_dir, signatures, options)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1230, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1401, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 1355, in _build_meta_graph_impl\r\n    options.namespace_whitelist, options.experimental_custom_gradients)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 857, in _fill_meta_graph_def\r\n    signatures = _generate_signatures(signature_functions, resource_map)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 601, in _generate_signatures\r\n    function, mapped_inputs, resource_map)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 553, in _call_function_with_mapped_captures\r\n    resource_map)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/saved_model/save.py\", line 476, in _map_captures_to_created_tensors\r\n    \"\\n\".join([repr(obj) for obj in trackable_referrers])))\r\nAssertionError: Tried to export a function which references untracked resource Tensor(\"85:0\", shape=(), dtype=resource). TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.\r\n\r\n```\r\n\r\n\r\nThe problem now becomes whether there's a way to use `tf.summary.scalar` in function decorated with`tf.function` ,and then the model is saved with `tf.saved_model.save\r\n`", "@sushreebarsa, @Saduf2019 I finally track the problem, but don't know how to solve it, it seems to be the tensorflow internal error. ", "@jvishnuvardhan Please make some update, thanks!", "Hi @sjtusmartboy ,Sorry for the late response, I was able to replicate this issue in [2.5](https://colab.research.google.com/gist/mohantym/edd42c8bd34781ccb4b0167c892c470e/github_51587.ipynb#scrollTo=1GTOs0VkWXM-) , [2.6 ](https://colab.research.google.com/gist/mohantym/621f4743875ecce01f2af477bf1e1218/github_51587_2-6.ipynb)and [nightly.](https://colab.research.google.com/gist/mohantym/1ed1f676f1a059791e19a2e9dead18b1/github_51587.ipynb#scrollTo=1GTOs0VkWXM-) ,Attaching [issue ](https://stackoverflow.com/questions/64847875/tf-keras-model-save-assertionerror-tried-to-export-a-function-which-reference)with similar Error stack trace for answer . Thanks!", "@k-w-w Can you please take a look at this issue? Thanks!", "@sjtusmartboy Sorry for the confusing error. Can you try passing `experimental_trackable=True` to your call to `tf.summary.create_file_writer()`? Docs: https://www.tensorflow.org/api_docs/python/tf/summary/create_file_writer\r\n\r\nNote that this parameter is only experimental and may still be subject to backwards-incompatible changes in future versions of TensorFlow.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51587\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51587\">No</a>\n"]}, {"number": 51584, "title": "how to record loss with tf.summary in the tf.function graph mode", "body": "**tensorflow 2.5, python 3.7**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport datetime\r\n\r\nclass Dense(tf.Module):\r\n    def __init__(self, input_dim, output_size, name=None):\r\n        super(Dense, self).__init__(name=name)\r\n        self.w = tf.Variable(\r\n            tf.random.normal([input_dim, output_size]), name='w')\r\n        self.b = tf.Variable(tf.zeros([output_size]), name='b')\r\n\r\n    def __call__(self, x):\r\n        y = tf.matmul(x, self.w) + self.b\r\n        return tf.nn.relu(y)\r\n\r\nmodel = Dense(2,4)\r\n\r\nclass Test(object):\r\n    def __init__(self):\r\n        self.output = model([[7.0, 3]])\r\n        self.optimizer = tf.compat.v1.train.AdamOptimizer(0.4)\r\n\r\n        self.step = 0\r\n\r\n        stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n        logdir = 'logs/test8/%s' % stamp\r\n\r\n        self.summary_writer = tf.summary.create_file_writer(logdir)\r\n        tf.summary.trace_on(graph=True, profiler=False)\r\n\r\n        for i in range(10):\r\n            self.run()\r\n\r\n    def compare(self, y_true, output):\r\n        return tf.square(y_true - output)\r\n\r\n    def loss_fn(self):\r\n        y_true = tf.ones([1,4])\r\n        self.output = model([[7.0, 3]])\r\n\r\n        comp = tf.py_function(self.compare, [y_true , self.output], tf.float32)\r\n        loss = tf.reduce_mean(comp) # error\r\n\r\n        # output2 = model([[7.0, 3]])\r\n        # loss = tf.reduce_mean(tf.square(y_true - output2))\r\n\r\n        tf.print(loss)\r\n        tf.print(self.step)\r\n\r\n        with self.summary_writer.as_default():\r\n            tf.summary.scalar('loss', loss, step=self.step)\r\n\r\n        self.step += 1\r\n        return loss\r\n\r\n    # @tf.function(autograph=True, jit_compile=True)\r\n    @tf.function()\r\n    def run(self):\r\n        train_op = self.optimizer.minimize(self.loss_fn)\r\n\r\nTest()\r\n```\r\n\r\nAs can be seen from the output, the variable `self.step` is not updated in `def loss_fn(self)`. The reason is simple from what is said in https://tensorflow.google.cn/tensorboard/migrate, \r\n\r\n```\r\nThe \"step\" value must be passed into each op via a the step argument\r\n\r\nTensorBoard requires a step value to render the data as a time series\r\nExplicit passing is necessary because the global step from TF 1.x has been removed, so each op must know the desired step variable to read\r\n```\r\n\r\nHowever, according to https://tensorflow.google.cn/api_docs/python/tf/compat/v1/train/AdamOptimizer?hl=en&version=nightly#expandable-3, \r\n\r\n`When eager execution is enabled, loss should be a Python function that takes no arguments and computes the value to be minimized`.\r\n\r\nDefinitely these two announcement contradict to each other when trying to use `tf.summary` in `def loss_fn(self)`. Any solutions? I just want to record the loss with `tf.summary`.", "comments": ["Problem solved by changing the code to graph mode", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51584\">No</a>\n"]}, {"number": 51581, "title": "Register unique op for Default device.", "body": "Adding unique op for DEVICE_DEFAULT.\r\n@penpornk ", "comments": []}, {"number": 51580, "title": "[XLA] Add back: lift bitcast PR ", "body": "#50153 was reverted.\r\nI'm adding it back with a fix: now we do not lift if any instruction change the layout.\r\n@akuegel ", "comments": ["I asked and the important is the user driver and the minimum driver version is 470 with CUDA 11.2.\r\nSo all should be good.\r\nI added the printing of the driver version when the error happens. Just in case something strange make that test run with an older driver.", "@nouiz  Can you please resolve conflicts? Thanks!", "> @nouiz Can you please resolve conflicts? Thanks!\r\n\r\nDone.", "Any idea what is happening?", "> Any idea what is happening?\r\n\r\nSorry for the delay, the problem is that the process is complicated. I approved the PR in Gerrit when it was imported from Github, as such I was not allowed to also approve it when it was sent out internally for review (it has to be approved by someone else). Then, additionally the test we got from which I had extracted the reproducer did not work anymore by now, so I couldn't verify myself that the issue is completely fixed, and I warned the reporter of the issue that the changelist will land again. After they didn't react, @cheshire approved the changelist and it got committed.", "Thanks for the follow up.", "I bisected a new test failure in JAX to this PR. I've only seen the failure under CUDA 10.2. I was able to reproduce it on a T4 GPU and a GTX 1080, so I don't think it's GPU-dependent.\r\n\r\nThe following HLO will reproduce the problem:\r\n```\r\nHloModule primitive_computation_reduce_sum.8\r\n\r\n%primitive_computation_add.3 (parameter.4: f32[], parameter.5: f32[]) -> f32[] {\r\n  %parameter.4 = f32[] parameter(0), metadata={op_type=\"add\" op_name=\"add\"}\r\n  %parameter.5 = f32[] parameter(1), metadata={op_type=\"add\" op_name=\"add\"}\r\n  ROOT %add.6 = f32[] add(f32[] %parameter.4, f32[] %parameter.5), metadata={op_type=\"add\" op_name=\"add\"}\r\n}\r\n\r\nENTRY %primitive_computation_reduce_sum.8 (parameter.1: f32[2,20,25,4]) -> f32[4] {\r\n  %parameter.1 = f32[2,20,25,4]{3,2,1,0} parameter(0), metadata={op_type=\"reduce_sum\" op_name=\"reduce_sum[ axes=(0, 1, 2) ]\"}\r\n  %constant.2 = f32[] constant(0), metadata={op_type=\"reduce_sum\" op_name=\"reduce_sum[ axes=(0, 1, 2) ]\"}\r\n  ROOT %reduce.7 = f32[4]{0} reduce(f32[2,20,25,4]{3,2,1,0} %parameter.1, f32[] %constant.2), dimensions={0,1,2}, to_apply=%primitive_computation_add.3, metadata={op_type=\"reduce_sum\" op_name=\"reduce_sum[ axes=(0, 1, 2) ]\"}\r\n}\r\n```\r\n\r\nThe symptom is the computation fails with `CUDA_ERROR_ILLEGAL_ADDRESS`.\r\n\r\nI'm going to attempt to roll back the PR.\r\n\r\n(It's possible this is not a bug in this PR but perhaps in something else, e.g., the CUDA toolkit, but rolling back seems like the safest course for the moment.)\r\n", "What was the problem? Crash? wrong value? Something else?", "On device crash (`CUDA_ERROR_ILLEGAL_ADDRESS`).", "@nouiz Do we have a clear reproducer which shows that index calculations from the bitcast are too slow? I'm still really surprised it's non-negligible compared to the memory read. I'd also be very curious if it can be bypassed in other ways (e.g. using linear index directly and not decomposing it at every step)", "@nouiz I'm wondering if the problem I outlined above is a `ptxas` bug. I tried using a CUDA 10.2 installation but replacing the `ptxas` binary that XLA uses at runtime with `ptxas` from CUDA 11.4. The test now passes.\r\n\r\nIf you can confirm this is a `ptxas` miscompilation that has already been fixed, we can probably just roll forward (given I doubt `ptxas` bugs in CUDA 10.2 are ever going to be fixed at this point...)?", "Edit: note: I just saw a very similar looking illegal address bug that also appears to involve a reduction (https://github.com/google/jax/issues/7752) and that also goes away with a newer `ptxas`.", "@hawkinsp I can't reproduce this on my current container.\r\nI do not have an CUDA 10.2 container that works with upstream TF. Some libs are missing. Do you have one you can share?\r\n\r\n@cheshire you undid recently a WAR again a CUDA bug. Could this be the issue? I'm talking about this revert commit:\r\n366e91a4316874c93e6fc10307b1cda6c4f2f712\r\n\r\nThe commit said that it is fixed in CUDA 10.2. But I do not know if it is fixed for all CUDA10.2, or only the the latest update of CUDA 10.2.2. @hawkinsp Which CUDA 10.2.* do you use? Can you try CUDA 10.2.2 or revert commit 366e91a4316874c93e6fc10307b1cda6c4f2f712 and see if this fix the bug?", "@hawkinsp It's pretty much a certainty this is a ptxas bug, we have seen that one before.", "> @cheshire you undid recently a WAR again a CUDA bug. Could this be the issue? I'm talking about this revert commit:\r\n366e91a\r\n\r\nYes, it could be that one.", "> @nouiz Do we have a clear reproducer which shows that index calculations from the bitcast are too slow? I'm still really surprised it's non-negligible compared to the memory read. I'd also be very curious if it can be bypassed in other ways (e.g. using linear index directly and not decomposing it at every step)\r\n\r\nI had a repro. I can dig it if you want. The issue was more pronounced on A100 then on V100.\r\nNote, the indexing is not trivial indexing. Sometimes LLVM optimize it well, but not always.\r\nWhen it doesn't, then we end up with division and module. XLA takes the linear index and map it back to an nd-indexing.\r\nInteger division and modulo are costly. If you combine that with a fusion that does non-trivial computation, then you hit the problem.\r\n\r\nNote, I already optimized such computation (linear indexing to nd-indexing) in previous project. So I'm not surprised to see that again.\r\n\r\nI think for now, we should check if the issue is related to 366e91a or not. ", "@nouiz Here's another way to reproduce: just put the `ptxas` from 10.2 in your `PATH` using a TF tree otherwise built using 11.x. (This means that XLA will use it to compile PTX; it's irrelevant what TF does for its kernels and this reproducer doesn't call anything precompiled.)\r\n\r\nI'm using the CUDA debian packages:\r\n```\r\nii  cuda-nvcc-10-2                     10.2.89-1                                   amd64        CUDA nvcc\r\n```\r\n\r\n`ptxas` reports itself as:\r\n```\r\n$ /usr/local/cuda-10.2/bin/ptxas --version                                                                                                                                             \u2502\r\nptxas: NVIDIA (R) Ptx optimizing assembler                                                                                                                                                                       \u2502\r\nCopyright (c) 2005-2019 NVIDIA Corporation                                                                                                                                                                       \u2502\r\nBuilt on Wed_Oct_23_19:23:38_PDT_2019                                                                                                                                                                            \u2502\r\nCuda compilation tools, release 10.2, V10.2.89\r\n```", "I verified that locally reverting https://github.com/tensorflow/tensorflow/commit/366e91a4316874c93e6fc10307b1cda6c4f2f712 appears to fix the problem.\r\n", "So, I think the solution is to revert it and add back my PR?\r\nDo I need to make a new PR?\r\n\r\nI wasn't able to reproduce the bug.", "@nouiz Yes, I think that works. No need for a new PR, I'll just roll back the roll back I made.", "Maybe not rollback just yet, https://partners.nvidia.com/Bug/ViewBug/2827811 claims that the ptxas bug was fixed in 10.2, was it actually not fixed?", "It was discussed to have the fix in CUDA 10.2, but it didn't happens. The fix is in CUDA 11.1.\r\nSo unless you drop CUDA 10.2 and CUDA 11.0 support, your WAR must be added back."]}, {"number": 51579, "title": "Error: \"Failed to connect to all addresses\", TPU on Colab", "body": "When I attempt to train on Colab using Keras and TPU, I get this error and training stops before the first epoch.\r\n\r\nLink to code for reproduction: https://colab.research.google.com/drive/1ddTaQNmKbapwDPhrPlDKpnoCb8Exa_XI?usp=sharing", "comments": ["@Tylersuard Please post this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThank you!", "@sushreebarsa Thank you!", "@Tylersuard Could you please confirm if you have posted this issue on Keras repo ? If you did so please move this issue to closed status .Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51579\">No</a>\n"]}, {"number": 51578, "title": "Add missing commas causing unwanted str concat", "body": "These are most likely unintentionally and causing the two strings to be implicitly concatenated. These are hard to catch bugs and were found using a Regex.", "comments": []}, {"number": 51577, "title": "Update flatbuffers in workspace.bzl", "body": "Updated Flatbuffers to \"2.0\" Uas dependency change to \"https://github.com/tensorflow/tensorflow/pull/51504\"", "comments": ["Flatbuffer 2.0 has some incompatible changes that we need to resolve from internally first. It will be difficult to debug and test from external contribution. Therefore I'll reject this PR for now, and we'll do the upgrade later. "]}, {"number": 51576, "title": "InvalidArgumentError in Model Maker Object Detection Tutorial ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. I used this tutorial Jupyter notebook. https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=Fw5Y7snSuG51\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab \r\n- TensorFlow version (use command below): ('v2.1.0-0-ge5bf8de410', '2.1.0')\r\n- Python version: 2.7.17\r\n\r\n**Describe the current behavior**\r\nIn the step 6, I used the following command to evaluate the TFlite model but I got an error message.  \r\n```\r\nmodel.evaluate_tflite('model.tflite', test_data)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-9-cf2cd774ff7b> in <module>()\r\n----> 1 model.evaluate_tflite('model.tflite', test_data)\r\n\r\n8 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py in evaluate_tflite(self, tflite_filepath, data)\r\n    187     ds = data.gen_dataset(self.model_spec, batch_size=1, is_training=False)\r\n    188     return self.model_spec.evaluate_tflite(tflite_filepath, ds, len(data),\r\n--> 189                                            data.annotations_json_file)\r\n    190 \r\n    191   def _export_saved_model(self, saved_model_dir: str) -> None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py in evaluate_tflite(self, tflite_filepath, dataset, steps, json_file)\r\n    386       normalize_factor = tf.constant([height, width, height, width],\r\n    387                                      dtype=tf.float32)\r\n--> 388       nms_boxes *= normalize_factor\r\n    389       if labels['image_scales'] is not None:\r\n    390         scales = tf.expand_dims(tf.expand_dims(labels['image_scales'], -1), -1)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in r_binary_op_wrapper(y, x)\r\n   1398       #   r_binary_op_wrapper use different force_same_dtype values.\r\n   1399       y, x = maybe_promote_tensors(y, x)\r\n-> 1400       return func(x, y, name=name)\r\n   1401 \r\n   1402   # Propagate func.__doc__ to the wrappers\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in _mul_dispatch(x, y, name)\r\n   1708     return sparse_tensor.SparseTensor(y.indices, new_vals, y.dense_shape)\r\n   1709   else:\r\n-> 1710     return multiply(x, y, name=name)\r\n   1711 \r\n   1712 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in multiply(x, y, name)\r\n    528   \"\"\"\r\n    529 \r\n--> 530   return gen_math_ops.mul(x, y, name)\r\n    531 \r\n    532 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in mul(x, y, name)\r\n   6234       return _result\r\n   6235     except _core._NotOkStatusException as e:\r\n-> 6236       _ops.raise_from_not_ok_status(e, name)\r\n   6237     except _core._FallbackException:\r\n   6238       pass\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6939   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6940   # pylint: disable=protected-access\r\n-> 6941   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6942   # pylint: enable=protected-access\r\n   6943 \r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: required broadcastable shapes [Op:Mul]\r\n```\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=Fw5Y7snSuG51\r\n\r\n", "comments": ["@dyoo-dev ,\r\nWe see that you are using tf v2.1. Can you please try to execute the code in latest stable version tf v2.6 and let us know if you are facing same issue.\r\nI try to execute the code in v2.6 and executed without any issue.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/4a632115f0513539aa330850493a71c7/model-maker-object-detection-tutorial.ipynb).Thanks\r\n", "@tilakrayal Thank you for your reply. I tried using tf v2.6 and had the same error. \r\nAlso, I looked at the code that you shared. I noticed that the code had the same issue. In step 6, the code has the same error message. ", "@dyoo-dev ,\r\nCan you please take a look at this links with similar error.[Link1](https://discuss.tensorflow.org/t/invalidargumenterror-required-broadcastable-shapes-op-mul/3824),[link2](https://stackoverflow.com/questions/67557515/invalidargumenterror-required-broadcastable-shapes-at-locunknown).It helps.Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51576\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51576\">No</a>\n"]}, {"number": 51575, "title": "[XLA, test] Fix a test when few GPU memory is available. ", "body": "This happens in particular when bazel run many tests on the same GPU.\r\n\r\n@cheshire \r\n\r\nThe error I got:\r\n```\r\n2021-08-18 22:13:48.303536: W tensorflow/stream_executor/stream_executor_pimpl.cc:497] Not enough memory to allocate 3600000000 on device 0 within provided limit. [used=0, limit=2147483648]\r\ntensorflow/compiler/xla/service/gpu/tests/tree_reduction_rewriter_test.cc:47: Failure\r\nValue of: RunMultipleTimes(hlo_text, true, &profiles, \"\", true)\r\n  Actual: false (Failed to allocate request for 3.35GiB (3600000000B) on device ordinal 0)\r\nExpected: true\r\n```\r\n\r\nWhen we tell bazel to run many tests in parallel, it set by default 2 G per jobs. This also speed up the test at the same time.", "comments": ["@nouiz Can you please resolve conflicts? Thanks!\r\n", "rebased."]}, {"number": 51574, "title": "Missing _keras_logits from model.predict(x) leads to difference in loss evaluation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.9.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: 11.2/7.6.5 \r\n- GPU model and memory: GeForce RTX 2080ti, 11019MiB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n`model.predict(x)` does not have attribute `_keras_logits`\r\n**Describe the expected behavior**\r\n`model.predict(x)` and `model(x)` produce identical outputs.\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(999)\r\n\r\nloss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\r\ny_true = [[1, 0, 0]]\r\ninputs = tf.keras.layers.Input(shape=(3,))\r\noutputs = tf.keras.layers.Dense(3, \r\n            activation='softmax',\r\n            kernel_initializer=tf.keras.initializers.glorot_normal(seed=42))(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\nx = np.random.random((1, 3))\r\ny_pred1 = model.predict(x)\r\ny_pred2 = model(x)\r\n\r\n# losses are different. One uses logits, the other doesn't.\r\nprint('model.predict(x) loss = ', loss_fn(y_true, y_pred1))\r\nprint('model(x) loss = ', loss_fn(y_true, y_pred2))  \r\n\r\nassert hasattr(y_pred1, '_keras_logits') == hasattr(y_pred2, '_keras_logits')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi! @Saduf2019 , Could you please look into this issue , providing [gist](https://colab.research.google.com/gist/mohantym/f9cfabd735f161da34c423a2bab113ff/github_51574.ipynb#scrollTo=cEQWVWqmiy6v) replicated in TF 2.3,2.4,2.5 for your reference. Issue is not occurring in TF 2.3 but afterwards .", "Thanks for looking into this. Related, it looks like the current way `binary_crossentropy` is written will always use logits so long as there is `_keras_logits` attribute cached and ignore `from_logits=` provided by the user. Is this intended? \r\n\r\nI was referring to this code block inside `def binary_crossentropy(target, output, from_logits=False):`\r\n```\r\nif hasattr(output, '_keras_logits'):\r\n    output = output._keras_logits  # pylint: disable=protected-access\r\n    if from_logits:\r\n      warnings.warn(\r\n          '\"`binary_crossentropy` received `from_logits=True`, but the `output`'\r\n          ' argument was produced by a sigmoid or softmax activation and thus '\r\n          'does not represent logits. Was this intended?\"')\r\n    from_logits = True\r\n```", "It has been done purposely I guess...... Cause you only convert it to only logits when you are training..... You don't need to logits when you infering", "> It has been done purposely I guess...... Cause you only convert it to only logits when you are training..... You don't need to logits when you infering\r\n\r\nSorry I didn't mean inference but forward pass to compute the loss. Doesn't it look to you that logits will be used for training even when user set `from_logits=False`? ", "If you are using it for training then ....then use model(x).....Cause if you use model.predict function inside a function which is wrapped around @tf.function.... Then it throws an error. model.predict is only used while inferring while model(x) is used when you are training\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @mohantym @Saduf2019, any updates?\r\n\r\nThanks.", "@don-tpanic \r\nPlease post this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues).\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n\r\nlet us know of [this](https://github.com/tensorflow/tensorflow/issues/51574#issuecomment-902777975) helps", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51574\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51574\">No</a>\n"]}, {"number": 51573, "title": "Build Fails with TF 2.6 on VS2019 (Win 10)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.6\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): VS2019 \r\n- CUDA/cuDNN version: 11.2 / 8.1.0\r\n- GPU model and memory: 2080 Max Q\r\n\r\nWhen attempting to build from source using the following command, it fails to build the pip package,\r\n\r\n**bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package**\r\n\r\n```\r\nERROR: C:/sdks/tensorflow/tensorflow/python/util/BUILD:610:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/adam/_bazel_adam/e7merofc/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30037\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30037\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.18362.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\\\Extensions\\Microsoft\\IntelliCode\\CLI;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30037\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\vs2019\\;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.18362.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Adam/anaconda3/envs/tensorflow/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Adam/anaconda3/envs/tensorflow/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\Adam\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\Adam\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/Enterprise/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\ncl : Command line warning D9002 : ignoring unknown option '-fno-strict-aliasing'\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C3861: 'copysign': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 69.130s, Critical Path: 6.47s\r\nINFO: 29 processes: 24 internal, 5 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["Update:- This issue looks like it is related to python 3.8, as I have rerun it with python 3.9 and it builds successfully. ", "@oracle3001 Thank you for the update!Could you please let us know if this issue is resolved ?Please feel free to move this issue to closed status if it is resolved .Thanks!", "This is still an issue for python 3.8. ", "@oracle3001 Could you please refer to the [link](https://www.tensorflow.org/install/source_windows) and let us know if it helps?Thank you!", "TF 2.6 builds absolutely fine with Python 3.9. However, if I keep everything the same, except I try to build versus Python 3.8, it fails with the error message above.\r\n\r\nThere is clearly some issue specifically with Python 3.8 / VS2019 and TF 2.6. \r\n\r\nTF 2.5 with 3.8 also builds fine. ", "Does Python 3.7 build ok?\r\n\r\nWe have encountered this issue and we patched it by using a slightly older MSVC compiler. The issue is on what symbols are publicly available under MSVC", "@oracle3001 \r\n\r\n> ERROR: C:/sdks/tensorflow/tensorflow/python/util/BUILD:610:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command\r\n\r\nThis error is more related to Microsoft C Compiler. Make sure MSVC 2019 PATH is properly set where the python is installed, so that cl.exe can be found. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51573\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51573\">No</a>\n"]}, {"number": 51571, "title": "Make ```tf.data.experimental.make_csv_dataset``` return many labels", "body": "**System information**\r\n- TensorFlow version (you are using):2.6.0\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n```tf.data.experimental.make_csv_dataset``` not can only support setting one column for label name . However , in some model , such as MMoE , there are many outputs .   \r\n\r\n**Will this change the current api? How?**\r\nIt will change the str parameter ```label_name``` to list parameter ```label_names``` like ```column_names``` \r\n\r\n**Who will benefit with this feature?**\r\nEveryone who use model with multiple outputs .\r\n\r\n**Any Other info.**\r\nI would like to change the code \r\n```\r\n  def map_fn(*columns):\r\n    \"\"\"Organizes columns into a features dictionary.\r\n    Args:\r\n      *columns: list of `Tensor`s corresponding to one csv record.\r\n    Returns:\r\n      An OrderedDict of feature names to values for that particular record. If\r\n      label_name is provided, extracts the label feature to be returned as the\r\n      second element of the tuple.\r\n    \"\"\"\r\n    features = collections.OrderedDict(zip(column_names, columns))\r\n    if label_name is not None:\r\n      label = features.pop(label_name)\r\n      return features, label\r\n    return features\r\n```\r\nto \r\n```\r\n  def map_fn(*columns):\r\n    \"\"\"Organizes columns into a features dictionary.\r\n    Args:\r\n      *columns: list of `Tensor`s corresponding to one csv record.\r\n    Returns:\r\n      An OrderedDict of feature names to values for that particular record. If\r\n      label_name is provided, extracts the label feature to be returned as the\r\n      second element of the tuple.\r\n    \"\"\"\r\n    features = collections.OrderedDict(zip(column_names, columns))\r\n    if label_names is not None:\r\n      if len(label_names) >= 2:\r\n        labels = collections.OrderedDict()\r\n        for label_name in label_names :\r\n          labels[label_name] = features.pop(label_name)\r\n        return features, labels\r\n      elif len(label_names) == 1:\r\n        label= features.pop(label_names[0])\r\n        return features, label\r\n    return features\r\n```\r\n", "comments": ["@DachuanZhao \r\nPlease feel free to create a pr for the proposed change or share the link where the change is to be made.", "> @DachuanZhao\r\n> Please feel free to create a pr for the proposed change or share the link where the change is to be made.\r\n\r\nHi , I have created a pr . If you have time , you can have a look at it . : )", "@DachuanZhao \r\nPlease update the pr as requested for.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 51570, "title": "RuntimeError: tensorflow/lite/kernels/conv.cc:349 input->dims->data[3] != filter->dims->data[3] (64 != 2)Node number 12 (CONV_2D) failed to prepare.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installation (pip package or built from source): pip package \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly-2.7.0.dev20210818\r\n- model arc: CRNN\r\n\r\n- my converter code, succeeded in tf2.6.0 and tf-nightly but failed in tf2.3.0,\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter._experimental_lower_tensor_list_ops = False\r\nconverter.representative_dataset = representative_data_gen\r\nlite_model = converter.convert()\r\nlite_model_file = f'{fp16_model_path}/crnn_lite.tflite'\r\nwith open(lite_model_file, 'wb') as f:\r\n    f.write(lite_model)\r\n```\r\nafter call **interpreter.allocate_tensors()**, runtime errors occur\u3002in this issue [https://github.com/tensorflow/tensorflow/issues/44548](url), install tf-nightly can fix this. please help me how can i fix this? thanks \r\n", "comments": ["Hi!@hotpeppeper , Can you please share complete code  including` interpreter.allocate_tensors()` snippet  . Could you please check out this [link](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter#usage_4) whether the code is as per official document or not?", "@mohantym model_file is the tflite file path\r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=model_file)\r\nprint(interpreter.get_input_details())\r\nprint(interpreter.get_output_details())\r\ninterpreter.allocate_tensors()\r\n```", "Could you try with this snippet \r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=model_file)\r\ninterpreter.allocate_tensors()\r\nprint(interpreter.get_input_details())\r\nprint(interpreter.get_output_details())\r\n```\r\n", "@mohantym the same runtime error", "Could you  please confirm the issue is not replicating in the latest version 2.6 or nightly but only in 2.3?", "@mohantym convert to tflite failed when i am using tf2.3\r\n```\r\nloc(callsite(callsite(callsite(unknown at \"crnn/bi_rnn/sequential_8/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_20624\") at \"StatefulPartitionedCall@__inference_signature_wra pper_72516\") at \"StatefulPartitionedCall\")): error: We cannot duplicate the value since it's not constant.\r\n error: Failed to duplicate values for the stateful op\r\n\r\nload model path:/data/xjming/model/Parse/crnn_ocr/git_loss_model/iter_19765000_weight_model.h5\r\nTraceback (most recent call last):\r\nFile \"/root/anaconda3/envs/ocr_tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\nenable_mlir_converter)\r\nFile \"/root/anaconda3/envs/ocr_tf2.3/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\nenable_mlir_converter)\r\nException: <unknown>:0: error: loc(callsite(callsite(callsite(unknown at \"crnn/bi_rnn/sequential_8/bidirectional/backward_lstm/PartitionedCall@__inference__wrapped_model_20624\") at \"StatefulPartitione dCall@__inference_signature_wrapper_72516\") at \"StatefulPartitionedCall\")): We cannot duplicate the value since it's not constant.\r\n```", "@mohantym this's the whole log [https://drive.google.com/file/d/1dNODup4OJwx8ShQFxfJNRh1rp-2PgwYe/view?usp=sharing](url)", "Hi @hotpeppeper ,Could you please provide a Colab gist with necessary model files ?", "Hi @ymodak ,Could you please look at this issue.", "@hotpeppeper Its unlikely for TF 2.3 version to receive any bug fixes except when we have security patches. There is a high possibility that this was fixed with later TF versions. Perhaps you can use latest tf versions for your case. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51570\">No</a>\n"]}, {"number": 51569, "title": "`ModuleNotFoundError: No module named 'keras'` in most tensorflow calls", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock script\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Manjaro Pahvo 21.1.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary (official arch repository)\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 11.4.0-1/8.2.2.26-1\r\n- GPU model and memory: Nvidia RTX 3060 Ti (8GB)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nMost (if not all) of the library calls give a `ModuleNotFoundError: No module named 'keras'`.\r\n\r\n**Describe the expected behavior**\r\nAll library calls should work without this import relative reference problem. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): A workaround for now is using `import tensorflow.keras as keras`\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\nprint(tf.version.VERSION)\r\nfashion_mnist = tf.keras.datasets.fashion_mnist\r\n```\r\n(code from: [https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification))\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nFile \"/usr/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\nmodule = self._load()\r\nFile \"/usr/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\nmodule = importlib.import_module(self.__name__)\r\nFile \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\nreturn _bootstrap._gcd_import(name[level:], package, level)\r\nFile \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\nFile \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\nFile \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\r\nFile \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\nFile \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\nFile \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\nFile \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'keras'\r\n```\r\n(output of code snippet above)", "comments": ["@TrentHawkins I tried your code on colab using TF v2.6.0 & didn't face any error reported , please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a29675557bad5134efd510dd264e71af/untitled396.ipynb) for your reference.Thank you! ", "I confirm. So it is an arch package issue. Thank you for your time.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51569\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51569\">No</a>\n"]}, {"number": 51568, "title": "How can I make multiple sessions share a graph? Not found: Container localhost does not exist", "body": "\r\nHow can I make multiple sessions share a graph by c_api?\r\nMy usage is as follows:\r\n**The first session is created using TF_LoadSessionFromSavedModel.\r\nThe second session is created using TF_NewSession.**\r\n\r\nIt is normal to call the TF_SessionRun of the first session. However, when calling the TF_SessionRun of the second session from another thread, the following error is reported:\r\n\r\n2021-08-19 14:56:00.772594: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at resource_variable_ops.cc:633 : **Not found: Container localhost does not exist**. (Could not find resource: localhost/embeddings/charactor_embeddings/weight)\r\n{{function_node __inference__inference_5828_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_at_tf_graph}} {{function_node __inference__inference_5828_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_at_tf_graph}} Error while reading resource variable encoder/layer_._0/attention/self/key/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/encoder/layer_._0/attention/self/key/bias)\r\n\t [[{{node encoder/layer_._0/attention/self/key/BiasAdd/ReadVariableOp}}]]\r\nterminate called after throwing an instance of 'std::runtime_error'\r\n  what():  {{function_node __inference__inference_5828_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_at_tf_graph}} {{function_node __inference__inference_5828_specialized_for_StatefulPartitionedCall_StatefulPartitionedCall_at_tf_graph}} Error while reading resource variable encoder/layer_._0/attention/self/key/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/encoder/layer_._0/attention/self/key/bias)\r\n\t [[{{node encoder/layer_._0/attention/self/key/BiasAdd/ReadVariableOp}}]]\r\n\t [[StatefulPartitionedCall/StatefulPartitionedCall]]\r\n\t [[StatefulPartitionedCall/StatefulPartitionedCall]]Aborted (core dumped)\r\n\r\n\r\n\r\nWhy? Usage problem or bug? We look forward to your reply.\r\n\r\n\r\n**System information**\r\ntensorflow version(2.3.0)\r\nLinux Ubuntu 16.04\r\n\r\n**code snippet:**\r\n\r\nstatic pthread_mutex_t sSpeakerClassToGraphMutex = PTHREAD_MUTEX_INITIALIZER;\r\nstatic std::map<const void*, const void*> sSpeakerClassToGraph;\r\n\r\nModel::Model(const void* classID, const std::string& model_filename, const std::vector<uint8_t>& config_options) {\r\n\r\n\tconst char* v = TF_Version();\r\n\tprintf(\"Model********************************************************************** v(%s)\\n\", v);\r\n        this->status = TF_NewStatus();\r\n\t// Create the session.\r\n\tTF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n\r\n\tif (!config_options.empty())\r\n\t{\r\n\t\tTF_SetConfig(sess_opts, static_cast<const void*>(config_options.data()), config_options.size(), this->status);\r\n\t\tthis->status_check(true);\r\n\t}\r\n\r\n\r\n\tpthread_mutex_lock(&sSpeakerClassToGraphMutex);\r\n\r\n\t std::map<const void*, const void*>::const_iterator iter = sSpeakerClassToGraph.find(classID);\r\n\tif (iter != sSpeakerClassToGraph.cend())\r\n\t{\r\n\t\tthis->graph = (TF_Graph* ) iter->second;\r\n\t\tprintf(\"(%p, %p)\\n\", classID, this->graph);\r\n\t\tthis->session =TF_NewSession(this->graph,sess_opts,status);\r\n\t}\r\n\telse\r\n\t{\r\n\t\tthis->graph = TF_NewGraph();\r\n\t\tTF_Buffer* RunOpts = NULL;\r\n\t\tconst char* tags = \"serve\";\r\n\t\tint ntags = 1;\r\n\t\tthis->session = TF_LoadSessionFromSavedModel(sess_opts, RunOpts, model_filename.c_str(), &tags, ntags, this->graph, NULL, this->status);\r\n\t\tsSpeakerClassToGraph[classID] = graph;\r\n\t\tprintf(\"(%p, %p) create\\n\", classID, graph);\r\n\t}\r\n\tpthread_mutex_unlock(&sSpeakerClassToGraphMutex);\r\n\tif (TF_GetCode(this->status) == TF_OK)\r\n\t{\r\n\t\tprintf(\"TF_LoadSessionFromSavedModel OK\\n\");\r\n\t}\r\n\telse\r\n\t{\r\n\t\tprintf(\"%s\", TF_Message(this->status));\r\n\t}\r\n\tTF_DeleteSessionOptions(sess_opts);\r\n\r\n\t// Check the status\r\n\tthis->status_check(true);\r\n}\r\n\r\n", "comments": ["@hankewei ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!", "Also please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/28287#issuecomment-519130227) with similiar error.It helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/51568\">No</a>\n"]}, {"number": 51567, "title": "Add `--config=rbe_lite_linux` in .bazelrc", "body": "Both `--config=rbe_cpu_linux` and `--config=rbe_linux_py3` expand to `--config=rbe_linux` which includes link opts for avx instructions. However that won't compile Tensorflow Lite. \r\n\r\nThis PR adds `--config=rbe_lite_linux` which reuses most of existing flags for building Tensorflow Lite with RBE.\r\n\r\nContext: b/195294181", "comments": ["Added document for the option `rbe_lite_linux`."]}, {"number": 51566, "title": "Model input channel Question", "body": "Generally, Tensorflow uses channel last. \r\n\r\nBut I get input from channel first because I converted Torch --> ONNX --> Pb --> TFlite.\r\n\r\ninput = (1, 3, 224, 224)\r\nout = TFLiteModel(input)\r\n\r\nIs there a method that can be used as above?", "comments": ["TFLite converter & runtime prefers NHWC data format since we have optimized kernels for the NHWC data format not other data formats. Currently, we don't have such a feature to produce the other data format. We will consider this post as the feature request.", "@abattery \r\nThanks, i solved it by performing a permute on the input."]}]