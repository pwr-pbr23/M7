[{"number": 48111, "title": "\"Graph is finalized and cannot be modified\" when loading saved Estimator", "body": "**System information**\r\n- OS Platform and Distribution: MacOS 11.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source): 4.0.0\r\n- GCC/Compiler version (if compiling from source): clang\r\n\r\n**Describe the current behavior**\r\n\r\nI have a DNNClassifier that I have created as follows:\r\n\r\n```python\r\n\r\n    estimator = tf.estimator.DNNClassifier(\r\n        feature_columns=get_feature_columns(),\r\n        hidden_units=[1024, 1024],\r\n        model_dir=\"./logs\",\r\n        n_classes=n_classes,\r\n        activation_fn=tf.nn.leaky_relu,\r\n        dropout=0.5,\r\n        optimizer=tf.compat.v1.train.RMSPropOptimizer(learning_rate=0.0001, momentum=0.9),\r\n    )\r\n```\r\n\r\nwhere the `get_feature_columns()` function just creates a bunch of `feature_column`s and returns them as a set:\r\n\r\n```python\r\ndef get_feature_columns():\r\n\r\n    feature_columns = [tf.feature_column.numeric_column(col_name) for col_name in numeric_cols]\r\n    ...\r\n    return set(feature_columns)\r\n```\r\n\r\nThe model fits and predicts just fine, and I am able to serialize it with:\r\n\r\n```python\r\nserving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\r\n    tf.feature_column.make_parse_example_spec(get_feature_columns()))\r\n\r\nestimator_path = estimator.export_saved_model('from_estimator', serving_input_fn)\r\n```\r\n\r\nHowever, when I try to load the resulting saved model, it fails with a RuntimeError:\r\n\r\n```python\r\nimported = tf.saved_model.load_v2(estimator_path)\r\n```\r\n\r\n```\r\n\r\nimported = tf.saved_model.load_v2(estimator_path)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n~/Yankees/nn_matchup/models/matchup_model_estimator.py in \r\n----> 554 imported = tf.saved_model.load_v2(estimator_path)\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags, options)\r\n    861     ValueError: If `tags` don't match a MetaGraph in the SavedModel.\r\n    862   \"\"\"\r\n--> 863   return load_internal(export_dir, tags, options)[\"root\"]\r\n    864 \r\n    865 \r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls, filters)\r\n    911                        \"version) cannot be loaded with node filters.\")\r\n    912     with ops.init_scope():\r\n--> 913       root = load_v1_in_v2.load(export_dir, tags)\r\n    914       root.graph_debug_info = debug_info\r\n    915 \r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(export_dir, tags)\r\n    276   \"\"\"Load a v1-style SavedModel as an object.\"\"\"\r\n    277   loader = _EagerSavedModelLoader(export_dir)\r\n--> 278   return loader.load(tags=tags)\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(self, tags)\r\n    222     wrapped = wrap_function.wrap_function(\r\n    223         functools.partial(self.load_graph, load_graph_returns, meta_graph_def),\r\n--> 224         signature=[])\r\n    225     saver, = load_graph_returns\r\n    226     restore_from_saver = self._extract_saver_restore(wrapped, saver)\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)\r\n    628           collections={}),\r\n    629       variable_holder=holder,\r\n--> 630       signature=signature)\r\n    631 \r\n    632 \r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in __init__(self, fn_graph, variable_holder, attrs, signature)\r\n    223   def __init__(self, fn_graph, variable_holder, attrs=None, signature=None):\r\n    224     self._variable_holder = variable_holder\r\n--> 225     _lift_unlifted_variables(fn_graph, variable_holder)\r\n    226     # We call __init__ after lifting variables so that the function's signature\r\n    227     # properly reflects the new captured inputs.\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _lift_unlifted_variables(graph, variable_holder)\r\n    176       if _should_lift_variable(old_variable):\r\n    177         new_variable = _lift_single_variable(\r\n--> 178             old_variable, graph, variable_holder)\r\n    179         lifted_variables[id(old_variable)] = new_variable\r\n    180         existing_captures.add(id(old_variable.handle))\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/eager/wrap_function.py in _lift_single_variable(old_variable, graph, variable_holder)\r\n    128       name=old_variable.op.name,\r\n    129       trainable=old_variable.trainable,\r\n--> 130       extra_handle_data=old_variable.handle)\r\n    131   new_variable._initializer_op = old_variable._initializer_op  # pylint: disable=protected-access\r\n    132   graph.add_capture(new_variable.handle, old_variable.handle)\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    262       return cls._variable_v2_call(*args, **kwargs)\r\n    263     else:\r\n--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    265 \r\n    266 \r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, trainable, caching_device, name, shape, dtype, constraint, synchronization, aggregation, extra_handle_data, distribute_strategy, **unused_kwargs)\r\n   1956             name=name,\r\n   1957             graph_mode=self._in_graph_mode,\r\n-> 1958             initial_value=extra_handle_data)\r\n   1959         if not context.executing_eagerly():\r\n   1960           with ops.name_scope(\"Read\"):\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py in _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, initial_value)\r\n    163       shared_name=shared_name,\r\n    164       name=name,\r\n--> 165       container=container)\r\n    166   if initial_value is None:\r\n    167     initial_value = handle\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py in var_handle_op(dtype, shape, container, shared_name, allowed_devices, name)\r\n   1204         \"VarHandleOp\", dtype=dtype, shape=shape, container=container,\r\n   1205                        shared_name=shared_name,\r\n-> 1206                        allowed_devices=allowed_devices, name=name)\r\n   1207   _result = _outputs[:]\r\n   1208   if _execute.must_record_gradient():\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    748       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n    749                                  name=scope, input_types=input_types,\r\n--> 750                                  attrs=attr_protos, op_def=op_def)\r\n    751 \r\n    752     # `outputs` is returned as a separate return value so that the output\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n   3516       An `Operation` object.\r\n   3517     \"\"\"\r\n-> 3518     self._check_not_finalized()\r\n   3519     if name is None:\r\n   3520       name = op_type\r\n\r\n~/anaconda3/envs/nn_matchup/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _check_not_finalized(self)\r\n   3106     \"\"\"\r\n   3107     if self._finalized:\r\n-> 3108       raise RuntimeError(\"Graph is finalized and cannot be modified.\")\r\n   3109 \r\n   3110   def _add_op(self, op, op_name):\r\n\r\nRuntimeError: Graph is finalized and cannot be modified.\r\n```\r\n", "comments": ["@fonnesbeck \r\nCan you please refer to this link with same error and let us know if it helps:[link](https://gitmemory.com/issue/tensorflow/tensorflow/26430/478064379)", "No, not at all. That example is using TF1 whereas I am on TF2. I am getting errors when loading the saved model, not on saving the model.", "@fonnesbeck \r\nCan you share complete code such that we could replicate the error reported. [ simple stand alone indented code or a colab gist with the error reported]", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48111\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48111\">No</a>\n"]}, {"number": 48110, "title": "[tf.data] graduate experimental `bucket_by_sequence_length` API to tf.data.Dataset", "body": "UPDATED: This PR graduates the `tf.data.experimental.bucket_by_sequence_length` API into `tf.data.Dataset.bucket_by_sequence_length` by making the following changes:\r\n\r\n - [x] Adds the deprecation decorator for the experimental API.\r\n - [x] Adds the necessary `bucket_by_sequence_length` method to `DatasetV2` class.\r\n - [x] Updates example in documentation with new API.\r\n - [x] Regenerate golden API's.\r\n - [x] Moved and updated the `bucket_by_sequence_length` test target from experimental/kernel_tests to kernel_tests\r\n - [x] Updated the RELEASE.md file\r\n\r\nTEST LOG\r\n```\r\nINFO: Build completed successfully, 352 total actions\r\n//tensorflow/python/data/kernel_tests:bucket_by_sequence_length_test     PASSED in 4.5s\r\n```\r\n\r\ncc: @jsimsa ", "comments": ["You will need to regenerate golden API files with:\r\n\r\n`bazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True`", "Sure, will update the PR.", "@jsimsa the `.pbtxt` files have been updated! ", "will take a look.", "@jsimsa so to expose the API as `tf.data.Dataset.bucket_by_sequence_length` the `tensorflow/python/data/ops/dataset_ops.py` has to import `tensorflow/python/data/experimental/ops/grouping.py`. However, this would lead to a cyclic import. I can try with a `LazyLoader` but, wouldn't it be messy?", "> @jsimsa so to expose the API as `tf.data.Dataset.bucket_by_sequence_length` the `tensorflow/python/data/ops/dataset_ops.py` has to import `tensorflow/python/data/experimental/ops/grouping.py`. However, this would lead to a cyclic import. I can try with a `LazyLoader` but, wouldn't it be messy?\r\n\r\nI see. You can avoid the cyclic import by creating BucketBySequenceDataset in dataset_ops.py and moving the implementation there.. See `UnbatchDataset` and `unbatch` in dataset_ops.py and experimental/batching.py for an example how to do this.", "@jsimsa the `bucket_by_sequence_length` API is having a dependency on `group_by_window` API for it's functionality. Lets graduate the `group_by_window` into `tf.data.Data.group_by_window` first so that graduating to `tf.data.Data.bucket_by_sequence_length` becomes straightforward.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48110) for more info**.\n\n<!-- need_author_cla -->", "> @jsimsa the `bucket_by_sequence_length` API is having a dependency on `group_by_window` API for it's functionality. Lets graduate the `group_by_window` into `tf.data.Data.group_by_window` first so that graduating to `tf.data.Data.bucket_by_sequence_length` becomes straightforward.\r\n\r\nsounds good", "@jsimsa made the changes. Please take a look."]}, {"number": 48109, "title": "TF 2.4 still depends on GRPCIO v1.32.0", "body": "GRPCIO was meant to be updated in TF 2.4 but TF 2.4.0 and 2.4.1 still depend on it: \r\n- https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/tools/pip_package/setup.py#L121\r\n- https://github.com/tensorflow/tensorflow/issues/45785#issuecomment-806233485\r\n", "comments": ["The requirements are updated with [TF 2.5 branch](https://github.com/tensorflow/tensorflow/blob/r2.5/tensorflow/tools/pip_package/setup.py#L130). See https://github.com/tensorflow/tensorflow/blob/96dfa5c37c8844d0fa7347c7a43c383ed9813022/tensorflow/tools/pip_package/setup.py#L130", "Thanks @ymodak ", "If we need to update in 2.4, please open a PR on the branch and it will be picked up on an eventual cherrypick.", "Closing this issue now, Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48109\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48109\">No</a>\n"]}, {"number": 48108, "title": "Can't load TFLite model on Android/iOS - NODE PAD failed to prepare ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): TF built on CentOS / current Docker version for Android\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung J2 Core \r\n- TensorFlow installed from (source or binary): Source (device)/Binary (conversion)\r\n- TensorFlow version (use command below): 2.4.1/Nightly\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\n[This model](https://github.com/tensorflow/tensorflow/files/6212646/vad.zip)  has been converted from Pytorch->ONNX->TFLite.\r\n\r\nLoading the ONNX model, converting to a saved model, converting to TFLite and loading in the TFLite interpreter works fine in a notebook on **nightly** (da68297):\r\n```\r\nfrom onnx_tf.backend import prepare\r\nimport onnx\r\nimport tensorflow as tf\r\n\r\nmodel_onnx = onnx.load('vad.onnx')\r\ntf_rep = prepare(model_onnx)\r\ntf_rep.export_graph('./tf_model')\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(\"./tf_model\")\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.allow_custom_ops=False\r\nconverter.experimental_new_converter =True\r\n\r\ntflite_model = converter.convert()\r\n\r\n# Save the model\r\nwith open(\"vad.tflite\", 'wb') as f:\r\n    f.write(tflite_model)\r\n    \r\ninterpreter = tf.lite.Interpreter(model_path=\"vad.tflite\")\r\ninterpreter.allocate_tensors()\r\n    \r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninput_shape = input_details[0]['shape']\r\n\r\ninput_buf = np.ones((1, 64, 1),dtype=np.float32)\r\n\r\ninput_buf=np.array(input_buf,dtype=np.float32)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_buf)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data.shape)\r\n```\r\n\r\nHowever, take the same model and load on Android (with TFLite C++)\r\n\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\n\r\nmodel = tflite::FlatBufferModel::BuildFromFile(filepath_c);\r\n\r\nauto builder = std::unique_ptr<tflite::InterpreterBuilder>(\r\nnew tflite::InterpreterBuilder(*model, resolver));\r\n\r\n(*builder)(&interpreter);\r\n\r\nconst std::vector<int>& inputs = interpreter->inputs();\r\ninterpreter->AllocateTensors();\r\n```\r\n\r\nand this will either fail with \"NODE PAD failed to prepare\" or crash with:\r\n\r\n```\r\nF/libc    (31008): Fatal signal 11 (SIGSEGV), code 1, fault addr 0x7e110870 in tid 31028 (1.ui), pid 31008 (example.example)\r\n*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\nBuild fingerprint: 'samsung/j2corey20ltecis/j2corey20lte:8.1.0/M1AJB/J260FUXXS1AUA1:user/release-keys'\r\nRevision: '2'\r\nABI: 'arm'\r\npid: 31008, tid: 31028, name: 1.ui  >>> com.example.example <<<\r\nsignal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x7e110870\r\n    r0 00000003  r1 7e110870  r2 00000000  r3 00000002\r\n    r8 a4edf754  r9 8eb6687c  sl a4edf740  fp 00000bc2\r\n    ip 7db32ea8  sp 8eb66838  lr 7a3b538d  pc 7a3b5178  cpsr 60070030\r\nbacktrace:\r\n    #00 pc 0051e178  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::ops::builtin::pad::ResizeOutputTensor(TfLiteContext*, tflite::ops::builtin::pad::PadContext*)+51)\r\n    #01 pc 0051e389  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::ops::builtin::pad::Prepare(TfLiteContext*, TfLiteNode*)+264)        \r\n    #02 pc 005c366f  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Subgraph::PrepareOpsStartingAt(int, std::__ndk1::vector<int, std::__ndk1::allocator<int>> const&, int*)+262)\r\n    #03 pc 005c2df1  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Subgraph::PrepareOpsAndTensors()+164)\r\n    #04 pc 005c2c37  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Subgraph::AllocateTensors()+202)\r\n    #05 pc 005c694b  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libtensorflowlite.so (tflite::Interpreter::AllocateTensors()+242)\r\n    #06 pc 000391bf  /data/app/com.example.example-AjJluQWzHSUT6tBdpcGEEA==/lib/arm/libvad.so (mfcc+294)\r\n    #07 pc 000046a0  <anonymous:88080000>\r\n```\r\nOccasionally it seems to successfully move past this Pad operation, and will then fail with Node number 2 (SPLIT) failed to prepare.\r\n\r\nThis happens no matter whether TFLite is built via the official Docker release (current nightly), with select ops, or from source nightly or source/2.4.1.\r\n\r\nAlso, the ONNX model cannot be *converted* to TFLite with 2.4.1, giving the following error:\r\n\r\n```<unknown>:0: error: loc(callsite(callsite(\"Pad_1@__inference___call___8660\" at \"PartitionedCall@__inference_signature_wrapper_8735\") at \"PartitionedCall\")): operand #0 does not dominate this use\r\n<unknown>:0: note: loc(\"PartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"Pad_1@__inference___call___8660\" at \"PartitionedCall@__inference_signature_wrapper_8735\") at \"PartitionedCall\")): operand defined here\r\n```\r\n\r\nIf I set `converter.experimental_new_converter =False`, then I get the following error during conversion:\r\n\r\n```\r\nValueError: None is only supported in the 1st dimension. Tensor 'serving_default_audio_signal' has invalid shape '[None, 64, None]'.\r\n```\r\n\r\nI've tried manually setting the input shapes, and this then fails with other errors\r\n\r\nInspecting the original ONNX model via netron.app doesn't show anything unusual:\r\n\r\n![vad onnx](https://user-images.githubusercontent.com/7238578/112647652-db497980-8e9c-11eb-9a0e-f1d39d26bfcc.png)\r\n\r\nI think I did manage to successfully convert the model once (possibly with 2.3.1), but then experienced a similar \"NODE xx failed to prepare\" when running on Android.\r\n\r\nThe original model was from https://github.com/NVIDIA/NeMo/blob/ddd7e13cc0b81a377a55279eec7fe4ce0752f05e/tutorials/asr/07_Online_Offline_Microphone_VAD_Demo.ipynb, if that helps.\r\n\r\nEDIT: on iOS with TFlite v2.4.1 built from source, the model converted with nightly errors with \"Node number 2 (SPLIT) failed to prepare\" and built with older version (2.3.1? not sure) errors with \r\n```\r\nPad value has to be greater than equal to 0. \r\nNode number 0 (PAD) failed to prepare\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should load properly on TFLite Android/iOS.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Could you verify whether the given input is valid in the above original onnx model and TF saved model? TF saved model can successfully handle the given inputs and we can easily spot the problem location.", "@abattery Thanks for taking a look. There's no problem with running the SavedModel directly:\r\n\r\n![image](https://user-images.githubusercontent.com/7238578/112739102-1a6eec00-8fbd-11eb-9c3d-0e91a18e00ef.png)\r\n\r\n", "@nmfisher if possible, could you provide the saved model directory to us for debugging?", "@abattery Sure, [here you go](https://github.com/tensorflow/tensorflow/files/6218523/tf_model.zip)\r\n", "At the tf-nightly version, the above saved model is successfully converted and the converted model is executed well with the TFLite benchmark tool.", "I think the above input tensor should have the (1, 64, 11) shape but in the above your code, it sets a tensor data with the (1, 64, 1) shape.\r\n\r\n```input_buf = np.ones((1, 64, 1),dtype=np.float32)```\r\n\r\n-->\r\n\r\n```input_buf = np.ones((1, 64, 11),dtype=np.float32)```\r\n", "Thanks @abattery but the Python conversion isn't the problem - that completes successfully with either (1,64,1) or (1,64,11).\r\n\r\nThe problem is the C++ code, which segfaults on the call to `interpreter->AllocateTensors();`.\r\n\r\nI've tried reshaping the tensors before calling AllocateTensors in C++, but this doesn't make a difference.", "Could you verify whether the TF version number, that the above C++ program was built with, is 2.4.1 or tf-nightly version?", "If possible, since the model is converted successfully with the tf-nightly version, please upgrade the TFLite C++ library in android/iOS to the tf-nightly version.", "@abattery I've tried building the TFLite C++ library both from 2.4.1 and nightly (and from the official Docker container, and directly from the Github repository with my existing NDK). None of those work.", "I actually successfully ran your model with the TFLite benchmark tool, which is built with TFLite C++ API including the AllocateTensors method invocation. Hmm.. I couldn't reproduce your issue. Could you make sure that the given TFLite model, being used for C++ API, is not an out-dated one? Can you verify whether the issue is reproducible with https://www.tensorflow.org/lite/performance/measurement#benchmark_tools ?", "Thanks @abattery - I just tried built with the latest master (1e8f4666f2fbc1b) (NOT nightly branch, which wouldn't even compile)  and the model can now be properly loaded on both Android and iOS in both the benchmark tool and my C++ code.\r\n\r\nThanks for the help, closing this issue.\r\n\r\nAlso for future reference, are the nightly releases actually built from the nightly branch? \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48108\">No</a>\n", "In my understanding, they are built with the latest master branch.", "Thanks @abattery, I think that might have been my problem (trying to build from nightly branch)."]}, {"number": 48107, "title": "Fix typo in setup.py", "body": "", "comments": []}, {"number": 48106, "title": "Call tf.lite.Interpreter.interpreter.invoke() makes program crash", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip install \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow==2.3.2\r\n\r\n### 2. Code\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass Tester(tf.Module):\r\n    def __init__(self):\r\n        super(Tester, self).__init__()\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[100], dtype=tf.float32)])\r\n    def test(self, x):\r\n        # return tf.reshape(x, [10, -1])\r\n        return tf.signal.rfft(x, [512])\r\n\r\n\r\nmodel = Tester()\r\nconcrete_func = model.test.get_concrete_function()\r\nconverter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n# converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='saved_models/pb/model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.experimental_new_converter = True\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite = converter.convert()\r\n\r\nTFLITE_FILE_PATH = 'content/tester.tflite'\r\nwith open(TFLITE_FILE_PATH, 'wb') as f:\r\n    f.write(tflite)\r\n\r\n# Load the TFLite model and allocate tensors.\r\n# interpreter = tf.lite.Interpreter(model_path=\"content/test_variable.tflite\")\r\ninterpreter = tf.lite.Interpreter(model_content=tflite)\r\ninterpreter.allocate_tensors()\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n```\r\n\r\n### 3. Failure after conversion\r\nModel conversion successful, but invoke() makes program crash with error msg:\r\n```\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n```\r\n**In the above simple program, if test function return tf.signal.rfft(x, [512]), program crash, if test function return tf.reshape(x, [10, -1]), it works fine.**\r\nThe saved tflite model which return tf.signal.rfft(x, [512]) is sharing with url:\r\nhttps://drive.google.com/file/d/1LY3h4MHIwGGi63TyWsNta6vNKluIdi38/view?usp=sharing\r\n\r\nCan anyone help me?\r\n", "comments": ["@zhangguanqun \r\nCan you please refer to similar issues and let us know if it helps: #32492, #34574, [link](https://github.com/tensorflow/tensorflow/issues/24376#issuecomment-448026297)", "@Saduf2019 \r\nThanks reply. \r\nI tried above three issues you mentioned, and I'm sure I don't have these problems, all the programs works fine on my computer.", "**UPDATE**\r\nIn python crash log from ubutun system, there is:\r\n```\r\nTitle\r\n    python3.8 crashed with SIGSEGV in tflite::FlexDelegate::CopyFromBufferHandle()\r\n\r\nStacktraceTop\r\n    tflite::FlexDelegate::CopyFromBufferHandle(TfLiteContext*, int, TfLiteTensor*)() from /home/zhangguanqun/venv_tf_2.3.2/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n```\r\nCopyFromBufferHandle() makes crash.", "Could you try conversion and model inference run with the more recent TF version? For example, TF 2.4 or tf-nightly?", "@abattery Thanks, it works fine on TF 2.4.1 and TF-nightly (2.5.0-dev20210210), but why not works on TF 2.3.X?", "@zhangguanqun We have improved the TFLite conversion pipelines and kernel implementations. Looks like the related issue around the PAD op had been fixed between TF 2.3 and TF 2.4.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48106\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48106\">No</a>\n", "Closing this issue since the problem has gone at the newer TF versions."]}, {"number": 48103, "title": "EfficientNet official VS non-official implementation + Reproducibility ", "body": "I've encountered weird phenomena with the official implementation of the `EfficientNet` model. The **validation score** doesn't improve as expected and far less than a non-official one. \r\n\r\n# Reproduce Issue\r\n\r\n```\r\n# Install non-official efficient network \r\n!pip install -U git+https://github.com/qubvel/efficientnet\r\n```\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nimport efficientnet.keras as efn \r\n\r\nbatch_size = 32 \r\nnum_classes = 10\r\nepochs = 3\r\nofficial_efficient_net = False\r\n```\r\n\r\n```\r\n## Model \r\ninput_shape = (32,32,3)\r\nif official_efficient_net:\r\n    base_model  = tf.keras.applications.efficientnet.EfficientNetB0(include_top=False,\r\n                                                                    weights=\"imagenet\", \r\n                                                                    input_shape=input_shape)\r\nelse:\r\n    base_model = efn.EfficientNetB0(include_top=False,\r\n                                        weights=\"imagenet\", \r\n                                        input_shape=input_shape)\r\n\r\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\ndense_layer = tf.keras.layers.Dense(10, activation='softmax')\r\nModel  = tf.keras.Sequential([base_model, global_average_layer, dense_layer])\r\n```\r\n\r\n```\r\n## Training Data\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\nx_train = x_train.astype('float32') / 255\r\nx_test = x_test.astype('float32') / 255\r\n\r\n## Training\r\nModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\ncnn = Model.fit(x_train, y_train, batch_size=batch_size, \r\n                epochs=epochs, validation_data=(x_test,y_test))\r\n```\r\n\r\nTested in `TensorFlow 2.4.1`. \r\nTraining log from **Official EfficientNet**\r\n>\r\nEpoch 1/3\r\n1563/1563 [==============================] - 128s 77ms/step - loss: 1.6419 - accuracy: 0.4368 - val_loss: 2.4821 - val_accuracy: 0.1817\r\nEpoch 2/3\r\n1563/1563 [==============================] - 119s 76ms/step - loss: 0.9287 - accuracy: 0.6800 - val_loss: 3.6128 - val_accuracy: 0.1184\r\nEpoch 3/3\r\n1563/1563 [==============================] - 119s 76ms/step - loss: 0.7524 - accuracy: 0.7478 - val_loss: 3.9361 - val_accuracy: 0.1000\r\n\r\n\r\nTraining log from **Non-official EfficientNet**\r\n>\r\nEpoch 1/3\r\n1563/1563 [==============================] - 128s 77ms/step - loss: 1.3312 - accuracy: 0.5551 - val_loss: 0.7306 - val_accuracy: 0.7449\r\nEpoch 2/3\r\n1563/1563 [==============================] - 119s 76ms/step - loss: 0.7034 - accuracy: 0.7614 - val_loss: 0.6250 - val_accuracy: 0.7846\r\nEpoch 3/3\r\n1563/1563 [==============================] - 119s 76ms/step - loss: 0.5726 - accuracy: 0.8098 - val_loss: 0.6379 - val_accuracy: 0.7813\r\n\r\nI've also tested with other ImageNet models (ie. `RestNet`, `DenseNet`). They all give a reasonable validation score. \r\n\r\n\r\n# Reproducibility \r\n\r\nApart from the above issue, there is another issue about reproducibility from `EfficientNet`. It was asked before, here [Issue #47174](https://github.com/tensorflow/tensorflow/issues/47174). I've tested with `TF 2.4.1` and I also face the same problem. \r\n\r\n\r\n", "comments": ["Remove\r\n```\r\nx_train = x_train.astype('float32') / 255\r\nx_test = x_test.astype('float32') / 255\r\n```\r\nfix the bug.", "@innat \r\nWhen we try to replicate the code shared i face a different error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/eeba421a301ad30bfe87b5291059b5b6/untitled580.ipynb). If possible share a colab gist with the issue reported.", "I am able to run the code on tf 2.4 but throws error on nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/1dcbc895897a50056f5882493622cf2d/untitled582.ipynb).\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48103\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48103\">No</a>\n"]}, {"number": 48102, "title": "TF2.4 ModelCheckpoint Cant Save Model", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Redhat 7.4):\r\n- TensorFlow installed from conda install\r\n- TensorFlow version (use command below): tf 2.4.1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version:  11.2/8.0.4\r\n- GPU model and memory:   GTX3090   24G\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\ncheckpoint1 = ModelCheckpoint(best_model_filepath,monitor= 'loss',verbose=1,save_best_only= True,mode= 'min',period= 1)  \r\ncheckpoint2 = ModelCheckpoint(best_model_filepath,monitor= 'loss',verbose=1,save_best_only= True,\r\n                                 save_weights_only=True,mode= 'min',period=1)\r\ncallbacks_list  = [checkpoint1,learning_rate_scheduler]\r\nmodel.fit(train_generator.generator(),steps_per_epoch=1000,epochs=300,callbacks=callbacks_list)\r\n\r\n\r\nThe checkpoint1  will cause an error, otherwise checkpoint2 will run successly.\r\nTHE ERROR LOG  Part:\r\n\r\n        312/312 [==============================] - 139s 398ms/step - loss: 94.4598 - CounterAttack_LL_loss: 93.7615 - CounterAttack_cls_out_loss: 0.6983\r\n\r\nEpoch 00001: loss improved from inf to 93.23533, saving model to model/CounterAttack_best_model.h5\r\nTraceback (most recent call last):\r\n  File \"train_v2.py\", line 1213, in <module>\r\n    train(continue_train=True)\r\n  File \"train_v2.py\", line 1070, in train\r\n    callbacks=callbacks_list)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1145, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 428, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1344, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 1396, in _save_model\r\n    self.model.save(filepath, overwrite=True, options=self._options)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 2002, in save\r\n    signatures, options, save_traces)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py\", line 154, in save_model\r\n    model, filepath, overwrite, include_optimizer)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 115, in save_model_to_hdf5\r\n    model_metadata = saving_utils.model_metadata(model, include_optimizer)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py\", line 155, in model_metadata\r\n    model_config['config'] = model.get_config()\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py\", line 650, in get_config\r\n    return copy.deepcopy(get_network_config(self))\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 240, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n\r\n... ...  (many repeat lines)\r\n\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 150, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 215, in _deepcopy_list\r\n    append(deepcopy(a, memo))\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 180, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/copy.py\", line 274, in _reconstruct\r\n    y = func(*args)\r\n  File \"/root/python_env/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\", line 190, in __init__\r\n    if value < 0:\r\nRecursionError: maximum recursion depth exceeded in comparison\r\n2021-03-26 15:34:35.680528: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.\r\n\t [[{{node PyFunc}}]]\r\n\r\n\r\n\r\n", "comments": ["@rcx986635 \r\nPlease share simple stand alone code or a colab gist with the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48101, "title": "Add a  tf.keras.layers.WeightedAverage", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tensorflow 2.4.1\r\n- Are you willing to contribute it (Yes/No): I will give example code\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI notice there is a ```tf.keras.layers.Average``` , But it does't have a parameter like ```weights``` . \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAll users who use ```tf.keras```\r\n\r\n**Any Other info.**\r\nI find the source code at [https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/merge.py#L327-L360](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/layers/merge.py#L327-L360) . \r\nAnd I give an example code here : \r\n```\r\n@keras_export('keras.layers.WeightedAverage')\r\nclass WeightedAverage(_Merge):\r\n  \"\"\"Layer that averages a list of inputs element-wise with weights.\r\n  It takes as input a list of value tensors and a list of weight .  All of the value tensors are the same shape .  It returns\r\n  a single tensor (also of the same shape as the value tensors ).\r\n  Example:\r\n  >>> x1 = np.ones((2, 2))\r\n  >>> x2 = np.zeros((2, 2))\r\n  >>> y = tf.keras.layers.WeightedAverage()([x1, x2],[2,3])\r\n  >>> y.numpy().tolist()\r\n  [[0.4, 0.4], [0.6, 0.6]]\r\n  Usage in a functional model:\r\n  >>> input1 = tf.keras.layers.Input(shape=(16,))\r\n  >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\r\n  >>> input2 = tf.keras.layers.Input(shape=(32,))\r\n  >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\r\n  >>> avg = tf.keras.layers.Average()([x1, x2],[1 for _ in range(16)])\r\n  >>> out = tf.keras.layers.Dense(4)(avg)\r\n  >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)\r\n  Raises:\r\n    ValueError: If there is a shape mismatch between the inputs and the shapes\r\n      cannot be broadcasted to match.\r\n  \"\"\"\r\n\r\n  def _merge_function(self, inputs , weights):\r\n    output = inputs[0] * weights[0]\r\n    weight_sum = weights[0]\r\n    for i in range(1, len(inputs)):\r\n      output += inputs[i] * weights[i]\r\n      if weights[i] < 0:\r\n        raise\r\n      weight_sum += weights[i]\r\n    if weight_sum == 0:\r\n        raise\r\n    return output / weight_sum \r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["@DachuanZhao,\r\nThank you for the details and for the code. Can you please specify the use cases for this feature. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Thank you for the details and for the code. Can you please specify the use cases for this feature. Thanks!\r\n\r\nYou can use it when you want to calculate  a weighted average number \r\nI find that ```tf.keras.layers.Lambda``` can also make it . I will close this issue . "]}, {"number": 48100, "title": "How do you convert mobilenet v3 checkpoints to .h5 file?", "body": "Hi, as pointed in https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L390-L429 :\r\n```\r\nThe weights for all 6 models are obtained and translated from the Tensorflow\r\n  checkpoints from TensorFlow checkpoints found [here]\r\n  (https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet/README.md).\r\n```\r\n\r\nHow do you convert the checkpoints to .h5 file? Is there any documentation about this?", "comments": ["@Kewenjing1020 \r\n\r\nKindly open a [stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow) issue for this as it is not a bug or feature request, Please post this kind of support questions at Stackoverflow. There is a big community to support and learn from your questions.\r\n\r\nYou may refer to these to links for an isea and move this issue to closed status after you reate one on stackoverflow: [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/efficientnet_weight_update_util.py), [link1](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/image_classification_efficientnet_fine_tuning.ipynb)", "Thanks! The related links help!", "@Kewenjing1020 \r\nPlease move this issue to closed status as issue is resolved."]}, {"number": 48099, "title": "Add riscv platform support for tflite with cmake.", "body": "This pr try to add the riscv platform support with cmake.\r\nIt includes:\r\n1. prebuilt binary download script for riscv clang compiler and qemu.\r\n2. the riscv cmake toolchain file which contains the toolchain and compiling options related setting for riscv platform\r\n3. the howto document for all build steps.", "comments": ["The qemu running result for benchmark tool:\r\n```\r\n/tensorflow/tensorflow/lite/tools/cmake/build$ /tensorflow/tensorflow/lite/tools/cmake/riscv/Prebuilt/qemu/linux/RISCV/bin/qemu-riscv64 \\\r\n  -cpu rv64,x-v=true,x-k=true,vlen=512,elen=64,vext_spec=v1.0 \\\r\n  -L /tensorflow/tensorflow/lite/tools/cmake/riscv/Prebuilt/toolchain/clang/linux/RISCV/sysroot \\\r\n  ./tools/benchmark/benchmark_model --help\r\n\r\nSTARTING!\r\nusage: ./tools/benchmark/benchmark_model <flags>\r\nFlags:\r\n        --input_layer=                          string  optional        input layer names\r\n        --min_nodes_per_partition=0             int32   optional        The minimal number of TFLite graph nodes of a partition that has to be reached for it to be de\r\n        ...\r\n```", "@terryheo\r\nCould you please review this pr? The riscv platform supporting with cmake.", "Thanks for your contribution!\r\nI have few comments on this.\r\n1. RISCV is not our primary target devices since TFLite don't have accelerated kernels\r\n2. I wanted to remove logic to maintain toolchain from this change. As you can see from https://www.tensorflow.org/lite/guide/build_cmake_arm, CMake builds works well by providing few necessary variables. I think RISCV build should be the similar. such as,\r\n```\r\nRISCVCC_PREFIX=$HOME/toolchains/riscv/bin/riscv64-unknown-linux-gnu-\r\ncmake -DCMAKE_C_COMPILER=${RISCVCC_PREFIX}gcc -DCMAKE_CXX_COMPILER=${RISCVCC_PREFIX}g++ -DTFLITE_ENABLE_XNNPACK=OFF ../tensorflow/lite/\r\n../tensorflow/lite/\r\n```\r\n\r\nIn summary, I'm OK to have a small section for RISCV under https://www.tensorflow.org/lite/guide/build_cmake_arm. But I'm reluctant to have a separate page for RISCV since we don't have optimized kernels.", "@JerryShih Can you please check @terryheo's comments and keep us posted ? Thanks!", "@JerryShih  Any update on this PR? Please. Thanks!", "@JerryShih Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 48098, "title": "Issue related to goldeneye DOS attack", "body": "I have been seeing this below error when I run the final command, can anyone help me fix it.\r\n\r\nException AttributeError: \"'NoneType' object has no attribute 'terminate'\" in <bound method Striker.__del__ of <Striker(Striker-2, initial)>> ignored\r\n\r\nShutting down GoldenEye\r\n![image](https://user-images.githubusercontent.com/64961683/112573985-7c0a4b80-8e41-11eb-9432-477ae4a81714.png)\r\n@jan Seidl @jseidl @giancluciano @samueloph", "comments": ["@Yakshith-Ramesh \r\nCan you please paste the error logs for us to look into the issue.", "> @Yakshith-Ramesh \n> Can you please paste the error logs for us to look into the issue.\n\nHi, thanks for responding. The error is shown at the bottom of screen as shown in the picture. There's no logs associated with the errors.", "@Yakshith-Ramesh \r\nPlease paste the error in text format, it would be difficult for users with similar error to search and us to look it up.", "> @Yakshith-Ramesh\r\n> Please paste the error in text format, it would be difficult for users with similar error to search and us to look it up.\r\n\r\nI am sorry about that. The error is as below \r\nException AttributeError: \"'NoneType' object has no attribute 'terminate'\" in <bound method Striker.__del__ of <Striker(Striker-2, initial)>> ignored\r\n\r\nShutting down GoldenEye\r\n\r\n[GoldenEYE error.txt](https://github.com/tensorflow/tensorflow/files/6209586/GoldenEYE.error.txt)\r\n", "@Yakshith-Ramesh \r\nThis is not a tensorflow related issue.\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]"]}, {"number": 48097, "title": "Updated xnnpack to commit f0cb70a1ae68c4b99047b7c5d26c40fa3a65f0e4 for cmake build.", "body": "Update for fp16 downloading path.", "comments": ["I don't know if we need to update other dependent library too.\r\nThe bazel build updating also changes the pthreadpool and cpu-info.\r\nhttps://github.com/tensorflow/tensorflow/commit/8060cb22de7115f4cc3e140a057ee7412ea3bbca", "@multiverse-tf could you review this PR?", "@multiverse-tf\r\nDo you mean use the latest XNNPACK commit or just rebase this pr?\r\nIf just rebase this pr, only the FP16 needs to be upgraded in this pr.", "> @multiverse-tf\r\n> Do you mean use the latest XNNPACK commit or just rebase this pr?\r\n> If just rebase this pr, only the FP16 needs to be upgraded in this pr.\r\n\r\nJust rebase. Thx for the quick response!", "@multiverse-tf \r\nDo you close this pr accidentally or this pr is useless anymore?", "> @multiverse-tf\r\n> Do you close this pr accidentally or this pr is useless anymore?\r\n\r\nSorry, I accidentally closed the PR :-(. Let me reopen it."]}, {"number": 48096, "title": "GPU device not found (Google colab)", "body": "I still got the same error after\r\n\r\n```\r\npip install tf-nightly-gpu\r\n\r\n%tensorflow_version 2.x  \r\nimport tensorflow as tf  \r\ndevice_name = tf.test.gpu_device_name()  \r\nif device_name != '/device:GPU:0':\r\n   raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n```\r\nin output:\r\n_**SystemError Traceback (most recent call last)\r\n\r\nin ()\r\n3 device_name = tf.test.gpu_device_name()\r\n4 if device_name != '/device:GPU:0':\r\n----> 5 raise SystemError('GPU device not found')\r\n6 print('Found GPU at: {}'.format(device_name))\r\n\r\nSystemError: GPU device not found**_\r\n\r\nbut comand\r\n`!nvidia-smi`\r\nget me:\r\n**_NVIDIA-SMI 460.56 Driver Version: 460.32.03 CUDA Version: 11.2_**", "comments": ["This is because colab hosts cuda 11.0 currently. Generally colab uses latest stable TF compatible cuda version (currently TF 2.4 and cuda 11.0)\r\nWe can expect colab using cuda 11.2 once TF 2.5 is released.\r\n```python\r\n!nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\r\nCuda compilation tools, release 11.0, V11.0.221\r\nBuild cuda_11.0_bu.TC445_37.28845127_0\r\n```", "@ymodak thank you so much. You are damn right, i have got the same output. But, but how does this information solve my problem?\r\nI have already seen this cause in other threads, but have not found working solutions to downgrade to CUDA = 10.0.0 for Colab. \r\n\r\nAnd i cant use downgrade tf to 2.4 \r\n`!pip install tensorflow-gpu==2.4`\r\nbecause in this case magic row `%tensorflow_version 2.x`  dont work", "@alex-sokolov2011,\r\nI was able to run the given code snippet without any issues using TF v2.4.1, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/76e1162458ebc4e287af31ab765cbdc1/48096.ipynb). Thanks!", "@alex-sokolov2011  Google colab hosts latest stable TF version by default (TF 2.4.1 with gpu support). Thus you don't need to install tf explicitly neither use tf 2.x magic function. You can simply try ;\r\n```python\r\nimport tensorflow as tf\r\ndevice_name = tf.test.gpu_device_name()  \r\nif device_name != '/device:GPU:0':\r\n   raise SystemError('GPU device not found')\r\nprint('Found GPU at: {}'.format(device_name))\r\n```\r\n", "@amahendrakar and @ymodak thank you so much\r\n\r\n That was real mystery, but now GPU ON. [here my test ](https://colab.research.google.com/drive/1uUdcdcuCUCpVE9Pkz16Hr9KgAXGIwMzA?usp=sharing).\r\n Sorry that I bothered you with that discussion.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48096\">No</a>\n"]}, {"number": 48094, "title": "Eagerly calling a Keras Conv2D crashes with `floating point exception`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0.dev20210215\r\n- Python version: 3.8.2\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: AMD Radeon Pro 5500M 8 GB\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nIn a virtualenv with TensorFlow installed, run\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.keras.layers.Conv2D(100, 3)(tf.constant([[[[]]]]))\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nEntire process dies, printing this error to the terminal: `floating point exception  python`\r\n\r\n**Describe the expected behavior**\r\n\r\nProcess does not die.  I would probably expect a `ValueError` or similar exception.", "comments": ["I ran the code shared but colab crashes, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/73e4368b893d9089f7eb41408218ab14/untitled575.ipynb)", "@psybuzz \r\nThe code given as the example at [keras website](https://keras.io/api/layers/convolution_layers/convolution2d/) works as expected. Gist [here](https://colab.research.google.com/gist/AdityaKane2001/b3fc660078d46974db4797052b6f3256/untitled575.ipynb). Also,\r\n`a = tf.keras.layers.Conv2D(10, (3,3),activation = 'relu')(tf.constant(np.random.rand(10,10,10,3)))`\r\nthis works as expected. \r\n> tf.keras.layers.Conv2D(100, 3)(tf.constant([[[[]]]]))\r\n\r\nAs you mentioned, this crashes the session. ", "This is fixed with TF 2.5. It raises an error and avoids crashing\r\n```python\r\nInvalidArgumentError: filter depth must be stricly positive, got 0 [Op:Conv2D]\r\n```", "Thanks, I checked with TF 2.5.0 and can confirm it throws an error!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48094\">No</a>\n"]}, {"number": 48093, "title": "Pin estimator", "body": "", "comments": []}, {"number": 48092, "title": "Mihaimaruseac pin estimator r2.5", "body": "", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48092) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 48091, "title": "Update version numbers for TensorFlow 2.5.0-rc0", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 5 -> 5\nPatch: 0 -> 0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.5.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:53:2.5.0\ntensorflow/tools/pip_package/setup.py:107:2.5.0\ntensorflow/tools/pip_package/setup.py:119:2.5.0\ntensorflow/tools/pip_package/setup.py:121:2.5.0\ntensorflow/tools/pip_package/setup.py:123:2.5.0\ntensorflow/lite/g3doc/guide/op_select_allowlist.md:782:2.5.0\ntensorflow/lite/micro/tools/ci_build/tflm_bazel/tensorflow.bzl:11:2.5.0\nBinary file \ntensorflow/lite/python/testdata/control_flow_v1_saved_model/saved_model.pb \nmatches\nBinary file \ntensorflow/python/compiler/tensorrt/model_tests/sample_model/saved_model.pb \nmatches\ntensorflow/python/keras/__init__.py:33:2.5.0\ntensorflow/tensorflow.bzl:51:2.5.0\n\nWARNING: Below are potentially instances of lingering old version string \n\"2.5.0\" in source directory \"tensorflow/\" that are not updated by this script. \nPlease check them manually!\ntensorflow/tools/pip_package/setup.py:53:2.5.0\ntensorflow/tools/pip_package/setup.py:107:2.5.0\ntensorflow/tools/pip_package/setup.py:119:2.5.0\ntensorflow/tools/pip_package/setup.py:121:2.5.0\ntensorflow/tools/pip_package/setup.py:123:2.5.0\ntensorflow/lite/g3doc/guide/op_select_allowlist.md:782:2.5.0\ntensorflow/lite/micro/tools/ci_build/tflm_bazel/tensorflow.bzl:11:2.5.0\nBinary file \ntensorflow/lite/python/testdata/control_flow_v1_saved_model/saved_model.pb \nmatches\nBinary file \ntensorflow/python/compiler/tensorrt/model_tests/sample_model/saved_model.pb \nmatches\ntensorflow/python/keras/__init__.py:33:2.5.0\ntensorflow/tensorflow.bzl:51:2.5.0\n```", "comments": []}, {"number": 48090, "title": "Update release notes for TensorFlow 2.5.0", "body": "This PR is intentionally incomplete. One of the Release Owners for 2.5.0\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": ["I don't seem to be authorized to edit this PR. Please will someone else add the changes from [closed PR 48581](https://github.com/tensorflow/tensorflow/pull/48581/files), also incorporating @sanjoy's spelling correction?", "Sure I will take care of it\n\nOn Tue, Apr 20, 2021 at 2:30 PM Duncan Riach ***@***.***>\nwrote:\n\n> I don't seem to be authorized to edit this PR. Please will someone add the\n> changes from closed PR 48581\n> <https://github.com/tensorflow/tensorflow/pull/48581/files>,\n> incorporating @sanjoy <https://github.com/sanjoy>'s spelling correction?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/48090#issuecomment-823612207>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AMWX6TKVIQJ4SDQX3M7DHTLTJXW65ANCNFSM4Z2NTT5A>\n> .\n>\n"]}, {"number": 48089, "title": "windows/subprocess.cc should log the command line for a failing invocations", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorflow fails on my system with an infinite loop of errors like: \r\n```\r\n2021-03-25 17:18:35.568069: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-03-25 17:18:37.778865: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.778941: W tensorflow/stream_executor/gpu/asm_compiler.cc:55] Couldn't invoke ptxas.exe --version\r\n2021-03-25 17:18:37.781460: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.782300: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2021-03-25 17:18:37.785605: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.795062: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.798410: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.807112: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.810587: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.819411: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.822604: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.834319: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.837815: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.849892: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.853291: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.880399: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n2021-03-25 17:18:37.884144: E tensorflow/core/platform/windows/subprocess.cc:283] Call to CreateProcess failed. Error code: 2\r\n```\r\n\r\nThis is probably because my system (CUDA, cuDNN, python, whatever...) versions are messed up, but without knowing what command was invoked it's hard to figure out a fruitful next step. \r\n\r\n=> It would be most helpful to have `CreateProcess` log the actual command line it was trying to invoke. \r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nPeople running into weird errors. \r\n\r\n**Any Other info.**\r\nLooking at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/windows/subprocess.cc#L287 it seems like changing\r\n\r\n```c++\r\n    LOG(ERROR) << \"Call to CreateProcess failed. Error code: \"\r\n               << GetLastError();\r\n```\r\n\r\nto something along the lines of: \r\n\r\n```c++\r\n    LOG(ERROR) << \"Call to CreateProcess failed. Error code: \"\r\n               << GetLastError() << \", command: '\" << command_line << \"'\";\r\n```\r\n\r\nwould do the trick. \r\n\r\n(Sorry, I don't have a c++ env, and there may well be a bug or two in that code, but a dev should be able to get the jist)", "comments": ["Created [a CL](https://critique-ng.corp.google.com/cl/365201271) with the suggested changes.", "Hi @jxtps ! PR #55254 which addressed this issue has been merged now. Shall we move this issue to closed status now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48088, "title": "Update release notes for TensorFlow 2.6.0", "body": null, "comments": []}, {"number": 48087, "title": "Initialize allowed_devices when creating a local client in BuildXlaCo\u2026", "body": "\u2026mpilationCache.\r\n\r\nw/o any allowed devices, the loop in GetStreamExecutors (platfrom_util.cc) sees\r\nall devices, and initializes all of them.\r\nIn a multigpu scenario, this causes OOMs, as CUDA contexts are allocated on\r\nall devices. This change results in GetStreamExecutors only initializing the\r\ncurrent device\r\n\r\nThis PR addresses https://github.com/horovod/horovod/issues/2548\r\n", "comments": []}, {"number": 48085, "title": "How to enable Multiple Object Tracking in tflite?", "body": "Hello everyone, I'm new here. I would like to know how can I enable realtime object tracking in tflite example for android(object detection). I need to count objects in real time with a moving camera. I've read older versions tflite allowed tracking on android, but now I'm confused about that. Can you help me? please\r\n", "comments": ["Please refer to the following document. This document explains how to enable object detection through TFLite.\r\nhttps://www.tensorflow.org/lite/examples/object_detection/overview", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48083, "title": "fix broken bazel macros, to fix TF serving build.", "body": "origin of this breakage is #47745 ", "comments": []}, {"number": 48082, "title": "Convert sed lines to a single git patch.", "body": "Also remove one more dimension check to match with reference kernels.\r\n\r\nTested with several benchmarks which exercise these conditions for fusion_f1.\r\n\r\nAs discussed in http://b/183497550#comment4", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48080, "title": "TFLite GPU, Treat Tensors as Linear if possible", "body": "Consider Tensors with only one dimension size != 1 to be also Linear Tensors.\r\n\r\nFor example, Tensor with shape (1,64,1,1) is now not considered as Linear, even though it can be safely treated as Linear. This commit changes the behavior so that all Tensors, where at most one dimension size is != 1 are treated as Linear.\r\n\r\nThis commit targets TFLite, GPU delegate.", "comments": ["I believe this actually solves issues described in #37102.", "IMO, treating all tensors as a linear tensor will break some operations that rely on the shapes. Could you add a reshape op in front of the mul op as @impjdi suggested in the original post?", "It will break nothing, just check where the code is used. Check function _CheckIfLinearConvertible_ usage.", "I am not sure whether this conversion is safe or not.\r\n\r\n@impjdi could you review this PR?", "It is especially annoying because with this treatment some tflite networks work with CPU  delegate but not with GPU delegate. The problem in my case occurs with networks with some legacy ops, e.g. BatchNorm where MUL operation node is used.", "No, this isn't really compatible, because [1, 4, 1, 1] and [1, 1, 1, 4] works completely different on the GPU due to the DHWC4 business.  I suggest to insert a proper `RESHAPE` to achieve the right dims rather than assuming that the CPU's memory layout is the \"correct\" way which isn't true.", "Thanks for consideration, the real issue is probably losing the notion of 'channels_first' vs 'channels_last' by premature optimization with Linear Tensors. The Linear Tensor should never have been used which would assure proper broadcasting if the actual shape has been kept and not discarded when converting to Linear Tensor. \r\n\r\nClosing and will progress on my own."]}, {"number": 48079, "title": "Consider Tensors with data in only single dimension to be Linear Tensor", "body": "Currently only Tensors where the last dimension is not of size 1 are recognized as Linear Tensors. However, in many applications Tensors can have shape like (1,64,1,1). These are currently not recognized as Linear. My commit aims to recognize these as being Linear.\r\n\r\nThe change affect TF Lite GPU delegate.", "comments": []}, {"number": 48078, "title": "Unable to get the prediction.", "body": "I am using flutter tflite to run my custom model.\r\nThe project githubrepo - https://github.com/vijayshankarrealdeal/The-Network\r\nand my model train code is -https://www.kaggle.com/vijayshankar756/notebookd234d927a6\r\n\r\nLaunching lib\\main.dart on sdk gphone x86 arm in debug mode...\r\nInvalid depfile: D:\\GitHub\\The-Network\\.dart_tool\\flutter_build\\0574b688c553aea69c3fd41ddb320c5c\\kernel_snapshot.d\r\nInvalid depfile: D:\\GitHub\\The-Network\\.dart_tool\\flutter_build\\0574b688c553aea69c3fd41ddb320c5c\\kernel_snapshot.d\r\n\u221a Built build\\app\\outputs\\flutter-apk\\app-debug.apk.\r\nConnecting to VM Service at ws://127.0.0.1:52078/AsfCjkEa20Q=/ws\r\nI/tflite  (15303): Initialized TensorFlow Lite runtime.\r\nW/System  (15303): A resource failed to call close.\r\nE/AndroidRuntime(15303): FATAL EXCEPTION: AsyncTask #1\r\nE/AndroidRuntime(15303): Process: com.example.hex, PID: 15303\r\nE/AndroidRuntime(15303): java.lang.RuntimeException: An error occurred while executing doInBackground()\r\nE/AndroidRuntime(15303): \tat android.os.AsyncTask$4.done(AsyncTask.java:415)\r\nE/AndroidRuntime(15303): \tat java.util.concurrent.FutureTask.finishCompletion(FutureTask.java:383)\r\nE/AndroidRuntime(15303): \tat java.util.concurrent.FutureTask.setException(FutureTask.java:252)\r\nE/AndroidRuntime(15303): \tat java.util.concurrent.FutureTask.run(FutureTask.java:271)\r\nE/AndroidRuntime(15303): \tat android.os.AsyncTask$SerialExecutor$1.run(AsyncTask.java:305)\r\nE/AndroidRuntime(15303): \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)\r\nE/AndroidRuntime(15303): \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)\r\nE/AndroidRuntime(15303): \tat java.lang.Thread.run(Thread.java:923)\r\nE/AndroidRuntime(15303): Caused by: java.lang.IllegalArgumentException: Cannot copy from a TensorFlowLite tensor (Identity) with shape [1, 1] to a Java object with shape [1, 2].\r\nE/AndroidRuntime(15303): \tat org.tensorflow.lite.Tensor.throwIfDstShapeIsIncompatible(Tensor.java:482)\r\nE/AndroidRuntime(15303): \tat org.tensorflow.lite.Tensor.copyTo(Tensor.java:252)\r\nE/AndroidRuntime(15303): \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:175)\r\nE/AndroidRuntime(15303): \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:360)\r\nE/AndroidRuntime(15303): \tat org.tensorflow.lite.Interpreter.run(Interpreter.java:319)\r\nE/AndroidRuntime(15303): \tat sq.flutter.tflite.TflitePlugin$RunModelOnImage.runTflite(TflitePlugin.java:481)\r\nE/AndroidRuntime(15303): \tat sq.flutter.tflite.TflitePlugin$TfliteTask.doInBackground(TflitePlugin.java:448)\r\nE/AndroidRuntime(15303): \tat sq.flutter.tflite.TflitePlugin$TfliteTask.doInBackground(TflitePlugin.java:422)\r\nE/AndroidRuntime(15303): \tat android.os.AsyncTask$3.call(AsyncTask.java:394)\r\nE/AndroidRuntime(15303): \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nE/AndroidRuntime(15303): \t... 4 more\r\nI/Process (15303): Sending signal. PID: 15303 SIG: 9\r\nLost connection to device.\r\nExited (sigterm)\r\n", "comments": ["Hi Vijay, \r\n\r\nPlease provide more concise code snippets so that we can better help you. \r\n\r\nBut looking at the failure message \r\n\r\n\r\nE/AndroidRuntime(15303): Caused by: java.lang.IllegalArgumentException: Cannot copy from a TensorFlowLite tensor (Identity) with shape [1, 1] to a Java object with shape [1, 2].\r\n\r\nYou might be training the binary classification model with a single output class but expects the model to have 2 output classes. It might worth checking if the Python code and Dart code is aligned.\r\n\r\nThanks,\r\nTiezhen\r\n", "actually i have given the github repo link of the project and the python note book from where model is made.\r\n\r\nThe project githubrepo - https://github.com/vijayshankarrealdeal/The-Network\r\nand my model train code is -https://www.kaggle.com/vijayshankar756/notebookd234d927a6", "The code snippet provided is too long to investigate the problem. Please try posting this question on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48078\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48078\">No</a>\n"]}, {"number": 48077, "title": "Updated copyright year to 2021", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48077) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!\r\n\r\n> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48077) for more info**.\r\n\r\n@googlebot I signed it!", "Thank you for the comment."]}, {"number": 48076, "title": "Tensorflow / TensorFlow Lite issue when compiling", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS, Big Sur, version 11.2.2\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: pip under venv\r\n- GPU model and memory: AMD Radeon Pro\u00a05300M 4 Go ; Intel\u00a0UHD Graphics\u00a0630 1536 Mo\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to run a code provided by a user on Github, that deals with .tflite models. I asked to other people to try it (same os, same version of python / tensorflow) and have been able to compile it and to get the output, while I endlessly get an error. I also tested the code on another Mac (Catalina + python 3.8) and that works. I did not find any source of relevant information on the internet to solve the issue. I'm pretty sure it has something to do with tensorflow.\r\nHere's the error : \r\n  File \"/Users/username/Desktop/Face_mask_detector/my3.8/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py\", line 423, in set_tensor\r\n    self._interpreter.SetTensor(tensor_index, value)\r\nValueError: Cannot set tensor: Got value of type STRING but expected type FLOAT32 for input 0, name: serving_default_input:0 \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```git clone https://github.com/tanhouren/Face_mask_detector```\r\n```cd Face_mask_detector``` \r\n```python3.8  --version```\r\n```python3.8 -m venv my3.8```\r\n```source my3.8/bin/activate```\r\n``` pip install tensorflow opencv-python``` \r\n```python --version```\r\n```python main.py```\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["@Sunkian I exactly followed all the commands you listed and I didn't face that error. The code was running well initially and during inference I faced `Segmentation fault` and the code crashed.\r\n\r\n```\r\npython main.py \r\nImporting library. This might take a while...\r\nMake sure your surrounding is bright\r\nINFO: Start inference\r\nSegmentation fault: 11\r\n```\r\n\r\nI don't think this is an error from TF. As i am not sure about their model, i think it is better if you raise an issue with that repository. May be the model was expecting huge memory or something wrong in the inference part of the code. I saw the inference was taking longer (like 10-15 sec) before crashing.\r\n\r\nI am closing this issue as this was not related to any TF example from TensorFlow website. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48076\">No</a>\n"]}]