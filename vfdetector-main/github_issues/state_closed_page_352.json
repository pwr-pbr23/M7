[{"number": 43527, "title": "\"IndexError: list index out of range\" when load EfficientDet SavedModel with tf.keras", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n I try to load SavedModel efficientDetD0 with load_model of keras:\r\n\r\n`model = tf.keras.models.load_model(\"saved_model\")'\r\n'model.summary()`\r\n\r\n\r\n\r\nAnd following error occur: \r\n\r\nTraceback (most recent call last):\r\n  File \"E:/Detection/test_infer_model.py\", line 129, in <module>\r\n    model = tf.keras.models.load_model(\"saved_model\")\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 121, in load\r\n    path, options=options, loader_cls=KerasObjectLoader)\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 633, in load_internal\r\n    ckpt_options)\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 221, in _load_all\r\n    self._finalize_objects()\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 526, in _finalize_objects\r\n    _finalize_saved_model_layers(layers_revived_from_saved_model)\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 706, in _finalize_saved_model_layers\r\n    inputs = infer_inputs_from_restored_call_function(call_fn)\r\n  File \"C:\\Users\\Luxury\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\load.py\", line 982, in infer_inputs_from_restored_call_function\r\n    spec = fn.concrete_functions[0].structured_input_signature[0][0]\r\nIndexError: list index out of range\r\n**Standalone code to reproduce the issue **\r\nlink colab: https://colab.research.google.com/drive/1fR8nu3INEWVUJJhex3Dline10296vxri?usp=sharing\r\nand model: https://drive.google.com/file/d/18D8NQp9zP4UW06jeG_75YQWyB531CWUj/view?usp=sharing\r\nI lost 2 days for searching on internet, stackoverflow even this repo's issue but no result, please help !\r\n", "comments": ["The ticket template requires:\r\n> Standalone code to reproduce the issue\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.", "> The ticket template requires:\r\n> \r\n> > Standalone code to reproduce the issue\r\n> > Provide a reproducible test case that is the bare minimum necessary to generate\r\n> > the problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nSorry, I updated", "@NguyenHongSon1103,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/b31e51328902aea167d3287d67b62a40/43527.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/3b0f693fb11f924a27c019d3f4d86106/43527-tf-nightly.ipynb#scrollTo=gpcvSq0y8iV9). Please find the attached gist.\r\n\r\nIn order to expedite the trouble-shooting process, could you please share the code to generate the `saved_model` files as well? Thanks!", "@amahendrakar \r\nI followed here https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html and used file \"exporter_main_v2.py\" from tensorflow repository ", "I am also getting the same error with models:\r\n* SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal loss (a.k.a Retinanet).\r\n* SSD with EfficientNet-b4 + BiFPN feature extractor, shared box predictor and focal loss (a.k.a EfficientDet-d4)\r\n\r\nThe config files are the same the ones in: models/research/object_detection/configs/tf2/\r\n\r\nI first train with: models/research/object_detection/model_main_tf2.py\r\n\r\nAnd then I export with: models/research/object_detection/exporter_main_v2.py\r\n\r\nBut then when I try to load the exported model I get the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/harrythomas/projects/xrfiber_effdet/pb_to_coreml.py\", line 82, in <module>\r\n    main()\r\n  File \"/Users/harrythomas/projects/xrfiber_effdet/pb_to_coreml.py\", line 22, in main\r\n    model = tf.keras.models.load_model(PATH_TO_MODEL, compile=False)\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\", line 187, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 120, in load\r\n    model = tf_load.load_internal(\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 632, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 194, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 130, in __init__\r\n    self._load_all()\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 221, in _load_all\r\n    self._finalize_objects()\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 526, in _finalize_objects\r\n    _finalize_saved_model_layers(layers_revived_from_saved_model)\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 706, in _finalize_saved_model_layers\r\n    inputs = infer_inputs_from_restored_call_function(call_fn)\r\n  File \"/Users/harrythomas/opt/anaconda3/envs/effdetvenv/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/load.py\", line 982, in infer_inputs_from_restored_call_function\r\n    spec = fn.concrete_functions[0].structured_input_signature[0][0]\r\nIndexError: list index out of range\r\n```\r\n\r\n\r\nSystem Information:\r\n* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n* Mobile device name if the issue happens on a mobile device:\r\n* TensorFlow installed from (source or binary): pip\r\n* TensorFlow version (use command below): 2.3.1\r\n* Python version: 3.8.3\r\n* Bazel version (if compiling from source):\r\n* GCC/Compiler version (if compiling from source):\r\n* CUDA/cuDNN version: CUDA 10.1.243, cuDNN 7.6.4 \r\n* GPU model and memory: 2x 1080 Ti 10Gb\r\n\r\nCode:\r\n```\r\nImport tensorflow as tf\r\nimport object_detection.models.keras_models.resnet_v1\r\nimport object_detection.models.feature_map_generators\r\nPATH_TO_MODEL = \u2018./saved_model/\u2018\r\nmodel = tf.keras.models.load_model(PATH_TO_MODEL)\r\n```\r\n\r\nI can load and make predictions with\r\n```\r\nmodel = tf.saved_model.load(PATH_TO_MODEL)\r\ntf_out = model(inputs)\r\n```\r\nHowever I wish to load the saved model so that I can edit it and then export it with coreML.", "Check also https://github.com/tensorflow/models/issues/8990#issuecomment-665672274", "I have the exact same problem @hlloydt\r\n\r\nI can load my saved model with tf and predict with it but i cannot load it with keras in order to save it in .h5 ", "> I have the exact same problem @hlloydt\r\n> \r\n> I can load my saved model with tf and predict with it but i cannot load it with keras in order to save it in .h5\r\n\r\nSame here", "Also fails with another model:\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\npath = os.path.join('.', 'models', 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8', 'saved_model')\r\nbase_model = tf.keras.models.load_model(path)\r\n```\r\n\r\nTF 2.3.1", "I have same problems.   Is there some other way to convert a saved_graph.pb to a frozen inference graph in TF 2.4?", "so what is the workaround to using Keras?   go through the colab tutorials:  \r\ntensorflow/models/research/object_detection/colab_tutorials/\r\n\r\nSpecifically, go through: inference_from_saved_model_tf2_colab.ipynb\r\nWith minor edits, this will run locally - you don't have to run on colab.    This works well and it will show you the pattern for utilizing your model without the Keras problems.\r\n", "Does reverting to an older version of tensorflow fix this issue as a workaround?\r\n", "There's a workaround works for me that no need to change your tf version: \r\n\r\n**tf version: 2.4.0, but I think others would be ok.**\r\n\r\n**My model settings:**\r\n**Model_type: keras subclassed model**\r\n**Save_way: used model.save() to save**\r\n**Load_way: used tf.keras.models.load_model() to load.**\r\n\r\n**MY Error raised in:** tensorflow\\python\\keras\\saving\\saved_model\\load.py line_1067 (or around)\r\n\r\n**I changed the original code:**\r\n\r\n```\r\n  spec = fn.concrete_functions[0].structured_input_signature[0][0]\r\n  for concrete in fn.concrete_functions[1:]:\r\n    spec2 = concrete.structured_input_signature[0][0]\r\n    spec = nest.map_structure(common_spec, spec, spec2)\r\n  return spec\r\n```\r\n\r\nto:\r\n\r\n```\r\n  spec = None\r\n  if len(fn.concrete_functions) > 0:\r\n    spec = fn.concrete_functions[0].structured_input_signature[0][0]\r\n  for concrete in fn.concrete_functions[1:]:\r\n    spec2 = concrete.structured_input_signature[0][0]\r\n    spec = nest.map_structure(common_spec, spec, spec2)\r\n  return spec\r\n```\r\n\r\nWhat I met was a error induced by the **fn.concrete_functions** which lets **the code run twice**, I haven't dig into python and tensorflow so I don't know how could it be like this, but **the first time the above part has a length of 4 (in my case), while at the second time it has a length of 0. So here maybe where the question is.**\r\n\r\nI used\r\n```\r\nprint(len(fn.concrete_functions))\r\nprint(type(fn.concrete_functions))\r\n```\r\nfor test, it turns out to be:\r\n\r\n4, list, 0, list\r\n\r\nroughly, but just like this. (If anyone know what's this is, I'd appreciate it if you tell me sth about it **: )**\r\n\r\nWish I could help you guys to solve your problem.\r\n", "The workaround doesn't work anymore in tf 2.4.1 as the error is not in the same location / block of code", "So what to do? I have trained the model and now I have a .pb file. It needs to be converted to .tflite", "same problem here with ssd_mobilenetV2 and TF2.4\r\nI tried to convert with \r\n`converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model('./exported-models/my_model_f32/saved_model')`\r\nand \r\n`converter = tf.lite.TFLiteConverter.from_saved_model('./exported-models/my_model_f32/saved_model')`\r\nwhich both failed.", "Exact same error here with ssd_mobilenet_v2_fpnlite_320x320 trained with tensorflow object detection api. Tensorflow= 2.4.1.\r\nMight not solve everyone's problem but you can load it with tf:\r\n`model = tf.saved_model.load(path)`", "I confirmed that the gist `model = tf.keras.models.load_model('/content/saved_model')` which @amahendrakar provided works with tf.nightly in the notebook here: https://colab.research.google.com/gist/monicadsong/7d0d5cbb36f078db9841d6360e9a7a91/43527-tf-nightly.ipynb\r\n\r\nI am closing this issue now, as I believe it is now fixed!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43527\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43527\">No</a>\n", "The issue still persists in tf v2.4.1. Also checked with 2.5.0-dev20210319 and here is a new error:\r\n\r\n`WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\r\n/home/mag/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:1060: UserWarning: object_detection.models.keras_models.resnet_v1 is not loaded, but a Lambda layer uses it. It may cause errors.\r\n  , UserWarning)\r\n/home/mag/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:1060: UserWarning: object_detection.models.feature_map_generators is not loaded, but a Lambda layer uses it. It may cause errors.\r\n  , UserWarning)\r\n--------------------------------------------------------------------------\r\nNameError                                Traceback (most recent call last)\r\n<ipython-input-4-344779352e07> in <module>\r\n      1 mysaved_model = output_directory_export+\"/saved_model/\"\r\n----> 2 model = tf.keras.models.load_model(mysaved_model)\r\n      3 model.summary()\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)\r\n    210         filepath = path_to_string(filepath)\r\n    211         if isinstance(filepath, six.string_types):\r\n--> 212           return saved_model_load.load(filepath, compile, options)\r\n    213 \r\n    214   raise IOError(\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)\r\n    156 \r\n    157   # Finalize the loaded layers and remove the extra tracked dependencies.\r\n--> 158   keras_loader.finalize_objects()\r\n    159   keras_loader.del_tracking()\r\n    160 \r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in finalize_objects(self)\r\n    627 \r\n    628     # Initialize graph networks, now that layer dependencies have been resolved.\r\n--> 629     self._reconstruct_all_models()\r\n    630 \r\n    631   def _unblock_model_reconstruction(self, layer_id, layer):\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_all_models(self)\r\n    646       all_initialized_models.add(model_id)\r\n    647       model, layers = self.model_layer_dependencies[model_id]\r\n--> 648       self._reconstruct_model(model_id, model, layers)\r\n    649       _finalize_config_layers([model])\r\n    650 \r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_model(self, model_id, model, layers)\r\n    693       (inputs, outputs,\r\n    694        created_layers) = functional_lib.reconstruct_from_config(\r\n--> 695            config, created_layers={layer.name: layer for layer in layers})\r\n    696       model.__init__(inputs, outputs, name=config['name'])\r\n    697       functional_lib.connect_ancillary_layers(model, created_layers)\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)\r\n   1293       if layer in unprocessed_nodes:\r\n   1294         for node_data in unprocessed_nodes.pop(layer):\r\n-> 1295           process_node(layer, node_data)\r\n   1296 \r\n   1297   input_tensors = []\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)\r\n   1241         input_tensors = (\r\n   1242             base_layer_utils.unnest_if_single_tensor(input_tensors))\r\n-> 1243       output_tensors = layer(input_tensors, **kwargs)\r\n   1244 \r\n   1245       # Update node index map.\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    968     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    969       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 970                                                 input_list)\r\n    971 \r\n    972     # Maintains info about the `Layer.call` stack.\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1106       # Check input assumptions set after layer building, e.g. input shape.\r\n   1107       outputs = self._keras_tensor_symbolic_call(\r\n-> 1108           inputs, input_masks, args, kwargs)\r\n   1109 \r\n   1110       if outputs is None:\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    838       return nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    839     else:\r\n--> 840       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    841 \r\n    842   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    878           self._maybe_build(inputs)\r\n    879           inputs = self._maybe_cast_inputs(inputs)\r\n--> 880           outputs = call_fn(inputs, *args, **kwargs)\r\n    881 \r\n    882         self._handle_activity_regularization(inputs, outputs)\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)\r\n    916     with backprop.GradientTape(watch_accessed_variables=True) as tape,\\\r\n    917         variable_scope.variable_creator_scope(_variable_creator):\r\n--> 918       result = self.function(inputs, **kwargs)\r\n    919     self._check_variables(created_variables, tape.watched_variables())\r\n    920     return result\r\n\r\n~/Abrar/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in <lambda>(x)\r\n     99   def _FixedPaddingLayer(self, kernel_size, rate=1):  # pylint: disable=invalid-name\r\n    100     return tf.keras.layers.Lambda(\r\n--> 101         lambda x: _fixed_padding(x, kernel_size, rate))\r\n    102 \r\n    103   def Conv2D(self, filters, kernel_size, **kwargs):  # pylint: disable=invalid-name\r\n\r\nNameError: name '_fixed_padding' is not defined`", "Hi @abrarum --\r\n\r\n(1) Is the index error still present in TF-nightly? I wasn't clear earlier, but the issue would still be in `2.4.1.`, so you will have to use TF-nightly until TF 2.5 is released to overcome this bug.\r\n\r\n(2) For your new error, can you file a new issue please? Thanks! "]}, {"number": 43526, "title": "Cannot install tensorflow-gpu==2.0.0 by pip", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: p40\r\n\r\n**Describe the problem**\r\n\r\nI got this error.\r\n```bash\r\nroot@dc45fa59c9f0:/workspace# pip install tensorflow-gpu==2.0.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0)\r\nERROR: No matching distribution found for tensorflow-gpu==2.0.0\r\n```", "comments": ["@jusonn \r\nPlease refer to this guide \r\nhttps://www.tensorflow.org/install\r\nuse commands :\r\n# Requires the latest pip\r\npip install --upgrade pip\r\n\r\n# Current stable release for CPU and GPU\r\npip install tensorflow", "@Saduf2019 \r\nThanks, \r\nbut it still not work.", "@jusonn \r\nPlease verify if the python installed is 64 bits, please refer to [this comment](https://github.com/tensorflow/tensorflow/issues/26578#issuecomment-471589989).", "Also refer to [comment](https://github.com/tensorflow/tensorflow/issues/26578#issuecomment-617275081), you can use google colab>change runtime>GPU", "python is installed with 64bit and I upgraded pip.\r\nbut i got same error\r\n```\r\nroot@dc45fa59c9f0:/workspace# pip install tensorflow-gpu==2.0.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1)\r\nERROR: No matching distribution found for tensorflow-gpu==2.0.0\r\n```\r\n\r\nCan you install tensorflow-gpu==2.0.0??", "@Saduf2019 \r\nIt was due to my python version.\r\ntensorflow 2.0.0 does not support python 3.8", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43526\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43526\">No</a>\n", "I'm getting this error with Python 3.7.5., not Python 8:\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.0.0rc2 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.6.0, 2.6.1, 2.6.2, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.8.0rc0)\r\nERROR: No matching distribution found for tensorflow-gpu==2.0.0rc2\r\n```\r\n\r\nHow do I solve this?"]}, {"number": 43525, "title": "Adding HDFS connection cache on tensorflow side #43187", "body": "This is a PR from TaiJi AI platform in Tencent.", "comments": []}, {"number": 43524, "title": "Fix the wrong cli parsing value for xnnpack_delegate option.", "body": "The original code uses a ptr address as the \"xnnpack_delegate\" setting value. It's incorrect.", "comments": ["@JerryShih  Can you please resolve conflicts? Thanks!", "@teijeong \r\nHi, I have fixed the merge conflict."]}, {"number": 43521, "title": "TF 2.0 create model-- 'Tensor' object has no attribute 'numpy'", "body": "tf version is 2.0,\r\nbelow is my import packages:\r\n```\r\nimport numpy as np\r\nimport keras.backend as K\r\nimport keras\r\nfrom keras.engine.topology import Layer\r\nfrom keras.layers import Input, Reshape,Dense, Activation, Flatten,Dropout,TimeDistributed,dot,concatenate,multiply,Permute,Add\r\nfrom keras.layers.core import Lambda, Activation\r\nfrom keras.models import Model\r\n```\r\nbelow is my code:\r\n```\r\n\tinp = Input(shape=(TIME_STEPS, INPUT_DIM,))\r\n\tinp_time = Input(shape=(1,))\r\n\tprint(inp_time.numpy())\r\n```\r\n\r\nbelow is my log\r\n```\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/Users/coolinear/Desktop/tjk_full_atte/demo_tjk_full_atte/model_full_atte.py\", line 71, in <module>\r\n    transformer_model(1251)\r\n  File \"/Users/coolinear/Desktop/tjk_full_atte/demo_tjk_full_atte/model_full_atte.py\", line 32, in transformer_model\r\n    print(inp_time.numpy())\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\nwhat can i do, to transform a tensor(there is variable \"inp_time\") to a int?", "comments": ["@ixNeo \r\n\r\nThis is duplicate of #43516. Can we close here and track the issue in #43516.\r\n\r\nPlease, see similar issue #27519 and [SO link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy) and see if it helps you.\r\n\r\nCan you try with tf.compat.v1.disable_eager_execution() ans see if the issue still persists. Thanks!", "You cannot use `numpy()` cause it produces a symbolic tensor. \r\nIt is documented at https://www.tensorflow.org/api_docs/python/tf/keras/Input:\r\n> Note that even if eager execution is enabled, Input produces a symbolic tensor (i.e. a placeholder). This symbolic tensor can be used with other TensorFlow ops", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43521\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43521\">No</a>\n"]}, {"number": 43520, "title": "Difference between keras and tf.keras", "body": "I found tf.keras is quite different with keras, in keras, bug occur when you use normal function instead Lambda wrapper function, bug is \"AttributeError: 'NoneType' object has no attribute '_inbound_nodes'\", while in tf.keras, it is allowed use normal function and tf.keras.layers. funtion simultaneously.\r\nkeras seems use different data structure, create innode between layers.\r\n\r\n", "comments": ["Please read this https://github.com/keras-team/keras/releases/tag/2.4.0", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43519, "title": "Need use cases to guide tensorflow installation", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/install\r\n\r\n## Description of issue (what needs changing):\r\n\r\nPeople want tensorflow for different reasons - to play, to learn from, to deploy in this/that/other environment, to integrate into legacy systems, etc. Not all installations are appropriate to all use cases. So far I can find zero guidance on integrating tensorflow inference module that works in python laboratory environment into Windows .exe desktop application built from C++ source. That is: \"Install on Windows\" is simply too coarse a description that applies to some use cases, but not to others. I've definitely installed something on Windows, and I can definitely use it for python-based explorations... but it is completely unclear to me if I can deploy it in production in the environment I need to, or what exactly the installation contains. I suggest that there needs to be a comprehensive (and growing) list of use cases that lay out how tensorflow can be used in conjunction with various requirements.\r\n\r\nSadly, the modern trend is to offer 2 sentences of introduction, and then say \"sign up and get started\". That's not helpful if it takes 6 months to realise there is either a dead end to the preferred development path, or a prohibitively expensive redeployment  to a new platform required for a legacy product :-( Right now, I simply cannot tell if either of those is a possibility... or something else.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? n/a\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? n/a\r\n\r\n### Returns defined\r\n\r\nAre return values defined? n/a\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined?  n/a\r\n\r\n### Usage example\r\n\r\nIs there a usage example? n/a\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? n/a\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? I have no idea what a pull request is, so that would be a no.\r\n", "comments": ["@omatai \r\nPlease follow instructions here: [link](https://www.tensorflow.org/install/pip), #42367, #36167 [these should guide you]\r\n\r\n- Ensure you have 64 bit cpu and python [python 3.8 works from tf version 2.2].\r\n- check if your[ AVX is supported (cpu supports AVX instruction sets.)](https://www.tensorflow.org/install/pip#hardware-requirements)\r\n- You have not installed the [Microsoft Visual C++ Redistributable package](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n\r\nThanks!", "> Please follow instructions here: [link](https://www.tensorflow.org/install/pip), #42367, #36167 [these should guide you]\r\n\r\n2 of those 3 suggestions are an admission that the documentation is insufficient - if the documentation were sufficient, there would be no need for external links. The 3rd link (to the pip install page) makes no mention of \"windows\" or \"C++\" or other elements I mentioned in the use case I am interested in. And nor should it - the guidance on what to install depending on the use case that interests you should exist at the top level to guide you towards the lower levels.\r\n\r\nThe guidance I see as missing is something to answer the question \"should I...?\" rather than \"can I...\"? Can I install tensorflow with pip. Yes. From source? Probably. But which SHOULD I do according to my intended use of tensorflow? I think I should probably do both... maybe. But there's a good chance someone will decide that a precompiled tensorflow DLL for Windows would be a good idea, issue that on some future release (or maybe it's already there?) and I shouldn't install and build from source at all because that would be a waste of time. In that utopia, I would only have to include a single header file, link to that DLL, and in literally 5 minutes I would be using tensorflow. (In contrast, see [here](https://stackoverflow.com/questions/49348987/what-is-the-best-way-to-deploy-a-tensorflow-trained-graph-into-production) for an example of what is more likely: a question that was only answered to the asker's satisfaction after 18 months.)\r\n\r\nAm I wasting my time currently? I don't know, because there is no guidance to tell me how to install/build/configure tensorflow that is directed towards particular use cases. Only when I get to the end of the install/build/configure mission will I be able to tell if I have done the right thing.\r\n\r\nIt's not about Windows or C++ or anything so specific - it's about the intended use of tensorflow, and which path(s) one should take to install/build/configure so that (a) one does not waste time exploring non-productive paths and instead (b) choose the correct path confidently from the outset.\r\n\r\n", "There's an uncountable number of possible use scenarios. There's a finite number of developers. We cannot simply document everything. The best way is to ask on StackOverflow (or a similar subsite there) about your specific scenario, with as many detials as possible. We redirect to SO, since we prefer to keep GitHub only for issues related to code.\r\n\r\nIn general, if you use the Python API and the pip package works for you, you install that. Otherwise, you build from source.\r\n\r\nIf you need C++ or other language bindings, you build from source again and then use the provided libraries (shared objects)", "If I could encourage you to take that last sentence and incorporate it into the \"Build from source\" documentation under Tensorflow Install instructions, that would be a very useful start. Why? Because essentially the documentation currently says:\r\n* Don't build from source (you idiot) - use the pre-compiled binaries\r\n* If you (are stupid enough to) build from source, you'll just build an installer for python anyway, so don't bother.\r\n\r\nBut what you are saying (that the documentation currently does not say) is that building from source produces libraries. Great! That is guidance that is missing: please document it. And also document where to find what else is built. And what it is useful for.\r\n\r\nAs it stands, the implicit guidance provided by the documentation (\"don't bother\") is distinctly unhelpful. And ok: so use cases might be far too fine-grained to be practical. But it would take very little to say things like:\r\n* If you need to do any of the following, you will need to build from source:\r\n  - this platform/application/context\r\n  - that platform/application/context\r\n  - other platform/application/context\r\n* When you build from source, you will create:\r\n  - this in this folder\r\n  - that in that folder\r\n  - other in other folder\r\n\r\nReading the \"build from source\" documentation as it stands, there appears to be no point whatsoever building from source. Similar applies to the API for various languages - there is no hint as to how to access it. Do I go outside and shout tensorflow::ops::fingerprint at the universe? I hope not. But from the documentation, I cannot be 100% certain this is not the case because the documentation fails to tell me what IS the case.", "Can you please clarify the request?\r\nBetter yet, these docs are over here and pull requests welcome :) https://github.com/tensorflow/docs/tree/master/site/en/install", "Sorry - I have given up using Tensorflow directly since it is too poorly documented. For a simple network, I have found that coding it from scratch by hand (a) took less time than trying to get TF to build and (b) ran fast enough for my needs. For more complex networks, I am hopeful that I can adapt the Windows [DirectML Samples](https://github.com/microsoft/DirectML) to my needs. I'm not in a position to contribute to the docs because I cannot make anything work where I need it to - integrated into a legacy C++ application on Windows :-(", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43519\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43519\">No</a>\n"]}, {"number": 43518, "title": "Keras saved model gives different accuracy compared to the original model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: NVidia driver v450.66 CUDA 11.0\r\n- GPU model and memory: 2080ti 11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n**Describe the expected behavior**\r\n\r\nThe following code is supposed to output the same accuracy from model and loaded_model, but somehow they're different.\r\nIf I run model.predict instead, they're consistent though.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nx = np.random.rand(10000, 10)\r\ny = np.random.choice([0, 1], (10000, ))\r\n\r\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\r\n\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(128, activation='relu'),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(32, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\n\r\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=10)\r\n\r\nmodel.save('./test_model/')\r\n\r\nloaded_model = tf.keras.models.load_model('./test_model/')\r\n\r\nprint(model.evaluate(x_test, y_test))\r\nprint(loaded_model.evaluate(x_test, y_test))\r\n```\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@peidaqi \r\n\r\nI have tried in colab with TF nightly version and i am not seeing any issue.Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/dbdf7583998dbe8ef9d607a59f2fbb30/untitled391.ipynb).Please, verify once and close the issue. Thanks!", "Ok - so this seems like a bug in TF 2.3.0 (and below) and is fixed in the new nightly build.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43518\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43518\">No</a>\n"]}, {"number": 43517, "title": "Getting a \u201cRuntimeError: Expected bias tensor to be a vector\u201d while trying to do full integer quantization on a keras model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Installed via pip\r\n- TensorFlow version (or github SHA if from source): [2.4.0.dev20200923]\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\nmodel = load_model('path_to_model/model_v1.h5', custom_objects = {'ScaleLayer': ScaleLayer, 'ReshapeLayer': ReshapeLayer})\r\n\r\nfor i,layer in enumerate(model.layers):\r\n    print (layer.trainable, layer.name)\r\n\r\ndef representative_dataset_gen():\r\n    i = 0\r\n    for i in range(1000, 3190):\r\n        img_crop = cv2.imread('path_to_img/Frame_'\r\n                                + str(i) + '.jpg')\r\n        img_crop = img_crop.astype(np.float32)\r\n        img_cropped = cv2.resize(img_crop, (150, 150))\r\n        print (img_cropped.shape)\r\n        oImage = np.reshape(img_cropped, (1, 150, 150, 3))\r\n        yield [oImage]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_quant_model = converter.convert()\r\nopen('./converted/full_int_quant.tflite', \"wb\").write(tflite_quant_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\nFile \"main.py\", line 77, in <module>\r\n    main(parse_arg(sys.argv[1:]))\r\n  File \"main.py\", line 46, in main\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 892, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 650, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 478, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, activations_type)\r\n  File \"/home/anand/virtual_environs/env_16.04_py3.6/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 98, in calibrate_and_quantize\r\n    np.dtype(activations_type.as_numpy_dtype()).num)\r\nRuntimeError: Expected bias tensor to be a vector.\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n\r\n[model_v1.txt](https://github.com/tensorflow/tensorflow/files/5273123/model_v1.txt)\r\n\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["https://stackoverflow.com/questions/64019538/getting-a-runtimeerror-expected-bias-tensor-to-be-a-vector-while-trying-to-do", "@An-Shank,\r\nOn running the given code, I am facing an error stating `NameError: name 'ScaleLayer' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a8d413a53e2475481040479ea3de8ebb/43517.ipynb ).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "> @An-Shank,\r\n> On running the given code, I am facing an error stating `NameError: name 'ScaleLayer' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a8d413a53e2475481040479ea3de8ebb/43517.ipynb).\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\nHere is the code:\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport cv2\r\nfrom keras import backend as K\r\nimport numpy as np\r\nfrom converter.model import ScaleLayer, ReshapeLayer\r\n\r\nclass ScaleLayer(KL.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(ScaleLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        assert len(input_shape) >= 2\r\n\r\n        if K.image_data_format() == 'channels_last':\r\n            ndim = int(input_shape[-1])\r\n        else:\r\n            ndim = int(input_shape[1])\r\n\r\n        self.gamma = self.add_weight(name='gamma', shape=(ndim, ))\r\n        self.beta = self.add_weight(name='beta', shape=(ndim, ))\r\n\r\n        super(ScaleLayer, self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        input_shape = K.int_shape(x)\r\n\r\n        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\r\n\r\n        broadcast_shape = [1] * len(input_shape)\r\n        broadcast_shape[bn_axis] = input_shape[bn_axis]\r\n\r\n        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\r\n        broadcast_beta = K.reshape(self.beta, broadcast_shape)\r\n\r\n        output = tf.math.multiply(x, broadcast_gamma)\r\n        output = tf.math.add(output, broadcast_beta)\r\n        return output\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n\r\nclass ReshapeLayer(KL.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(ReshapeLayer, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        assert len(input_shape) >= 2\r\n        super(ReshapeLayer, self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        s = K.shape(x)\r\n        zeros_w = tf.zeros((s[0], 1, s[2], s[3]), tf.float32)\r\n        r = K.concatenate([x, zeros_w], 1)\r\n\r\n        s = K.shape(r)\r\n        zeros_h = tf.zeros((s[0], s[1], 1, s[3]), tf.float32)\r\n        r = K.concatenate([r, zeros_h], 2)\r\n        return r    \r\n\r\n    def compute_output_shape(self, input_shape):\r\n        shape = tf.TensorShape(input_shape).as_list()\r\n        if K.image_data_format() == 'channels_last':\r\n            shape[1] = shape[1] + 1\r\n            shape[2] = shape[2] + 1\r\n        else:\r\n            shape[2] = shape[2] + 1\r\n            shape[3] = shape[3] + 1\r\n        return tf.TensorShape(shape)\r\n\r\nNUM_OF_SAMPLES = 1000\r\nDIR_PATH = '/storage/DataSetCleanedCropped/training/001/'\r\n\r\nmodel = load_model('/home/anand/model_v1.h5', custom_objects = {'ScaleLayer': ScaleLayer, 'ReshapeLayer': ReshapeLayer})\r\n\r\nfor i,layer in enumerate(model.layers):\r\n    print (layer.trainable, layer.name)\r\n\r\ndef representative_dataset_gen():\r\n    i = 0\r\n    for i in range(1000, 3190):\r\n        img_crop = cv2.imread('/storage/DataSetCleanedCropped/training/001/Frame_'\r\n                                + str(i) + '.jpg')\r\n        img_crop = img_crop.astype(np.float32)\r\n        img_cropped = cv2.resize(img_crop, (150, 150))\r\n        print (img_cropped.shape)\r\n        oImage = np.reshape(img_cropped, (1, 150, 150, 3))\r\n        yield [oImage]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\ntflite_quant_model = converter.convert()\r\nopen('./converted/full_int_quant.tflite', \"wb\").write(tflite_quant_model)", "> @An-Shank,\r\n> On running the given code, I am facing an error stating `NameError: name 'ScaleLayer' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a8d413a53e2475481040479ea3de8ebb/43517.ipynb).\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n\r\nApologies for the formatting errors in previous comment. Please find the code below\r\n\r\n    import tensorflow as tf\r\n    from tensorflow import keras\r\n    import cv2\r\n    from keras import backend as K\r\n    import numpy as np\r\n    from converter.model import ScaleLayer, ReshapeLayer\r\n\r\n    class ScaleLayer(KL.Layer):\r\n        def init(self, **kwargs):\r\n            super(ScaleLayer, self).init(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            assert len(input_shape) >= 2\r\n\r\n            if K.image_data_format() == 'channels_last':\r\n                ndim = int(input_shape[-1])\r\n            else:\r\n                ndim = int(input_shape[1])\r\n\r\n            self.gamma = self.add_weight(name='gamma', shape=(ndim, ))\r\n            self.beta = self.add_weight(name='beta', shape=(ndim, ))\r\n\r\n            super(ScaleLayer, self).build(input_shape)\r\n\r\n        def call(self, x):\r\n            input_shape = K.int_shape(x)\r\n\r\n            bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\r\n\r\n            broadcast_shape = [1] * len(input_shape)\r\n            broadcast_shape[bn_axis] = input_shape[bn_axis]\r\n\r\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\r\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\r\n\r\n            output = tf.math.multiply(x, broadcast_gamma)\r\n            output = tf.math.add(output, broadcast_beta)\r\n            return output\r\n\r\n        def compute_output_shape(self, input_shape):\r\n            return input_shape\r\n\r\n    class ReshapeLayer(KL.Layer):\r\n        def init(self, **kwargs):\r\n            super(ReshapeLayer, self).init(**kwargs)\r\n\r\n        def build(self, input_shape):\r\n            assert len(input_shape) >= 2\r\n            super(ReshapeLayer, self).build(input_shape)\r\n\r\n        def call(self, x):\r\n            s = K.shape(x)\r\n            zeros_w = tf.zeros((s[0], 1, s[2], s[3]), tf.float32)\r\n            r = K.concatenate([x, zeros_w], 1)\r\n\r\n            s = K.shape(r)\r\n            zeros_h = tf.zeros((s[0], s[1], 1, s[3]), tf.float32)\r\n            r = K.concatenate([r, zeros_h], 2)\r\n            return r    \r\n\r\n        def compute_output_shape(self, input_shape):\r\n            shape = tf.TensorShape(input_shape).as_list()\r\n            if K.image_data_format() == 'channels_last':\r\n                shape[1] = shape[1] + 1\r\n                shape[2] = shape[2] + 1\r\n            else:\r\n                shape[2] = shape[2] + 1\r\n                shape[3] = shape[3] + 1\r\n            return tf.TensorShape(shape)\r\n\r\n    NUM_OF_SAMPLES = 1000\r\n    DIR_PATH = '/storage/DataSetCleanedCropped/training/001/'\r\n\r\n    model = load_model('/home/anand/model_v1.h5', custom_objects = {'ScaleLayer': ScaleLayer, 'ReshapeLayer': ReshapeLayer})\r\n\r\n    for i,layer in enumerate(model.layers):\r\n        print (layer.trainable, layer.name)\r\n\r\n    def representative_dataset_gen():\r\n        i = 0\r\n        for i in range(1000, 3190):\r\n            img_crop = cv2.imread('/storage/DataSetCleanedCropped/training/001/Frame_'\r\n            + str(i) + '.jpg')\r\n            img_crop = img_crop.astype(np.float32)\r\n            img_cropped = cv2.resize(img_crop, (150, 150))\r\n            print (img_cropped.shape)\r\n            oImage = np.reshape(img_cropped, (1, 150, 150, 3))\r\n            yield [oImage]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_dataset_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n\r\n    tflite_quant_model = converter.convert()\r\n    open('./converted/full_int_quant.tflite', \"wb\").write(tflite_quant_model)\r\n", "@An-Shank,\r\nOn running the code I am facing an error stating `AttributeError: 'NoneType' object has no attribute 'astype'`.\r\n\r\nCould you please share the dataset (i.e. the contents of `DIR_PATH`) you are using in the code, so that we can reproduce the issue on our end. Thanks!", "Hi,\r\nI am facing the same error when trying to perform full integer quantization with mixed 16 bits activations and 8 bits weights ( tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8). In fact I have the same issue if i use classic method (tf.lite.OpsSet.TFLITE_BUILTINS).\r\nI did several tests. First it is working if i convert a MobileNet V1 network, so the error depends on the network architecture.\r\nBy dichotomy, i have found the layer which causes the error. I guess the error is raised when there is a ADD (AddV2 in my case) operation right after a Conv2D operation. The converter try to merge Add operation as a bias in Conv2D. In my case, the tensor dimension of the data to be added is [1,1,1,64]. Maybe the converter is expecting [1,64] which could explain the error raised.\r\nHere is my code used for conversion:\r\n```\r\nimport tensorflow as tf\r\n#import cv2\r\nimport tensorflow_datasets as tfds\r\n\r\nimport os\r\n\r\ndef representative_dataset_gen():\r\n  #tf.enable_eager_execution()\r\n  ds = tfds.load(\"flic\",shuffle_files=True, split='train')\r\n  assert isinstance(ds, tf.data.Dataset)\r\n  print(ds)\r\n  num_calibration_steps = 10;\r\n  for _ in range(num_calibration_steps):\r\n    #image, = data.take(1)\r\n    example = ds.take(1)\r\n    for i in example:\r\n        image = i[\"image\"]\r\n        name = i[\"moviename\"]\r\n        print(name)\r\n        print(\"type: \" + str(type(image)))\r\n        image = tf.image.resize(image,size=[256,256])\r\n        image = tf.expand_dims(image, axis=0)\r\n        #image = tf.transpose(image, [0, 3, 1, 2])\r\n        yield [image]\r\n\r\n\r\n# Convert onnx model to tflite format\r\n\r\n# convert it first in tensorflow frozen graph\r\nif not os.path.exists(\"./convertedModels/tensorflow\"):\r\n    os.makedirs(\"./convertedModels/tensorflow\")\r\n\r\n#Then convert it in tflite format\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('pretrained/model_frozen.pb', #TensorFlow freezegraph .pb model file\r\n                                                      input_arrays=['FCN_1/AlexNet/mul'], # name of input arrays as defined in torch.onnx.export function before.\r\n                                                      output_arrays=['FCN_1/Sum'],  # name of output arrays defined in torch.onnx.export function before.\r\n                                                      input_shapes={ 'FCN_1/AlexNet/mul': [1,256,256,3]}\r\n                                                      )\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n\r\nconverter.inference_input_type = tf.int16  # or tf.uint8\r\nconverter.inference_output_type = tf.int16  # or tf.uint8\r\n\r\n\r\nconverter.representative_dataset = tf.lite.RepresentativeDataset(\r\n    representative_dataset_gen)\r\n\r\ntf_lite_model = converter.convert()\r\n# save the converted model\r\nopen('fc4_quant.tflite', 'wb').write(tf_lite_model)\r\n\r\n```\r\nHere is the frozen graph of my network:\r\n\r\n[model_frozen.zip](https://github.com/tensorflow/tensorflow/files/5326664/model_frozen.zip)\r\n\r\n", "@ntreepoint,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "@amahendrakar , i have logged a new issue available here: https://github.com/tensorflow/tensorflow/issues/43877", "> Could you please share the dataset (i.e. the contents of `DIR_PATH`) you are using in the code, so that we can reproduce the issue on our end. Thanks!\r\n\r\n@An-Shank,\r\nAny updates regarding this? Is this still an issue? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43516, "title": "TF 2.0 create model-- 'Tensor' object has no attribute 'numpy'", "body": "```\r\n1 source code\r\n-------------------\r\ninp = Input(shape=(TIME_STEPS, INPUT_DIM,))\r\ninp_time = Input(shape=(1,))\r\nprint(tf.executing_eagerly())  # return True\r\n\r\nprint(inp_time.numpy()) # error\r\n# inp_time=inp_time.eval()/ K.get_value()/ x.numpy()/ K.eval()\r\nx = ConvAtteShare(inp_time=inp_time.numpy(),kernel=(32,32),stride=(16,16),filter_num=3)(inp)\r\nx = Dense(kind_num,activation='softmax')(x)\r\nm = Model(inputs=[inp,inp_time], outputs=[x], name='convatte-test')\r\nprint(m.summary())\r\n\r\n2 error\r\n----------------------------\r\nprint(inp_time.numpy())\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "comments": ["@ixNeo \r\nCode shared is incomplete, please provide with simple stand alone code to replicate the issue or is possible share a colab gist with issue faced along with tf version and error log. [When error description include '_TensorLike', then mostly it is due to using mixing of functions from keras and tf.keras in the code.]\r\n\r\n\r\nWith respect to error reported, please refer to : [link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy), [link1](https://github.com/tensorflow/tensorflow/issues/27519#issuecomment-490388556), #40569\r\nThanks!", "> @ixNeo\r\n> Code shared is incomplete, please provide with simple stand alone code to replicate the issue or is possible share a colab gist with issue faced along with tf version and error log. [When error description include '_TensorLike', then mostly it is due to using mixing of functions from keras and tf.keras in the code.]\r\n> \r\n> With respect to error reported, please refer to : [link](https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy), [link1](https://github.com/tensorflow/tensorflow/issues/27519#issuecomment-490388556), #40569\r\n> Thanks!\r\n\r\ntf version is 2.0, \r\nbelow is my import packages:\r\n```\r\nimport numpy as np\r\n# import tensorflow.keras.backend as K\r\nimport keras.backend as K\r\n\r\nimport keras\r\nfrom keras.engine.topology import Layer\r\nfrom keras.layers import Input, Reshape,Dense, Activation, Flatten,Dropout,TimeDistributed,dot,concatenate,multiply,Permute,Add\r\nfrom keras.layers.core import Lambda, Activation\r\nfrom keras.models import Model\r\n```\r\n\r\nbelow is my log:\r\n```\r\n\tinp = Input(shape=(TIME_STEPS, INPUT_DIM,))\r\n\tinp_time = Input(shape=(1,))\r\n\tprint(inp_time.numpy())\r\n\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/Users/coolinear/Desktop/tjk_full_atte/demo_tjk_full_atte/model_full_atte.py\", line 71, in <module>\r\n    transformer_model(1251)\r\n  File \"/Users/coolinear/Desktop/tjk_full_atte/demo_tjk_full_atte/model_full_atte.py\", line 32, in transformer_model\r\n    print(inp_time.numpy())\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```", "@ixNeo \r\nPlease provide your issue in a colab gist along with error in it, have you referred and tried the solutions shared, please share the output after you have applied then to your code.\r\n", "You cannot use `numpy()` cause it produces a symbolic tensor. \r\nIt is documented at https://www.tensorflow.org/api_docs/python/tf/keras/Input:\r\n> Note that even if eager execution is enabled, Input produces a symbolic tensor (i.e. a placeholder). This symbolic tensor can be used with other TensorFlow ops", "> You cannot use `numpy()` cause it produces a symbolic tensor.\r\n> It is documented at https://www.tensorflow.org/api_docs/python/tf/keras/Input:\r\n> \r\n> > Note that even if eager execution is enabled, Input produces a symbolic tensor (i.e. a placeholder). This symbolic tensor can be used with other TensorFlow ops\r\n\r\nit means i cannot get input's value ?", "@ixNeo It is not a bug I think you can close this. For support questions please post on https://stackoverflow.com/questions/tagged/tensorflow"]}, {"number": 43515, "title": "Pybadge error compiling example sketch micro_speech_arcarda", "body": "@tensorflow/micro\r\n\r\n**System information**\r\nWindows 10 laptop\r\n- TensorFlow installed from (source or binary): Both Arduino_TensorflowLite 2.1.0-ALPHA and Adafruit Tensorflow Lite 1.2.1 fronm the Arduino IDE libraries\r\n- Tensorflow version (commit SHA if source): dont know \r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Pybadge M4 with external microphone\r\n\r\n**Describe the problem**\r\ngot the following compile error:\r\nArduino: 1.8.13 (Windows 10), Board: \"Adafruit pyBadge M4 Express (SAMD51), Enabled, 180 MHz (overclock), Fastest (-Ofast), 50 MHz (standard), TinyUSB, Off\"\r\nMultiple libraries were found for \"Adafruit_ZeroDMA.h\"\r\nUsed: C:\\Users\\Joanna\\AppData\\Local\\Arduino15\\packages\\adafruit\\hardware\\samd\\1.6.2\\libraries\\Adafruit_ZeroDMA\r\nIn file included from C:\\Users\\Joanna\\Documents\\Arduino\\libraries\\Adafruit_TensorFlow_Lite\\examples\\micro_speech_arcada\\micro_speech_arcada.ino:22:\r\nNot used: C:\\Users\\Joanna\\Documents\\Arduino\\libraries\\Adafruit_Zero_DMA_Library\r\naudio_provider.h:19:10: fatal error: tensorflow/lite/c/c_api_internal.h: No such file or directory\r\n19 | #include \"tensorflow/lite/c/c_api_internal.h\"\r\n| ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nexit status 1\r\ntensorflow/lite/c/c_api_internal.h: No such file or directory\r\n\r\n\r\n**Please provide the exact steps when you ran into the problem**\r\nSetup new Arduino IDE\r\nAdded Adafruit https://adafruit.github.io/arduino-board-index/package_adafruit_index.json\r\nto boards manager url in preferences\r\nAdded Adafruit SAMD Boards 1.6.2\r\nAdded all Adafruit arcada libraries 2.5.0\r\nLoaded Examples > Adafruit Tensorflow Lite > micro_speech_arcarda.\r\n\r\nGot the attached error\r\n\r\n", "comments": ["This issue is more suitable on [AdaFruit TF Lite repository](https://github.com/adafruit/Adafruit_TFLite/issues) since the example is hosted on that repo.\r\nSee https://github.com/adafruit/Adafruit_TFLite/tree/master/examples/micro_speech_arcada\r\nPlease can try posting it [there](https://github.com/adafruit/Adafruit_TFLite/issues) for quicker response.\r\nThank you.", "Thank you - I have posted it there.", "Hi Can I ask why you think it should go in the Adafruit TF Lite repository? \r\nThe example requires both the Adafruit and Arduino TF Lite libraries/\r\nCould it be an Arduino Library issue as that is where header file structure is i,e, src/tensorflow/lite/c/?", "> See https://github.com/adafruit/Adafruit_TFLite/tree/master/examples/micro_speech_arcada\r\n\r\nThe above example resides in AdaFruit library therefore future edits should be made in that repo.\r\nThanks!"]}, {"number": 43514, "title": "Is there a tf-serving document updated for tf2 ?", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\n[https://www.tensorflow.org/tfx/serving/serving_advanced](https://www.tensorflow.org/tfx/serving/serving_advanced)\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThis documment use tf1 function , Is there a new version for tf2 ?", "comments": ["@DachuanZhao Please post this issue [here](https://github.com/tensorflow/serving/issues) as this issue is more suited to Tensorflow serving not here. Thanks!", "> @DachuanZhao Please post this issue [here](https://github.com/tensorflow/serving/issues) as this issue is more suited to Tensorflow serving not here. Thanks!\r\n\r\nok \uff0cI will close this issue . "]}, {"number": 43513, "title": "Use std::endl to force flush of output", "body": "Change-Id: Id44c6731e93bbada4ba086db91974ac7e2d65c40\r\n\r\nThis resolves dropped output into 'FATAL' (just std::cerr for the label_image), if/when std:cerr is buffered.\r\n", "comments": ["@teijeong, is there anything else I could to move this pull request forward?", "@mihaimaruseac \r\n> `LOG(...) << str` already inserts a `\\n` and flushes output at the end\r\n> \r\n> See https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/default/logging.cc;l=225-232;drc=6b126156d962547f4b4f66cf19aabfe14d75dd88\r\nIndeed, LOG in tensorflow inserts  `\\n` and flushes output at the end. However label_image example doesn't use LOG from tensorflow, it locally defines LOG like this:\r\n`#define LOG(x) std::cerr`\r\nhttps://github.com/tensorflow/tensorflow/blob/a0bb0084885edd64004155f70b49893e008bcccc/tensorflow/lite/examples/label_image/label_image.cc#L53\r\n", "I would say to upgrade to use the macros, but then the macros are from TF and this is TFLite so there might be size issues. Let's go ahead with this PR then", "Actually, can you add the `\"\\n\"` on the `LOG` macro definition instead?", "@mihaimaruseac , macros can't be used to add `std::endl` at the end, since it only controls front of `LOG(ERR) << \"foo bar\";` and `std::endl` should be added at the end.\r\n\r\nPerhaps LOG(ERR) could be redefined into proper function that adds `std::endl` (similarly what LOG in TF does).  But this is rather more involved.", "Oh, right. Plus, I see there are usages with `\\n` already\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/913e0999ec338a3002a7056cdb592d59965ac5e8/tensorflow/lite/examples/label_image/label_image.cc#L262-L265\r\n\r\nCan you make the change to use `\\n` instead of `std::endl`? Or do we really need the flushing behavior?", "> Or do we really need the flushing behavior?\r\n\r\nYes, we need flushing, `LOG_ERR` used in error conditions, where the next thing what  label_image does  is `exit(-1);` and all this output never reaches console (the process stderr/stdout).\r\n\r\nP.S. This pull request t (https://github.com/vsilyaev/tensorflow/commit/3c311e180d8e1aa7f7a7870f6d7eeea0c525d924) replaces all uses of `\\n` in the label_image example with `std::endl`.\r\n", "That is not true. This experiment shows that the output is still printed. Unless I am missing something else?\r\n\r\n```cc\r\n#include <iostream>\r\n\r\nint main() {\r\n\tstd::cerr << \"First message\\n\";\r\n\tstd::abort();\r\n\tstd::cerr << \"Second message\\n\";\r\n\treturn 0;\r\n}\r\n```\r\n\r\n```console\r\n[test] \u03bb g++ -O2 -Wall -Wextra test.cc -o gcc\r\n[test] \u03bb clang++ -O2 -Wall -Wextra test.cc -o clang\r\n[test] \u03bb ./gcc \r\nFirst message\r\nAborted (core dumped)\r\n[test] \u03bb ./gcc \r\nFirst message\r\nAborted (core dumped)\r\n[test] \u03bb ./gcc \r\nFirst message\r\nAborted (core dumped)\r\n[test] \u03bb ./clang \r\nFirst message\r\nAborted (core dumped)\r\n[test] \u03bb ./clang \r\nFirst message\r\nAborted (core dumped)\r\n[test] \u03bb ./clang \r\nFirst message\r\nAborted (core dumped)\r\n```", "@vsilyaev Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "@gbaned  I believe one piece of a puzzle is a difference between `std::cerr` and `std::cout` and `\\n` flushes `std:cerr`.\r\nSince tflite/label_image uses `std::cerr`,  I'm closing this.", "`std::cout` also flushes on `\\n`. `std::clog` is the one that does not", "> std::cout also flushes on \\n. std::clog is the one that does not\r\n\r\nThen we are not out of the woods yet. I have copy of label_image.c where LOG was accidentally modified to be `std::cout`, and without `<<std::endl` output wouldn't show on a console in a case of crush/`exit(-1)`.\r\n\r\nI would need to dig into this little bit further.\r\n\r\n", "@vsilyaev Any update on this PR? Please. Thanks!", "@gbaned,  I've updated pull_request/Use_std_endl_for_flush so now all LOG are auto-insert std::endl, so LOG in tensorflow/lite/examples/label_image  works the same as LOG inside tensforflow/lite . ", "@mihaimaruseac , force-pushed update where LOG is defined in the log.h"]}, {"number": 43512, "title": "einsum for SparseTensor", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMultiply two tensors that are not necessarily sparse matrices (2D tensors). For example, a 'tf.einsum' like function available for  sparse tensors. Converting sparse tensors to dense tensors must be avoided.\r\n\r\nAn example: Multiply A and B where A is a 2D dense tensor and B is a 3D sparse tensor as follows: C[i,j] = \\sum_k A[i,k] B[i,k,j]\r\n```\r\n# Example\r\n# inputs\r\nA = tf.constant([[1, 2, 3],\r\n                 [4, 5, 6],\r\n                 [7, 8, 9]], tf.float32)\r\nB = tf.sparse.SparseTensor(values=tf.constant([1,1,1,1,2,1,1,1,1,1,-1], tf.float32),indices=[[0,0,1],[0,1,2],[0,2,0],[1,0,0],[1,1,1],[1,1,2],[1,2,2],[2,0,2],[2,1,1],[2,2,1],[2,2,2]], dense_shape=[3,3,3])\r\n\r\n\r\n# output\r\nC = tf.constant([[3, 1, 2],\r\n                 [4, 10, 11],\r\n                 [9, 8, -1]], tf.float32)\r\n```\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nA 'tf.sparse.einsum(A,B,'...', a_sparse = False, b_sparse = True)' kind of function would be very useful for dealing with this kind of operations. \r\n\r\n**Who will benefit with this feature?**\r\nDevelopers that need to deal with sparse tensor multiplications in high dimensions (grater than 2D sparse tensors)\r\n\r\n**Any Other info.**\r\n", "comments": ["@curiarteb \r\nPlease confirm if this is a duplicate of #43497, if yes please close this as it is tracked at #43497.", "Yes. This is a duplicate of #43497. Ok. I will close this one and maintain the #43497 issue.\nEl 24 sept 2020 6:52 +0200, Saduf2019 <notifications@github.com>, escribi\u00f3:\n> @curiarteb\n> Please confirm if this is a duplicate of #43497, if yes please close this as it is tracked at #43497.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n", "Moving this to closed status with confirmation as this is a duplicate."]}, {"number": 43511, "title": "Fix the build for benchmarks for bluepill and STM32F4.", "body": "The root cause here is that a static inside a function still has locks\r\naround it, but a global static does not. This is the behavior if we use\r\nno-threadsafe-statics.\r\n\r\nAlso note that while this change fixes the build, the current HEAD is broken for `TARGET=bluepill` and `TAGS=cmsis-nn`. See issue #43510 for more details and PR #43509 improves the coverage for the CI system.\r\n\r\nManually tested with the following commands:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=stm32f4 TAGS=cmsis-nn keyword_benchmark person_detection_benchmark person_detection_experimental_benchmark\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=bluepill keyword_benchmark person_detection_benchmark person_detection_experimental_benchmark\r\n```", "comments": []}, {"number": 43509, "title": "Build benchmarks as part of `make build`", "body": "This should make it easier to ensure that benchmarks can be run on a variety of platforms.\r\n\r\nNote that in light of #43510 we may additionally want to build bluepill with TAGS=cmsis-nn but keeping that as a separate change in the interest of incrementally improving our CI coverage.\r\n\r\nManually confirmed that `make build` now also builds the benchmarks.\r\n\r\nAddresses b/169358565", "comments": ["@njeffrie: I just merged master and this PR is ready for review."]}, {"number": 43508, "title": "tensorflow build and install", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": []}, {"number": 43507, "title": "[Intel MKL] Adding support for FusedBatchNorm and AvgPool ops in eager mode", "body": "", "comments": []}, {"number": 43506, "title": "[Intel MKL] Adding support for MaxPool with native format", "body": "This PR needs to be merged after https://github.com/tensorflow/tensorflow/pull/43505", "comments": ["Thank you for reviewing this PR. I have addressed your comments.", "@mahmoud-abuzaina Can you please check @penpornk's comments and resolve conflicts?. Thanks!", "@gbaned I have resolved the conflicts. Thanks!\r\n@penpornk Thank you for the review. I updated the comment."]}, {"number": 43505, "title": "[Intel MKL] Adding support for AvgPool with native format", "body": "", "comments": ["Thank you for reviewing the PR. I have addressed your comments."]}, {"number": 43504, "title": "[Intel MKL] Adding support for FusedBatchNorm op with native format", "body": "", "comments": ["@mahmoud-abuzaina  Can you please resolve conflicts? Thanks!", "@gbaned I have resolved the conflicts. Thanks!", "Thank you for the suggestions. I have addressed your comments.", "@penpornk thank you for the notification. I will work on the \"cleanup\" PR."]}, {"number": 43503, "title": "Training AlexNet structure on entire ImageNet in Tensorflow 2.0, But why the loss and accuracy are all flat?", "body": "I am trying to replicate AlexNet model on ImageNet (for learning purposes). The dataset is 1.2 million ImageNet dataset with 50K validation. \r\nModel is a sequential keras CNN model, and I am training this model on 8 GPUs. \r\n\r\nThe code can be seen here:\r\n\r\n[Code on GitHub](https://github.com/anejad/Convolutional-Neural-Network-Champions/blob/master/AlexNet/AlexNet_Tensorflow_Full.py).\r\n\r\nHowever, when I start to train this model (8 GPUs, Batch size=32*nGPU) I get flat loss and accuracy:\r\nEpoch 14/90\r\n5004/5004 [==============================] - 748s 150ms/step - loss: 6.9080 - accuracy: 8.8055e-04 - val_loss: 6.9078 - val_accuracy: 9.6154e-04\r\nEpoch 15/90\r\n5004/5004 [==============================] - 747s 149ms/step - loss: 6.9080 - accuracy: 9.6798e-04 - val_loss: 6.9078 - val_accuracy: 3.2051e-04\r\nEpoch 16/90\r\n5004/5004 [==============================] - 745s 149ms/step - loss: 6.9080 - accuracy: 0.0010 - val_loss: 6.9077 - val_accuracy: 0.0013\r\nEpoch 17/90\r\n5004/5004 [==============================] - 748s 149ms/step - loss: 6.9080 - accuracy: 9.5559e-04 - val_loss: 6.9078 - val_accuracy: 8.0128e-04\r\nEpoch 18/90\r\n5004/5004 [==============================] - 749s 150ms/step - loss: 6.9080 - accuracy: 8.8055e-04 - val_loss: 6.9078 - val_accuracy: 0.0014\r\nEpoch 19/90\r\n5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 9.4924e-04 - val_loss: 6.9078 - val_accuracy: 0.0013\r\nEpoch 20/90\r\n5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 9.3051e-04 - val_loss: 6.9078 - val_accuracy: 0.0016\r\nEpoch 21/90\r\n5004/5004 [==============================] - 748s 149ms/step - loss: 6.9080 - accuracy: 0.0010 - val_loss: 6.9078 - val_accuracy: 0.0011\r\nEpoch 22/90\r\n5004/5004 [==============================] - 749s 150ms/step - loss: 6.9080 - accuracy: 9.8047e-04 - val_loss: 6.9078 - val_accuracy: 0.0014\r\nEpoch 23/90\r\n5004/5004 [==============================] - 748s 149ms/step - loss: 6.9080 - accuracy: 7.3067e-04 - val_loss: 6.9078 - val_accuracy: 9.6154e-04\r\nEpoch 24/90\r\n5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 0.0011 - val_loss: 6.9078 - val_accuracy: 0.0014\r\nEpoch 25/90\r\n5004/5004 [==============================] - 750s 150ms/step - loss: 6.9080 - accuracy: 0.0010 - val_loss: 6.9078 - val_accuracy: 1.6026e-04\r\n\r\nAny ideas how to fix this or what causes the learning to be flat???\r\n\r\n\r\n\r\n", "comments": ["This is very similar to this issue [here](https://stackoverflow.com/questions/63236540/tensorflow-loss-and-accuracy-stay-flat-training-cnn-on-image-classification). Please take a look at it.", "> This is very similar to this issue [here](https://stackoverflow.com/questions/63236540/tensorflow-loss-and-accuracy-stay-flat-training-cnn-on-image-classification). Please take a look at it.\r\n\r\nThis case is completely different than mine. I am trying to train the Tensorflow model on 1000 classes so I cannot use Binary mode in my ImageDataGenerator. Am I missing anything? ", "This is the code:\r\n```\r\ndef Pipelines(train_directory,validation_directory,BATCH_SIZE,SEED,image_height, image_width,classes):\r\n    train_datagen = ImageDataGenerator(\r\n                  rescale=1./255,) \r\n                  #rotation_range=10,\r\n                  #width_shift_range=0.1,\r\n                  #height_shift_range=0.1,\r\n                  #shear_range=0.1,\r\n                  #zoom_range=0.1)\r\n\r\n    train_generator = train_datagen.flow_from_directory(train_directory,\\\r\n                                                    target_size=(image_height, image_width),\\\r\n                                                    color_mode=\"rgb\",\\\r\n                                                    batch_size=BATCH_SIZE,\\\r\n                                                    seed=SEED,\\\r\n                                                    shuffle=True,\\\r\n                                                    classes=classes,\r\n                                                    class_mode=\"categorical\")\r\n    valid_datagen=ImageDataGenerator(rescale=1./255)\r\n\r\n    valid_generator = valid_datagen.flow_from_directory(validation_directory,\r\n                                                            target_size=(image_height, image_width),\r\n                                                            color_mode=\"rgb\",\r\n                                                            batch_size=BATCH_SIZE,\r\n                                                            seed=SEED,\r\n                                                            shuffle=True,\r\n                                                            classes=classes,\r\n                                                            class_mode=\"categorical\",\r\n                                                            )\r\nreturn train_generator,valid_generator\r\n\r\ndef AlexNet( input_shape, num_classes):\r\n    initializer = tf.keras.initializers.GlorotNormal()\r\n    model = Sequential(name='AlexNet')\r\n    model.add(Conv2D(96, kernel_size=(11,11), strides= 4,\r\n                    padding= 'valid', activation= 'relu',\r\n                    input_shape= input_shape, kernel_initializer= initializer))\r\n    model.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\r\n                          padding= 'valid', data_format= None))\r\n\r\n    model.add(Conv2D(256, kernel_size=(5,5), strides= 1,\r\n                    padding= 'same', activation= 'relu',\r\n                    kernel_initializer= initializer))\r\n    model.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\r\n                          padding= 'valid', data_format= None)) \r\n\r\n    model.add(Conv2D(384, kernel_size=(3,3), strides= 1,\r\n                    padding= 'same', activation= 'relu',\r\n                    kernel_initializer= initializer))\r\n\r\n    model.add(Conv2D(384, kernel_size=(3,3), strides= 1,\r\n                    padding= 'same', activation= 'relu',\r\n                    kernel_initializer= initializer))\r\n\r\n    model.add(Conv2D(256, kernel_size=(3,3), strides= 1,\r\n                    padding= 'same', activation= 'relu',\r\n                    kernel_initializer= initializer))\r\n\r\n    model.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\r\n                          padding= 'valid', data_format= None))\r\n\r\n    model.add(Flatten())\r\n    model.add(Dense(4096, activation= 'relu'))\r\n    model.add(Dropout(rate=0.5))\r\n    model.add(Dense(4096, activation= 'relu'))\r\n    model.add(Dropout(rate=0.5))\r\n    model.add(Dense(1000, activation= 'relu'))\r\n    model.add(Dense(num_classes, activation= 'softmax'))\r\n\r\n    model.compile(optimizer= tfa.optimizers.SGDW(learning_rate=0.01,momentum=0.9,weight_decay=0.0005,nesterov=False),\r\n                loss='categorical_crossentropy',\r\n                metrics=['categorical_accuracy'])\r\n    return model\r\n\r\ndef TrainModel(model,valid_generator,train_generator,EPOCHS,BATCH_SIZE,PATIENCE_FRACTION):   \r\n    early_stopping=EarlyStopping(monitor='val_loss',min_delta=0.0005,patience=int(EPOCHS*PATIENCE_FRACTION))\r\n    checkpoint=ModelCheckpoint(filepath='./models/checkpoint_saves/AlexNet_checkpoint_save.h5',\\\r\n                               monitor='val_loss',save_weights_only=False)\r\n    callback_list = [early_stopping,checkpoint]\r\n    history=model.fit(train_generator,\r\n                    epochs=EPOCHS,\r\n                    validation_data=valid_generator,\r\n                    callbacks=callback_list,\r\n                    steps_per_epoch=train_generator.samples//BATCH_SIZE,\r\n                    validation_steps= valid_generator.samples//BATCH_SIZE,\r\n                    verbose=1)\r\n    # laoding the best model \r\n    checkpoint_model=tf.keras.models.load_model('./models/checkpoint_saves/AlexNet_checkpoint_save.h5')\r\n    return checkpoint_model,history\r\n\r\n# Train model\r\nbs=BATCH_SIZE\r\nif strategy.num_replicas_in_sync>0:\r\n    bs=BATCH_SIZE*strategy.num_replicas_in_sync\r\nmodel,history=TrainModel(model,valid_generator,train_generator,EPOCHS,bs,PATIENCE_FRACTION)\r\n```", "Hi @anejad, are you still facing this problem? Also what strategy are you using?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43503\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43503\">No</a>\n"]}, {"number": 43502, "title": "keras.layers.experimental.preprocessing.Discretization fails for mixed precision training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Databricks Runtime 7.3\r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: AWS p3.xlarge\r\n\r\n**Describe the current behavior**\r\nkeras.layers.experimental.preprocessing.Discretization fails when mixed precision is enabled\r\n\r\nErrors:\r\n```\r\nTypeError: Value passed to parameter 'input' has DataType float16 not in list of allowed values: int32, int64, float32, float64\r\ntensorflow/python/keras/layers/preprocessing/discretization.py in call(self, inputs)\r\n     99           dense_shape=array_ops.identity(inputs.dense_shape))\r\n    100     else:\r\n--> 101       return math_ops._bucketize(inputs, boundaries=self.bins)  # pylint: disable=protected-access\r\n````\r\n\r\n```python\r\nraw_input = keras.Input(shape=1, name=\"raw_input\")\r\nboundaries = [0, 1, 2, 3]\r\ndiscretization = keras.layers.experimental.preprocessing.Discretization(bins=boundaries)(raw_input)\r\n```\r\n\r\nThis appears to come from tf.math.bucketize()\r\n\r\n**Describe the expected behavior**\r\nThe replacement for TF Feature Columns should support mixed-precision training out of the box.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nWork around:\r\n```\r\nraw_input = keras.Input(shape=1, name=\"raw_input\")\r\nboundaries = [0, 1, 2, 3]\r\ndiscretization = keras.layers.experimental.preprocessing.Discretization(bins=boundaries, autocast=False)(raw_input)\r\n```\r\n\r\nHowever, I am having issues with serialization.", "comments": ["autocast doesn't appear to be serialized:\r\n\r\ndiscretization.get_config()\r\n\r\nOut[281]: {'name': 'discretization_5',\r\n 'trainable': True,\r\n 'dtype': {'class_name': 'Policy', 'config': {'name': 'mixed_float16'}},\r\n 'bins': ListWrapper([0, 1, 2, 3])}", "To fix serialization, I just changed the code to:\r\n```python\r\n  def call(self, floatx_inputs):\r\n    import tensorflow.keras.backend as K\r\n    \r\n    inputs = K.cast(floatx_inputs, \"float32\")\r\n ....\r\n```\r\n", "/cc @tanzhenyu ", "@jeisinge,\r\nCould you please install the latest TF-nightly version and check if you are facing the same issue. Thanks!", "@amahendrakar It was reproducible I think you can route this forward", "@jeisinge \r\nCould you please try on latest version of tf and let us know if you still face the same issue.\r\n\r\nI ran the code provided on tf 2.4 and do not see the type error mention in the issue template, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/aeb515f9c0563bd0a2bdd14b216e548c/untitled589.ipynb)", "I apologize for the delay.  I can confirm that this is no longer an issue with TF 2.4!  Thank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43502\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43502\">No</a>\n"]}, {"number": 43501, "title": "call back error ", "body": "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9fb782eee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\nthis the model\r\nModel: \"sequential\"\r\n\r\n\r\nthis the error \r\n\r\nmy code is \r\nprediction(consine):\r\nnew_model = tf.keras.models.load_model(\"model.h5\")\r\nprint(new_model.summary())\r\ntest_output = new_model.predict(cosine_s, verbose=0)\r\n return test_output\r\n\r\n\r\nI make call to this function every time i received new data.  ", "comments": ["i make a call ", "@mubarkso \r\nPlease share with complete stand alone code such that we can replicate the issue faced, or if possible a colab gist with error and tf version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43499, "title": "eigen download link points to google storage instead of bitbucket", "body": "This is my first time doing a pull request from a forked repo... apologies in advance if I did it wrong.", "comments": ["Question: Do you want this to go on the 1.15 branch? We are only updating `rX.Y` branches when we do a patch release for the `X.Y` release and we only do these releases on security vulnerabilities.\r\n\r\nIt is recommended to do the fix in master and then cherry-pick it to the old branches, as needed.", "> Question: Do you want this to go on the 1.15 branch? We are only updating `rX.Y` branches when we do a patch release for the `X.Y` release and we only do these releases on security vulnerabilities.\r\n> \r\n> It is recommended to do the fix in master and then cherry-pick it to the old branches, as needed.\r\n\r\nIt's my understanding that the master branch pulls a different version of Eigen, and the link is correct in master.  I suggest that this get pushed only into branch r1.15\r\n\r\nI'm new to this process, so I will defer to your best judgement.", "Perfect. Then we will merge this when we do a new patch release for 1.15"]}, {"number": 43498, "title": "ValueError: Unknown layer when i am loading of the model using tf.keras.models.load_model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow version (use command below): Latest\r\n\r\nHi,\r\n\r\nI am facing a model loading issue using  `tf.keras.models.load_model`.\r\n\r\nI saved custom keras model name- CustomModel\r\n\r\n`model.save('model.h5')`\r\n\r\nand then I try to load\r\n1. \r\n`new_model = keras.models.load_model('model.h5', custom_objects={'CustomModel': CustomModel})`\r\n\r\n**Error**\r\n\r\n> usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in class_and_config_for_serialized_keras_object(config, module_objects, custom_objects, printable_module_name)\r\n> 294 cls = get_registered_object(class_name, custom_objects, module_objects)\r\n> 295 if cls is None:\r\n> --> 296 raise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n> 297\r\n> 298 cls_config = config['config']\r\n> \r\n> ValueError: Unknown layer: Mean\r\n\r\n2. \r\n`new_model = keras.models.load_model('model.h5', custom_objects={'CustomModel': CustomModel, 'Mean':keras.metrics.Mean})`\r\n\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py in load_weights_from_hdf5_group(f, layers)\r\n> 684 'containing ' + str(len(layer_names)) +\r\n> 685 ' layers into a model with ' + str(len(filtered_layers)) +\r\n> --> 686 ' layers.')\r\n> 687\r\n> 688 # We batch weight value assignments in a single backend call\r\n> \r\n> ValueError: You are trying to load a weight file containing 2 layers into a model with 3 layers.\r\n\r\n\r\nI made a dummy notebook. https://colab.research.google.com/drive/1GH-WxghmkhgLSO2vVcSKgQ4UI3WX6GKC?usp=sharing\r\n\r\nIt looks like a bug in tf.keras.models.load_model.", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/ab99e61f56c7aa3e0c182a2c40e2d20d/43498.ipynb). Thanks!", "@Ankur3107 It can be a good idea to save and load model using `tf` format since you have custom objects in your model. \r\n```python\r\nmodel.save('model.tf')\r\nnew_model = tf.keras.models.load_model('model.tf')#, custom_objects={'CustomModel': CustomModel})\r\n```\r\nIn `tf` format you need not pass a `custom_objects` arg as well (TF:23).\r\n\r\nFor saving models with custom objects, subclassed models in `h5` format see https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43498\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43498\">No</a>\n", "Hi @ymodak \r\n\r\nYes I have tried .tf format and it worked. I have shared because it was not working with .h5 because I feel this is a bug. \r\n\r\nNow is this working with h5 or this is not a bug?", "To save custom objects to HDF5 you need to define `get_config` method.\r\nSee https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects", "> To save custom objects to HDF5 you need to define `get_config` method.\r\n> See https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects\r\n\r\nHi, I have get_config in my custom layer, but still get the same error. "]}, {"number": 43496, "title": "[Keras] Loading a model with custom layers in savedmodel format is very slow", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary pip\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nI'm saving a keras model in savedmodel and in h5. While in H5 the loading takes 3 seconds it can take up to 15 seconds in savedmodel format. I also noticed that reloading the keras model as a savedmodel instead of keras is faster (in our private use case 6seconds against 15 seconds with keras).\r\n\r\nWe also noticed that the loading time depends on the platform and can vary given the number of cores available. \r\n\r\n**Describe the expected behavior**\r\nI would expect a faster loading for both tf.keras.models.load_model('tf') and tf.saved_model.load. \r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/tanguycdls/4ef12115456f788c61771ab4fef5fdbb/untitled.ipynb\r\nThe model is:\r\n\r\n````\r\ninps = []\r\nfor name in range(30):\r\n  inps.append(Input(ragged=True, shape=(None,), name=str(name)))\r\nclass SumRagged(tf.keras.layers.Layer):\r\n    def call(self, input):\r\n        return tf.reduce_sum(input, axis=1)\r\nout_summed = [SumRagged()(Embedding(10, 2)(inp)) for inp in inps]\r\nconcat = Dense(1)(Concatenate()(out_summed))\r\nmodel = tf.keras.models.Model(inps, concat)\r\n````\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nIn the current benchmarks:\r\n\r\n### saving:\r\n\r\n- savedmodel : 9.98 s per loop\r\n- H5: 55.7 ms per loop\r\n\r\n### Loading:\r\n- savedmodel (keras api): 5 seconds\r\n- savedmodel: 3 seconds\r\n- H5: 56ms.\r\n", "comments": ["I have tried in colab with TF versions 2.3, nightly version(`2.4.0-dev20200923`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/41c2b484af80e33880a96faefd8625e4/untitled389.ipynb). Thanks!", "@tanguycdls Agree with you that save_format=`tf` is slower than `h5` format. However, there are some key differences between these two formats. You can check [save_and_load_model]() for more details.\r\n\r\nOne of the key difference is listed in the above resource as \r\n\r\n> The key difference between HDF5 and SavedModel is that HDF5 uses object configs to save the model architecture, while SavedModel saves the execution graph. Thus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the original code.\r\n\r\nAs far as my personal experience, `tf` format has better functionality over `h5` format. Thanks!", "Hi ! Thanks a lot for the details @jvishnuvardhan ! \r\n\r\nI agree with you the usage of the tf saved model is way better than the h5 features especially with custom layers for Keras: we actually changed our code base for that specific feature (no code requirements while reloading the model) ! \r\n\r\nHowever we're trying to optimize our timings and the cost of reloading the model in Savedmodel seems really high so I wanted to know if we can hope to have it faster in the next Tensorflow releases ? \r\n\r\nI especially find odd the differences between reloading a savedmodel keras with the keras api (tf.keras.models.load_model) and tf.savedmodel format directly: in the example above it took two additional seconds (3 (saved_model)-> 5 seconds (tf.keras.load_model)) but in my (private) model use case the diff is 5 seconds --> 15 seconds. \r\n\r\nThanks again !", "@tanguycdls `tf.saved_model.load` and `tf.keras.model.load_model` sound similar but has one critical difference. \r\n\r\nThe object returned by tf.saved_model.load is not a Keras object (i.e. doesn't have .fit, .predict, .summary, etc. methods) whereas the object returned by `tf.keras.model.load_model` is a Keras object. \r\n\r\n```\r\nloaded_model_keras = tf.keras.models.load_model('/tmp/test', compile=False)\r\nloaded_model_keras.summary()\r\n```\r\n\r\nYou can take `loaded_model_keras` and keep training it by running .fit. I didn't go through the source code but looks like saving this Keras object that has additional functionality takes little more time. Hope it helps. Thanks!\r\n", "Hi Thank you for the precision ! I think my issue might be related to https://github.com/tensorflow/tensorflow/issues/43263"]}, {"number": 43495, "title": "Cannot convert CenterNet+KeyPoints Model Zoo tf2 models to tflite", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary): binary (tf-nightly)\r\n- TensorFlow version (use command below): 2.4.0-dev20200907\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 11.0/8.0\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I try to convert a model from TensorFlow 2 Detection Model Zoo into a tflite version, I get some error.\r\n\r\n[EDIT] This is only the case for models using CenterNet AND KeyPoints. All other models are OK\r\n\r\n**Describe the expected behavior**\r\n\r\nConvertion is successfull\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nDownload \"CenterNet Resnet50 V1 FPN Keypoints 512x512\" from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\r\n\r\nUncompress the archive\r\n\r\nRun \"tflite_convert --saved_model_dir=centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8/saved_model --output_file=model.tflite\"\r\n\r\nI got the following error:\r\n\r\nloc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): error: requires element_shape to be 1D tensor during TF Lite transformation pass\r\nloc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\nTraceback (most recent call last):\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): requires element_shape to be 1D tensor during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): see current operation: %456 = \"tf.TensorListReserve\"(%67, %77) {device = \"\"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>\r\n<unknown>:0: error: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): see current operation: %456 = \"tf.TensorListReserve\"(%67, %77) {device = \"\"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/biroute/.local/bin/tflite_convert\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 640, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 623, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/tflite_convert.py\", line 239, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 726, in convert\r\n    output_tensors)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 643, in convert\r\n    **converter_kwargs)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 573, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/biroute/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): requires element_shape to be 1D tensor during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): see current operation: %456 = \"tf.TensorListReserve\"(%67, %77) {device = \"\"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>\r\n<unknown>:0: error: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: note: loc(callsite(callsite(\"map/TensorArrayV2_2@__inference___call___13919\" at \"StatefulPartitionedCall@__inference_signature_wrapper_15800\") at \"StatefulPartitionedCall\")): see current operation: %456 = \"tf.TensorListReserve\"(%67, %77) {device = \"\"} : (tensor<i32>, tensor<i32>) -> tensor<!tf.variant<tensor<*xf32>>>\r\n", "comments": ["Sorry for encountering this issue. Current TFLite MLIR converter is able to handle 1-D TensorLists only. After loading saved model, you might need to specify input shapes of the model to make the TensorLists proper shapes in Python.", "@abattery Thanks for your quick answer.\r\n\r\nI tried the following code, and it gives exactly the same previous error (which is normal):\r\n\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    tflite_model = converter.convert()\r\n    open(\"model.tflite\", \"wb\").write(tflite_model)\r\n\r\nthen I modified it as you requested, but I now have a different error:\r\n\r\n    saved_model = tf.saved_model.load(\"centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8/saved_model\")\r\n    concrete_func = saved_model.signatures[\"serving_default\"]\r\n    concrete_func.inputs[0].set_shape([1, 512, 512, 3])\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n    tflite_model = converter.convert()\r\n    open(\"model.tflite\", \"wb\").write(tflite_model)\r\n\r\n020-09-24 09:36:47.818409: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n    2020-09-24 09:36:47.818448: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\n    loc(\"Func/StatefulPartitionedCall/input/_0\"): error: requires all operands and results to have compatible element types\r\n    Traceback (most recent call last):\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 196, in toco_convert_protos\r\n        model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py\", line 32, in wrapped_toco_convert\r\n        return _pywrap_toco_api.TocoConvert(\r\n    Exception: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n    <unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x512x512x3x!tf.quint8>) -> tensor<1x512x512x3xui8>\r\n\r\n\r\n    During handling of the above exception, another exception occurred:\r\n\r\n    Traceback (most recent call last):\r\n      File \"export_model.py\", line 13, in <module>\r\n        main()\r\n      File \"export_model.py\", line 9, in main\r\n        tflite_model = converter.convert()\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 1135, in convert\r\n        return super(TFLiteConverterV2, self).convert()\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 960, in convert\r\n        return super(TFLiteFrozenGraphConverterV2,\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 641, in convert\r\n        result = _toco_convert_impl(\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 594, in toco_convert_impl\r\n        data = toco_convert_protos(\r\n      File \"/usr/local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n        raise ConverterError(str(e))\r\n    tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n    <unknown>:0: note: loc(\"Func/StatefulPartitionedCall/input/_0\"): see current operation: %1 = \"tf.Identity\"(%arg0) {device = \"\"} : (tensor<1x512x512x3x!tf.quint8>) -> tensor<1x512x512x3xui8>\r\n\r\nI do understand that the tensor has not the expected type (quint8 vs ui8) but I didn't find how to modify it...\r\n\r\nThanks for your help,", "still crashing with tf-nightly-2.4.0.dev20200930\r\n", "Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/09d4c59e95cf33664294291c9d9e3647/43495.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/4305e4aacd3fef25bef156da96c0617b/43495-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Hi,\r\n\r\nI have made a simple test: I have downloaded every models in \"TensorFlow 2 Detection Model Zoo\" and tried to convert them with tflite_convert.\r\n\r\nIt appears that only 4 models failed to convert:\r\n\r\ncenternet_hg104_1024x1024_kpts_coco17_tpu-32\r\ncenternet_hg104_512x512_kpts_coco17_tpu-32\r\ncenternet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8\r\ncenternet_resnet50_v2_512x512_kpts_coco17_tpu-8\r\n\r\nThe common point in those models is that they are the ones using both centernet AND keypoints.\r\n\r\nI hope it will give you some clues to investigate...\r\n", "Hi TensorFlow Team,\r\n\r\nIt seems that the bug was on the side of object_detection library.\r\n\r\nIt has been fixed in commit baacb20:\r\nhttps://github.com/tensorflow/models/commit/baacb20d15066935e4a23c09b1c1a6843331172f#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f\r\n\r\nI have trained and exported successfully a centernet with keypoints model with the new object_detection library and it's now ok.\r\n\r\nAs long as the models in Model Zoo will be trained again with new object_detection framework, they will be ok too!\r\n\r\nThanks for your support", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43495\">No</a>\n", "> Hi TensorFlow Team,\r\n> \r\n> It seems that the bug was on the side of object_detection library.\r\n> \r\n> It has been fixed in commit baacb20:\r\n> [tensorflow/models@baacb20#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f](https://github.com/tensorflow/models/commit/baacb20d15066935e4a23c09b1c1a6843331172f#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f)\r\n> \r\n> I have trained and exported successfully a centernet with keypoints model with the new object_detection library and it's now ok.\r\n> \r\n> As long as the models in Model Zoo will be trained again with new object_detection framework, they will be ok too!\r\n> \r\n> Thanks for your support\r\n\r\nHey, can you please tell me how to convert this model to tflite because I still can not do it ? ", "> Hi TensorFlow Team,\r\n> \r\n> It seems that the bug was on the side of object_detection library.\r\n> \r\n> It has been fixed in commit baacb20:\r\n> [tensorflow/models@baacb20#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f](https://github.com/tensorflow/models/commit/baacb20d15066935e4a23c09b1c1a6843331172f#diff-ee421de3daf8a640c16b7f6f5a5d3399d28ded3a5c073b9ee10e5e2a88b7e94f)\r\n> \r\n> I have trained and exported successfully a centernet with keypoints model with the new object_detection library and it's now ok.\r\n> \r\n> As long as the models in Model Zoo will be trained again with new object_detection framework, they will be ok too!\r\n> \r\n> Thanks for your support\r\n\r\nHello @twkx,\r\n\r\nI have the same issue as you, can you give some more detailed information on how you fixed this issue?\r\nI have checked out to the commit \"baacb20\" but it has not fixed my issue!\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"export_tflite_tf2.py\", line 13, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1135, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 961, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 644, in convert\r\n    **converter_kwargs)\r\n  File \"/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 613, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/ardit/repos/solaborate/Solaborate.ML/solaborate_packages/model_training/tf_env/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 216, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Func/StatefulPartitionedCall/input/_0\"): requires all operands and results to have compatible element types\r\n```\r\n\r\n", "Hi Everyone,\r\n\r\nWe have a change in progress to fix this from our side too.\r\nTo explain the problem,\r\nYour model has input with TF type QUINT8, TFLite doesn't understands this type, since this type lacks all quantization information that TFLite relies on.\r\n\r\nSo instead, if you can update your model to use UINT8 instead then it should work.\r\n\r\nWe are working to include this change inside the converter if we saw it, so we can handle these cases without users need to change the model.\r\n\r\nThanks", "The model \"CenterNet Resnet50 V1 FPN Keypoints 512x512\" from [tf2_detection_zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) has a flex op: 'GatherV2' and that's the reason it cannot be converted to a TFLite model (without setting the right flags).\r\n\r\nHowever, the error message is a bit misleading. The input tensor is \"uint8\" and not \"quint8\". The actual error, i.e, the presence of a Flex or Select TF op shows up if we update the model's input shape, save it, and then use this model.\r\n\r\n**1. UNSUCCESSFUL CONVERSION**\r\n```\r\n# RUN ONCE (Update the model)\r\nmodel = tf.saved_model.load(model_dir)\r\nconcrete_func = model.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([1, 512, 512, 3])\r\ntf.saved_model.save(model, model_dir)\r\n# RUN REPEATEDLY (Convert the model)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_dir, signature_keys=['serving_default'])\r\ntflite_model = converter.convert()\r\n```\r\nERROR:\r\n```\r\n.\r\n.\r\ngoogle3.third_party.tensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(\"GatherV2@__inference___call___40671\" at \"StatefulPartitionedCall@__inference_restored_function_body_69773\") at \"StatefulPartitionedCall@__inference_signature_wrapper_70526\") at \"StatefulPartitionedCall\")): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(callsite(\"GatherV2_1@__inference___call___40671\" at \"StatefulPartitionedCall@__inference_restored_function_body_69773\") at \"StatefulPartitionedCall@__inference_signature_wrapper_70526\") at \"StatefulPartitionedCall\")): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(\"cond/GatherV2_3@__inference_cond_true_13295_36597\"): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"cond/GatherV2_4@__inference_cond_true_13295_36597\"): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"cond/GatherV2_5@__inference_cond_true_13295_36597\"): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"cond/GatherV2_7@__inference_cond_true_13295_36597\"): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: loc(\"cond/GatherV2_6@__inference_cond_true_13295_36597\"): 'tf.GatherV2' op is neither a custom op nor a flex op\r\n<unknown>:0: error: failed while converting: 'main': \r\nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \r\nTF Select ops: GatherV2\r\nDetails:\r\n\ttf.GatherV2 {batch_dims = 1 : i64, device = \"\"}\r\n\ttf.GatherV2 {batch_dims = 2 : i64, device = \"\"}\r\n```\r\n\r\n\r\n\r\n**2. SUCCESSFUL CONVERSION**\r\nTo successfully convert this model, run the following code by setting the correct converter flags :\r\n\r\n```\r\n# RUN ONCE (Update the model)\r\nmodel = tf.saved_model.load(model_dir)\r\nconcrete_func = model.signatures['serving_default']\r\nconcrete_func.inputs[0].set_shape([1, 512, 512, 3])\r\ntf.saved_model.save(model, model_dir)\r\n# RUN REPEATEDLY (Convert the model)\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_dir, signature_keys=['serving_default'])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```", "@MeghnaNatraj can you please specify which kind of saved model you use here:\r\n \r\n> model = tf.saved_model.load(model_dir)\r\n> converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, signature_keys=['serving_default'])\r\n\r\nIve used both .pb model that were created by exporter_main_v2.py and export_tflite_graph_tf2.py, in the first case the model has wrong input shape of 'shape': array([1, 1, 1, 3], dtype=int32). And for the second exported I got huge accuracy drop, the detection scores are about 0.0001 each.\r\n\r\nWhile the inference graph created by exporter_main_v2.py and runs via tf works satisfactorily. Ive also tryied model quantization steps, no matter what I got low scores while using centernet architectures converted to the tflite.\r\n\r\nMaybe there is a wrong way of how Im using inference after the tflite conversion, but it works ok when I run inference on the efficentdent tflite models.", "@kirienkomaxym  The files you are referring to are unrelated to this issue. With regards to the [this comment above](https://github.com/tensorflow/tensorflow/issues/43495#issuecomment-764702064) I used model \"CenterNet Resnet50 V1 FPN Keypoints 512x512\" from [tf2_detection_zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). The direct link the models is [CenterNet Resnet50 V1 FPN Keypoints 512x512](http://download.tensorflow.org/models/object_detection/tf2/20200711/centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8.tar.gz)\r\n\r\nThe `model_dir` is initialized to the path to the downloaded model. eg: `model_dir=/path/to/CenterNet_Resnet50_V1_FPN_Keypoints_512x512_model_dir` after it has been unzipped/uncompressed.\r\n\r\nYou can update the input shape as shown in [this comment above](https://github.com/tensorflow/tensorflow/issues/43495#issuecomment-764702064). You need to verify if the inference using the TF model, TF float model are accurate before proceeding to [post training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization).", "UPDATE:\r\n\r\nWith the latest TF Nightly build, conversion works directly without having to update the model\r\n\r\n```\r\n# Install TF Nightly (latest TF version which is ongoing development)\r\n# TODO: Restart your runtime after running the following command\r\n!pip install tf-nightly\r\n\r\n# Import commands\r\nimport tensorflow as tf\r\n\r\n# Convert the TF model to TF Lite model\r\n# TODO: Update the path to the model as given below\r\nmodel_dir = \"centernet_resnet50_v1_fpn_512x512_kpts_coco17_tpu-8/saved_model\"\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_dir, signature_keys=['serving_default'])\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n*Note: The model 'tf.ConcatV2' op which is not available in TFLite but it is defined in TF. See instructions: https://www.tensorflow.org/lite/guide/ops_select, which provides details on how you can run inference on this model.*  ", "@MeghnaNatraj I have followed the steps as stated above. Unable to set the tensor, i am getting message ValueError: Cannot set tensor: Dimension mismatch. Got 562 but expected 1 for dimension 1 of input 0., while using the Interpreter\r\n", "ON inspecting input details following is being displayed\r\nprint(input_details)\r\n\r\n[{'name': 'serving_default_input_tensor:0', 'index': 0, 'shape': array([1, 1, 1, 3], dtype=int32), 'shape_signature': array([ 1, -1, -1,  3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\nprint(output_details)\r\n[{'name': 'StatefulPartitionedCall:7', 'index': 46005, 'shape': array([    1, 49104,     1], dtype=int32), 'shape_signature': array([    1, 49104,     1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:4', 'index': 46120, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:6', 'index': 45988, 'shape': array([    1, 49104,     4], dtype=int32), 'shape_signature': array([    1, 49104,     4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:5', 'index': 46103, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 46191, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 46173, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 46156, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 46138, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]", "Following is the link to the google colab file https://colab.research.google.com/drive/1dpJ05nLl-stqu8lC9P4mwoIEbBmyxFBU#scrollTo=7jf7RQ98iY4i\r\n\r\nCan anyone guide me in resolving the issue...", "@VirVisha is it possible to provide access to the [colab](https://colab.research.google.com/drive/1dpJ05nLl-stqu8lC9P4mwoIEbBmyxFBU#scrollTo=7jf7RQ98iY4i)? (enable view permission for everyone)", "Done the same the link is https://drive.google.com/file/d/1dpJ05nLl-stqu8lC9P4mwoIEbBmyxFBU/view?usp=sharing", "Suggestion: it would be better to file a new post instead of discussing on the closed post in order to keep each issue focused. ", "I have done the same at this link https://github.com/tensorflow/tensorflow/issues/50301", "Then, let's move on the discussion at the new post #50301."]}, {"number": 43494, "title": "Bug in tf.math.rsqrt", "body": "Hello!\r\n\r\nOn TF 2.3, about the function `tf.math.rsqrt(x, name=None)` the documentation says:\r\n\r\n>x | A\u00a0tf.Tensor. Must be one of the following types:\u00a0bfloat16,\u00a0half,\u00a0float32,\u00a0float64.\u00a0int32\r\n\r\nWhen run with `int32` type:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.constant([2, 0, -2])\r\ntf.math.rsqrt(x)\r\n```\r\n\r\nI get the following error:\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\r\n        ; NodeDef: {{node Rsqrt}}; Op<name=Rsqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Rsqrt]\r\n```\r\n\r\nThe list raised by the error and the list given in the documentation are not the same. Then either it is an error in the documentation or a bug in the implementation of the function.\r\n\r\nThanks.\r\n\r\n", "comments": ["/cc @av8ramit for https://github.com/tensorflow/tensorflow/commit/9244e2a75f84fa7033456608b79ca8b5fca42202", "@yashk2810 any idea on why this is happening?", "@av8ramit It is currently: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/compat/ops_history_v2/Rsqrt.pbtxt \r\n\r\nWhat was the origin of your `int32` comment in the mentioned commit?", "If I recall correctly it was autogenerated following a process outlined by @yashk2810 . It appears to me to be a one-off typo, but regardless I wanted to loop him in in case this is a larger issue. \r\n\r\n@jplu thanks for pointing this out, I have a fix landing shortly that will remove the extraneous int32.", "https://github.com/tensorflow/tensorflow/commit/ccb76501431551f269083d78969097f9dabc8789 with the fix to the documentation.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43494\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43494\">No</a>\n"]}, {"number": 43493, "title": "TensorFlow build is failing on Bazel CI (Release Bazel) for MacOS and Windows", "body": "https://buildkite.com/bazel/tensorflow/builds/5772#d0cb252a-2fab-4c9c-aea3-a1b9e914ea15\r\n\r\nWindows seems to have been failing since Sep 18 2020\r\nMacOS seems to have been failing since Sep 19 2020\r\n\r\n", "comments": ["Sorry, those aren't our builds -- the Bazel team maintains them. Can you replicate the failure in our build-from-source instructions?", "I think this would be fixed by #43474 ", "Fixed in 74027b89402720e598d5512dddf3ea7737df7ffe", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43493\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43493\">No</a>\n"]}]