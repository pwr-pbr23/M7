[{"number": 6391, "title": "tf.sparse_tensor_to_dense does not have a gradient", "body": "Differentiating `tf.sparse_tensor_to_dense` and also `tf.sparse_to_dense` returns `None`. At the same time, a hacky way of converting a sparse tensor to a dense tensor by using `tf.sparse_add` works. Please see the code below:\r\n\r\n```python \r\nindices = tf.placeholder(tf.int64, (None, 2))\r\nvalues = tf.placeholder(tf.float32, (None,))\r\nsparse_tensor = tf.SparseTensor(indices, values, (5, 7))\r\ndense_tensor1 = tf.sparse_tensor_to_dense(sparse_tensor)\r\ndense_tensor2 = tf.sparse_add(tf.zeros((5, 7)), sparse_tensor)\r\nsum1 = tf.reduce_sum(dense_tensor1)\r\nsum2 = tf.reduce_sum(dense_tensor2)\r\nprint tf.gradients(sum1, values)\r\n>>> [None]\r\nprint tf.gradients(sum2, values)\r\n>>> [tf.Tensor ...]\r\n```\r\n\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nDidn't find anything.\r\n\r\n### Environment info\r\n\r\n1. A link to the pip package you installed:\r\n\r\npip install --user https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`\r\n\r\n0.11.0rc1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nPlease see the code above. \r\n", "comments": ["@ebrevdo, @concretevitamin , please take a look at this issue.", "This uses sparse to dense op, which is mostly deprecated in favor of scatter_nd op.  That op has a gradient.  We should change the sparse to dense Python wrapper to call scatter_nd.", "Zongheng, if you have time to change the sparse to dense wrapper.  Or simister if he is around can do it.  I'm away till late December.", "Since a workaround exists, we'll gladly accept a patch :)", "Meanwhile, you guys could at least print a warning, raise an exception, deprecate this API, or add a line to the documentation. I do like Tensorflow, and I think you are doing a great job, but for people outside Google it is often very unclear which ops are \"mostly deprecated\" in favor of which. ", "I second that printing a warning should be in order for this. I only found a bug in my code relating to this issue by looking at the graph in tensorboard and saw that one of my variables was a different colour...", "i have a partial fix on the way\n\nOn Apr 19, 2017 7:07 AM, \"derluke\" <notifications@github.com> wrote:\n\n> I second that printing a warning should be in order for this. I only found\n> a bug in my code relating to this issue by looking at the graph in\n> tensorboard and saw that one of my variables was a different colour...\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6391#issuecomment-295282517>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyJvZgtdkA45LgYPkDmk3_5a8U_Qks5rxhU3gaJpZM4LQSVP>\n> .\n>\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "BTW this op now has a gradient.\n\nOn Jun 16, 2017 2:45 PM, \"Olivia\" <notifications@github.com> wrote:\n\n> Closed #6391 <https://github.com/tensorflow/tensorflow/issues/6391>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6391#event-1127563146>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7j8NV_YNQw3c-PZJOnLAmmWm7WSks5sEvczgaJpZM4LQSVP>\n> .\n>\n", "The example provided above still does return None -- added `sparse_tensor_dense_matmul` and `sparse_to_dense` to the mix because in my use case I can (with some overhead) use each of those functions.\r\n\r\n## Executed:\r\n\r\n```Python\r\n$ docker run -it tensorflow/tensorflow:1.4.0-py3 python3\r\nPython 3.5.2 (default, Sep 14 2017, 22:51:06) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> indices = tf.placeholder(tf.int64, (None, 2))\r\n>>> values = tf.placeholder(tf.float32, (None,))\r\n>>> sparse_tensor = tf.SparseTensor(indices, values, (5, 7))\r\n>>> dense_tensor1 = tf.sparse_tensor_to_dense(sparse_tensor)\r\n>>> dense_tensor2 = tf.sparse_to_dense(indices, (5, 7), values)\r\n>>> dense_tensor3 = tf.sparse_add(tf.zeros((5, 7)), sparse_tensor)\r\n>>> dense_tensor4 = tf.sparse_tensor_dense_matmul(sparse_tensor, tf.zeros((7, 5)))\r\n>>> sum1 = tf.reduce_sum(dense_tensor1)\r\n>>> sum2 = tf.reduce_sum(dense_tensor2)\r\n>>> sum3 = tf.reduce_sum(dense_tensor3)\r\n>>> sum4 = tf.reduce_sum(dense_tensor4)\r\n>>> print(tf.gradients(sum1, values))\r\n[None]\r\n>>> print(tf.gradients(sum2, values))\r\n[None]\r\n>>> print(tf.gradients(sum3, values))\r\n[<tf.Tensor 'gradients_2/SparseTensorDenseAdd_grad/GatherNd:0' shape=(?,) dtype=float32>]\r\n>>> print(tf.gradients(sum4, values))\r\n[<tf.Tensor 'gradients_3/SparseTensorDenseMatMul/SparseTensorDenseMatMul_grad/Sum:0' shape=(?,) dtype=float32>]\r\n```\r\n\r\n## Ready to copy & run:\r\n\r\n```Python\r\nimport tensorflow as tf\r\nindices = tf.placeholder(tf.int64, (None, 2))\r\nvalues = tf.placeholder(tf.float32, (None,))\r\nsparse_tensor = tf.SparseTensor(indices, values, (5, 7))\r\ndense_tensor1 = tf.sparse_tensor_to_dense(sparse_tensor)\r\ndense_tensor2 = tf.sparse_to_dense(indices, (5, 7), values)\r\ndense_tensor3 = tf.sparse_add(tf.zeros((5, 7)), sparse_tensor)\r\ndense_tensor4 = tf.sparse_tensor_dense_matmul(sparse_tensor, tf.zeros((7, 5)))\r\nsum1 = tf.reduce_sum(dense_tensor1)\r\nsum2 = tf.reduce_sum(dense_tensor2)\r\nsum3 = tf.reduce_sum(dense_tensor3)\r\nsum4 = tf.reduce_sum(dense_tensor4)\r\nprint(tf.gradients(sum1, values))\r\nprint(tf.gradients(sum2, values))\r\nprint(tf.gradients(sum3, values))\r\nprint(tf.gradients(sum4, values))\r\n```", "Wait, how did I just unassign @concretevitamin? I don't even have rights to assign anybody \ud83d\ude15\r\n\r\n![screen shot 2017-11-14 at 15 28 10](https://user-images.githubusercontent.com/3015996/32784971-97d9471c-c950-11e7-89a8-0f592c58904b.png)\r\n", "That is supremely odd.", "I'll chalk that up to a bug in github.", "is this fixed in 1.12.0?", "It should have been fixed a year ago. Is that incorrect?", "@martinwicke But I did not the function call of `scatter_nd_add` in https://github.com/tensorflow/tensorflow/blob/v1.12.0/tensorflow/python/ops/sparse_ops.py. Correct me if I am wrong. Thanks", "It is still not fixed...", "Here is the solution.\r\n\r\n1) open  YOURPATH/tensorflow/python/ops/sparse_grad.py\r\n2) Remove line 33 ops.NotDifferentiable(\"SparsetoDense\")\r\n3) At the end, append\r\n\r\n@ops.RegisterGradient(\"SparseToDense\")\r\ndef _SparseToDenseGrad(op, grad):\r\n  sparse_indices, output_shape, _, _ = op.inputs\r\n\r\n  sparse_values_grad = array_ops.gather_nd(grad, sparse_indices)\r\n  default_value_grad = math_ops.reduce_sum(grad) - math_ops.reduce_sum(\r\n      sparse_values_grad)\r\n  return [\r\n      array_ops.zeros_like(sparse_indices),\r\n      array_ops.zeros_like(output_shape), sparse_values_grad, default_value_grad\r\n  ]\r\n\r\nThis implementation is from TF2.0 version.\r\n\r\n\r\n"]}, {"number": 6390, "title": "Extract attention matrix during decoding", "body": "It seems like it's not possible to access the attention weights during decoding since the attention() method in seq2seq_model isn't called during the forward step. Otherwise, is there another feature to allow for this, or was this intended?", "comments": ["You might try asking this question on StackOverflow. Others users may have found a way to do it there. If you cannot find a way to do it after asking there, please reply here, and we will open it as a feature request. Thanks!"]}, {"number": 6389, "title": "[TensorFlow.org] Anchor Tag Incorrect Reference", "body": "Nothing really serious but I noticed the link on `see Pip installation for Windows` on getting started doc body is using an anchor separated with dashes `#pip-installation-on-windows`  while the corresponding `id` uses underscores `#pip_installation_on_windows`.\r\n\r\nFor instance, the body anchor (which works on github) \r\n`#pip-installation-on-windows` \r\nand the right side nav-bar `Index` link, working normally, with underscores\r\n`#pip_installation_on_windows`\r\n\r\nI was going to submit a PR but changing to the latter breaks on github. ", "comments": ["That should be picked up by our formatting tool.  I'll take a look. ", "This is now fixed.  Thanks for pointing it out.", "@wolffg Awesome. No problem!"]}, {"number": 6388, "title": "CTC TimeOrder argument inconsistency", "body": "For example, here's signature for dynamicRNN:\r\n```\r\ntf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)\r\n```\r\n\r\nHere is for ctc_loss:\r\n```\r\ntf.nn.ctc_loss(labels, inputs, sequence_length, preprocess_collapse_repeated=False, ctc_merge_repeated=True, time_major=True)\r\n\r\n```\r\n\r\nThere is the difference between default behavior for **time_major**? (Rest of CTC module is similar in behaving time first.) This is unintuitive and should be changed to batch first in whole CTC module by default to be in consistency with rest of tensorflow library. (Or make other function time first, as long as there's consensus). Conv1D similarly puts batch dimension first, than time one. (Also did other of labels and inputs arguments have changed in ctc_loss between 0.11 and 0.12?)\r\n\r\nIn enforcing consistency developers using this library have less on their mental workload and enhancing productivity.", "comments": ["@ebrevdo, could you comment on this issue?", "Time major is more efficient with rnns, but dynamic_rnn was made batch major for consistency with the rest of tf.  Since most users of CTC are experts, and are clearly going to be combining it with rnns, we are encouraging users to use time major here as well.  If you're using dynamic_rnn, I encourage you to use the flag time_major=True for efficiency.", "The order of arguments may have changed. Andrew, were you the one who made that change?  See Martin wickes recent announcement on the tf discussion mailing list.", "Thanks for the tip, I'll use time major in RNN as well."]}, {"number": 6387, "title": "Sparsemax", "body": "The sparsemax op is an alternative to the softmax op, that allows the\r\noutput to be sparse (zero properbility) while stil sharing many\r\nmathematical properties with softmax.\r\n\r\nThe cross entropy loss doesn't work with sparsemax as log(0) is not\r\ndefined, thus there is also a sparsemax loss function. This loss\r\nfunction have a gradient equivalent to that of cross entropy when\r\nusing softmax.\r\n\r\nOriginal sparsemax article: https://arxiv.org/abs/1602.02068\r\n\r\n----\r\n\r\nI had some issues with getting the numerical precision good enough for `assertAllCloseAccordingToType`. subtracting `mean(logits)` helped a bit but not enough, thus I have expanded the assert method such that the tolerance can be specified.\r\n\r\n----\r\n\r\n_This code was developed by me (@andreasmadsen), @FrederikWR and @MarcoDalFarra. The original code can be found in https://github.com/AndreasMadsen/course-02456-sparsemax. The project was supervised by @alrojo._\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for the pull request! It's very well written.\r\n\r\nFor now, we think this should live outside of the official repo. We can add it later if it proves to be useful to the wider community.", "We where originally informed by @alrojo, who had talked to some TensorFlow members, that this was of interrest. It must have been a misunderstanding.", "Ah, do you mind mentioning who you talked to? Perhaps they can review.", "Can one of the admins verify this patch?", "Hi guys. I talked to alrojo and I'm happy to try to move this forward.\r\n\r\nOne main thing that came up is how we could do it without the extra kernels (which is a large burden to support). Here is a question to you guys:\r\n\r\nIt seems like a lot of the work is because you need sorting. But tf.nn.top_k can be used to sort. Did you consider using it? Could you try it to simplify the code?", "> It seems like a lot of the work is because you need sorting. But tf.nn.top_k can be used to sort. Did you consider using it? Could you try it to simplify the code?\r\n\r\nI think this is true, there are two issues as far as I'm aware:\r\n\r\n* `tf.nn.top_k` has no GPU implementation.\r\n* I don't know how to define a gradient on a composite operator (not a kernel, but an graph).\r\n\r\nOn the subject of simplifying code: I don't think subtracting the mean is particular important, I helps only very little in some situations. Unlike softmax, sparsemax mostly consists of additions and some multiplications (no `exp`). So this part could likely be removed without noticeable issues. In our experiments, we did in fact not include it.", "I don't think we need to worry about top_k not having a GPU kernel-- it's a core op, so adding kernels to it is more straightforward than adding (and supporting and maintaining) a few new kernels in contrib.\r\n\r\nAs for gradients, it can be done with Defun and grad_func, see for example https://github.com/tensorflow/tensorflow/issues/4661. But I'm curious why it is needed at all?", "> I don't think we need to worry about top_k not having a GPU kernel-- it's a core op, so adding kernels to it is more straightforward than adding (and supporting and maintaining) a few new kernels in contrib.\r\n\r\nOriginally we implemented sparsemax from a performance perspective, not from a maintenance perspective.\r\n\r\nI've replaced the kernels such the implementations just use core ops (see https://github.com/tensorflow/tensorflow/pull/6387/commits/ccf12a892d4a9b4f3166e5818374bf65e7a13195). One issue I couldn't solve was how to extract the cumsum values without depending on the batch size `.get_shape()[0]` (see https://github.com/tensorflow/tensorflow/pull/6387/commits/ccf12a892d4a9b4f3166e5818374bf65e7a13195#diff-e786beafb4ecbf8faeae13d5d3d75ea4R66).\r\n\r\n> As for gradients, it can be done with Defun and grad_func, see for example #4661. But I'm curious why it is needed at all?\r\n\r\nIt is not necessary as such, but the gradients of `sparsemax` and `sparsemax_loss` are very simple. I assume it will be more performant to express them directly, than just using backprop though a fairly complex graph. I tried to implement them using `Defun` as you sugested, unfortunately I get `ValueError: Cannot convert an unknown Dimension to a Tensor: ?` errors. Can you take a look at https://github.com/tensorflow/tensorflow/pull/6387/commits/ca605df0991d4de2f4f78c7381e9ed30d53548cf ?\r\n\r\n\r\n<details>\r\n  <summary>full error message</summary>\r\n\r\n```\r\n$ PYTHONPATH=tensorflow/_python_build python3 tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\r\n\r\n/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py:969: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\r\n  argspec = inspect.getargspec(func)\r\nEE.\r\n======================================================================\r\nERROR: testDouble (__main__.SparsemaxTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py\", line 491, in apply_op\r\n    preferred_dtype=default_dtype)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/constant_op.py\", line 212, in _dimension_tensor_conversion_function\r\n    raise ValueError(\"Cannot convert an unknown Dimension to a Tensor: %s\" % d)\r\nValueError: Cannot convert an unknown Dimension to a Tensor: ?\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 258, in testDouble\r\n    self._test_dtype('float64')\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 224, in _test_dtype\r\n    self._test_sparsemax_against_numpy(dtype, random, use_gpu=False)\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 83, in _test_sparsemax_against_numpy\r\n    tf_sparsemax_op, tf_sparsemax_out = self._tf_sparsemax(z, dtype, use_gpu)\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 74, in _tf_sparsemax\r\n    tf_sparsemax_op = sparsemax(z.astype(dtype))\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 885, in __call__\r\n    return self.instantiate(input_types)(*args, **kwargs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 860, in instantiate\r\n    _ = defined.name  # Fully instantiate the function definition.\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 608, in name\r\n    self._create_definition_if_needed()\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 659, in _create_definition_if_needed\r\n    outputs = self._func(*inputs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/contrib/sparsemax/python/ops/sparsemax.py\", line 90, in sparsemax\r\n    return fprop(logits)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 885, in __call__\r\n    return self.instantiate(input_types)(*args, **kwargs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 860, in instantiate\r\n    _ = defined.name  # Fully instantiate the function definition.\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 608, in name\r\n    self._create_definition_if_needed()\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 659, in _create_definition_if_needed\r\n    outputs = self._func(*inputs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/contrib/sparsemax/python/ops/sparsemax.py\", line 67, in fprop\r\n    z_sorted, _ = nn.top_k(z, k=dims)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/ops/nn_ops.py\", line 1946, in top_k\r\n    return gen_nn_ops._top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/ops/gen_nn_ops.py\", line 2464, in _top_kv2\r\n    name=name)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py\", line 504, in apply_op\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/constant_op.py\", line 212, in _dimension_tensor_conversion_function\r\n    raise ValueError(\"Cannot convert an unknown Dimension to a Tensor: %s\" % d)\r\nValueError: Cannot convert an unknown Dimension to a Tensor: ?\r\n\r\n======================================================================\r\nERROR: testFloat (__main__.SparsemaxTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py\", line 491, in apply_op\r\n    preferred_dtype=default_dtype)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/constant_op.py\", line 212, in _dimension_tensor_conversion_function\r\n    raise ValueError(\"Cannot convert an unknown Dimension to a Tensor: %s\" % d)\r\nValueError: Cannot convert an unknown Dimension to a Tensor: ?\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 255, in testFloat\r\n    self._test_dtype('float32')\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 224, in _test_dtype\r\n    self._test_sparsemax_against_numpy(dtype, random, use_gpu=False)\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 83, in _test_sparsemax_against_numpy\r\n    tf_sparsemax_op, tf_sparsemax_out = self._tf_sparsemax(z, dtype, use_gpu)\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 74, in _tf_sparsemax\r\n    tf_sparsemax_op = sparsemax(z.astype(dtype))\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 885, in __call__\r\n    return self.instantiate(input_types)(*args, **kwargs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 860, in instantiate\r\n    _ = defined.name  # Fully instantiate the function definition.\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 608, in name\r\n    self._create_definition_if_needed()\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 659, in _create_definition_if_needed\r\n    outputs = self._func(*inputs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/contrib/sparsemax/python/ops/sparsemax.py\", line 90, in sparsemax\r\n    return fprop(logits)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 885, in __call__\r\n    return self.instantiate(input_types)(*args, **kwargs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 860, in instantiate\r\n    _ = defined.name  # Fully instantiate the function definition.\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 608, in name\r\n    self._create_definition_if_needed()\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py\", line 659, in _create_definition_if_needed\r\n    outputs = self._func(*inputs)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/contrib/sparsemax/python/ops/sparsemax.py\", line 67, in fprop\r\n    z_sorted, _ = nn.top_k(z, k=dims)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/ops/nn_ops.py\", line 1946, in top_k\r\n    return gen_nn_ops._top_kv2(input, k=k, sorted=sorted, name=name)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/ops/gen_nn_ops.py\", line 2464, in _top_kv2\r\n    name=name)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/op_def_library.py\", line 504, in apply_op\r\n    values, as_ref=input_arg.is_ref).dtype.name\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/ops.py\", line 716, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/constant_op.py\", line 212, in _dimension_tensor_conversion_function\r\n    raise ValueError(\"Cannot convert an unknown Dimension to a Tensor: %s\" % d)\r\nValueError: Cannot convert an unknown Dimension to a Tensor: ?\r\n\r\n----------------------------------------------------------------------\r\nRan 3 tests in 0.029s\r\n\r\nFAILED (errors=2)\r\n```\r\n</details>", "Great thanks for the new implementation, looks much more maintainable! Just a few more things.\r\n\r\n* You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of `.get_shape()` why not use `tf.shape`, would that work?\r\n\r\n* I understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.\r\n\r\n* As for your error, it looks like `dims = logits.get_shape()[1]` is `None` and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try `tf.shape` instead?\r\n\r\n* Finally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.\r\n\r\nGreat thanks again for doing all of this!", "> You wrote: \" how to extract the cumsum values without depending on the batch size\" -- I'm not sure I understand. Do you need the batch size to be a python integer rather than a tensor? I mean, instead of .get_shape() why not use tf.shape, would that work?\r\n\r\nThanks `tf.shape` works, I was just not aware of it.\r\n\r\n> I understand that you implemented it with performance in mind first, and I think that's great. Could you compare the performance of your hand-written kernels and the new implementation that only uses core ops? I'm very curious how much is getting lost here -- if it's 20% then maybe it's ok, but if it's more then we might need to think how to speed things up.\r\n\r\n> Finally, if you have time and after everything works, I'd be very curious how much faster the hand-written backprop is compared to TF-computed backprop. It might be an interesting case for the new TF compiler.\r\n\r\nYes, once I get the hand-written backprop version to work I will benchmark the diffrent versions.\r\n\r\n> As for your error, it looks like dims = logits.get_shape()[1] is None and this makes things fail. Again -- you're trying to get the shape statically, and that's not always available. Could you try tf.shape instead?\r\n\r\nI changed it to  `tf.shape`, now I get a strange recursion error: [full error message](https://gist.github.com/AndreasMadsen/8dca7e8b3ead6b5b339fd4a8c5bc8b03).", "Oh, I think you can't use fprop in bprop when fprop is decorated with bprop like you have. Just make an fprop_raw function, same as your fprop but without @Defun, and call that one in bprop and fprop.\r\n\r\nHope things will work soon, looking forward to it!", "> Oh, I think you can't use fprop in bprop when fprop is decorated with bprop like you have. Just make an fprop_raw function, same as your fprop but without `@Defun`, and call that one in bprop and fprop.\r\n\r\nThat was it. It a little strange, as #4661 is about depending on `fprop` in `bprop`.\r\n\r\nAnyway, I'm getting a new error.\r\n\r\n```\r\nPYTHONPATH=tensorflow/_python_build python3 tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\r\n/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/function.py:969: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\r\n  argspec = inspect.getargspec(func)\r\nEE.\r\n======================================================================\r\nERROR: testDouble (__main__.SparsemaxTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 258, in testDouble\r\n    self._test_dtype('float64')\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 224, in _test_dtype\r\n    self._test_sparsemax_against_numpy(dtype, random, use_gpu=False)\r\n  File \"tensorflow/tensorflow/contrib/sparsemax/python/kernel_tests/sparsemax_test.py\", line 88, in _test_sparsemax_against_numpy\r\n    self.assertShapeEqual(p_sparemax, tf_sparsemax_op)\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/test_util.py\", line 594, in assertShapeEqual\r\n    self.assertAllEqual(np_array.shape, tf_tensor.get_shape().as_list())\r\n  File \"/Users/Andreas/Sites/tensorflow/_python_build/tensorflow/python/framework/tensor_shape.py\", line 782, in as_list\r\n    raise ValueError(\"as_list() is not defined on an unknown TensorShape.\")\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\r\n\r\nI don't understand why the TensorShape would be unknown, as the sparsemax op is passed the numpy array directly. The tests also worked for both the kernel and `name_scope` implementations.", "Here is an initial benchmark:\r\n\r\n| size & case                                | kernel CPU     | name_scope CPU   |\r\n|:--------------------------------|:---------------|:-----------------|\r\n| (1000, 10) sparsemax            | 0.052 \u00b1 0.0032 | 0.203 \u00b1 0.0364   |\r\n| (1000, 10) sparsemax_grad       | 0.071 \u00b1 0.0009 | 0.300 \u00b1 0.0122   |\r\n| (1000, 10) sparsemax_loss       | 0.060 \u00b1 0.0021 | 0.213 \u00b1 0.0112   |\r\n| (1000, 10) sparsemax_loss_grad  | 0.066 \u00b1 0.0015 | 0.310 \u00b1 0.0127   |\r\n| (1000, 100) sparsemax           | 0.293 \u00b1 0.0094 | 0.988 \u00b1 0.0689   |\r\n| (1000, 100) sparsemax_grad      | 0.373 \u00b1 0.0172 | 1.410 \u00b1 0.0110   |\r\n| (1000, 100) sparsemax_loss      | 0.330 \u00b1 0.0085 | 0.979 \u00b1 0.0119   |\r\n| (1000, 100) sparsemax_loss_grad | 0.447 \u00b1 0.0158 | 1.597 \u00b1 0.0159   |\r\n\r\n_`size` specifies the size of the logits and label matrix, both where randomly generated. For each case, 100 iterations where performed to avoid clock issues and `tf.Variable().initializer` was used to avoid a memory transfer overhead. Finally each experiment was repeated 10 times, the summary of those repetitions are shown as `mean \u00b1 95% confidence interval`._", "Ok, so can I assume that the pure-TF-ops implementation works now? That's good!\r\n\r\nThe benchmarks look a bit disappointing. But I see 2 things to try. For one, did you take a look at the timeline? Maybe it'll make it more clear what's eating the time. The other thing is that we should probably focus on larger sizes, how about (16K, 512) or so?\r\n\r\nThanks again for doing this! I think the pure-TF code is basically ready for review and checking-in, but it might be useful to think a bit more about performance first.", "> Ok, so can I assume that the pure-TF-ops implementation works now? That's good!\r\n\r\nThe Defun version with a defined `grad_func` doesn't work. I still get the error from https://github.com/tensorflow/tensorflow/pull/6387#issuecomment-269468829.", "Hmm, are you sure you want `self.assertShapeEqual(p_sparemax, tf_sparsemax_op)` and not `self.assertShapeEqual(p_sparemax, tf_sparsemax_out)` there? I though you want to compare the numpy shapes? I'm very curious if the grad_func helps solve some performance problems!", "The second argument to `self.assertShapeEqual` must be a `tf.Tensor`. see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L584\r\n\r\nAlso, it worked for the other two implementations.", "You could do a `tf.set_shape` at the end to set the shape of the Tensor properly. Or just use assertEqual on x.shape and y.shape if x and y are numpy objects.", "> You could do a tf.set_shape at the end to set the shape of the Tensor properly. Or just use assertEqual on x.shape and y.shape if x and y are numpy objects.\r\n\r\nI find it really odd that I have to do that, but okay.\r\n\r\nHere are the benchmark results:\r\n\r\n|                                 | kernel CPU     | kernel GPU      | name_scope CPU   | Defun CPU       |\r\n|:--------------------------------|:---------------|:----------------|:-----------------|:----------------|\r\n| (1000, 10) sparsemax            | 0.097 \u00b1 0.0053 | 0.110 \u00b1 0.0645  | 0.347 \u00b1 0.0573   | 0.372 \u00b1 0.0941  |\r\n| (1000, 10) sparsemax_grad       | 0.163 \u00b1 0.0022 | 0.109 \u00b1 0.0032  | 0.534 \u00b1 0.0066   | 0.687 \u00b1 0.0093  |\r\n| (1000, 10) sparsemax_loss       | 0.133 \u00b1 0.0037 | 0.105 \u00b1 0.0029  | 0.358 \u00b1 0.0047   | 0.399 \u00b1 0.0043  |\r\n| (1000, 10) sparsemax_loss_grad  | 0.132 \u00b1 0.0033 | 0.091 \u00b1 0.0014  | 0.573 \u00b1 0.0078   | 0.959 \u00b1 0.0118  |\r\n| (1000, 100) sparsemax           | 0.602 \u00b1 0.0535 | 0.221 \u00b1 0.0042  | 1.509 \u00b1 0.0626   | 1.494 \u00b1 0.0701  |\r\n| (1000, 100) sparsemax_grad      | 1.045 \u00b1 0.0275 | 0.238 \u00b1 0.0046  | 2.429 \u00b1 0.0511   | 2.967 \u00b1 0.0461  |\r\n| (1000, 100) sparsemax_loss      | 0.819 \u00b1 0.0410 | 0.281 \u00b1 0.0044  | 1.741 \u00b1 0.0390   | 1.739 \u00b1 0.0420  |\r\n| (1000, 100) sparsemax_loss_grad | 0.984 \u00b1 0.0472 | 0.227 \u00b1 0.0031  | 2.634 \u00b1 0.0455   | 3.752 \u00b1 0.0364  |\r\n| (1000, 512) sparsemax           | 3.180 \u00b1 0.0149 | 1.739 \u00b1 0.0463  | 6.008 \u00b1 0.0546   | 5.677 \u00b1 0.0560  |\r\n| (1000, 512) sparsemax_grad      | 3.972 \u00b1 0.0260 | 1.779 \u00b1 0.0169  | 8.192 \u00b1 0.0301   | 11.660 \u00b1 0.0233 |\r\n| (1000, 512) sparsemax_loss      | 3.496 \u00b1 0.0271 | 1.688 \u00b1 0.0088  | 6.319 \u00b1 0.0404   | 6.370 \u00b1 0.0203  |\r\n| (1000, 512) sparsemax_loss_grad | 3.807 \u00b1 0.0859 | 1.720 \u00b1 0.0073  | 9.001 \u00b1 0.0540   | 13.823 \u00b1 0.0294 |\r\n| (20, 16384) sparsemax           | 2.748 \u00b1 0.0427 | 26.740 \u00b1 0.0171 | 4.808 \u00b1 0.0598   | 4.691 \u00b1 0.0679  |\r\n| (20, 16384) sparsemax_grad      | 3.417 \u00b1 0.0588 | 26.715 \u00b1 0.0147 | 6.217 \u00b1 0.0557   | 9.567 \u00b1 0.0327  |\r\n| (20, 16384) sparsemax_loss      | 3.148 \u00b1 0.0744 | 27.026 \u00b1 0.0262 | 5.056 \u00b1 0.0366   | 5.272 \u00b1 0.0159  |\r\n| (20, 16384) sparsemax_loss_grad | 3.422 \u00b1 0.0411 | 26.772 \u00b1 0.0108 | 6.851 \u00b1 0.0401   | 11.347 \u00b1 0.0505 |", "Some strange things. Why do you run (20, 16K)? I thought 16K labels with dense tensors of size 512 is more realistic?\r\n\r\nBut, more importantly, it's strange that Defun makes things slow. When benchmarking TF one always has to first run the benchmark a few hundred steps without measuring, just to warm-up the graph and stuff. Are you doing that?", "> Why do you run (20, 16K)? I thought 16K labels with dense tensors of size 512 is more realistic?\r\n\r\nI just don't have the resources to run any bigger.\r\n\r\n> When benchmarking TF one always has to first run the benchmark a few hundred steps without measuring, just to warm-up the graph and stuff. Are you doing that?\r\n\r\nThis is the benchmark suite I wrote https://github.com/AndreasMadsen/tensorflow-sparsemax-benchmark. Do you have an example of how to ideally benchmark a TF function, I think that would be more productive.", "I think there is something I don't understand about how you specify sizes. A normal softmax from vectors of size 512 into 16K labels easily fits into any GPU, so I'm clearly not understanding something here.\r\n\r\nBut as for your benchmarks: you should always first run at least one iteration and throw away the timing, only later start true measurements. The graph does a fair bit of one-time allocations, especially with Defun, no need to measure them.\r\n\r\nGive it another look, if the benchmarks still show that the Defun isn't helping, then I think we should check in the code without it and focus on optimizing that. Did you look at the TF timeline to see what takes so much time? (We can optimize performance after checking this in too, as you prefer.)", "> But as for your benchmarks: you should always first run at least one iteration and throw away the timing, only later start true measurements. The graph does a fair bit of one-time allocations, especially with Defun, no need to measure them.\r\n\r\nI've changed it such it runs it 100 times before it measures anything. It also doesn't measure the memory transfer anymore.\r\n\r\n> I think there is something I don't understand about how you specify sizes. A normal softmax from vectors of size 512 into 16K labels easily fits into any GPU, so I'm clearly not understanding something here.\r\n\r\nIt was a matter of time, not memory. I created a job script that I can submit to our HPC system so that is not a problem, but it literally takes 12h to run the benchmark. \r\n\r\n> Did you look at the TF timeline to see what takes so much time? \r\n\r\nNo I have not, I want to setup the benchmarks correctly before I profile it.\r\n\r\n----\r\n\r\nBenchmark results:\r\n\r\n|                                  | [kernel CPU](https://github.com/AndreasMadsen/tensorflow/commit/b9679dbc6a7ba7302a9d6c0f1541459b95a014b2)      | [kernel GPU](https://github.com/AndreasMadsen/tensorflow/commit/b9679dbc6a7ba7302a9d6c0f1541459b95a014b2)       | [name_scope CPU](https://github.com/AndreasMadsen/tensorflow/commit/6be1549ce6e930f407f7ab59503b43072a53263d)   | [Defun CPU](https://github.com/AndreasMadsen/tensorflow/commit/1e5e47065cdd299e05fe8d34feafff6729efd2c5)        |\r\n|:---------------------------------|:----------------|:-----------------|:-----------------|:-----------------|\r\n| (1000, 10) sparsemax             | 0.096 \u00b1 0.0008  | 0.079 \u00b1 0.0009   | 0.325 \u00b1 0.0032   | 0.336 \u00b1 0.0030   |\r\n| (1000, 10) sparsemax_grad        | 0.163 \u00b1 0.0009  | 0.106 \u00b1 0.0014   | 0.536 \u00b1 0.0061   | 0.685 \u00b1 0.0045   |\r\n| (1000, 10) sparsemax_loss        | 0.131 \u00b1 0.0020  | 0.104 \u00b1 0.0015   | 0.356 \u00b1 0.0039   | 0.399 \u00b1 0.0039   |\r\n| (1000, 10) sparsemax_loss_grad   | 0.129 \u00b1 0.0031  | 0.088 \u00b1 0.0005   | 0.573 \u00b1 0.0040   | 0.950 \u00b1 0.0050   |\r\n| (1000, 100) sparsemax            | 0.554 \u00b1 0.0571  | 0.213 \u00b1 0.0013   | 1.494 \u00b1 0.0316   | 1.493 \u00b1 0.0291   |\r\n| (1000, 100) sparsemax_grad       | 1.029 \u00b1 0.0267  | 0.237 \u00b1 0.0029   | 2.399 \u00b1 0.0495   | 3.067 \u00b1 0.0318   |\r\n| (1000, 100) sparsemax_loss       | 0.844 \u00b1 0.0182  | 0.277 \u00b1 0.0034   | 1.758 \u00b1 0.0616   | 1.735 \u00b1 0.0159   |\r\n| (1000, 100) sparsemax_loss_grad  | 0.994 \u00b1 0.0278  | 0.227 \u00b1 0.0009   | 2.632 \u00b1 0.0245   | 3.723 \u00b1 0.0316   |\r\n| (1000, 512) sparsemax            | 3.238 \u00b1 0.0351  | 1.746 \u00b1 0.0010   | 5.973 \u00b1 0.0187   | 5.618 \u00b1 0.0239   |\r\n| (1000, 512) sparsemax_grad       | 3.918 \u00b1 0.0288  | 1.793 \u00b1 0.0019   | 8.079 \u00b1 0.0312   | 11.630 \u00b1 0.0235  |\r\n| (1000, 512) sparsemax_loss       | 3.480 \u00b1 0.0181  | 1.673 \u00b1 0.0016   | 6.218 \u00b1 0.0433   | 6.309 \u00b1 0.0219   |\r\n| (1000, 512) sparsemax_loss_grad  | 3.887 \u00b1 0.0627  | 1.754 \u00b1 0.0048   | 8.893 \u00b1 0.0575   | 13.813 \u00b1 0.0495  |\r\n| (512, 16384) sparsemax           | 60.092 \u00b1 1.4167 | 633.671 \u00b1 0.2451 | 86.176 \u00b1 0.9363  | 91.747 \u00b1 0.9430  |\r\n| (512, 16384) sparsemax_grad      | 69.699 \u00b1 0.9295 | 634.124 \u00b1 0.3004 | 115.631 \u00b1 1.1464 | 187.159 \u00b1 1.0400 |\r\n| (512, 16384) sparsemax_loss      | 61.021 \u00b1 0.7513 | 632.289 \u00b1 0.2675 | 94.075 \u00b1 0.6780  | 97.617 \u00b1 0.2923  |\r\n| (512, 16384) sparsemax_loss_grad | 67.598 \u00b1 1.7163 | 633.777 \u00b1 0.3879 | 125.269 \u00b1 0.8877 | 213.390 \u00b1 1.1162 |\r\n", "Ok, thanks! It looks like the Defun isn't worth it. Can we remove it and check in the code? I'll ask about top_k GPU kernel in the meantime.", "> Ok, thanks! It looks like the Defun isn't worth it. Can we remove it and check in the code? I'll ask about top_k GPU kernel in the meantime.\r\n\r\nI've removed the defun commit.", "Ok, thanks! I think the code is ready, but please add one more short unit test for the changed `assertAllCloseAccordingToType` function (just so it's at least ran once in the tests).", "added tests for `assertAllCloseAccordingToType` as requested.", "@tensorflow-jenkins test this please.", "@tensorflow-jenkins test this please\r\n", "Looks like lint checks are failing due to some bad indentation:\r\n\r\ntensorflow/python/framework/test_util_test.py:198: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\nCould you correct that? Thanks!", "Sure. What is the command for running just the lint check?", "Take a look here (search for FAIL):\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/2635/console", "@AndreasMadsen any luck?\r\n\r\n```\r\nFAIL: Found 9 non-whitelited pylint errors:\r\ntensorflow/python/framework/test_util_test.py:198: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/test_util_test.py:204: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/test_util_test.py:205: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/framework/test_util_test.py:212: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/test_util_test.py:219: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/test_util_test.py:220: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\ntensorflow/python/framework/test_util_test.py:228: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/test_util_test.py:236: [W0311(bad-indentation), ] Bad indentation. Found 6 spaces, expected 4\r\n\r\ntensorflow/python/framework/test_util_test.py:237: [W0311(bad-indentation), ] Bad indentation. Found 10 spaces, expected 6\r\n\r\n```", "@drpngx No. Finding the errors is no problem, but I would like to lint the files myself before committing, unfortunately I can't find the appropriate command.", "Gotcha.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/ci_sanity.sh#L84", "@drpngx I see, unfortunately it depends on a specific python version being in a specific location. I've committed the lint fixes, I hope they are correct but I have no way to be sure.", "Fixed the spelling error too.", "@tensorflow-jenkins test this please\r\n", "One more thing, commas missing in the BUILD file. Hopefully that's the last thing. (And yes, we need a better process for that, I think.)\r\n\r\nFAIL: buildifier found errors and/or warnings in above BUILD files.\r\nbuildifier suggested the following changes:\r\n15c15\r\n<     \"tf_py_test\"\r\n---\r\n>     \"tf_py_test\",\r\n28a29\r\n>         \"//tensorflow/python:array_ops\",\r\n31d31\r\n<         \"//tensorflow/python:array_ops\",\r\n", "I found only one missing comma", "@tensorflow-jenkins test this please\r\n", "Please also move\r\n  \"//tensorflow/python:array_ops\",\r\nin the sparsemax/BUILD file 2 lines up (so that the order is alphabetic). Thanks!", "Should be in alphabetical order now", "@tensorflow-jenkins test this please", "It looks like this part is ok now. The failing things look unrelated, I think there is a recent problem with testing. I'll re-test tomorrow, it might be all fine.", "Thanks! We're having build issues that cause the system to timeout. We rebooted machines to see if the problem would go away.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "We have new failures, including a test timeout.\r\n\r\n```\r\n14:14:18  11/189 Test  #28: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/conv_ops_test.py ............................***Failed    6.81 sec\r\n14:14:18 Traceback (most recent call last):\r\n14:14:18   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/conv_ops_test.py\", line 32, in <module>\r\n14:14:18     from tensorflow.contrib import layers\r\n14:14:18   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 52, in <module>\r\n14:14:18     from tensorflow.contrib import sparsemax\r\n14:14:18 ImportError: cannot import name 'sparsemax'\r\n```\r\n\r\nCan you `bazel test -c opt tensorflow/...`?", "This is the output I get from running ` bazel test -c opt tensorflow/...` https://gist.github.com/AndreasMadsen/a11009cd3cda4d6058dfb4dee5072356\r\n\r\ntl;dr it fails with:\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_Andreas/99eb1091050df9ac5ca7bd7d8ea0247e/external/protobuf/BUILD:579:1: C++ compilation of rule '@protobuf//:internal/_api_implementation.so' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 41 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/protobuf/python/google/protobuf/internal/api_implementation.cc:31:10: fatal error: 'Python.h' file not found\r\n#include <Python.h>\r\n         ^\r\n1 error generated.\r\n```\r\n\r\nI've just executed the tests individually with `python path_to_test.py`. I've also confirmed that `help(tf.contrib.sparsemax)` works.", "Did you run bazel fetch?\n\nOn Jan 19, 2017 8:31 AM, \"Andreas Madsen\" <notifications@github.com> wrote:\n\n> This is the output I get from running bazel test -c opt tensorflow/...\n> https://gist.github.com/AndreasMadsen/a11009cd3cda4d6058dfb4dee5072356\n>\n> tl;dr it fails with:\n>\n> ERROR: /private/var/tmp/_bazel_Andreas/99eb1091050df9ac5ca7bd7d8ea0247e/external/protobuf/BUILD:579:1: C++ compilation of rule '@protobuf//:internal/_api_implementation.so' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 41 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n> external/protobuf/python/google/protobuf/internal/api_implementation.cc:31:10: fatal error: 'Python.h' file not found\n> #include <Python.h>\n>          ^\n> 1 error generated.\n>\n> I've just executed the tests individually with python path_to_test.py.\n> I've also confirmed that help(tf.contrib.sparsemax) works.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6387#issuecomment-273824937>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbTQ2_4VNhOqrVvM8jkiZl6ZSlT4Hks5rT4_0gaJpZM4LQJo->\n> .\n>\n", "No. I ran `bazel fetch tensorflow/...` now and gets the same error (though the `libtool` warnings no longer appears):\r\n\r\n```\r\nINFO: Found 1289 targets and 839 test targets...\r\nERROR: /private/var/tmp/_bazel_Andreas/99eb1091050df9ac5ca7bd7d8ea0247e/external/protobuf/BUILD:579:1: C++ compilation of rule '@protobuf//:internal/_api_implementation.so' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 41 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/protobuf/python/google/protobuf/internal/api_implementation.cc:31:10: fatal error: 'Python.h' file not found\r\n#include <Python.h>\r\n         ^\r\n```", "This is python development packages. On ubuntu, install the `python-dev` package.", "@drpngx It is a `brew` install on MacOS and `python3-config --include` points to the correct directories.", "Strange. How did you build before?", "I just followed the [Setting up TensorFlow for Development](https://www.tensorflow.org/get_started/os_setup#setting_up_tensorflow_for_development) instructions.\r\n\r\n```\r\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nmkdir _python_build\r\ncd _python_build\r\nln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .\r\nln -s ../tensorflow/tools/pip_package/* .\r\npython3 setup.py develop\r\n```", "@tensorflow-jenkins test this please", "@AndreasMadsen It looks like your test is timing out on CPU. Could you possibly split it up into smaller test cases that can be run in parallel to bring down the time?", "I've reduced the number of test observations from `100` to `10`. I'm not sure what the criteria is for splitting up a test, can I simply split `testFloat` into `test_sparsemax_against_numpy_float`, `test_sparsemax_of_zero_float`, ...? I just followed the pattern used in other tests.", "@AndreasMadsen I agree that it is probably difficult to split the tests more than they currently are. If the size reduction does the trick, I'm OK with merging this. Thanks for the quick response.", "@tensorflow-jenkins test this please", "@AndreasMadsen: The errors look unrelated, but I would like to wait and re-run the tests after the fixes in https://github.com/tensorflow/tensorflow/pull/7209 are merged to be safe. Thanks for your patience.", "@tensorflow-jenkins test this please", "@AndreasMadsen it looks like some of the errors are genuine, e.g.\r\n\r\n10:19:22 Traceback (most recent call last):\r\n10:19:22   File \"C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/conv_ops_test.py\", line 32, in <module>\r\n10:19:22     from tensorflow.contrib import layers\r\n10:19:22   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\", line 52, in <module>\r\n10:19:22     from tensorflow.contrib import sparsemax\r\n10:19:22 ImportError: cannot import name 'sparsemax'\r\n\r\nI suspect you need to add an entry for sparsemax in the top-level tensorflow/BUILD file, in the all_opensource_files filegroup.", "@rmlarsen I see, thanks. I've added `sparsemax` to the `all_opensource_files` filegroup.", "@tensorflow-jenkins test this please", "@AndreasMadsen OK one more change (I hope): To make this work with our cmake build, you need to add your module here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/tf_python.cmake#L375\r\n\r\nJust follow the existing pattern.", "Done", "@tensorflow-jenkins test this please", "@gunan AFAICT the remaining build error in unrelated to Andreas' code. Can you please confirm that it's OK to merge this?", "It looks like agent went down during build.\r\nBut as there are cmake changes in the PR, would it be ok to wait testing just windows cmake (We can merge when windows passes.\r\nhere is the build:\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/1060/", "@gunan it looks like the build you kicked off finished successfully. I'll merge this now.\r\n\r\nhttp://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/1060/console\r\n\r\n@AndreasMadsen thank you for the contribution. This is really nice.", "Thanks for the continued guidance."]}, {"number": 6386, "title": "Dense CRF implemenation", "body": "Is there an easy way to implement or use the dense CRF within tensorflow.  More spesifically i would love to train my network with dens CRF. You can see more information about dense CRF from this paper.[DenseCRF poster ](http://vladlen.info/papers/densecrf-poster.pdf). You can see C++ implementation form here: [Dense CRF C++](https://github.com/lucasb-eyer/pydensecrf/tree/d824b89ee3867bca3e90b9f04c448f1b41821524/pydensecrf/densecrf/src). Is there a way to include this in tensorflow and built with it? ", "comments": ["This question is more appropriate for StackOverflow. Please re-ask there. Thank you!"]}, {"number": 6385, "title": "Unable to build Android Example App using Bazel on Windows", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/6101\r\nhttps://github.com/tensorflow/tensorflow/issues/6383\r\nhttp://stackoverflow.com/questions/40978859/creating-simple-android-app-using-android-studio-and-tensorflow\r\n\r\n### Environment info\r\nOperating System: Windows\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCuda 8.0\r\ncuDNN 5.0\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\nI downloaded the latest ZIP on 18 Dec 2016. Sorry, I have no idea how to get the version.\r\n\r\n2. The output of `bazel version`\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:13 2016 (1481136433)\r\nBuild timestamp: 1481136433\r\nBuild timestamp as int: 1481136433\r\n\r\nMy goal is to build an Android App which loads pre-trained TensorFlow model and runs it on an Android device. I am working on Android Studio on Windows. Unfortunately using Android Studio on Linux is currently not an option.\r\n\r\nFollowing previous advice from this forum, I am trying to build an Android example using Bazel on Windows. \r\nI've successfully installed Bazel using Chocolatey.\r\nFollowing the build instructions, I changed the SDK and NDK paths in WORKSPACE file and ran:\r\n**bazel build //tensorflow/examples/android:tensorflow_demo**\r\n\r\nSo far I did the following to fix errors:\r\n1) copied aapt.exe to aapt, zipalign.exe to zipalign, since their name is different on Windows/Linux\r\n2) installed using pacman gcc \r\n\r\nBut I am stuck again. I am getting the following error which I have no idea how to fix:\r\n\r\n```\r\nERROR: C:/Users/Andrey/AppData/Local/Temp/_bazel_Andrey/gd3-gSwg/external/protobuf/BUILD:73:1: C++ compilation of rule '@protobuf//:protobuf_lite' failed: arm-linux-androideabi-gcc failed: error executing command external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables ... (remaining 44 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/protobuf/src/google/protobuf/stubs/structurally_valid.cc:588:1: fatal error: opening dependency file bazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/stubs/structurally_valid.d: No such file or directory\r\n }  // namespace google\r\n ^\r\ncompilation terminated.\r\nERROR: C:/Tools/tensorflow-master/tensorflow/examples/android/BUILD:58:1: output 'tensorflow/examples/android/tensorflow_demo_symbols/R.txt' was not created.\r\nERROR: C:/Tools/tensorflow-master/tensorflow/examples/android/BUILD:58:1: output 'tensorflow/examples/android/tensorflow_demo.srcjar' was not created.\r\nERROR: C:/Tools/tensorflow-master/tensorflow/examples/android/BUILD:58:1: output 'tensorflow/examples/android/proguard/tensorflow_demo/_tensorflow_demo_proguard.cfg' was not created.\r\nERROR: C:/Tools/tensorflow-master/tensorflow/examples/android/BUILD:58:1: output 'tensorflow/examples/android/tensorflow_demo_processed_manifest/AndroidManifest.xml' was not created.\r\nERROR: C:/Tools/tensorflow-master/tensorflow/examples/android/BUILD:58:1: output 'tensorflow/examples/android/tensorflow_demo_files/resource_files.zip' was not created.\r\nERROR: C:/Tools/tensorflow-master/tensorflow/examples/android/BUILD:58:1: output 'tensorflow/examples/android/tensorflow_demo.ap_' was not created.\r\nERROR: C:/Users/Andrey/AppData/Local/Temp/_bazel_Andrey/gd3-gSwg/external/protobuf/BUILD:73:1: output 'external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/stubs/stringprintf.o' was not created.\r\nERROR: C:/Users/Andrey/AppData/Local/Temp/_bazel_Andrey/gd3-gSwg/external/protobuf/BUILD:73:1: output 'external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/arenastring.o' was not created.\r\nERROR: C:/Users/Andrey/AppData/Local/Temp/_bazel_Andrey/gd3-gSwg/external/protobuf/BUILD:113:1: output 'external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/io/strtod.o' was not created.\r\nERROR: C:/Users/Andrey/AppData/Local/Temp/_bazel_Andrey/gd3-gSwg/external/protobuf/BUILD:73:1: output 'external/protobuf/_objs/protobuf_lite/external/protobuf/src/google/protobuf/repeated_field.o' was not created.\r\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4.722s, Critical Path: 1.16s\r\n\r\n```\r\n1. Please advice how to proceed?\r\n2. Isn't there an easier way to achieve what I want? I just need pre-built TensorFlow binaries for Android Studio on Windows, to build an example application.  So far I found only pre-built Python distribution and Bazel installation on Windows\r\n", "comments": ["@petewarden, you mentioned you have had experience with android builds on windows.\r\n", "Ok, after trying to build the Bazel example Android project on Linux, and reverse engineering the .apk package, it looks like it depends only on a single .so native TensorFlow Android library, called **libtensorflow_demo.so**. \r\n\r\nCurrently in order to build this simple example on Windows, I needed to:\r\n1) Install chocolatey\r\n2) Install bazel\r\n3) Install pacman\r\n4) Download gcc using pacman\r\n5) Change CONFIGURATION file and find my SDK and NDK\r\n6) Change some sdk .exe files to no extenstion, to fit Linux \r\n7) And it still does not work \ud83d\udc4e \r\n... That is a very hard and tedious process\r\n\r\nI think that it could really help if there was an example Android Studio project for windows which _does not use Bazel at all_.  Instead let's put the mentioned above pre-compiled TensorFlow library available for download (for all possible Android architectures - **arm64-v8a**, **armeabi**, **armeabi-v7a** ,**x86_64**) ,together with a simple tutorial.\r\n\r\nObviously, it will not be as flexible as the ability to re-build TensorFlow, but for 99% of users it will suffice to update the pre-built libraries once in a while.", "Unfortunately Bazel doesn't support Android on Windows yet.\r\n\r\nHowever we do have cmake support in [tensorflow/contrib/android/cmake](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/contrib/android/cmake/) that should let you build the library necessary for simple Android inference on Windows (this excludes some native code created specifically for the demo). Full demo support should be coming soon, possibly including pre-built libs.\r\n\r\nAs a sort-of-hacky alternative you can grab the APK built by Jenkins [here](http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-android/lastBuild/TF_BUILD_CONTAINER_TYPE=ANDROID,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=android-slave/), and unzip it to grab libtensorflow_demo.so to manually place into your project.", "@AndreyRub Please see http://ci.tensorflow.org/view/Nightly/job/nightly-android/ for a full selection of prebuilt Android TF libs, which has just come online.\r\n\r\nYou can either download libtensorflow_inference.so for something that can be dropped into a standalone app, or grab\r\nlibtensorflow_demo.so if you're trying to get the TF Android demo to run (contains libtensorflow_inference.so + demo-specific native code).\r\n\r\nIn either case you will need libandroid_tensorflow_inference_java.jar as well for the Java counterpart.", "@andrewharp or @AndreyRub Could you please provide some steps for such goal:\r\nIn order to build simple example on Windows with retrained model, I needed to:\r\n1) Install chocolatey\r\n2) Install bazel\r\n3) ....\r\n\r\nI am really stuck with it. Thank you.", "@bulatyauheni Bazel does not support Android on Windows yet. You might try the [cmake build](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android/cmake), but easiest thing to do if you just want to plug something into your Android Studio project is to grab the pre-built libraries referenced above.", "@andrewharp  Hey Andrew I am trying to build tensorflow/examples/android on windows 7 using Android Studio. Is it possible or not ?\r\nAnd even if I can build it can you look into this error if possible \r\nI also tried to write another commandLine 'export JAVA_HOME=\"$(ls -d C:/Program\\ Files/Java/jdk* | sort | tail -n 1)\"' but even this didn't work \r\n![errorandroidstudio](https://cloud.githubusercontent.com/assets/23556208/23362443/c57c1eb2-fd1b-11e6-82f2-b7133b23fd34.jpg)\r\n", "@Anmolk22 I'll try to help you. Bazel does not work properly on Windows, so for building project you should use prebuilded libs.\r\n1) (Open this link and download last stable build archive http://ci.tensorflow.org/view/Nightly/job/nightly-android/) It should contains following files\r\n![image](https://cloud.githubusercontent.com/assets/14091770/23396894/00e19050-fda7-11e6-9a6d-7167b79b1bbd.png)\r\n2) Remove buildNative and copyNativeLibs tasks from build.gradle file.\r\n3) Copy libs from archive to libs derectory of android project.", "The native libs necessary for the demo can now be built with make via Android Studio, completely removing the need for Bazel on Linux and OS X system. Set `buildWithMake` to true in build.gradle to try.\r\n\r\nHowever, it seems some extra changes will still be required to get this to get the make builds to work on Windows, so reopening to track.", "Ok, I've got something sort of working. It needs some more tweaking, and I would recommend sticking with the prebuilts as described by @bulatyauheni above, but if you're feeling adventurous here's how you can build the Android TF libraries on Windows 10. \r\n\r\nFirst, install Bash for Windows as described here: https://msdn.microsoft.com/en-us/commandline/wsl/about\r\n\r\nOnce that's done, start bash and run the following, adjusting directories appropriately:\r\n\r\n```\r\n# Install necessary components\r\nsudo apt-get install make unzip autoconf ccache autogen libtool g++ zlibc-dev\r\n\r\n# Download and install the NDK (has to be the Linux NDK)\r\nwget https://dl.google.com/android/repository/android-ndk-r12b-linux-x86_64.zip\r\nmkdir -p ~/android\r\nunzip android-ndk-r12b-linux-x86_64.zip -d ~/android\r\n\r\n# Build once to confirm it works and generate proto libs (which will be reused).\r\nexport NDK_ROOT=~/android/android-ndk-r12b\r\nexport CC_PREFIX=ccache\r\ntensorflow/contrib/makefile/build_all_android.sh \\\r\n-s tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in \\\r\n-t \"libtensorflow_inference.so libtensorflow_demo.so\"\r\n```\r\n\r\nNow create a file named \"makefile_helper.bat\" in tensorflow/ and add the following to it:\r\n```\r\necho \"BUILDING!\"\r\nbash -c 'NDK_ROOT=~/android/android-ndk-r12b CC_PREFIX=ccache tensorflow/contrib/makefile/build_all_android.sh -T -s tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in -t \"libtensorflow_inference.so libtensorflow_demo.so\"'\r\nexit\r\n```\r\n(The '-T' will prevent it from redownloading and compiling protobuf every time)\r\n\r\nThen update build.gradle buildNativeMade to the following:\r\n```\r\ntask buildNativeMake(type: Exec) {\r\n    workingDir '../../..'\r\n    commandLine = ['cmd', '/C', 'start', 'makefile_helper.bat']\r\n}\r\n```\r\n\r\nNow, if you set `buildWithMake = true`, it *should* create the native libs exactly where gradle expects to find them and copy them into the app dir.\r\n\r\nThis should work similarly for bazel on Windows, if you wish to install that under bash instead.\r\n\r\nI'd prefer not to check in a separate helper script, so if anybody has luck getting the command to run inline in the Exec task itself please let me know.\r\n\r\n", "@bulatyauheni @AndreyRub \r\nI tried to follow the steps to build android example.\r\n1. imported examples/android/build gradle\r\n2. Edited the path to pre-built so \r\ndef demoLibPath = '../libtensorflow_demo.so'\r\ndef inferenceLibPath = '../libtensorflow_inference.so'\r\n3. I now get the following error\r\n**Error:Execution failed for task ':buildNativeBazel'.\r\n> A problem occurred starting process 'command '/usr/local/bin/bazel''**\r\nPlease find the attached build file.\r\n\r\nAny help would be very useful!\r\n\r\n\r\n[build.txt](https://github.com/tensorflow/tensorflow/files/885234/build.txt)\r\n\r\n", "@ramarajan09  \r\nTry to remove this task", "@bulatyauheni I tried to remove that task, however it throws multiple error in this task \r\ntasks.whenTaskAdded { task ->\r\n    if (task.name == 'assembleDebug') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n    if (task.name == 'assembleRelease') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n}\r\n\r\nCould you share the build file you used", "@ramarajan09\r\nTry to remove this task instead\r\ntask buildNativeBazel(type: Exec) {\r\n    workingDir '../../..'\r\n    commandLine bazelLocation, 'build', '-c', 'opt',  \\\r\n         'tensorflow/examples/android:tensorflow_native_libs',  \\\r\n         '--crosstool_top=//external:android/crosstool',  \\\r\n         '--cpu=' + cpuType,  \\\r\n         '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain'\r\n}", "@bulatyauheni Thanks for pointing it out, I have now removed buildNativeBazel and copyNativeLibs. \r\nAlso, I created a folder call libs in my project folder and copied the four folders obtained from pre-builts (i.e benchmark model, libandroid_tensorflow_lib.lo, libtensorflow_demo.so, libtensorflow_inference.so). Each of these folders have arm64-v8a, armeabi-v7a, x86, x86_64 subfolders).\r\n\r\nAfter building the app, the app throws up following error. \r\n\r\nProcess: org.tensorflow.demo, PID: 10513\r\n                                                                     java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.demo-1/base.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_0_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_1_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_2_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_3_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_4_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_5_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_6_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_7_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_8_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_9_apk.apk\"],nativeLibraryDirectories=[/data/app/org.tensorflow.demo-1/lib/arm64, /system/lib64, /vendor/lib64]]] couldn't find \"libtensorflow_demo.so\"\r\n                                                                         at", "If you unzip the apk, what do you see?", "You dont define a good url of bazel:\r\n\r\ndef bazelLocation = '/usr/bin/bazel'\r\n", "To update on this, a prebuilt TensorFlow AAR is now provided at https://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/tensorflow.aar\r\n\r\nThis contains tensorflow_inference_java.jar and tensorflow_inference.so native libs for armeabi-v7a, arm64-v8a, x86, and x86_64.\r\n\r\nThis means to integrate TF into your app built with Gradle, you can simply download the file to e.g. \"aarDir/\", and then add the following into your gradle build file:\r\n\r\n```\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        flatDir {\r\n            dirs 'aarDir'\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile(name:'tensorflow', ext:'aar')\r\n}\r\n```\r\n\r\nThis will provide TensorFlowInferenceInterface as well as the full TF Java API. We'll be putting this up on jcenter soon so no manual downloading will be necessary, just a couple of Gradle lines.\r\n\r\n**Note that for the official Android demo** in particular, you'll still need to download/build **libtensorflow_demo.so** as well (which provides image conversion/object tracking support). It also needs to be built for the highest archictecture your device uses, since tensorflow.aar now provides more than armeabi-v7a and all native libs must be present at e.g. arm64 if your device uses arm64. It's a straightforward library, so we'll be adding a cmake build for libtensorflow_demo.so specifically so it can be built on Windows directly.\r\n\r\nYou'll also want to remove references to libtensorflow_inference.so (but not libtensorflow_demo.so), tensorflow/java, and tensorflow/contrib/android from build.gradle since these are now provided by the AAR.\r\n\r\nCoupled with the AAR download it will then be possible to build the entire demo on Windows without much, if any, manual tweaking; we should be able to auto-detect Windows and choose the appropriate build process.", "Hi Andrew,\r\n\r\nThank you very much for the suggestion.  Do you know where the libs folder would be, I'm having difficulty knowing where to drop the folders and if any paths need to change on the build.gradle file. Thanks.", "@vyse8 The folder name is arbitrary, it's just relative to the build.gradle file. So for the TF android example you could create a `tensorflow/examples/android/libs` folder and put the aar in there. Then the snippet in my comment above would work to build in TF support.\r\n\r\nAn additional step in the case of the TF demo is necessary: you'll want to remove mentions of libtensorflow_inference.so, tensorflow/java, and tensorflow/contrib/android from build.gradle so that gradle doesn't try to include them in the APK twice, now that these are being provided by the aar. We'll get this streamlined once we've got the AAR published on jcenter.", "Hi Andrew,\r\n\r\nThanks for the quick follow-up.  I created the libs folder as you had suggested and it looks like I might be hitting a different error now.  Any ideas on what might cause this?\r\n\r\nERROR: C:/users/mattv/onedrive/documents/github/tensorflow/tensorflow/contrib/android/BUILD:72:1: error loading package 'tensorflow/core': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/mattv/onedrive/documents/github/tensorflow/tensorflow/workspace.bzl\", line 98\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"C:/users/mattv/onedrive/documents/github/tensorflow/tensorflow/workspace.bzl\", line 87, in _apply_patch\r\n                _execute_and_check_ret_code(repo_ctx, [\"patch\", \"-p1\", \"-d\", r...), <2 more arguments>])\r\n        File \"C:/users/mattv/onedrive/documents/github/tensorflow/tensorflow/workspace.bzl\", line 79, in _execute_and_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(256) when executing 'patch -p1 -d C:/users/mattv/appdata/local/temp/_bazel_mattv/cpz5iwfo/external/protobuf -i C:/users/mattv/onedrive/documents/github/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: java.io.IOException: CreateProcess(): The system cannot find the file specified.\r\n and referenced by '//tensorflow/contrib/android:libtensorflow_inference.so'.\r\nERROR: Analysis of target '//tensorflow/examples/android:tensorflow_demo' failed; build aborted.\r\nINFO: Elapsed time: 8.017s\r\n\r\n\r\n\r\n\r\nMy build.gradle is as follows:\r\n\r\n// This file provides basic support for building the TensorFlow demo\r\n// in Android Studio with Gradle.\r\n//\r\n// Note that Bazel is still used by default to compile the native libs,\r\n// and should be installed at the location noted below. This build file\r\n// automates the process of calling out to it and copying the compiled\r\n// libraries back into the appropriate directory.\r\n//\r\n// Alternatively, experimental support for Makefile builds is provided by\r\n// setting buildWithMake below to true. This will allow building the demo\r\n// on Windows machines, but note that full equivalence with the Bazel\r\n// build is not yet guaranteed. See comments below for caveats and tips\r\n// for speeding up the build, such as as enabling ccache.\r\n\r\n// Set to true to build with make.\r\n// NOTE: Running a make build will cause subsequent Bazel builds to *fail*\r\n// unless the contrib/makefile/downloads/ and gen/ dirs are deleted afterwards.\r\ndef buildWithMake = false\r\n\r\n// Controls output directory in APK and CPU type for Bazel builds.\r\n// NOTE: Does not affect the Makefile build target API (yet), which currently\r\n// assumes armeabi-v7a. If building with make, changing this will require\r\n// editing the Makefile as well.\r\ndef cpuType = 'armeabi-v7a'\r\n\r\n// Output directory in the local directory for packaging into the APK.\r\ndef nativeOutDir = 'libs/' + cpuType\r\n\r\n// Default to building with Bazel and override with make if requested.\r\ndef nativeBuildRule = 'buildNativeBazel'\r\ndef demoLibPath = '../../../bazel-bin/tensorflow/examples/android/libtensorflow_demo.so'\r\n//def inferenceLibPath = '../../../bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so'\r\nif (buildWithMake) {\r\n    nativeBuildRule = 'buildNativeMake'\r\n    demoLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_demo.so'\r\n    //inferenceLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_inference.so'\r\n}\r\n\r\n// Defines the NDK location for Makefile builds. Does *not* affect Bazel builds.\r\n// Override with your absolute NDK location if this fails to get the location\r\n// automatically.\r\ndef makeNdkRoot = System.getenv('NDK_ROOT')\r\n\r\n// If building with Bazel, this is the location of the bazel binary.\r\n// NOTE: Bazel does not yet support building for Android on Windows,\r\n// so in this case the Makefile build must be used as described above.\r\ndef bazelLocation = 'c://tools//msys64//usr//bin//bash.exe'\r\n\r\nproject.buildDir = 'gradleBuild'\r\ngetProject().setBuildDir('gradleBuild')\r\n\r\n// import DownloadModels task\r\nproject.ext.ASSET_DIR = projectDir.toString() + '/assets'\r\nproject.ext.TMP_DIR   = project.buildDir.toString() + '/downloads'\r\n\r\nbuildscript {\r\n    repositories {\r\n        jcenter()\r\n    }\r\n\r\n    dependencies {\r\n        classpath 'com.android.tools.build:gradle:2.3.1'\r\n    }\r\n}\r\n\r\napply plugin: 'com.android.application'\r\n\r\nandroid {\r\n    compileSdkVersion 23\r\n    buildToolsVersion \"25.0.2\"\r\n\r\n    lintOptions {\r\n        abortOnError false\r\n    }\r\n\r\n    sourceSets {\r\n        main {\r\n            // TensorFlow Java API sources.\r\n            java {\r\n                srcDir '../../java/src/main/java'\r\n                exclude '**/examples/**'\r\n            }\r\n\r\n            // Android TensorFlow wrappers, etc.\r\n            java {\r\n                srcDir '../../contrib/android/java'\r\n            }\r\n\r\n            // Android demo app sources.\r\n            java {\r\n                srcDir 'src'\r\n            }\r\n\r\n            manifest.srcFile 'AndroidManifest.xml'\r\n            resources.srcDirs = ['src']\r\n            aidl.srcDirs = ['src']\r\n            renderscript.srcDirs = ['src']\r\n            res.srcDirs = ['res']\r\n            assets.srcDirs = [project.ext.ASSET_DIR]\r\n            jniLibs.srcDirs = ['libs']\r\n        }\r\n\r\n        debug.setRoot('build-types/debug')\r\n        release.setRoot('build-types/release')\r\n    }\r\n}\r\n\r\ntask buildNativeBazel(type: Exec) {\r\n    workingDir '../../..'\r\n    commandLine bazelLocation, 'build', '-c', 'opt',  \\\r\n         'tensorflow/examples/android:tensorflow_native_libs',  \\\r\n         '--crosstool_top=//external:android/crosstool',  \\\r\n         '--cpu=' + cpuType,  \\\r\n         '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain'\r\n}\r\n\r\n//task buildNativeMake(type: Exec) {\r\n   // environment \"NDK_ROOT\", makeNdkRoot\r\n    // Tip: install ccache and uncomment the following to speed up\r\n    // builds significantly.\r\n    // environment \"CC_PREFIX\", 'ccache'\r\n   // workingDir '../../..'\r\n   // commandLine 'tensorflow/contrib/makefile/build_all_android.sh',  \\\r\n       //  '-s',  \\\r\n      //   'tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in',  \\\r\n    //     '-t',  \\\r\n   //      'libtensorflow_inference.so libtensorflow_demo.so'  \\\r\n         //, '-T'  // Uncomment to skip protobuf and speed up subsequent builds.\r\n//}\r\n\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        flatDir {\r\n            dirs 'libs'\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile(name:'tensorflow', ext:'aar')\r\n}\r\n\r\n\r\n//task copyNativeLibs(type: Copy) {\r\n    //from demoLibPath\r\n    //from inferenceLibPath\r\n  //  into nativeOutDir\r\n    //duplicatesStrategy = 'include'\r\n    //dependsOn nativeBuildRule\r\n    //fileMode 0644\r\n//}\r\n\r\n\r\ntasks.whenTaskAdded { task ->\r\n    if (task.name == 'assembleDebug') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n    if (task.name == 'assembleRelease') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n}\r\n\r\n// Download default models; if you wish to use your own models then\r\n// place them in the \"assets\" directory and comment out this line.\r\napply from: \"download-models.gradle\"\r\n\r\n\r\n", "@vyse8 It appears you have no dependency remaining on the nativeBuildRule, so I'm not sure why you would be getting this bazel error. Are you sure you didn't change the build.gradle file after you got this error?\r\n\r\nAlso, if you were building with bazel on Windows, you wouldn't be able to simply change bazel to bash and have it work. You'd need to do something similar to the workaround with the helper script in my comment above.", "@andrewharp I completely forgot to include the helper script.  I'm adding bash to my Windows now and will let you know how it looks after that.  I really appreciate the help.  Hoping to learn enough from this example to incorporate into academic work but tensorflow hasn't been too friendly so far ha.", "@vyse8 Note that if you're going as far as to install bash for Windows to build libtensorflow_demo.so, then you might as well build libtensorflow_inference.so with make/Bazel as well. There's no real advantage to using the AAR file in this case.\r\n\r\nRight now, the AAR is intended to be the easy approach if you're incorporating TF into a standalone app (that doesn't require libtensorflow_demo.so like the TF demo does). You can use it with the demo of course, but you'll also need to also download the nightly binaries for libtensorflow_demo.so [here](https://ci.tensorflow.org/view/Nightly/job/nightly-android/). You could also just download libtensorflow_demo.so there as well rather than the AAR (then you wouldn't need to mess with the Java source sets).\r\n\r\nAs mentioned we plan to keep streamlining this for Windows to make all approaches more straightforward.", "@andrewharp thanks for the extra info.  My end goal is to leverage this into my standalone app, but I was hoping to learn the connections and calls via the example.  I think I might be confused on a few of the configuration options but I believe that I'm close.  In regards to the copyNativeLibs section of build.gradle, should this be commented out?\r\n\r\ntask copyNativeLibs(type: Copy) {\r\n    from demoLibPath\r\n    from inferenceLibPath\r\n    into nativeOutDir\r\n    duplicatesStrategy = 'include'\r\n    dependsOn nativeBuildRule\r\n    fileMode 0644\r\n}\r\n\r\n**When I leave it uncommented I receive the following error within Android Studio:**\r\n\r\n Error:(148, 0) Could not get unknown property 'inferenceLibPath' for task ':copyNativeLibs' of type org.gradle.api.tasks.Copy.<a href=\"openFile:C:\\Users\\mattv\\OneDrive\\Documents\\GitHub\\tensorflow\\tensorflow\\examples\\android\\build.gradle\">Open File</a>\r\n\r\nIf I do comment out the CopyNativeLibs block then I receive the following when attempting to run within Android Studio:\r\n\r\nError:Could not determine the dependencies of task ':assembleDebug'.\r\n> Task with path 'copyNativeLibs' not found in root project 'android'.\r\n\r\nThoughts? \r\n\r\nThanks again.", "@vyse8 You'll need to revert all the edits you made to build.gradle first before applying the edits from my bash for Windows workaround -- it can't find inferenceLibPath for example because you've commented it out.\r\n", "@andrewharp Good point.  So I think I'm close thanks to your suggestions above.  Android Studio was able to install the apk but throws this error on launch:\r\n\r\n04-21 17:18:37.945 3510-3510/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                   Process: org.tensorflow.demo, PID: 3510\r\n                                                                   java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.demo-2/base.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_dependencies_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_0_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_1_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_2_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_3_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_4_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_5_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_6_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_7_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_8_apk.apk\", zip file \"/data/app/org.tensorflow.demo-2/split_lib_slice_9_apk.apk\"],nativeLibraryDirectories=[/data/app/org.tensorflow.demo-2/lib/x86, /system/fake-libs, /data/app/org.tensorflow.demo-2/base.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_dependencies_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_0_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_1_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_2_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_3_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_4_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_5_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_6_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_7_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_8_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-2/split_lib_slice_9_apk.apk!/lib/x86, /system/lib, /vendor/lib]]] couldn't find \"libtensorflow_demo.so\"\r\n\r\n\r\n\r\nIt seems like it might be having trouble finding the libtensorflow_demo.so still but I'm not sure where it wants it placed.  My build file now appears as follows:\r\n\r\n// This file provides basic support for building the TensorFlow demo\r\n// in Android Studio with Gradle.\r\n//\r\n// Note that Bazel is still used by default to compile the native libs,\r\n// and should be installed at the location noted below. This build file\r\n// automates the process of calling out to it and copying the compiled\r\n// libraries back into the appropriate directory.\r\n//\r\n// Alternatively, experimental support for Makefile builds is provided by\r\n// setting buildWithMake below to true. This will allow building the demo\r\n// on Windows machines, but note that full equivalence with the Bazel\r\n// build is not yet guaranteed. See comments below for caveats and tips\r\n// for speeding up the build, such as as enabling ccache.\r\n\r\n// Set to true to build with make.\r\n// NOTE: Running a make build will cause subsequent Bazel builds to *fail*\r\n// unless the contrib/makefile/downloads/ and gen/ dirs are deleted afterwards.\r\ndef buildWithMake = true\r\n\r\n// Controls output directory in APK and CPU type for Bazel builds.\r\n// NOTE: Does not affect the Makefile build target API (yet), which currently\r\n// assumes armeabi-v7a. If building with make, changing this will require\r\n// editing the Makefile as well.\r\ndef cpuType = 'armeabi-v7a'\r\n\r\n// Output directory in the local directory for packaging into the APK.\r\ndef nativeOutDir = 'libs/' + cpuType\r\n\r\n// Default to building with Bazel and override with make if requested.\r\ndef nativeBuildRule = 'buildNativeBazel'\r\ndef demoLibPath = '../../../bazel-bin/tensorflow/examples/android/libtensorflow_demo.so'\r\ndef inferenceLibPath = '../../../bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so'\r\nif (buildWithMake) {\r\n    nativeBuildRule = 'buildNativeMake'\r\n    demoLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_demo.so'\r\n    inferenceLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_inference.so'\r\n}\r\n\r\n// Defines the NDK location for Makefile builds. Does *not* affect Bazel builds.\r\n// Override with your absolute NDK location if this fails to get the location\r\n// automatically.\r\ndef makeNdkRoot = System.getenv('NDK_ROOT')\r\n\r\n// If building with Bazel, this is the location of the bazel binary.\r\n// NOTE: Bazel does not yet support building for Android on Windows,\r\n// so in this case the Makefile build must be used as described above.\r\ndef bazelLocation = 'c://tools//msys64//usr//bin//bash.exe'\r\n\r\nproject.buildDir = 'gradleBuild'\r\ngetProject().setBuildDir('gradleBuild')\r\n\r\n// import DownloadModels task\r\nproject.ext.ASSET_DIR = projectDir.toString() + '/assets'\r\nproject.ext.TMP_DIR   = project.buildDir.toString() + '/downloads'\r\n\r\nbuildscript {\r\n    repositories {\r\n        jcenter()\r\n    }\r\n\r\n    dependencies {\r\n        classpath 'com.android.tools.build:gradle:2.3.1'\r\n    }\r\n}\r\n\r\napply plugin: 'com.android.application'\r\n\r\nandroid {\r\n    compileSdkVersion 23\r\n    buildToolsVersion \"25.0.2\"\r\n\r\n    lintOptions {\r\n        abortOnError false\r\n    }\r\n\r\n    sourceSets {\r\n        main {\r\n            // TensorFlow Java API sources.\r\n            java {\r\n                srcDir '../../java/src/main/java'\r\n                exclude '**/examples/**'\r\n            }\r\n\r\n            // Android TensorFlow wrappers, etc.\r\n            java {\r\n                srcDir '../../contrib/android/java'\r\n            }\r\n\r\n            // Android demo app sources.\r\n            java {\r\n                srcDir 'src'\r\n            }\r\n\r\n            manifest.srcFile 'AndroidManifest.xml'\r\n            resources.srcDirs = ['src']\r\n            aidl.srcDirs = ['src']\r\n            renderscript.srcDirs = ['src']\r\n            res.srcDirs = ['res']\r\n            assets.srcDirs = [project.ext.ASSET_DIR]\r\n            jniLibs.srcDirs = ['libs']\r\n        }\r\n\r\n        debug.setRoot('build-types/debug')\r\n        release.setRoot('build-types/release')\r\n    }\r\n}\r\n\r\ntask buildNativeBazel(type: Exec) {\r\n    workingDir '../../..'\r\n    commandLine bazelLocation, 'build', '-c', 'opt',  \\\r\n         'tensorflow/examples/android:tensorflow_native_libs',  \\\r\n         '--crosstool_top=//external:android/crosstool',  \\\r\n         '--cpu=' + cpuType,  \\\r\n         '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain'\r\n}\r\n\r\ntask buildNativeMake(type: Exec) {\r\n    //environment \"NDK_ROOT\", makeNdkRoot\r\n    // Tip: install ccache and uncomment the following to speed up\r\n    // builds significantly.\r\n     //environment \"CC_PREFIX\", 'ccache'\r\n    workingDir '../../..'\r\n    commandLine = ['cmd', '/C', 'start', 'makefile_helper.bat']\r\n   //commandLine 'tensorflow/contrib/makefile/build_all_android.sh',  \\\r\n       //  '-s',  \\\r\n      //   'tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in',  \\\r\n    //     '-t',  \\\r\n   //      'libtensorflow_inference.so libtensorflow_demo.so'  \\\r\n         //, '-T'  // Uncomment to skip protobuf and speed up subsequent builds.\r\n}\r\n\r\nallprojects {\r\n    repositories {\r\n        jcenter()\r\n        flatDir {\r\n            dirs 'libs'\r\n        }\r\n    }\r\n}\r\n\r\ndependencies {\r\n    compile(name:'tensorflow', ext:'aar')\r\n}\r\n\r\n\r\ntask copyNativeLibs(type: Copy) {\r\n    from demoLibPath\r\n    from inferenceLibPath\r\n    into nativeOutDir\r\n    duplicatesStrategy = 'include'\r\n    dependsOn nativeBuildRule\r\n    fileMode 0644\r\n}\r\n\r\n\r\ntasks.whenTaskAdded { task ->\r\n    if (task.name == 'assembleDebug') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n    if (task.name == 'assembleRelease') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n}\r\n\r\n// Download default models; if you wish to use your own models then\r\n// place them in the \"assets\" directory and comment out this line.\r\napply from: \"download-models.gradle\"\r\n\r\n\r\n\r\nAny ideas?  I think you've almost got me there heh, sorry for how rookie I am with TF in general.", "@vyse8 My guess is that you're running on an arm64 device, and since the app finds an arm64 variant of libtensorflow_inference.so (provided by tensorflow.aar) it expects to find the same for libtensorflow_demo.so. In this case you can fix this by changing  cpuType from \"armeabi-v7a\" to \"arm64-v8a\".\r\n\r\nAlso note that you appear to now be manually building libtensorflow_inference.so in addition to grabbing it from the aar -- so you might want to go back and comment out those references or remove the dependency on the aar. Otherwise the behavior might get confusing.", "@andrewharp T**hanks again.  I went ahead and made the following changes to my build.gradle file(commented out the lib references and changed cpuType):**\r\n\r\ndef cpuType = 'arm64-v8a'\r\n\r\n// Output directory in the local directory for packaging into the APK.\r\ndef nativeOutDir = 'libs/' + cpuType\r\n\r\n// Default to building with Bazel and override with make if requested.\r\ndef nativeBuildRule = 'buildNativeBazel'\r\ndef demoLibPath = '../../../bazel-bin/tensorflow/examples/android/libtensorflow_demo.so'\r\ndef inferenceLibPath = '../../../bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so'\r\nif (buildWithMake) {\r\n    nativeBuildRule = 'buildNativeMake'\r\n    //demoLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_demo.so'\r\n    //inferenceLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_inference.so'\r\n}\r\n\r\n\r\n\r\nI'm hitting the same error with the Unsatisfied Link Error upon launch, however it looks like this might be related to my jar file.  I don't think I ever added the JAR file for tensorflow but I remember it being in one of my downloads.  Is this necessary for the AAR implementation?  If so, where should this JAR file be placed?  I can't say for sure if that's throwing this error but it seems to be the general consensus online.", "@vyse8 You don't need the jar if you're using the aar (nor do you need to refer to the tensorflow/contrib/android or tensorflow/java directories from gradle.build). The aar contains the jar + libtensorflow_inference.so.\r\n\r\nYou're building with make, so you definitely do not want to comment out the make demoLibPath. Otherwise Gradle won't be able to find libtensorflow_demo.so to place in the APK once it's built.\r\n\r\nYou can double check what's actually being put into the APK with `unzip -v tensorflow_demo.apk` (or whatever the actual apk filename is)\r\n", "@andrewharp Thanks.  Okay so I added the demoLibPath references back in but the same error of Unsatisfied Link Error seen above is still occurring.  I'm thinking my file structure is the culprit since it can't find the SO files.  Is this appropriate?\r\n\r\nMy aar file is located in C:\\Users\\mattv\\OneDrive\\Documents\\GitHub\\tensorflow\\tensorflow\\examples\\android\\libs\r\n\r\nWithin Android Studio, I don't see my libs folder but it looks like there is a jniLibs folder with the same content (aar file).  I attempted to change my def nativeOutDir to jniLibs + cputype but encoutered the same issue so I've since changed it back.  Here is the full error that I see in the Android Monitor:\r\n\r\n04-21 18:40:57.458 4041-4041/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                   Process: org.tensorflow.demo, PID: 4041\r\n                                                                   java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.demo-1/base.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_dependencies_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_0_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_1_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_2_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_3_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_4_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_5_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_6_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_7_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_8_apk.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_9_apk.apk\"],nativeLibraryDirectories=[/data/app/org.tensorflow.demo-1/lib/x86, /data/app/org.tensorflow.demo-1/base.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_dependencies_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_0_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_1_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_2_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_3_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_4_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_5_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_6_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_7_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_8_apk.apk!/lib/x86, /data/app/org.tensorflow.demo-1/split_lib_slice_9_apk.apk!/lib/x86, /vendor/lib, /system/lib]]] couldn't find \"libtensorflow_demo.so\"\r\n                                                                       at java.lang.Runtime.loadLibrary(Runtime.java:367)\r\n                                                                       at java.lang.System.loadLibrary(System.java:1076)\r\n                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.<clinit>(TensorFlowImageClassifier.java:36)\r\n                                                                       at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:0)\r\n                                                                       at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:122)\r\n                                                                       at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:159)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:421)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:428)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\r\n                                                                       at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\r\n                                                                       at android.view.TextureView.getHardwareLayer(TextureView.java:368)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:15151)\r\n                                                                       at android.view.View.draw(View.java:15948)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:15169)\r\n                                                                       at android.view.View.draw(View.java:15948)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                                                                       at android.view.View.draw(View.java:16181)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:15174)\r\n                                                                       at android.view.View.draw(View.java:15948)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:15169)\r\n                                                                       at android.view.View.draw(View.java:15948)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                                                                       at android.view.View.updateDisplayListIfDirty(View.java:15169)\r\n                                                                       at android.view.View.draw(View.java:15948)\r\n                                                                       at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                                                                       at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                                                                   \tat android.view.View.draw(View.java:1618\r\n\r\n\r\nThanks.", "@vyse8 What are the contents of the apk as seen with unzip -v?\r\n\r\nI'm not sure why you'd have an aar in jnilibs unless you put it there yourself (and you probably shouldn't).\r\n\r\nAll you should need to do is change cpuType to \"armeabi-v7a\". The code is already set up to composite the directory name correctly for you.", "@andrewharp From what I can tell it looks like anything that is placed in my libs folder gets moved to jnilibs. I believe this occurs because of the following lines in build.gradle\r\n\r\nmanifest.srcFile 'AndroidManifest.xml'\r\n            resources.srcDirs = ['src']\r\n            aidl.srcDirs = ['src']\r\n            renderscript.srcDirs = ['src']\r\n            res.srcDirs = ['res']\r\n            assets.srcDirs = [project.ext.ASSET_DIR]\r\n            jniLibs.srcDirs = ['libs']\r\n\r\n\r\nI went ahead and changed the cputype back to armeabi-v7a but still getting the libtensorflow_demo.so couldn't be found error.  I did find some additional info by using the Analyze apk feature in Android Studio.  It looks like at the root there is a lib folder, and within that folder are the types (x86, x86_64, arm64_v8a, armeabi-v7a).  However, within those folders, the only file listed in each one is libtensorflow_inference.so.  This would explain why it is having difficulty finding the libtensorflow_demo.so.  Should this file be located in the same directory ideally? I don't see it anywhere within the apk.\r\n\r\nAlso, if I were wanting to build the sample Hello World tensorflow example within Android, would I only need the latest inference file?  Thank you so much for all of this help btw.", "@vyse8 In your situation I'd suggest just ignoring that the AAR even exists and following only the instructions in the [original workaround](https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-285208600). It's hard to tell what's going on if we don't even know what's in the APK (again, you can find out with unzip -v).", "@andrewharp my apologies, I was unfamiliar with unzip so I equated it to Analyze APK, I see now that it provides a better output.  Here it is:\r\n\r\n Length   Method    Size  Cmpr    Date    Time   CRC-32   Name\r\n--------  ------  ------- ---- ---------- ----- --------  ----\r\n    4148  Defl:N     1184  72% 1980-00-00 00:00 745f1818  AndroidManifest.xml\r\n     773  Defl:N      604  22% 1980-00-00 00:00 1dec76f1  META-INF/CERT.RSA\r\n    4944  Defl:N     2282  54% 1980-00-00 00:00 bb9069fb  META-INF/CERT.SF\r\n    4882  Defl:N     2241  54% 1980-00-00 00:00 2fd450e2  META-INF/MANIFEST.MF\r\n     476  Defl:N      298  37% 1980-00-00 00:00 b6b9a342  assets/BUILD\r\n   10492  Defl:N     5554  47% 1980-00-00 00:00 bdb9b97a  assets/imagenet_comp_graph_label_strings.txt\r\n   42335  Defl:N     4901  88% 1980-00-00 00:00 90dd4646  assets/multibox_location_priors.txt\r\n18556306  Defl:N 16704579  10% 1980-00-00 00:00 cdeb207e  assets/multibox_model.pb\r\n  563897  Defl:N   270926  52% 1980-00-00 00:00 7d7f4ff3  assets/stylize_quantized.pb\r\n53884595  Defl:N 50111288   7% 1980-00-00 00:00 6d3b8d15  assets/tensorflow_inception_graph.pb\r\n  131978  Stored   131978   0% 1980-00-00 00:00 3228d14c  assets/thumbnails/style0.jpg\r\n  113303  Stored   113303   0% 1980-00-00 00:00 13596907  assets/thumbnails/style1.jpg\r\n  151688  Stored   151688   0% 1980-00-00 00:00 a3e9c196  assets/thumbnails/style10.jpg\r\n  181154  Stored   181154   0% 1980-00-00 00:00 adb4f0a3  assets/thumbnails/style11.jpg\r\n  107888  Stored   107888   0% 1980-00-00 00:00 79dbb7d0  assets/thumbnails/style12.jpg\r\n  198586  Stored   198586   0% 1980-00-00 00:00 c97505a9  assets/thumbnails/style13.jpg\r\n  104584  Stored   104584   0% 1980-00-00 00:00 5832132a  assets/thumbnails/style14.jpg\r\n  152269  Stored   152269   0% 1980-00-00 00:00 005ad28f  assets/thumbnails/style15.jpg\r\n  168251  Stored   168251   0% 1980-00-00 00:00 6bccba6c  assets/thumbnails/style16.jpg\r\n  146736  Stored   146736   0% 1980-00-00 00:00 132cbdcb  assets/thumbnails/style17.jpg\r\n  173496  Stored   173496   0% 1980-00-00 00:00 9a6a37a7  assets/thumbnails/style18.jpg\r\n  227300  Stored   227300   0% 1980-00-00 00:00 a11b35b4  assets/thumbnails/style19.jpg\r\n  140121  Stored   140121   0% 1980-00-00 00:00 34147cc2  assets/thumbnails/style2.jpg\r\n  134506  Stored   134506   0% 1980-00-00 00:00 3df5a425  assets/thumbnails/style20.jpg\r\n  138136  Stored   138136   0% 1980-00-00 00:00 0754c15d  assets/thumbnails/style21.jpg\r\n  168478  Stored   168478   0% 1980-00-00 00:00 25998a8e  assets/thumbnails/style22.jpg\r\n  108525  Stored   108525   0% 1980-00-00 00:00 91f2c6b0  assets/thumbnails/style23.jpg\r\n  131414  Stored   131414   0% 1980-00-00 00:00 ead622b6  assets/thumbnails/style24.jpg\r\n  190320  Stored   190320   0% 1980-00-00 00:00 b39eb2ce  assets/thumbnails/style25.jpg\r\n  181100  Stored   181100   0% 1980-00-00 00:00 a86a5141  assets/thumbnails/style3.jpg\r\n  181565  Stored   181565   0% 1980-00-00 00:00 9dfc14ff  assets/thumbnails/style4.jpg\r\n  162432  Stored   162432   0% 1980-00-00 00:00 2e8636d3  assets/thumbnails/style5.jpg\r\n  167919  Stored   167919   0% 1980-00-00 00:00 560fdd11  assets/thumbnails/style6.jpg\r\n   97875  Stored    97875   0% 1980-00-00 00:00 684b2c5b  assets/thumbnails/style7.jpg\r\n  121177  Stored   121177   0% 1980-00-00 00:00 d552dee2  assets/thumbnails/style8.jpg\r\n  208059  Stored   208059   0% 1980-00-00 00:00 83817fe1  assets/thumbnails/style9.jpg\r\n     740  Defl:N      471  36% 1980-00-00 00:00 e766d2a8  classes.dex\r\n   46084  Defl:N    23500  49% 1980-00-00 00:00 e4185116  classes2.dex\r\n14990000  Defl:N  4886533  67% 1980-00-00 00:00 5177fbad  lib/arm64-v8a/libtensorflow_inference.so\r\n 9553964  Defl:N  3998890  58% 1980-00-00 00:00 2f76c5e6  lib/armeabi-v7a/libtensorflow_inference.so\r\n15087484  Defl:N  4981805  67% 1980-00-00 00:00 bdf2df5f  lib/x86/libtensorflow_inference.so\r\n15022832  Defl:N  4885743  68% 1980-00-00 00:00 41fd2ecb  lib/x86_64/libtensorflow_inference.so\r\n     234  Stored      234   0% 1980-00-00 00:00 c560f719  res/drawable-hdpi-v4/tile.9.png\r\n    1129  Stored     1129   0% 1980-00-00 00:00 02df764b  res/drawable-xxhdpi-v4/ic_action_info.png\r\n   10714  Stored    10714   0% 1980-00-00 00:00 fcef8ddf  res/drawable-xxhdpi-v4/ic_launcher.png\r\n     404  Defl:N      228  44% 1980-00-00 00:00 97e57137  res/layout/activity_camera.xml\r\n     960  Defl:N      395  59% 1980-00-00 00:00 0df7a886  res/layout/camera_connection_fragment.xml\r\n    1356  Defl:N      501  63% 1980-00-00 00:00 fd74248b  res/layout/camera_connection_fragment_stylize.xml\r\n     784  Defl:N      319  59% 1980-00-00 00:00 3cce775d  res/layout/camera_connection_fragment_tracking.xml\r\n    4616  Stored     4616   0% 1980-00-00 00:00 6c367d8a  resources.arsc\r\n--------          -------  ---                            -------\r\n131783009         89887795  32%                            50 files", "Ok, if you really want to use the AAR try a directory different than libs/ for it (it could be \"foobarbaz\" as long as you update the dirs directive). \"libs/\" may be conflicting with where gradle expects to find the native libs.\r\n\r\nYou might also try the [this option](https://github.com/tensorflow/tensorflow/issues/6385#issuecomment-282973536) for manually placing the prebuilts in the right place.\r\n\r\nWe'll have cmake working for libtensorflow_demo.so possibly next week, so really this is not worth spending too much time on.", "@andrewharp thanks for your assistance. Now that I scroll up I realize I hit the same issue that ramarajan09 had 22 days ago prior to the AAR.  It looks like we both got the same exact error message so I'm not sure what is causing that for us.  From further analysis of the apk it looks like dragging it to an arbitrary created libs folder doesn't signal to the APK to include it in the build.  Do you know how these libs can be included as I think that would solve both of our issues?  \r\n\r\nOne question that I think I might be stumbling on is the libs path.  What is the recommendation as mine is currently set to:\r\n\r\ndef demoLibPath = '../libs/armeabi-v7a/libtensorflow_demo.so'\r\ndef inferenceLibPath = '../libs/armeabi-v7a/libtensorflow_inference.so'\r\n\r\nThe full path in reality is C:\\Users\\mattv\\OneDrive\\Documents\\GitHub\\tensorflow\\tensorflow\\examples\\android\\libs\\armeabi-v7a.\r\n\r\nWould this be causing the issue?", "@vyse8 \r\nThe paths you're quoting there are where the files need to be copied from after they are built. If you put the .so files in their destination manually they're irrelevant. As you have them now it seems to be pointing to tensorflow/examples/libs, which is incorrect both because that path doesn't exist (note the ..), and because I assume you meant tensorflow/examples/_android_/libs, which is actually the destination, not the source (which would be wherever bazel/make create them).\r\n\r\nTo clarify:\r\n\r\nThe _AAR_, if you use it, needs to go in an arbitrary folder that matches \"aardir\" in:\r\n```\r\nallprojects {\r\nrepositories {\r\njcenter()\r\nflatDir {\r\ndirs 'aardir'\r\n}\r\n}\r\n}\r\n```\r\n\r\nThe _.so files_, if placed manually, need to be put in the arbitrary folder that matches the \"libs\" in:\r\n`        jniLibs.srcDirs = ['libs']\r\n`\r\n\r\nThe only restriction is possibly that these two dirs should be different -- my original instruction to place the AAR in libs/ may have been incorrect (I actually tested using a different directory).\r\n\r\nNote that when AS shows you the project outline visually, I think it shows up as \"jniLibs\", though the actual folder on your HD is \"libs\".", "I used the prebuilt libraries (copied the folder `libs `from the prebuilt APK as is in the root of the project) and modified the script as follows (I commented unnecessary tasks) and it is now building OK.\r\n```\r\n// This file provides basic support for building the TensorFlow demo\r\n// in Android Studio with Gradle.\r\n//\r\n// Note that Bazel is still used by default to compile the native libs,\r\n// and should be installed at the location noted below. This build file\r\n// automates the process of calling out to it and copying the compiled\r\n// libraries back into the appropriate directory.\r\n//\r\n// Alternatively, experimental support for Makefile builds is provided by\r\n// setting buildWithMake below to true. This will allow building the demo\r\n// on Windows machines, but note that full equivalence with the Bazel\r\n// build is not yet guaranteed. See comments below for caveats and tips\r\n// for speeding up the build, such as as enabling ccache.\r\n\r\n// Set to true to build with make.\r\n// NOTE: Running a make build will cause subsequent Bazel builds to *fail*\r\n// unless the contrib/makefile/downloads/ and gen/ dirs are deleted afterwards.\r\ndef buildWithMake = false\r\n\r\n// Controls output directory in APK and CPU type for Bazel builds.\r\n// NOTE: Does not affect the Makefile build target API (yet), which currently\r\n// assumes armeabi-v7a. If building with make, changing this will require\r\n// editing the Makefile as well.\r\ndef cpuType = 'armeabi-v7a'\r\n\r\n// Output directory in the local directory for packaging into the APK.\r\ndef nativeOutDir = 'libs/' + cpuType\r\n\r\n// Default to building with Bazel and override with make if requested.\r\n//def nativeBuildRule = 'buildNativeBazel'\r\ndef demoLibPath = 'libs/' + cpuType + 'libtensorflow_demo.so'\r\ndef inferenceLibPath = 'libs/' + cpuType + 'libtensorflow_inference.so'\r\n//if (buildWithMake) {\r\n//    nativeBuildRule = 'buildNativeMake'\r\n//    demoLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_demo.so'\r\n//    inferenceLibPath = '../../../tensorflow/contrib/makefile/gen/lib/libtensorflow_inference.so'\r\n//}\r\n\r\n// Defines the NDK location for Makefile builds. Does *not* affect Bazel builds.\r\n// Override with your absolute NDK location if this fails to get the location\r\n// automatically.\r\ndef makeNdkRoot = System.getenv('NDK_ROOT')\r\n\r\n// If building with Bazel, this is the location of the bazel binary.\r\n// NOTE: Bazel does not yet support building for Android on Windows,\r\n// so in this case the Makefile build must be used as described above.\r\ndef bazelLocation = '/usr/local/bin/bazel'\r\n\r\nproject.buildDir = 'gradleBuild'\r\ngetProject().setBuildDir('gradleBuild')\r\n\r\n// import DownloadModels task\r\nproject.ext.ASSET_DIR = projectDir.toString() + '/assets'\r\nproject.ext.TMP_DIR   = project.buildDir.toString() + '/downloads'\r\n\r\nbuildscript {\r\n    repositories {\r\n        jcenter()\r\n    }\r\n\r\n    dependencies {\r\n        classpath 'com.android.tools.build:gradle:2.3.1'\r\n    }\r\n}\r\n\r\napply plugin: 'com.android.application'\r\n\r\nandroid {\r\n    compileSdkVersion 23\r\n    buildToolsVersion \"25.0.2\"\r\n\r\n    lintOptions {\r\n        abortOnError false\r\n    }\r\n\r\n    sourceSets {\r\n        main {\r\n            // TensorFlow Java API sources.\r\n            java {\r\n                srcDir '../../java/src/main/java'\r\n                exclude '**/examples/**'\r\n            }\r\n\r\n            // Android TensorFlow wrappers, etc.\r\n            java {\r\n                srcDir '../../contrib/android/java'\r\n            }\r\n\r\n            // Android demo app sources.\r\n            java {\r\n                srcDir 'src'\r\n            }\r\n\r\n            manifest.srcFile 'AndroidManifest.xml'\r\n            resources.srcDirs = ['src']\r\n            aidl.srcDirs = ['src']\r\n            renderscript.srcDirs = ['src']\r\n            res.srcDirs = ['res']\r\n            assets.srcDirs = [project.ext.ASSET_DIR]\r\n            jniLibs.srcDirs = ['libs']\r\n        }\r\n\r\n        debug.setRoot('build-types/debug')\r\n        release.setRoot('build-types/release')\r\n    }\r\n}\r\n\r\n//task buildNativeBazel(type: Exec) {\r\n//    workingDir '../../..'\r\n//    commandLine bazelLocation, 'build', '-c', 'opt',  \\\r\n//         'tensorflow/examples/android:tensorflow_native_libs',  \\\r\n//         '--crosstool_top=//external:android/crosstool',  \\\r\n//         '--cpu=' + cpuType,  \\\r\n//         '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain'\r\n//}\r\n\r\n//task buildNativeMake(type: Exec) {\r\n//    environment \"NDK_ROOT\", makeNdkRoot\r\n//    // Tip: install ccache and uncomment the following to speed up\r\n//    // builds significantly.\r\n//    // environment \"CC_PREFIX\", 'ccache'\r\n//    workingDir '../../..'\r\n//    commandLine 'tensorflow/contrib/makefile/build_all_android.sh',  \\\r\n//         '-s',  \\\r\n//         'tensorflow/contrib/makefile/sub_makefiles/android/Makefile.in',  \\\r\n//         '-t',  \\\r\n//         'libtensorflow_inference.so libtensorflow_demo.so'  \\\r\n//         //, '-T'  // Uncomment to skip protobuf and speed up subsequent builds.\r\n//}\r\n\r\n\r\ntask copyNativeLibs(type: Copy) {\r\n    from demoLibPath\r\n    from inferenceLibPath\r\n    into nativeOutDir\r\n    duplicatesStrategy = 'include'\r\n//    dependsOn nativeBuildRule\r\n    fileMode 0644\r\n}\r\n\r\n\r\ntasks.whenTaskAdded { task ->\r\n    if (task.name == 'assembleDebug') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n    if (task.name == 'assembleRelease') {\r\n        task.dependsOn 'copyNativeLibs'\r\n    }\r\n}\r\n\r\n// Download default models; if you wish to use your own models then\r\n// place them in the \"assets\" directory and comment out this line.\r\n//apply from: \"download-models.gradle\"\r\n\r\n\r\n```", "Another update: libtensorflow_demo.so is now (as soon as the above commits get pushed to master) an optional library. If not present, YUV->RGB color conversion will be performed in Java (possibly slower), and object tracking will be disabled in the TF Detect demo, but otherwise things will work as usual.\r\n\r\nThis means the demo can be built on Windows with just the [prebuilt tensorflow.aar](http://ci.tensorflow.org/view/Nightly/job/nightly-android/lastSuccessfulBuild/artifact/out/) dropped into your project folder. We're in the process of uploading this to jcenter, at which point it will only take a couple of lines of Gradle config to add a TF dep to Android projects.\r\n\r\nFull cmake support for building TF is also still on the roadmap (no timeline for this yet though).", "The AAR build of TF is now [live](https://bintray.com/google/tensorflow/tensorflow-android)! This is labeled 1.2.0 preview, but note that this is merely to indicate it comes between 1.1.0 and 1.2.0 (the 1.2.0 release branch has not even been cut yet). We'll be updating it on every point release going forward.\r\n\r\nYou can enable it in your Android app by adding the following to build.gradle:\r\n```\r\nrepositories {\r\n    jcenter()\r\n}\r\ndependencies {\r\n    compile 'org.tensorflow:tensorflow-android:+'\r\n}\r\n```\r\n\r\nFor the official TF demo, you'll also want to remove the following to prevent redundant Java classes from being included:\r\n```\r\n            // TensorFlow Java API sources.\r\n            java {\r\n                srcDir '../../java/src/main/java'\r\n                exclude '**/examples/**'\r\n            }\r\n\r\n            // Android TensorFlow wrappers, etc.\r\n            java {\r\n                srcDir '../../contrib/android/java'\r\n            }\r\n```\r\nAnd remove the reference to the locally-built inference lib as well:\r\n`from inferenceLibPath`\r\n\r\nIf using an arm64 device, change cpuType from armeabi-v7a to arm64-v8 (since you'll need libtensorflow_demo.so to be built at the highest architecture target your device can use that the AAR provides libtensorflow_inference.so at).\r\n\r\nIf using bazel to build, change the following in buildNativeBazel:\r\n`'tensorflow/examples/android:tensorflow_native_libs'` -> `'tensorflow/examples/android:libtensorflow_demo.so'`\r\n\r\nand if using make, change the following in buildNativeMake:\r\n         `'libtensorflow_inference.so libtensorflow_demo.so'` -> `'libtensorflow_demo.so'`\r\n\r\nOnce fd69bb2 and efa08d8 are pushed to master (which will make libtensorflow_demo.so an optional lib), you could also just remove the native lib dependencies altogether.\r\n\r\nWe'll be cleaning this up to make it easier to switch soon, and also add a cmake build for libtensorflow_demo.so", "Hi @andrewharp Just tried to follow some of the advice in here to get this running on Windows. I've found that by far the easiest method was to just download the nightly binaries and stick them in. I tried to use your above method to use the aar but I got an error saying that the compile method was not available. Seemed to suggest it was a gradle issue. Either way I couldn't fix and resorted to sticking the prebuilts in. Not sure if you've come across this?\r\n\r\nOn another note, this thread has lots of good information and is more detailed than the documentation. Are there plans to get the documentation up to spec to contain all this information? It's slightly unclear atm, especially regarding Windows. I did notice that you said some progress was being made. Thanks for your help.", "@jubjamie Can you paste the exact error message please? Haven't seen that one before.\r\n\r\nThe README.md files have recently been updated with AAR instructions, and I'm working on a detailed getting started guide that will cover all of the approaches to building and integrating TensorFlow into apps.", "@andrewharp I've got around the error so not able to get it up again I don't think. It seems to be a common error within android so i've put a very **similar** message below. Most solutions appear to suggest removing the compile line however that's for those specific stack overflow situations. Here it's not so simple.\r\n\r\n```\r\nError:(9, 0) Gradle DSL method not found: 'compile()'\r\nPossible causes:The project 'AlexTest' may be using a version of Gradle that does not contain the method.\r\nThe build file may be missing a Gradle plugin.\r\nlink: Apply Gradle plugin\r\n```\r\n\r\nIn my scenario I obviously had gradle installed etc. I noticed that in the latest release there is another compile call at the very end of build.gradle which it didn't pick up on (but I wonder if that is because it didn't get that far?). This was using android on Windows 10.\r\n\r\nI was about to open another issue regarding performance of the android app using custom tensorflow for poets models (takes 5 times as long) but not sure it's really a bug or a stack overflow question. Can I email you? Otherwise I can open a stack overflow and paste the link here? @petewarden here too perhaps?", "@jubjamie Ok -- I assume this sort of thing can be cleared up in Android Studio by just clicking to install/upgrade whatever Gradle-related thing it asks you to? Or were you building from the command line?\r\n\r\nRe Inception: the performance of v3 (from the TF for Poets guide) is known to be slower than v1 (aka 5h, bundled with the demo). It's a much larger model so some difference is expected. Speeding it up is more of a Stack Overflow topic, but if you're interested you might try seeing what ops are taking up the most time on your device with tensorflow/tools/benchmark:benchmark_model.", "As a note on performance, I've recently updated the TensorFlow for Poets script with the option to save out MobileNet models, which can be a lot smaller and faster than the default Inception v3, at the cost of some accuracy:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/tutorials/image_retraining.md#other-model-architectures\r\n\r\nIt looks like the original issue is fixed, or at least obsolete now though? I'm closing this for the moment, please reopen with more information if that's incorrect.", "The link referring to the [AAR build](https://bintray.com/google/tensorflow/tensorflow-android) is now giving a 404. Has the link for this windows build method now changed?", "So still no Windows support after a year when this issue was first brought up?  Just tried to build the demo. \r\n\r\norg.gradle.api.tasks.TaskExecutionException: Execution failed for task ':buildNativeBazel'.\r\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:100)\r\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:70)\r\n\tat org.gradle.api.internal.tasks.execution.OutputDirectoryCreatingTaskExecuter.execute(OutputDirectoryCreatingTaskExecuter.java:51)\r\n\tat \r\n...\r\nCaused by: java.io.IOException: Cannot run program \"/usr/local/bin/bazel\" (in directory \"C:\\GitProjects\\tensorflow\"): CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat net.rubygrapefruit.platform.internal.DefaultProcessLauncher.start(DefaultProcessLauncher.java:25)\r\n\t... 10 more\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:386)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:137)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 11 more\r\n\r\n", "there still no bazel support on windows, you can use cmake, but still its performance still no guarantee though", "Hey guys, I am trying to include some experimental operators located in the contrib folders(tensor_forest) to a tensorflow build so that I can load a tensorflow model in my xamarin.android project using these operators without an error that states that the operators are not recognized. \r\n\r\nIn order to do this I thought I started with trying to build tensorflow android throught this command: \r\n`bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cxxopt=-std=c++11    --cpu=armeabi-v7a --verbose_failures`\r\n\r\nBut I keep getting these kinds of errors \r\n`ERROR: C:/users/cuypeni/_bazel_cuypeni/jumq43mw/external/com_google_absl/absl/numeric/BUILD.bazel:25:1: C++ compilation of rule '@com_google_absl//absl/numeric:int128' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed: clang failed: error executing command\r\n  cd C:/users/cuypeni/_bazel_cuypeni/jumq43mw/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\Users\\cuypeni\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\cuypeni\\bin;C:\\Program Files\\Anaconda3;C:\\Program Files\\Anaconda3\\Library\\mingw-w64\\bin;C:\\Program Files\\Anaconda3\\Library\\usr\\bin;C:\\Program Files\\Anaconda3\\Library\\bin;C:\\Program Files\\Anaconda3\\Scripts;C:\\Python37\\Scripts;C:\\Python37;C:\\Program Files\\Docker\\Docker\\Resources\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files (x86)\\Shoreline Communications\\ShoreWare Client;C:\\Program Files\\nodejs;C:\\Program Files (x86)\\Microsoft SQL Server\\Client SDK\\ODBC\\130\\Tools\\Binn;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\Tools\\Binn;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\DTS\\Binn;C:\\Program Files (x86)\\Microsoft SQL Server\\140\\Tools\\Binn\\ManagementStudio;C:\\Program Files\\dotnet;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn;C:\\Program Files\\Git\\cmd;C:\\ProgramData\\chocolatey\\bin;C:\\Users\\cuypeni\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\cuypeni\\AppData\\Roaming\\npm;C:\\Users\\cuypeni\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\ProgramData\\chocolatey\\lib\\msys2;C:\\msys64\\usr\\bin;C:\\Program Files\\Git\\usr\\bin\\vendor_perl;C:\\Program Files\\Git\\usr\\bin\\core_perl\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Program Files/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Program Files/Anaconda3/lib/site-packages\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -D__ANDROID_API__=14 -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target armv7-none-linux-androideabi -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/bin/external/bazel_tools -w -std=c++11 -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare --sysroot=external/androidndk/ndk/platforms/android-14/arch-arm -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_absl/absl/numeric/int128.cc -o bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(239): CreateProcessW(\"C:\\users\\cuypeni\\_bazel_cuypeni\\jumq43mw\\execroot\\org_tensorflow\\external\\androidndk\\ndk\\toolchains\\llvm\\prebuilt\\windows-x86_64\\bin\\clang\" -D__ANDROID_API__=14 -isystemexternal/androidndk/ndk/sysroot/usr/include/arm-linux-androideabi -target armv7-none-linux-androideabi -march=armv7-a -mfloat-abi=softfp -mfpu=vfpv3-d16 -gcc-toolchain external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/windows-x86_64 -fpic -ffunction-sections -funwind-tables -fstack-protector-strong -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -no-canonical-prefixes -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/genfiles/external/com_google_absl -iquote bazel-out/armeabi-v7a-opt/bin/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/genfiles/external/bazel_tools -iquote bazel-out/armeabi-v7a-opt/bin/external/bazel_tools -w -std=c++11 -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-sign-compare --sysroot=external/androidndk/ndk/platforms/android-14/arch-arm -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_absl/absl/numeric/int128.cc -o bazel-out/armeabi-v7a-opt/bin/external/com_google_absl/absl/numeric/_objs/int128/int128.o): The system cannot find the file specified.\r\n`\r\n\r\nIs there still no bazel support for windows? \r\nPlease some help I'm  stuck on this ... ", "Anmolk22  hi can you please tell how you should build libtensorflow_infrence.so file \r\ni am using this command bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a but it give me error\r\nERROR: C:/users/sabir-pc/_bazel_sabir-pc/zp6kp3ov/external/com_google_absl/absl/strings/BUILD.bazel:32:1: C++ compilation of rule '@com_google_absl//absl/strings:strings' failed (Exit 1)\r\nIn file included from external/com_google_absl/absl/strings/str_cat.cc:15:\r\nIn file included from external/com_google_absl\\absl/strings/str_cat.h:62:\r\nIn file included from external/com_google_absl\\absl/strings/numbers.h:37:\r\nIn file included from external/com_google_absl\\absl/numeric/int128.h:29:\r\nIn file included from external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include\\cmath:305:\r\nIn file included from external/androidndk/ndk/sources/android/support/include\\math.h:32\r\n3 errors generated.\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 212.615s, Critical Path: 18.57s\r\nINFO: 213 processes: 213 local.\r\nFAILED: Build did NOT complete successfully"]}, {"number": 6384, "title": "Error in einsum with unspecified dimensions", "body": "Code:\r\n\r\n\r\n    import tensorflow as tf\r\n    \r\n    x = tf.placeholder(dtype=tf.int32)\r\n    \r\n    A = tf.random_normal([13, x, 512])\r\n    B = tf.random_normal([512, 3])\r\n    C = tf.einsum('ijk,kl->ijl', A, B)\r\n    \r\n    with tf.Session() as sess:\r\n        print(sess.run(C, {x: 7}))\r\n\r\nOutput:\r\n\r\n    Traceback (most recent call last):\r\n      File \"/home/noam/code/test/test_einsum.py\", line 7, in <module>\r\n        C = tf.einsum('ijk,kl->ijl', A, B)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/ops/special_math_ops.py\", line 212, in einsum\r\n        axes_to_sum)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/ops/special_math_ops.py\", line 341, in _einsum_reduction\r\n        product = _reshape_if_necessary(product, uncompacted_shape)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/ops/special_math_ops.py\", line 366, in _reshape_if_necessary\r\n        return array_ops.reshape(tensor, new_shape)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2448, in reshape\r\n        name=name)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 493, in apply_op\r\n        raise err\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 490, in apply_op\r\n        preferred_dtype=default_dtype)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 669, in convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 176, in _constant_tensor_conversion_function\r\n        return constant(v, dtype=dtype, name=name)\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\", line 165, in constant\r\n        tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 441, in make_tensor_proto\r\n        tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\", line 441, in <listcomp>\r\n        tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\r\n      File \"/home/noam/miniconda3/envs/ml/lib/python3.5/site-packages/tensorflow/python/util/compat.py\", line 65, in as_bytes\r\n        (bytes_or_text,))\r\n    TypeError: Expected binary or unicode string, got 13\r\n\r\nUsing Tensorflow version 0.12.0-rc0, also tested and fails the same way on 0.12.0-rc1.", "comments": ["Having the same issue. Does anyone know of a workaround until this gets fixed?", "I'm having the same issuse.\r\n\r\n@asrivat1: I guess you've doing something like multiplication of each element of the batch of shape [B,I,J] with a matrix of shape [J,K]. In that case you could just reshape the batch to [B\\*I,J] and do the ordinary tf.matmul to obtain the result of shape [B\\*I,K], and then reshape this to [B,I,K]. To do the reshaping you can determine the tensor shapes with tf.shape.", "This appears to work for me on python2.7 just fine. It appears you are using python3. Perhaps try forcing the string to be bytes and/or unicode. Are you all using Python 3?", "I'm in 2.7. @aselle are you on 0.12.0-rc1? I found two workarounds. The first is to pad out the shape of the tensors to some fixed size using 0s via `tf.pad`, and this fixes the `tf.einsum` bug. The other solution is what @j88k suggested, and it's about 80% faster for my program. Also worth noting that for joining on multiple dimensions, `tf.batch_matmul` works wonders. The only disadvantage is that according to `Timeline` I'm now spending as much time transposing and reshaping my tensors as I am actually multiplying them.", "Also worth noting I'm using Cython and Anaconda. It's possible that's part of the problem.", "+1\r\nI'm having the same error using einsum where one of the dims is unknown.\r\nI'm using python 3.5 (Anaconda).\r\nIt will be great to have this fixed.", "@aselle - Tested on 2.7, fails the same way:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"/tmp/test.py\", line 7, in <module>\r\n\t\tC = tf.einsum('ijk,kl->ijl', A, B)\r\n\t  File \"/home/noam/miniconda3/envs/2.7test/lib/python2.7/site-packages/tensorflow/python/ops/special_math_ops.py\", line 212, in einsum\r\n\t\taxes_to_sum)\r\n\t  File \"/home/noam/miniconda3/envs/2.7test/lib/python2.7/site-packages/tensorflow/python/ops/special_math_ops.py\", line 341, in _einsum_reduction\r\n\t\tproduct = _reshape_if_necessary(product, uncompacted_shape)\r\n\t  File \"/home/noam/miniconda3/envs/2.7test/lib/python2.7/site-packages/tensorflow/python/ops/special_math_ops.py\", line 366, in _reshape_if_necessary\r\n\t\treturn array_ops.reshape(tensor, new_shape)\r\n\t  File \"/home/noam/miniconda3/envs/2.7test/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2448, in reshape\r\n\t\tname=name)\r\n\t  File \"/home/noam/miniconda3/envs/2.7test/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 493, in apply_op\r\n\t\traise err\r\n\tTypeError: Expected binary or unicode string, got 13\r\n\r\n\r\nTensorflow 0.12.1, installed from `pip`. I created a fresh empty anaconda environment (`conda create -n 2.7test python=2.7`), issued `pip install tensorflow` and ran the code in the original bug report.", "Could you try building master from source? A [fix](https://github.com/tensorflow/tensorflow/commit/30d2a2235a66e158757b13b6dd0bab7bf5e60492) went in  that isn't in r0.12 that sounds as if it might address the issue, at least from the title.", "@michaelisard I didn't build from source, but I did add the line:\r\n\r\n    new_shape = tuple(-1 if x is None else x for x in new_shape)\r\n\r\nto `tensorflow/python/ops/special_math_ops.py` in 0.12.1 manually, and it does seem to fix the issue. I probably won't have time to build from source in the upcoming week.", "OK I will close for now but please reopen if the problem doesn't go away with the new code.", "Just built from source and can confirm that this is now working. Thanks!"]}, {"number": 6383, "title": "Cannot build Android example on Windows using Bazel", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/6101\r\nhttp://stackoverflow.com/questions/40978859/creating-simple-android-app-using-android-studio-and-tensorflow\r\n\r\n### Environment info\r\nOperating System: Windows\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nCuda 8.0\r\ncuDNN 5.0\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`)\r\nI downloaded the latest ZIP on 18 Dec 2016. Sorry, I have no idea how to get the version.\r\n\r\n2. The output of `bazel version`\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:13 2016 (1481136433)\r\nBuild timestamp: 1481136433\r\nBuild timestamp as int: 1481136433\r\n\r\nI am trying to build an Android example using Bazel on Windows. \r\nI've successfully installed Bazel using Chocolatey.\r\nFollowing the build instructions, I need to change the SDK and NDK paths in WORKSPACE file and run:\r\n\r\n**bazel build //tensorflow/examples/android:tensorflow_demo**\r\n\r\nI am having trouble with configuring the WORKSPACE file. I need to uncomment the Android lines and add the paths.  However, no matter what I try, I get an error.\r\n\r\nI've tried:\r\n**\r\n1. path = \"C:/Users/Andrey/AppData/Local/Android/sdk\"\r\n2. path = \"/C/Users/Andrey/AppData/Local/Android/sdk\"\r\n3. path = \"C:\\Users\\Andrey\\AppData\\Local\\Android\\sdk\"\r\n4. path = \"C:\\\\Users\\\\Andrey\\\\AppData\\\\Local\\\\Android\\\\sdk\"**\r\n\r\nFor 1,2,4 I am getting:\r\n\r\nERROR: C:/Tools/tensorflow-master/WORKSPACE:3:1: indentation error.\r\nERROR: error loading package 'external': Failed to parse WORKSPACE file.\r\nINFO: Elapsed time: 0.317s\r\n\r\nFor 3 I am getting:\r\nERROR: C:/Tools/tensorflow-master/WORKSPACE:9:12: escape sequence not implemented: \\U.\r\nERROR: C:/Tools/tensorflow-master/WORKSPACE:14:10: escape sequence not implemented: \\U.\r\nERROR: C:/Tools/tensorflow-master/WORKSPACE:3:1: indentation error.\r\nERROR: error loading package 'external': Failed to parse WORKSPACE file.\r\nINFO: Elapsed time: 0.309s\r\n\r\nThere is a hint that it is related to indentation, but I could not find anything problematic.\r\nI am attaching an example file for (1)\r\n[WORKSPACE_1.txt](https://github.com/tensorflow/tensorflow/files/659363/WORKSPACE_1.txt)\r\n_(Note: I change the extension to .txt since I could not upload the original file.)_\r\n\r\n\r\nAny idea?\r\n", "comments": ["Sorry, my mistake. I accidentally uncommented a line I should not have ", "Hey can you please share the version which worked for you? as I am getting the same error\r\nThanks!\r\n", "[Better late than never] If the path for the SDK/NDK versions is added in the form of `C:\\Users\\[username]\\AppData\\Local\\Android\\sdk` you need to change the backslashes ( \\ ) to normal slashes ( / )", "replace  \"\\\\\" C:\\\\appData\\\\Local\\\\", "'\\\\\\\\'"]}, {"number": 6382, "title": "contrib\\cmake: how properly depend on generated tensorflow .lib", "body": "Used contrib\\cmake ( https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake ) on Windows 10 64bit and VS 2015 64bit to build tensorflow  for C++.\r\n\r\nOpened generated tf_tutorials_example_trainer.vcxproj  in VS Release x64 - runs VERY GOOD, but rebuilds whole tensorflow every time i hit play! Tried Build->Configuration Manager->Uncheck projects that i don't want to always build - errors (what projects i need to keep checked to build only tf_tutorials_example_trainer.vcxproj?).\r\n\r\nHow to depend on tensorflow statically (or at least prevent long build)?\r\n\r\nFailed attempts: \r\n```\r\nCreated new VS console project Release x64 (to prevent full rebuilds). \r\nUsed step-by-step build https://gist.github.com/derofim/b4f150da1269b81af8d12744df730708\r\nAdded tensorflow include dirs.\r\nAdded .lib files:\r\nRelease\\tf_protos_cc.lib\r\nprotobuf\\src\\protobuf\\Release\\libprotobuf.lib\r\nUsed same flags as in example project.\r\n\r\nError: No session factory registered for the given session options\r\n\r\nTried solutions:\r\n/WHOLEARCHIVE as in https://msdn.microsoft.com/en-us/library/mt732963.aspx - LNK1000: Internal error during IncrBuildImage\r\n/OPT:NOREF - no effect\r\n/FORCE:MULTIPLE  - no effect\r\n /INCLUDE - what to place here for force inclusion of unused symbols? May it help?\r\n```", "comments": ["@vit-stepanovs I feel like your answer at #6396 might be also applicable here?", "Yes, it looks like the same issue. ", "Then closing this issue.\r\n\r\nlater this month, we will move all windows support onto bazel.\r\nI think it will look slightly different, and we will be able to use bazel's incremental build then.\r\n\r\nUntil bazel arrives, I suggest tring suggestions at #6396.\r\nClosing this issue."]}, {"number": 6381, "title": "Discussion Online Expanding the Number of Distributed Training Workers ", "body": "Tensorflow distributed training adds a new feature:\r\nWhen we use the distributed tensorflow training task, if the number of workers is too small to start, but not very good convergence. . Especially when used in conjunction with k8s, then tensorflow should support the online expansion of the current number of workers training tasks.\r\n\r\nMy idea is: Use python interface, notify the ps server, the new worker task_index and worker addr. such as:\r\n\r\nWe pre-start the cluster as follows:\r\n# On ps0.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 0\r\n# On ps1.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 1\r\n# On worker0.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 0\r\n# On worker1.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 1\r\n\r\n\r\nWe started the original two ps server ,,, two workers. .\r\nNow let's add a third worker\r\n\r\n\r\nImport tensorflow as tf\r\nDef main (_):\r\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps0.example.com:2222') as sess:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')\r\n\r\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps1.example.com:2222') as sess:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')\r\n\r\nIf name == \"main\":\r\n\u00a0\u00a0\u00a0\u00a0Tf.app.run ()\r\n\r\nJust run this python script, and then start worker2.\r\nWorker2 startup script:\r\n$ Python trainer.py\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = worker --task_index = 2\r\n\r\nAt this point you should be able to see the worker2 has started training.\r\n\r\nIf ps0 or ps1 restart, it should be added worker2:\r\n$ Python trainer.py\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = ps --task_index = 0\r\n\r\n\r\nThe design idea is as follows:\r\n1. Increase the number of workers online, only need to notify all of the ps server can be.\r\n2. Through the use of the python session interface, you can connect a different ps server, and then send an increase in worker instructions can be given to them.\r\n\r\n\r\n\r\nI've done this at https://github.com/ipdcode/tensorflow.\r\n\r\nWelcome to the official google and we discuss to see if there is a need to increase this feature.\r\n\r\n\r\n\r\n\r\n", "comments": ["@mrry, could you comment on this proposal and see if we want to accept the PR.\r\n", "I was interested in your idea.\r\nI tried to build  source code of \"ipdcode/tensorflow\", but I could not build it.\r\n`\r\nVirtual Status AddOnlineWorker(CallOptions* call_options,...\r\n`\r\nIt does not seem to be possible to build because there is no implementation of this function.\r\nI wonder if any corrections are not enough...", "We've discussed this problem within the team, and think we've come up with a design that meets our internal requirements. The basic idea is that we'd move `tf.train.ClusterSpec` from being an argument of `tf.train.Server` to being a configuration option when you create a `tf.Session`. This would let you build clusters on an ad hoc basis, and would remove limitations like requiring all PS tasks to know the identities of all workers at startup time.\r\n\r\n@saeta is going to be working on this, so I've assigned the issue to him. He is currently figuring out the details, and should have more to share soon. ", "Thank you for sharing information.\r\nI would like to be able to efficiently allocate online streaming data to Tensorflow machine nodes.\r\nI would like to dynamically perform real-time processing in response to changes in streaming for models that want to predict and learn simultaneously, such as LSTM or DQN.\r\nI understand that the current Tensorflow needs to pre-allocate machines in advance.\r\nI expect to implement such online distributed processing.", "Hi @dancing80s . Currently TensorFlow assumes a cluster is a fixed size and shape. I'm working on a change that does the following:\r\n\r\n 1. Worker nodes can boot up without needing to know the whole cluster shape.\r\n 2. A cluster shape (i.e. list of nodes) is tied to a session (instead of being process-wide).\r\n 3. A client can specify the cluster shape at session creation time.\r\n\r\nSo what this would look like is a tensorflow worker binary can be launched on all nodes in the cluster (e.g. [grpc_tensorflow_server.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server.cc), just with no required command-line arguments). Then, a client (e.g. python script) can come along and decide to use a subset of the nodes, and create a session referencing that subset, run its computation on it, and simultaneously, another client can be running its own computation on a separate subset in a different session.\r\n\r\nOne thing to note: my work will not make it possible to change the cluster shape within a session. You'll have to manage that client side by creating new sessions under the hood, and copying data (e.g. training parameters) across.\r\n\r\nHope that helps!", "Oh, if you need online inference (i.e. predict), I highly recommend you check out [TensorFlow Serving](https://github.com/tensorflow/serving). The team that's put this together has solved a number of non-intuitive challenges you might encounter when running inference in an online environment. Their systems absolutely support updating the models while serving requests, and have tooling to do so.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 6380, "title": "Fix issues seen in TensorFlow GPU cmake builds.", "body": "Models is mvoed to another repo, and it is not fetched so removing them.\r\nlosses is only a python op, and does not exist under core/ops. So removing all references for it there.", "comments": []}, {"number": 6379, "title": "Inconsistency in parameter naming", "body": "```\r\ntf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\r\ntf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\r\ntf.nn.conv3d(input, filter, strides, padding, name=None)\r\n```\r\n\r\nThere's discrepancy between parameter naming between conv1d and 2,3d counterparts. Firstly value vs input, and then singular vs plural filter(s) (singular would be correct as in 2d/3s case since function accepts only single filter if I understood documentation correctly.)", "comments": ["Thanks for pointing out the inconsistency.  To fix it, we'll probably have to support both for a while but at least make the correct ones documented.  conv2d/3d do take in multiple strides (they can take in a single value too), so in isolation, these names were somewhat sensible, but they could definitely be made more consistent with each other.\r\n\r\nThere's a generic 'convolution' parameter here that is more agnostic about the dimensionality, for what it's worth: https://github.com/tensorflow/tensorflow/blob/a276e0999ab4223ac36d75221028d3e8835c60ae/tensorflow/python/ops/nn_ops.py#L472", "Closing as it is resolved"]}, {"number": 6378, "title": "WhereOp: Race condition between counting the number of true elements and writing them", "body": "I have the same problem as issue #4033 and [tensorflow/models#486](https://github.com/tensorflow/models/issues/486) happening to me in my own project with the nightly wheel. It is running fine on CPU, the problem only happens on GPU (titan x pascal).\r\n\r\nThe code that's causing the problem is this:\r\n`pair_idxs = tf.where(tf.greater_equal(iou, params.thresh))`\r\n(iou is a tensor, params.thresh is a python float)\r\n\r\nWIth the error message: \r\n```\r\nInvalidArgumentError (see above for traceback): WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 31641 elements; but when writing their indices, saw 187 elements.\r\n         [[Node: Where = Where[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](GreaterEqual/_25)]]\r\n```\r\n\r\nCuda:\r\n```\r\n-rw-r--r-- 1 foo bar   52M Dec  8 20:06 libcublas_device.a\r\nlrwxrwxrwx 1 foo bar    16 Dec  8 20:06 libcublas.so -> libcublas.so.8.0\r\nlrwxrwxrwx 1 foo bar    19 Dec  8 20:07 libcublas.so.8.0 -> libcublas.so.8.0.45\r\n-rwxr-xr-x 1 foo bar   40M Dec  8 20:06 libcublas.so.8.0.45\r\n-rw-r--r-- 1 foo bar   46M Dec  8 20:07 libcublas_static.a\r\n-rw-r--r-- 1 foo bar  546K Dec  8 20:07 libcudadevrt.a\r\nlrwxrwxrwx 1 foo bar    16 Dec  8 20:06 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 foo bar    19 Dec  8 20:07 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 foo bar  406K Dec  8 20:06 libcudart.so.8.0.44\r\n-rw-r--r-- 1 foo bar  757K Dec  8 20:07 libcudart_static.a\r\nlrwxrwxrwx 1 foo bar    15 Dec  8 20:06 libcufft.so -> libcufft.so.8.0\r\nlrwxrwxrwx 1 foo bar    18 Dec  8 20:07 libcufft.so.8.0 -> libcufft.so.8.0.44\r\n-rwxr-xr-x 1 foo bar  140M Dec  8 20:06 libcufft.so.8.0.44\r\n-rw-r--r-- 1 foo bar  124M Dec  8 20:07 libcufft_static.a\r\nlrwxrwxrwx 1 foo bar    16 Dec  8 20:06 libcufftw.so -> libcufftw.so.8.0\r\nlrwxrwxrwx 1 foo bar    19 Dec  8 20:07 libcufftw.so.8.0 -> libcufftw.so.8.0.44\r\n-rwxr-xr-x 1 foo bar  466K Dec  8 20:06 libcufftw.so.8.0.44\r\n-rw-r--r-- 1 foo bar   42K Dec  8 20:07 libcufftw_static.a\r\nlrwxrwxrwx 1 foo bar    17 Dec  8 20:06 libcuinj64.so -> libcuinj64.so.8.0\r\nlrwxrwxrwx 1 foo bar    20 Dec  8 20:06 libcuinj64.so.8.0 -> libcuinj64.so.8.0.44\r\n-rwxr-xr-x 1 foo bar  6.2M Dec  8 20:06 libcuinj64.so.8.0.44\r\n-rw-r--r-- 1 foo bar  1.6M Dec  8 20:07 libculibos.a\r\nlrwxrwxrwx 1 foo bar    16 Dec  8 20:06 libcurand.so -> libcurand.so.8.0\r\nlrwxrwxrwx 1 foo bar    19 Dec  8 20:07 libcurand.so.8.0 -> libcurand.so.8.0.44\r\n-rwxr-xr-x 1 foo bar   57M Dec  8 20:07 libcurand.so.8.0.44\r\n-rw-r--r-- 1 foo bar   57M Dec  8 20:06 libcurand_static.a\r\nlrwxrwxrwx 1 foo bar    18 Dec  8 20:06 libcusolver.so -> libcusolver.so.8.0\r\nlrwxrwxrwx 1 foo bar    21 Dec  8 20:07 libcusolver.so.8.0 -> libcusolver.so.8.0.44\r\n-rwxr-xr-x 1 foo bar   52M Dec  8 20:06 libcusolver.so.8.0.44\r\n-rw-r--r-- 1 foo bar   22M Dec  8 20:07 libcusolver_static.a\r\nlrwxrwxrwx 1 foo bar    18 Dec  8 20:07 libcusparse.so -> libcusparse.so.8.0\r\nlrwxrwxrwx 1 foo bar    21 Dec  8 20:06 libcusparse.so.8.0 -> libcusparse.so.8.0.44\r\n-rwxr-xr-x 1 foo bar   41M Dec  8 20:07 libcusparse.so.8.0.44\r\n-rw-r--r-- 1 foo bar   50M Dec  8 20:06 libcusparse_static.a\r\n```\r\n\r\ncuDNN:\r\n```\r\nlrwxrwxrwx 1 foo bar  13 Jul 27 07:55 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 foo bar  17 Jul 27 07:55 libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 foo bar 76M Jul 27 07:53 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 foo bar 67M Jul 27 07:53 libcudnn_static.a\r\n```\r\n\r\n`tenserflow.__version__: 0.12.head`\r\n\r\n", "comments": ["I managed to build a small reproducing example:\r\n```\r\niou = tf.placeholder(shape=[1], dtype=tf.float32)\r\npair_idxs = tf.where(tf.greater_equal(iou, 0.5))\r\n\r\nwith tf.Session():\r\n    tf.global_variables_initializer().run()\r\n    idxs = pair_idxs.eval(feed_dict={iou: [0.1]})\r\n```\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): WhereOp: Race condition between counting the number of true elements and writing them.  When counting, saw 205 elements; but when writing their indices, saw 1 elements.\r\n         [[Node: Where = Where[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](GreaterEqual/_3)]]\r\n```\r\n", "This also fails, but commenting out the assignment makes it work.\r\n```\r\niou = tf.placeholder(shape=[1], dtype=tf.float32)\r\ncmp = tf.Variable([False], dtype=tf.bool)\r\ncmp = cmp.assign(iou >= 0.5)\r\npair_idxs = tf.where(cmp)\r\n\r\nwith tf.Session():\r\n    tf.global_variables_initializer().run()\r\n    idxs = pair_idxs.eval(feed_dict={iou: [0.1]})\r\n```", "@zheng-xq, do you have any insights on the concurrency issue here. I don't see the problem with the Titan X Maxwell edition on current nightly.", "On my system a similar error was produced as a result of the GPU running out of memory.", "I met the same issue, turned out I already started a training instance so all my GPU memory are used up. After I killed that instance, the issue is gone. ", "@hosang, does this occur on a more recent version of TensorFlow? 0.12 is quite old at this point.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "@aselle updating to a newer version fixed the problem for me and I didn't encounter the problem again.", "@hosang Maybe you could close the issue then."]}, {"number": 6377, "title": "error getting gradient of `tf.nn.crelu` (concat_v2 along axis -1)", "body": "I am on MacOS Sierra with CPU only nightly. \r\ntf.__git_version__ is 0.12.0-rc0-1156-g8c18a00-dirty.\r\n\r\nThe error comes from the gradient calculation in concat_v2 when axis = -1 is used to get the last dimension.  I could not find axis=-1 documentation for concat_v2 but that is how crelu is implemented.  \r\n\r\nConcat_v2 along last dimension as a positive int works fine.  The forward pass with axis = -1 also works fine, it is just getting a gradient that gives an error.\r\n\r\nReproduce with:\r\n\r\n    import tensorflow as tf\r\n\r\n    inputs = tf.random_normal([64, 32,32, 3])\r\n    inputs2 = tf.random_normal([64, 32,32, 3])\r\n    hidden = tf.concat_v2([inputs, inputs2], axis=-1)  # axis = 3 works fine\r\n    grad = tf.gradients(hidden, [inputs])\r\n\r\n    with tf.Session() as sess:\r\n\tprint( sess.run(grad))\r\n\r\nGives error:\r\n\r\n> ...which was originally created as op u'concat_v2', defined at:\r\n  File \"crelu.py\", line 5, in <module>\r\n    hidden = tf.concat_v2([inputs, inputs2], axis=-1)\r\n  File \"/Users/mikowals/projects/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1062, in concat_v2\r\n    name=name)\r\n  File \"/Users/mikowals/projects/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 498, in _concat_v2\r\n    name=name)\r\n  File \"/Users/mikowals/projects/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/mikowals/projects/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2389, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/mikowals/projects/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n>InvalidArgumentError (see above for traceback): Concat dim is out of range: -1 vs. 4\r\n\t [[Node: gradients/concat_v2_grad/ConcatOffset = ConcatOffset[N=2, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](concat_v2/axis, gradients/concat_v2_grad/Shape, gradients/concat_v2_grad/Shape_1)]]\r\n", "comments": ["Thanks for reporting this. I can reproduce this on the nightly build as well. Looks like grad might not support negative axes. We'll look into it. As you say the work around is to use non-negative axes for now. ", "The fix commit is now in r1.0 branch. I will close this issue as fixed. Let me know if you still see any issue with negative axis concat grad."]}, {"number": 6375, "title": "Make VLOG minimum level can be updated via environment #6366", "body": "Added changes to enable specifying VLOG minimum level. With this patch, user can specify VLOG log level to enable VLOG output, like\r\n```\r\nexport TF_CPP_MIN_VLOG_LEVEL=2\r\n```\r\nTested with fully_connected_feed.py. \r\n\r\nMore details see issue #6366 \r\n\r\nThanks,", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@vrv @cwhipkey, I just updated pull request, mind to take a look again? Thanks!", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thanks @jhseu for merge the PR and thanks @vrv / @cwhipkey for reviews/feedbacks!", "This is to hint that the usual case is when verbose logging is off.  When verbose logging is on, we presumably don't care as much about the performance so don't mind the mis-hint."]}, {"number": 6374, "title": "BUG when trainning with multiple ps-server", "body": "I post a issue previous https://github.com/tensorflow/tensorflow/issues/6326\r\n\r\nThe original problem is solved by @yaroslavvb  with add `sharded=True` to Saver's __init__ .\r\n\r\n#### BUT , One more problem occurs: \r\n\r\n* when I training with 5 workers and **m (m=1)** ps-server, then training process works well.\r\n* BUT when training with 5 workers and **m (m>1)** ps-servers, then training process crash with error `NotFoundError (see above for traceback): ./supervisor/model.ckpt-0_temp_994ae96906954e838fc2f3481ce8f296/part-00001-of-00002.data-00000-of-00001`\r\n\r\nIt seems that when have multiple ps-servers , the checkpoint-save has some weird bugs.\r\n\r\nThe detail error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"distributed_deepcake.py\", line 441, in <module>\r\n    exit(1)\r\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\r\n    yield\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\r\n    self.run_loop()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\r\n    global_step=self._sv.global_step)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'save/SaveV2_4', defined at:\r\n  File \"distributed_deepcake.py\", line 293, in <module>\r\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\r\n    tensors)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nOnly find one issue and it's myself . https://github.com/tensorflow/tensorflow/issues/6326\r\n\r\n### Environment info\r\nOperating System:\r\ncentos 6.5 ,glibc 2.17, gcc6.2, Python 2.7.12, `tensorflow newest version 0.12rc1 with cpu`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nwith tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n    # Read TFRecords files for training\r\n    filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.train),\r\n        num_epochs=epoch_number)\r\n    serialized_example = read_and_decode(filename_queue)\r\n    batch_serialized_example = tf.train.shuffle_batch(\r\n        [serialized_example],\r\n        batch_size=batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    features = tf.parse_example(\r\n        batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    batch_labels = features[\"label\"]\r\n    batch_ids = features[\"ids\"]\r\n    batch_values = features[\"values\"]\r\n\r\n    # Read TFRecords file for validatioin\r\n    validate_filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.eval),\r\n        num_epochs=epoch_number)\r\n    validate_serialized_example = read_and_decode(validate_filename_queue)\r\n    validate_batch_serialized_example = tf.train.shuffle_batch(\r\n        [validate_serialized_example],\r\n        batch_size=validate_batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    validate_features = tf.parse_example(\r\n        validate_batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    validate_batch_labels = features[\"label\"]\r\n    validate_batch_ids = features[\"ids\"]\r\n    validate_batch_values = features[\"values\"]\r\n    logits = inference(batch_ids, batch_values)\r\n    batch_labels = tf.to_int64(batch_labels)\r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\r\n                                                                   batch_labels)\r\n    loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\r\n\r\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\r\n\r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n    # Initialize saver and summary\r\n    steps_to_validate = FLAGS.steps_to_validate\r\n    init_op = tf.initialize_all_variables()\r\n\r\n    saver = tf.train.Saver(max_to_keep = 2)\r\n    keys_placeholder = tf.placeholder(\"float\")\r\n    keys = tf.identity(keys_placeholder)\r\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\r\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\r\n                                                'softmax': inference_softmax.name,\r\n                                                'prediction': inference_op.name}))\r\n\r\n    summary_op = tf.merge_all_summaries()\r\n\r\n\r\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                         logdir=\"./supervisor/\",\r\n                         init_op=init_op,\r\n                         summary_op=summary_op,\r\n                         saver=saver,\r\n                         global_step=global_step,\r\n                         save_model_secs=60)\r\n\r\nwith sv.managed_session(server.target) as sess:\r\n\r\n    while not sv.should_stop():\r\n        # Get coordinator and run queues to read data\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n        try:\r\n            while not coord.should_stop():\r\n                _, loss_value, step = sess.run([train_op, loss, global_step])\r\n\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done training after reading all data\")\r\n        finally:\r\n            coord.request_stop()\r\n\r\n        coord.join(threads)\r\n```\r\n", "comments": ["this is a bug....when use distributed training,,,,if ps server restart,,,you must copy worker0 checkpoint dir to all ps server Same directory\u3002\u3002\u3002\u3002\u3002then restart ps server", "@guoying1030  thanks, but my ps server doesn't restart.  It happens at the begining of training.\r\nMy start order is:   \r\n1\u3001start ps0   \r\n2\u3001start ps1\r\n3\u3001start worker0\r\n4\u3001start worker1\r\nwhen every server started , it report error like that.", "@mrry could you take a look when you get a chance?", "I don't think this is a bug, but it might be poor documentation, and/or grounds for a feature request.\r\n\r\nIt looks like the problem arises because the chief worker and the parameter server do not share the same file system (because the client performs some file operations on the checkpoint data locally, but the checkpoint data is loaded by a `Restore` op that runs on the parameter server). This has been a requirement since time immemorial, but I don't think it's well documented.\r\n\r\nYou can work around this problem by using a shared filesystem (e.g. HDFS, GCS, or an NFS mount at the same mount point) on the workers and the parameter servers.", "@mrry  thanks for your reply.  is tensorflow support dump the checkpoint  and read/write data to hdfs directly\uff1f  ", "HDFS support is built as part of the standard PIP package, and it supports reading and writing checkpoints (as well as input data, summaries, etc.). You'll need to set the `HADOOP_HDFS_HOME` environment variable to use it.", "thanks, I will try it and give my feedback.", "Thank you @mrry for the advice. The conclusion I'm drawing is to use a distributed file system when using distributed TensorFlow. So I'm going to close this one out. If you're still experiencing this issue @ericyue, let me know and I'll re-open.", "I'll also add that I will gladly review a pull request updating our documentation to make this more clear to users. We've done a lot of work recently getting GCS to be available for everyone by default in our build, which should hopefully make this very simple to do.", "Hit this issue with even only one parameter server with distributed workers. Looks exactly as what @mrry described. Can we update the subject of this issue accordingly? "]}, {"number": 6373, "title": "Update index.md", "body": "You may refer to this page, right? The original link is not accessible.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Thanks for the contribution.\r\nIt looks like the when I go to the link you provided, I get a https error. is that expected?\r\n\r\nAlso, could you sign the CLA and then comment on the PR as instructed by the google cla bot?", "I signed the CLA, thanks.", "The original link shows me on the page:\r\nNot Found\r\nThe requested URL /~treebank/ was not found on this server.\r\nDo not know how it looks like on your computer?\r\n\r\nSo I think updated it to some other pages. \r\nThe link I provided:\r\n![the link I provided](https://cloud.githubusercontent.com/assets/3758964/21291332/3da9e770-c4a0-11e6-8d63-06d9c4e788b6.png)\r\n\r\nPrevious link:\r\n![previous link](https://cloud.githubusercontent.com/assets/3758964/21291331/3da92e16-c4a0-11e6-91df-b9d7fff2e922.png)\r\n\r\n", "Our system doesn't show the CLA as being signed yet. Mind trying again?", "@JacobIsrael123 you will need to type exactly `I signed it!` to your comment, as it is a bot checking this.\r\nI was not able to manually verify your CLA.", "I signed it!", "We still cannot verify your CLA, neother manually, nor automated.\r\nIs it possible your commit email is hidden in github, or is it maybe different than the one you used to sign the Google CLA?", "Sorry for the inconvenience. My first time to contribute. What do you mean it is hidden in github? I changed the setting-> profile-> public email to my email signed email just now. Could you double check?", "CLAs look good, thanks!\n\n<!-- ok -->", "Maybe that matters. Google bot auto-replied just now. ", "Jenkins, test this please", "Why the link is still not updated in the website after this issue already been merged?\r\n[https://www.tensorflow.org/tutorials/recurrent/](https://www.tensorflow.org/tutorials/recurrent/)", "There's a separate process to push updates to the website."]}, {"number": 6372, "title": "Fix typo on exception message", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Jenkins, test this please", "Looks good to me, but I'll be in and out of internet access for the next couple of days, deferring to @jhseu to merge it in."]}, {"number": 6371, "title": "Improved support for OpenCL devices", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I believe the windows cmake failures are introduced by this PR. \r\nEspecially, these lines look suspicious:\r\n```\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"BestSplits\" device_type: \"CPU\"') for unknown op: BestSplits\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"CountExtremelyRandomStats\" device_type: \"CPU\"') for unknown op: CountExtremelyRandomStats\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"FinishedNodes\" device_type: \"CPU\"') for unknown op: FinishedNodes\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"GrowTree\" device_type: \"CPU\"') for unknown op: GrowTree\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"SampleInputs\" device_type: \"CPU\"') for unknown op: SampleInputs\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"ScatterAddNdim\" device_type: \"CPU\"') for unknown op: ScatterAddNdim\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"TopNInsert\" device_type: \"CPU\"') for unknown op: TopNInsert\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"TopNRemove\" device_type: \"CPU\"') for unknown op: TopNRemove\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"TreePredictions\" device_type: \"CPU\"') for unknown op: TreePredictions\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"UpdateFertileSlots\" device_type: \"CPU\"') for unknown op: UpdateFertileSlots\r\n00:18:02.468 E c:\\tf_jenkins\\home\\workspace\\tensorflow-pr-win-cmake-py\\tensorflow\\core\\framework\\op_kernel.cc:925] OpKernel ('op: \"ReinterpretStringToFloat\" device_type: \"CPU\"') for unknown op: ReinterpretStringToFloat\r\n```\r\n\r\nCould you take a look?", "Merge PR, so CLA is OK.\r\nCLA bot wont update its status, it is a known issue.", "I fixed the basic_gpu_test failure. The linalg_grad_test failure was fixed in cl/142147027, we just need to wait for it to be merged in github."]}, {"number": 6370, "title": "Library not loaded: @rpath/libcudart.8.0.dylib but I have libcudart.7.5.dylib", "body": "I have the following error when importing tensor flow (GPU OSX version);\r\n\r\n```\r\nImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n```\r\n\r\nIt looks to me like it is looking for `@rpath/libcudart.8.0.dylib` but in my `/usr/local/cuda/lib` I have `libcudart.7.5.dylib` and no 8.0. How do I redirect it?", "comments": ["You should install from source code. The default CUDA of binary package is 8.0 and CUDNN 5.1.", "Version 0.11 and comes compiled for 7.5\n\nOn Sat, Dec 17, 2016 at 6:15 PM, \u53f2\u4e1a\u6c11 <notifications@github.com> wrote:\n\n> You should install from source code. The default CUDA of binary package is\n> 8.0 and CUDNN 5.1.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6370#issuecomment-267799096>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHDYWT752XLZhKoYO0zWfohXymgIZks5rJJcpgaJpZM4LPtPN>\n> .\n>\n", "According to https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/g3doc/get_started/os_setup.md, r0.11 also need CUDA 8.0 and CUDNN v5.", "Sorry, I meant version 10.\nCUDA filename is baked into the final executable, in order to change the\nCUDA version you have to recompile\n\nOn Sun, Dec 18, 2016 at 5:53 PM, \u53f2\u4e1a\u6c11 <notifications@github.com> wrote:\n\n> According to https://github.com/tensorflow/tensorflow/blob/r0.11/\n> tensorflow/g3doc/get_started/os_setup.md, r0.11 also need CUDA 8.0 and\n> CUDNN v5.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6370#issuecomment-267865257>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHOcDNkH53jW1zaPIfVux05q1GWsgks5rJeOogaJpZM4LPtPN>\n> .\n>\n", "You may also update CUDA and CuDNN, (if you are the administrator of the device) to use the later versions of tensorflow.\r\n\r\nClosing this issue as @yaroslavvb has identified and explained the problem.", "Thanks "]}, {"number": 6369, "title": "error: conv3d_transpose layer", "body": "The conv3d_transpose seems buggy\r\n\r\n>>> import tensorflow as tf\r\n\r\n>>> deconv3d = tf.nn.conv3d_transpose(tf.zeros((10,3,3,3,10), dtype=tf.float32), tf.zeros((4,4,4,8,10), dtype=tf.float32), output_shape=[10,6,6,6,8], strides=[1, 1, 1, 1, 1])\r\n\r\n>>> deconv2d = tf.nn.conv2d_transpose(tf.zeros((10,3,3,10), dtype=tf.float32), tf.zeros((4,4,8,10), dtype=tf.float32), output_shape=[10,6,6,8], strides=[1, 1, 1, 1])\r\n\r\n>>> deconv3d\r\n<tf.Tensor 'conv3d_transpose:0' shape=(?, ?, ?, ?, ?) dtype=float32>\r\n\r\n>>> deconv2d\r\n<tf.Tensor 'conv2d_transpose:0' shape=(10, 6, 6, 8) dtype=float32>\r\n", "comments": ["Why this issue was closed? I have the exact same problem.", "You can resolve this issue by updating your tf to 0.12+"]}, {"number": 6368, "title": "docs: add a troubleshooting section to the faq", "body": "docs: add a troubleshooting section to the faq\r\n\r\nSometimes we could have an issue with a running docker, specifically with the TLS certificate.\r\nIt is important for a beginner to be aware of that.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Hello !\nIt's done, I signed it!\n\nKind regard,Jean-Claude KOUASSI. \n\n    Le Vendredi 16 d\u00e9cembre 2016 20h03, Tensorflow Jenkins <notifications@github.com> a \u00e9crit :\n \n\n Can one of the admins verify this patch?\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.  \n\n   ", "CLAs look good, thanks!\n\n<!-- ok -->", "@Kjeanclaude any luck with this?", "Jenkins, test this please.", "Merged by hand and addressed comment."]}, {"number": 6367, "title": "Improve data existence check in test_tutorials.sh: ptb_word_lm", "body": "", "comments": []}, {"number": 6366, "title": "Make VLOG minimum level can be updated via environment", "body": "I found inside c++ core implementation there're lots of verbose messages like:\r\n```\r\n    VLOG(1) << \"Direct session inter op parallelism threads: \" << num_threads;\r\n```\r\nThey will be very helpful for developer to understand what happens internally and troubleshoot problems.\r\n\r\nHowever I found in implementation, it doesn't look like works without changing the code. (Credit to @yaroslavvb for the solution, see discussion thread: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/D0_3jG5_XRQ)\r\n```\r\n    // TODO(jeff): Define a proper implementation of VLOG_IS_ON\r\n    #define VLOG_IS_ON(lvl) ((lvl) <= 0)\r\n\r\n    #define VLOG(lvl)      \\\r\n        if (VLOG_IS_ON(lvl)) \\\r\n          ::tensorflow::internal::LogMessage(__FILE__, __LINE__, tensorflow::INFO)\r\n```\r\n\r\nIt will be very helpful if we can update the minimum lvl via passing environment.", "comments": ["And I'm interested to contribute to the feature, if folks think it is a valid feature requirement, I can start working on it.\r\n\r\nThanks!", "vlog is also useful for debugging out of memory situations, right now the only way to get Tensor allocation/deallocation information is though vlog `__LOG_MEMORY__` messages on stdout", "The probably reason that VLOG is disabled in code is that it would cause a significant runtime penalty. It might be placed in tight loops where comparing against a variable number would cause performance degradation. As it is now, VLOG blocks will basically be eliminated from compiled code and have no performance penalty. Replacing VLOG_IS_ON(lvl) definition with a call to getenv() would not be ok, because getenv() is relatively expensive. One would probably need to make some static data that is initialized by the environment variable once on translation unit initialization.\r\n\r\nPerhaps @vrv or @andydavis1 has more insight?", "https://github.com/tensorflow/tensorflow/pull/6375 is a reasonable solution that is being worked on: it checks the env var once statically on first use and just checks against the member variable on every call, which shouldn't be that expensive.", "Thanks for reviews from @cwhipkey, @vrv, @aselle, very helpful!\r\n\r\nFor performance concerns from @aselle, I think @vrv's comment should solve the problem, refer to below code review comment:\r\nhttps://github.com/tensorflow/tensorflow/pull/6375#discussion-diff-93282773R130\r\n\r\nI'm going to update the patch according to your feedbacks. ", "Sounds good.... marking as contributions welcome since it is being worked on.", "#6375 is merged, closing this one, thanks everyone for your feedbacks!"]}, {"number": 6365, "title": "Support of gradient for TF.MOD", "body": "Hello there,\r\n\r\nWhen I include `tf.mod` in my Graph, I have this error:\r\n\r\n`LookupError: No gradient defined for operation 'RNN/while/LSTM/Mod' (op type: Mod)`\r\n\r\nWill it be handled in the future?\r\n\r\nThanks!\r\n\r\nReferences: https://www.tensorflow.org/api_docs/python/math_ops/arithmetic_operators#mod\r\nTensorflow version: `0.12.0-rc1`", "comments": ["\r\n@ops.RegisterGradient(\"Mod\")\r\ndef _mod_grad(op, grad):\r\n    x, y = op.inputs\r\n    gz = grad\r\n    x_grad = gz\r\n    y_grad = tf.reduce_mean(-(x // y) * gz, reduction_indices=[0], keep_dims=True)\r\n    return x_grad, y_grad", "I would like to use `tf.mod` in my graph, as well, in order to define a loss function that minimizes the error in a variable that wraps (e.g. an angle with a range of [-pi,pi)). For example, if\r\n`a1 = pi - eps` and `a2 = -pi + eps`,\r\nthen I would like\r\n`a1 - a2 = -2 * eps`,\r\nas opposed to\r\n`a1 - a2 = -2 * eps + 2 * pi`.\r\n\r\nIn order to test the above solution, must I rebuild Tensorflow from source? I'm currently using 0.12.0rc1.\r\nThanks!\r\n\r\nEdit - Instructions for registering a gradient function can be found here: https://www.tensorflow.org/how_tos/adding_an_op/#implement_the_gradient_in_python", "@kleinma For Python-only changes such as above, you need not rebuild TensorFlow from source", "I'm not sure if mod in models is well-defined enough to know what the gradient should be in all cases. The angle case makes sense, but lots of uses of mod (binning, etc) are inherently discontinuous, certainly with integers. @girving, do you think we should provide a default mod gradient, or just let people override them appropriately for their application when they see this error?\r\n", "The gradient of mod is zero a.e., so it's TensorFlow gradient is zero everywhere.  We should definitely define a zero gradient (returning None).", "@aselle Unassigning myself since I'm on leave.", "@sherrym would you take a look since @girving is out? It sounds as if the gradient should be 0 for mod as defined, but there would be some customers for an angular_mod with a different gradient.", "Any news guys? @girving ", "Wait a sec guys! The gradient of tf.mod(x, y) w.r.t x is 1, except for x / y = integer where it is not defined... but since these points are a set of measure zero, the gradient should always be one (?). Gradient of int division (//) or floor is of course zero...\r\n\r\nOne plot says it all:\r\n```\r\nxvals = np.linspace(0, 10, 100)\r\nyvals = xvals % 3.14\r\nplt.plot(xvals, yvals)\r\n```\r\n\r\nAnyway, looking forward for the gradients!", "Ah, thought we were talking about integer mod, which has no gradient.  Yes, float mod has gradient 1 w.r.t. x a.e., and also an a.e. defined gradient w.r.t. y.", "OK actually I just assumed tf.mod = % but didn't think about it further...", "@harpone check this out: http://math.stackexchange.com/questions/1849280/derivative-of-remainder-function-wrt-denominator", "@yuefengz Can you take a look?", "That stackexchange link is very helpful. I will implement the gradient function.", "@yuefengz thanks!", "@yuefengz any updates on your part?", "@yuntatsai is working on a change, we will check it soon. Thanks for your patience."]}, {"number": 6364, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "yes16.12.2016, 19:03, \"Tensorflow Jenkins\" <notifications@github.com>:Can one of the admins verify this patch?\n\n\u2014You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or mute the thread.", "Really you make a PR just for that?", "This is a cherry-pick from inside TF, so I'll ignore CLAbot."]}, {"number": 6363, "title": "User Contributed Tutorials", "body": "It would be great if there was a section in the tensorflow.org website where the community could provide tutorials for new users. There are a lot of great tutorials in the web and having some on the tensorflow.org website would be very valuable for people just starting out. Thoughts?\r\n\r\nHere is an example of what I would love to share:\r\nhttps://bitbucket.org/hrojas/learn-tensorflow ", "comments": ["@wolffg and @martinwicke, is there a good place on devsite for adding these?\r\n", "Please add them to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/resources/index.md\r\n\r\nThis is less prominent than we'd like at the moment, but whatever new place will be seeded with information from there."]}, {"number": 6362, "title": "error: stride must be less than or equal to kernel size", "body": "I'm using TensorFlow 0.12.0rc0 for CPU. This version seems not support a stride less or equal to kernel size, so it reports this error. It works fine for GPU version", "comments": ["Did you try the rc1? \r\nIs it possible it got resolved in that?", "Closing due to inactivity. If provided new information, I will re-open."]}, {"number": 6361, "title": "Using Multiple CPU Threads in an op", "body": "Hi all,\r\n\r\nI'm interested in using multithreading within a CPU op to increase speed. More specifically, I have some linear algebra code which, for batched matrix multiplies, is much faster than MKL (and probably Eigen).\r\nI should be able to get the corresponding GPU version working without much trouble.\r\n\r\nCurrently my code uses OpenMP and SIMD pragmas which, as I understand it, don't play nicely with tensorflow at the moment. I've been digging through the documentation all day but haven't found any examples of a CPU op with multithreading (that isn't just calling eigen) which I can use.\r\n\r\nCould anyone please point me in the right direction here?\r\n\r\nCheers!\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n#22 #1747 \r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Sep 15 00:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Sep 15 00:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Sep 15 00:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   415432 Sep 15 00:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Sep 15 00:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Dec 15 12:42 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Dec 15 12:42 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 79337624 Dec 15 12:42 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Dec 15 12:42 /usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\ngit rev-parse HEAD\r\ndbe5e17e2ed307e86e1a6e79e558ec3e335d46fc\r\n\r\n2. The output of `bazel version`\r\nbazel version\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nN/A\r\n\r\n### What other attempted solutions have you tried?\r\nN/A\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nN/A", "comments": ["@sdrelton You may find https://github.com/tensorflow/tensorflow/issues/5277 useful where an op has been implemented both with OpenMP and Eigen's ThreadPool implementation. The Eigen documentation is available [here](https://bitbucket.org/eigen/eigen/src/f5cf4207b67ee7ac304c1300873ae7ef93537956/unsupported/Eigen/CXX11/src/Tensor/README.md).", "@sjperkins Thanks very much, I'll take a good look!", "Please re-post on stackoverflow if this question is not resolved."]}]