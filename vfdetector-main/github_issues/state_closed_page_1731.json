[{"number": 969, "title": "Remove external/<repositoryName>/ prefix from C includes statements", "body": "This prefix is invalid when using TensorFlow as a remote repository.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, please try to test this?\n", "And by that I mean, test this, please?\n", "Please hold off, more includes are coming.\n\nOn Tue, Feb 2, 2016 at 10:08 PM Martin Wicke notifications@github.com\nwrote:\n\n> And by that I mean, test this, please?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/969#issuecomment-178819371\n> .\n", "Here they are :)\n", "Alright, Jenkins, test this please.\n", "Can you squash these commits?\n", "Done\n", "Thanks!\n"]}, {"number": 968, "title": "tensorflow.bzl.bzl doesn't exist, unsurprisingly", "body": "I get a pile of build errors after pulling.  One of them is that `tensorflow.bzl.bzl` doesn't exist.  Did someone rewrite the build rules recently?\n\n```\nbazel build ...\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:11:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:12:6: First argument of load() is a path, not a label. It should start with a single slash if it is an absolute path..\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:11:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:12:6: file '/tensorflow:tensorflow.bzl.bzl' was not correctly loaded. Make sure the 'load' statement appears in the global scope in your file.\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:21:13: Traceback (most recent call last):\n  File \"/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD\", line 14\n    cc_library(name = \"cc_op_gen_main\", srcs = [\"...\"], <3 more arguments>)\n  File \"/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD\", line 21, in cc_library\n    tf_copts\nname 'tf_copts' is not defined.\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:28:1: name 'tf_gen_op_wrappers_cc' is not defined.\nERROR: /usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD:66:13: Traceback (most recent call last):\n  File \"/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD\", line 63\n    cc_binary(name = \"tutorials_example_traine...\", <4 more arguments>)\n  File \"/usr/local/google/home/geoffreyi/tensorflow/tensorflow/cc/BUILD\", line 66, in cc_binary\n    tf_copts\nname 'tf_copts' is not defined.\n... (many more)\n```\n", "comments": ["This is on my Goobuntu machine.\n", "You need bazel 0.1.4\n"]}, {"number": 967, "title": "add distributed, non-blocking thread pool implementation", "body": "Current thread pool implementation is centralized and non-scalable.\nThis change adds distributed, non-blocking thread pool implementation.\nBoth implementations co-exist and can be chosen using\nTF_THREAD_POOL env var.\n\nFixes #551\nFixes #583\nUpdate #932\nUpdate #933\n", "comments": ["Can one of the admins verify this patch?\n", "I've been told that Eigen library requires thread pool to execute tasks in a strict FIFO order. This thread pool does not execute tasks in FIFO order. So I am not sure how to proceed.\n\nI am also not sure about benchmarks. I did not find any. On various tests that I executed this thread pool showed 2-6x speedup.\n\n@jeremybarnes \n", "@benoitsteiner, can you answer this question or reassign?\n", "Can one of the admins verify this patch?\n", "We could use a wrapper along the following lines to ensure FIFO ordering on top of a scheduler that does not give any ordering guarantees. I've tried to fix deadlocks in label_image example by applying the FIFO wrapper in TensorContractionThreadPool, but no success (even if I use FIFO for all tasks submitted in TensorContractionThreadPool, it still deadlocks).\n\n``` c\nclass FIFO {\n public:\n  typedef std::function<void()> F;\n\n  F operator()(F f) {\n    {\n      std::unique_lock<std::mutex> lock(m_mutex);\n      m_queue.push(std::move(f));\n    }\n    return [this]() {\n      F f;\n      {\n        std::unique_lock<std::mutex> lock(m_mutex);\n        f = std::move(m_queue.front());\n        m_queue.pop();\n      }\n      f();\n    };\n  }\n\n private:\n  std::mutex m_mutex;\n  std::queue<F> m_queue;\n};\n```\n", "It seems fundamentally wrong to require ordering of operations that are submitted to a thread pool, since even if the jobs are scheduled on a thread in order, they won't necessarily start or complete in any order.  The only thing that we can really guarantee is that at least one job from n-1 to n-numThreads will complete before job n is scheduled.  I wonder if the FIFO-ness is really the issue here.\n", "Yes, FIFO is the issue.\nConsider you have a thread pool with N threads and 2*N tasks where first N tasks wait for completion of the last N tasks. If you start running last tasks first, everything is OK. If you start running first tasks, you deadlock.\n", "@benoitsteiner: Can you comment on this? \n", "As of change `ab02c5`, we consider the CPU scaling issue generally resolved, so I'm going to close this PR. Please file new issues with benchmarks and steps to reproduce if you still see any performance issues.\n"]}, {"number": 966, "title": "Add a .gitignore entry and a missing srcs_version spec", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@vrv: Can you review this?  Two commits, each a one line change.\n", "@tensorflow-jenkins: test this please\n", "Merged\n"]}, {"number": 965, "title": "Test failure on Mac: //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test", "body": "One of the C++ unit test three method in gpu_allocator_retry_test.cc failed: \n**//tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test**\n\nSee below for detailed error log:\n\n```\nINFO: From Testing //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test:\n==================== Test output for //tensorflow/core:common_runtime_gpu_gpu_allocator_retry_test:\nRunning main() from test_main.cc\n[==========] Running 3 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 3 tests from GPUAllocatorRetryTest\n[ RUN      ] GPUAllocatorRetryTest.RetrySuccess\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:136] Consumer 0 is 1090\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:136] Consumer 1 is 1090\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:136] Consumer 2 is 1089\n[       OK ] GPUAllocatorRetryTest.RetrySuccess (55 ms)\n[ RUN      ] GPUAllocatorRetryTest.NoRetryFail\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:160] Consumer 0 is 534506\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:160] Consumer 1 is 534511\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:160] Consumer 2 is 534504\n**tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:164: Failure\nValue of: has_failed_\n  Actual: false\nExpected: true**\n[  FAILED  ] GPUAllocatorRetryTest.NoRetryFail (10811 ms)\n[ RUN      ] GPUAllocatorRetryTest.RetryInsufficientFail\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:183] Consumer 0 is 23\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:183] Consumer 1 is 0\nI tensorflow/core/common_runtime/gpu/gpu_allocator_retry_test.cc:183] Consumer 2 is 0\n[       OK ] GPUAllocatorRetryTest.RetryInsufficientFail (10001 ms)\n[----------] 3 tests from GPUAllocatorRetryTest (20867 ms total)\n\n[----------] Global test environment tear-down\n[==========] 3 tests from 1 test case ran. (20867 ms total)\n[  PASSED  ] 2 tests.\n[  FAILED  ] 1 test, listed below:\n[  FAILED  ] GPUAllocatorRetryTest.NoRetryFail\n\n 1 FAILED TEST\n```\n", "comments": ["Machine machine details:\nOS: \"Darwin\", \nKernel: \"15.3.0\", \nArchitecture: \"i386\", \nBazel_version: \"Build label: 0.1.4-homebrew\", \nJava_version: \"1.8.0_65\", \nPython_version: \"2.7.10\", \ngpp_version: \"Apple LLVM version 7.0.2 (clang-700.1.81)\"\n", "FWIW I am unable to reproduce this failure on my MBP. Same machine details above except my kernel version is 15.2.0.\n", "Interestingly, I upgraded my kernel to 15.3.0 and it failed 1 out of 10 times with the above error.\n\nNot sure if it was flaky before or flaky after.  Probably the former.\n", "For expediency, should we just disable this test on Mac for now?\n\n1) Most macs don't have GPUs that TensorFlow supports right now\n2) It's not clear that gpu_retry_allocator has that much of a long life :)\n\nThoughts @martinwicke and @zheng-xq ?\n", "I support whatever makes the testing work. We're not officially supporting\nGPU on Mac, so not sure this needs to work.\nOn Wed, Feb 3, 2016 at 18:39 Vijay Vasudevan notifications@github.com\nwrote:\n\n> For expediency, should we just disable this test on Mac for now?\n> \n> 1) Most macs don't have GPUs that TensorFlow supports right now\n> 2) It's not clear that gpu_retry_allocator has that much of a long life :)\n> \n> Thoughts @martinwicke https://github.com/martinwicke and @zheng-xq\n> https://github.com/zheng-xq ?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/965#issuecomment-179586725\n> .\n", "@caisq: can we disable this test for the mac suite and call it a day then?\n", "@vrv For the PIP install-test, it is easy to blacklist a test. Unfortunately this is not a PIP install-test. It is a C++ unit test. Is there an easy way to disable this C++ unit test depending on the OS in Mac i Bazel, without changing the source code? \n", "@vrv If run without --cuda, this test is useless and shouldn't run at all,\ncorrect? And we don't build with --cuda on Mac. Or at least we shouldn't.\n\nOn Thu, Feb 4, 2016 at 10:49 AM caisq notifications@github.com wrote:\n\n> @vrv https://github.com/vrv For the PIP install-test, it is to\n> blacklist a test. Unfortunately this is not a PIP install-test. It is a C++\n> unit test. Is there an easy way to disable this C++ unit test depending on\n> the OS in Mac i Bazel, without changing the source code?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/965#issuecomment-179997450\n> .\n", "The test doesn't require a GPU -- it's a test of a module that is used by the GPU allocator code.\n", "(and I am not sure how to disable this only on mac yet.. I can look into it)\n", "I guess my point is, if we're building without GPU support, how useful is\nthis test anyway -- shoudn't it be disabled whenever cuda support is off?\n\nAnd if so, how would we do that? The py_cuda_tests only enable cuda, they\ndon't disable tests without cuda.\n\nOn Thu, Feb 4, 2016 at 1:14 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> (and I am not sure how to disable this only on mac yet.. I can look into\n> it)\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/965#issuecomment-180054351\n> .\n", "honestly, the easiest way is just to put \n\n```\n#if GOOGLE_CUDA\n```\n\naround the test suite.  I can do this later today.\n", "Do we have a similar thing for python tests? If not, we should probably\nmake one.\n\nOn Thu, Feb 4, 2016 at 2:05 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> honestly, the easiest way is just to put\n> \n> #if GOOGLE_CUDA\n> \n> around the test suite. I can do this later today.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/965#issuecomment-180072354\n> .\n", "Turns out using #if GOOGLE_CUDA is a bad idea.\n\nNewest proposal:\nadd 'tags=[\"nomac\"]' to the test, and make jenkins Mac build invoke bazel using:\n\n```\nbazel test --test_tag_filters=-nomac\n```\n\n@caisq: does this sound reasonable?\n", "This sounds reasonable to me, @vrv. However, one thing I don't understand is why the most recent several mac builds show the test passed... Did you already change some related things? See:\nhttp://ci.tensorflow.org/job/tensorflow-master-mac/139/\nhttp://ci.tensorflow.org/job/tensorflow-master-mac/140/\nhttp://ci.tensorflow.org/job/tensorflow-master-mac/141/ \n", "I think the test is just flaky, and other recent changes might have made it less flaky.  If it doesn't fail again, maybe there's nothing to do for now.\n", "Agreed, @vrv. The most recent one failed again. http://ci.tensorflow.org/job/tensorflow-master-mac/142/console\n\nPlease implement the 'tags=[\"nomac\"]' solution. \n", "\"Fixed\" in 155cb0ff84e9ece3bee0f06eb3bf5f92d5ca15ac\n"]}, {"number": 964, "title": "Test failure on Mac: //tensorflow/python:directory_watcher_test", "body": "The following Python unit test fails on Mac: **//tensorflow/python:directory_watcher_test**\n\nBelow is the log with detailed error info:\n\n```\n..F...F...\n======================================================================\nFAIL: testFinishesLoadingFileWhenSwitchingToNewFile (__main__.DirectoryWatcherTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py\", line 102, in testFinishesLoadingFileWhenSwitchingToNewFile\n    self.assertWatcherYields(['b', 'c'])\n  File \"/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py\", line 63, in assertWatcherYields\n    self.assertEqual(list(self._watcher.Load()), values)\nAssertionError: Lists differ: ['c'] != ['b', 'c']\n\nFirst differing element 0:\nc\nb\n\nSecond list contains 1 additional elements.\nFirst extra element 1:\nc\n\n- ['c']\n+ ['b', 'c']\n\n======================================================================\nFAIL: testMultipleWrites (__main__.DirectoryWatcherTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py\", line 82, in testMultipleWrites\n    self.assertWatcherYields(['x', 'y', 'z'])\n  File \"/Users/jenkins/workspace/experimental-cais-tensorflow-mac-python27-copt_pip_install-test/pip_install_tests/directory_watcher_test.py\", line 63, in assertWatcherYields\n    self.assertEqual(list(self._watcher.Load()), values)\nAssertionError: Lists differ: [] != ['x', 'y', 'z']\n\nSecond list contains 3 additional elements.\nFirst extra element 0:\nx\n\n- []\n+ ['x', 'y', 'z']\n\n----------------------------------------------------------------------\nRan 10 tests in 0.051s\n\nFAILED (failures=2)\n```\n", "comments": ["Machine machine details:\nOS: \"Darwin\", \nKernel: \"15.3.0\", \nArchitecture: \"i386\", \nBazel_version: \"Build label: 0.1.4-homebrew\", \nJava_version: \"1.8.0_65\", \nPython_version: \"2.7.10\", \ngpp_version: \"Apple LLVM version 7.0.2 (clang-700.1.81)\"\n", "Okay, I think I have a fix for this on Mac.  @danmane in case he's interested.\n\nThe problem seems to be that in the DirectoryWatcherTest, the ByteLoader implementation assumes that you can read EOF from a file, have another thread append to the file, and then continue reading from the last known seek position.  Apparently this breaks under mac os x but not under linux.\n\nI changed the ByteLoader code to keep track of the number of bytes read, and when you see EOF, to try seeking to the current number of bytes read and trying again to read from the file.  Doing this allows the test to pass.  I couldn't find any documentation on this behavior difference between OS X and Linux though.  I'll try to send a review in tomorrow.\n", "Thanks, @vrv. Looking forward to seeing the fix. \n", "Thanks @vrv!\n"]}, {"number": 963, "title": "Autoload folders and pickle files if already exist (avoid extract)", "body": "improved `extract(filename)` and `load(data_folders, min_num_images_per_class)` to avoid extractions of .tar.gz if folders already exist and  the creation of already dumped pickle files is also avoided\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLA signed\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "I don't want that behavior. It will silently cause irrecoverable issues if the user aborts the tarball extraction in the middle. If you don't want to re-extract, you can simply skip this cell. Note that this is different from the maybe_download() case since it's easy to check for download size.\n", "How can you skip these cells if you reopen your notebook in a second moment? The `train_folders` and `train_datasets` variables are not defined until cells execution.\nIt's true that if the user aborts the extraction in the middle issues can happen, but this is already happening during the dumping of the pickle files where if the user aborts that operation, the pickle files are incomplete.\n", "Aha, not having these variables defined is indeed a problem.\nThere are some changes to this notebook that are about to be pushed to the public github, let me think about it in the meantime, it seems to be a problem for several people. Thanks for clarifying.\n", "Can one of the admins verify this patch?\n", "It would not be enough to write a file after the execution of `extractall` and verify the existence of this file when trying to read from folders? In this way if the user aborts the extraction the file doesn't exist and successive runs will try to extract the files again\n", "A fix similar to yours should be pushed shortly (sorry, my tree was not in a state where I could accept PRs easily). Thanks!\n"]}, {"number": 962, "title": "Uninstall Bazel", "body": "Hi!\n\nI ran into an error while running code very similar to the [RNN tutorial](https://www.tensorflow.org/versions/0.6.0/tutorials/recurrent/index.html). \n\nI believe the issue was the exact [problem](https://github.com/tensorflow/tensorflow/issues/481) that was apparently fixed now. So I wanted to uninstall my version of Tensorflow and install it from source code in order to get the latest updates.\n\nBut (!) after installing Bazel and downloading the tensorflow repo, I ran into an error while compiling. A file, \"transpose_op_gpu.cu.d\" was said to be missing and the compilation could not continue. Strange.\n\nSo then I wanted to install tensorflow again \"normally\" using pip. That seems to have worked, but when I try to import tensorflow in python it just says \n\n```\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n```\n\nSo my question is: How do I **uninstall** Bazel!?\n", "comments": ["You should run python interpreter from a directory that is not a tensorflow repo root.\n", "Yes, I run it from my project directory.\n\nI got this answer from Bazel to uninstall it:\n`rm -fr ~/.bazel ~/.bazelrc`\n"]}, {"number": 961, "title": "Failed to build pip package for Python 3: `PyString_FromStringAndSize` not declared", "body": "I was building pip package for Python 3.5. bazel reported an error during \n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures\n```\n\nThe failing command was:\n\n```\ncd /home/z/.cache/bazel/_bazel_z/4c5a3f7b23103a2fe3f8cd834eaf9376/tensorflow && \\\nexec env - \\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\nthird_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc \\\n(lots of flags here)\n-c bazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc \\\n-o bazel-out/local_linux-py3-opt/bin/tensorflow/python/_objs/_pywrap_tensorflow.so/tensorflow/python/pywrap_tensorflow.pic.o\n```\n\nThe error was:\n\n```\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc: In function 'PyObject* _wrap_TF_PRunSetup(PyObject*, PyObject*)':\nbazel-out/local_linux-py3-opt/bin/tensorflow/python/pywrap_tensorflow.cc:4974:66: error: 'PyString_FromStringAndSize' was not declared in this scope\n       resultobj = PyString_FromStringAndSize(*arg6, strlen(*arg6));\n                                                                  ^\n```\n\nIt seemed to be a Python 2/3 compatibility issue, since `PyString_FromStringAndSize` is no longer provided by Python 3 headers.\n\n`pywrap_tensorflow.cc` reads like:\n\n```\n  resultobj = SWIG_Py_Void();\n  {\n    if (!arg5->ok()) {\n      RaiseStatusNotOK(*arg5, SWIGTYPE_p_tensorflow__Status);\n      SWIG_fail;\n    } else {\n      resultobj = PyString_FromStringAndSize(*arg6, strlen(*arg6));\n      delete *arg6;\n    }\n  }\n  return resultobj;\n```\n\nI'm not sure whether it was caused by tensorflow or my configuration, but I did find some suspicious code in `tensorflow/python/client/tf_session.i`, that closely resembles the problematic code:\nhttps://github.com/tensorflow/tensorflow/blob/97f585d506cccc57dc98f234f4d5fcd824dd3c03/tensorflow/python/client/tf_session.i#L211\n", "comments": ["change tf_session.i line 211:\n$result = PyString_FromStringAndSize($2, strlen($2));\nTo:\n%#if PY_MAJOR_VERSION < 3\n$result = PyString_FromStringAndSize(\n%#else\n$result = PyUnicode_FromStringAndSize(\n%#endif\n$2, strlen($2));\n", "The fix proposed by @blueegg worked for me. Thanks.\n", "We fixed this recently in https://github.com/tensorflow/tensorflow/commit/5ff6d34a05b2eb49f7a79a4d0b78ada9a6842b6c\n"]}, {"number": 960, "title": "bug in tf_session.i came from commit:8a59748", "body": "line 211:\n    $result = PyString_FromStringAndSize(_$2, strlen(_$2));\nTo:\n%#if PY_MAJOR_VERSION < 3\n    $result = PyString_FromStringAndSize(\n%#else\n    $result = PyUnicode_FromStringAndSize(\n%#endif\n        _$2, strlen(_$2));\n", "comments": ["We fixed this just recently in https://github.com/tensorflow/tensorflow/commit/5ff6d34a05b2eb49f7a79a4d0b78ada9a6842b6c\n"]}, {"number": 959, "title": "Android demo app crashes \"tensorflow/examples/android/jni/jni_utils.cc:91 'asset' Must be non NULL\"", "body": "When I try to launch the Android demo app, the app crashes with the following error.\n\nDevice: Nexus5x\nAndroid version: 6.0\nBuild number: MDB08L\n\nI also tried the app on a Nexus5 running Android 5.1, but resulted in the same error.\n\n```\n02-02 17:02:28.666 2308-2323/org.tensorflow.demo A/native: tensorflow/examples/android/jni/jni_utils.cc:91 'asset' Must be non NULL\n02-02 17:02:28.666 2308-2323/org.tensorflow.demo A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 2323 (ImageListener)\n02-02 17:02:28.767 487-487/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\n02-02 17:02:28.768 487-487/? A/DEBUG: Build fingerprint: 'google/bullhead/bullhead:6.0/MDB08L/2343525:user/release-keys'\n02-02 17:02:28.768 487-487/? A/DEBUG: Revision: 'rev_1.0'\n02-02 17:02:28.768 487-487/? A/DEBUG: ABI: 'arm'\n02-02 17:02:28.768 487-487/? A/DEBUG: pid: 2308, tid: 2323, name: ImageListener  >>> org.tensorflow.demo <<<\n02-02 17:02:28.768 487-487/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\n02-02 17:02:28.792 487-487/? A/DEBUG: Abort message: 'tensorflow/examples/android/jni/jni_utils.cc:91 'asset' Must be non NULL'\n02-02 17:02:28.792 487-487/? A/DEBUG:     r0 00000000  r1 00000913  r2 00000006  r3 f3b7f978\n02-02 17:02:28.793 487-487/? A/DEBUG:     r4 f3b7f980  r5 f3b7f930  r6 00000000  r7 0000010c\n02-02 17:02:28.793 487-487/? A/DEBUG:     r8 f3b7e9ac  r9 f3b7e9a0  sl f3b7e96c  fp f3b7e980\n02-02 17:02:28.793 487-487/? A/DEBUG:     ip 00000006  sp f3b7e900  lr f6deef15  pc f6df0c00  cpsr 400f0010\n02-02 17:02:28.800 487-487/? A/DEBUG:     #00 pc 00041c00  /system/lib/libc.so (tgkill+12)\n02-02 17:02:28.800 487-487/? A/DEBUG:     #01 pc 0003ff11  /system/lib/libc.so (pthread_kill+32)\n02-02 17:02:28.800 487-487/? A/DEBUG:     #02 pc 0001c73f  /system/lib/libc.so (raise+10)\n02-02 17:02:28.800 487-487/? A/DEBUG:     #03 pc 000198f1  /system/lib/libc.so (__libc_android_abort+34)\n02-02 17:02:28.800 487-487/? A/DEBUG:     #04 pc 000174b0  /system/lib/libc.so (abort+4)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #05 pc 00b9b500  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (tensorflow::internal::LogMessage::GenerateLogMessage()+1508)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #06 pc 00b9ba18  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (tensorflow::internal::LogMessageFatal::~LogMessageFatal()+28)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #07 pc 00647424  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (AAsset*&&& tensorflow::internal::CheckNotNull<AAsset*&>(char const*, int, char const*, AAsset*&&&)+160)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #08 pc 00648158  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (ReadFileToProto(AAssetManager*, char const*, google::protobuf::MessageLite*)+316)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #09 pc 00649090  /data/app/org.tensorflow.demo-2/lib/arm/libtensorflow_demo.so (Java_org_tensorflow_demo_TensorflowClassifier_initializeTensorflow+804)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #10 pc 00013eed  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (int org.tensorflow.demo.TensorflowClassifier.initializeTensorflow(android.content.res.AssetManager, java.lang.String, java.lang.String, int, int, int)+168)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #11 pc 0001527b  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.TensorflowImageListener.initialize(android.content.res.AssetManager, org.tensorflow.demo.RecognitionScoreView, android.os.Handler)+166)\n02-02 17:02:28.801 487-487/? A/DEBUG:     #12 pc 00010c87  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.CameraConnectionFragment.createCameraPreviewSession()+442)\n02-02 17:02:28.802 487-487/? A/DEBUG:     #13 pc 0000f86f  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.CameraConnectionFragment.access$400(org.tensorflow.demo.CameraConnectionFragment)+50)\n02-02 17:02:28.802 487-487/? A/DEBUG:     #14 pc 0000dd1b  /data/app/org.tensorflow.demo-2/oat/arm/base.odex (offset 0xd000) (void org.tensorflow.demo.CameraConnectionFragment$2.onOpened(android.hardware.camera2.CameraDevice)+222)\n02-02 17:02:28.802 487-487/? A/DEBUG:     #15 pc 734ffe8d  /data/dalvik-cache/arm/system@framework@boot.oat (offset 0x1ec4000)\n```\n", "comments": ["The app crashes if we don't download model data to assets dir.\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\n\n$ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip -O /tmp/inception5h.zip\n$ unzip /tmp/inception5h.zip -d tensorflow/examples/android/assets/\n", "Oops, thank for pointing this out!\n"]}, {"number": 958, "title": "Generating whl for linux", "body": "Hi all,\nI am trying to compile tensorflow for ubuntu 14.04 by following instruction at https://www.tensorflow.org/versions/master/get_started/os_setup.html#create-pip.\n\nMade it as far as:\nbazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\n with correct output.  The instructions seems to stop there without any indication of how to generate whl for pip and further how to install with pip.  Any pointers will be appreciated.\n\nThanks\nLEon\n", "comments": ["Create the pip package and install\n\nWhen building from source, you will still build a pip package and install that.\n\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n# To build with GPU support:\n\n$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n\n$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n\n# The name of the .whl file will depend on your platform.\n\n$ pip install /tmp/tensorflow_pkg/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n", "Thanks for response junluan. Did the first build command and got an error:\n\n C++ compilation of rule '//tensorflow/python:_pywrap_tensorflow.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 90 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\n", "We would need to see the output when run with --verbose_failures to know what's going wrong.\n", "(Closing issue to help triage more active bugs.  Feel free to comment again and we'll reopen).\n"]}, {"number": 957, "title": "Tutorial ImportError: No module named examples.tutorials.mnist.input_data", "body": "I am beginner in tensorflower\n\ninstall tensorflow with pip \n    `sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl`\n\nI want to start with the tutorial \"MNIST For ML Beginners\"\nbut first import get error...\n`import tensorflow.examples.tutorials.mnist.input_data`\n`File \"<stdin>\", line 1, in <module>`\n`ImportError: No module named examples.tutorials.mnist.input_data`\n\nOS:Ubuntu 14.04\nrun on virtualbo\n", "comments": ["Stack overflow is perhaps better for these sort of questions. Have you tried\nhttps://www.google.com/search?q=ImportError%3A+No+module+named+examples.tutorials.mnist.input_data\n\nBTW: It is something is wrong with the python environment. Have the pip finished without error? Are you using virtualenv?\n", "thanks for your help \npip install finished without error and I have not using virtualenv\nI will ask on Stack Overflow \n", "Examples are missing from the .whl installer for Mac. (VirtualEnv, python2.7).\n\nMost of the examples do not work, because import path is incorrect and have to be changed manually.\n"]}, {"number": 955, "title": "Tons of warnings related to GUARDED_BY on Mac", "body": "I get a bunch of these:\n\n```\nIn file included from tensorflow/core/framework/load_library.cc:20:\n./tensorflow/core/framework/op_kernel.h:945:63: warning: 'guarded_by' attribute requires arguments whose type is annotated with 'capability' attribute; type here is 'tensorflow::mutex' [-Wthread-safety-attributes]\n  gtl::InlinedVector<WrappedAllocator, 4> wrapped_allocators_ GUARDED_BY(mu_);\n                                                              ^\n./tensorflow/core/platform/default/thread_annotations.h:52:53: note: expanded from macro 'GUARDED_BY'\n#define GUARDED_BY(x) THREAD_ANNOTATION_ATTRIBUTE__(guarded_by(x))\n                                                    ^\n./tensorflow/core/platform/default/thread_annotations.h:42:57: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'\n#define THREAD_ANNOTATION_ATTRIBUTE__(x) __attribute__((x))\n```\n", "comments": ["We should probably run this by a clang-knowledgeable person, since the answer is presumably very simple given that knowledge.\n", "@keveman: Do you know the right person to ask? \n", "@girving I have tried to fix them a week... https://github.com/jendap/tensorflow/commit/7db4674218f4a8822d2db5d4f63682beb05ba6fd put down the 50+ MB log on mac build of CI down to less than 1MB.\n\nBut it does not go away completely. The problem is that std::unique_lock does not have SCOPED_LOCKABLE annotation in /Library/Developer/CommandLineTools/usr/include/c++/v1/mutex. Either we have to create our MutexLock (should not be too hard) or disable the warning.\n", "@jendap Was this ever fixed?\n", "I see 3-4 GUARDED_BY warnings in today's head --\nhttp://pastebin.com/AbSxRzKJ\nThat doesn't seem like a \"ton\" since there are >600 warnings not related to\nguarded_by\n\nOn Wed, Jun 8, 2016 at 3:43 PM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @jendap https://github.com/jendap Was this ever fixed?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/955#issuecomment-224753254,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHAaLgpa6zrrJa8DvRnsFLvZmJtDVks5qJ0WMgaJpZM4HRFUZ\n> .\n", "Closing since \"tons\" no longer applies. :)\n"]}, {"number": 954, "title": "Add a missing `import` statement to show correct error messages.", "body": "`_googletest.py` uses `sys.stderr.write` and `sys.exit` without importing `sys`.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Merged\n", "Thanks, @vrv.\n"]}, {"number": 953, "title": "Failed to enqueue async memcpy from device to host, with latest code (relapse?)", "body": "On a Linux machine with a GPU, with the GPU configuration, I ran the following command: \n`bazel test -c opt --config=cuda //tensorflow/python:framework_function_test`\n\nThen I got the following error, which seems to be similar to the closed issues. I'm using the latest code of the master branch\nhttps://github.com/tensorflow/tensorflow/issues/719\nhttps://github.com/tensorflow/tensorflow/issues/713\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcurand.so.7.0. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2785\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0Ki\nB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 3.62GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x704a80000 extends to 0x7ec261000\n...I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n..I tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n**E tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_INVALID_VALUE; host dst: 0x7f16e40008c0; GPU src: 0x265d480; size: 4=0x4**\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:232] GPU->CPU Memcpy failed\nexternal/bazel_tools/tools/test/test-setup.sh: line 51:   714 Aborted                 (core dumped) \"$@\"\n", "comments": ["This does look like a problem, and there have been changes to this code recently.  \n", "After investigation it appears that this test was not intended to be run on GPU.  A fix is in progress.\nThanks for reporting the problem!\n", "Fixed in e27da590fec8eed886ecd1cf3d0c2575dffeaa09\n"]}, {"number": 952, "title": "Error in //tensorflow/core:ops_array_grad_test", "body": "On a machine with a GPU, under the GPU configuration, I ran the command: bazel test -c opt --config=cuda //tensorflow/core:ops_array_grad_test\n\nand I got the following error (see highlight in bold font)\n## exec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:99] Couldn't open CUDA library libcurand.so.7.0. LD_LIBRARY_PATH: \nI tensorflow/stream_executor/cuda/cuda_rng.cc:333] Unable to load cuRAND DSO.\nRunning main() from test_main.cc\n[==========] Running 4 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 4 tests from ArrayGradTest\n[ RUN      ] ArrayGradTest.PackGrad\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.2785\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.91GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 4.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 3.62GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x704a80000 extends to 0x7ec261000\n[       OK ] ArrayGradTest.PackGrad (481 ms)\n[ RUN      ] ArrayGradTest.UnpackGrad\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n[       OK ] ArrayGradTest.UnpackGrad (2 ms)\n[ RUN      ] ArrayGradTest.ConcatGrad\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:680] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n**E tensorflow/core/common_runtime/executor.cc:273] Executor failed to create kernel. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**\n         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]\n**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c21f10 Compute status: Not found: **No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**\n         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]\n         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]\n**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c224b0 Compute status: Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**\n         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]\n         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]\n         [[Node: dx/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_18_dx\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c224b0 Compute status: Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)\n*\\*         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]\n         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]\n         [[Node: dx/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_20_dx\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n**W tensorflow/core/common_runtime/executor.cc:1096] 0x7f70d3c224b0 Compute status: Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**\n         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]\n         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]\n         [[Node: dx/_13 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_22_dx\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n**F tensorflow/core/ops/array_grad_test.cc:126] Check failed: ::tensorflow::Status::OK() == (sess->Run( {{\"dim\", test::AsScalar(dim)}, {\"x0:0\", x0}, {\"x1:0\", x1}, {\"dy:0\", dy}}, {\"dx:0\", \"dx:1\", \"dx:2\"}, {}, &out)) (OK vs. Not found: No registered 'ZerosLike' OpKernel for GPU devices compatible with node n8 = ZerosLike[T=DT_INT32](n0)**\n         [[Node: n8 = ZerosLike[T=DT_INT32](n0)]]\n         [[Node: dx = SymbolicGradient[Tin=[DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT], Tout=[DT_INT32, DT_FLOAT, DT_FLOAT], f=Concat[N=2, T=DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](_recv_dim_0/_1, _recv_x0_0/_3, _recv_x1_0/_5, _recv_dy_0/_7)]]\n         [[Node: dx/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_18_dx\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]])\nexternal/bazel_tools/tools/test/test-setup.sh: line 51:  9618 Aborted                 (core dumped)\n", "comments": ["Your particular problem is that the ZerosLikeOp kernel defined in constant_op.cc is only registered for float and double (contant_op.cc:234/235), instead it should be registered for all numeric types.\n\nIf you can write a fix, that would be awesome. \n\nThere's a larger issue here that should be cleaned up. We have at least three different zeros_like implementations lying around: One zeros_like in array_ops.py, one ZerosLikeOp in constant_op.cc, and one ZerosLike method in control_flow_ops.py's ControlFlowState (that last one may be legitimate, but should be refactored to use ZerosLikeOp). Assigning @sherrym for triage on that issue.\n", "It appears that although the change set fixed the ZerosLikeOp-not-defined-for-GPU issue, the test \n`bazel test -c opt --config=cuda //tensorflow/core:ops_array_grad_test`\n\nstill fails, with a seg fault: \n\n> [       OK ] ArrayGradTest.PackGrad (505 ms)\n> [ RUN      ] ArrayGradTest.UnpackGrad\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n> [       OK ] ArrayGradTest.UnpackGrad (3 ms)\n> [ RUN      ] ArrayGradTest.ConcatGrad\n> I tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n> external/bazel_tools/tools/test/test-setup.sh: line 51: 20332 Segmentation fault      (core dumped) \"$@\"\n", "@zffchen78 \n"]}, {"number": 951, "title": "fix gradient of tf.floor", "body": "Fix #897 .\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "LGTM if tests pass.\n", "Merged\n"]}, {"number": 950, "title": "sigmoid_cross_entropy_loss_with_logits: ReLU input is not finite", "body": "```\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\n     [[Node: _send_nl_1_hs_100_lr_0p1/div_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=6247430638048453470, tensor_name=\"nl_1_hs_100_lr_0p1/div_1:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/div_1)]]\nTraceback (most recent call last):\n  File \"rnn.py\", line 80, in <module>\n    training=True)\n  File \"rnn.py\", line 18, in run_epoch\n    model.seq_targets: seq_targets\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nCaused by op u'nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics', defined at:\n  File \"rnn.py\", line 70, in <module>\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 61, in __init__\n    .minimize(self.loss)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 445, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 126, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 119, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'nl_1_hs_100_lr_0p1/logistic_loss/Relu', defined at:\n  File \"rnn.py\", line 70, in <module>\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 58, in __init__\n    losses = tf.nn.sigmoid_cross_entropy_with_logits(outputs, targets_concat)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 273, in sigmoid_cross_entropy_with_logits\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 547, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n```\n\nWhen training a model using tf.nn.sigmoid_cross_entropy_loss_with_logits and RMSPropOptimizer, I am running into a ReLu gradient error. Since the initial learning rate for this instance of the model is quite high and this is a RNN, I am pretty confident that this is an exploding gradient issue with my model. However, the ReLU operation that my model is exploding on is part of an optimization to \"ensure stability and avoid overflow\", but it seems to be having the opposite effect. \n", "comments": ["For quick reference, here is the full source code of sigimoid_cross_entropy_with_logits\n\n```\ndef sigmoid_cross_entropy_with_logits(logits, targets, name=None):\n  \"\"\"Computes sigmoid cross entropy given `logits`.\n\n  Measures the probability error in discrete classification tasks in which each\n  class is independent and not mutually exclusive.  For instance, one could\n  perform multilabel classification where a picture can contain both an elephant\n  and a dog at the same time.\n\n  For brevity, let `x = logits`, `z = targets`.  The logistic loss is\n\n        z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n      = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n      = (1 - z) * x + log(1 + exp(-x))\n      = x - x * z + log(1 + exp(-x))\n\n  To ensure stability and avoid overflow, the implementation uses\n\n      max(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n  `logits` and `targets` must have the same type and shape.\n\n  Args:\n    logits: A `Tensor` of type `float32` or `float64`.\n    targets: A `Tensor` of the same type and shape as `logits`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `logits` with the componentwise\n    logistic losses.\n  \"\"\"\n  with ops.op_scope([logits, targets], name, \"logistic_loss\") as name:\n    logits = ops.convert_to_tensor(logits, name=\"logits\")\n    targets = ops.convert_to_tensor(targets, name=\"targets\")\n    # The logistic loss formula from above is\n    #   x - x * z + log(1 + exp(-x))\n    # For x < 0, a more numerically stable formula is\n    #   -x * z + log(1 + exp(x))\n    # To avoid branching, we use the combined version\n    #   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n                        math_ops.log(1 + math_ops.exp(-math_ops.abs(logits))),\n                        name=name)\n\n```\n", "This sounds like an issue with your model rather than a bug in TensorFlow, you may get more help from stackoverflow community on an issue like this\n", "@yaroslavvb: Sorry, I should have been more clear: I agree with you that this is an exploding gradient issue with my model. \n\nThe reason I believe this **is** also a bug with tensorflow is that I am using a loss function from the official tensorflow API (sigmoid_cross_entropy_loss_with_logits). The documentation of this function leads me to believe that it is supposed to ensure numerical stability during training, and this is a counterexample where it did not. Perhaps I am misunderstanding the documentation?\n", "Numerical stability means the result of computation is more accurate. But when you have exploding gradients, the growth in magnitudes is a feature of your training procedure rather than numerical error. It's like putting i=i**2 in a loop, you are going to run out of range even if you use numerically stable implementation of square operator\n"]}, {"number": 949, "title": "typo in xw_plus_b", "body": "", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 948, "title": "Improved 1_notmnist", "body": "- Added a .gitignore for files created by ipython\n- Changed data_folders to have only folders.\n", "comments": ["Can one of the admins verify this patch?\n", "Closing due to inactivity.\n"]}, {"number": 947, "title": "sparse_softmax_cross_entropy_with_logits fails when labels is a placeholder", "body": "Running the following results in `ValueError: Shapes (?, 1) and (?,) must have the same rank`:\n\n```\n import tensorflow as tf\n from tensorflow.python.ops.nn import sparse_softmax_cross_entropy_with_logits as sparse_ce\n\n logits = tf.placeholder('float32', (None, 32))\n labels = tf.placeholder('int64', (None, 1))\n loss = tf.reduce_mean(sparse_ce(logits, labels))\n```\n\nMaking the minibatch dimension explicit makes no difference: `ValueError: Shapes (32, 1) and (32,) must have the same rank`. \n\nWhen I run the unit test for this function there is no problem, but I note that the unit test passes NumPy ndarrays, which are converted to constant tensors. Playing around a bit it seems it fails whenever `labels` is a placeholder.\n", "comments": ["Reread the docs and realized `labels` should have `ndim=1`. Due to an unrelated error in my original code (not the snippet above) due to an unrelated error, I was mislead to thinking I needed to use an extra dimension for `labels`. Everything runs fine now. \n"]}, {"number": 946, "title": "running my built tensorflow with cifar10 core dump", "body": "I have followed the instructions to build tensorflow with master branch with gpu.\nthe only difference is that when I installed cuda7, I can't install the driver because I am not the root.\nThe admin already installed cuda 7.5 and drivers.\nAlso, I can't copy cuda 7 so to /usr/local/lib.\nwhen running ./configure I specify the cuda 7 path instead of default value.\nAfter building, I installed it within virtualenv.\n$virtualenv --system-site-packages ~/tfenv\n$source ~/tfenv/bin/activate\n$pip install /tmp/tensorflow_pkg/tensorflow-0.6.0-py2-none-any.whl\n\n$export LD_LIBRARY_PATH=/home/lli/cuda-7.0/lib64\n$python cifar10_train.py\n.....\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 8.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:67] Creating bin of max chunk size 16.00GiB\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 1925 get requests, put_count=1701 evicted_count=1000 eviction_rate=0.587889 and unsatisfied allocation rate=0.687792\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 4577 get requests, put_count=4625 evicted_count=1000 eviction_rate=0.216216 and unsatisfied allocation rate=0.213022\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 256 to 281\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:207] could not find cudnnCreate in cudnn DSO; dlerror: /home/lli/tfenv/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cudnnCreate\nAborted (core dumped)\n", "comments": ["there are 8  TITAN X cards installed in this machine.\n", "F tensorflow/stream_executor/cuda/cuda_dnn.cc:207] could not find cudnnCreate in cudnn DSO\n\nsuggests that you didn't point to the right cudnn path.\n\n1) Double check that you pass the right paths when running ./configure before building the pip package\n2) Try \n\n```\nTF_UNOFFICIAL_SETTING=1 ./configure\n```\n\nto specify the cudnn versions / library paths.\n"]}, {"number": 945, "title": "Ops added after calling start_queue_runners() are broken", "body": "(Using the OSX CPU-only  v.0.6 wheel). \n\nI think this is a minimal example:\n\n```\nimage = tf.constant(0)\nimages = tf.train.shuffle_batch([image], batch_size=1, capacity=100, min_after_dequeue=1)\n\na = tf.constant(1)\nwith tf.Session():\n    b = a+1\n    tf.train.start_queue_runners()\n    c = a+1\n\n    print(a.eval())\n    print(b.eval())\n    print(c.eval())\n```\n\nIt _usually_ crashes on the `c.eval()` giving the following error:\n\n```\nTraceback (most recent call last):\n  File \"test.py\", line 11, in <module>\n    print(c.eval())\n  File \".../python2.7/site-packages/tensorflow/python/framework/ops.py\", line 460, in eval\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\n  File \".../python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2910, in _eval_using_default_session\n    return session.run(tensors, feed_dict)\n  File \".../python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \".../python2.7/site-packages/tensorflow/python/client/session.py\", line 446, in _do_run\n    six.reraise(e_type, e_value, e_traceback)\n  File \".../python2.7/site-packages/tensorflow/python/client/session.py\", line 428, in _do_run\n    target_list)\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Not found: FetchOutputs node add_1:0: not found\n```\n\nEven just a clearer error message in this situation would be a big help. \n", "comments": ["The issue is that start_queue_runners() starts (one or more) threads that are running portions of the graph.  Since graph modification is not thread-safe, you will see problems if the graph is modified after start_queue_runners().  @mrry do we have a way of making this an error?\n", "We could use [`tf.Graph.finalize()`](https://www.tensorflow.org/versions/0.6.0/api_docs/python/framework.html#Graph.finalize), which is intended for such purposes. To the original point: the `tf.Graph` methods for adding ops are definitely not thread-safe, and it would require a big redesign to achieve that (with only a small upside).\n", "An error would be nice, it could save other beginners a headache.\n\nThanks for the clarifications.\n", "Sounds like `start_queue_runners` should call `finalize` (possibly with a flag to turn it off if you really know what you're doing)?\n", "@josh11b: Should I reassign? \n", "I think this is fixed since acac487ac4ebaa6edb3e3f866d41cbd12546a107.\n"]}, {"number": 944, "title": "Change types of constant numeric literals.", "body": "This PR fixes mismatched types in numeric literals usages.\nFor examples, use 'size_t{0}' and '1u' instead of '0' and '1' respectively.\nThis removes 26 simple compiler warnings in 20 files\non 'signed' and 'unsigned' comparisons on CHECK_\\* and DCHECK_\\* macros.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins: test this please\n", "Merged\n", "Thank you, @vrv .\n"]}, {"number": 943, "title": "Udacity word2vec notebook: more explicit examples for batch generation algo", "body": "@vincentvanhoucke As I struggled a bit (with pen & paper) to make sense of what's really happening in the `generate_batch` function of the word2vec notebook, I'd like to offer this revision, with examples which make it a bit more explicit IMO. Here's a link to the rendered version of what I'm proposing:\n\nhttps://github.com/cjauvin/tensorflow/blob/b0b231a7202db3d4b4e31fa361cd1078719b9ba9/tensorflow/examples/udacity/5_word2vec.ipynb\n", "comments": ["Can one of the admins verify this patch?\n", "Merged. Thanks!\n"]}, {"number": 942, "title": "build fail with cuda: sparse_xent_op.h", "body": "OS: Debian Stretch 64\nCuda: 7.5\nCudnn: 6.5\nCompiler: gcc-4.9 (default gcc-5)\nBazel: 0.1.4\n\nSince I had a problem using gcc-5 building (see at the bottom), I tried forcing it to use **gcc-4.9** by editing third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.\n\nNow the error is about **sparse_xent_op.h**.\n\nPlease help, thank you.\n\n```\n $ bazel build -s  -j 1  -c opt  --config=cuda //tensorflow/cc:tutorials_example_trainer                                                                                                                                            [0:28:58]\nWARNING: Output base '/auto/master04/weitang114/.cache/bazel/_bazel_weitang114/043c55a899c217c377c622f5ff5a22ca' is on NFS. This may lead to surprising failures and undetermined behavior.\nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore\n_unsupported_sandboxing.\nINFO: Found 1 target...\n>>>>> # //tensorflow/core:gpu_kernels [action 'Compiling tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc']\n(cd /auto/master04/weitang114/.cache/bazel/_bazel_weitang114/043c55a899c217c377c622f5ff5a22ca/tensorflow && \\\n  exec env - \\\n    PATH=/home/master/04/weitang114//bin/:/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/local/cuda-7.5/bin:/home/master/04/weitang114//bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG\n-ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel\n-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external\n/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem external/bazel_tools/tools/cpp/gcc3\n -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/l\nibpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genf\niles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-b45554449873 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-b45554449873 -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt\n/genfiles/third_party/gpus/cuda -x cuda '-DGOOGLE_CUDA=1' '-nvcc_options=relaxed-constexpr' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-sy\nstem-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/sparse_xent_op_gpu.cu.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/s\nparse_xent_op_gpu.cu.d -c tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/sparse_xent_op_gpu.cu.o)\nINFO: From Compiling tensorflow/core/kernels/sparse_xent_op_gpu.cu.cc:\n./tensorflow/core/kernels/sparse_xent_op.h(58): error: explicit type is missing (\"int\" assumed)\n\n./tensorflow/core/kernels/sparse_xent_op.h(58): error: expected a \">\"\n\n./tensorflow/core/kernels/sparse_xent_op.h(58): error: expected a type specifier\n\n./tensorflow/core/kernels/sparse_xent_op.h(73): error: explicit type is missing (\"int\" assumed)\n\n./tensorflow/core/kernels/sparse_xent_op.h(73): error: expected a \">\"\n\n./tensorflow/core/kernels/sparse_xent_op.h(90): error: explicit type is missing (\"int\" assumed)\n\n./tensorflow/core/kernels/sparse_xent_op.h(90): error: expected a \">\"\n\n./tensorflow/core/kernels/sparse_xent_op.h(90): error: expected a type specifier\n\n./tensorflow/core/kernels/sparse_xent_op.h(104): error: explicit type is missing (\"int\" assumed)\n\n./tensorflow/core/kernels/sparse_xent_op.h(104): error: expected a \">\"\n\n./tensorflow/core/kernels/sparse_xent_op.h(122): error: identifier \"int64\" is undefined\n\n./tensorflow/core/kernels/sparse_xent_op.h(133): error: identifier \"int64\" is undefined\n\n./tensorflow/core/kernels/sparse_xent_op.h(185): error: no instance of constructor \"tensorflow::generator::SparseXentLossGenerator<T>::SparseXentLossGenerator [with T=float]\" matches the argument list\n            argument types are: (Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16>, <error-type>)\n          detected during:\n            instantiation of \"void tensorflow::functor::SparseXentEigenImpl<Device, T>::Compute(const Device &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<<error-type>, 1, Eigen::DenseIndex>::ConstVec, te\nnsorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with Device=tensorflow::GPUDevice, T=float]\"\ntensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(39): here\n            instantiation of \"void tensorflow::functor::SparseXentFunctor<tensorflow::GPUDevice, T>::operator()(const tensorflow::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<tensorflow::int64,\n 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float]\"\ntensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(46): here\n\n./tensorflow/core/kernels/sparse_xent_op.h(193): error: no instance of constructor \"tensorflow::generator::SparseXentGradGenerator<T>::SparseXentGradGenerator [with T=float]\" matches the argument list\n            argument types are: (Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, int>, 16>, Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, int>, 16>, <error-type>)\n          detected during:\n            instantiation of \"void tensorflow::functor::SparseXentEigenImpl<Device, T>::Compute(const Device &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<<error-type>, 1, Eigen::DenseIndex>::ConstVec, te\nnsorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with Device=tensorflow::GPUDevice, T=float]\"\ntensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(39): here\n            instantiation of \"void tensorflow::functor::SparseXentFunctor<tensorflow::GPUDevice, T>::operator()(const tensorflow::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<tensorflow::int64,\n 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float]\"\ntensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(46): here\n\ntensorflow/core/kernels/sparse_xent_op_gpu.cu.cc(39): error: no suitable user-defined conversion from \"Eigen::TensorMap<Eigen::Tensor<const tensorflow::int64, 1, 1, Eigen::DenseIndex>, 16>\" to \"Eigen::TensorMap<Eigen::Tensor<const <error-\ntype>, 1, 1, Eigen::DenseIndex>, 16>\" exists\n          detected during instantiation of \"void tensorflow::functor::SparseXentFunctor<tensorflow::GPUDevice, T>::operator()(const tensorflow::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<tens\norflow::int64, 1, Eigen::DenseIndex>::ConstVec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Vec, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::Matrix) [with T=float]\"\n(46): here\n\n./tensorflow/core/kernels/sparse_xent_op.h(91): error: identifier \"labels\" is undefined\n\n16 errors detected in the compilation of \"/tmp/tmpxft_00007ac3_00000000-10_sparse_xent_op_gpu.cu.compute_52.cpp1.ii\".\nERROR: /auto/master04/weitang114/tensorflow/tensorflow/core/BUILD:321:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/sparse_xent_op_gpu.cu.o' was not created.\nERROR: /auto/master04/weitang114/tensorflow/tensorflow/core/BUILD:321:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\n<br>\n## <br>\n\n(For the details how I changed to gcc-4.9, in case I did something wrong)\nAt first I used gcc-5(by default), nvcc compiling **core/kernels/bias_op_gpu.cu.cc**: \n\n```\n/usr/lib/gcc/x86_64-linux-gnu/5/include/mwaitxintrin.h(36): error: identifier \"__builtin_ia32_monitorx\" is undefined.\n/usr/lib/gcc/x86_64-linux-gnu/5/include/mwaitxintrin.h(42): error: identifier \"__builtin_ia32_mwaitx\" is undefined.\n```\n\nThen I followed #8 to modify **crosstool_wrapper_driver_is_not_gcc** into\n\n```\n 46 CURRENT_DIR = os.path.dirname(sys.argv[0])\n 47 CPU_COMPILER = ('/usr/bin/gcc-4.9')\n 48 NVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'\n 49 GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')\n 50 LLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')\n 51 PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)\n```\n", "comments": ["Our GPU build is broken  -- probably sync back to Thursday and you should be alright until we fix it.\n", "Fix is incoming.\n", "Just pushed the fix\n", "Thank you very much!\n", "The bug still seems to remain as of ed6f783690a35e54441881930d4291894bc03285:\n\n```\nINFO: From Compiling tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc:\n\n# Omitting warnings\n\n/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(36): error: identifier \"__builtin_ia32_monitorx\" is undefined\n\n/usr/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/mwaitxintrin.h(42): error: identifier \"__builtin_ia32_mwaitx\" is undefined\n\n2 errors detected in the compilation of \"/tmp/tmpxft_00002a59_00000000-7_adjust_contrast_op_gpu.cu.cpp1.ii\".\n```\n\nBuild process is as follows:\n\n```\n  TF_UNOFFICIAL_SETTING=1 ./configure <<EOF\n/usr/bin/python\ny\n7.5\n/opt/cuda\n4.0.7\n/opt/cuda\n5.2\nEOF\n\n    bazel build --jobs 4 --config=cuda -c opt --verbose_failures \\\n        --define=D_MWAITXINTRIN_H_INCLUDED=1 \\\n        --define=__STRICT_ANSI__=1 \\\n        //tensorflow/tools/pip_package:build_pip_package\n```\n\nEnvironnement is:\n- bazel 0.1.5-1\n- cuda 7.5\n- cudnn 4\n- gcc 5.3.0\n- boost 1.60 (is it even relevant?)\n- python3 (/usr/bin/python is python3)\n\nI also tried without the extra flags.\n", "Most likely the flags for you are passing for STRICT_ANSI etc aren't making it to nvcc.  You might need to edit the crosstool file in third_party/gpus/ to pass the flags there.\n", "Indeed...\nThank you very much for this indication.\nThe following patch seems to fix the issue, althought I am honestly not quite sure about what these flags exactly do:\n\n``` diff\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL b/third_party/gpus/crosstool/CROSSTOOL\nindex dfde7cd..547441f 100644\n--- a/third_party/gpus/crosstool/CROSSTOOL\n+++ b/third_party/gpus/crosstool/CROSSTOOL\n@@ -46,6 +46,8 @@ toolchain {\n   # Use \"-std=c++11\" for nvcc. For consistency, force both the host compiler\n   # and the device compiler to use \"-std=c++11\".\n   cxx_flag: \"-std=c++11\"\n+  cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"\n+  cxx_flag: \"-D__STRICT_ANSI__\"\n   linker_flag: \"-lstdc++\"\n   linker_flag: \"-B/usr/bin/\"\n```\n\nCould anyone kindly reopen this bug until this patch gets reviewed, and integrated (or not) please?\n", "I would argue this belongs in a new issue, the existing bug was for a separate problem.\n"]}, {"number": 941, "title": "build fail on eigen", "body": "On Ubuntu 14.04 in a VirtualBox\n\nFollowed setup and install instructions and ran:\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nStops on error:\n\nINFO: From Compiling tensorflow/core/kernels/conv_grad_ops.cc:\nIn file included from ./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/Core:35:0,\n                 from ./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/Tensor:14,\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\n                 from ./tensorflow/core/framework/types.h:23,\n                 from ./tensorflow/core/framework/type_traits.h:22,\n                 from ./tensorflow/core/framework/allocator.h:25,\n                 from ./tensorflow/core/framework/op_kernel.h:22,\n                 from ./tensorflow/core/framework/numeric_op.h:19,\n                 from tensorflow/core/kernels/conv_grad_ops.cc:22:\n./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h: In static member function 'static void Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 4, 1, long int>, 16>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 4>, const Eigen::TensorContractionOp<const Eigen::array<Eigen::IndexPair<long int>, 2ul>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorImagePatchOp<-1l, -1l, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16> > >, const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 3>, const Eigen::TensorReverseOp<const Eigen::array<bool, 4ul>, const Eigen::TensorMap<Eigen::Tensor<const float, 4, 1, long int>, 16> > > > > >; bool Vectorizable = true]':\n./external/eigen_archive/eigen-eigen-b45554449873/unsupported/Eigen/CXX11/src/Core/util/EmulateArray.h:24:67: warning: array subscript is above array bounds [-Warray-bounds]\n   EIGEN_STRONG_INLINE T& operator[](size_t index) { return values[index]; }\n                                                                   ^\nINFO: From Compiling tensorflow/core/kernels/cwise_op_add.cc:\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee file:///usr/share/doc/gcc-4.8/README.Bugs for instructions.\nERROR: /home/sandra/deeplearning/tensorflow/tensorflow/core/BUILD:342:1: C++ compilation of rule '//tensorflow/core:kernels' failed: gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections ... (remaining 78 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 815.245s, Critical Path: 772.81s\n\nI tried running the gcc call and it said somethine like eigen_archive/... didn't exist. I'm not sure where it's mean to be or whether it's generated by this step, but it's not under bazel-bin/external like the other \"archive\" folders.\n\n$ ls bazel-bin/external/\njpeg_archive/ png_archive/  re2/          \n\nAny help?\n", "comments": ["The eigen code is downloaded from bitbucket and stored by bazel under bazel-tensorflow/external/eigen_archive.\n\nCan you tell me the exact version of gcc you're using ? I am able to compile on ubuntu without any problem, but I might be using a slightly different revision of the compiler.\n", "gcc version 4.8.4 (Ubuntu 4.8.4-2ubuntu1~14.04) \n\nHow do I run Bazel just to build eigen? Maybe I can try that, and see what errors.\n\nOr I can try to download eigen to that folder myself if you can tell me the version and expected folder format after expansion?\n[\n  Update: I see the file name now under WORKSPACE. And to infer, I guess the format is the static libraries similar to jpeg_archive. What's the difference between .a and  .a-2.params in the build?\n  Update2: After downloading it, I now remember that eigen's header only? Should I just copy it there for include?\n]\n\nLooking around, I see eigen3 under third_party/eigen3\n\n$ ls *\nBUILD  LICENSE\n\nEigen:\nArray     Core   Eigenvalues  Householder  LeastSquares  OrderingMethods  StdDeque\nCholesky  Dense  Geometry     Jacobi       LU            QR               SVD\n\nunsupported:\nEigen\n", "Maybe you can download Eigen directly from bitbucket and see if you can successfully compile the tests:\ncd /some/dir/\nhg clone HTTPS://bitbucket.org/eigen/eigen\nmkdir build\ncd build\ncmake /some/dir/eigen DEIGEN_TEST_CXX11=ON\nmake check\n", "Is this still a problem? \n", "Couldn't get it running under VirtualBox on my Mac, so kind of moved on to\ndo it on a native Ubuntu system (which worked).  I'll give it another shot\nin VirtualBox when I have time. Thanks for following up though!\n\nOn Mon, Feb 22, 2016 at 5:57 PM, Benoit Steiner notifications@github.com\nwrote:\n\n> Is this still a problem?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/941#issuecomment-187425788\n> .\n", "When you try again, can you make sure that you create the VM with a lot of memory? gcc may simply have been running out of memory. \n"]}, {"number": 940, "title": "Notmnist lowmem", "body": "As explained in this [Udacity thread](https://discussions.udacity.com/t/deep-learning-notmnist-data-curation/45051) and as requested by Vincent in Issue #936, I am issuing this PR to update the way the notMNIST dataset is loaded into memory, so that it requires less memory.\n\nThe idea is to load the dataset in pieces, curate each piece separately, and merge them.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Cool, can you wrap the couple of lines that go past the edge and squash all the commits? I'll run a test in my local env this weekend or Monday and pull it. Thanks again!\n", "Merged, thanks!\n"]}, {"number": 939, "title": "Python PIP test-on-install", "body": "PIP package is built and installed in a Docker container. Then the Python unit tests in the TF source are moved to a separate folder and ran with the plain Python environment without Blaze.\n\nFour out of 151 tests were skipped for now due to dependency on modules in TF source that are not publicly exported.\n\nTests performed: Jenkins experimental builds for CPU and GPU Python test-on-install, see passing results at: \nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/1/consoleFull\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-gpu-python27-copt_pip_install-test/11/consoleFull\n", "comments": ["Can one of the admins verify this patch?\n", "Also the '&&' at the end of most lines is annoying and confusing. And it probably does not do what you wanted to do. \"set -e\" at the beginning is a good option if you want a script to fail on first subcommand failure.\n", "nit: you should drop the \"(wicke's comments addressed)\" from PR title\n", "FYI, I made a few additional small changes today to get the PIP test-on-install working on Mac (w/o Docker).\n", "@jendap @martinwicke @vrv Please let me know if you have any further comments on this PR. \n", "I'm good with it.\n", "Looking at\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-tensorflow-cpu-python27-copt_pip_install-test/ws/\n\nHow many directories it creates? _python_build, pip_install_tests, pip_whl? Can it live inside one - maybe \".test_installation\" or \".pip_test\"? And it should probably be in .gitignore as well.\n", "It looks good. Just drop the one script, cat the log output on FAILURE and unify the directories into one.\n\nThen squash the commits and merge it.\n\n(Other things like potential separation test_installation and pip.sh can come later)\n", "Regarding the question of what directories are created. It currently creates \"pip_whl\" and \"pip_install_tests\". The \"_python_build\" was created by a previous version of the file and never got deleted because nothing deletes it. I'll consolidate the directories into one common parent folder:\npip_test/whl (for the .whl file to live) \npip_test/tests (for the test files and logs to live)\n\nI'll add \"pip_test\" to .gitignore as well. \n", "Can one of the admins verify this patch?\n", "Comments from @jendap have been addressed. Commits squashed. @martinwicke or @vrv, could you please merge it? \n", "Merged\n"]}]