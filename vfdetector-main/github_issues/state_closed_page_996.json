[{"number": 23508, "title": "AttributeError: 'Estimator' object has no attribute '_distribution'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.2/7.2.1\r\n- GPU model and memory: TitanXp 12Gb\r\n\r\nWhen I use an Estimator with a MirroredDistributionStrategy, I encounter the error \"AttributeError: 'Estimator' object has no attribute '_distribution'\". This seems to be tied to this [line](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/estimator/estimator.py#L1330), which has `self._distribution.unwrap(per_device_hook)[0]`. I'm pretty sure this should be `self._train_distribution.unwrap(per_device_hook)[0]`.\r\n\r\nThe estimator is clearly never assigned a '_distribution' attribute (this is the only line on which its referenced) so I assume this is just a typo. Let me know if I'm missing something.", "comments": ["Same problem on a fresh install from source from the master branch. I made that change and it works perfectly.", "@rmrao  This should not be a problem with TF version 1.12. Have you tried with 1.12 ?", "@yuefengz  Any inputs on this ?", "@harshini-gadige This was broken when I did it on the master branch a few days ago, which is v1.12.", "I haven\u2019t tried with 1.12 although the reference to _distribution still exists in the source of 1.12, so not sure that would do anything. I can test later today ", "This bug has been fixed in the master. 1.12 does have this bug.", "@rmrao  - This issue is fixed and please wait for the next release. Thanks !", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 23507, "title": "Update imports from contrib to tf.profiler", "body": " The instruction [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/tfprof) says \"please use `tf.profiler.xxx` instead of `tf.contrib.tfprof.xxx`\"", "comments": ["Hey @terrytangyuan, thanks for the PR! It looks like tf.profiler.ProfileContext isn't defined though. `AttributeError: module 'tensorflow._api.v1.profiler' has no attribute 'ProfileContext'`", "@ChrisAntaki Good catch! I've addressed that issue in https://github.com/tensorflow/tensorflow/pull/23647 and requested your review there. This PR now depends on #23647. ", "Ping @ChrisAntaki  can you please take a look ? Thanks!", "Looks like we're blocked on #23647", "I am closing this as well since I assume you'll be doing this from internal together with the new API design. "]}, {"number": 23506, "title": "After Installing Tensorflow when I TRy to verify I getting Follwing error can some body help on this", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    NO custom code or Module\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n    Windows 8.1 (64 bit)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n   No mobile device\r\n- TensorFlow installed from (source or binary):\r\n   binary \r\n- TensorFlow version (use command below):\r\n    >pip install tensorflow\r\n    >pip3 install tensorflow\r\n    Currently default comes as tensorflow\r\nC:\\Phyton>pip3 show tensorflow\r\nName: tensorflow\r\nVersion: 1.11.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: opensource@google.com\r\nLicense: Apache 2.0\r\nLocation: c:\\users\\avinash.t\\anaconda3\\lib\\site-packages\r\nRequires: termcolor, astor, tensorboard, keras-applications, numpy, grpcio, whee\r\nl, keras-preprocessing, setuptools, six, protobuf, gast, absl-py\r\nRequired-by:\r\n \r\n- Python version:\r\n 3.6.7\r\n- Bazel version (if compiling from source):\r\nNo compilation used \r\n- GCC/Compiler version (if compiling from source):\r\nNo compilation used \r\n- CUDA/cuDNN version:\r\nNo CUDA\r\n- GPU model and memory:\r\nNo GPU\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nVersion is :- 1.11\r\nWhen i execute the above command following error occurs\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n**Describe the current behavior**\r\n   LOG is Posted\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nC:\\Users\\avinash.t\\Anaconda3>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi\r\nt (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensor\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensor'\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\",\r\nline 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init_\r\n_.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_\r\ntensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["I'm getting the same error after migrating to Tensorflow 1.11 e cwpt I'm pretty sure my error is related to cuDNN64_90.dll not being found (I'm on cuFDNN64_91.dll) \r\n\r\nYou might try down grading Tensorflow back to 1.5 or 1.7", "C:\\Users\\avinash.t>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 b\r\nt (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in i\r\nport_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routi\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in i\r\nport_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\",\r\nline 24, in <module>\r\n    from tensorflow.python import *  # pylint: disable=redefined-builtin\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init\r\n_.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in i\r\nport_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routi\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap\r\ntensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\avinash.t\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in i\r\nport_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_prob\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\nThis is for tensorflow 1.7.0", "It worked with tensoflow == 1.5\r\nC:\\Users\\avinash.t>python\r\nPython 3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bi\r\nt (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> print(tf.reduce_sum(tf.random_normal([1000, 1000])))\r\nTensor(\"Sum:0\", shape=(), dtype=float32)\r\n>>>", "@mrry what dll's is TF1.5 looking for and TF1.7? \r\n\r\nAlso built-in self check functions not returning correct dll codes or files that are missing when using TF1.11", "The self-check functions are once again available in the latest versions of TensorFlow (1.15 / 2.0), so please try again with a more recent version, and open a new issue if they don't work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23506\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23506\">No</a>\n"]}, {"number": 23505, "title": "Facing this error", "body": "2018-11-04 14:31:10.477036: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nGPU mode with 1.0 usage\r\n2018-11-04 14:31:11.087112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 745 major: 5 minor: 0 memoryClockRate(GHz): 1.0325\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.36GiB\r\n2018-11-04 14:31:11.087670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-04 14:31:12.174849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-04 14:31:12.175138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-04 14:31:12.175377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-04 14:31:12.175722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory) -> physical GPU (device: 0, name: GeForce GTX 745, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n2018-11-04 14:31:12.177136: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2018-11-04 14:31:12.177430: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 3.60G (3865470464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\nFinished in 15.694188356399536s\r\n\r\n2018-11-04 14:31:24.869864: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n[Finished in 30.221s]\r\n<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda install -c aaronzs tensorflow-gpu\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 9.0 and cuDNN7.0.5 \r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Looking at the error message, the failure is probably caused by OOM on the GPU side, the variables require more storage space than the GPU has. Please ask your question on StackOverflow, and let's continue the discussion there. The issue tracker on GitHub is for bug reports, feature requests and documentation problems.", "[Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)\r\n"]}, {"number": 23483, "title": "tensorflow-lite on android: using tflite Interpreter to get an image in the output", "body": "\r\nI am trying to use the workflow of Tensorflow-for-poets-2 TFLite tutorial, https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6\r\n\r\nBut, instead of image classification, I am trying to do style transfer. It means that the input and the output of my network are images (compared to the original example, where the input is an image and the output is a list of scores).\r\n\r\nOne of my many problems is to get the output-processed image from the tflite inference:\r\n\r\nAfter i loaded the tflite model, i have the tflite Interpreter tflite . Using this Interpreter I run the inference:\r\n\r\n`tflite.run(imgData, Out_imgData);`\r\nwhere the\r\n\r\n`imgData, Out_imgData`\r\n\r\nare ByteBuffers, created in the same way as in Tensorflow-for-poets-2 TFLite tutorial, https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#6.\r\n\r\nNow I have my inference output as a ByteBuffer\r\n\r\n`Out_imgData`\r\n\r\nI want to convert it to a bitmap. How can this be done?\r\nOr do you have any other suggestions on how to apply inference that at the end I i can take the output image (for example I want to save the image and display it on an imageview) ", "comments": ["I succeeded to convert the Out_imgData (ByteBuffer object) into a bitmap.\r\nthe solution is:\r\n\r\n```\r\nOut_imgData.rewind();\r\nBitmap bitmap_out = Bitmap.createBitmap(DIM_IMG_SIZE_X , DIM_IMG_SIZE_Y, Bitmap.Config.ARGB_8888);\r\nint pixel = 0;\r\nint[] intValues_test = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\nmBitmapIn.getPixels(intValues_test, 0, mBitmapIn.getWidth(), 0, 0, mBitmapIn.getWidth(), mBitmapIn.getHeight());\r\nint[] pixels = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\nfor (int i = 0; i < DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y; i++) {\r\n          final int val = intValues_test[pixel++];\r\n          int a = 0xFF;\r\n          float r = Out_imgData.getFloat();\r\n          float g = Out_imgData.getFloat();\r\n          float b = Out_imgData.getFloat();\r\n          pixels[i] = a << 24 | (int)r << 16 | (int)g << 8 | (int)b;\r\n        }\r\nbitmap_out.setPixels(pixels, 0, DIM_IMG_SIZE_X, 0, 0, DIM_IMG_SIZE_X, DIM_IMG_SIZE_Y);\r\n\r\n```\r\n\r\n\r\n", "Nagging Assignee @ymodak: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I apologize for the delay in response, glad that it worked for you. Closing this issue since its solved. Feel free to reopen if the issue still persists. Thanks for posting the solution.", "> I succeeded to convert the Out_imgData (ByteBuffer object) into a bitmap.\r\n> the solution is:\r\n> \r\n> ```\r\n> Out_imgData.rewind();\r\n> Bitmap bitmap_out = Bitmap.createBitmap(DIM_IMG_SIZE_X , DIM_IMG_SIZE_Y, Bitmap.Config.ARGB_8888);\r\n> int pixel = 0;\r\n> int[] intValues_test = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\n> mBitmapIn.getPixels(intValues_test, 0, mBitmapIn.getWidth(), 0, 0, mBitmapIn.getWidth(), mBitmapIn.getHeight());\r\n> int[] pixels = new int[DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y];\r\n> for (int i = 0; i < DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y; i++) {\r\n>           final int val = intValues_test[pixel++];\r\n>           int a = 0xFF;\r\n>           float r = Out_imgData.getFloat();\r\n>           float g = Out_imgData.getFloat();\r\n>           float b = Out_imgData.getFloat();\r\n>           pixels[i] = a << 24 | (int)r << 16 | (int)g << 8 | (int)b;\r\n>         }\r\n> bitmap_out.setPixels(pixels, 0, DIM_IMG_SIZE_X, 0, 0, DIM_IMG_SIZE_X, DIM_IMG_SIZE_Y);\r\n> ```\r\nHey is this method used for grayscale images or RGB images. Moreover what is the mBitmapIn var?\r\n", "How does interpreter return bytebuffer. Can someone please guide. "]}, {"number": 23482, "title": "No such file or directory  #include \"npy_1_7_deprecated_api.h\"", "body": "#1243 similar problem re-appears on Centos 7.5 w/ kernel 3.10.0-862.14.4.el7.x86_64 w/ 32 GB of memory after\r\n\r\n`git clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit pull\r\ngit checkout v1.11.0\r\n`\r\n\r\nleads to\r\n\r\n`ERROR: /home/user/tensorflow/tensorflow/python/BUILD:5553:1: C++ compilation of rule '//tensorflow/python:framework/fast_tensor_util.so' failed (Exit 1) gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG ... (remaining 49 argument(s) skipped)\r\n\r\nUse --sandbox_debug to see verbose messages from the sandbox\r\nIn file included from bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/ndarrayobject.h:18:0,\r\n                 from bazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/arrayobject.h:4,\r\n                 from bazel-out/host/genfiles/tensorflow/python/framework/fast_tensor_util.cpp:581:\r\nbazel-out/host/genfiles/external/local_config_python/numpy_include/numpy/ndarraytypes.h:1821:36: fatal error: npy_1_7_deprecated_api.h: No such file or directory\r\n #include \"npy_1_7_deprecated_api.h\"\r\n                                    ^\r\ncompilation terminated.\r\nINFO: Elapsed time: 71.255s, Critical Path: 14.15s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 545 processes: 545 processwrapper-sandbox.\r\nFAILED: Build did NOT complete successfully\r\n`\r\n\r\n_Originally posted by @ng0177 in https://github.com/tensorflow/tensorflow/issues/1243#issuecomment-435663567_", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. Thank you for your cooperation.\r\n", "Sorry, I thought the set of information is complete: a fresh Centos 7.5 following https://docs.bazel.build/versions/master/install-redhat.html to install bazel, then the download script from above. As to python that comes with standard Centos epel repositories:\r\n\r\nPython 3.4.9\r\npip 8.1.2 from /usr/lib/python3.4/site-packages (python 3.4)\r\n\r\nCan anyone on reproduce the problem on Centos or installed it successfully? Thank you!", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "As 1.x is deprecated, please try on latest version . Thank you  "]}, {"number": 23481, "title": "Add image-pipe ops for zero-copy image generation", "body": "An ops in place of `tf.keras.preprocessing.ImageDataGenerator` with very high-performance data generation performance loading from on-disk original image directories to GPU directly with ZeroCopy, which could achieve **~96% performance of synthetic dataset training** for modern models like Resnet50/Inception3/..\r\n\r\n1) Multi-worker deterministic image input by configuration of `seed`, which is not supported by `tf.keras.preprocessing.ImageDataGenerator`;\r\n2) Support direct image generation with either `NCHW` or `NHWC` format;\r\n3) Support target image resize in place and interleaving generation;\r\n4) Reference of internal image directory format -\r\n\r\n```sh\r\n/train/\r\n    /class-monkey/\r\n        aug_1.jpg\r\n        aug_2.jpg\r\n    /class-bird/\r\n        aug_1.jpg\r\n        aug_2.jpg\r\n```", "comments": ["Thanks for the contribution! Since we are in the process of [sunsetting `tf.contrib`](https://github.com/tensorflow/community/pull/18), we are not currently accepting new submodules.\r\n\r\nHowever, this contribution might make sense as part of the [Special Interest Group on IO](https://groups.google.com/a/tensorflow.org/forum/#!topic/developers/2IjSiQjUw5o), which is planned to host a repository of community-maintained I/O-related code (primarily `Dataset` and `FileSystem` implementations).\r\n\r\nI'm going to close this PR for now, but please consider joining the [SIG IO mailing list](https://groups.google.com/a/tensorflow.org/forum/#!forum/io), and contributing to that repository when it is set up.\r\n\r\n/cc @ewilderj ", "OK, Thanks.", "Thanks", "@ghostplant The sig-io and its repo is in place now: https://github.com/tensorflow/io\r\n\r\nYou can consider open a PR in the repo and join the discussion in sig-io groups: https://groups.google.com/a/tensorflow.org/forum/#!forum/io", "@yongtang Yeah, that's great!", "@yongtang Do you know there is a C-based header library included in Tensorflow that could iterate filesystem's directories and file elements on any generic OS platform (e.g. Linux, Windows, MacOS) like what `os.walk` in python does?", "OK, I found a good reference here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/posix/posix_file_system.cc", "@ghostplant You can also check [MatchingFilesDatasetOp](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/matching_files_dataset_op.cc), which can iterate filesystem's directories and files on Linux, Windows, and Mac as well. Its Python API is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/experimental/ops/matching_files.py)."]}, {"number": 23480, "title": "Quick fix for 462a79b that breaks GDR.", "body": "Please see https://github.com/tensorflow/tensorflow/issues/22081#issuecomment-435316069 for the context.\r\n\r\n@wangshuaizs Could you please check if this patch solves your problem?\r\n\r\nThis patch should fix #22081.", "comments": []}, {"number": 23479, "title": "Pretty print the dtype in error message with eager", "body": "This fix is related to #23452 where the dtype in the error message is a non-descriptive integer:\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> print(1.2*tf.constant(2))\r\nTraceback (most recent call last):\r\n...\r\n...\r\nTypeError: Cannot convert value 1.2 to EagerTensor with requested dtype: 3\r\n>>>\r\n```\r\n\r\nThis fix converts the integer (e.g., `3`) to a descriptive string:\r\n```\r\nTypeError: Cannot convert value 1.2 to EagerTensor with requested dtype: int32\r\n```\r\n\r\nThis fix fixes #23452.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 23478, "title": "Will Python 3.7 support come soon?", "body": "Will Tensorflow support be extended to Python 3.7 anytime in the near future? Given that Miniconda has upgraded their default Python to 3.7,  I imagine there'd be higher demand to install TF on Python 3.7.\r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Stretch\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: Master branch\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n", "comments": ["this week, with 1.13.0rc0 ( as far as I know, not so sure )", "Thanks @alanpurple ! I build TF from source with a bunch of C flags in my Docker so I'll be happy to report to whether/if things break during the build or not!", "@sadatnfs  -  Please track #20517 for more updates. Thanks !", "[Tensorflow 1.13.1](https://github.com/tensorflow/tensorflow/releases/tag/v1.13.1) now supports Python 3.7.", "> [Tensorflow 1.13.1](https://github.com/tensorflow/tensorflow/releases/tag/v1.13.1) now supports Python 3.7.\r\n\r\nI cannot find the package on this page:\r\nhttps://www.tensorflow.org/install/pip?lang=python3? "]}, {"number": 23477, "title": "Tensorflow's Estimator stops training", "body": "I asked this question on Stackoverflow 2 days ago (https://stackoverflow.com/questions/53089945/tensorflows-estimator-stops-training).\r\n\r\nI am training a model using Tensorflow's Estimator, but it suddenly stops training after 2600 steps after performing an evaluation. This is the last part of my code:\r\n\r\n```\r\ndef train():\r\n    train_input_func = lambda: input_fn(mode='train')\r\n    eval_input_func = lambda: input_fn(mode='eval')\r\n\r\n    est_conf = tf.estimator.RunConfig(cfg.model_dir, save_checkpoints_secs=120)\r\n    estimator = tf.estimator.Estimator(model_fn, cfg.model_dir, est_conf)\r\n\r\n\r\n    Path(estimator.eval_dir()).mkdir(parents=True, exist_ok=True)\r\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_func)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_func, throttle_secs=120)\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n\r\nif __name__ == '__main__':\r\n    train()\r\n```\r\n\r\nand this is the `input_fn`:\r\n\r\n```\r\ndef input_fn(mode=None):\r\n        data_generator = lambda: data_loader.data_generator(mode=mode)\r\n\r\n        dataset = tf.data.Dataset.from_generator(data_generator,\r\n                                                 output_types=(tf.int32, tf.int32),\r\n                                                 output_shapes=([None], [None]))\r\n\r\n        if mode is 'train':\r\n            dataset.shuffle(cfg.shuffle_buffer).repeat(1000)\r\n\r\n        dataset = dataset.padded_batch(cfg.batch_size, padded_shapes=([None],[None])).prefetch(1)\r\n\r\n        return dataset\r\n```\r\n\r\nThe training simply stops like this:\r\n\r\n```\r\nloss = 3.530099, step = 2500 (4.443 sec)          \r\nglobal_step/sec: 25.3421 \r\nloss = 1.3306174, step = 2600 (3.946 sec)         \r\nSaving checkpoints for 2604 into ./model/model.ckpt.                                                \r\nCalling model_fn.        \r\nDone calling model_fn.   \r\nStarting evaluation at 2018-11-03-12:15:23        \r\nGraph was finalized.     \r\n2018-11-03 15:45:23.655651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-03 15:45:23.655681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-03 15:45:23.655685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0          \r\n2018-11-03 15:45:23.655689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N          \r\n2018-11-03 15:45:23.655805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10402 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nRestoring parameters from ./model/model.ckpt-2604 \r\nRunning local_init_op.   \r\nDone running local_init_op.                       \r\nEvaluation [10/100]      \r\nEvaluation [20/100]      \r\nEvaluation [30/100]      \r\nEvaluation [40/100]      \r\nEvaluation [50/100]      \r\nEvaluation [60/100]      \r\nEvaluation [70/100]      \r\nEvaluation [80/100]      \r\nEvaluation [90/100]      \r\nEvaluation [100/100]     \r\nFinished evaluation at 2018-11-03-12:15:26        \r\nSaving dict for global step 2604: acc = 0.9593193, global_step = 2604, loss = 2.918349              \r\nSaving 'checkpoint_path' summary for global step 2604: ./model/model.ckpt-2604                      \r\nLoss for final step: 0.83287233.                  \r\n\r\n```\r\nIsn't it supposed to continue training until the end of the last epoch?\r\n\r\nAlso, if I run it on CPU, I recieve this:\r\n\r\n```\r\nCaused by op 'cond_1/GatherV2_1', defined at:                                                                                                                                            \r\n  File \"tf_pos_lstm.py\", line 125, in <module>                                                                                                                                           \r\n    train()                                                                                                                                                                              \r\n  File \"tf_pos_lstm.py\", line 122, in train\r\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 471, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 610, in run\r\n    return self.run_local()\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\", line 711, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 356, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1181, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1211, in _train_model_default\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\", line 1169, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"tf_pos_lstm.py\", line 56, in model_fn\r\n    crf_params)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 250, in crf_log_likelihood\r\n    transition_params)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 114, in crf_sequence_score\r\n    false_fn=_multi_seq_fn)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/layers/utils.py\", line 202, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py\", line 59, in smart_cond\r\n    name=name)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2087, in cond\r\n    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1920, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 106, in _multi_seq_fn\r\n    transition_params)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 320, in crf_binary_score\r\n    flattened_transition_indices)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2669, in gather\r\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3232, in gather_v2\r\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\r\nFile \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/home/nazarbin/miniconda3/envs/cpuenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): indices[12,23] = 551 is not in [0, 529)\r\n         [[{{node cond_1/GatherV2_1}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](cond_1/Reshape_4, cond_1/ad\r\nd_2, gradients/f_count)]]\r\n```\r\n\r\n\r\n**System information**\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux deep 4.13.0-46-generic #51-Ubuntu SMP Tue Jun 12 12:36:29 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 7.2.0-8ubuntu3.2) 7.2.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux deep 4.13.0-46-generic #51-Ubuntu SMP Tue Jun 12 12:36:29 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy               1.14.5\r\nprotobuf            3.6.0\r\ntensorflow          1.11.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.11.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nSat Nov  3 16:05:47 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.67                 Driver Version: 390.67                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   43C    P5    37W / 250W |      0MiB / 11176MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n```\r\n", "comments": ["@karmel PTAL", "Nagging Assignee @rchao: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Moved this issue to estimator repo (https://github.com/tensorflow/estimator/issues/16)  to be tracked. Thanks."]}, {"number": 23476, "title": "Python Tensorflow crashes on saving the data", "body": "I made a Python script and I almost got it working, I am using this code at the end of my script:\r\n\r\n```\r\nfilepath = \"RNN_Final-{epoch:02d }-{val_acc:.3f}\"\r\n\r\n#Saves only the best ones\r\ncheckpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'))\r\n\r\nhistory = model.fit(\r\n    train_x, train_y,\r\n    batch_size=BATCH_SIZE,\r\n    epochs=EPOCHS,\r\n    validation_data=(validation_x, validation_y),\r\n    callbacks=[tensorboard, checkpoint])\r\n```\r\nAnd this is the error I get:\r\n\r\n```\r\nFile \"/var/www/test.nl/test.py\", line 162, in <module>\r\n    callbacks=[tensorboard, checkpoint])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 1605, in fit\r\n    validation_steps=validation_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 238, in fit_loop\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.py\", line 214, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.py\", line 568, in on_epoch_end\r\n    filepath = self.filepath.format(epoch=epoch + 1, **logs)\r\nValueError: Invalid conversion specification\r\n```\r\n\r\nThe script crashes on this line:\r\n```\r\ncallbacks=[tensorboard, checkpoint])\r\n```\r\nWhat can I do, or what am I doing wrong?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 23475, "title": "Using tensor as parameter for tf.range and tf.slice", "body": "**System information**\r\n- TensorFlow version (you are using): 1.11 cpu, Python 3.6.5\r\n- Are you willing to contribute it (Yes/No): Yes? But contribute what?\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am using a model that will generate 2 intermediate values `yp1` and `yp2`( with shape [1, ] ) during training, who are supposed to be the start index and end index of another tensor `context_idxs` who has type [1, *] where the second dimension will change each input.\r\n\r\nNow I'd like to extract `context_idxs[yp1 : yp2]` and further feed them into another LSTM cell, but since yp1 and yp2 is tensor, I could not do that directly or with `tf.slice`. I find a function called `tf.gather` that seems to be what I'm looking for, however, I need to generate a range `[yp1, yp1+1, yp1+2, ..., yp2]` to use `tf.gather`.\r\n\r\nThen here comes the problem. `tf.range` seems  not to support using tensor as parameter. However, I can't know the value of yp1 and yp2 in the model (I know how to print them in sess.run, but I'm using them inside the model rather than from outside the graph).\r\n\r\nI'll use a quick example\r\n```python\r\nself.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\r\nself.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\r\n\r\n# ??\r\n# how do I get context_idxs[yp1 : yp2]? If context_idxs has shape [64, *], how do I get context_idxs[ : , yp1 : yp2]?\r\n```\r\n\r\nMy questions are:\r\n1. How can I get dynamic slice `context_idxs[yp1 : yp2]` of one tensor based on other tensors?\r\n2. If I want to batch some values, for example, the yp1 and yp2 becomes `[64, 1]`, and context_idxs becomes [64, *], is there still any way to do so?\r\n3. Is there any other way to show the value of variable just-in-time without having to `sess.run` each time? Something like normal python will be great.\r\n\r\n**Will this change the current api? How?**\r\nMaybe. I know tensorflow is based on graph but I think allowing just-in-time calculation of values will greatly make debugging easier. Normal python will make life much easier...\r\n\r\n**Who will benefit with this feature?**\r\nI don't know if anyone else needs it as I'm not an expert.\r\n\r\n**Any Other info.**\r\nUbuntu 18.04.1\r\nPython 3.6.5\r\n", "comments": ["For question 1, the slice operation takes in tensors, for instance:\r\n```python\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\nyp1 = tf.constant(1)\r\nyp2 = tf.constant(3)\r\ncontext_ids = tf.constant([0, 2, 4, 8])\r\nsess.run(context_ids[yp1:yp2])\r\n```\r\nReturns `[2, 4]`.\r\n\r\nFor question 2, you'll need to clarify. If you set, for instance, in the example above,\r\n```python\r\ncontext_ids = tf.constant([[0, 10], [2, 12], [4, 14], [8, 18]])\r\nsess.run(context_ids[yp1:yp2, :])\r\n```\r\nYou'll get `[[2, 12], [4, 14]]`.\r\nIf you don't know the rank, you can use `...` instead of `:`.\r\n\r\nFor question 3, please look into \"eager mode\", which will be part of tensorflow 2.0, to be released soon.\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Hi drpngx, thanks for your reply, I think I need to make my questions more clear.\r\n\r\nAbout question 1, the yp1 and yp2 are not constant, but the output of a network(whose last layer is a linear layer), therefore yp1 and yp2 might be changing each time. When I tried to use them as indices, I got an error. I forgot to record it and I will see if I can reproduce it later.\r\n\r\nFor questions 2, suppose `context_ids = tf.constant([[0, 10], [2, 12], [4, 14], [8, 18]])`\r\nI have yp1 and yp2 of shape (3, 2), e.g., `[[0, 1], [1, 2], [0, 2]]` and I expect to get result `[[0], [12], [8, 18]]` (it might be padded and be the input of next layer). Is it possible?\r\n\r\nI look forward to eager mode.\r\n\r\nThanks!"]}, {"number": 23474, "title": "how can i solve this issue on importing tensorflow in linux?", "body": ">\r\n\r\n****System information****\r\n- OS Platform and Distribution: (Linux Ubuntu 18.04):\r\n- M device (e.HP-proBook) .\r\n- Tensor-flow installed by running on terminal \r\npip3 install --upgrade tensorflow-gpu\r\n- TensorFlow version:\r\n- Python version:3.6.6\r\n- Installed using pip.\r\n -I dont have a CUDA/cuDNN version but  Intelhaswall.\r\n- GPU model :Intel\u00ae Haswell Mobile\r\n-memory:7.7 Gi\r\n**the problem is :** \r\nI cant install and using tensor flow or numpy for object detection algorithms.\r\nI don't know how can i install virtualenv.\r\nI run on terminal\r\n$sudo apt install python-pip python3-pip      \r\n$pip3 install --upgrade tensorflow \r\nthe tensor flow already installed \r\nbut ,,when i run \r\n$python3\r\nand import tensorflow the error was \r\n![screenshot from 2018-11-03 09-05-52](https://user-images.githubusercontent.com/44716923/47949282-f708f980-df48-11e8-9032-98813b6a8767.png)\r\n\r\n\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n______________________________________________________________________\r\n\r\n**Any other info / logs**\r\ni dont want to use a black-screen !!! to type a command i want to clone a codes and running it from a GitHub,starting coding .. somone advice me to setup \r\n", "comments": ["**tensorflow-gpu** is for computers with an NVidia GPU.  See https://www.tensorflow.org/install/pip.  Assuming you only have the Intel graphics adapter, you'll need to uninstall **tensorflow-gpu** and install **tensorflow**.  This will run on your CPU.  I don't believe there is formal OpenCL/Intel GPU acceleration support but you can google for info on this if you're curious. ", "Did you try bjascob's suggestion?\r\nYou can follow the steps given below to create virtual environment:\r\nvirtualenv -p python3 venv-tf-new    \r\nsource venv-tf-new/bin/activate   \r\npip3 install tensorflow ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23473, "title": "tf.matmul fails with CUBLAS_STATUS_NOT_SUPPORTED for large matrices when using CUDA 9.1 ", "body": "I am hitting this issue on multiple development machines with different GPUs, and all versions of Tensorflow >= 1.9.0. I have a CUDA 9.1 requirement on the host machine, so downgrading to CUDA 9.0 is not an option for me. \r\n\r\nSee https://stackoverflow.com/questions/50911052/tensorflow-matmul-blas-xgemmbatched-launch-failed/50918250 for a possibly related issue, though that involved Tensorflow 1.8.0.\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes, a minimum repro is below_\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Linux Ubuntu 16.04_\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: _n/a_\r\n- TensorFlow installed from (source or binary): _source_\r\n- TensorFlow version (use command below): _1.9.0 to 1.11.0 (does not reproduce in 1.8.0)_\r\n- Python version: _2.7_\r\n- Bazel version (if compiling from source): _Bazel 0.10.0 for  Tensorflow 1.9.0, Bazel 0.18.0 for Tensorflow 1.10.0 and newer_\r\n- GCC/Compiler version (if compiling from source): _GCC 5.4.0_\r\n- CUDA/cuDNN version: _CUDA 9.1.85 / cuDNN 7.0.5_\r\n- GPU model and memory: _GTX 1080 w/ 7400 MB, GTX 1060 w/ 5600 MB_\r\n\r\n**Describe the current behavior**\r\n\r\ntf.matmul is failing to multiply matrices above a certain size, with error:\r\n`failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED`\r\n\r\nI have confirmed using `nvidia-smi` that the GPU is nowhere close to running out of memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe matrix multiplication should complete successfully.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThis is borrowed from the stackoverflow link above:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\ntf.Session(config=config).close()\r\n\r\ndef calc():\r\n    N = 15 # works for N <= 14\r\n    a = 16\r\n    b = 8\r\n    X = np.random.rand(N, 11520, b, 1).astype(np.float32)\r\n    print(X.nbytes*1e-6, \"MB\")\r\n    W = np.random.rand(N, 11520, a, b).astype(np.float32)\r\n    print(W.nbytes*1e-6, \"MB\")\r\n    X_ = tf.constant(X, name=\"X-constant\", dtype=tf.float32)\r\n    W_ = tf.constant(W, name=\"W-constant\", dtype=tf.float32)\r\n\r\n    return tf.matmul(W_, X_, name=\"mymatmul\")\r\n\r\ntf.reset_default_graph()\r\na = calc()\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nb = sess.run(a)\r\nsess.close()\r\nprint(b.shape)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nI found a workaround for this issue is to patch `CUDABlas::DoBlasGemmBatchedInternal` in `tensorflow/stream_extractor/cuda/cuda_blas.cc` to disable the `#if CUDA_VERSION >= 9010` block which calls `wrap::cublasGemmBatchedEx`. Downgrading to Tensorflow <= 1.8.0 also resolves the issue for me (that codeblock was added in Tensorflow 1.9.0).\r\n\r\nRunning the above code (N=15) with Tensorflow 1.11.0:\r\n```\r\n2018-11-02 23:13:48.059128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 23:13:48.059860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.40GiB\r\n2018-11-02 23:13:48.059884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:13:48.282603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:13:48.282631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:13:48.282636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:13:48.282790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(5.529599999999999, 'MB')\r\n(88.47359999999999, 'MB')\r\n2018-11-02 23:13:48.963544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:13:48.963579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:13:48.963585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:13:48.963588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:13:48.963722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7137 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-11-02 23:13:51.146858: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED\r\n2018-11-02 23:13:51.146900: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details\r\nTraceback (most recent call last):\r\n  File \"./tf_matmul.py\", line 25, in <module>\r\n    b = sess.run(a)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\r\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\r\n\r\nCaused by op u'mymatmul', defined at:\r\n  File \"./tf_matmul.py\", line 22, in <module>\r\n    a = calc()\r\n  File \"./tf_matmul.py\", line 19, in calc\r\n    return tf.matmul(W_, X_, name=\"mymatmul\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 2015, in matmul\r\n    a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1245, in batch_mat_mul\r\n    \"BatchMatMul\", x=x, y=y, adj_x=adj_x, adj_y=adj_y, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[172800,16,8], b.shape=[172800,8,1], m=16, n=1, k=8, batch_size=172800\r\n\t [[{{node mymatmul}} = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](W-constant, X-constant)]]\r\n```\r\n\r\nReducing the matrix size (N=14):\r\n```\r\n2018-11-02 23:18:29.127555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 23:18:29.128409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\r\n2018-11-02 23:18:29.128441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:18:29.482292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:18:29.482346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:18:29.482361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:18:29.482670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(5.160959999999999, 'MB')\r\n(82.57535999999999, 'MB')\r\n2018-11-02 23:18:30.200524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:18:30.200586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:18:30.200598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:18:30.200605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:18:30.200837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7131 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(14, 11520, 16, 1)\r\n\r\n```\r\n\r\nAfter patching `CUDABlas::DoBlasGemmBatchedInternal` (N=15):\r\n\r\n```\r\n2018-11-02 23:28:05.667458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 23:28:05.668033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 7.39GiB\r\n2018-11-02 23:28:05.668046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:28:05.866043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:28:05.866069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:28:05.866074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:28:05.866218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(5.529599999999999, 'MB')\r\n(88.47359999999999, 'MB')\r\n2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n(15, 11520, 16, 1)\r\n\r\n```\r\n\r\nPatch for workaround:\r\n```patch\r\ndiff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc\r\nindex ab7091b..50579b6 100644\r\n--- a/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n+++ b/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n@@ -18,6 +18,8 @@ limitations under the License.\r\n \r\n #define SE_CUDA_DATA_HALF CUDA_R_16F\r\n \r\n+#define USE_CUBLAS_GEMM_BATCHED_EX false\r\n+\r\n #include \"tensorflow/stream_executor/cuda/cuda_blas.h\"\r\n \r\n // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As\r\n@@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(\r\n \r\n   cudaDataType_t data_type = CUDADataType<T>::type;\r\n \r\n-#if CUDA_VERSION >= 9010\r\n+#if CUDA_VERSION >= 9010 && USE_CUBLAS_GEMM_BATCHED_EX\r\n   int cc_major, cc_minor;\r\n   if (stream->parent()->GetDeviceDescription().cuda_compute_capability(\r\n           &cc_major, &cc_minor) &&\r\n```", "comments": ["Forwarding this to @azaks2 who has access to more CUDA expertise than I do.", "I had the same issue with cuda 9.2. Last patch for cuda fixes it.", "@sh1ng Thanks. I believe I was using the latest cuda 9.1 patches but will double-check. If that is the case, maybe this codepath should just be enabled for cuda 9.2 and later.", "Closing as stale because we use CUDA 10.1 now.  Please reopen if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23473\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23473\">No</a>\n", "> (5.529599999999999, 'MB')\r\n> (88.47359999999999, 'MB')\r\n> 2018-11-02 23:28:06.452688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n> 2018-11-02 23:28:06.452722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2018-11-02 23:28:06.452727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n> 2018-11-02 23:28:06.452730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n> 2018-11-02 23:28:06.452858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7129 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n> (15, 11520, 16, 1)\r\n> ```\r\n> \r\n> Patch for workaround:\r\n> \r\n> ```diff\r\n> diff --git a/tensorflow/stream_executor/cuda/cuda_blas.cc b/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n> index ab7091b..50579b6 100644\r\n> --- a/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n> +++ b/tensorflow/stream_executor/cuda/cuda_blas.cc\r\n> @@ -18,6 +18,8 @@ limitations under the License.\r\n>  \r\n>  #define SE_CUDA_DATA_HALF CUDA_R_16F\r\n>  \r\n> +#define USE_CUBLAS_GEMM_BATCHED_EX false\r\n> +\r\n>  #include \"tensorflow/stream_executor/cuda/cuda_blas.h\"\r\n>  \r\n>  // Both Eigen Half.h and CUDA cuda_fp16.h provide similar typedef for __half. As\r\n> @@ -2482,7 +2484,7 @@ port::Status CUDABlas::DoBlasGemmBatchedInternal(\r\n>  \r\n>    cudaDataType_t data_type = CUDADataType<T>::type;\r\n>  \r\n> -#if CUDA_VERSION >= 9010\r\n> +#if CUDA_VERSION >= 9010 && USE_CUBLAS_GEMM_BATCHED_EX\r\n>    int cc_major, cc_minor;\r\n>    if (stream->parent()->GetDeviceDescription().cuda_compute_capability(\r\n>            &cc_major, &cc_minor) &&\r\n> ```\r\n\r\nHi, I have the same problem. \r\n\r\n2020-04-01 20:21:43.695815: E tensorflow/stream_executor/cuda/cuda_blas.cc:652] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_NOT_SUPPORTED\r\n2020-04-01 20:21:43.695883: E tensorflow/stream_executor/cuda/cuda_blas.cc:2574] Internal: failed BLAS call, see log for details\r\n\r\nInternalError (see above for traceback): Blas xGEMMBatched launch failed : a.shape=[245760,8,16], b.shape=[245760,16,16], m=8, n=16, k=16, batch_size=245760\r\n\t [[node layers/einsum/MatMul (defined at /home/pai/3dfaceRe/RandLA-Net/RandLANet.py:327)  = BatchMatMul[T=DT_FLOAT, adj_x=false, adj_y=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layers/einsum/Reshape, layers/einsum/Reshape_1)]]\r\n\t [[{{node optimizer/gradients/layers/Encoder_layer_3LFAmlp2/BiasAdd_grad/BiasAddGrad/_663}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_6262_...iasAddGrad\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nI have Tensorflow=1.12, Cudatoolkit=9.2, and Cudnn=7.6.5 in Conda environment. I couldn't find the file of  \"tensorflow/stream_executor/cuda/cuda_blas.cc\" and only have \"tensorflow/stream_executor/cuda/cuda_blas.h\" \r\n\r\nDoes anyone know how to solve this problem? ", "> I have Tensorflow=1.12, Cudatoolkit=9.2, and Cudnn=7.6.5 in Conda environment.\r\n\r\nIf at all possible I'd suggest using a more recent version of TF.  1.12 is quite old, and we're not in a position to debug issues with it.", "My case due to a version conflict in CUDA9.0 and TensorFlow 1.12.0\r\n\r\nEasiest solution:  use a NumPy function wrap in TensorFlow.\r\n\r\nuse result = tf.py_func(np_matmul, [x, y], tf.float32) instead.\r\n"]}, {"number": 23471, "title": "Won't build tf r1.12rc2 because of MPI support", "body": "**System information**\r\n- OS Platform and Distribution: CentOS Linux release 7.5.1804 (Core)\r\n- TensorFlow installed from (source or binary): source, official release r1.12rc2\r\n- TensorFlow version: r1.12rc2\r\n- Python version: 3.6.6 (anaconda)\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.18.1\r\n- GCC/Compiler version (if compiling from source): 8.2.0 with experimental clang support (of configure),\r\nbut also not working on gcc4 (centos7-native), gcc6 (rh-scl-devtoolset-6), gcc7 (rh-scl-devtoolset-7)\r\n- CUDA/cuDNN version: None, integrated nGraph\r\n- GPU model and memory: None, CPU: AMD Ryzen 7 1800X\r\n\r\n**Describe the problem**\r\nA released source won't build if I enable MPI support flags while configuring.\r\nTried multiple gcc versions, however there was no luck.\r\nOpenMPI was also installed from official OMPI src release v3.1.2.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1> Usual ./configure after tarball extract\r\n2> config: Apache Ignite (Y) / XLA JIT (Y) / SYCL (N) / ROCm (N) / CUDA (N) / clang (Y) / MPI (Y) / COPT (-march=native) / Android (N)\r\n3>  bazel --server_javabase=$JAVA_HOME build --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" --config=opt --config=ngraph //tensorflow/tools/pip_package:build_pip_package\r\n4> (optional) if I disable MPI support while ./configure -ing, then tf r1.12rc2 builds successfully.\r\n\r\n**Any other info / logs**\r\n_Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached._\r\n\r\n[Manual Patch]\r\nI applied this manual fix: [[Pull req #20147](https://github.com/tensorflow/tensorflow/pull/20147/files)] of tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc only.\r\n\r\n[Logs]\r\n```\r\nINFO: From Compiling tensorflow/stream_executor/device_description.cc [for host]:\r\ntensorflow/stream_executor/device_description.cc:148:18: warning: 'DivideCeil' is deprecated: Use MathUtil::CeilOfRatio directly instead. [-Wdeprecated-declarations]\r\n  *block_count = DivideCeil(element_count, *threads_per_block);\r\n                 ^\r\n./tensorflow/stream_executor/device_description.h:362:1: note: 'DivideCeil' has been explicitly marked deprecated here\r\nABSL_DEPRECATED(\"Use MathUtil::CeilOfRatio directly instead.\")\r\n^\r\nexternal/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'\r\n#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))\r\n                                                ^\r\n1 warning generated.\r\nERROR: /home/xo/pyenv-ngraph/mpicol/tensorflow-1.12.0-rc2/tensorflow/contrib/mpi_collectives/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:58:6: error: explicit specialization of 'CopyTensorData<Eigen::ThreadPoolDevice>' after instantiation**\r\nvoid CopyTensorData<CPUDevice>(void* dst, void* src, size_t size) {\r\n     ^\r\n**./tensorflow/contrib/mpi_collectives/kernels/ring.h:165:3:** note: implicit instantiation first required here\r\n  CopyTensorData<Device>((void*)buffer, (void*)input->tensor_data().data(),\r\n  ^\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:71:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, int>' after instantiation**\r\nGENERATE_ACCUMULATE(int);\r\n^\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8:** note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n**./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5:** note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:72:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, long long>' after instantiation**\r\nGENERATE_ACCUMULATE(long long);\r\n^\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8:** note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n**./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5:** note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:73:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, float>' after instantiation**\r\nGENERATE_ACCUMULATE(float);\r\n^\r\n**tensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8:** note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n**./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5:** note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\n4 errors generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n**INFO:** Elapsed time: 1532.667s, Critical Path: 295.58s\r\n**INFO:** 6912 processes: 6912 local.\r\n**FAILED:** Build did NOT complete successfully\r\n```", "comments": ["Ping @jthestness", "Hi guys. I'm afraid we haven't moved beyond TF 1.7 yet, so I haven't encountered these issues myself. However, there might be some things that could help track down the issue:\r\n\r\nFirst, I don't think you should need to manually apply #20147, since it has already been merged. That change was in response to TF changes that moved the namespaces for threading between roughly TF 1.4 and 1.7. Also this summer, TF again changed namespaces (maybe for devices?) in roughly v1.8, and I suspect we will need to make changes analogous to #20147 to find the right includes and get the appropriate device types. (I'm actually surprised that the errors show Eigen::ThreadPoolDevice as the specialization type, since I believe the device specializations should be CPUDevice, GPUDevice, etc. --- that's a big red flag).\r\n\r\nIf I had time to debug this, I'd start by trying to bisect the builds starting around TF 1.8 and identify the PR that caused MPI collectives to stop building correctly. From there, it should be straightforward to figure out the appropriate includes to get correct device types.\r\n\r\nI'd also recommend a quick test to declare the explicit specializations with Eigen::ThreadPoolDevice right after the CopyTensorData and AccumulateTensorData declarations in `mpi_collectives/kernels/ring.h` just to see if that might fix the problem.", "@inscite  Any update ?", "@harshini-gadige Sorry for late response. Currently, I omitted the MPI support by skipping build flags (not to use distributed tensorflow, for a while.) Moreover, the brand-new release 1.12.0 came out recently; therefore, I have lots of configurations to test.\r\n\r\nAs @jthestness mentioned about several configurations for baseline, I'll try multiple builds for my desire.\r\n- OS: CentOS 7.5 x86_64\r\n- Host: KVM (due to the lack of physical hosts however it won't give any hardship)\r\n- GCC: 8.2.0 (custom build, since native 4.8.x won't build r1.12.0rc2 in my experience)\r\n- TF: 1.8/1.9/1.10.0/1.11.0/1.12.0\r\n\r\nIf there are any progresses or unexpected results, I'll leave some comments.\r\nThx for your consideration.\r\n\r\nRegards,\r\n@inscite\r\n", "Hi guys. Just a little update from my side: Although I haven't gotten to the root cause of the problem posted here, I was able to build and run with TF version 1.8.0. There is a small bug that needed to be fixed manually (though it was fixed in TF with #18907, commit 26b2814096bf6d9dd0af91a37a0706e17450b9ec). The patch for that fix is below.\r\n\r\nThis build indicates that TF 1.8.0 should work for @inscite 's needs with a small change. I will hopefully be able to test newer TF versions this week.\r\n\r\n```\r\ndiff --git a/tensorflow/contrib/mpi/mpi_utils.h b/tensorflow/contrib/mpi/mpi_utils.h\r\nindex df055ff567..4091925fc0 100644\r\n--- a/tensorflow/contrib/mpi/mpi_utils.h\r\n+++ b/tensorflow/contrib/mpi/mpi_utils.h\r\n@@ -22,6 +22,7 @@ limitations under the License.\r\n #include <string>\r\n #include <vector>\r\n \r\n+#include \"tensorflow/core/platform/logging.h\"\r\n #include \"tensorflow/core/lib/strings/str_util.h\"\r\n \r\n // Skip MPI C++ bindings support, this matches the usage in other places\r\n```", "Update: I've been able to build all minor versions of TF up to 1.12.0 (1.8.0, 1.9.0, 1.10.0, 1.11.0, and 1.12.0). They all appear to run a small test correctly. Here's my configuration so you can try to match most of it:\r\nUbuntu 18.04\r\nGCC 7.3.0\r\nPython 3.6.5\r\nEnvironment: VirtualEnv\r\nBazel version: For TF v1.7.0, v1.8.0: 0.10.0. For TF v1.9.0: 0.11.0. For TF v1.10.0, v1.11.0, v1.12.0: 0.15.0 (Following: https://www.tensorflow.org/install/source#tested_source_configurations)\r\nCUDA 9.2.88, cuDNN 7.1.4\r\nIntel CPUs, NVIDIA GPUs\r\n\r\nHere are some small caveats worth noting:\r\nTF 1.8.0: Requires a patch with #18907, commit 26b2814\r\nTF 1.9.0: Requires a patch with #20147, commit 1b79572c2d4 (HUGE THANKS TO @yongtang for both of these patches!)\r\nTF 1.11.0: Requires commit 89979f42e82\r\nTF 1.11.0 and 1.12.0: Requires that the virtual environment install Keras pip packages, keras_applications and keras_preprocessing (and maybe h5py?), per issue #21518\r\n\r\nAt this point, the most likely issue in @incite's original post is the Bazel version. I'd try changing that first.", "@jthestness   Thank you for your deep analysis.\r\n\r\n@inscite  -  Any update ? Does changing the Bazel version solve your issue ? Please keep us posted. Thanks !", "Greetings!\r\n\r\nHere's some progress about tensorflow source build.\r\nI'll update extra progress for multiple releases.\r\n\r\n**[Common DevEnv]**\r\nCPU: Intel(R) Xeon(R) Gold 6126 (skylake)\r\nOS: CentOS 7.3.1611\r\nGCC: 8.2.0\r\nOpenMPI: Mellanox HPC-X MPI (v3.0.1a1)\r\nPYENV: Anaconda Virtualenv w/ python 3.6.5\r\nJDK: 11.0.1\r\n\r\n**[Bootstrapping]**\r\nBefore every bazel build, I did..\r\n```\r\n$ bazel clean --expunge --async\r\n$ bazel shutdown\r\n$ rm -rf <bazel cache>\r\n```\r\n\r\n**[r1.12.0]**\r\n* XLA | MPI | MKL optz:\r\n```\r\n$ bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n* NOXLA | MPI | nGraph optz:\r\n```\r\n$ bazel build --config=opt --config=ngraph //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n- bazel 0.15.0 | XLA | MPI | MKL optz: Success\r\n- bazel 0.18.0 | XLA | MPI | MKL optz: Success (bazel version may not an issue)\r\n- bazel 0.18.0 | NOXLA | MPI | nGraph optz: Success (graph compiler may not an issue)\r\n\r\n**[r1.11.0]**\r\n* XLA | MPI | MKL optz:\r\n```\r\n$ bazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n# after I met build error,\r\n$ git clone https://github.com/tensorflow/tensorflow.git\r\n$ git checkout 89979f42e827d9eb5c349259a5aa2ec32d38c86a\r\n```\r\n* NOXLA | MPI | nGraph optz:\r\n```\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n- bazel 0.18.0 | XLA | MPI | MKL optz | no patch 89979f42e827d9eb5c349259a5aa2ec32d38c86a: Fail\r\n- bazel 0.18.0 | XLA | MPI | MKL optz | patch 89979f42e827d9eb5c349259a5aa2ec32d38c86a: Success\r\n- bazel 0.18.0 | NOXLA | MPI | nGraph optz: Fail (however the error is related to nGraph, not a MPI)\r\n\r\nOne suspicious log I found is related to bazel warning:\r\n```\r\nWARNING: Output base '/home/xo/.cache/bazel/_bazel_xo/385bf1ed0f5ed6366bd1e06f79b126ed' is on NFS. This may lead to surprising failures and undetermined behavior.\r\n```\r\nActually, I moved 'base source build partition' from NFS to Lustre (there are no local storage) since I experienced multiple errors while building several applications with gcc.\r\nDoes it cause any errors while building tensorflow? I still have no idea.\r\n\r\n**[Conclusion]**\r\nI also succeed to build TF r1.11.0/1.12.0 with minimal revision.\r\nSince there are a couple of new stable releases, I'll only stick with r1.11.0, r1.12.0.\r\nStill, for backup usage, I'll test for MPI support.\r\nThank you for your support: @jthestness @harshini-gadige @byronyi @yongtang \r\n\r\nMay I close the issue?", "@inscite Thanks for the investigation! It looks like the issue could be closed, but feel free to reopen if the issue remains.", "Hey, I am using tf1.14 with Intel MKL dependency [gcc 7.4.0; Machine: Sky Lake-E ;bazel:0.26.1] and the problem persists :\r\nIn reference to https://github.com/tensorflow/tensorflow/issues/23471#issuecomment-438420573 , I have Keras,+_preprocessing,+_applications & h5py installed in my environment.\r\n\r\nERROR: /home/vallari/tensorflow-master/tensorflow/contrib/mpi_collectives/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:58:6: error: explicit specialization of 'CopyTensorData<Eigen::ThreadPoolDevice>' after instantiation\r\nvoid CopyTensorData<CPUDevice>(void* dst, void* src, size_t size) {\r\n     ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:165:3: note: implicit instantiation first required here\r\n  CopyTensorData<Device>((void*)buffer, (void*)input->tensor_data().data(),\r\n  ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:71:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, int>' after instantiation\r\nGENERATE_ACCUMULATE(int);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:72:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, long long>' after instantiation\r\nGENERATE_ACCUMULATE(long long);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:73:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, float>' after instantiation\r\nGENERATE_ACCUMULATE(float);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\n4 errors generated.\r\n\r\n\r\n\r\n\r\n\r\n", "Greetings @rvallari1,\r\n\r\nIt seems like your situation is quite far from the initial problem of this issue.\r\nStill, I can confirm that tensorflow r1.14 source build with MKL/MPI support have an ISSUE.\r\n\r\nThis issue can be fixed with two file patch. (Thanks to [17437#issuecomment-372552174)](https://github.com/tensorflow/tensorflow/issues/17437#issuecomment-372552174)\r\nThe patch below contains an additional fix (since r1.14 have a different configuration from r1.10)\r\n\r\n```\r\ndiff -r tensorflow-1.14.0-org/tensorflow/contrib/mpi_collectives/BUILD tensorflow-1.14.0/tensorflow/contrib/mpi_collectives/BUILD\r\n55a56\r\n>         \"//tensorflow/stream_executor\",\r\ndiff -r tensorflow-1.14.0-org/tensorflow/tensorflow.bzl tensorflow-1.14.0/tensorflow/tensorflow.bzl\r\n1773c1773\r\n<             clean_dep(\"//tensorflow/core:lib\"),\r\n---\r\n>             # clean_dep(\"//tensorflow/core:lib\"),\r\n```\r\n\r\n```\r\nINFO: Elapsed time: 8634.383s, Critical Path: 346.61s\r\nINFO: 12862 processes: 12862 local.\r\nINFO: Build completed successfully, 13719 total actions\r\n```\r\n\r\nAs a result, this build ends in success, finally.\r\nI had not built the package as whl yet, but you can try.\r\n\r\nFurthermore...\r\n1. DO NOT FORGET to include mpi directory in LD_LIBRARY_PATH or ldconfig.\r\nThe easiest way to check this configuration actually visible while executing './configure'.\r\nIf the library path of OMPI set correctly, the configuration process get path automatically.\r\nIf not, then you have to make sure the right environmental setup.\r\n2. Whenever you initiate new build, please clean-up previous bazelrc configuration.\r\nTry 'bazel clean --expunge --async' and 'rm -f .tf_configure.bazelrc' if exists.\r\n\r\nGood luck!\r\n\r\nTested environment:\r\nOS: CentOS 7.6 x86_64\r\nGCC: 7.4.0 with manual source build\r\nOpenMPI: 3.1.4 with manual source build", "@inscite the problem persists. used bazel clean --expunge --async' and 'rm -f .tf_configure.bazelrc..\r\nput all the paths in library and editted the code as per you suggested.\r\ngcc: 7.4.0\r\nOpenMPI: 4.0.1\r\n\r\nWere you able to build it ?\r\n", "Also, I am able to build tf1.12 with mkl support from source , but I require an updated version of tf[1.13.1 or higher].. ", "@rvallari1 May I request the error log you have seen? Is it same with your previous log?\r\nMeanwhile, how about the MPI directory lookup while tensorflow setup configuration?\r\nBelow is an example of my previous configuration output.\r\nSee and check if there exist any differences.\r\n\r\n```\r\n$ ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.24.1 installed.\r\nPlease specify the location of python. [Default is /home/<user>/miniconda3/envs/tf1140mklmpi/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/<user>/miniconda3/envs/tf1140mklmpi/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/<user>/miniconda3/envs/tf1140mklmpi/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]:\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]:\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]:\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: y\r\nMPI support will be enabled for TensorFlow.\r\n\r\nPlease specify the MPI toolkit folder. [Default is ]: /home/<user>/opt/openmpi-3.1.4\r\n(Comment: In my case, This directory does not show automatically in wrong case.\r\nOtherwise, try fill your OpenMPI path.)\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n$ bazel --server_javabase=$JAVA_HOME build --copt=\"-DEIGEN_USE_VML\" --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```", "No difference except one command:-\r\nPlease specify the location of python. [Default is /home/vallari/anaconda3/envs/inteldp/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/vallari/anaconda3/envs/inteldp/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/vallari/anaconda3/envs/inteldp/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: y\r\nClang will be downloaded and used to compile tensorflow.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: y\r\nMPI support will be enabled for TensorFlow.\r\n\r\nPlease specify the MPI toolkit folder. [Default is /opt/intel/compilers_and_libraries_2019.4.243/linux/mpi/intel64]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: -config=mkl\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\n\r\nCOMMAND: \r\nbazel build --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er //tensorflow/tools/pip_package:build_pip_package \r\n\r\n\r\nERROR:\r\nERROR: /home/vallari/tensorflow-master/tensorflow/contrib/mpi_collectives/BUILD:40:1: C++ compilation of rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so' failed (Exit 1)\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:58:6: error: explicit specialization of 'CopyTensorData<Eigen::ThreadPoolDevice>' after instantiation\r\nvoid CopyTensorData<CPUDevice>(void* dst, void* src, size_t size) {\r\n     ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:165:3: note: implicit instantiation first required here\r\n  CopyTensorData<Device>((void*)buffer, (void*)input->tensor_data().data(),\r\n  ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:71:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, int>' after instantiation\r\nGENERATE_ACCUMULATE(int);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:72:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, long long>' after instantiation\r\nGENERATE_ACCUMULATE(long long);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:73:1: error: explicit specialization of 'AccumulateTensorData<Eigen::ThreadPoolDevice, float>' after instantiation\r\nGENERATE_ACCUMULATE(float);\r\n^\r\ntensorflow/contrib/mpi_collectives/kernels/ring.cc:65:8: note: expanded from macro 'GENERATE_ACCUMULATE'\r\n  void AccumulateTensorData<CPUDevice, type>(type * dst, type * src, \\\r\n       ^\r\n./tensorflow/contrib/mpi_collectives/kernels/ring.h:221:5: note: implicit instantiation first required here\r\n    AccumulateTensorData<Device, T>(segment_update, segment_recv,\r\n    ^\r\n4 errors generated.\r\n", "Dear @rvallari1,\r\nhow's going with your issue?\r\nBased on my survey, I would like to ask you some questions.\r\n\r\n- Are you going to build your own tensorflow with Intel Parallel Studio XE?\r\n- (If so, ) Do you have a separate installation of anaconda or miniconda?\r\n- (If not, ) Do you really have an installation of OpenMPI?"]}, {"number": 23470, "title": "Error in tensorflow __init__: TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'", "body": "- TensorFlow version: v1.11.0-0-gc19e29306c 1.11.0\r\n- Python version: \r\n    3.6.4, via Anaconda (Anaconda3-5.3.0-Linux-x86_64), and also 3.6.7, and 3.7 via Anaconda\r\n    also 3.6.4 as a direct install\r\n- Centos7, in a Docker container, with all dependencies pip installed\r\n\r\nDuring \\_\\_init\\_\\_ on import of tensorflow,  seeing ```TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'`` - exception trace below\r\n\r\n```\r\ntry:\r\n    print(\"Importing tensorflow\", flush=True)\r\n    import tensorflow as tf\r\n    print(\"Imported tensorflow\", tf, flush=True)\r\n   \r\nexcept:\r\n    pass\r\n```\r\nThe circumstances are unusual and difficult to reproduce. The code runs fine except in situ installed to support a Streams python map operator. What's needed here is some guidance as to what could cause this specific failure at this specific location in the `__init__` method on import (specifically, at  `import tensorflow.python.ops import array_ops`), and what measures should be taken to address it.\r\n\r\n**Other info / logs**\r\n```\r\nImporting tensorflow\r\n02 Nov 2018 22:01:40.634-0400 [62] ERROR #splapplog,J[0],P[0],CandidateEvents.MinimalExtractedObjectsSRLNN,python M[splpy_general.h:importModule:352]  - CDIST0305E: Fatal error: missing module: CandidateEventsExtractor.\r\nTraceback (most recent call last):\r\n  File \"/tmp/StorylineAggregator1.distributed/toolkits/com.ibm.btn.util.candidate_events_extractor/opt/python/streams/CandidateEventsExtractor.py\", line 5, in <module>\r\n    from SRLNN_Pipeline import SRLNN_Pipeline\r\n  File \"/tmp/StorylineAggregator1.distributed/toolkits/com.ibm.btn.util.candidate_events_extractor/opt/python/modules/SRLNN_Pipeline.py\", line 9, in <module>\r\n    from sentence_embeddings import read_corpus, do_faiss\r\n  File \"/tmp/StorylineAggregator1.distributed/toolkits/com.ibm.btn.util.candidate_events_extractor/opt/python/modules/sentence_embeddings.py\", line 19, in <module>\r\n    import tensorflow_hub as hub\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow_hub/__init__.py\", line 21, in <module>\r\n    import tensorflow as tf\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 52, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 27, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 35, in <module>\r\n    from tensorflow.python.ops import array_ops\r\n  File \"/opt/conda/envs/srl-nn/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 153, in <module>\r\n    listdiff.__doc__ = gen_array_ops.list_diff.__doc__ + \"\\n\" + listdiff.__doc__\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\r\n02 Nov 2018 22:01:40.639-0400 [62] ERROR #splapplog,J[0],P[0],CandidateEvents.MinimalExtractedObjectsSRLNN,spl_pe M[PEImpl.cpp:logTerminatingException:2183]  - CDISR5033E: An exception occurred during the execution of the CandidateEvents.MinimalExtractedObjectsSRLNN operator. Processing element number 0 is terminating.\r\n02 Nov 2018 22:01:40.639-0400 [62] ERROR #splapptrc,J[0],P[0],CandidateEvents.MinimalExtractedObjectsSRLNN,spl_operator M[PEImpl.cpp:handleOperatorFailure:661]  - CDISR5030E: An exception occurred during the execution of the CandidateEvents.MinimalExtractedObjectsSRLNN operator. The exception is: Unknown Python error", "comments": ["This turned out to be, as expected, an environment issue, resolved by proper setting of PYTHONPATH, among other steps."]}, {"number": 23469, "title": "ModelCheckpoint Callback Doesn't Log Validation Accuracy on ML Engine", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ML Engine Instance (Debian)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 2.7.14\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI've set up a CNN and have created callbacks for model checkpointing, TensorBoard visualization, and early stopping.  I have ~408K samples in my `tf.data.Dataset`, am using a batch size of 256, and train my model with 1,000 steps per epoch with 1000 epochs.  When I do this, everything trains correctly on ML Engine but I don't get logging like I normally do.\r\n\r\n**Describe the expected behavior**\r\n\r\nI should see logging on ML Engine that indicates validation accuracy for each epoch, like this:\r\n\r\nEpoch 2/2000\r\n1/200 [..............................] - ETA: 2:00 - loss: 5.7345 - acc: 0.1055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n\r\n2/200...\r\n\r\nWhen I set `steps_per_epoch` to be only 200, I *do* see validation accuracy logging as expected.  For what it's worth, the training still works and produces .h5 files.  But it's crucial that I be able to view logging as the model's training.\r\n\r\nI spoke with folks on the ML Engine team and it's not an issue on their end.  They suspect it's a bug with the `tf.keras.callbacks.ModelCheckpoint` callback or me misusing `steps_per_epoch` somehow.  Is there a certain value I shouldn't exceed with `steps_per_epoch`?  I wanted to do one full pass over the data, so that should be: # samples / (steps_per_epoch * batch_size), which would be ~1,593 steps.  Even 1,000 steps_per_epoch causes this issue though.\r\n\r\n**Code to reproduce the issue**\r\n\r\nUnfortunately, I can't share the data I'm using.  I'm using a CNN built with `tf.keras.layers` layers (called `model`) and my compile step, callbacks definitions, and fit step look like this:\r\n\r\n        model.compile(\r\n            loss='sparse_categorical_crossentropy',\r\n            optimizer='adam',\r\n            metrics=['accuracy'],\r\n        )\r\n        callbacks = [\r\n            ModelCheckpoint(\"model_{epoch:04d}_{val_acc:.4f}.h5\",\r\n                            monitor='val_acc',\r\n                            verbose=1,\r\n                            save_best_only=True,\r\n                            mode='max'),\r\n            TensorBoard(),\r\n            EarlyStopping(monitor='val_acc', patience=5, min_delta=0, mode='max')\r\n        ]\r\n\r\n        model.fit(dataset, steps_per_epoch=1000, epochs=1000, validation_data=val_dataset,\r\n                  validation_steps=3, callbacks=callbacks)\r\n\r\n**Other info / logs**\r\n\r\nHere's what I see in the ML Engine logs:\r\n\r\n`14:24 Epoch 1/4`\r\n\r\n`15:00 Starting evaluation`\r\n\r\nWith no logging in between \ud83d\udc4e \r\n", "comments": ["@bclayman,\r\nSorry for the delayed response. Can you please confirm if your issue is resolved?\r\n\r\nAlso, the argument, `steps_per_epoch` is not Mandatory. So, you can try leaving it empty and the value will be derived as stated below, in the [Documentation of Model.fit (steps_per_epoch)](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit):\r\n\r\n> When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If x is a tf.data dataset, and 'steps_per_epoch' is None, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument. This argument is not supported with array inputs.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23469\">No</a>\n"]}, {"number": 23468, "title": "Add option for inferring op attributes from inputs", "body": "In graph execution mode, the `NodeDefBuilder` will take care of inferring type and number attributes from operation inputs as they are added. Eager execution does not support for now. Unfortunately, some clients, as in Java, assume this feature to be present in both cases.\r\n\r\nThis PR is to enhance eager execution mode with input attribute inference to replicate normal (graph) mode. This behaviour is marked as optional and is by default disabled, so current clients that already take care of doing that inference on their own won't be impacted.", "comments": ["Update for the reviewers: I've noticed that the comment above the new option in `c_api.h` was outdated and I took a chance to push a new version that is more accurate, sorry for the trouble. ", "@iganichev , @alextp , I have a question for you guys: if the input attributes inference is applied in all cases (without option), should it remain in the C API or would it be better if it is done directly by the underlying `EagerOperation` core class?", "Doing it in EagerOperation is better. Ideally the C API only has API glue, and all the functionality is done in the core classes.", "@alextp : Ok, I checked to move automatic attribute inference at the core level but finally decided to left it in the C API because:\r\n1- This feature purpose is to establish a contract between the core and its clients, which is also the purpose of the C API. The core should be free to do it the way that suits it better.\r\n2- We won't accidentally break our clients if this \"hidden\" feature is removed in the core later\r\n3- ...it was simpler ;)", "To be honest, I not too aware of how Python in building eager operations, I added this feature to support the upcoming eager mode in Java. I can try to do the exercise, can you confirm me that Python is going through the C API for building those operations?", "Yes, python goes through the C API. @akshaym has more details.", "There are 2 python paths where this happens.\r\n\r\nOne is here (which your code should simplify): https://github.com/tensorflow/tensorflow/blob/0cfa4c157d317ccb4ccd1f6c85773f973d628695/tensorflow/python/eager/pywrap_tfe_src.cc#L2590\r\n\r\nThe other happens in the generated python code for op binding. For example (copy pasted from the generated code):\r\n```python\r\ndef accumulate_nv2_eager_fallback(inputs, shape, name=None, ctx=None):\r\n  r\"\"\"This is the slowpath function for Eager mode.\r\n  This is for function accumulate_nv2\r\n  \"\"\"\r\n  _ctx = ctx if ctx else _context.context()\r\n  if not isinstance(inputs, (list, tuple)):\r\n    raise TypeError(\r\n        \"Expected list for 'inputs' argument to \"\r\n        \"'accumulate_nv2' Op, not %r.\" % inputs)\r\n  _attr_N = len(inputs)  ##### <- Length inference\r\n  shape = _execute.make_shape(shape, \"shape\")\r\n  _attr_T, inputs = _execute.args_to_matching_eager(list(inputs), _ctx)  ###### <- Type inference\r\n  _inputs_flat = list(inputs)\r\n  _attrs = (\"N\", _attr_N, \"T\", _attr_T, \"shape\", shape)\r\n  _result = _execute.execute(b\"AccumulateNV2\", 1, inputs=_inputs_flat,\r\n                             attrs=_attrs, ctx=_ctx, name=name)\r\n  _execute.record_gradient(\r\n      \"AccumulateNV2\", _inputs_flat, _attrs, _result, name)\r\n  _result, = _result\r\n  return _result\r\n```\r\n\r\nThe code generator is this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/python_op_gen.cc\r\n\r\nWe pass these attrs when calling the backward function (hence they are included in the call to record_gradient), but I don't *think* they are necessary. ", "So I think in sight of this all we need is to check on the existing python tests that the manually filled attrs match the inferred attrs. If they do (and I think they should) I'm happy. Can you add this check to tfe_execute?", "Thanks a lot @akshaym for the details.\r\n\r\n@alextp, just to clarify, you are suggesting to do this check (i.e. manual attrs == inferred attrs) in existing Python tests or at runtime (i.e. in `TFE_Execute` like you mentioned)?", "Do it at runtime in tfe_execute now. We can then remove it later when we make the python code stop populating these attrs.", "Ok, things are unfortunately getting complicated:\r\n\r\n1. I can't validate duplicated attribute values in `TFE_Execute` since setting more than once the same attribute results in replacing the former value by the later. You can see this happening [here](https://github.com/tensorflow/tensorflow/blob/37b4d2ef3f90f53f8050f557038b65ff6d1a06bc/tensorflow/core/common_runtime/eager/attr_builder.h#L135). I can replace that logic by validating at the same spot that duplicate attributes must all have the same value (btw, I don't know if there are real cases of duplicate attributes right now).\r\n\r\n2. I think we forgot something when we've decided to enable input attrs inference by default. Inference of input list attributes can only work if the client adds it using the new `TFE_OpAddInputList`. Currently, clients call `TFE_OpAddInput` for each element of an input list instead. That makes it impossible for us to detect when ends the list or when starts the next input, and we lose track of the input attributes (we actually assume each input is a single one). Possible workarounds are:\r\n    - Disable automatically input attribute inference when we detect (easily) that an input list is being fed by calling `TFE_OpAddInput` multiple times.\r\n    - Have a distinct methods to support attribute inference (e.g. `TFE_OpAddInferableInput` and `TFE_OpAddInferableInputList`).\r\n    - Restore the option to enable/disable input attribute inference at `TFE_Op` construction, so the client knows that `TFE_OpAddInput` should be used exclusively for adding single inputs.\r\n\r\nAny other suggestion?", "On Tue, Nov 13, 2018 at 10:11 PM Karl Lessard <notifications@github.com>\nwrote:\n\n> Ok, things are unfortunately getting complicated:\n>\n>    1.\n>\n>    I can't validate duplicated attribute values in TFE_Execute since\n>    setting more than once the same attribute results in replacing the former\n>    value by the later. You can see this happening here\n>    <https://github.com/tensorflow/tensorflow/blob/37b4d2ef3f90f53f8050f557038b65ff6d1a06bc/tensorflow/core/common_runtime/eager/attr_builder.h#L135>.\n>    I can replace that logic by validating at the same spot that duplicate\n>    attributes must all have the same value (btw, I don't know if there are\n>    real cases of duplicate attributes right now).\n>\n> There are no use cases of duplicate attributes so we should check that\nthey are not duplicates and if it's set twice that they match.\n\n\n\n>\n>    1.\n>    2.\n>\n>    I think we forgot something when we've decided to enable input attrs\n>    inference by default. Inference of input list attributes can only work if\n>    the client adds it using the new TFE_OpAddInputList. Currently,\n>    clients call TFE_OpAddInput for each element of an input list instead.\n>    That makes it impossible for us to detect when ends the list or when starts\n>    the next input, and we lose track of the input attributes (we actually\n>    assume each input is a single one). Possible workarounds are:\n>    - Disable automatically input attribute inference when we detect\n>       (easily) that an input list is being fed by calling TFE_OpAddInput\n>       multiple times.\n>\n> Yeah, I think this is fine for now.\n\n\n>\n>    1.\n>       - Have a distinct methods to support attribute inference (e.g.\n>       TFE_OpAddInferableInput and TFE_OpAddInferableInputList).\n>       - Restore the option to enable/disable input attribute inference at\n>       TFE_Op construction, so the client knows that TFE_OpAddInput should\n>       be used exclusively for adding single inputs.\n>\n> Any other suggestion?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/23468#issuecomment-438549315>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxa2zaMWkg4gyp957SVf5otwPTofFks5uu7QPgaJpZM4YMbtn>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp : just to tell you that I just merged and fixed the previous conflicts, in case you noticed them. Thank you", "@karllessard Please rebasee your PR.", "@rmlarsen : done", "@rmlarsen is it possible to merge this PR soon? I would like it to be included in 1.13 release, thanks", "> @rmlarsen is it possible to merge this PR soon? I would like it to be included in 1.13 release, thanks\r\n\r\n@rmlarsen  Could you please approve this in order to proceed with next steps to help this get merged.", "I just realized that this PR has not been merged yet. It was approved a long time ago but was blocked by a conflict, which I resolved on Dec 6th. Can we please go forward with this change now? Thanks\r\n\r\nCC: @rmlarsen @hgadig @alextp ", "@martinwicke, is it possible the merge got stucked the same way as it happened [a month ago](https://github.com/tensorflow/tensorflow/pull/23980#issuecomment-446294116)?", "Yeah, it was stuck somewhere. The re-tagging seemed to have helped. Sorry about this!", "@martinwicke : No problem, thanks!\r\n@alextp : It looks like you might need to reapprove, maybe because I rebase the code a few days ago?", "@hgadig do we know why this hasn't merged yet?", "> @hgadig do we know why this hasn't merged yet?\r\n\r\nIt is stucked in the internal CL due to merge conflicts. Cloned to a new CL now.\r\n@alextp  Could you please approve this https://critique.corp.google.com/#review/235245041", "@karllessard apparently we have an internal test that disallows new CHECKs in code so we need to remove the ones in attr_builder.h", "@alextp : No problem, I'll update the PR this weekend", "@alextp : Done, please take a look for reapproval and let's pray!"]}, {"number": 23467, "title": "TensorRT crashes on converting a simple convolutional graph", "body": "TensorRT crashes during conversion of a trivial graph with the following error:\r\n\r\n```\r\npython: customWinogradConvActLayer.cpp:48:\r\nnvinfer1::cudnn::WinogradConvActLayer::WinogradConvActLayer(const string&, const EngineTensors&, const EngineTensors&, const nvinfer1::ConvolutionParameters&, bool, const std::vector<float>&): \r\nAssertion `matchNbDims(inputs[0], outputs[0]) && (inputs.size() == 1 || inputs[1].extent == outputs[0].extent)' failed.\r\n```\r\n\r\nThe following minimal example reproduces this issue:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt\r\n\r\ninput = tf.placeholder(tf.float32, shape=(1, 32, 32, 3))\r\nweights = tf.ones([3, 3, 3, 8], tf.float32)\r\noutput = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME')\r\noutput = output + tf.ones((8,), dtype=tf.float32)\r\noutput = tf.nn.relu(output, name='output')\r\n\r\ntensorrt.create_inference_graph(\r\n    input_graph_def=tf.get_default_graph().as_graph_def(), outputs=[output.op.name])\r\n```\r\n\r\nI've been able to reproduce this issue on multiple machines with similar configurations.\r\n\r\nA few things to note:\r\n\r\n- The addition and the terminal operation are critical for the issue to manifest\r\n- The `relu` can be replaced with most other ops (eg: `tf.identity`) and the issue still manifests\r\n\r\nA variant of this bug was previously reported here: https://github.com/tensorflow/tensorflow/issues/22577 (and was closed without resolution).\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 2.7.12\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory: GTX 1080 Ti", "comments": ["Nagging Assignee @aaroey: It has been 17 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@smit-hinsu ", "I faced the same problem with @ethereon, do we have some updates for the problem?", "Thanks for reporting this issue. But, closing this issue to consolidate all the discussions in the original reported issue #22577.\r\n\r\nJust posted an update there. https://github.com/tensorflow/tensorflow/issues/22577#issuecomment-447532086"]}, {"number": 23466, "title": "Segmentation Fault When Running Multiple Sessios In a Row", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 1.4.1\r\n- **Bazel version (if compiling from source)**: 0.6.0\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 1080 TI\r\n- **Exact command to reproduce**: session_one->Run(\r\n          {{INPUT_LAYER, input_tensor}},\r\n          {OUTPUT_LAYER}, {}, &outputs_tensor);\r\nRun the above method twice in a row\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n### Describe the problem\r\nIt appears that multiple sessions cannot be ran right after the other, otherwise I get segmentation fault. There is gotta be a delay in between Run() methods (such as extra computations)\r\n\r\n### Source code / logs\r\nFor instance, the following code makes my program have a segmentation fault:\r\n```\r\nStatus run_session_one_status = session_ONE->Run(\r\n          {{THIS_GRAPH_input_layer, input_tensor_one}},\r\n          {THIS_GRAPH_output_layer}, {}, &output_tensor_one);\r\n  if (!run_session_one_status.ok()) {\r\n      LOG(ERROR) <<\r\n                 \"run_session_one_status failed.\" << run_session_one_status;\r\n  }\r\n  Status run_session_two_status = session_TWO->Run(\r\n          {{DIFFERENT_GRAPH_input_layer, input_tensor_two}},\r\n          {DIFFERENT_GRAPH_output_layer}, {}, &output_tensor_two);\r\n  if (!run_session_two_status.ok()) {\r\n      LOG(ERROR) <<\r\n                 \"run_session_two_status failed.\" << run_session_two_status;\r\n  }\r\n```\r\n\r\nWhereas the following code stops the segmentation fault from happening:\r\n\r\n```\r\nStatus run_session_one_status = session_ONE->Run(\r\n          {{THIS_GRAPH_input_layer, input_tensor_one}},\r\n          {THIS_GRAPH_output_layer}, {}, &output_tensor_one);\r\n  if (!run_session_one_status.ok()) {\r\n      LOG(ERROR) <<\r\n                 \"run_session_one_status failed.\" << run_session_one_status;\r\n  }\r\n\r\n  /// adding extra operations just to give GPU a break\r\n  feature_vector.clear();\r\n  for (uint32_t i = 0; i < output_tensor_one[0].NumElements(); ++i) {\r\n      feature_vector.push_back(output_tensor_one[0].flat<float>()(i));\r\n  }\r\n\r\n  Status run_session_two_status = session_TWO->Run(\r\n          {{DIFFERENT_GRAPH_input_layer, input_tensor_two}},\r\n          {DIFFERENT_GRAPH_output_layer}, {}, &output_tensor_two);\r\n  if (!run_session_two_status.ok()) {\r\n      LOG(ERROR) <<\r\n                 \"run_session_two_status failed.\" << run_session_two_status;\r\n  }\r\n```\r\n\r\nI have plenty of GPU memory, and each session only needs 0.5% of my GPU's total memory. And, yes, I tried giving each session more memory (e.g., 50% per session, etc.).\r\n\r\nEven though I found a work around, I still wanted to throw the bug out there for the TensorFlow team to be aware of it.\r\n\r\n", "comments": ["@azaks2   PTAL", "Do you have a small example that can reproduce the issue? At the very least could you please share the stack trace for the thread that produces SIGSEGV", "The sample is presented above. I think that is enough. The rest is private because if part of my company"]}, {"number": 23465, "title": "Bug in image_captioning_with_attention.ipynb ", "body": "**System information:** ***Running the notebook on Colab.***\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): Colab ('1.12.0-rc2')\r\n- Python version: Colab (3.6)\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Colab\r\n- GPU model and memory: Colab\r\n\r\n**Describe the current behavior**\r\n\r\nError in Cell 27 (Training): \r\n\r\n`InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/`\r\n\r\n**Describe the expected behavior**\r\n\r\nIf `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 is commented out, the notebook runs end-to-end on Colab. \r\n\r\n**Code to reproduce the issue**\r\n\r\nThe reason this happens could be because in Cells 9 and 10, Tokenizer is defined with `oov_token=\"<unk>\"` in Cell 9, which assigns \"\\<unk\\> : 1\" in the token dictionary, and then `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 assigns \"\\<unk\\>: top_k + 1\" which makes the number \\<unk\\> was previously assigned to non-existent in the dictionary and causes trouble during lookup. \r\n\r\nThe following demonstration explains why we can get an error when we refer to the dictionary later on. \r\n\r\n```\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimport re\r\nimport numpy as np\r\n\r\nsent = \"<start> Alice in wonderland. <end>\"\r\nsent2 = \"<start> When suddenly Alice saw a White Rabbit. <end>\"\r\nx_train = [sent, sent2]\r\n\r\ntop_k = 5\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \r\n                                                  oov_token=\"<unk>\", \r\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\r\ntokenizer.fit_on_texts(x_train)\r\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\r\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\r\nprint(tokenizer.word_index)\r\nidx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'\r\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1 # the problematic line\r\nprint(tokenizer.word_index)\r\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\r\nindex_word[idx]\r\n```\r\n\r\n`Output:`\r\n```\r\n{'<unk>': 1, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}\r\n{'<unk>': 6, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-11-55bbbd305835> in <module>()\r\n     11 print(tokenizer.word_index)\r\n     12 index_word = {value:key for key, value in tokenizer.word_index.items()}\r\n---> 13 index_word[idx]\r\n\r\nKeyError: 1\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-58-03bc9960ded7> in <module>()\r\n     19             for i in range(1, target.shape[1]):\r\n     20                 # passing the features through the decoder\r\n---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)\r\n     22 \r\n     23                 loss += loss_function(target[:, i], predictions)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n<ipython-input-54-b844d20e3fc2> in call(self, x, features, hidden)\r\n     16 \r\n     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n---> 18     x = self.embedding(x)\r\n     19 \r\n     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)\r\n    175     if dtype != 'int32' and dtype != 'int64':\r\n    176       inputs = math_ops.cast(inputs, 'int32')\r\n--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)\r\n    178     return out\r\n    179 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)\r\n    311       name=name,\r\n    312       max_norm=max_norm,\r\n--> 313       transform_fn=None)\r\n    314 \r\n    315 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)\r\n    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):\r\n    132       with ops.colocate_with(params[0]):\r\n--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),\r\n    134                        ids, max_norm)\r\n    135         if transform_fn:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)\r\n   2671     # TODO(apassos) find a less bad way of detecting resource variables without\r\n   2672     # introducing a circular dependency.\r\n-> 2673     return params.sparse_read(indices, name=name)\r\n   2674   except AttributeError:\r\n   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)\r\n    756         tape.variable_accessed(self)\r\n    757       value = gen_resource_variable_ops.resource_gather(\r\n--> 758           self._handle, indices, dtype=self._dtype, name=name)\r\n    759     return array_ops.identity(value)\r\n    760 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)\r\n    611       else:\r\n    612         message = e.message\r\n--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    614 \r\n    615 \r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/\r\n```", "comments": ["Hi Aakanksha, thanks for reaching out. The Tokenizer code that I updated made it into TF 1.12(https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/text.py) takes into account the \"unk\" token and the num_words parameter. As I was trying to work around this issue at the time I wrote this notebook, and the tokenizer code being updated lead to the error. If you try out the following code, it should work as expected. \r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\nimport re\r\nimport numpy as np\r\n\r\nsent = \"<start> Alice in wonderland. <end>\"\r\nsent2 = \"<start> When suddenly Alice saw a White Rabbit. <end>\"\r\nx_train = [sent, sent2]\r\n\r\ntop_k = 5\r\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, \r\n                                                  oov_token=\"<unk>\", \r\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\r\ntokenizer.fit_on_texts(x_train)\r\ntrain_seqs = tokenizer.texts_to_sequences(x_train)\r\nprint(tokenizer.word_index)\r\nidx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'\r\nprint (idx)\r\nindex_word = {value:key for key, value in tokenizer.word_index.items()}\r\nindex_word[idx]\r\n```\r\n\r\nAlso, if you want to, you can open a PR and fix this issue. I think you will only have to remove the following lines.\r\n\r\n```\r\ntokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}\r\n# putting <unk> token in the word2idx dictionary\r\ntokenizer.word_index[tokenizer.oov_token] = top_k + 1\r\n```", "Nagging Assignee @ymodak: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23464, "title": "tf.keras with tf.dataset takes longer per epoch eventually and sometimes errors out", "body": "<em>Performance Issue</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):No\r\n- TensorFlow version (use command below):1.10.1\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:9.1.85\r\n- GPU model and memory: Tesla P4, 7GB\r\n\r\n**Describe the current behavior**\r\nThe time taken per epoch hugely varies from 10 min to 2 hours from 1st epoch to 2nd. Below are the accurate arguments used in `tf.dataset` creation and its consumption by `tf.keras.model`.\r\n\r\n**Describe the expected behavior**\r\nEvery epoch should roughly take the same time\r\n\r\n**Code to reproduce the issue**\r\nIts a little complicated to share ready to run code due to privacy. But here is crux of it,\r\n\r\n```python\r\ndataset = tf.data.TFRecordDataset(data_files, num_parallel_reads=79)\r\n\r\nif is_training:\r\n    dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(1743, -1, 42))\r\n\r\ndataset = dataset.apply(tf.contrib.data.map_and_batch(decode, 128,\r\n                                                        num_parallel_batches=58,\r\n                                                        drop_remainder=is_training)\r\n                        )\r\ndataset = dataset.prefetch(1064)\r\n```\r\nand fit a `tf.keras.model` like so,\r\n```python\r\nmodel.fit(\r\n    dataset.make_one_shot_iterator(),\r\n    steps_per_epoch=model_meta.train_records // 128,\r\n    epochs=2,\r\n    validation_data=test_dataset.make_one_shot_iterator(),\r\n    validation_steps=test_records // 128\r\n)\r\n```\r\n\r\n**Question 2**: It also errors out almost at the end of the 2nd epoch with `tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence`. Not sure why it did not fail in the first epoch.", "comments": ["@Nithanaroy you should create the issue through https://github.com/tensorflow/tensorflow/issues => \"New Issue\" => \"Bug/Performance Issue\" and provide all required information there. In particular, as much as possible you should provide a minimal reproducible example. Your issue is currently missing information about system environment, TensorFlow version, and your example is not reproducible.", "Sorry, completely forgot about those important details while raising an issue using *convert comment to issue* option. Added all the info. ", "@jsimsa any help please?", "@Nithanaroy I am assigning this to @fchollet who is familiar with how Keras uses API\r\n\r\nNote that it might be difficult to diagnose this issue without a reproducible example.", "Just a quick question. I provide a one shot iterator to model.fit based on some examples i saw online.\r\n\r\nmodel.fit(\r\n   dataset.make_one_shot_iterator(),...\r\n\r\n However i saw just providing tf.dataset also works from the documentation. Is making the iteration the root cause for the above problems?", "I believe that providing a dataset is recommended.", "Seeing the similiar issue by using TFRecordDataset one shot iterator together with Keras fit", "@haoyuz I believe that this is related to what we discussed offline ... it might be worth referring this issue in your fix", "Sorry for the late reply --- I noticed that you're using TF 1.10 which won't include some recent changes/fixes we've made to Keras (many of them are related to tf.data, memory efficiency, and Keras model graph). Could you please verify if the problem remains when you use the nightly TensorFlow build? If so, could you also provide a link to the full code so that we can also reproduce on our end? Thanks!", "Several recent fixes in Keras, tf.data, and eager should help with the problem. Without further information I will close this issue for now. Feel free to reopen if you find this is still a problem in latest TensorFlow. Thanks!"]}, {"number": 23463, "title": "[Bug] TensorRT converted graph gives wrong output", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.11.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 9.0/7\r\n- GPU model and memory: GTX 1080, 8GB\r\n\r\nenv.txt:\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux yiwei-desktop 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux yiwei-desktop 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.15.2                \r\nprotobuf                           3.6.0                 \r\ntensorflow-gpu                     1.11.0                \r\ntensorflow-tensorboard             0.1.8                 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.11.0\r\ntf.GIT_VERSION = v1.11.0-0-gc19e29306c\r\ntf.COMPILER_VERSION = v1.11.0-0-gc19e29306c\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Nov  1 18:42:17 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\r\n| 29%   46C    P2    42W / 180W |   1391MiB /  8112MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2190      G   /usr/lib/xorg/Xorg                           582MiB |\r\n|    0      5166      G   compiz                                       493MiB |\r\n|    0      6354      G   ...-token=6EB0A193120E130143DC89992F94A977   196MiB |\r\n|    0      7163      C   /usr/lib/libreoffice/program/soffice.bin     107MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n```\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nWhen using TensorRT to convert a graph that contains a specific structure, the output of the graph would be wrong compare to the output of from the original graph with same input. Sometimes the output can be as large as infinity or some very big numbers.\r\n\r\n**Describe the expected behavior**\r\nWe expect the two graph to output similiarly (Normally, we get absolute error < 1e-5)\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\n\r\n\r\n# Set to False would not trigger the bug.\r\nBUGGED_VERSION = True\r\n\r\n\r\ndef build_graph_from_def(graph_def, input_nodes, output_nodes):\r\n    \"\"\"\r\n    build the actual graph from definition\r\n    \"\"\"\r\n    tf.reset_default_graph()\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        return_tensors = [operation_name +\r\n                          \":0\" for operation_name in input_nodes + output_nodes]\r\n        tensors = tf.import_graph_def(graph_def=graph_def, name=\"\",\r\n                                      return_elements=return_tensors)\r\n        input_tensor_list = tensors[:len(input_nodes)]\r\n        output_tensor_list = tensors[len(input_nodes):]\r\n\r\n    return graph, input_tensor_list, output_tensor_list\r\n\r\n\r\ndef conv(inp, name):\r\n    # Seems to be data-format irrelevant. Tested with both NCHW and NHWC.\r\n    # However, when bias is zero (as initialized by tf.layers.conv2d by default)\r\n    # The bug would trigger.\r\n    if BUGGED_VERSION:\r\n        return tf.layers.conv2d(inp, filters=16, kernel_size=(3, 3), name=name)\r\n    else:\r\n        return tf.layers.conv2d(inp, filters=16, kernel_size=(3, 3),\r\n                                bias_initializer=tf.variance_scaling_initializer(), name=name)\r\n\r\n\r\ndef main():\r\n    with tf.variable_scope(\"Net\"):\r\n        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=\"input\")\r\n        conv_shared = conv(inp, \"conv_shared\")\r\n        conv1_1 = conv(conv_shared, \"conv1_1\")\r\n        conv1_2 = conv(conv1_1, \"conv1_2\")\r\n        conv2_1 = conv(conv_shared, \"conv2_1\")\r\n        conv2_2 = conv(conv2_1, \"conv2_2\")\r\n    input_nodes = [\"Net/input\"]\r\n\r\n    # We need to output both to trigger the error.\r\n    # Also note that if we includes \"Net/conv_shared/BiasAdd\" to output_nodes, the bug\r\n    # disappears.\r\n    output_nodes = [\"Net/conv1_2/BiasAdd\", \"Net/conv2_2/BiasAdd\"]\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        const_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, sess.graph.as_graph_def(), output_nodes)\r\n\r\n    optimized_graph_def = trt.create_inference_graph(\r\n        input_graph_def=const_graph_def,\r\n        outputs=output_nodes,\r\n        max_batch_size=1,\r\n        max_workspace_size_bytes=1 << 25)\r\n\r\n    graph, input_tensors, output_tensors = build_graph_from_def(\r\n        optimized_graph_def, input_nodes, output_nodes)\r\n\r\n    # Seems to be input-value irrelevant, also tested with np.ones and random\r\n    input_value = np.zeros((1, 28, 28, 3))\r\n    with tf.Session(graph=graph) as sess:\r\n        converted_output = sess.run(output_tensors[0], feed_dict={\r\n                                    input_tensors[0]: input_value})\r\n\r\n    graph, input_tensors, output_tensors = build_graph_from_def(\r\n        const_graph_def, input_nodes, output_nodes)\r\n    with tf.Session(graph=graph) as sess:\r\n        original_output = sess.run(output_tensors[0], feed_dict={\r\n            input_tensors[0]: input_value})\r\n\r\n    deltas = np.abs(converted_output - original_output)\r\n\r\n    # Output would be random and varies each time (maybe correct by accident),\r\n    # indicating being memory or graph-weight specific issue.\r\n    print(\"min={}\".format(np.min(deltas)))\r\n    print(\"max={}\".format(np.max(deltas)))\r\n    print(\"median={}\".format(np.median(deltas)))\r\n    print(\"mean={}\".format(np.mean(deltas)))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n\r\n**Other info / logs**\r\nThis may be Nvidia's bug as well, yet not able to test that directly. Thoughts about the problem has been included in the sample code. A typical output looks like the following:\r\n```\r\n2018-11-02 11:51:59.336625: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-11-02 11:51:59.401658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-11-02 11:51:59.402192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 6.11GiB\r\n2018-11-02 11:51:59.402205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 11:51:59.676511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 11:51:59.676541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 11:51:59.676547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 11:51:59.676703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-11-02 11:51:59.778756: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-11-02 11:51:59.778802: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-11-02 11:51:59.778973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 11:51:59.778987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 11:51:59.778992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 11:51:59.778996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 11:51:59.779084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-11-02 11:51:59.787791: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'Net/', converted to graph\r\n2018-11-02 11:51:59.787804: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\n2018-11-02 11:52:01.472421: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:952] Engine Net/my_trt_op_0 creation for segment 0, composed of 13 nodes succeeded.\r\n2018-11-02 11:52:01.475092: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-11-02 11:52:01.475574: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-11-02 11:52:01.476125: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: tf_graph\r\n2018-11-02 11:52:01.476138: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 21 nodes (-10), 20 edges (-10), time = 1.141ms.\r\n2018-11-02 11:52:01.476143: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 26 nodes (5), 26 edges (6), time = 0.54ms.\r\n2018-11-02 11:52:01.476147: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (-12), 14 edges (-12), time = 1685.31897ms.\r\n2018-11-02 11:52:01.476151: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 14 nodes (0), 14 edges (0), time = 0.442ms.\r\n2018-11-02 11:52:01.476155: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (0), 14 edges (0), time = 0.576ms.\r\n2018-11-02 11:52:01.476159: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:404] Optimization results for grappler item: Net/my_trt_op_0_native_segment\r\n2018-11-02 11:52:01.476163: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 14 nodes (0), 13 edges (0), time = 0.537ms.\r\n2018-11-02 11:52:01.476167: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   layout: Graph size after: 14 nodes (0), 13 edges (0), time = 0.339ms.\r\n2018-11-02 11:52:01.476171: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (0), 13 edges (0), time = 0.071ms.\r\n2018-11-02 11:52:01.476175: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   constant folding: Graph size after: 14 nodes (0), 13 edges (0), time = 0.405ms.\r\n2018-11-02 11:52:01.476180: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:406]   TensorRTOptimizer: Graph size after: 14 nodes (0), 13 edges (0), time = 0.066ms.\r\n2018-11-02 11:52:01.505052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 11:52:01.505082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 11:52:01.505088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 11:52:01.505092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 11:52:01.505183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-11-02 11:52:01.527376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-11-02 11:52:01.527397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-02 11:52:01.527403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-11-02 11:52:01.527407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-11-02 11:52:01.527486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5857 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nmin=4.95342129678e+29\r\nmax=2.45457421171e+31\r\nmedian=6.9235024529e+30\r\nmean=8.45157562195e+30\r\n\r\n```\r\n", "comments": ["@pooyadavoodi", "I tried this with the latest TF out of master branch, and the output seems reasonable; the delta is <e-7:\r\n\r\nWhat version of TRT are you using?\r\n\r\nMy log:\r\n```\r\n2018-12-20 00:55:30.801395: I tensorflow/core/grappler/devices.cc:50] Number of eligible GPUs (core count >= 8): 1\r\n2018-12-20 00:55:30.801540: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-12-20 00:55:30.801930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n2018-12-20 00:55:30.801966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-20 00:55:30.801981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 \r\n2018-12-20 00:55:30.801990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N \r\n2018-12-20 00:55:30.802169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14361 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 7.0)\r\n2018-12-20 00:55:30.811068: I tensorflow/contrib/tensorrt/segment/segment.cc:461] There are 5 ops of 3 different types in the graph that are not converted to TensorRT: Identity, Placeholder, NoOp, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).\r\n2018-12-20 00:55:30.811232: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:940] Number of TensorRT candidate segments: 1\r\n2018-12-20 00:55:32.873080: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1042] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 18 nodes succeeded.\r\n2018-12-20 00:55:32.874741: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] Optimization results for grappler item: tf_graph\r\n2018-12-20 00:55:32.874761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 21 nodes (-10), 20 edges (-10), time = 2.416ms.\r\n2018-12-20 00:55:32.874766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   layout: Graph size after: 26 nodes (5), 26 edges (6), time = 1.088ms.\r\n2018-12-20 00:55:32.874769: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 26 nodes (0), 26 edges (0), time = 1.683ms.\r\n2018-12-20 00:55:32.874772: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   TensorRTOptimizer: Graph size after: 9 nodes (-17), 8 edges (-18), time = 2063.13501ms.\r\n2018-12-20 00:55:32.879605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n2018-12-20 00:55:32.879630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-20 00:55:32.879636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 \r\n2018-12-20 00:55:32.879640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N \r\n2018-12-20 00:55:32.879743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14361 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 7.0)\r\n2018-12-20 00:55:32.903856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n2018-12-20 00:55:32.903883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-20 00:55:32.903889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 \r\n2018-12-20 00:55:32.903893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N \r\n2018-12-20 00:55:32.903989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14361 MB memory) -> physical GPU (device: 0, name: Graphics Device, pci bus id: 0000:01:00.0, compute capability: 7.0)\r\nmin=0.0\r\nmax=4.76837158203125e-07\r\nmedian=5.960464477539063e-08\r\nmean=6.655410800249228e-08\r\n```", "Closing this issue as it is not reproducible with the latest version. See https://github.com/tensorflow/tensorflow/issues/23463#issuecomment-448817970. \r\n\r\nFeel free to reopen if the issue persists for you in the latest TensorRT version."]}, {"number": 23462, "title": "Update download_dependencies.sh", "body": "After removing contrib from the path to lite there is one less step back to make.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Duplicate of #23424 .", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "The path has been already updated in the source code. "]}, {"number": 23461, "title": "[TFLite] Fix dependencies script after migration", "body": "After migration from `contrib`, the `download_dependencies.sh` script needs to go one less directory up.", "comments": ["Closing as duplicate of #23424."]}, {"number": 23460, "title": "Update beam_search_decoder.py", "body": "#22172 \r\nprobably not the neatest way to update my previous pull request...", "comments": ["@georgesterpu  Please sign the CLA in order to proceed looking into this PR.", "@ebrevdo  -  I think we need to wait until the user signs a CLA to proceed. Please correct me if I'm wrong.", "Sure; I guess I thought George had written one.  Thanks!\n\nOn Mon, Nov 19, 2018 at 2:43 PM, harshini-gadige <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> - I think we need to wait until the\n> user signs a CLA to proceed.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/23460#issuecomment-440068936>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimysoxIEEa5jk57WvOaiHm95qAwLdks5uwzQYgaJpZM4YMBxK>\n> .\n>\n", "Hello,\r\nI just signed the CLA with my university email, although gmail is listed as the primary one on GitHub, fingers crossed.", "Hi. Is the problem still on my side ?", "> Hi. Is the problem still on my side ?\r\n\r\nThanks for signing CLA. We are looking into the PR."]}, {"number": 23459, "title": "AOT compilation of an Estimator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:1.11\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:NA\r\n\r\nI find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to `tf.compile` the resulting predictor, produce the `.so` and link it to one's project..\r\n\r\nSo I have my calib class in which I define a simple linear estimator\r\n\r\n    self.model = tf.estimator.LinearRegressor(\r\n            feature_columns=self.feature_columns,\r\n            model_dir = self.model_dir)\r\n\r\nAfter training, I want to \r\n1- get the trained model with optimal parameters (load it in my variable self.model)\r\n2- extract the graph and freeze it\r\n3- tf.compile that graph\r\n\r\nI could not find any way to do parts 1- and 2-. \r\nCan you please point me to a good way to it?", "comments": ["StackOverflow is more suited for such questions. \r\n\r\nSteps 1/2 are covered here: https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators", "Thanks @superbobry .. \r\n\r\nI asked the question on SO indeed, but no chance for the moment, my question is \r\n\r\n\r\n[https://stackoverflow.com/questions/53122412/aot-compilation-of-an-estimator](url)\r\n\r\nI already read your link but could not see any Python API  to get my variable `self.model` to contain the Estimator object again, after I read a savedModel from my export directory \r\n\r\n    with tf.Session() as sess:\r\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],              \r\n                                                    export_ts_dir)\r\n    # Now how can I get self.model to contain an Estimator object again\r\n    # self.model = .....?\r\n\r\nAlso for step 2, it seems in order to freeze the graph, I need to know the output nodes. As I could not get my `self.model` to contain the Estimator object, I do not know how to extract the graph, neither how to get the output node name from that graph", "This [https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf](url)\r\nhelped a lot"]}, {"number": 23458, "title": "XLA creates CUDA contexts on GPUs not in gpu_options.visible_device_list", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.12.0-rc2\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: CUDA 10.0, cuDNN 7.4\r\n- GPU model and memory: 8xV100 32GB\r\n\r\n**Describe the current behavior**\r\nWhen XLA is enabled, CUDA contexts are created on every device visible to the CUDA driver (excluding devices with CUDA_VISIBLE_DEVICES works, but gpu_options.visible_device_list in ConfigProto does not).\r\n\r\n**Describe the expected behavior**\r\nWhen gpu_options.visible_device_list is specified, CUDA contexts should only be created on the devices listed.\r\n\r\n**Code to reproduce the issue**\r\nTo reproduce, run the following on a multi-gpu system and check nvidia-smi during the 20 second sleep.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nconfig.gpu_options.visible_device_list=\"0\"\r\n\r\ninput = tf.placeholder(tf.float32, [1000])\r\ndata = np.random.rand(1000).astype('float32')\r\noutput = tf.nn.softmax(tf.nn.relu(input))\r\n\r\nwith tf.Session(config=config) as sess:\r\n  res = sess.run(output, feed_dict={input: data})\r\n  print(\"Session executed, check devices with nvidia-smi\")\r\n  time.sleep(20)\r\n  print(\"Exiting\")\r\n```\r\n\r\n**Other info / logs**\r\n\r\n", "comments": ["Assigning back to Todd as I don't see any of the GPUOptions being used in the tensorflow/compiler and I doubt if we use BaseGPUDevice for XLA.", "Any news @tatatodd  ?", "@smit-hinsu @tatatodd @harshini-gadige any news possible ? This issue is really limiting when using XLA...", "I have forwarded this to the team at nvidia that has started to work on XLA:GPU.  Perhaps this will be a good first bug for someone there.", "#24158 should fix this.", "Thanks @samikama. It could have solved the issue, but it didn't", "@DEKHTIARJonathan, the PR prevents XLA from running graph on unselected devices but doesn't prevent cuda handles from being created for them. There is another PR in progress to address that as well.", "Any way you can point me to this work?\nThanks a lot", "@DEKHTIARJonathan ,\r\n\r\nCould you please try running command below with tensorflow-gpu==1.12 and tensorflow-nightly-gpu using tensorflow/benchmarks repository and post the results here.\r\n\r\n```\r\nmpiexec --allow-run-as-root --output-filename  RUNLOG --bind-to socket -np 4 python tf_cnn_benchmarks.py     --batch_size=256     --num_batches=90 --display_every=100     --model=resnet50     --optimizer=momentum     --variable_update=horovod       --use_fp16=True     --nodistortions    --xla     --num_gpus=1     --loss_type_to_report=base_loss --print_training_accuracy -compute_lr_on_cpu=True --single_l2_loss_op=True\r\n```\r\nThanks,\r\nSami\r\n", "Still not solved sorry", "@samikama  I tested the latest master branch which has all the patches in #24303 . Still met the following error message:\r\n```console\r\n2018-12-18 16:36:39.158400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA\r\nnode, so returning NUMA node zero\r\n2018-12-18 16:36:39.164734: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0xd7a8650 executing computations on platform CUDA. Devices:\r\n2018-12-18 16:36:39.164800: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2018-12-18 16:36:39.167879: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2500135000 Hz\r\n2018-12-18 16:36:39.176563: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0xdb76e50 executing computations on platform Host. Devices:\r\n2018-12-18 16:36:39.176602: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>\r\n2018-12-18 16:36:39.177118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties:\r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.62\r\npciBusID: 0000:00:07.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2018-12-18 16:36:39.177152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n2018-12-18 16:36:39.210918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-12-18 16:36:39.210949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0\r\n2018-12-18 16:36:39.210958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N\r\n2018-12-18 16:36:39.211299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10466 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:00:07.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/pretraining_output/model.ckpt.\r\n**2018-12-18 16:37:03.376000: E tensorflow/core/grappler/grappler_item_builder.cc:595] Graph preprocessing failed: Invalid argument: Invalid device ordinal value (1). Valid range is [0, 0].\r\n        while setting up XLA_GPU_JIT device number 1**\r\n```\r\n\r\nIt's a single process and set the config.gpu_options.visible_device_list to 0.  The test is based on bert's implementation which is using the estimator API as below:\r\n```python\r\n  config.gpu_options.visible_device_list = 0\r\n  ...\r\n  run_config = tf.contrib.tpu.RunConfig(\r\n      cluster=tpu_cluster_resolver,\r\n      master=FLAGS.master,\r\n      model_dir=model_dir,\r\n      session_config=config,\r\n      save_checkpoints_steps=save_checkpoints_steps,\r\n      tpu_config=tf.contrib.tpu.TPUConfig(\r\n          iterations_per_loop=FLAGS.iterations_per_loop,\r\n          num_shards=FLAGS.num_tpu_cores,\r\n          per_host_input_for_training=is_per_host))\r\n  ...\r\n  estimator = tf.contrib.tpu.TPUEstimator(\r\n      use_tpu=FLAGS.use_tpu,\r\n      model_fn=model_fn,\r\n      config=run_config,\r\n      train_batch_size=FLAGS.train_batch_size,\r\n      eval_batch_size=FLAGS.eval_batch_size)\r\n\r\n```", "@samikama  I did a quick test and found the the function XlaGpuDeviceFactory::CreateDevices() was called twice. At the first time, the session_options.config.gpu_options().visible_device_list() has correct value \"0\", thus we can see that it created xla device correctly. At the second time, the visible_device_list become empty, thus it will enumerate all the devices and encounter that assertion.", "@gongzg \r\n\r\nI don't have access to a tpu. Can you give me a GPU based reproducer, similar to one above.\r\n\r\nThanks,\r\nSami\r\n", "> I don't have access to a tpu.\r\n\r\nWhat's leading us to think that this has something to do with TPUs?", "Can I use tpu estimator to run on GPUs? \r\nsnippet above is using TPU estimator.", "> snippet above is using TPU estimator.\r\n\r\nAh, I see that now -- my browser wasn't showing the most recent edit to the comment.\r\n\r\nThis code is very clearly running on a machine with at least one GPU.\r\n\r\n> 2018-12-18 16:36:39.177152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0\r\n\r\nAre you imagining that this machine contains both a GPU and a TPU?", "I don't know. But in the past I have seen rouge sessions being created in python code and this may be one of these cases. It could be a genuine bug as well,  but I could be able to run benchmarks repository with this without any issues. Otherwise I need to trace TPU estimator for stray sessions and it is not trivial to do without running it.", ">> Are you imagining that this machine contains both a GPU and a TPU?\r\n\r\n> I don't know. \r\n\r\nWell, can we think through this a bit?\r\n\r\nIt seems to me that based on the evidence we have, either (a) you need a TPU in order to run this code and this machine has both a GPU and a TPU, or (b) you don't need a TPU in order to run this code.  Right?  Do we have a sense of which of these is more likely?  Are there simple experiments we could conduct that would e.g. tell us whether (b) is true?", "@samikama  @jlebar   My environment only has GPU installed and without enable XLA, it could run smoothly on GPU.  You can also refer https://github.com/google-research/bert\uff0c you can see \u201con All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud TPU.\u201c Actually\uff0c the TPU estimator will fallback to GPU when it could not find TPU available.\r\n\r\n@samikama You can simply add the config option to the bert's code and run the pretraining example.", "@samikama Please refer the following patch for bert's run_pretraining.py. \r\n\r\n```patch\r\ndiff --git a/run_pretraining.py b/run_pretraining.py\r\nindex b118f62..6b24744 100644\r\n--- a/run_pretraining.py\r\n+++ b/run_pretraining.py\r\n@@ -427,10 +427,16 @@ def main(_):\r\n         FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\r\n\r\n   is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\r\n+\r\n+  config = tf.ConfigProto()\r\n+  config.gpu_options.allow_growth = True\r\n+  config.gpu_options.visible_device_list = '0'\r\n+  config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n   run_config = tf.contrib.tpu.RunConfig(\r\n       cluster=tpu_cluster_resolver,\r\n       master=FLAGS.master,\r\n       model_dir=FLAGS.output_dir,\r\n+      session_config=config,\r\n       save_checkpoints_steps=FLAGS.save_checkpoints_steps,\r\n       tpu_config=tf.contrib.tpu.TPUConfig(\r\n           iterations_per_loop=FLAGS.iterations_per_loop,\r\n```", "optimize_dataset_op is ignoring GPU configuration and constructing devices for everything leading to allocations on GPUs even if XLA was not compiled in. The issue is coming from grappler_item_builder.cc file which instantiates devices with an empty session configuration which leads to allocation. Since with #24303 we started filter the xla devices to prevent allocations, issue that @gongzg observed surfaced. \r\n#24461 addresses this issue.", "@nluehr you can close the issue. TF Nightly build shows that the problem is fixed."]}, {"number": 23457, "title": "Previous Issue Persists in Android Demo (https://github.com/tensorflow/tensorflow/issues/21431)", "body": "https://github.com/tensorflow/tensorflow/issues/21431 is  both listed as awaiting response and closed. With latest version of TensorFlow I have same compilation issue in Android Demo project.\r\n\r\nNamely the Java Compiler Error:\r\n\"error: cannot find symbol class Fill where T is a type-variable:T extends Object declared in class Zeros\"\r\nwhen attempting to run to either device or simulator.  The Gradle build is successful but project cannot be run.\r\n\r\nTFMobile is still important to me still because TFLite doesn't support Switch and as a result many example graphs do not work in TFLite yet.\r\n", "comments": ["@shashishekhar   PTAL", "Nagging Assignee @shashishekhar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "jdduke@ for Java API and SelectOps feedback.", "We're actively working to support ops related to control flow, like Switch. Did you try the workaround noted in https://github.com/tensorflow/tensorflow/issues/21431#issuecomment-424173564?", "For this error in Android Studio:\"error: cannot find symbol class Fill where T is a type-variable:T extends Object declared in class Zeros\"\r\nYou need do (Build.gradle) : \r\ndef nativeBuildSystem = 'none'\r\n  \ud83d\udc4d \r\n ", "Closing this issue as it was in \"awaiting response\" status for more than 7 days. Feel free to post your comments and we will reopen this issue(if required). ", "@FAMM2017 \r\nYou need do (Build.gradle) :\r\ndef nativeBuildSystem = 'none'\r\nIt has been stated in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md ](url) that the demo can be built using Android Studio even if  nativeBuildSystem = 'bazel'. However, if I try to build using bazel I get the same error as this issue:\r\n**error: cannot find symbol class Fill\r\nwhere T is a type-variable:\r\nT extends Object declared in class Zeros**\r\n@hgadig  the issue should be reopened", "> @FAMM2017\r\n> You need do (Build.gradle) :\r\n> def nativeBuildSystem = 'none'\r\n> It has been stated in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md ](url) that the demo can be built using Android Studio even if nativeBuildSystem = 'bazel'. However, if I try to build using bazel I get the same error as this issue:\r\n> **error: cannot find symbol class Fill where T is a type-variable: T extends Object declared in class Zeros**\r\n> @hgadig the issue should be reopened\r\n\r\nSame here. This issue has to be reopened as it still persists.", "@FAMM2017\r\nYou need do (Build.gradle) :\r\ndef nativeBuildSystem = 'none'\r\nIt has been stated in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/README.md that the demo can be built using Android Studio even if nativeBuildSystem = 'bazel'. However, if I try to build using bazel I get the same error as this issue:\r\nerror: cannot find symbol class Fill where T is a type-variable: T extends Object declared in class Zeros\r\n@hgadig the issue should be reopened\r\n\r\nSame here"]}]