[{"number": 20263, "title": "Computing gradients in extracted subgraph which contains a 'while_loop'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072 \r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0.5\r\n- **GPU model and memory**:\r\nTitan XP\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\ng1=tf.Graph()\r\nsess1=tf.Session(graph=g1)\r\nwith g1.as_default():\r\n    with sess1.as_default():\r\n        i=tf.constant(0, name=\"input\")\r\n        out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\r\n        loss=tf.square(out,name='loss')\r\n        graph_def = tf.graph_util.convert_variables_to_constants(sess1,g1.as_graph_def(),['output/Exit'])\r\n\r\ng2 = tf.Graph()\r\nwith g2.as_default():\r\n    tf.import_graph_def(graph_def,name='')\r\n    i_imported = g2.get_tensor_by_name(\"input:0\")\r\n    out_imported = g2.get_tensor_by_name(\"output/Exit:0\")\r\n    tf.gradients(out_imported, i_imported)\r\n\r\n```\r\noutput:\r\n```\r\nINFO:tensorflow:Froze 0 variables.\r\nConverted 0 variables to const ops.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-908dc1dee750>\", line 23, in <module>\r\n    tf.gradients(out_imported, i_imported)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 543, in _GradientsHelper\r\n    ops.get_default_graph(), to_ops, from_ops, colocate_gradients_with_ops)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 195, in _PendingCount\r\n    between_op_list, between_ops, colocate_gradients_with_ops)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1456, in MaybeCreateControlFlowState\r\n    loop_state.AddWhileContext(op, between_op_list, between_ops)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1262, in AddWhileContext\r\n    outer_forward_ctxt = forward_ctxt.outer_context\r\n\r\nAttributeError: 'NoneType' object has no attribute 'outer_context'\r\n```\r\n### Describe the problem\r\nTF  [issue #7404](https://github.com/tensorflow/tensorflow/issues/7404) describes that when trying to form a gradient op in an imported (sub)graph, a 'No attribute 'outer_context'' error occurs. This issue was closed with the recommendation to use `tf.train.import_meta_graph` instead, so the outer context related to the while op is included.\r\n\r\nHowever, this does not fully solve the problem. In some deep-learning related settings, one might want to train a model, extract a subgraph (i.e., remove training related ops), connect the extracted subgraph into a larger graph to serve as part of an ensemble, GAN and so on, and then retrain the larger graph. When there is no dependence on outer context, one can easily use graph editing tools such as `tf.graph_util.convert_variables_to_constants` or `tf.graph_util.extract_sub_graph` to achieve that, exporting and importing subgraphs and then forming new tf.gradients operations. \r\n\r\nThe minimal example above, adapted from issue 7404, shows how this approach fails when a tf.while is used and the outer context is missing. Importing and exporting the metagraph instead would leave the tf.square op in the graph. While for this minimal example being forced to save also the loss tensor looks like a very minor limitation it is easy to conceive actual applications in which there are large parts of the graph which we might really want to exclude (e.g., a decoder network in Capsule-network training). \r\n\r\nRight now, the dependence on outer context for computing gradients for tf.while is incompatible with tf.graph_util.extract_sub_graph and similar operations that operate on graphdefs. I believe that this is not a negligible functionality limitation.  A [related StackOverflow question](https://stackoverflow.com/questions/50663594/computing-gradients-in-extracted-tensorflow-subgraph-which-contains-a-while-loo) was upvoted but left unanswered.\r\n\r\nIn general, from the perspective of an API user who is ignorant of the internal implementation of TF, any dependence of operations on information stored out of the graphdef is not expected, hinders graph editing (as I try to convey above) and seems patchy.", "comments": ["@Tal-Golan,\r\nSorry for the delayed response. Your code could be executed successfully, with **`Tensorflow Version 1.15.2`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/49763dd0e78f5d06a448750d824cd1c3/gh_20263.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20263\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20263\">No</a>\n"]}, {"number": 20262, "title": "R1.8", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "The release branches are managed by TensorFlow team member and does not need contribution from TF community. Closing this for now."]}, {"number": 20261, "title": "MultivariateNormalDiag Bugs", "body": "MultivariateNormalDiag Has different output with scipy.stats.multivariate_normal\r\nCode like that\r\n```\r\nloc=[0., 0., 0.]\r\nscale_diag=[0.1, 0.1, 0.1]\r\n\r\n# method1\r\ntfd = tf.contrib.distributions\r\nmvn = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\r\nsess = tf.Session()\r\nprint(\"Tensorflow MultivariateNormalDiag:\")\r\nprint(mvn.prob(loc).eval(session=sess))  \r\n\r\n# method2\r\nfrom scipy.stats import multivariate_normal\r\np = multivariate_normal.pdf(loc, mean=loc, cov=scale_diag)\r\nprint(\"\\nScipy multivariate_normal:\\n\", p)\r\n```\r\nOutput:\r\n```\r\nTensorflow MultivariateNormalDiag:\r\n63.49365\r\n\r\nScipy multivariate_normal:\r\n 2.0078450647771446\r\n```\r\nWhy that?", "comments": ["Another Problem: Why prob > 1 when scale_diag is very small? If prob > 1 , How can I implement log Likelihood of the prob? like\r\n`loss = -log(prob)`", "Solved! In tf MultivariateNormalDiag, the covariance matrix = scale_diag * scale_diag.T, In scipy multivariate_normal, the covariance matrix = diag(scale_diag). So the different result. By the way, prob > 1 is normal when det(covariance) is very small, this means that the distribution of Gauss is very certain.\r\n"]}, {"number": 20260, "title": "Unknown argument output_layer=final_result", "body": "when i execute this code:\r\nbazel-bin/tensorflow/examples/label_image/label_image \\\r\n> output_layer=final_result \\\r\n> labels=/tf_files/retrained_labels.txt \\\r\n> image=/tf_files/flower_photos/daisy/5547758_eea9edfd54_n.jpg \\\r\n> graph=/tf_files/retrained_graph.pb\r\n\r\ni got the error:\r\nE tensorflow/examples/label_image/main.cc:319] Unknown argument output_layer=final_result\r\nusage: bazel-bin/tensorflow/examples/label_image/label_image", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@kSimpleCoder : Use ' --' before any arguments, e.g. --output_layer,"]}, {"number": 20259, "title": "How to predict on a new image given a trained tensorflow CNN graph?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  I build a CNN with 2 convolutional layers(tf.nn.conv2d) and a fully connected layer (tf.contrib.layers.fully_connected). I saved the model in a checkpoint path and I can see the weights in each of the layers. However, I wanted to see how I can use the restored model to predict on a new image that I have without explicitly putting all weights. Remember, the restored graph already has weights and I just want to be able to pass a new image data and be able to throw out a prediction. \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.0.1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": []}, {"number": 20258, "title": "Batched tf.linspace; documentaton says its possible, but returns error", "body": "Following documentation [here](https://www.tensorflow.org/api_docs/python/tf/lin_space) it says that in `tf.linspace(start=A,stop=B, n)`, `A` and `B` are to be Tensors. But\r\n\r\n\r\n    A=tf.constant(np.array([1.,2.,3.]))\r\n\r\n    B=tf.constant(np.array([3.,4.,5.]))\r\n\r\n    tf.linspace(A,B,3)\r\n\r\n\r\nreturns an error, that A should have shape 0, which contradicts the documentation. Is there some other way to achieve this? The desired output is\r\n\r\n`array([[1,2,3],[2,3,4],[3,4,5]])`", "comments": ["I agree that it's clear to document them as \"a 0-D Tensor\". \r\n\r\nAs to your case, would you mind using tf.data? Dataset.range(1, 5).apply(sliding_window_batch(3, 1))", "@facaiy  Not at all. Thank you!", "I created a PR #20264 to update the docstring.", "@facaiy Could you please show me an example how to use it? I do not know how to employ it. How do I get a tensor from it? Thank you kindly.\r\n\r\n@yongtang Great!", "@MarkGreeny \r\n\r\nTo make your example complete, you need to specify the `num` parameter, as the `tf.linspace` requires it.\r\n\r\n    A=tf.constant(np.array([1.,2.,3.])) \r\n    B=tf.constant(np.array([3.,4.,5.])) \r\n    N=3#or any integer you'd like \r\n\r\nWith that in mind, your request is fulfilled by\r\n\r\n    def batched_linspace(from_,to_,N):\r\n      fromto=tf.stack([from_,to_],axis=-1)\r\n      return tf.map_fn(lambda ft: tf.linspace(ft[0],ft[1],N), fromto)\r\n\r\n    sess.run(batched_linspace(A,B,N))\r\n\r\nwhich promptly returns\r\n\r\n> array([[1., 2., 3.],\r\n       [2., 3., 4.],\r\n       [3., 4., 5.]])\r\n\r\nas requested.\r\n\r\nTry to think about TensorFlow, like **Linear Algebra + Functional Programming.**. Its all here.", "@MarkDaoust Perhaps the tutorial [ Importing Data ](https://www.tensorflow.org/versions/master/programmers_guide/datasets) would be helpful for you.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = (tf.data.Dataset.range(1, 6)\r\n                          .apply(tf.contrib.data.sliding_window_batch(3, 1))\r\n                          .batch(3))\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n  res = sess.run(next_element)\r\n  print(res)\r\n# [[1 2 3]\r\n#  [2 3 4]\r\n#  [3 4 5]]\r\n```\r\n\r\nor \r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nc = tf.map_fn(lambda x: tf.range(x, x+3), tf.range(1, 4))\r\n\r\nwith tf.Session() as sess:\r\n  res = sess.run(c)\r\n  print(res)\r\n```"]}, {"number": 20257, "title": "`transpose` in `tf.contrib.tensorrt` cause `dimension error`", "body": "", "comments": []}, {"number": 20256, "title": "Branch 201783076", "body": "", "comments": []}, {"number": 20255, "title": "[Feature Request] iterator.has_next() for dataset api", "body": "Hi, I'd need some type of `iterator.has_next()` to loop through a dataset:\r\n\r\n```python3\r\ndataset = tf.data.Dataset[...]\r\n\r\niterator = dataset.make_initializable_iterator()\r\nwith tf.control_dependencies([iterator.initializer]):\r\n    retval = tf.while_loop(lambda _: iterator.has_next(), body_fn, init_vals)\r\n```\r\n\r\nAt the moment I add some sample index to the dataset and check if the index value is lower than the number of sampels.\r\nHowever, this is a very ugly solution which might not be possible in every situation.\r\n\r\nIs there any reason that there exists an iterator without a has_next() method?\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: none\r\n- **GCC/Compiler version (if compiling from source)**: none\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: none\r\n", "comments": ["Concrete example:\r\nI'd like to iterate over the batches of a dataset, perform some `map_fn` on it and reduce the result with `reduce_fn`.\r\n\r\nCurrently I need to pass `last_elem` to check if the iterator has more elements:\r\n```python3\r\ndef map_reduce(last_elem: tf.Tensor, data: tf.data.Dataset, map_fn, reduce_fn=tf.add, **kwargs):\r\n    iter = data.make_initializable_iterator()\r\n    \r\n    def cond(idx, val):\r\n        # idx is a vector of sample indices\r\n        return tf.not_equal(tf.gather(idx, tf.size(idx) - 1), last_elem)\r\n    \r\n    def body_fn(old_idx, old_val):\r\n        idx, val = iter.get_next()\r\n        \r\n        return idx, reduce_fn(old_val, map_fn(idx, val))\r\n    \r\n    def init_vals():\r\n        idx, val = iter.get_next()\r\n        return idx, map_fn(idx, val)\r\n    \r\n    with tf.control_dependencies([iter.initializer]):\r\n        _, reduced = tf.while_loop(cond, body_fn, init_vals(), **kwargs)\r\n    \r\n    return reduced\r\n```", "Nagging Assignee @michaelisard: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "We aren't planning to add `Iterator.has_next()`, but are considering a non-throwing substitute for `Iterator.get_next()`.\r\n\r\nFor the map/reduce style of computation, I'd recommend not using `tf.while_loop()` and an explicit iterator. Instead, use `Dataset` transformations like `dataset.map(map_fn)`, `tf.contrib.data.group_by_reducer()` (note that this currently requires a nightly version of TF), and `tf.contrib.data.get_single_element()`. We plan to coalesce single-group `tf.contrib.data.group_by_reducer()` and `tf.contrib.data.get_single_element()` into a single `reduce()` method, and @jsimsa may be able to comment more on the plans there.\r\n", "Hi @Hoeze, if you wish to operate over fixed-length batches of elements, I would suggest using `group_by_window` (see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/kernel_tests/bucketing_test.py#L201) for an example) over `group_by_reducer`, but I otherwise agree with @mrry's advice.\r\n\r\nI will keep this issue open and close it when I added the coalesced `reduce()` operation.", "Thank you very much for your help.\r\n\r\nI'm splitting my dataset into batches to reduce the memory load.\r\nHaving a method which allows applying reduction operations on datasets and retrieving the result would be simply awesome.", "Hello @Hoeze there is already a way to do that.\r\n\r\nHere is an example that illustrates how to sum the elements of a dataset:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef key_fn(value):\r\n  return np.int64(0)\r\n\r\ndef init_fn(key):\r\n  return np.int64(0)\r\n\r\ndef reduce_fn(value, state):\r\n  return state + value\r\n\r\ndef finalize_fn(state):\r\n  return state\r\n\r\nreducer = tf.contrib.data.Reducer(init_fn, reduce_fn, finalize_fn)\r\ndataset = tf.data.Dataset.range(10).apply(tf.contrib.data.group_by_reducer(key_fn, reducer))\r\nvalue = tf.contrib.data.get_single_element(dataset)\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(value))\r\n```\r\n\r\nThis example produces `45` as the output.", "Wow, thank you @jsimsa for this example. You just made ~100 lines of strange hacky code unnecessary :+1: \r\n\r\nI can't wait for getting this into a release", "The coalesced reduce functionality has been implemented in https://github.com/tensorflow/tensorflow/commit/e9869ece182be721dc07fe8ecb7c7288f2fce90f."]}, {"number": 20254, "title": "Update Eigen version to commit fd6845384b866b28d336f43ffa70b982f9f3056e", "body": "PR #20229 included a change to the Eigen version that failed to\r\ncompile on ppc64le. rmlarsen created a pull request in Eigen to\r\nfix the compile failure:\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/410\r\n\r\nThis patch is to pick up the Eigen version of that patch.", "comments": ["bzmirror link ready. starting tests."]}, {"number": 20253, "title": "Contrib Loss Function Bug", "body": "### System information\r\nI've reproduced this problem on multiple systems including two with CPU only and three with GPU. The specs below represent one such system.\r\n\r\nHave I written custom code\r\nYes\r\n\r\nOS Platform and Distribution\r\nMac OS X 10.13.2\r\n17.3.0 Darwin Kernel 17.3.0: Thu Nov  9 18:09:22 PST 2017; root:xnu-4570.31.3~1/RELEASE_X86_64 x86_64\r\n\r\nTensorFlow installed from\r\nPip (python3)\r\n\r\nTensorFlow version\r\n1.8.0\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\nN/A\r\n\r\nGPU model and memory\r\nN/A\r\n\r\nExact command to reproduce\r\nPlease see code below\r\n\r\n### Describe the problem\r\nThere are several conditions that cause triplet_semihard_loss from tf.contrib to return nan as the loss. This totally messes up the gradient and causes divergence (at least in my build). Below is a minimal reproduction of one way to cause nan which is to have a batch of all different classes which causes there to be no positive for any chosen anchor. \r\n\r\nThe lack of a negative does not result in nan, just a super high loss which while not as bad, is still not what I'd expect.\r\n\r\nWhen running backprop, I would expect to have no gradient if there are no valid triplets to train off of.\r\n\r\nI'm happy to contribute to fixes if you guys agree this is a bug.\r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.losses import metric_learning\r\n\r\nlabels = tf.placeholder(tf.int32, [None], name='labels')\r\nembeddings = tf.placeholder(tf.float32, [None, 128], name='embeddings')\r\n\r\nloss_ph = metric_learning.triplet_semihard_loss(labels, embeddings)\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nloss = sess.run(loss_ph, feed_dict={\r\n    embeddings: np.random.rand(2, 128),\r\n    labels: np.array([1, 2])\r\n})\r\n\r\nprint(\"loss\", loss)\r\n# loss nan\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler Updated!", "@alextp Could you take a look at this?", "Bump @alextp \r\nI imagine you're pretty busy. I just want to make sure you've seen this.", "I've seen this. However I didn't write the code nor do we promise we support code in contrib to the same level. Glancing over the implementation I'm not surprised you see NaNs.\r\n\r\nI'd happy if you submit pull requests clearing this up somehow.", "Hi @jhorowitz, I'm wondering what would be the most helpful fix here.\r\n\r\nI agree you can reproduce the bug by failing to provide at least one anchor-positive pair for a metric learning batch, but that also seems like kind of a rare edge case in metric learning? The safest thing seems to be to do a tf assert runtime check on the validity of the provided label set, but also maybe the right fix is just improving the documentation?\r\n", "Hi @coreylynch,\r\n\r\nI definitely think usability could be improved via documentation but I don't think that would be enough. The issues exists in other scenarios as well when there are both positive and negative examples for any given anchor, lacking a positive was just the easiest to reproduce. The tf.assert would need to compute the triplets in order to comprehensively determine if a nan will be produced. \r\n\r\nI can provide samples if that would be helpful. Please advise.\r\n\r\nThanks,\r\nJosh", "Closing out this issue, as `tf.contrib` is no longer supported in TensorFlow 2.0. \r\n\r\nIf you would like to debug the code for `tensorflow.contrib.losses.metric_learning` and upgrade the code for the loss function, a good home for it might be [TensorFlow Addons](https://www.github.com/tensorflow/addons). \r\n\r\nThanks! \ud83d\ude42 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20253\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/20253\">No</a>\n"]}, {"number": 20252, "title": "tensorflow/core/framework/op_kernel.cc:1318] Not found ", "body": "What is the reason this error is happening ?\r\n\r\nWARNING:tensorflow:From /home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\n2018-06-23 21:43:50.471573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-23 21:43:50.471640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-23 21:43:50.471665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\r\n2018-06-23 21:43:50.471684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\r\n2018-06-23 21:43:50.471857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10654 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2018-06-23 21:43:55.304510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-23 21:43:55.304589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-23 21:43:55.304618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0\r\n2018-06-23 21:43:55.304636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N\r\n2018-06-23 21:43:55.304812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10654 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\nEpoch 1/25\r\n2018-06-23 21:44:00.405493: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at resource_variable_ops.cc:389 : Not found: Resource localhost/RMSprop/iterations/N10tensorflow3VarE does not exist.\r\nTraceback (most recent call last):\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable RMSprop/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/RMSprop/lr/N10tensorflow3VarE does not exist.\r\n         [[Node: training/RMSprop/ReadVariableOp_4 = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](RMSprop/lr)]]\r\n         [[Node: training/RMSprop/gradients/add_9/add_grad/Shape_1/_1445 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2193_training/RMSprop/gradients/add_9/add_grad/Shape_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 161, in <module>\r\n    callbacks=[TensorBoard(log_dir='./log')])\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 1598, in fit_generator\r\n    initial_epoch=initial_epoch)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py\", line 191, in fit_generator\r\n    x, y, sample_weight=sample_weight, class_weight=class_weight)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py\", line 1390, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py\", line 2824, in __call__\r\n    fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/Admin/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable RMSprop/lr from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/RMSprop/lr/N10tensorflow3VarE does not exist.\r\n         [[Node: training/RMSprop/ReadVariableOp_4 = ReadVariableOp[dtype=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](RMSprop/lr)]]\r\n         [[Node: training/RMSprop/gradients/add_9/add_grad/Shape_1/_1445 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2193_training/RMSprop/gradients/add_9/add_grad/Shape_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Already fixed thanks. I previous using these import statement in pre-trained xception model and visual the information in the tensorboard using callback occur these error. After I change to second statement work for me.\r\n\r\nI changing the import statement from \r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.callbacks import TensorBoard\r\nfrom keras.optimizers import SGD\r\n\r\n\r\nto\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\r\nfrom tensorflow.python.keras.callbacks import TensorBoard\r\n", "I just change the TensorBoard import statement from:\r\n\r\n> from keras.callbacks import TensorBoard\r\n\r\nto:\r\n\r\n> from tensorflow.python.keras.callbacks import TensorBoard\r\n\r\nAnd everything goes without any error.", "ubuntu 16.04\r\ntensorflow 1.15\r\nkeras 2.3.1\r\nand i get exactly the same problem when trying this example [https://keras.io/examples/tensorboard_embeddings_mnist/]"]}, {"number": 20251, "title": "Tensorflow recognized my GPU which is GTX 1060, but is using my CPU to train", "body": "Stuck on this\r\nINFO:tensorflow:Starting Queues.\r\nINFO:tensorflow:global_step/sec: 0\r\nINFO:tensorflow:global_step/sec: 0\r\nINFO:tensorflow:global_step/sec: 0\r\n\r\n![capture](https://user-images.githubusercontent.com/28718783/41813713-073d9444-7709-11e8-89cd-faeadedc74e7.JPG)\r\n![dddd](https://user-images.githubusercontent.com/28718783/41813731-3fc754e4-7709-11e8-83df-f781949e9e57.JPG)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have not written custom code.\r\nWindows 10-64 bit\r\nTensorflow installed from pip, version 1.8.0\r\nBazel version 0.5.4\r\nCUDA 9.0\r\ncuDNN64-7\r\nGigabyte GTX 1060-6GB VRAM\r\n\r\nCommand: python train.py --logtostderr train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n\r\nModel used ssd_mobilenet_v1", "Nagging Assignee @robieta: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I believe that this is an issue related to tensorflow/models, not tensorflow/tensorflow."]}, {"number": 20250, "title": "Fix build issue while creating Python API", "body": "This fix tries to address the issue raised in #20233 where build could fail with the following error:\r\n```\r\n\"/var/tmp/portage/sci-libs/tensorflow-1.9.0_rc1/work/bazel-base-python3_6/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 182, in get_api_init_text\r\n    package not in module.__name__):\r\nTypeError: argument of type 'NoneType' is not iterable\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 3202.457s, Critical Path: 83.98s\r\nINFO: 4901 processes, local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nThis fix addresses the issue.\r\n\r\nThis fix fixes #20233.\r\n\r\nNOTE: The patch is provided by @cjmayo (\ud83d\udc4d )\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Adding @case540 since the original bug was raised in 1.9rc1, which might need a cherrypick."]}, {"number": 20249, "title": "Add float16 support for tf.contrib.image.transform", "body": "This fix tries to address the issue raised in #20243 where there were no float16 support for `tf.contrib.image.transform`. This fix adds the `float16` support and enables related tests.\r\n\r\nThis fix fixes #20243.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 20248, "title": "Tensorflow-gpu requires AVX instruction to import; even if using gpu for all calculations", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 18.04 Bionic Beaver\r\nAMD Phenom x4 II 965\r\n- **TensorFlow installed from (source or binary)**: binary; docker image tensorflow-1.8.0-gpu-py3\r\n- **TensorFlow version (use command below)**: 1.18.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\nCuDNN 7.0.5\r\n- **GPU model and memory**:\r\nGeforce GTX 760 (compute capability 3.0) 2GB\r\n- **Exact command to reproduce**:\r\n`import tensorflow as tf` on tensorflow-gpu on system without AVX\r\n\r\n### Describe the problem\r\n\r\nJupyter Kernel crashes when doing `import tensorflow as tf`using a system that does not have AVX.\r\n\r\nSimilarly if i enter a python shell and do `import tensorflow as tf` i get an `Illegal Instruction (core dumped)` error and it exits the shell.\r\n\r\nI have confirmed this is due to the AVX instruction because when i use the docker image Tensorflow-1.15.0-gpu-py3 I am able to successfully import tensorflow.\r\n\r\n*Unfortunately*, tensorflow 1.15.0 requires compute capability 3.5 or higher which is extremely frusterating because i cannot find a build of tensorflow online that supports compute capaibility 3.0 but does not use AVX instructions.\r\n\r\nWhy do i need a CPU that supports AVX just to import tensorflow when i dont actually need the AVX instruction since i have a supported GPU (at least it is supported on all tensorflow versions other than 1.15.0).\r\n\r\nIs there a way around this or do i need to build tensorflow from source to not use AVX but to allow compute capability 3.0? I tried using tensorflow-gpu-1.18.0-devel-py to build a new version from source but it did not even ask me what compute capabilities i wanted to build for and so i just let it build but it took over 24 hours and it was still going. \r\n\r\nCan someone either provide a tensorflow-gpu release with CUDA 9.0 and CuDNN 7 support that does not use AVX and supports compute capability 3.0 or at least tell me the best way to obtain such a build (since apparently building using the docker-devel image wont work).", "comments": ["Issue a duplicate of #19584. Please see recommendations there.\r\nThere are also packages shared by the community with such specs.", "@gunan I don't see any builds linked in that issue. Only a docker image for building. Can you at least show me where the build for the non-AVX, Compute Capable 3.0, tensorflow is?"]}, {"number": 20247, "title": "Update or-tools to v6.7.2", "body": "This fix updates or-tools from 253f795 (dated 03/21/2017) to the latest versioned release version of v6.7.2\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20246, "title": "Update sqlite to 3.24.0", "body": "This fix updates sqlite from 3.23.1 to 3.24.0\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20245, "title": "Update grpc to v1.12.1", "body": "This fix updates grpc from d184fa2 to the newer versioned release of v1.12.1.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang @gunan This commit appears to have been reverted somehow?\r\nNot entirely sure how, but this commit is in master but when looking at the latest tensorflow/workspace.bzl its gone again.", "@perfinion I am not sure why it disappeared as well. Created a PR #20513 to pick it up again.", "@yifeif to check."]}, {"number": 20244, "title": "register float16 for gpu kenel of scatter and scatter_nd functions", "body": "Fix #20219.\r\n\r\nI compile the codes and run those related gpu test cases. All seems passed.", "comments": ["The py3 test are failing. Can u fix the test?", "@qlzh727 Thanks. There seems something wrong with communication between GPU and CPU. Do you have any idea or know who could give a hand?\r\n\r\n\r\n```python\r\n2018-06-25 17:12:34.342209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 10757 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:07.0, compute capability: 3.7)\r\n2018-06-25 17:12:34.539138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-06-25 17:12:34.539178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-25 17:12:34.539185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0\r\n2018-06-25 17:12:34.539189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N\r\n2018-06-25 17:12:34.539759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10757 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:07.0, compute capability: 3.7)\r\n2018-06-25 17:12:34.567676: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of half_re\r\nfor attr 'tensor_type'; NodeDef: Variable/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_HALF_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAdd/indices, ^ScatterAdd/updates); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2018-06-25 17:12:34.567724: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of half_ref\r\nfor attr 'tensor_type'; NodeDef: Variable/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_HALF_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAdd/indices, ^ScatterAdd/updates); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n[[Node: Variable/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_HALF_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAdd/indices, ^ScatterAdd/updates)]]\r\n```", "Adding Alex for some help. @josh11b do you know who's the expert in this part?", "LGTM.\r\n\r\nOverall the kernel might be slower because it does not operator in half2. But we can improve that later if necessary.", "The GPU py3 tests are still failing. All reviewers, please take a closer look.", "Thank you, @qlzh727. \r\n\r\n@zheng-xq @alextp \r\nGPU python 3 tests failed. I checked the log file:\r\n\r\n```python\r\n2018-06-26 17:23:41.925524: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of half_ref\r\n\t for attr 'tensor_type'\r\n\t; NodeDef: Variable/_1 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_HALF_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAdd/indices, ^ScatterAdd/updates); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n```\r\n\r\nThere seems something wrong with updating variable. I'm not familiar with low-level implementation of Variable, do you have any idea about the problem? Thanks.", "I don't understand why we have a ref dtype argument to a send node. @facaiy can you try coercing the failing test to run on the CPU?", "@alextp I'm afraid not, because those tests failed are related to float16. Moreover, they are all passed in CPU and GPU(Python2)  environment.\r\n\r\nI don't know why we bind `_Recv` node with a ref type `tensor_type=DT_HALF_REF`. Is it a bug when building graph?", "Yes, _Send and _Recv cannot send / receive refs, because those are not\ntransferrable. But this should not be possible.\n\nOn Thu, Jun 28, 2018 at 6:04 PM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> I'm afraid not, because those tests\n> failed are related to float16. Moreover, they are all passed in CPU and\n> GPU(Python2) environment.\n>\n> I don't know why we bind _Recv node with a ref type\n> tensor_type=DT_HALF_REF. Is it a bug when building graph?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20244#issuecomment-401216901>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZFWGycLaIaDhbTGZDehSIPAzq7qks5uBX0hgaJpZM4U0zaj>\n> .\n>\n\n\n-- \n - Alex\n", "I don't know how to debug it. Could you give me some suggestion? Thanks.", "@facaiy can you reproduce locally? I found that if I build with cuda and python3 and run the test I can see the error.", "@alextp Thank you for your help. I'm afraid this is beyond the scope of my knowledge. So I'd like close the PR and let others take over the issue."]}, {"number": 20243, "title": "Feature Request: Let tf.contrib.image.transform supports half type", "body": "Currently tf.contrib.image.transform doesn't support half type. It is great if it can support. Thanks.\r\n\r\n@ringw ", "comments": ["@x10000year I added a PR #20249 for float16 type support with tf.contrib.image.transform."]}, {"number": 20242, "title": "Update docstring of sparse_softmax_cross_entropy", "body": "This fix tries to address the discrepancy between the docstring and the actual implementation of sparse_softmax_cross_entropy.\r\n\r\nThe implementation of sparse_softmax_cross_entropy supports float16, float32, and float64, though the docstring only specified float32 and float64. This fix addresses the discrepancy.\r\n\r\nNOTE: The `sparse_softmax_cross_entropy` calls `nn.sparse_softmax_cross_entropy_with_logits`:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py#L912\r\n, which converts `float16` to `float32`, so `float16` is supported:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L2035-L2036\r\n\r\n\r\nThis fix fixes #20231.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 20241, "title": "[Tensorflow C++ API]Multiple session on single GPU", "body": "We are using tensorflow 1.4 c++ for run-time prediction.  Our 4 models are loaded by 4 session instances within one project on one GPU. The project breaks down after a few predictions. \r\n\r\nThe output error message is:  [tensorflow\\core\\common_runtime\\bfc_allocator.cc:464] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) \r\n\r\nIs this memory problem? ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\nOs: Windows 10 \r\nHave I written custom code - No\r\nTensorFlow version -1.4.0\r\nCUDA/cuDNN version - 8.0\r\nGPU model and memory - 1x NVIDIA GTX1050 / 4 GB \r\n\r\nWe followed an unofficial tutorial to build tensorflow c++ API for WINDOWS10 using. We design 2 threads, each thread has 2 instances of session which load the different model to process the different task. We found when we run 2 models in one thread and 1 model in another thread at the same time, everything is ok, but both 2 threads load 2 models simultaneously, the project would break down. We tried to set 'set_per_process_gpu_memory_fraction(0.3)' to restrict each process memory usage, but still the same issue, and memory usage seems not change. So, I wonder to know whether Tensorflow 1.4 does not support the multiple processes on single device well like this. Or we need to do the specific operation to make this happen. \r\n\r\nBy the way, 4 models are 2 deeplab V3 Xeption65 + 2 mobilenet V1_1.0_224\r\n\r\nThanks a lot. ", "I receive also the following message upon evaluation of Tensorpack FasterRCNN module: \r\n\r\n1031 10:42:17 @train.py:484] Running evaluation ...\r\nBackend TkAgg is interactive backend. Turning interactive mode on.\r\n  0%|          | 172/40504 [00:36<1:28:26,  7.60it/s]2018-10-31 10:46:31.508464: F tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum) \r\n\r\nanyone figured this out?\r\n\r\nThanks...", "Nagging Assignee @shivaniag: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It seems no one to answer. So I close it here.", "What is the fix for this,I have same issue", "@learnermaxRL \r\nWell, I'm not sure why but for me downgrading tensorflow 1.11 to 1.10 fixed this issue...\r\nI'd be happy to know it worked for you.\r\nGood luck :-)"]}, {"number": 20240, "title": "Bazel build Failed in compiling libtensorflow.so from source using the mkl config in windows 10", "body": "### System information\r\n- **OS Platform and Distribution )**: win10 x64\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: R1.9\r\n- **Python version**:  Anaconda3 Python3.6.2\r\n- **Bazel version (if compiling from source)**: 0.14.0\r\n- **GCC/Compiler version (if compiling from source)**: VS2015 Update3\r\n- **CUDA/cuDNN version**: none\r\n- **GPU model and memory**: none\r\n- **Exact command to reproduce**: \r\nbazel build -c opt --copt=/arch:AVX  --config=mkl\\\r\n  tensorflow:libtensorflow.so \\\r\n  tensorflow/tools/lib_package:clicenses_generate \\\r\n  tensorflow/java:libtensorflow_jni.so \\\r\n  tensorflow/tools/lib_package:jnilicenses_generate\r\n\r\n### Describe the problem\r\nI want to build libtensorflow.so with \"cpu + mkl\"  in win10 os.\r\n\r\nI had builded the libtensorflow.so using the script (run_libtensorflow.bat) in the directory: tensorflow/tools/ci_build/windows/cpu/bazel/.This script works very well,but then a link error occur \r\nif I add --config=mkl to the option of bazel build. \r\n\r\nI think this link error is about mkl library,and this link error exist a period of time.\r\nI tried to build libtensorflow.so r1.8,after fixing some compilation error the same link error occur. \r\n\r\n### The verbose log are following \r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/tools/Anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from d:\\tensorflow_git_reps\\tensorflow_r1.9\\tools\\bazel.rc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading rc options for 'build' from d:\\tensorflow_git_reps\\tensorflow_r1.9\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/tools/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/tools/Anaconda3/lib/site-packages --python_path=C:/tools/Anaconda3/python.exe --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_CUDA=0 --action_env TF_DOWNLOAD_CLANG=0 --define grpc_no_ares=true --strip=always --config monolithic --copt=-w --host_copt=-w --verbose_failures\r\nINFO: Found applicable config definition build:monolithic in file d:\\tensorflow_git_reps\\tensorflow_r1.9\\tools\\bazel.rc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:mkl in file d:\\tensorflow_git_reps\\tensorflow_r1.9\\tools\\bazel.rc: --define=using_mkl=true -c opt --define=using_mkl=true\r\nWARNING: G:/tf/_bazel_50219889/4o2fzunw/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in G:/tf/_bazel_50219889/4o2fzunw/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: G:/tf/_bazel_50219889/4o2fzunw/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in G:/tf/_bazel_50219889/4o2fzunw/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: G:/tf/_bazel_50219889/4o2fzunw/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in G:/tf/_bazel_50219889/4o2fzunw/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nINFO: Analysed target //tensorflow:libtensorflow.so (77 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: D:/tensorflow_git_reps/tensorflow_r1.9/tensorflow/BUILD:480:1: Linking of rule '//tensorflow:libtensorflow.so' failed (Exit 1169): link.exe failed: error executing command \r\n  cd G:/tf/_bazel_50219889/4o2fzunw/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/tools/Anaconda3/python.exe\r\n    SET PYTHON_LIB_PATH=C:/tools/Anaconda3/lib/site-packages\r\n    SET TEMP=C:\\Users\\50219889\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TMP=C:\\Users\\50219889\\AppData\\Local\\Temp\r\n    SET USE_LINKER=1\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/link.exe /nologo /DLL /WHOLEARCHIVE:external/mkl_windows/lib/libiomp5md.lib /WHOLEARCHIVE:external/mkl_windows/lib/mklml.lib /SUBSYSTEM:CONSOLE -DEFAULTLIB:advapi32.lib -pthread /MACHINE:X64 @bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so-2.params /DEFAULTLIB:msvcrt.lib\r\nLINK : warning LNK4044: unrecognized option '/pthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lpthread'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/ldl'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\nLINK : warning LNK4044: unrecognized option '/lm'; ignored\r\n**mklml.lib(mklml.dll) : error LNK2005: __NULL_IMPORT_DESCRIPTOR already defined in libiomp5md.lib(libiomp5md.dll)**\r\n   Creating library bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.exp\r\nlibbatch_kernels.lo(batch_kernels.o) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function \"void __cdecl tensorflow::`dynamic initializer for 'registrar__body__0__object''(void)\" (??__Eregistrar__body__0__object@tensorflow@@YAXXZ)\r\nlibarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported\r\nlibarithmetic_optimizer.a(arithmetic_optimizer.o) : warning LNK4217: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported in function \"private: bool __cdecl tensorflow::grappler::`anonymous namespace'::ReorderCastAndTranspose::NodeIsOnCpuOrGpu(class tensorflow::NodeDef const *)const \" (?NodeIsOnCpuOrGpu@ReorderCastAndTranspose@?A0x3f420f79@grappler@tensorflow@@AEBA_NPEBVNodeDef@4@@Z)\r\nliblayout_optimizer.a(layout_optimizer.o) : warning LNK4049: locally defined symbol ?DEVICE_GPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_GPU) imported\r\nbazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so : fatal error LNK1169: one or more multiply defined symbols found\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 150.103s, Critical Path: 34.36s\r\nINFO: 2 processes, local.\r\nFAILED: Build did NOT complete successfully", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code", "Have I written custom code: No.", "@tatianashp @gunan @meteorcloudy Do we currently support the MKL build on Windows? It looks like the BUILD rules might not be correct... that error message corresponds to [linking a static library multiple times](https://social.msdn.microsoft.com/Forums/en-US/5d79a108-6516-42d9-9626-05c622d2a007/want-to-fix-a-linker-warning?forum=winappswithnativecode).", "@mrry I never tried to build tensorflow with MKL support on Windows, I believe it's currently not support.", "+ @agramesh1 ", "@tatianashp @mrry  We only have verified Windows build using CMake. We are testing bazel  Windows build now.", "I'm taking a look at this and have reproduced the error. WIP.", "@claynerobison Thank you!", "@claynerobison  -  Any update ?", "@harshini-gadige We are working the feature into our sprints. ", "@claynerobison Hi, any update?", "See https://github.com/tensorflow/tensorflow/issues/23420", "addressed in comments on [#23420](https://github.com/tensorflow/tensorflow/issues/23420)\r\n\r\nClosing"]}, {"number": 20239, "title": "apply gpu memory fraction option to static memory analysis during identifying memory swap candidates", "body": "run  tests (with GPU support), no reported errors. \r\n$ bazel test -c opt --config=cuda //tensorflow/...\r\n$ bazel test -c opt --config=cuda //tensorflow/python/...", "comments": ["Seems reasonable to me, but the swapping bit of the memory optimizer is much more Benoit's field.", "Nudge for review.", "hi, is there anything I need do for the failed internal ci build?", "let me kick off the new test and make sure its not flaky.", "@qlzh727 can you help trigger a new build validation? I got some update in the memory_optimizer_test.cc. thanks !! ", "Nagging Reviewer @allenlavoie, @benoitsteiner: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 15 days with no activity and the `awaiting review` label has been applied.", "kokoro:run please. ", "Ping @benoitsteiner for review.", "Sorry for the delay.\r\n\r\nTaking another look, is this not covered by https://github.com/tensorflow/tensorflow/blob/fe12159e60258d0b6cae9b580e248b22e80f3f7b/tensorflow/core/grappler/clusters/single_machine.cc#L108 ?\r\n\r\nIt seems like we should be setting memory_size to account for this fraction centrally (even though swapping is the major consumer) if it doesn't account for this fraction already. Is there some way we can unit test the effect of GPU memory limiting on Grappler's device information?\r\n\r\n(CC @rmlarsen who has reviewed some of that code)", "@allenlavoie I confirmed the memory_optimizer.cc is using virtual_cluster instead of singplemachine.cc. So I believe I need to add some similar logic for virtual_cluster.cc? ", "Sounds reasonable to me. Yao, does accounting for the per-process memory fraction in virtual_cluster make sense to you?", "@allenlavoie @zhangyaobit Let me recap a bit about the issues, and possible solutions. \r\n\r\n**Issues**\r\nIn memory optimizer, when it try to get GPU memory with following code, it get the full GPU memory size, this is not correct when use already set per_process_gpu_memory_fraction.  https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L969, \r\n\r\n\r\n**Analysis**\r\n- VirtualCluster is used in memoryoptimizer which is called by metaoptimizer (https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/optimizers/meta_optimizer.cc#L305).  \r\n- VirtualCluster->GeDevices() (in https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L949) return the \"std::unordered_map<string, DeviceProperties> devices_\" defined in VirtualCLuster's base class Cluster.h. \r\n- devices_ is initialized during VirtualCluster constructors (https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/clusters/virtual_cluster.cc). \r\n in this usage of memory_optimizer, the consturctor used is the 3rd one: https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/clusters/virtual_cluster.cc#L42. \r\n\r\n- Once devices_ is set, calling prop.memory_size() in https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/optimizers/memory_optimizer.cc#L969 return the total gpu memory (because of https://github.com/tensorflow/tensorflow/blob/575db18083d5437fd7a87ccf3414303498911bb1/tensorflow/core/grappler/clusters/utils.cc#L101). It does not care about setting like per_process_gpu_memory_fraction.  \r\n\r\n\r\n\r\n**On the other hand**\r\n\r\n @allenlavoie suggests that actually another cluster single_machine.cc is overriding the device property's memory_size (https://github.com/tensorflow/tensorflow/blob/fe12159e60258d0b6cae9b580e248b22e80f3f7b/tensorflow/core/grappler/clusters/single_machine.cc#L108) considering about user defined per_process_gpu_memory_fraction. (Though single_machine.cc is initlizing its devices_ in Provision, which is different with VirtualCluster, but I think it's not a big deal.)\r\n\r\nSo there seems to be 2 solutions:\r\n\r\n- Override the memory_size in VirtualCluster's constructor ( I think only the 3rd constructor should be modified, since the first 2 constructor accept DeviceProperties as input, which should be controlled by caller). \r\n- My current fixing. \r\n\r\nI prefer the 1st one, while still appreciate your comments about this. Thanks a lot!!  ", "I also like the 1st approach, which seems to have the least changes to the existing interface. I have one question, if there are multiple GPUs, which GPU do you choose to overwrite?", "@zhangyaobit  I think all GPU will be overwritten, because once per_process_gpu_memory_fraction is set, it  should impact all GPU, right? And single machine is doing this way. https://github.com/tensorflow/tensorflow/blob/fe12159e60258d0b6cae9b580e248b22e80f3f7b/tensorflow/core/grappler/clusters/single_machine.cc#L108 \r\n\r\nWhat do you think? :) ", "@pengwa This sounds good to me. Thanks.", "@zhangyaobit  I reverted previous changes, and merged with latest master. please take a look. :)  ", "great!! thanks @allenlavoie  @zhangyaobit !!"]}, {"number": 20238, "title": "Fix the name of MKL dockerfile in update_version.py.", "body": "", "comments": []}, {"number": 20237, "title": "[Intel MKL] Optimized implementation of GatherND using OpenMP", "body": "Eigen version does not parallelize well. With my parallel version, I get almost 2X improvement.", "comments": ["Nudge for review.", "@nhasabni, is this PR still alive? The sanity check is failing for this PR. Please take a look.", "@qlzh727 Yes, this PR is still active. I will look into the failure.", "It's ok to skip adding the shard non-openmp version.\n\nOn Thu, Aug 9, 2018, 10:46 AM Niranjan Hasabnis <notifications@github.com>\nwrote:\n\n> @qlzh727 <https://github.com/qlzh727> Yes, this PR is still active. I\n> will look into the failure.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20237#issuecomment-411840553>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimwENFF88v-4-_XIOp4BeWhs7cIuyks5uPHWRgaJpZM4U0hwb>\n> .\n>\n", "@qlzh727 Is it possible to restart the tests? I looked into the CI failure, I do not think it is related to my change. It was in `do_bazel_nobuild: bazel nobuild`. \r\n\r\nThanks @ebrevdo!", "Kicking off new tests.", "@qlzh727 Thanks for kicking off the run. I see 2 failures reported, but I am not able to access any of the logs files for the failures.", "The clang formatting seems to be failing, can u update your code and fix that?", "Can u access that link?\r\n\r\nhttps://source.cloud.google.com/results/invocations/3707dc61-9a5a-4b9c-9f9a-277c1f4c6145/targets/%2F%2Ftensorflow%2Ftools%2Fci_build:gen_ci_clang_format_out/log", "@qlzh727 Yes I can. But I don't see what are clang formatting errors. All that I see is my change and the command that was executed. I don't see any output log.\r\n\r\nIf I run clang formatting myself, all I see is 2 errors that are not related to my change.\r\n\r\n```\r\n$ cpplint.py tensorflow/core/kernels/gather_nd_op_cpu_impl.h\r\n\r\ntensorflow/core/kernels/gather_nd_op_cpu_impl.h:16:  #ifndef header guard has wrong style, please use: TENSORFLOW_CORE_KERNELS_GATHER_ND_OP_CPU_IMPL_H_  [build/header_guard] [5]\r\n\r\ntensorflow/core/kernels/gather_nd_op_cpu_impl.h:160:  #endif line should be \"#endif  // TENSORFLOW_CORE_KERNELS_GATHER_ND_OP_CPU_IMPL_H_\"  [build/header_guard] [5]\r\n\r\nDone processing tensorflow/core/kernels/gather_nd_op_cpu_impl.h\r\nTotal errors found: 2\r\n```", "There is a diff of the original file and the output of clang, you need to update your file to match the clang output, otherwise it will complain for the formatting.", "@qlzh727 So all tests look good now?", "@ebrevdo, please approve the PR if it LG to you. I will then sync and merge it.", "Nagging Assignee @qlzh727: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ebrevdo is on holiday at the moment, will ping him to approve the change once he's back."]}, {"number": 20236, "title": "[ROCm] Bazel build and continuous integration infrastructure", "body": "This pull request is to start introduce support for ROCm platform to TensorFlow. In this initial pull request, 2 components are addressed:\r\n\r\n- bazel build system\r\n- continuous integration logic\r\n\r\nAuthors:\r\n- Jack Chung: jack.chung@amd.com\r\n- Jeffrey Poznanovic: Jeffrey.Poznanovic@amd.com\r\n- Peng Sun: Peng.Sun@amd.com", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "signed CLA", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20235, "title": "Update Eigen version to commit e5e305a158a029f5b5f837bf821411a51439a970.", "body": "PiperOrigin-RevId: 201624024", "comments": ["This level of eigen contains a typo that fails the compile on ppc64le. Is it possible to not merge this into 1.9 until the typo is fixed? I can create a PR for the master branch when it is fixed.\r\n\r\nThe bug is this commit: https://bitbucket.org/eigen/eigen/commits/d2f8e1b572c0/\r\nThey are missing the closing ')'. I requested an account at eigen so I can comment on the bug. (Trying to get this fixed as soon as possible) ", "@wdirons we really should have this change in release R1.9. It will be a major performance improvement. However, I can submit a PR to Eigen to fix the issue you mention first.", "Thank you @rmlarsen , I also got my team mate to add a comment to the bug: http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1555\r\n\r\nI'll keep an eye on it for a fix.", "@wdirons Upstream Eigen PR: https://bitbucket.org/eigen/eigen/pull-requests/410/fix-typo-in-pbend-for-altivec/diff", "@rmlarsen , your patch was merged and I verified it on ppc64le. Thank you!\r\n\r\nPR #20254 created to update Eigen in the master branch.", "Re-triggering Windows Bazel build for a third time. Not sure why it keeps on failing, but I feel its unlikely due to this change.", "Windows Bazel build still failing :(", "@case540 ready to merge when the tests pass."]}, {"number": 20234, "title": "Make sure calibrator don't miss last batch", "body": "", "comments": ["Friendly ping @samikama, are there any updates?"]}]