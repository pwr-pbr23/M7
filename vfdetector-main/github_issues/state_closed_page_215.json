[{"number": 48183, "title": "Minor updates to documentation ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): N/A\r\n- Tensorflow version (commit SHA if source): 899fdb415dc970d5bca7d98a7dddf95968ef07c2\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): N/A\r\n\r\n**Describe the problem**\r\nSome documentation is outdated and needs an update.\r\nThe memory mgmt markdown needs an update regarding offline planned tensor allocations.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nN/A\r\n", "comments": ["@freddan80,\r\nLooks like PR [#48184](https://github.com/tensorflow/tensorflow/pull/48184) has been merged successfully. \r\n\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks! "]}, {"number": 48182, "title": "Fix the spawning of subprocess on windows displaying a console window ", "body": "Make it so that when running on the Windows platform the creation of a subprocess does not spawn a new console window. \r\nCloses https://github.com/tensorflow/tensorflow/issues/46854", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48182) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 48181, "title": "R1.8", "body": "Complie C++ project", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48181) for more info**.\n\n<!-- need_sender_cla -->", "@wanbeRichMan Thank you for your interest. We don't merge release branches back into master. \r\nCC @mihaimaruseac\r\n"]}, {"number": 48180, "title": "Unable to install tensorflow 2.4.1 with python3.8.2 pip", "body": "\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@millenniumbismay,\r\nIn order to expedite the trouble-shooting process, could you please provide the following details \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory\r\n\r\nand the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "Also, try running the below commands and let us know if you are still facing the same issue\r\n\r\n```\r\npip3 install --upgrade pip\r\npip3 install --upgrade tensorflow\r\n```\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48180\">No</a>\n"]}, {"number": 48179, "title": "Converting saved_model to .tflite with empty Signatures Key", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installation: pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tensorflow == 2.4.1\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel_dir = 'model'\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS,\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\n\r\ntflite_model = converter.convert()\r\nfo = open(\"model.tflite\", \"wb\")\r\nfo.write(tflite_model)\r\nfo.close\r\n```\r\nLink to model: https://drive.google.com/file/d/1hU2zOpMPqH0gaASiDLmTZ6MFTotMEfw9/view?usp=sharing\r\n\r\n\r\n### 3. Failure after conversion\r\nConversion failed.\r\n\r\n```\r\nValueError: Only support a single signature key.\r\n```\r\n\r\n### 5. (optional) Any other info / logs\r\n```\r\nmodel_dir = 'model'\r\nsaved_model = tf.saved_model.load(export_dir=model_dir)\r\nSignature =  saved_model.signatures\r\nprint(Signature, \"Len: \", len(Signature))\r\n```\r\nOutput:\r\n```\r\n_SignatureMap({}) Len:  0\r\n```", "comments": ["For this case, could you embed your saved model into a keras model and then convert the keras model to the TFLite converter?", "I hope I understand you well. As you suggested, I tried to convert the saved model to the keras model (h5) , but  i got some troubles.\r\n\r\nkeras==2.4.3\r\ntensorflow==2.4.1\r\n\r\nCode:\r\n```\r\nimport tensorflow as tf\r\nmodel_dir = 'model'\r\nh5_model = \"mymodel.h5\"\r\nmodel = tf.saved_model.load(model_dir)\r\n# model = tf.keras.models.load_model(model_dir) # trying both method to load model\r\ntf.keras.models.save_model(model, h5_model)\r\n```\r\nError:\r\n```\r\nAttributeError: '_UserObject' object has no attribute '_is_graph_network'\r\nException ignored in: <function CapturableResourceDeleter.__del__ at 0x7fc91c7200d0>\r\nTypeError: super() argument 1 must be type, not None\r\n```\r\n\r\nWhen i switch to tf-nightly-2.6.0.dev20210329 got:\r\n\r\n```\r\nAttributeError: '_UserObject' object has no attribute 'outputs'\r\n```\r\n\r\nSo looking into similar problem i tried load keras layer:\r\ntf-hub-nightly-0.12.0.dev202103300006\r\n\r\n```\r\nimport tensorflow_hub as hub\r\nmodel_dir = 'model'\r\nmodel_layers = hub.KerasLayer(model_dir)\r\n```\r\n\r\nError:\r\n```\r\nraise ValueError(\"Signature name has to be specified for non-callable \"\r\nValueError: Signature name has to be specified for non-callable saved models (if not legacy TF1 Hub format)\r\n```\r\n\r\nThere are any other option left to convert?", "@NeRR0,\r\nPlease refer this [Stack Overflow Answer](https://stackoverflow.com/a/62211811/11530462) for help to convert the Models from **`Tensorflow Saved Model`** Format to **`H5`** Format. Thanks!", "@rmothukuru \r\nI've seen and tested this answer before, but I still have a problem like I wrote on my second answer and the same\r\n `summary()` problem as the comments on the stackoverflow answer. Can I count on your help or ideas?", "@NeRR0 Could you share the information about the inputs and output specification of the saved model?\r\n\r\nSorry, I am not saying that the saved model should be converted to the h5 file format. Instead, I am thinking of the following code snippet:\r\n\r\n```\r\nmodel = tf.saved_model.load(model_dir)\r\n\r\nkeras_model = tf.keras.Sequential()\r\nkeras_model.add(...)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\n```", "@abattery\r\n\r\nIf you only ask for the size of the input then its (?, 256, 256 ,3).\r\n\r\nThe model return 3 outputs: \r\nI. shape=(1, 5), dtype=float32)\r\nII. shape=(1, 24, 3), dtype=float32)\r\nIII. shape=(1, 24, 3), dtype=float32)\r\n\r\nI would like to share more information but unfonetly I am unable to read them from Netron. I dont know if its important but\r\nthe entire architecture is based on ResNet101 with modifications and the model is saved by method `tf.saved_model.save`. \r\n\r\nAbout your code snippet, you propose to re-write a whole model using keras layers?\r\n\r\n\r\nBelow I put a link from debug mode from calling the method model, maybe there is some important information that will be useful. [https://i.imgur.com/k68J5xY.png](url)", "@NeRR0 is it possible to regenerate the saved model again with the signature def information? Please refer to https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export Then you can specify the name of signature at the signature_keys in the from_saved_model API.", "Did any one solved this issue? ", "not yet..", "If you hit this issue, please export your saved model with the signature information. You can refer to https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48179\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48179\">No</a>\n"]}, {"number": 48177, "title": "Time Series Prediction Bug", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Anaconda Distribution\r\n- TensorFlow version (use command below): unknown 2.3.0\r\n- Python version: 2.8.6\r\n- Bazel version (if compiling from source): No\r\n- GCC/Compiler version (if compiling from source): No\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` \r\n\r\n\r\n**Describe the current behavior**\r\n****\r\nWARNING:tensorflow:AutoGraph could not transform <bound method split_window of Total window size: 22\r\nInput indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\r\nLabel indices: [21]\r\nLabel column name(s): ['Temperature_mean']> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING: AutoGraph could not transform <bound method split_window of Total window size: 22\r\nInput indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\r\nLabel indices: [21]\r\nLabel column name(s): ['Temperature_mean']> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module 'gast' has no attribute 'Index'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n(TensorSpec(shape=(None, 21, 6), dtype=tf.float32, name=None),\r\n TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))\r\n\r\n**Describe the expected behavior**\r\n****\r\nAccording to the tutorial this should be the output:\r\n(TensorSpec(shape=(None, 6, 19), dtype=tf.float32, name=None),\r\n TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n****\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nFollowing this TF tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series#2_split\r\nThe error happens in \"w1.train.element_spec\" part\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@mgmanzanillo,\r\nI did not face any errors while running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/e7f299f46e51b6f0b9eb645b307a4369/time_series.ipynb#scrollTo=daJ0-U383YVs&line=2&uniqifier=1) and [TF v2.4](https://colab.research.google.com/gist/amahendrakar/d4ce2e1745ce9bd39745f21469f4d4c2/48177-2-4.ipynb#scrollTo=daJ0-U383YVs&line=1&uniqifier=1). Please check the linked gist for reference.\r\n\r\nRegarding the warning messages, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/47802#issuecomment-799555152) from issue #47802 and check if it helps. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48177\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48177\">No</a>\n"]}, {"number": 48176, "title": "FP16 support for grouped convolutions", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, grouped convolutions (at least for Conv2D) doesn't see any speedup from FP16 training, although memory usage is still lowered training in FP16. When doing a mixed precision test, running tf.nn.conv2d and K.conv2d with grouped convolutions in FP16 has equal performance to FP32 and running the same operations without groups (same number of input/output channels) has about an 80% speedup in FP16.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["@BearNinja123,\r\nCouple of points:\r\n1. Documentation of [`Mixed Precision`](https://www.tensorflow.org/guide/mixed_precision) states: \r\n\r\n> While mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs and Cloud TPUs. NVIDIA GPUs support using a mix of float16 and float32, while TPUs support a mix of bfloat16 and float32.\r\n> \r\n> Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions.\r\n\r\nCan you please confirm if you are using latest `GPU` with `Compute Capability` 7.0 or higher?\r\n\r\n2.  Can you please try using [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and confirm if you are experiencing the same `performance` (because this API is more active compared to the ones you specified)?\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48175, "title": "tensorflow: AutoGraph could not transform data", "body": "Error reported in Python 3.9.2 tf-nightly:\r\n\"WARNING:tensorflow:AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x0000019783941EE0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: invalid syntax (tmp0qln7o16.py, line 48)\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\"\r\n\r\nThis feature works in Tensorflow for Python 3.8 but not in tf-nightly for Python 3.9", "comments": ["@raposter Could you please fill the issue template. Also, please provide the simple standalone code/colab link to reproduce the issue at our end.\r\n\r\nPlease have a look at this [issue](https://github.com/tensorflow/tensorflow/issues/47802#issuecomment-799555152). Hope it helps!.\r\n", "```\r\nfrom random import random\r\nfrom numpy import array\r\nfrom matplotlib import pyplot\r\nfrom matplotlib.patches import PathPatch\r\nfrom matplotlib.path import Path\r\nfrom keras.models import Sequential\r\nfrom keras.layers import LSTM\r\nfrom keras.layers import Dense\r\n\r\n# generate a rectangle with random width and height\r\ndef random_rectangle():\r\n\twidth, height = random(), random()\r\n\tpoints = list()\r\n\t# bottom left\r\n\tpoints.append([0.0, 0.0])\r\n\t# bottom right\r\n\tpoints.append([width, 0.0])\r\n\t# top right\r\n\tpoints.append([width, height])\r\n\t# top left\r\n\tpoints.append([0.0, height])\r\n\treturn points\r\n\r\n# plot a rectangle\r\ndef plot_rectangle(rect):\r\n\t# close the rectangle path\r\n\trect.append(rect[0])\r\n\t# define path\r\n\tcodes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\r\n\tpath = Path(rect, codes)\r\n\taxis = pyplot.gca()\r\n\tpatch = PathPatch(path)\r\n\t# add shape to plot\r\n\taxis.add_patch(patch)\r\n\taxis.set_xlim(-0.1,1.1)\r\n\taxis.set_ylim(-0.1,1.1)\r\n\tpyplot.show()\r\n\r\n# generate input and output sequences for one random rectangle\r\ndef get_samples():\r\n\t# generate rectangle\r\n\trect = random_rectangle()\r\n\tX, y = list(), list()\r\n\t# create input output pairs for each coordinate\r\n\tfor i in range(1, len(rect)):\r\n\t\tX.append(rect[i-1])\r\n\t\ty.append(rect[i])\r\n\t# convert input sequence shape to have 1 time step and 2 features\r\n\tX, y = array(X), array(y)\r\n\tX = X.reshape((X.shape[0], 1, 2))\r\n\treturn X, y\r\n\r\n# use a fit LSTM model to generate a new rectangle from scratch\r\ndef generate_rectangle(model):\r\n\trect = list()\r\n\t# use [0,0] to seed the generation process\r\n\tlast = array([0.0,0.0]).reshape((1, 1, 2))\r\n\trect.append([[y for y in x] for x in last[0]][0])\r\n\t# generate the remaining 3 coordinates\r\n\tfor _ in range(3):\r\n\t\t# predict the next coordinate\r\n\t\tyhat = model.predict(last, verbose=0)\r\n\t\t# use this output as input for the next prediction\r\n\t\tlast = yhat.reshape((1, 1, 2))\r\n\t\t# store coordinate\r\n\t\trect.append([[y for y in x] for x in last[0]][0])\r\n\treturn rect\r\n\r\n# define model\r\nmodel = Sequential()\r\nmodel.add(LSTM(10, input_shape=(1, 2)))\r\nmodel.add(Dense(2, activation='linear'))\r\nmodel.compile(loss='mae', optimizer='adam')\r\nmodel.summary()\r\n\r\n# fit model\r\nfor i in range(25000):\r\n\tX, y = get_samples()\r\n\tmodel.fit(X, y, epochs=1, verbose=2, shuffle=False)\r\n\r\n# generate new shapes from scratch\r\nrect = generate_rectangle(model)\r\nplot_rectangle(rect)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "@raposter \r\nPlease share a colab gist with the error reported.", " I've removed Version 3.9 and cannot regererate any errors and am unfamiliar with what a colab gist is.\u00a0 So, I cannot help you with this problem.\u00a0 The code I included is from a tutorial and can readily be run in the 3.9 environment.Thanks\n    On Friday, April 9, 2021, 12:07:43 PM PDT, Saduf2019 ***@***.***> wrote:  \n \n \n\n\n@raposter\nPlease share a colab gist with the error reported.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@raposter \r\nPlease provide the code in format such that we can replicate the error reported, as the code shared has too many indentation errors.", " Enclosed is the source code taken from the LSTM Networks book by Jason Bownlee.Thanks\n    On Wednesday, April 21, 2021, 12:05:11 PM PDT, Saduf2019 ***@***.***> wrote:  \n \n \n\n\n@raposter\nPlease provide the code in format such that we can replicate the error reported, as the code shared has too many indentation errors.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "@raposter I formatted your code and ran it in colab (which is similar to Jupyter notebook) and I don't see any issue there. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/483b8bebb873c0ba894ab0fef496bbb9/untitled.ipynb). \r\n\r\nTwo things i modified in your code\r\n1. imported keras from tensorflow (`from tensorflow import keras`)\r\n2. changed for loop iterations from 25000 to 250 (just to speed up)\r\n\r\nPlease verify it and close if the issue was resolved for you. If it was not resolved, please share a jupyter notebook or python file for reproducing the issue. Thanks", " My issue occurred with Python 3.9.\u00a0 It looks like from the gist, that Python 3.7 was used.\u00a0 If not, and it works in Python 3.9 then please close the issue.Thanks\u00a0\u00a0\n    On Thursday, April 22, 2021, 02:38:09 PM PDT, Vishnuvardhan Janapati ***@***.***> wrote:  \n \n \n\n\n@raposter I formatted your code and ran it in colab (which is similar to Jupyter notebook) and I don't see any issue there. Please check the gist here.\n\nTwo things i modified in your code\n   \n   - imported keras from tensorflow (from tensorflow import keras)\n   - changed for loop iterations from 25000 to 250 (just to speed up)\n\nPlease verify it and close if the issue was resolved for you. If it was not resolved, please share a jupyter notebook or python file for reproducing the issue. Thanks\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@raposter Is it possible for you to check with `python3.9`? You can download `*.py` file from my colab and run it. Thanks!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48175\">No</a>\n"]}, {"number": 48174, "title": "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard contains broken links", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe doc for the `embeddings_metadata` argument contains a broken link. It says \"See the details about metadata files format.\", and the link redirects to https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional, which is broken.\r\n", "comments": ["@atn832 ,\r\nCan you please share us the exact link where **`embeddings_metadata`** was located, so that we can fix it? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 48172, "title": "Update release notes for TensorFlow 2.5.0", "body": "", "comments": []}, {"number": 48171, "title": "tf.cast zeroes out input tensor", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but a stock example would fail too, e.g. \"style_transfer.ipynb\" fails on my setup when trying to convert image into a floating point tensor.\r\n- OS Platform and Distribution: Windows 7, 64 bit\r\n- TensorFlow installed from: binary, installed using command \"pip install tensorflow==2.3.2\"\r\n- TensorFlow version: v2.3.1-38-g9edbe5075f7 2.3.2\r\n- Python version: WinPython64 ver.3.8.7.0cod (Python version 3.8.7)\r\n- CUDA/cuDNN version: CUDA Version 10.1.243, i.e. 10.1 update 2 / cuDNN ver.7.6.5.32\r\n- GPU model and memory:  Nvidia Geforce GTX 750 TI, memory: 2GB\r\n\r\n**Describe the current behavior**\r\nCUDA is successfully recognized by the TensorFlow - I get the following message when the TensorFlow is started:\r\n`(2021-03-29 22:59:14.061267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll)`\r\nCUDA also runs its own basic self-tests without apparent issues.\r\n\r\nUnfortunately, when I am trying to run more complex code, error messages are generated (e.g. when trying to run TensorFlow's style transfer tutorial \"style_transfer.ipynb\". I traced the issue to the casting of integer numbers to floating point numbers, which generates the floating point zeros. There was an earlier bug described here: https://github.com/tensorflow/tensorflow/issues/14147 which sounds similar.\r\n\r\nHowever, I do not seem to need to use a lot of GPU memory to trigger the issue. Even the following very basic test:\r\n```\r\nimport tensorflow as tf\r\nivalue = tf.constant(10)\r\nprint(ivalue)\r\nfvalue = tf.cast(ivalue, tf.float32)\r\nprint(fvalue)\r\n```\r\nproduces output\r\n\r\n```\r\ntf.Tensor(10, shape=(), dtype=int32)\r\ntf.Tensor(0.0, shape=(), dtype=float32)\r\n```\r\nHowever, if I disable the CUDA acceleration by issuing the following commands at the start of my script:\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n```\r\nthe script works correctly, as expected.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the script above to output\r\n```\r\ntf.Tensor(10, shape=(), dtype=int32)\r\ntf.Tensor(10.0, shape=(), dtype=float32)\r\n```\r\nwithout having to disable CUDA.\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease see above. Please let me know if I need to run a more complex code to help identify what is wrong more precisely.\r\n\r\n**Other info / logs**\r\nI actually attempted to install several combinations of versions of TensorFlow and CUDA, trying to see if it will make any difference. Specifically, I tried installing TensorFlow 2.2.0, 2.3.0 and 2.3.2 with various minor versions of CUDA 10.1 and cuDNN 7.6. It made no difference at all.", "comments": ["@specke  I tried to reproduce the issue with  TF version 2.3 and 2.4 as well  but I am not experiencing any issues and it is working as expected. \r\n```\r\ntf.Tensor(10, shape=(), dtype=int32)\r\ntf.Tensor(10.0, shape=(), dtype=float32)\r\n```\r\nPlease find the gist [here](https://colab.research.google.com/gist/saikumarchalla/882dda21d65b19c1c2c33fd9442d045a/-48171.ipynb).Thanks!", "I understand that this must be specific to my setup - otherwise you'd have thousands more people complaining!\r\n\r\nSo it has something to do with how TF connects to GPU on my machine. Can you at least advise about the debugging process in this situation? As in: since I already happen to have the setup where things do not work as expected, I would like to at least try to identify what the actual issue is.", "@specke  Could you please  create a virtual environment and test your code again. May be that works. Thanks!", "@saikumarchalla, my apologies for the delay. This is the record of what happens when I create a new virtual environment, install TensorFlow ver.2.3.1 onto it and try to run the commands above:\r\n[tf_cuda_bug.log](https://github.com/tensorflow/tensorflow/files/6267680/tf_cuda_bug.log).\r\n\r\nI did not try using TensorFlow ver,2.4 because it requires CUDA 11.0, which does not support Windows 7. I can try installing it and running it, of course, but I thought that using a supported combination of versions of TensorFlow, CUDA and cuDNN is important.", "@specke Is it still an issue at your end  and please let us know.", "Yes, it is still an issue, I am awaiting for a tensorflower since 12th of April. CUDA-enabled Tensorflow is unusable on my setup.", "@specke Is this still an issue for you? Can you please check with recent TF versions and let us know whether it was resolved for you. \r\n\r\nIf you are still facing the issue, it is better to uninstall all cuda related files, restart, and reinstall cuda libraries. Another option is to install conda package and then install related cuda tools. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48171\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48171\">No</a>\n"]}, {"number": 48168, "title": "fixed issue #48026: unintended mask shape", "body": "", "comments": []}, {"number": 48166, "title": "Prediction from saved Estimator is Tensor with no numpy method", "body": "**System information**\r\n- OS Platform and Distribution: MacOS 11.2.3\r\n- TensorFlow installed from: source\r\n- TensorFlow version (use command below): GIT_VERSION: v1.12.1-48291-g8867b44e4cd  VERSION: 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source):  4.0.0-homebrew\r\n- GCC/Compiler version (if compiling from source): clang\r\n\r\nI have an Estimator model that I have fit:\r\n\r\n```python\r\n    estimator = tf.estimator.DNNClassifier(\r\n        feature_columns=get_feature_columns(),\r\n        hidden_units=[128, 128],\r\n        n_classes=n_classes,\r\n        activation_fn=tf.nn.swish,\r\n        dropout=DROPOUT,\r\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n    )\r\n\r\n    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(batch_size=128), max_steps=10_000)\r\n    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(training=False))\r\n\r\n    eval_result, _ = tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\nand have saved it to disk using `export_saved_model`. When I load the model into a new Python session, it generates predictions without error:\r\n\r\n```python\r\nimported = tf.compat.v2.saved_model.load(estimator_path)\r\n\r\ndef dfrow_to_example(input_df):\r\n    example = tf.train.Example()\r\n    for feature_name, value in input_df.iteritems():\r\n        if value.dtype == np.int64:\r\n            example.features.feature[feature_name].int64_list.value.extend(\r\n            list(value.values))\r\n        elif value.dtype == np.float64:\r\n            example.features.feature[feature_name].float_list.value.extend(\r\n            list(value.values))\r\n    return example\r\n\r\n\r\ndef predict_input_fn(input_df):\r\n    example = dfrow_to_example(input_df)\r\n    return tf.constant([example.SerializeToString()])\r\n\r\npredict_fn = imported.signatures['predict']\r\n\r\nprediction = predict_fn(examples=predict_input_fn(input_data))\r\n```\r\n\r\nHowever, I am returned a dict of Tensors, but they do not have `numpy()` methods, so I cannot access the values:\r\n\r\n```python\r\n{'classes': <tf.Tensor 'StatefulPartitionedCall_4:3' shape=(1, 1) dtype=string>,\r\n 'probabilities': <tf.Tensor 'StatefulPartitionedCall_4:5' shape=(1, 18) dtype=float32>,\r\n 'all_class_ids': <tf.Tensor 'StatefulPartitionedCall_4:0' shape=(1, 18) dtype=int32>,\r\n 'all_classes': <tf.Tensor 'StatefulPartitionedCall_4:1' shape=(1, 18) dtype=string>,\r\n 'logits': <tf.Tensor 'StatefulPartitionedCall_4:4' shape=(1, 18) dtype=float32>,\r\n 'class_ids': <tf.Tensor 'StatefulPartitionedCall_4:2' shape=(1, 1) dtype=int64>}\r\n```\r\n\r\n```\r\n\r\nprediction['probabilities'].numpy()\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n~/Yankees/nn_matchup/models/matchup_model_test_saved.py in <module>\r\n----> 1 prediction['probabilities'].numpy()\r\n\r\nAttributeError: 'Tensor' object has no attribute 'numpy'\r\n```\r\n\r\nHave I done something wrong with my input function? \r\n", "comments": ["@fonnesbeck,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'get_feature_columns' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2ef70c901976d666a27d04cc204f51d0/48166.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48166\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48166\">No</a>\n"]}, {"number": 48165, "title": "Python tensorflow and tensorflow-cpu dependencies", "body": "Although undocumented, it seems that the [tensorflow-cpu](https://pypi.org/project/tensorflow-cpu/) package is meant to be a CPU-only version of tensorflow to enable smaller package sizes.\r\n\r\nGiven that there is no dependency relationship between `tensorflow-cpu` and `tensorflow`, ecosystem libraries have no way of specifying that they need either package (see for example [this issue](https://discuss.python.org/t/conditional-package-install-depending-on-other-packages-in-environment/4140)).\r\n\r\nIn hindsight, the most obvious solution would have been to make a `tensorflow-core` package or something, but idk if that's possible without a lot of work or breaking things.\r\n\r\nI'm not sure what the solution is here, but I just wanted to raise the issue.", "comments": ["There was an old  similar thread at https://github.com/tensorflow/tensorflow/issues/7166", "I see, thank you for linking me! Indeed it does seem like the same issue. It's unfortunate that no resolution was reached.", "I'll close this since it really is a dupe of #7166, resolution or not", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48165\">No</a>\n"]}, {"number": 48164, "title": "Check that OPTIMIZED_KERNEL_DIR command line option is valid.", "body": "See http://b/183546742 for more details.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 48163, "title": "Keras: Remove leftover Python 2 compatibility code", "body": "This PR removes some leftover Python 2 compatibility code from Keras since the minimal Python version is now 3.6.", "comments": []}, {"number": 48162, "title": "LoadDataset op uses a bytes string as its name", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux NixOS 20.09\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen trying to use an `op_regex` in `tf.debugging.experimental.enable_dump_debug_info` while using `tf.data.experimental.load` you get an error that you `cannot use a string pattern on a bytes-like object`. Looking at the stack trace (below) it appears the error comes from the fact that the `LoadDataset` op is using a bytes string for the op name while other ops in `load` use regular strings. \r\n\r\n**Describe the expected behavior**\r\nYou should be able to use a regex to filter ops as described on the api reference page for `tf.data.experimental.load`.\r\n\r\n**Other info / logs**\r\nWith a string regex\r\n```\r\nTraceback (most recent call last):\r\n  File \"/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/NVME/CubeCobraRecommender/src/ml/train_draftbots.py\", line 114, in <module>\r\n    train_dataset = load_picks(pick_cache_dir / 'train', batch_size)\r\n  File \"/mnt/NVME/CubeCobraRecommender/src/non_ml/parse_picks.py\", line 395, in load_picks\r\n    return tf.data.experimental.load(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py\", line 201, in load\r\n    return _LoadDataset(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py\", line 129, in __init__\r\n    variant_tensor = gen_experimental_dataset_ops.load_dataset(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_experimental_dataset_ops.py\", line 5636, in load_dataset\r\n    return load_dataset_eager_fallback(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_experimental_dataset_ops.py\", line 5698, in load_dataset_eager_fallback\r\n    _result = _execute.execute(b\"LoadDataset\", 1, inputs=_inputs_flat,\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 140, in execute_with_callbacks\r\n    callback(op_name, tuple(inputs), attrs, tensors, name)\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py\", line 595, in callback\r\n    writer.WriteExecution(self._dump_eager_tensors(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py\", line 505, in _dump_eager_tensors\r\n    if (self._should_dump_tensor(op_type, tensor.dtype) and\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py\", line 675, in _should_dump_tensor\r\n    re.match(self._op_regex, op_type))\r\n  File \"/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/re.py\", line 191, in match\r\n    return _compile(pattern, flags).match(string)\r\nTypeError: cannot use a string pattern on a bytes-like object\r\n```\r\nand with a bytes regex\r\n```\r\nTraceback (most recent call last):\r\n  File \"/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/mnt/NVME/CubeCobraRecommender/src/ml/train_draftbots.py\", line 114, in <module>\r\n    train_dataset = load_picks(pick_cache_dir / 'train', batch_size)\r\n  File \"/mnt/NVME/CubeCobraRecommender/src/non_ml/parse_picks.py\", line 395, in load_picks\r\n    return tf.data.experimental.load(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py\", line 201, in load\r\n    return _LoadDataset(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/io.py\", line 122, in __init__\r\n    self._reader_func = dataset_ops.StructuredFunctionWrapper(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 3525, in __init__\r\n    self._function = wrapper_fn.get_concrete_function()\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3051, in get_concrete_function\r\n    graph_function = self._get_concrete_function_garbage_collected(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3019, in _get_concrete_function_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 906, in func_graph_from_py_func\r\n    func_args = _get_defun_inputs_from_args(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1142, in _get_defun_inputs_from_args\r\n    return _get_defun_inputs(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 1215, in _get_defun_inputs\r\n    placeholder = graph_placeholder(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/eager/graph_only_ops.py\", line 45, in graph_placeholder\r\n    callback_outputs = op_callbacks.invoke_op_callbacks(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/framework/op_callbacks.py\", line 202, in invoke_op_callbacks\r\n    new_outputs = callback(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py\", line 581, in callback\r\n    return self._instrument_symbolic_tensors(\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py\", line 415, in _instrument_symbolic_tensors\r\n    if (not self._should_dump_tensor(op_type, tensor.dtype) or\r\n  File \"/mnt/NVME/CubeCobraRecommender/.venv/lib/python3.8/site-packages/tensorflow/python/debug/lib/dumping_callback.py\", line 675, in _should_dump_tensor\r\n    re.match(self._op_regex, op_type))\r\n  File \"/nix/store/v3bj7jrns4sk6yj2rp30p6v2l7p707az-python3-3.8.8/lib/python3.8/re.py\", line 191, in match\r\n    return _compile(pattern, flags).match(string)\r\nTypeError: cannot use a bytes pattern on a string-like object\r\n```", "comments": ["@ruler501,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48162\">No</a>\n"]}, {"number": 48161, "title": "Add a reference to the API review practices", "body": "", "comments": ["The `TF API Owners` in the first paragraph of https://github.com/tensorflow/community/blob/master/governance/api-reviews.md is dead btw. Looks like I'm not the only one: https://github.com/tensorflow/community/issues/280"]}, {"number": 48160, "title": "micro: add INT8 support to ADD_N op", "body": "Added support for INT8 to the ADD_N operator.\r\n\r\nReference Issue #46162", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`."]}, {"number": 48159, "title": "ImportError on Raspberry Pi 4 Model B: cannot import name 'symbol_database' from 'google.protobuf' ", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution: Linux raspberrypi 5.10.17-v7l+ #1403 SMP Mon Feb 22 11:33:35 GMT 2021 armv7l GNU/Linux\r\n- Raspberry Pi 4 Model B\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7\r\n- Google Protobuf version: 3.15.6\r\n\r\n", "comments": ["@VasyaSmolyar,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem?\r\n\r\nAlso, take a look at [this comment](https://stackoverflow.com/a/40981878) from a similar StackOverflow query and check if it helps. Thanks!", "Reinstalling protobuf is not helps at all. I install binary TF from here https://github.com/Qengineering/Tensorflow-Raspberry-Pi/", "@VasyaSmolyar,\r\nPlease follow the [official guide](https://www.tensorflow.org/install/pip#raspberry-pi) while installing TensorFlow and make sure you have all the required dependencies listed in the [system requirements](https://www.tensorflow.org/install/pip#system-requirements). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48159\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48159\">No</a>\n"]}, {"number": 48158, "title": "Fix depthwise_conv_test for xtensa", "body": "Remove check causing test failure.\r\n\r\nProgress towards b/183497550", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Tagging @nyadla-sys "]}, {"number": 48156, "title": "Huge code size after compile the TFLM hello world example for ARM", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): SHA\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM Cortex M33 bare mental (SSE 200)\r\n\r\n**Describe the problem**\r\n##########\r\nI want to implement the TFLM into my own ARM board, which has a flash not more than 512KB. And my compiler is arm compiler.  The first step I did is to try the hello world example. I expected a binary file less than 20KB as it says in the website, but my compilation result is 348KB. I don't know how to handle that.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n#############\r\nFirst, I generated a static library by using `make -f tensorflow/lite/micro/tools/make/Makefile TOOLCHAIN=armclang TARGET_TOOLCHAIN_ROOT=/usr/pack/armmisc-201912-tonl/ARMCompiler6.13/bin/ TARGET=cortex_m_generic TARGET_ARCH=cortex-m33 microlite ARMLMD_LICENSE_FILE:=xxx`. Even the generated static library `libtensorflow-microlite.a` is 1.1MB, which is really large compared other issues posted here.\r\nThen, I modified my makefile, adding the lib directory into the dependency,\r\n```\r\n$(OUTPUT_DIR)/$(BUILD_S_NS)/$(TEST_NAME).elf:\t$(CPU_MCU_OBJ) $(MICROLITE_LIB_PATH)  \r\n\t@set -euo pipefail;\\  \r\n\techo \"Making $@ ...\";\\  \r\n\tmkdir -p $(@D);\\  \r\n\texport ARMLMD_LICENSE_FILE=$(ARMLMD_LICENSE_FILE);\\\r\n\t$(LD) $(LDFLAGS) -o $@ $^ > $@.log`\r\n```\r\nI also changed the ops from `all_ops_resolver` to `micro_mutable_op_resolver.h`, and modified the code in `main_function.cc hello_world_test.cc` to \r\n```\r\n// static tflite::AllOpsResolver resolver;\r\n  \r\n  static tflite::MicroMutableOpResolver<1> micro_op_resolver(error_reporter);\r\n  if (micro_op_resolver.AddFullyConnected() != kTfLiteOk) {\r\n    return;\r\n  }\r\n\r\n```\r\nThe `CPU_MCU_OBJ` includes the five source code which should be compiled, `main.cc main_functions.cc models.cc constants.cc output_handler.cc`.  It will generate `.elf .hex .disass .map`, I really care about the size of `.hex` file, which is 348KB, far more larger than it indicated less than 20KB in the website. \r\n", "comments": ["@ymodak @saikumarchalla Is there anyone can give me some hint?", "Hi, I have maybe the same issue. \r\n\r\nI am currently trying the example Hello World on **Zephyr OS**.\r\n\r\n#################################################\r\n\r\nThe example code takes **157kB** of **FLASH** memory. \r\n\r\n- From original example code:\r\n\r\n        Memory region         Used Size  Region Size  %age Used\r\n           FLASH:      157376 B         1 MB     15.01%\r\n            SRAM:       19112 B       448 KB      4.17%\r\n           SRAM1:          0 GB        64 KB      0.00%\r\n        IDT_LIST:          0 GB         2 KB      0.00%\r\n\r\nI first thought it was the size of the model, so I take away the entire model on purpose, then I get the result of **140kB**, which indicates the TF Lite for Micro itself takes the most of the space rather than the model. \r\n\r\n- From the code without model (_model.cc/model.h_)\r\n\r\n        Memory region         Used Size  Region Size  %age Used\r\n           FLASH:      140376 B         1 MB     13.39%\r\n            SRAM:       10792 B       448 KB      2.35%\r\n           SRAM1:          0 GB        64 KB      0.00%\r\n        IDT_LIST:          0 GB         2 KB      0.00%\r\n#################################################\r\n\r\nPlus: The `.hex` file also takes **442kB** in this case. On the contrary, `.hex` only takes around **52kB** when without TensorFlow in this case.\r\n\r\n#################################################\r\n\r\nIs there any solution for this?\r\n\r\n\r\n\r\n ", "@NatakuG it looks like no answer until now, and I do not know how to handle it :)", "Hi all,\r\n\r\nI find the answer on my side.\r\n\r\nOn the TensorFlow guide website: https://www.tensorflow.org/lite/microcontrollers/library\r\n\r\n- It says:\r\n> all_ops_resolver.h or micro_mutable_op_resolver.h can be used to provide the operations used by the interpreter to run the model. Since all_ops_resolver.h pulls in every available operation, it uses a lot of memory. In production applications, you should use micro_mutable_op_resolver.h to pull in only the operations your model needs.\r\n\r\nAnd in my code, I'm using:\r\n\r\n\tstatic tflite::MicroMutableOpResolver<4> resolver;\r\n\r\nInstead of:\r\n\r\n\tXXX static tflite::AllOpsResolver resolver; XXX\r\n\r\nAnd then add the operations that I need, for example:\r\n\r\n\tresolver.AddFullyConnected();\r\n\r\nHere's the result, it drops to 5% of FLASH:\r\n\r\n        Memory region    Used Size  Region Size  %age Used\r\n           FLASH:       52432 B         1 MB      5.00%\r\n            SRAM:       16976 B       448 KB      3.70%\r\n           SRAM1:          0 GB        64 KB      0.00%\r\n        IDT_LIST:          0 GB         2 KB      0.00%\r\n\r\n@napoleonwar I hope this can help you on your side too", "@napoleonwar,\r\n\r\nCan you take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/48156#issuecomment-858452541) by @NatakuG, and let us know if it helps in resolving your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48156\">No</a>\n"]}, {"number": 48155, "title": "Filename clashes in /tmp during flatbuffer download", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 1e8f4666f2fbc1bdd4ce2797b218de0453cffc63\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): all\r\n\r\n**Describe the problem**\r\n\r\nThe script `tensorflow/lite/micro/tools/make/flatbuffers_download.sh` creates a temporary files/dirs in `/tmp`\r\nwhose names are not uniqified and not all of which are subsequently removed.  This means builds by different users on a shared server host fail and junk is left lying around in `/tmp`.\r\n\r\nA small patch correcting these issues by using `mktemp`  is attached.  This approach is the same\r\nas that used in the  `tensorflow/lite/micro/tools/make/download_and_extract.sh` script.\r\n\r\n[tmp_filename_clash_fix.patch.txt](https://github.com/tensorflow/tensorflow/files/6221742/tmp_filename_clash_fix.patch.txt)\r\n\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nmake -f tensorflow/lite/micro/tools/make/Makefile\r\n\r\n", "comments": ["Thanks for the fix. Created https://github.com/tensorflow/tensorflow/pull/48241 with the patch.\r\n\r\nFeel free to send a PR as well.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48155\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48155\">No</a>\n"]}, {"number": 48154, "title": "Compact Multi-Class Boosted Trees configuration ", "body": "I am trying implement the TFBT algorithm from the [Compact multi-class boosted trees](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA) paper. As I understand this [paper](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA) used the [BoostedTreeClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933) with [this](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees_test.py#L403) config.\r\n\r\n- Would you please indicate the same **setting** and **hyperparameter** which you used in your paper?\r\n- Also, would you verify that the mentioned paper used the above code?\r\n- How can I have Fit method in [BoostedTreeClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933)?\r\n\r\n", "comments": ["@samanemami,\r\nThe usage of **`Boosted Trees Using Estimators`** is demonstrated in the [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/estimator/boosted_trees) . It is further elaborated in [this Tutorial](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding).\r\n\r\nIf this is not what you are looking for, can you please elaborate your point. Thanks!", "I have read this [Tutorial](https://www.tensorflow.org/tutorials/estimator/boosted_trees). The thing is I need to be sure that this is the same model that you used in this [paper](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA).\r\nAlso, I need to have the configuration of the hyper-parameter which you used in this [paper](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA).", "@samanemami,\r\nThe Tutorial, [Boosted trees using Estimators](https://www.tensorflow.org/tutorials/estimator/boosted_trees) states, \r\n\r\n> This tutorial is an end-to-end walkthrough of training a Gradient Boosting model using decision trees with the tf.estimator API. \r\n\r\nSo, we can say that the Model used in the [paper](https://ieeexplore.ieee.org/abstract/document/8257910?casa_token=OcIH95osyb4AAAAA:OwLjp3Fs9BbWlYGx_mtC96HRjaDU2311UxfgEjpJCocDCKSbXUaKdOaQnEBwT9u8BfrZefYqNA) is same as in [the Tutorial](https://www.tensorflow.org/tutorials/estimator/boosted_trees) because both are related to **`Gradient Boosting Estimators`**.\r\n\r\nPlease refer [this code](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1096-L1405) for the Configuration of the Hyper-Parameters. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48154\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48154\">No</a>\n"]}, {"number": 48153, "title": "Using GridSearchCV in BoostedTreesClassifier", "body": "I'm trying to Implement [GridsearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) on [BoostedTreesClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933). Since [BoostedTreesClassifier](https://github.com/tensorflow/estimator/blob/781c0d30c6bf100aa174591dd97cb70fc39d294d/tensorflow_estimator/python/estimator/canned/boosted_trees.py#L1933) has not the fit method, is there any alternative to implement the GridsearchCV?", "comments": ["@samanemami,\r\nPlease check this [Stack Exchange Answer](https://stats.stackexchange.com/a/433880). [This article](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/) might also help you. Thanks!", "@rmothukuru \r\n`Estimators BoostedTreesClassifier`, instead of `fit`,  has a `train`, which is indicated in the [tutorial](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier) also. So, it is not possible to implement Sklearn gridsearch as indicated on the [page](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/), or this [link](https://medium.com/ml-book/neural-networks-hyperparameter-tuning-in-tensorflow-2-0-a7b4e2b574a1) you have sent.\r\nSo is there any other solution to add `fit` in order to use the above solutions, or any updates in `Estimators BoostedTreesClassifier`?", "I solved the issue by defining a new [estimator](https://github.com/samanemami/TFBoostedTree/blob/main/TFBT.py) which contains the `fit` and `score` method based on the Sklearn standard.\r\nThe [proposed](https://github.com/samanemami/TFBoostedTree/blob/main/TFBT.py) script is available on the following repository.\r\n[TFBoostedTree](https://github.com/samanemami/TFBoostedTree/tree/main)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48153\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48153\">No</a>\n"]}, {"number": 48152, "title": "TF2 getting tf.keras.metrics.AUC().result() greater than 1 on mirroredStrategy", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:  yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:source\r\n-   **TensorFlow version (use command below)**:2.3.0\r\n-   **Python version**:3.7\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**: 2 Tesla V100 GPU\r\n-   **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nHi. I find the tf.keras.metrics.AUC() is returning wrong values when using MirroredStrategy on 2 GPUs, while it works correctly on non-distributed settings. For the sake of data privacy I cannot provide my data or code to reproduce, but I believe it is the same problem described as in \r\nhttps://stackoverflow.com/questions/62405592/tensorflow-2-metrics-produce-wrong-results-with-2-gpus. \r\nwhich uses mnist data to reproduce. We both get an AUC value greater than 3, while other metrics look normal. \r\n\r\nThanks.", "comments": ["@rczj2102,\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue?\r\n\r\n\r\n\r\n> For the sake of data privacy I cannot provide my data or code to reproduce\r\n\r\nIn this case, could you please provide a dummy code snippet which can replicate the error in both distributed and non-distributed environments? \r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48152\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48152\">No</a>\n", "ValueError: Shapes (None, 2) and (None, 1) are incompatible\r\ntf2.4.0 \r\n\tMETRICS = [\r\n\t\ttf.keras.metrics.TruePositives(name='tp'),\r\n\t\ttf.keras.metrics.FalsePositives(name='fp'),\r\n\t\ttf.keras.metrics.TrueNegatives(name='tn'),\r\n\t\ttf.keras.metrics.FalseNegatives(name='fn'),\r\n\t\ttf.keras.metrics.BinaryAccuracy(name='accuracy'),\r\n\t\ttf.keras.metrics.Precision(name='precision'),\r\n\t\ttf.keras.metrics.Recall(name='recall'),\r\n\t\ttf.keras.metrics.AUC(name='auc'),\r\n\t]\r\n\r\n     if i use it like this   model.compile(optimizer='sgd',loss='mse',metrics=METRICS)\r\ni will get the error\r\nValueError: Shapes (None, 2) and (None, 1) are incompatible\r\nwhat can i do it please", "> @rczj2102, Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue?\r\n> \r\n> > For the sake of data privacy I cannot provide my data or code to reproduce\r\n> \r\n> In this case, could you please provide a dummy code snippet which can replicate the error in both distributed and non-distributed environments?\r\n> \r\n> Thanks!\r\n\r\nupdate the tf  to  2.4.1,  any other soft need update|?  the version need to match again?"]}, {"number": 48151, "title": "tf. feature_column.input_layer(dataVale, feature_embedding) sometimes gets out of order when processing feature_embedding lists", "body": "When bucketized_column is used to divide buckets and generate one-hot code, the running result is inconsistent with the actual input feature list.\r\n\r\nwindows10,tensorflow==1.13.1,python==3.7.7\r\n\r\n-----------------------------------------python-----------------------------------------------------------\r\nimport tensorflow as tf\r\nimport json\r\nimport pandas as pd\r\n\r\n\r\ndef get_feature_columns(tain_file):\r\n    data = pd.read_csv(tain_file, nrows=5).round(5)\r\n    feature_size = data.columns.shape[0] - 1\r\n    feature_columns = data.columns[:-1].tolist()\r\n    return feature_columns, feature_size\r\n\r\n\r\ndef read_row(csv_row):\r\n    record_defaults = [[0.]] * FEATURE_SIZE + [[0]]\r\n    row = tf.decode_csv(csv_row, record_defaults=record_defaults)\r\n    return dict(zip(FEATURE_COLUMNS, row[:-1])), row[-1]\r\n\r\n\r\ndef _feature_column(COLUMNS):\r\n    with open('./feature_fm_bucket.json', 'r') as f:\r\n        boundary_dict = json.load(f)\r\n    boundary_dict = {k: sorted(x for x in v if abs(x) != float(\"inf\")) for k, v in boundary_dict.items()}\r\n    length = 0\r\n    for kk in boundary_dict:\r\n        length += len(kk)\r\n\r\n    feature_embedding = []\r\n    for col in COLUMNS:\r\n        buket_col = tf.feature_column.bucketized_column(\r\n            tf.feature_column.numeric_column(col), boundary_dict[col])\r\n        feature_embedding.append(buket_col)\r\n\r\n    return feature_embedding, boundary_dict\r\n\r\n\r\nfeature_name = 'user_click_num_7d'\r\n# FEATURE_COLUMNS = [feature_name]\r\n\r\ntain_file = './demo.csv'\r\n\r\nFEATURE_COLUMNS, FEATURE_SIZE = get_feature_columns(tain_file)\r\n\r\nfeature_embedding, boundary_dict = _feature_column(FEATURE_COLUMNS)\r\n\r\nTRAIN_FILENAMES = [tain_file]\r\n\r\nbatch_size = 1\r\nfilenames = TRAIN_FILENAMES\r\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\r\ndataset = dataset.flat_map(lambda filename: tf.data.TextLineDataset(TRAIN_FILENAMES).skip(1))\r\ndataset = dataset.map(lambda line: read_row(line), num_parallel_calls=15,\r\n                      ).batch(batch_size).repeat(1)\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nds = iterator.get_next()\r\ndataVale = ds[0]\r\n\r\norder = tf.feature_column.input_layer(dataVale, feature_embedding)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(tf.local_variables_initializer())\r\n\r\nvalue = sess.run(order)\r\nfirst_line_value = value[0]\r\n\r\ncount = 0\r\nfor i, v1 in enumerate(FEATURE_COLUMNS):\r\n    print(\"line=\" + str(i + 1) + \", \", v1, end=', one_hot=[')\r\n    jj = 0\r\n    for j in range(len(boundary_dict[v1]) + 1):\r\n        vv = first_line_value[count]\r\n        count += 1\r\n        print(vv, end=',')\r\n        if vv == 1:\r\n            jj = j\r\n\r\n    print(end='], index=')\r\n    print(jj, end='')\r\n    print()\r\n-----------------------------------------------------python-------------------------------------------\r\n\r\n-----------------------------------------feature_fm_bucket.json----------------------------------------------------\r\n\r\n{\"mt_ses_pv_maxmin_30d\":[0.01,0.03,0.06,0.1,0.16,0.24,0.37,0.58],\"it_uv_15d\":[2,5,6,7,8,9,10,11,12],\"it_ck_pv_ws_3_30d\":[0.04,0.07,0.09,0.11,0.13,0.15,0.18,0.22,0.26,0.31,0.38,0.49,0.64,0.8,0.92,0.96,0.98],\"it_ck_pv_ws_3_15d\":[0.06,0.1,0.13,0.16,0.19,0.22,0.25,0.28,0.32,0.37,0.43,0.53,0.65,0.8,0.92,0.96,0.98],\"rt_it_uv_3h\":[1,2,3,4,5,6,7],\"it_lk_uv_7\":[0.001,0.002,0.003,0.005,0.008,0.01],\"it_lk_uv_30d\":[0.0005,0.001,0.003],\"it_collect_uv_15d\":[0.001,0.002,0.003,0.006],\"rt_it_fp_ws_st_ro_1_3h\":[0.01,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.17,0.19],\"it_st_pv_ws_1_3d\":[0.27,0.34,0.38,0.41,0.44,0.46,0.48,0.5,0.52,0.54,0.55,0.58,0.6,0.64,0.68,0.75,0.83,0.93],\"mt_ses_dur_weight_30d\":[0.01,0.02,0.04,0.06,0.1,0.2],\"rt_ur_tr\":[0.01,0.02,0.03,0.04,0.05,0.07],\"it_ck_pv_ws_1_15d\":[0.02,0.04,0.06,0.07,0.09,0.1,0.11,0.13,0.15,0.17,0.2,0.24,0.3,0.37,0.47,0.6,0.75,0.88,0.96],\"mt_nlp_tag_max\":[0.02,0.04,0.05,0.07,0.08,0.1,0.12,0.15,0.17,0.19,0.21,0.24,0.28,0.33,0.41],\"it_fp_ck_pv_15d\":[5,6,7,8,9,10,11,12],\"ur_ck_num_7\":[0.7,1,2,2.5,3,3.5,4,4.7],\"it_prop_ar_favorites_num\":[2,3,4],\"rt_it_fp_ws_st_ro_3h\":[0.06,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.19,0.2,0.23,0.25,0.29,0.32,0.38],\"it_prop_ar_articlenum\":[2,3,4],\"it_lk_uv_15d\":[0.001,0.003,0.005,0.007,0.1,0.2],\"rt_mt_it_dgc\":[0.57,0.7,0.78,0.83,0.87,0.9,0.92,0.96,0.98],\"it_fp_st_pv_7\":[7,8,9,10,11,12,13],\"it_fp_ck_pv_3d\":[4,5,6,7,8,9,10,11],\"it_lk_uv_1\":[0.0001,0.0002,0.0003,0.0005,0.001,0.003,0.01],\"rt_ur_st_show_num\":[1,2,3,4,5],\"rt_it_pv\":[3,4,5,6,7,8],\"it_ck_pv_ws_1_30d\":[0.01,0.03,0.04,0.05,0.06,0.07,0.08,0.1,0.12,0.14,0.17,0.21,0.27,0.35,0.46,0.6,0.75,0.88,0.96],\"mt_ses_pv_weight_7\":[0.01,0.02,0.04,0.07,0.13,0.26],\"it_fp_st_pv_30d\":[7,8,9,10,11,12,13,14],\"it_prop_ar_articlenum_4\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1],\"it_prop_ar_articlenum_5\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\"it_prop_ar_articlenum_2\":[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1],\"it_prop_ar_articlenum_3\":[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1],\"it_fp_ck_pv_30d\":[5,6,7,8,9,10,11,12],\"rt_it_fp_ck_pv\":[3,4,5,6,7,8],\"it_rp_uv_3d\":[0.001,0.002,0.003,0.004,0.01],\"rt_it_fp_ws_ck_ro_3h\":[0.01,0.03,0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.15,0.16,0.18,0.21,0.25,0.3],\"rt_it_uv\":[3,4,5,6,7,8],\"rt_mt_ses_wsim_top5\":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],\"it_quality_effect_score\":[0.8,0.88,0.93,0.96,0.97,0.98,0.99],\"rt_mt_ses_wsim_top1\":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],\"rt_mt_ses_wsim_top2\":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],\"ur_dr_30d\":[5,6,7,8],\"it_fp_ag_pv_tr_3d\":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.2],\"it_ck_pv_ws_1_3d\":[0.19,0.26,0.3,0.34,0.37,0.39,0.42,0.44,0.46,0.48,0.51,0.53,0.56,0.59,0.64,0.7,0.79,0.89,0.97],\"mt_ses_dur_weight_15d\":[0.01,0.03,0.04,0.07,0.11,0.22],\"mt_ses_lst\":[1,2],\"it_fp_ws_pv_tr_30d\":[0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],\"it_uv_30d\":[1,5,6,7,8,9,10,11,12],\"it_fp_ag_pv_tr_15d\":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.2],\"mt_ses_ss_7\":[1,2,3,5,8],\"it_cb_ro_7\":[1.12,1.2,1.23,1.27,1.3,1.32,1.34,1.36,1.39,1.41,1.44,1.48,1.51,1.57,1.65,1.82],\"rt_it_fp_ws_tr\":[0.02,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15],\"rt_it_fp_ws_st_ro_1h\":[0.01,0.02,0.03,0.04,0.06],\"it_quality_rare_score\":[0.24,0.38,0.48,0.57,0.63,0.73,0.76,0.79,0.81,0.83,0.86,0.88,0.89,0.91,0.95,0.98,0.99],\"it_collect_uv_7\":[0.001,0.002,0.004],\"mt_ses_dur_15d\":[2,3,4,5,6,7],\"it_st_pv_ws_3_7\":[0.25,0.33,0.37,0.41,0.44,0.46,0.49,0.52,0.55,0.59,0.63,0.7,0.79,0.89,0.97],\"it_rp_uv_30d\":[0.001,0.002,0.003,0.005,0.011],\"mt_ses_dur_30d\":[1,2,3,4,5,6,7],\"rt_it_fp_ws_ck_ro_1_3h\":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.14],\"it_fp_ck_pv_1\":[3,4,5,6,7,8,9,10],\"it_fp_ck_pv_7\":[4,5,6,7,8,9,10,11],\"rt_it_fp_ck_pv_1h\":[1,2,3,4,5],\"rt_it_fp_ws_tr_1h\":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.11],\"rt_it_pv_1h\":[1,2,3,4,5],\"ur_ck_num_30d\":[1,2,3,4,5,6],\"mt_ses_ss_90d\":[1,2,3,4,5,9,12],\"it_quality_cover_score\":[0.42,0.52,0.53,0.54,0.55,0.56,0.6,0.65],\"it_fp_ag_pv_tr_30d\":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.19],\"context_event_hour\":[8,12,14,18,21],\"it_uv_1\":[3,4,5,6,7,8,9,10],\"rt_mt_ses_maxmin\":[0],\"it_uv_7\":[1,5,6,7,8,9,10,11,12],\"it_rp_uv_15d\":[0.001,0.002,0.003,0.005,0.009],\"it_rp_uv_1\":[0.0005,0.001,0.003],\"mt_it_lst_type\":[1,2,4,6],\"it_st_pv_ws_1_7\":[0.09,0.14,0.16,0.19,0.21,0.22,0.24,0.26,0.28,0.3,0.33,0.37,0.41,0.46,0.56,0.67,0.8,0.91],\"it_fp_ws_pv_tr_1\":[0.03,0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17],\"it_fp_ag_pv_tr_1\":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.2],\"rt_mt_ses_sim_lst4\":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],\"rt_mt_ses_sim_lst5\":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],\"it_collect_uv_1\":[0.001,0.002],\"rt_it_pv_3h\":[1,2,3,4,5,6,7],\"rt_mt_ses_sim_lst1\":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],\"rt_mt_ses_sim_lst2\":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],\"rt_mt_ses_sim_lst3\":[0.025,0.03,0.04,0.05,0.07,0.09,0.2],\"it_prop_ar_comment_num\":[2,3,4],\"it_lk_uv_3d\":[0.0002,0.0003,0.0006,0.002],\"ur_dr_7\":[5,6,7,8],\"rt_mt_ses_wsim_top4\":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],\"it_st_pv_ws_1_15d\":[0.04,0.06,0.08,0.09,0.11,0.12,0.14,0.15,0.17,0.2,0.23,0.27,0.33,0.41,0.52,0.66,0.79,0.91],\"it_cb_ro_1\":[1.11,1.19,1.23,1.26,1.29,1.31,1.34,1.36,1.39,1.41,1.45,1.48,1.52,1.58,1.67,1.86],\"it_ck_pv_ws_3_7\":[0.19,0.26,0.31,0.35,0.39,0.42,0.44,0.47,0.5,0.54,0.58,0.65,0.73,0.83,0.92,0.96,0.98],\"mt_ses_ss_60d\":[1,2,3,4,5,9,17],\"mt_nlp_sec_tag_avg\":[0.03,0.05,0.07,0.09,0.11,0.12,0.14,0.18,0.23,0.33],\"rt_ur_ck_num\":[1,1.4,1.9,2,2.4,2.7,3],\"ur_dr_15d\":[5,6,7,8],\"rt_it_uv_1h\":[1,2,3,4,5],\"it_st_pv_ws_3_30d\":[0.05,0.08,0.11,0.13,0.15,0.18,0.21,0.25,0.29,0.35,0.43,0.54,0.69,0.88,0.97],\"mt_ses_dur_maxmin_15d\":[0.02,0.04,0.08,0.13,0.21,0.34,0.55],\"it_fp_ws_pv_tr_15d\":[0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],\"it_fp_ag_pv_tr_7\":[0.05,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.2],\"rt_it_fp_ws_tr_3h\":[0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.14],\"it_fp_st_pv_3d\":[6,7,8,9,10,11,12,13],\"rt_it_fp_ck_pv_3h\":[1,2,3,4,5,6,7],\"rt_mt_ses_wsim_top3\":[0.001,0.002,0.003,0.005,0.006,0.008,0.01,0.015,0.02],\"ur_ck_num_15d\":[1,2,3,4,5],\"it_prop_time_decay\":[0.12,0.15,0.2,0.24,0.27,0.31,0.36,0.41,0.46,0.5,0.57,0.61,0.65,0.69,0.74,0.77,0.81,0.84],\"it_prop_ar_fellowsnum\":[2,3,4,5],\"it_prop_ar_lk_num\":[2,3,4],\"mt_ses_pref\":[0.01,0.03,0.05,0.07,0.11,0.17,0.29],\"it_uv_3d\":[1,4,5,6,7,8,9,10,11],\"rt_mt_it_weights\":[0.06,0.16,0.25,0.33,0.37,0.47,0.5,0.6,0.7],\"it_fp_ws_pv_tr_7\":[0.04,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],\"mt_ses_pv_maxmin_7\":[0.02,0.05,0.09,0.18,0.32,0.56],\"it_ck_pv_ws_1_7\":[0.06,0.1,0.12,0.15,0.17,0.19,0.2,0.22,0.24,0.27,0.3,0.33,0.37,0.42,0.51,0.62,0.76,0.88,0.96],\"it_rp_uv_7\":[0.001,0.002,0.003,0.007],\"it_cb_ro_15d\":[1.13,1.2,1.23,1.27,1.3,1.32,1.34,1.36,1.39,1.41,1.44,1.47,1.5,1.55,1.62,1.81],\"it_fp_ws_pv_tr_3d\":[0.03,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.18],\"mt_ses_pv_15d\":[1,2,3,4,5],\"it_cb_ro_30d\":[1.13,1.2,1.24,1.27,1.3,1.32,1.33,1.35,1.38,1.4,1.43,1.46,1.5,1.55,1.61,1.78],\"mt_ses_pv_weight_30d\":[0.01,0.02,0.04,0.06,0.09,0.18],\"mt_nlp_tag_num\":[1,2,3,6],\"mt_ses_ss_15d\":[1],\"rt_it_fp_ws_ck_ro_1h\":[0.0003,0.0008,0.01,0.02,0.03,0.04],\"mt_ses_dur_7\":[1,3,4,5,6],\"mt_ses_pv_maxmin_15d\":[0.02,0.04,0.08,0.13,0.22,0.35,0.57],\"it_cb_ro_3d\":[1.12,1.2,1.23,1.27,1.3,1.32,1.35,1.37,1.4,1.42,1.45,1.49,1.53,1.58,1.65,1.87],\"it_quality_time_score\":[0.12,0.27,0.39,0.45,0.49,0.54,0.58,0.61,0.65,0.68,0.71,0.74,0.76,0.77,0.78,0.9,0.96],\"it_collect_uv_3d\":[0.001,0.002,0.003],\"it_st_pv_ws_1_30d\":[0.02,0.04,0.05,0.06,0.08,0.09,0.1,0.12,0.14,0.17,0.2,0.24,0.31,0.4,0.51,0.66,0.79,0.91],\"mt_nlp_tag_avg\":[0.02,0.04,0.05,0.06,0.07,0.09,0.1,0.12,0.14,0.16,0.19,0.22,0.26,0.31,0.39],\"it_collect_uv_30d\":[0.001,0.002,0.003,0.004,0.007],\"it_prop_publishtime\":[347,422,504,604,722,844,976,1104,1376,1563,1769,2035,2340,2606,2808,3158,3751,4188,16536],\"mt_ses_dur_maxmin_30d\":[0.01,0.03,0.05,0.09,0.14,0.22,0.34,0.54],\"mt_nlp_tag_sum\":[0.03,0.05,0.06,0.08,0.11,0.13,0.16,0.18,0.21,0.24,0.28,0.33,0.39,0.49,0.72],\"mt_ses_pv_30d\":[1,2,3,4,5],\"it_prop_ar_share_cnt\":[2,3,4],\"it_fp_st_pv_1\":[6,7,8,9,10,11,12],\"mt_ses_ss_30d\":[1,2,3,4,5,7,14],\"mt_nlp_sec_tag_sum\":[0.03,0.06,0.09,0.11,0.14,0.18,0.22,0.28,0.36,0.49],\"rt_mt_ses_sim_top4\":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],\"rt_mt_ses_sim_top5\":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],\"mt_ses_pv_7\":[1,2,3,4],\"mt_ses_pv_weight_15d\":[0.01,0.02,0.04,0.06,0.11,0.21],\"rt_mt_ses_sim_top1\":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],\"rt_mt_ses_sim_top2\":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],\"rt_mt_ses_sim_top3\":[0.025,0.029,0.034,0.041,0.049,0.058,0.07,0.086,0.115],\"mt_nlp_sec_tag_max\":[0.03,0.06,0.08,0.1,0.12,0.14,0.17,0.21,0.25,0.34],\"it_st_pv_ws_3_15d\":[0.09,0.13,0.16,0.19,0.22,0.25,0.28,0.31,0.36,0.41,0.47,0.58,0.71,0.88,0.97],\"it_fp_st_pv_15d\":[7,8,9,10,11,12,13,14],\"rt_mt_it_rank\":[0.5,0.72,0.85],\"mt_ses_dur_weight_7\":[0.01,0.02,0.04,0.08,0.13,0.28],\"mt_ses_dur_maxmin_7\":[0.02,0.05,0.1,0.18,0.32,0.55]}\r\n\r\n-----------------------------------------feature_fm_bucket.json----------------------------------------------------\r\n\r\n\r\n-----------------------------------------demo.csv----------------------------------------------------\r\n\r\ncontext_event_hour,it_cb_ro_15d,it_cb_ro_1,it_cb_ro_30d,it_cb_ro_3d,it_cb_ro_7,it_ck_pv_ws_1_15d,it_ck_pv_ws_1_30d,it_ck_pv_ws_1_3d,it_ck_pv_ws_1_7,it_ck_pv_ws_3_15d,it_ck_pv_ws_3_30d,it_ck_pv_ws_3_7,it_collect_uv_15d,it_collect_uv_1,it_collect_uv_30d,it_collect_uv_3d,it_collect_uv_7,it_fp_ag_pv_tr_15d,it_fp_ag_pv_tr_1,it_fp_ag_pv_tr_30d,it_fp_ag_pv_tr_3d,it_fp_ag_pv_tr_7,it_fp_ck_pv_15d,it_fp_ck_pv_1,it_fp_ck_pv_30d,it_fp_ck_pv_3d,it_fp_ck_pv_7,it_fp_st_pv_15d,it_fp_st_pv_1,it_fp_st_pv_30d,it_fp_st_pv_3d,it_fp_st_pv_7,it_fp_ws_pv_tr_15d,it_fp_ws_pv_tr_1,it_fp_ws_pv_tr_30d,it_fp_ws_pv_tr_3d,it_fp_ws_pv_tr_7,it_lk_uv_15d,it_lk_uv_1,it_lk_uv_30d,it_lk_uv_3d,it_lk_uv_7,it_prop_ar_articlenum,it_prop_ar_comment_num,it_prop_ar_favorites_num,it_prop_ar_fellowsnum,it_prop_ar_lk_num,it_prop_ar_share_cnt,it_prop_publishtime,it_prop_time_decay,it_quality_cover_score,it_quality_effect_score,it_quality_rare_score,it_quality_time_score,it_rp_uv_15d,it_rp_uv_1,it_rp_uv_30d,it_rp_uv_3d,it_rp_uv_7,it_st_pv_ws_1_15d,it_st_pv_ws_1_30d,it_st_pv_ws_1_3d,it_st_pv_ws_1_7,it_st_pv_ws_3_15d,it_st_pv_ws_3_30d,it_st_pv_ws_3_7,it_uv_15d,it_uv_1,it_uv_30d,it_uv_3d,it_uv_7,mt_nlp_sec_tag_avg,mt_nlp_sec_tag_max,mt_nlp_sec_tag_sum,mt_nlp_tag_avg,mt_nlp_tag_max,mt_nlp_tag_num,mt_nlp_tag_sum,mt_ses_ss_15d,mt_ses_ss_30d,mt_ses_ss_60d,mt_ses_ss_7,mt_ses_ss_90d,mt_it_lst_type,mt_ses_dur_15d,mt_ses_dur_30d,mt_ses_dur_7,mt_ses_dur_maxmin_15d,mt_ses_dur_maxmin_30d,mt_ses_dur_maxmin_7,mt_ses_dur_weight_15d,mt_ses_dur_weight_30d,mt_ses_dur_weight_7,mt_ses_lst,mt_ses_pref,mt_ses_pv_15d,mt_ses_pv_30d,mt_ses_pv_7,mt_ses_pv_maxmin_15d,mt_ses_pv_maxmin_30d,mt_ses_pv_maxmin_7,mt_ses_pv_weight_15d,mt_ses_pv_weight_30d,mt_ses_pv_weight_7,ur_ck_num_15d,ur_ck_num_30d,ur_ck_num_7,ur_dr_15d,ur_dr_30d,ur_dr_7,rt_it_fp_ck_pv,rt_it_fp_ck_pv_1h,rt_it_fp_ck_pv_3h,rt_it_fp_ws_ck_ro_1_3h,rt_it_fp_ws_ck_ro_1h,rt_it_fp_ws_ck_ro_3h,rt_it_fp_ws_tr,rt_it_fp_ws_tr_1h,rt_it_fp_ws_tr_3h,rt_it_fp_ws_st_ro_1_3h,rt_it_fp_ws_st_ro_1h,rt_it_fp_ws_st_ro_3h,rt_it_pv,rt_it_pv_1h,rt_it_pv_3h,rt_it_uv,rt_it_uv_1h,rt_it_uv_3h,rt_mt_it_dgc,rt_mt_it_rank,rt_mt_it_weights,rt_mt_ses_maxmin,rt_mt_ses_sim_lst1,rt_mt_ses_sim_lst2,rt_mt_ses_sim_lst3,rt_mt_ses_sim_lst4,rt_mt_ses_sim_lst5,rt_mt_ses_sim_top1,rt_mt_ses_sim_top2,rt_mt_ses_sim_top3,rt_mt_ses_sim_top4,rt_mt_ses_sim_top5,rt_mt_ses_wsim_top1,rt_mt_ses_wsim_top2,rt_mt_ses_wsim_top3,rt_mt_ses_wsim_top4,rt_mt_ses_wsim_top5,rt_ur_ck_num,rt_ur_tr,rt_ur_st_show_num,label\r\n11,1.398,1.374,1.398,1.406,1.404,0.08342,0.08342,0.45096,0.17688,0.17996,0.17996,0.38242,0.0038,0.00282,0.0038,0.00282,0.00322,0.16433,0.15416,0.16416,0.15696,0.16032,11.03429,8.61505,11.03429,9.36134,10.28606,12.83994,10.48414,12.83994,11.21267,12.11637,0.16137,0.14493,0.16137,0.15045,0.15611,0.00209,0.00019,0.00209,0.00015,0.00036,2.0,4.0,4.0,3.0,3.0,4.0,1666,0.435,0.564,0.99997,0.917776,0.63613,0.00308,0.00172,0.00308,0.00168,0.00241,0.09246,0.09246,0.47343,0.19089,0.19325,0.19325,0.39934,11.189,8.722,11.189,9.429,10.335,0.16,0.16,0.16,0.186,0.186,1,0.186,0.0,0.0,0.0,0.0,0.0,1.0,5.784,5.784,3.401,0.313,0.239,0.049,0.031,0.02,0.007,0.0,0.047,4.159,4.159,0.693,0.13,0.1,0.004,0.014,0.011,0.001,4.92,5.308,4.234,7.257,7.105,6.953,6.863,4.174,5.943,0.09343,0.03659,0.32193,0.11717,0.06461,0.10553,0.15844,0.06329,0.37468,6.828,4.111,5.916,6.783,4.007,5.858,0.889,1.0,0.667,0.0,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.024,0.0,0.0,0.0,0.0,0.0,1.946,0.02483,3.714,1\r\n\r\n-----------------------------------------demo.csv----------------------------------------------------\r\n\r\nThe running results are as follows:\r\nIn column 108, ur_ck_num_7sub bucket value:\r\n[0.7,1,2,2.5,3,3.5,4,4.7]\r\n\r\nThe ur_ck_num_7value value in column 108 is initial 4.4.\r\nline=108,  ur_ck_num_7, one_hot=[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,], index=5;\r\nline=149,  rt_ur_ck_num, one_hot=[0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,], index=4\r\n\r\nThe ur_ck_num_7 value in column 108 is changed to 114.234.\r\nline=108,  ur_ck_num_7, one_hot=[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,], index=5\uff1aThe data has not changed.;\r\nline=149,  rt_ur_ck_num, one_hot=[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,], index=5\uff1aThe data changes.\r\n\r\nWe expected line 108 to change, but it was line 149 where the data changed.", "comments": ["@CaptainDP \r\nWe see that you are using 1.13 version of tensorflow, as 1.x is no more supported please upgrade to 2.x [2.4 is the stable version] and let us know if you face any issues.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48151\">No</a>\n"]}, {"number": 48150, "title": "Add an example to test a single component", "body": "", "comments": []}, {"number": 48148, "title": "`TFLiteConverter` breaks `SignatureDefs` when used with full integer quantization", "body": "**System information**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installation (pip package or built from source): binary \r\n- TensorFlow library (version, if pip package or github SHA, if built from source): `2.5.0-dev20210319` and `2.6.0-dev20210329`\r\n\r\n**Describe the current behavior**\r\n\r\nConverting `SignatureDefs` together with full int8 quantization by setting\r\n```python\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n```\r\nbreaks the signature runner since the signature still refers to tensors that [have been removed when converting to full int8 quantization](https://github.com/tensorflow/tensorflow/blob/305926798c36e2babfd31395317db33c55490d5b/tensorflow/lite/python/lite.py#L745).\r\n\r\nPlease see [this notebook](https://colab.research.google.com/drive/1stwy8M-tLDBCPW8_BRXpBs5ZOzlm94Jz?usp=sharing) for the full code to reproduce the problem.\r\n\r\nFor more background on why this is problematic for multi-output models please checkout #47927\r\n", "comments": ["Thanks again @lgeiger for reporting this!", "Thanks @lgeiger for reporting this. We will work on a fix.\r\n\r\nThanks", "This has been fixed in a0e5ec8cb940fbea193706f0c6609be6dad5a961 by @MeghnaNatraj so the issue doesn't exist in tf-nightly anymore.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48148\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48148\">No</a>\n"]}, {"number": 48146, "title": "GPU Profiling: MemoryProfile do not contain memory events.", "body": "**System information**\r\n- Have I written custom code in: C++\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 2.3.2\r\n- Python version: 3.6\r\n- Bazel version: 3.1.0\r\n- GCC/Compiler version: g++ (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2 /8\r\n- GPU model and memory: GeForce GTX 1060 6GB \r\n\r\n**Describe the current behavior**\r\n\r\nI'm trying to use the Tensorflow C++ profiling lib to extract the memory footprint of one network on my GPU.\r\nHere is the code I use to trace my inference and extract the memory profile:\r\n```\r\nauto tracer = CreateGpuTracer();\r\ntracer->Start(); \r\n\r\n// Inference\r\ntensorflow::RunOptions run_options;\r\nrun_options.set_trace_level(tensorflow::RunOptions::FULL_TRACE);\r\ntensorflow::RunMetadata run_metadata;\r\nrun_status = session->Run(run_options, input_tf_tensors, output_names, {}, &tf_outputs, &run_metadata);\r\ntracer->Stop();\r\n\r\nXSpace xspace;\r\nstatus = tracer->CollectData(&xspace);\r\n\r\nstd::vector<const XPlane*> device_planes_gpu = FindPlanesWithPrefix(xspace, \"/device:GPU:\");\r\nconst XPlane* plane_gpu = device_planes_gpu[0];\r\nMemoryProfile memory_profile = ConvertXPlaneToMemoryProfile(*plane_gpu);\r\n\r\nstd::string json_output;\r\ngoogle::protobuf::util::MessageToJsonString(memory_profile, &json_output);\r\nLOG(INFO) << json_output;\r\n```\r\n\r\nHere the TF logs that shows how the GPU is found, Cuda libs are found and that some events are collected:\r\n```\r\n    2021-03-26 15:28:19.841126: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs\r\n    2021-03-26 15:28:19.841822: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcupti.so.10.2\r\n    2021-03-26 15:28:20.823429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n    2021-03-26 15:24:34.402625: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1513] CUPTI activity buffer flushed\r\n    2021-03-26 15:24:34.402648: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 562 callback api events and 562 activity events. \r\n```\r\n \r\nUnfortunately no event either HostEventType::kMemoryDeallocation or HostEventType::kMemoryAllocation is collected in the XSpace and consequently the MemoryProfile is empty. The resulting memory summary is:\r\n```\r\n    {\"memoryProfilePerAllocator\":{},\"numHosts\":1,\"memoryIds\":[]}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect that the MemoryProfile contains at least one memoryProfilePerAllocator containing one MemoryProfileSummary.\r\n\r\nFrom this object I should be able to get the information to print the Memory profile summary shown in the profiler guide:\r\nhttps://www.tensorflow.org/guide/profiler#memory_profile_tool\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\nMy inference works as expected.\r\n", "comments": ["Hello do you have any update about this issue?", "@jbaiocchi knows this better.", "Please note that there's no official C++ API for programmatic profiling in TF2.\r\n\r\nYour code is enabling TF1 profiling capabilities in session->Run() by setting a trace level in RunOptions passed. Don't do that to avoid conflicts with the TF2 profiler.\r\n\r\nDo not use CreateGpuTracer() as it is not a public API. It is a factory function for instantiating the GPU tracer, which is a plugin to ProfilerSession. Using the factory function directly leads to undefined / untested behavior. Furthermore, that's not the part of the profiler that creates a memory profile. Likewise, the functions for processing xplane are meant for internal use of the profiler.\r\n\r\nYou might try something like the following instead:\r\n\r\n```\r\nauto tracer = tensorflow::ProfilerSession::Create(ProfilerSession::DefaultOptions());\r\n\r\ntensorflow::RunOptions run_options;\r\ntensorflow::RunMetadata run_metadata;\r\nrun_status = session->Run(run_options, input_tf_tensors, output_names, {}, &tf_outputs, &run_metadata);\r\n\r\nXSpace xspace;\r\nstatus = tracer->CollectData(&xspace);\r\n\r\nconst XPlane* host_plane = FindPlaneWithName(xspace, \"/host:CPU\");\r\nMemoryProfile memory_profile = ConvertXPlaneToMemoryProfile(*host_plane);\r\n\r\nstd::string json_output;\r\ngoogle::protobuf::util::MessageToJsonString(memory_profile, &json_output);\r\nLOG(INFO) << json_output;\r\n```\r\n\r\nThe code above will only work if profiling a local GPU. It will not work if session is dispatching to a remote worker.\r\nI re-iterate that none of these are officially supported C++ APIs and we are likely to break them in the future.", "@jbaiocchi  \r\nMany thanks for your reply! \r\n\r\nI had the feeling that I was not using something meant to be used officially, but I looked for a C++ profiling API everywhere and I did not found any. I'm open to any suggestion on how to profile my GPU programmatically in C++ with something more suitable. \r\n\r\nThanks for the suggestion of using the ProfilerSession, it cleaned my code a bit, but  the issue is still there. `memory_profile.memory_profile_per_allocator()` is empty and I don't have any \r\nevent_type == HostEventType::kMemoryDeallocation or event_type == HostEventType::kMemoryAllocation in the plane_gpu.\r\n\r\nHere few logs: \r\n```\r\n2021-04-21 09:24:17.576700: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\r\n2021-04-21 09:24:17.576747: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs\r\n2021-04-21 09:24:17.577510: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcupti.so.10.2\r\n2021-04-21 09:24:18.651453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2021-04-21 09:24:19.245044: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-04-21 09:24:20.036518: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:794] Collecting 4909 kernel records, 427 memcpy records.\r\n2021-04-21 09:24:20.051242: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 5336 callback api events and 5336 activity events. \r\n```\r\n \r\nCould it be an issue of TF 2.3.2 ?\r\n\r\nThanks again\r\n", "The memory allocation events are not generated by CUPTI but by BFCAllocator::AddTraceMe. Is that code executed?\r\nhttps://github.com/tensorflow/tensorflow/blob/ccee426384468b152aba22e1a9f9a3fd2f92bf00/tensorflow/core/common_runtime/bfc_allocator.cc", "Yes it's executed.\r\n\r\nHere some logs in a different run.\r\n```\r\n2021-04-21 08:07:35.731454: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.75GiB with freed_by_count=0. The caller indicates that this\r\nis not a failure, but may mean that there could be performance gains if more memory were available.                                                                                                     \r\n2021-04-21 08:07:35.731608: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\r\n2021-04-21 08:07:35.977655: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.21GiB with freed_by_count=0. The caller indicates that this\r\nis not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-04-21 08:07:36.056622: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 626.31MiB with freed_by_count=0. The caller indicates that thi\r\ns is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-04-21 08:07:36.174983: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 566.38MiB with freed_by_count=0. The caller indicates that thi\r\ns is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2021-04-21 08:07:36.192989: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 566.38MiB with freed_by_count=0. The caller indicates that thi\r\ns is not a failure, but may mean that there could be performance gains if more memory were available.       \r\n2021-04-21 08:07:36.219557: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB with freed_by_count=0. The caller indicates that this\r\nis not a failure, but may mean that there could be performance gains if more memory were available.         \r\n2021-04-21 08:07:36.242641: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.15GiB with freed_by_count=0. The caller indicates that this\r\nis not a failure, but may mean that there could be performance gains if more memory were available.     \r\n2021-04-21 08:07:36.272578: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this\r\nis not a failure, but may mean that there could be performance gains if more memory were available.       \r\n2021-04-21 08:07:36.296201: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this\r\nis not a failure, but may mean that there could be performance gains if more memory were available.     \r\n2021-04-21 08:07:36.301138: W tensorflow/core/common_runtime/bfc_allocator.cc:246] Allocator (GPU_0_bfc) ran out of memory trying to allocate 428.55MiB with freed_by_count=0. The caller indicates that thi\r\ns is not a failure, but may mean that there could be performance gains if more memory were available.   \r\n``` ", "I think the other bug in your code is that you are looking for the events in the GPU side instead of the host side, try:\r\n```\r\nconst XPlane* host_plane = FindPlaneWithName(xspace, \"/host:CPU\");\r\nMemoryProfile memory_profile = ConvertXPlaneToMemoryProfile(*host_plane);\r\n```\r\nIt would be better to directly call ConvertXSpaceToMemoryProfileJson:\r\nhttps://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/profiler/convert/xplane_to_memory_profile.cc", "> ```\r\n> const XPlane* host_plane = FindPlaneWithName(xspace, \"/host:CPU\");\r\n> MemoryProfile memory_profile = ConvertXPlaneToMemoryProfile(*host_plane);\r\n> ```\r\nTried this, unfortunately it does not work, I have the same result.\r\n\r\n> It would be better to directly call ConvertXSpaceToMemoryProfileJson:\r\n> https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/profiler/convert/xplane_to_memory_profile.cc\r\n\r\nI dont think I can because it was introduced in a later version of TF, I use 2.3.2.", "The memory profiler was added in 2.3 so you should have the necessary pieces.\r\nMore debugging tips:\r\n- Make sure you are not disabling TraceMe (host_tracer_level in ProfilerOptions should be at least 2)\r\n- Make sure the TraceMe's in BFCAllocator::AddTraceMe are actually executed while ProfilerSession is active.\r\n- Debug ConvertXPlaneToMemoryProfile to see why it doesn't find the events (are the events missing or skipped)?\r\n- Take a look at the commit history and see if any bug fixes might apply:   https://github.com/tensorflow/tensorflow/commits/a0f8ebc1cbe599b32461c77b6a538985f00315a2/tensorflow/core/profiler/convert/xplane_to_memory_profile.cc\r\n  In particular:\r\nhttps://github.com/tensorflow/tensorflow/commit/36d55f1c562e89d0e1eddc0a7d4d79049cabb466#diff-7b16b05d8b0b859750fec0ff5a9aeaa2fba390bfa858421152202ed15eadafd9", "@ibreschi Could you please try as per the above comment  and let us know if this issue still persists ? Thanks!", "@sushreebarsa  I did not managed to fix the issue using TF2.3.\r\nI'm trying to compile using 2.4 witch has the commit mentioned above, I'll let you know as soon as I managed to do it. ", "@ibreschi Could you please let us know if you have tried as per the [comment](https://github.com/tensorflow/tensorflow/issues/48146#issuecomment-824224482) using latest stable version of TF 2.6.0?Please let us know if the issue still persists ? Please have a look at the[ link ](https://github.com/tensorflow/tensorflow/commits/a0f8ebc1cbe599b32461c77b6a538985f00315a2/tensorflow/core/profiler/convert/xplane_to_memory_profile.cc), [link1](https://github.com/tensorflow/tensorflow/commit/36d55f1c562e89d0e1eddc0a7d4d79049cabb466#diff-7b16b05d8b0b859750fec0ff5a9aeaa2fba390bfa858421152202ed15eadafd9) and let us know if it helps?Thanks!  ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48146\">No</a>\n"]}]