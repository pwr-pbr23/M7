[{"number": 46195, "title": "micro: Add Sony Spresense board target", "body": "Add build terget on Sony Spresense board.\r\nTo build it, Spresense SDK is required.\r\nAnd hello_world, micro_speech and preson_detection are added for the board.\r\n\r\nThe related issue is #46240 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@takayoshi-k Can you please check @petewarden's comments and keep us posted ? Thanks!", "@gbaned @petewarden \r\n\r\nYes. I have checked the comment and thank you for it.\r\nI am working on it now. And I will force push it my branch in this week.\r\n\r\nThank you.", "Hi @petewarden \r\n\r\nSorry for late, but I add some comments and README.md in each examples as your change request.\r\nPlease review it and let me know if you have additional requests.", "Hi @petewarden \r\n\r\nThank you for execute a test_code_style.sh on your CI.\r\n\r\nI updated for something the tool pointed out.\r\nPlease check it.", "Hi,\r\nBecause of additional style error again, I changed the code to fit the style checker.\r\n", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 46194, "title": "compute gradient error:  'KerasTensor' object has no attribute '_id',   (tensorflow 2.4.0)", "body": "Hi, i'm using tensorflow 2.4.0 ,  and want to compute gradients with keras tensor,  but failed.\r\n\r\nI'm using python3.8\r\n\r\n**Below are the log info:**\r\n\r\nTraceback (most recent call last):\r\n  File \"demo-resnet-18-v4.py\", line 119, in <module>\r\n    cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, \"activation_16\")\r\n  File \"demo-resnet-18-v4.py\", line 65, in grad_cam\r\n    grads = gtape.gradient(loss, var_list)\r\n  File \"/Users/baonansen/miniconda3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\", line 1080, in gradient\r\n    flat_grad = imperative_grad.imperative_grad(\r\n  File \"/Users/baonansen/miniconda3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\", line 79, in imperative_grad\r\n    return pywrap_tfe.TFE_Py_TapeGradient(\r\nAttributeError: 'KerasTensor' object has no attribute '_id'\r\n\r\n**below are the code:**\r\n\r\n 53 def grad_cam(input_model, image, category_index, layer_name):\r\n 54     with tf.GradientTape() as gtape:\r\n 55         nb_classes = 53\r\n 56         target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\r\n 57         x = Lambda(target_layer, output_shape = target_category_loss_output_shape)(input_model.output)\r\n 58         model = Model(inputs=input_model.input, outputs=x)\r\n 59         model.summary()\r\n 60\r\n 61         loss = K.sum(model.output)\r\n 62         conv_output =  [l for l in model.layers if l.name == layer_name][0].output\r\n 63\r\n 64         var_list = [conv_output]\r\n 65     grads = gtape.gradient(loss, var_list)\r\n\r\n", "comments": ["refine the code format:\r\n\r\n\r\ndef grad_cam(input_model, image, category_index, layer_name):\r\n    \r\n    with tf.GradientTape() as gtape:\r\n        nb_classes = 53\r\n        \r\n        target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\r\n        x = Lambda(target_layer, output_shape = target_category_loss_output_shape)(input_model.output)\r\n        \r\n        model = Model(inputs=input_model.input, outputs=x)\r\n        model.summary()\r\n\r\n        loss = K.sum(model.output)\r\n        conv_output =  [l for l in model.layers if l.name == layer_name][0].output\r\n\r\n        var_list = [conv_output]\r\n    grads = gtape.gradient(loss, var_list)", "by the way: The input model is a resnet model , implemented by tf.keras", "@realbns2008 \r\n\r\nPlease, share colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> @realbns2008\r\n> \r\n> Please, share colab link or simple standalone code with proper indentation and supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\nHi ravikyram, below is the main python code\r\nhttps://colab.research.google.com/drive/1fZtcnOY7mRMnFHihGsve3jA6RXk05dv-?usp=sharing", "@ravikyram \r\n\r\nhere is the link of the model i use:\r\nhttps://drive.google.com/file/d/1N013WMlgmDYOtblSxcmMGKB1cY3cRsaM/view?usp=sharing", "@realbns2008 \r\n\r\nI am not able to reproduce the issue. Please, help me with reproducing the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/86a53142054eec129f25dadd147401db/untitled547.ipynb).Thanks!", "@ravikyram \r\nHi, it looks like that the model path is wrong.\r\ndid you download the model to path \"/content/industry_classifier_v4.h5\" ?", "the model link looks fine \"https://drive.google.com/file/d/1N013WMlgmDYOtblSxcmMGKB1cY3cRsaM/view?usp=sharing\"\r\nI download the model from this link, and test it on my machine, and it reproduce the issue", "I am getting the same error in Tensorflow 2.4.1 when computing gradients.\r\n\r\nHere is a reproducible example of the issue:\r\nhttps://drive.google.com/file/d/1po6RoHQ_cok64EHXVrOIej1UXNnYsFIy/view?usp=sharing\r\n\r\nAny help would greatly appreciated.", "Tensorflow 2.4.1 exact same problem", "Can confirm, running into the same problem in TF 2.4.1", "I am getting the same error in tensorflow 2.4.1 when computing gradients.\r\n", "yes, I am having the same error with 2.4.1.", "i have the same error with 2.4.1", "same issue! with 2.4.1 any anyone has a suggestion?!", "same issue here computing gradients on tf.keras.application.VGG16 with 2.4.1", "I have the same issue, is someone who has a workaround?", "Adding Tomer who worked on keras_tensor.", "Using `tf.GradientTape` as a part of Keras functional model construction is unsupported, and we have no plans on the roadmap to add support for it at this point in time.\r\n\r\nIf you want your model to produce gradients as outputs (though that does not appear to be the case in the above examples):\r\nUse a subclass model that uses `tf.GradientTape` to output the gradients from `call`.\r\n\r\nIf you are just trying to use GradientTape with already-built Keras models: Call the model on actual tensors with values inside the `tf.GradientTape` scope rather than using the symbolic Keras inputs/outputs.\r\n\r\nFor some of these use cases where you don't want to write a custom training loop from scratch, you may also want to consider using a custom training step if you need to use gradienttape in nontrivial ways:\r\nhttps://keras.io/guides/customizing_what_happens_in_fit/", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46194\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46194\">No</a>\n"]}, {"number": 46193, "title": "Path fix for local build of openssh", "body": "", "comments": []}, {"number": 46192, "title": " PB to Tflite", "body": "import tensorflow as tf\r\n\r\npath = 'E:\\\\Code\\\\PythonCode\\\\AliYunCode\\\\JS-CODE-20210104-1008-CNNX-GRU-H64-CTC-C1_0.pb'\r\n\r\ninputs = [\"input\"]\r\n\r\noutputs = [\"dense_decoded\"]\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs)\r\n\r\nconverter.post_training_quantize = True\r\n\r\ntflite_model = converter.convert()\r\n\r\nopen(\"tiny_160000.tflite\", \"wb\").write(tflite_model)\r\n\r\n-----\r\nERROR:Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, GATHER, LEAKY_RELU, LESS, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, NOT_EQUAL, RANGE, REDUCE_ANY, RESHAPE, SELECT, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, SUB, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\r\n", "comments": ["I think you need to train model with https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2 since your TF model file is based on the control flow v1 ops, which are not supported by TFLite. \r\n\r\nAnd there are some TF operators that are not covered by the TensorFlow Lite operator set. You can use the Select TF op option:\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\nhttps://www.tensorflow.org/lite/guide/reduce_binary_size", "have you tried using bazel ?\r\n\r\n> \r\n> \r\n> import tensorflow as tf\r\n> \r\n> path = 'E:\\Code\\PythonCode\\AliYunCode\\JS-CODE-20210104-1008-CNNX-GRU-H64-CTC-C1_0.pb'\r\n> \r\n> inputs = [\"input\"]\r\n> \r\n> outputs = [\"dense_decoded\"]\r\n> \r\n> converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(path, inputs, outputs)\r\n> \r\n> converter.post_training_quantize = True\r\n> \r\n> tflite_model = converter.convert()\r\n> \r\n> open(\"tiny_160000.tflite\", \"wb\").write(tflite_model)\r\n> \r\n> ERROR:Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, GATHER, LEAKY_RELU, LESS, LOGICAL_AND, MAXIMUM, MAX_POOL_2D, MINIMUM, MUL, NOT_EQUAL, RANGE, REDUCE_ANY, RESHAPE, SELECT, SHAPE, SPARSE_TO_DENSE, STRIDED_SLICE, SUB, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, Enter, Exit, LoopCond, Merge, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46192\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46192\">No</a>\n"]}, {"number": 46190, "title": "[Dockerfile] move stubs to the end of LD_LIBRARY_PATH", "body": "This lands PR #44732.", "comments": []}, {"number": 46189, "title": "Generalising class weights for all label ranks", "body": "Currently, model.fit(class_weight={0: 1.0, ...}) only works if y_data is of\r\nrank 2, e.g. (batch_size, label_dim).\r\n\r\nThis change allows any label shape to have its outputs weighted,\r\ne.g. image or temporal / sequence data. The relevant tests\r\nare included.\r\n\r\nAddresses issue #40457", "comments": ["have done some local tests but waiting on a few more before requesting review", "I'd like to add to the tests. They should check that the data handler does actually produce logits that are adjusted by the class and sample weights as expected. Still in progress for this reason.\r\n\r\nIf there are additional suggestions / locations I should add unittests, please let me know. This is my first contribution so I'm unfamiliar with the test structure, despite some searching for the right places.\r\n\r\nI think doing the above should give reasonable assurance that the feature is working, though, as the DataHandler directly serves the elsewhere-tested model.fit() function.", "ah-ha, I have (re)discovered the rub with 3+ dimensional class weightings - there is no way to know whether the final dimension is one-hot encoded or a multidimensional output. I will spend a little time thinking but I think there would have to be some structural assertions that I'm not sure we're looking to make.\r\n\r\nIt could be inferred from number of classes in class_weights provided, but that would cause problems if the a flattened output happened to have the same number of dimensions as output classes", "I think this is ready and tested. I'll wait to see if unittests pass, then move to ready for review.\r\n\r\nI'm getting a strange failure on the log caching locally, where\r\nnew_func_graph = \"INFO:absl:tensorflow:Creating new FuncGraph for Python function\" is not being found in the evaluate logs\r\nwhich is fixed if I change to:\r\nnew_func_graph = \"Level 1:tensorflow:Creating new FuncGraph for Python function\"\r\n\r\nI don't think I changed anything, so hopefully it just won't happen on this end.", "@j-bernardi  Can you please resolve conflicts? Thanks!", "Hi Bernardi. I was also follow your thread here for the issue: Generalising class weights for all label ranks.\r\nOne of the main reasons why i wanted to use the latest tf-nightly builds.\r\nCurrently i cannot get class weights to work for multi label data (crash/error out for labels 3 or more)\r\nWould be great to get your working implementation running.", "@gbaned  sure - just commenting to say it's not dead, but I haven't got around to it! \r\n\r\nCheers", "Oops. Will look into it.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Yes intending to work on this change still, apologies for the stall for anyone following.", "Converted to draft. Still intending to make the PR, but I'm struggling to get tests running on my new machine and not sure what led to the most recent failures.", "Managed to get the tests running locally again. Let's see if the checks turn anything up, then ready for re-review", "@j-bernardi Can you please resolve conflicts? Thanks!\r\n", "@j-bernardi Any update on this PR? Please. Thanks!", "@gbaned still on the todo list, if you're happy to leave it open for now. Thanks for continued interest in this change, to all involved.\r\n\r\nI noticed the last response to reviewer requests went unreviewed from April 29th until the merge conflict on 25th June. I appreciate the reviewer likely had a lot on their plate(!), however I wanted to check I followed the correct process for requesting re-review, or if there's anything else I can do to help get this in? Thanks again!", "@j-bernardi Sorry for the delay. Once conflicts are resolved, we will try to review this PR as early as possible.  Thank you. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I had intended to get back to this, but have not found the time. Converting to draft and I will re-request review once I get onto it, perhaps next month, if that's ok.\r\n\r\nCheers all", "@j-bernardi sorry that we weren't able to review this PR in a timely fashion. When you want to work on it again, please open the PR at [keras-team/keras](https://github.com/keras-team/keras) instead, which is our new development location. Over there we should be able to provide a timely review. Improving the contributor experience was the entire motivation for moving to a separate pure-Python repo."]}, {"number": 46187, "title": "Add target and pass_string parameters to the renode test script to fix #46186", "body": "This allows the `test_with_renode.sh` script to be called via the makefile with all the parameters needed to run on a given target, while also staying consistent with the other test scripts.\r\n\r\nAs a result of this change, the makefile is passing in the following parameters to the test scripts:\r\n param 1 - test binary path\r\n param 2 - string to determine that test passes\r\n param 3 - target\r\n\r\nParameter 3 is only used for `test_with_renode.sh`\r\n\r\nManually tested that the following commands now pass:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_kernel_add_test\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=stm32f4 TAGS=cmsis-nn test_kernel_fully_connected_test\r\n```\r\n\r\nFixes #46186", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46186, "title": "running a single test with renode is broken.", "body": "@tensorflow/micro\r\n\r\nWhile the following command passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test\r\n```\r\n\r\nRunning a single test with renode (for example):\r\n```bash\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill test_kernel_add_test\r\n```\r\n\r\nfails with:\r\n```\r\ntensorflow/lite/micro/testing/test_with_renode.sh tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/kernel_add_test '~~~ALL TESTS PASSED~~~'\r\ntensorflow/lite/micro/testing/test_with_renode.sh: line 69: $ROBOT_SCRIPT: ambiguous redirect\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:663: test_kernel_add_test] Error 1\r\n```\r\n\r\nThe reason is that the changes from https://github.com/tensorflow/tensorflow/pull/45787 are incompatible with how the Makefile calls the test script when running an individual test (as opposed to `make test`).\r\n\r\n", "comments": []}, {"number": 46185, "title": "Continue removing `-march=native`", "body": "", "comments": []}, {"number": 46184, "title": "Remove `-march=native`. Testing for #45744, #45866, #44701", "body": "", "comments": []}, {"number": 46183, "title": "tflite-nnapi Handle version 2 of transpose conv", "body": "We should support version 2 which contains INT8 instead of float\r\nin nnapi. Update the verification to version 2", "comments": ["This should fix https://github.com/tensorflow/tensorflow/issues/46098"]}, {"number": 46182, "title": "Update the style checks to also include the reference kernels.", "body": "Prior to this change we were only looking at code within the tensorflow/lite/micro directory. However, TFLM does share the reference implementations with Lite and so we are also checking for license and formatting in those files.\r\n\r\nThis should help detect errors faster, for example, PR https://github.com/tensorflow/tensorflow/pull/45814 missed adding a license to the header but that wasn't detected until the PR was imported internally.\r\n\r\nSee http://b/169948621 and http://b/175315163 for more details.\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 46181, "title": "Publishing Tensorflow wheel package for s390x architecture", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.3.1\r\n- Python version: Python 3.6.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n\r\n**Describe the problem**\r\nCurrently `pip install tensorflow` command on s390x architecture errors out with `No matching distribution found for tensorflow`.\r\n\r\nThis happens because there is no corresponding s390x arch wheel available on [PyPI](https://pypi.org/simple/tensorflow/).\r\n\r\nI am investigating whether we could push s390x wheel on PyPI and, if so, what are the steps needed.\r\n\r\nNightly/release builds for s390x appear to produce corresponding wheel for s390x as seen [here](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/lastSuccessfulBuild/artifact/tensorflow_whl/tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl).  There also exists a `Release` Artifact which can contain s390x Tensorflow wheel which can be pushed to PyPI.\r\n\r\nPlease let me know if/how this can be achieved. I have tried installing the s390x wheel available from the [CI](https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/lastSuccessfulBuild/artifact/tensorflow_whl/tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl) and it appears to have installed on s390x using following commands:\r\n```\r\napt-get install sudo wget git unzip zip python3-dev python3-pip openjdk-11-jdk pkg-config libhdf5-dev libssl-dev libblas-dev liblapack-dev gfortran cython3 -y\r\nldconfig\r\nexport GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=True\r\npip3 install grpcio\r\npip3 install numpy wheel scipy portpicker protobuf==3.13.0\r\nwget https://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/lastStableBuild/artifact/tensorflow_whl/tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl\r\npip3 install tensorflow-2.5.0-cp36-cp36m-linux_s390x.whl\r\n```\r\nThanks.", "comments": ["This cannot be done by Google team as we lack headcount for support. Instead, it can be done via SIG Build, as a different project.", "Thanks @mihaimaruseac - I presume [this](https://github.com/tensorflow/build) is what we are referring to.", "Yep, that's correct.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46181\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46181\">No</a>\n"]}, {"number": 46180, "title": "Fix SparseDenseCwiseMulOrDivGrad scalar case", "body": "Fixes #46008.", "comments": ["Thanks for suggestion! [N, 0] indices array looks weird to me because number of elements inside it is zero, which means that we are gathering nothing. In that case, what should we expect the output shape of gather_nd? Shape of (N,) or a scalar.", "I agree that size-zero arrays are always a bit weird, but they're often a useful special case. I'd return shape (N,) if the requested indices were shape (N, 0); requested indices with shape (0,) would return a scalar (if that's supported). That matches the general case of (..., N) -> (...) for a full index, right?\r\n\r\nWe do something similar with slices in Python, and so does NumPy:\r\nhttps://numpy.org/doc/stable/reference/arrays.indexing.html#detailed-notes\r\n> An empty (tuple) index is a full scalar index into a zero dimensional array. x[()] returns a scalar if x is zero dimensional and a view otherwise. On the other hand x[...] always returns a view.\r\n\r\nIt's just adding some leading dimensions, but seems like the same principle.", "@WindQAQ  Can you please check @allenlavoie's comments and keep us posted ? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@WindQAQ  Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 46179, "title": "Fix release notes", "body": "There were a few items left straggling in #44220, getting ready for an eventual 2.4 patch release", "comments": []}, {"number": 46178, "title": "Apple M1 chip - illegal hardware instruction", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Big Sur 11.0.1\r\n- TensorFlow installed from (source or binary): https://www.tensorflow.org/install/pip\r\n- TensorFlow version: Latest stable (pip)\r\n- Python version: Python 3.8.5\r\n- Installed using virtualenv? pip? conda?: exactly like in the instruction https://www.tensorflow.org/install/pip\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nAfter running the verification steps \r\n```\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\n```\r\n\r\nThe following error appear\r\n```\r\nillegal hardware instruction\r\n```\r\n\r\n\r\n**Any other info / logs**\r\nThis issue only appear on the Mac with the Apple M1 chip. The same setup procedure https://www.tensorflow.org/install/pip works fine on my other Mac`s.", "comments": ["We don't support the M1 chip. This comes from an Apple fork.", "Hello Mihai,\r\n\r\nthanks for your quick response. \r\nThis information rather surprises me especially since Apple and TensorFlow did quite some advertising about it. \r\n\r\n* This means we will have to work on some kind of private Apple fork rather then on the official TensorFlow version?\r\n* Are there any plans to integrate this directly into TensorFlow?\r\n* This also means we are bounded to TensorFlow 2.4 on Apple M1 Macs?\r\n\r\nIt seems somehow odd to use a fork just because it is another chip.\r\n\r\n\r\n\r\n", "The new chip is on ARM, afaik. We only release x86_64 official pips and leave everything else to the community.\r\n\r\nthere is #45404 to make this repo build on the ARM chips but will take a while until it lands and we still don't have this supported during release time. We have less than 1 engineer left for releasing.", "```\r\nIn the near future, we\u2019ll be making updates like this even easier for users \r\nto get these performance numbers by integrating the forked version into\r\nthe TensorFlow master branch.\r\n\r\nhttps://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html\r\n```\r\nCan you provide us an ETA for that?\r\n\r\nGreat to see the community is working on that https://github.com/tensorflow/tensorflow/pull/45404. But it usually takes a lot of time till builds are implemented and integrated. That means I have to rely on Apple and their support. This is a big issue in my opinion. \r\n", "Afaik Apple has not communicated at all with the team that does the releases. Given that there is only 1 person (actually less since releases are not prioritized), there cannot be an ETA.", "https://github.com/tensorflow/tensorflow/issues/46044#issuecomment-753549970\r\nI hope this comment will help you before merge  the Apple fork, which seems it will be a longgggg time.\r\n:)\r\n\r\n", "Shouldn't it work when using **Rosetta 2** though? Unfortunately it doesn't when running `arch -x86_64 python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nEdit: Looks like this goes over how to fix it: Compile TensorFlow from source using Bazel on Rosetta 2 https://github.com/tensorflow/tensorflow/issues/46044#issuecomment-753549970", "For Rosetta2: see #46044 (master branch should be able to compile, 2.4 branch waiting for a patch release)\r\nFor M1 chip, we landed #45404 so now if you compile this repo from source you should also get a working TF.\r\n\r\nWe are currently looking at how we can provide ongoing support.\r\n\r\nI think we can close this now and leave #46044 open.", "The blog post specifically links to a separate tensorflow repo that needs to be built, not the version that comes from PyPi\r\n\r\nhttps://blog.tensorflow.org/2020/11/accelerating-tensorflow-performance-on-mac.html", "Closing this issue in the light of #45404. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46178\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46178\">No</a>\n", "Is there any update on this issue? Now it is June 12th and I'm still getting this error. ", "I wanted to add that I get this not just with tensorflow, but with gcc/g++ (more often if I have some optimization/target pragmas), and with lots of things that use gcc/g++, like a [rust crate](https://github.com/servo/pathfinder/tree/master/demo/native) I just tried to run.", "Hey, It's July  I am still facing the issue, Is there a solution yet?", "You guys must see this post \r\nhttps://developer.apple.com/metal/tensorflow-plugin/\r\n\r\nthis post describes how to import TensorFlow with there plugins", "I am M1 Pro user. I met this problem after activating the virtual environment Anaconda and installing tensorflow. When I run my program and it failed. I'm perplexed about it and hope that Tim Cook can help me."]}, {"number": 46177, "title": "AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>>", "body": "**System information**\r\n- TensorFlow installed from (source or binary): Hosted on colab server\r\n- TensorFlow version (use command below): TF 4.0\r\n- Python version: 3.6.9 (Latest version Hosted on google colab)  \r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nTue Jan  5 14:24:33 2021       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   56C    P0    62W / 149W |   2206MiB / 11441MiB |      0%      Default |\r\n|                               |                      |                 ERR! |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n### Current behavior\r\n\r\nWARNING:tensorflow : AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>> and will run it as-is.\r\n\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n\r\nCause: <cyfunction Socket.send at 0x7fdb48bd4e58> is not a module, class, method, function, traceback, frame, or code object\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n\r\nWARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdb3138d5f8>> and will run it as-is.\r\n\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\n\r\nCause: <cyfunction Socket.send at 0x7fdb48bd4e58> is not a module, class, method, function, traceback, frame, or code object\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nThe parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\r\nWARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fdb465698c8> and will run it as-is.\r\n\r\nCause: while/else statement not yet supported\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nThe parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\r\nWARNING: AutoGraph could not transform <function wrap at 0x7fdb465698c8> and will run it as-is.\r\n\r\nCause: while/else statement not yet supported\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n\r\n**Describe the expected behavior**\r\n\r\nThis warning is preventing to use AutoGraph but i think execution could be faster if it generates a tensorflow graph to run the code.\r\n\r\n\r\n", "comments": ["well after exploring a little bit I have used tf-nightly but same response is generated. I tried to collect the log files by changing verbosity of the autograph but still no identification of this warning. If in past this issue has been explained/solved please mention it here because i couldn't find one.", "@zyberg2091,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here. Alternatively, you can share the Colab notebook you are running.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/45354#issuecomment-740039243) from a similar issue and check if it helps. Thanks!\r\n", "Stand alone code to reproduce the warning : Tensorflow version (TF 4.0)\r\n\r\nhttps://colab.research.google.com/drive/1N-rSuvqD8hoM3_wh7HLQQfdB52jvuv_E#scrollTo=GzRZfBxlPHvf", "Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/5962f4656dfed02cfb0a8d4495c0ad01/46177.ipynb). Thanks!", "@amahendrakar @ymodak I am having the same problem. Any idea on how I can solve it? I am using the same library but my model is not training beyond the 1st epoch, so I think that this may be a factor in the issue. Is there any other workaround except suppressing them?", "It appears that transformers is using some extensions that aren't compatible with autograph. At any rate, transformer itself should still be compatible with tracing, which still runs in graph mode. So I recommend writing something like this, which should be enough to clear the autograph warnings without other issues:\r\n\r\n```\r\n@tf.autograph.experimental.do_not_convert\r\ndef run_trace_only(bert_base_model, input_word_ids):\r\n    return bert_base_model(input_word_ids)[0]\r\n\r\ndef custom_model(bert_base_model):\r\n    input_word_ids = Input(shape=(200,), dtype=tf.int32)\r\n    sequence_output = run_trace_only(bert_base_model, input_word_ids)\r\n    ...\r\n```\r\n\r\nAfter running it that way, I see two other warnings remaining, which might be explained by the transformers documentation.\r\n", "Was able to replicate the issue in TF 2.6.0-dev20210603,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/7dd26433f06d99d2f027fdadf586c3cf/untitled162.ipynb?authuser=1)..Thanks !", "Hi @zyberg2091 ! This issue is not replicating in [2.8](https://colab.research.google.com/gist/mohantym/cc1bdfa18c81425a1894843a27986552/untitled162.ipynb?authuser=1#scrollTo=T2xB5gdWuOdp) version. Can we close this issue now?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46177\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46177\">No</a>\n"]}, {"number": 46176, "title": "TFX ML Metadata storage to PostgreSQL", "body": "I wanted to store my metadata information of ML artifacts to PostgreSQL but I couldn't find any libraries that support MLMD for PostgreSQL. Is there any API's that could support to PostgreSQL other than MySQL/SQLite?", "comments": ["@cah-aswini-jalla,\r\nThis issue is not related to Tensorflow but it is related to `ML Metadata` and `PostgreSQL`. Please raise the issue in the respective Repository so that it will be looked into by the appropriate Engineers. Thanks!"]}, {"number": 46175, "title": " AttributeError: module 'tensorflow' has no attribute 'data'", "body": "Installing on a Windows 10 PC using CPU (no GPU)\r\nWorking in an Anaconda environment\r\n\r\nHave previously used TF 1.5, but now for NLP work I needed to move to the new generation\r\n\r\nI am using Python 3.6 installed tensorflow 2.2. Received the same issue with tensorflow 2.1\r\n\r\nRunning the line:\r\ntext_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\r\n\r\nI get:\r\n**_**Traceback (most recent call last):\r\n\r\n  File \"<ipython-input-34-3c9c34f69f7e>\", line 2, in <module>\r\n    text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'data'**_**\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'data'\r\n\r\n\r\nIt looks like an installation program, so I reinstalled, using different versions. Any help much appreciated\r\n\r\n(tensorflow2) C:\\Users\\mehes>conda list\r\n# packages in environment at C:\\Users\\mehes\\Anaconda3\\envs\\tensorflow2:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_tflow_select             2.2.0                     eigen\r\nabsl-py                   0.11.0           py36ha15d459_0    conda-forge\r\naiohttp                   3.7.3            py36h68aa20f_0    conda-forge\r\nalabaster                 0.7.12                     py_0    conda-forge\r\nappdirs                   1.4.4              pyh9f0ad1d_0    conda-forge\r\nargh                      0.26.2          pyh9f0ad1d_1002    conda-forge\r\nastor                     0.8.1              pyh9f0ad1d_0    conda-forge\r\nastroid                   2.4.2            py36h9f0ad1d_1    conda-forge\r\nasync-timeout             3.0.1                   py_1000    conda-forge\r\nasync_generator           1.10                       py_0    conda-forge\r\natomicwrites              1.4.0              pyh9f0ad1d_0    conda-forge\r\nattrs                     20.3.0             pyhd3deb0d_0    conda-forge\r\nautopep8                  1.5.4              pyh9f0ad1d_0    conda-forge\r\nbabel                     2.9.0              pyhd3deb0d_0    conda-forge\r\nbackcall                  0.2.0              pyh9f0ad1d_0    conda-forge\r\nbackports                 1.0                        py_2    conda-forge\r\nbackports.functools_lru_cache 1.6.1                      py_0    conda-forge\r\nbcrypt                    3.2.0            py36h779f372_1    conda-forge\r\nblack                     20.8b1                     py_0    conda-forge\r\nbleach                    3.2.1              pyh9f0ad1d_0    conda-forge\r\nblinker                   1.4                        py_1    conda-forge\r\nbrotlipy                  0.7.0           py36hc753bc4_1001    conda-forge\r\nca-certificates           2020.12.5            h5b45459_0    conda-forge\r\ncached-property           1.5.1                      py_0    conda-forge\r\ncachetools                4.1.1                      py_0    conda-forge\r\ncertifi                   2020.12.5        py36ha15d459_0    conda-forge\r\ncffi                      1.14.4           py36he58ceb7_1    conda-forge\r\nchardet                   3.0.4           py36hd36e781_1008    conda-forge\r\nclick                     7.1.2              pyh9f0ad1d_0    conda-forge\r\ncloudpickle               1.6.0                      py_0    conda-forge\r\ncolorama                  0.4.4              pyh9f0ad1d_0    conda-forge\r\ncryptography              3.3.1            py36he58ceb7_0    conda-forge\r\ndecorator                 4.4.2                      py_0    conda-forge\r\ndefusedxml                0.6.0                      py_0    conda-forge\r\ndiff-match-patch          20200713           pyh9f0ad1d_0    conda-forge\r\ndocutils                  0.16             py36ha15d459_2    conda-forge\r\nentrypoints               0.3             pyhd8ed1ab_1003    conda-forge\r\nflake8                    3.8.4                      py_0    conda-forge\r\nfuture                    0.18.2           py36ha15d459_2    conda-forge\r\ngast                      0.2.2                      py_0    conda-forge\r\ngoogle-auth               1.24.0             pyhd3deb0d_0    conda-forge\r\ngoogle-auth-oauthlib      0.4.1                      py_2    conda-forge\r\ngoogle-pasta              0.2.0              pyh8c360ce_0    conda-forge\r\ngrpcio                    1.34.0           py36h4374274_0    conda-forge\r\nh5py                      3.1.0           nompi_py36hf359dfe_100    conda-forge\r\nhdf5                      1.10.6          nompi_h5268f04_1113    conda-forge\r\nhelpdev                   0.7.1              pyhd8ed1ab_0    conda-forge\r\nicu                       68.1                 h0e60522_0    conda-forge\r\nidna                      2.10               pyh9f0ad1d_0    conda-forge\r\nidna_ssl                  1.1.0           py36h9f0ad1d_1001    conda-forge\r\nimagesize                 1.2.0                      py_0    conda-forge\r\nimportlib-metadata        3.3.0            py36ha15d459_2    conda-forge\r\nimportlib_metadata        3.3.0                hd8ed1ab_2    conda-forge\r\nintel-openmp              2020.3             h57928b3_311    conda-forge\r\nintervaltree              3.0.2                      py_0    conda-forge\r\nipykernel                 5.4.2            py36h7b7c402_0    conda-forge\r\nipython                   7.16.1           py36h7b2dad6_2    conda-forge\r\nipython_genutils          0.2.0                      py_1    conda-forge\r\nisort                     5.7.0              pyhd8ed1ab_0    conda-forge\r\njedi                      0.17.2           py36ha15d459_1    conda-forge\r\njinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\r\njpeg                      9d                   h8ffe710_0    conda-forge\r\njsonschema                3.2.0                      py_2    conda-forge\r\njupyter_client            6.1.7                      py_0    conda-forge\r\njupyter_core              4.7.0            py36ha15d459_0    conda-forge\r\njupyterlab_pygments       0.1.2              pyh9f0ad1d_0    conda-forge\r\nkeras-applications        1.0.8                      py_1    conda-forge\r\nkeras-preprocessing       1.1.0                      py_0    conda-forge\r\nkeyring                   21.8.0           py36ha15d459_0    conda-forge\r\nkrb5                      1.17.2               hbae68bd_0    conda-forge\r\nlazy-object-proxy         1.4.3            py36h779f372_2    conda-forge\r\nlibblas                   3.9.0                     6_mkl    conda-forge\r\nlibcblas                  3.9.0                     6_mkl    conda-forge\r\nlibclang                  11.0.0          default_h5c34c98_2    conda-forge\r\nlibcurl                   7.71.1               h4b64cdc_8    conda-forge\r\nliblapack                 3.9.0                     6_mkl    conda-forge\r\nlibpng                    1.6.37               h1d00b33_2    conda-forge\r\nlibprotobuf               3.14.0               h7755175_0    conda-forge\r\nlibsodium                 1.0.18               h8d14728_1    conda-forge\r\nlibspatialindex           1.9.3                he025d50_3    conda-forge\r\nlibssh2                   1.9.0                hb06d900_5    conda-forge\r\nm2w64-gcc-libgfortran     5.3.0                         6    conda-forge\r\nm2w64-gcc-libs            5.3.0                         7    conda-forge\r\nm2w64-gcc-libs-core       5.3.0                         7    conda-forge\r\nm2w64-gmp                 6.1.0                         2    conda-forge\r\nm2w64-libwinpthread-git   5.0.0.4634.697f757               2    conda-forge\r\nmarkdown                  3.3.3              pyh9f0ad1d_0    conda-forge\r\nmarkupsafe                1.1.1            py36hc753bc4_2    conda-forge\r\nmccabe                    0.6.1                      py_1    conda-forge\r\nmistune                   0.8.4           py36h68aa20f_1002    conda-forge\r\nmkl                       2020.4             hb70f87d_311    conda-forge\r\nmsys2-conda-epoch         20160418                      1    conda-forge\r\nmultidict                 5.1.0            py36h68aa20f_0    conda-forge\r\nmypy_extensions           0.4.3            py36ha15d459_2    conda-forge\r\nnbclient                  0.5.1                      py_0    conda-forge\r\nnbconvert                 6.0.7            py36ha15d459_3    conda-forge\r\nnbformat                  5.0.8                      py_0    conda-forge\r\nnest-asyncio              1.4.3              pyhd8ed1ab_0    conda-forge\r\nnumpy                     1.14.5                   pypi_0    pypi\r\nnumpydoc                  1.1.0                      py_1    conda-forge\r\noauthlib                  3.0.1                      py_0    conda-forge\r\nopenssl                   1.1.1i               h8ffe710_0    conda-forge\r\nopt_einsum                3.3.0                      py_0    conda-forge\r\npackaging                 20.8               pyhd3deb0d_0    conda-forge\r\npandoc                    2.11.3.2             h8ffe710_0    conda-forge\r\npandocfilters             1.4.2                      py_1    conda-forge\r\nparamiko                  2.7.2              pyh9f0ad1d_0    conda-forge\r\nparso                     0.7.0              pyh9f0ad1d_0    conda-forge\r\npathspec                  0.8.1              pyhd3deb0d_0    conda-forge\r\npexpect                   4.8.0              pyh9f0ad1d_2    conda-forge\r\npickleshare               0.7.5                   py_1003    conda-forge\r\npip                       20.3.3             pyhd8ed1ab_0    conda-forge\r\npluggy                    0.13.1           py36hd36e781_3    conda-forge\r\nprompt-toolkit            3.0.8              pyha770c72_0    conda-forge\r\nprotobuf                  3.14.0           py36he2d232f_0    conda-forge\r\npsutil                    5.8.0            py36h68aa20f_0    conda-forge\r\nptyprocess                0.7.0              pyhd3deb0d_0    conda-forge\r\npyasn1                    0.4.8                      py_0    conda-forge\r\npyasn1-modules            0.2.7                      py_0    conda-forge\r\npycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge\r\npycparser                 2.20               pyh9f0ad1d_2    conda-forge\r\npydocstyle                5.1.1                      py_0    conda-forge\r\npyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge\r\npygments                  2.7.3              pyhd8ed1ab_0    conda-forge\r\npyjwt                     1.7.1                      py_0    conda-forge\r\npylint                    2.6.0            py36h9f0ad1d_1    conda-forge\r\npyls-black                0.4.6              pyh9f0ad1d_0    conda-forge\r\npyls-spyder               0.3.0              pyhd8ed1ab_0    conda-forge\r\npynacl                    1.4.0            py36h3a74357_2    conda-forge\r\npyopenssl                 20.0.1             pyhd8ed1ab_0    conda-forge\r\npyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\r\npyqt                      5.12.3           py36ha15d459_6    conda-forge\r\npyqt-impl                 5.12.3           py36he2d232f_6    conda-forge\r\npyqt5-sip                 4.19.18          py36he2d232f_6    conda-forge\r\npyqtchart                 5.12             py36he2d232f_6    conda-forge\r\npyqtwebengine             5.12.1           py36he2d232f_6    conda-forge\r\npyreadline                2.1             py36h9f0ad1d_1002    conda-forge\r\npyrsistent                0.17.3           py36h68aa20f_1    conda-forge\r\npysocks                   1.7.1            py36hd36e781_2    conda-forge\r\npython                    3.6.12          h39d44d4_0_cpython    conda-forge\r\npython-dateutil           2.8.1                      py_0    conda-forge\r\npython-jsonrpc-server     0.4.0              pyh9f0ad1d_0    conda-forge\r\npython-language-server    0.36.2             pyhd8ed1ab_0    conda-forge\r\npython_abi                3.6                     1_cp36m    conda-forge\r\npytz                      2020.5             pyhd8ed1ab_0    conda-forge\r\npywin32                   228              py36h779f372_0    conda-forge\r\npywin32-ctypes            0.2.0           py36h9f0ad1d_1002    conda-forge\r\npyyaml                    5.3.1            py36hc753bc4_1    conda-forge\r\npyzmq                     20.0.0           py36hb0157bd_1    conda-forge\r\nqdarkstyle                2.8.1              pyhd8ed1ab_2    conda-forge\r\nqt                        5.12.9               h5909a2a_2    conda-forge\r\nqtawesome                 1.0.2              pyhd8ed1ab_0    conda-forge\r\nqtconsole                 5.0.1              pyhd8ed1ab_0    conda-forge\r\nqtpy                      1.9.0                      py_0    conda-forge\r\nregex                     2020.11.13       py36h68aa20f_0    conda-forge\r\nrequests                  2.25.1             pyhd3deb0d_0    conda-forge\r\nrequests-oauthlib         1.3.0              pyh9f0ad1d_0    conda-forge\r\nrope                      0.18.0             pyh9f0ad1d_0    conda-forge\r\nrsa                       4.6                pyh9f0ad1d_0    conda-forge\r\nrtree                     0.9.4            py36h089df06_2    conda-forge\r\nscipy                     1.5.3            py36h7ff6e69_0    conda-forge\r\nsetuptools                49.6.0           py36hd36e781_2    conda-forge\r\nsix                       1.15.0             pyh9f0ad1d_0    conda-forge\r\nsnowballstemmer           2.0.0                      py_0    conda-forge\r\nsortedcontainers          2.3.0              pyhd8ed1ab_0    conda-forge\r\nsphinx                    3.4.2              pyhd8ed1ab_0    conda-forge\r\nsphinxcontrib-applehelp   1.0.2                      py_0    conda-forge\r\nsphinxcontrib-devhelp     1.0.2                      py_0    conda-forge\r\nsphinxcontrib-htmlhelp    1.0.3                      py_0    conda-forge\r\nsphinxcontrib-jsmath      1.0.1                      py_0    conda-forge\r\nsphinxcontrib-qthelp      1.0.3                      py_0    conda-forge\r\nsphinxcontrib-serializinghtml 1.1.4                      py_0    conda-forge\r\nspyder                    4.2.1            py36ha15d459_0    conda-forge\r\nspyder-kernels            1.10.1           py36ha15d459_0    conda-forge\r\nsqlite                    3.34.0               h8ffe710_0    conda-forge\r\ntensorboard               2.1.1                    pypi_0    pypi\r\ntensorboard-plugin-wit    1.7.0              pyh9f0ad1d_0    conda-forge\r\ntensorflow                2.1.0           eigen_py36hdbbabfe_0\r\ntensorflow-base           2.1.0           eigen_py36h49b2757_0\r\ntensorflow-estimator      2.1.0                    pypi_0    pypi\r\ntermcolor                 1.1.0                      py_2    conda-forge\r\ntestpath                  0.4.4                      py_0    conda-forge\r\ntextdistance              4.2.0              pyhd8ed1ab_0    conda-forge\r\nthree-merge               0.1.1              pyh9f0ad1d_0    conda-forge\r\ntk                        8.6.10               h8ffe710_1    conda-forge\r\ntoml                      0.10.2             pyhd8ed1ab_0    conda-forge\r\ntornado                   6.1              py36h68aa20f_0    conda-forge\r\ntraitlets                 4.3.3            py36h9f0ad1d_1    conda-forge\r\ntyped-ast                 1.4.2            py36h68aa20f_0    conda-forge\r\ntyping-extensions         3.7.4.3                       0    conda-forge\r\ntyping_extensions         3.7.4.3                    py_0    conda-forge\r\nujson                     4.0.1            py36h003fed8_1    conda-forge\r\nurllib3                   1.26.2             pyhd8ed1ab_0    conda-forge\r\nvc                        14.2                 hb210afc_2    conda-forge\r\nvs2015_runtime            14.28.29325          h5e1d092_0    conda-forge\r\nwatchdog                  1.0.2            py36ha15d459_0    conda-forge\r\nwcwidth                   0.2.5              pyh9f0ad1d_2    conda-forge\r\nwebencodings              0.5.1                      py_1    conda-forge\r\nwerkzeug                  0.16.1                     py_0    conda-forge\r\nwheel                     0.36.2             pyhd3deb0d_0    conda-forge\r\nwin_inet_pton             1.1.0            py36h9f0ad1d_1    conda-forge\r\nwincertstore              0.2             py36h9f0ad1d_1005    conda-forge\r\nwrapt                     1.11.2           py36h779f372_1    conda-forge\r\nyaml                      0.2.5                he774522_0    conda-forge\r\nyapf                      0.30.0             pyh9f0ad1d_0    conda-forge\r\nyarl                      1.6.3            py36h68aa20f_0    conda-forge\r\nzeromq                    4.3.3                h0e60522_3    conda-forge\r\nzipp                      3.4.0                      py_0    conda-forge\r\nzlib                      1.2.11            h62dcd97_1010    conda-forge", "comments": ["@mbhoshen \r\n\r\nPlease, refer the [link](https://stackoverflow.com/questions/47143289/tensorflow-attributeerror-module-object-has-no-attribute-data),#34547 and see if it helps you.\r\nIf the issue still persists please share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "Thanks\nIt is under Windows. I am moving a solution from Colab to Windows to\nprepare before moving across  my company firewall, and the problem is in\nthe Windows version\nI am running on a laptop, using its CPU. running tensorflow 2.1.0\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow import data\nimport os\nimport pathlib\nos.chdir(r'C:\\Users\\mehes\\nlp')\ndata_dir =  r'C:\\Users\\mehes\\nlp\\20_newsgroup'\ndirnames = os.listdir(data_dir)\nprint(\"Number of directories:\", len(dirnames))\nprint(\"Directory names:\", dirnames)\n\nfnames = os.listdir(data_dir +'\\\\'+ \"comp.graphics\")\nprint(\"Number of files in comp.graphics:\", len(fnames))\nprint(\"Some example filenames:\", fnames[:5])\n\nprint(open(data_dir +'\\\\'+ \"comp.graphics\" +'\\\\'+ \"38987\").read())\n\nsamples = []\nlabels = []\nclass_names = []\nclass_index = 0\nfor dirname in sorted(os.listdir(data_dir)):\n    class_names.append(dirname)\n    dirpath = data_dir +'\\\\'+ dirname\n    fnames = os.listdir(dirpath)\n    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n    for fname in fnames:\n        fpath = dirpath +'\\\\'+ fname\n        f = open(fpath, encoding=\"latin-1\")\n        content = f.read()\n        lines = content.split(\"\\n\")\n        lines = lines[10:]\n        content = \"\\n\".join(lines)\n        samples.append(content)\n        labels.append(class_index)\n    class_index += 1\n\nprint(\"Classes:\", class_names)\nprint(\"Number of samples:\", len(samples))\n\n# Shuffle the data\nseed = 1337\nrng = np.random.RandomState(seed)\nrng.shuffle(samples)\nrng = np.random.RandomState(seed)\nrng.shuffle(labels)\n\n# Extract a training & validation split\nvalidation_split = 0.2\nnum_validation_samples = int(validation_split * len(samples))\ntrain_samples = samples[:-num_validation_samples]\nval_samples = samples[-num_validation_samples:]\ntrain_labels = labels[:-num_validation_samples]\nval_labels = labels[-num_validation_samples:]\n\nfrom tensorflow.keras.layers.experimental.preprocessing import\nTextVectorization\n\nAlready at the beginning,  following your reference tried to add\n*from tensorflow import data*\n\nand received\n\n\n\n\n\n\n*from tensorflow import dataTraceback (most recent call last):  File\n\"<ipython-input-35-2f185c821dd5>\", line 1, in <module>    from tensorflow\nimport dataImportError: cannot import name 'data'*\n\nWhich I understand to mean that this installation of tensorflow just does\nnot include a 'data' module\n\nThere is indeed a folder tensorflow-2.1.0.data in site-packages, but it\ndoes not seem to have anything to do with it\n\nDr. Moshe Hoshen  \u05de\u05e9\u05d4 \u05d7\u05e9\u05df\nDegania St \u05d3\u05d2\u05e0\u05d9\u05d4 15\nJerusalem \u05d9\u05e8\u05d5\u05e9\u05dc\u05d9\u05dd 96143\nIsrael\nTel. (972-2) 6519199\n\u05e0\u05d9\u05d9\u05d3 054-8061962\nmbhoshen@gmail.com\nhttp://www.linkedin.com/profile/view?id=92054595&trk=tab_pro\n\n\nOn Tue, 5 Jan 2021 at 15:25, ravikyram <notifications@github.com> wrote:\n\n> @mbhoshen <https://github.com/mbhoshen>\n>\n> Please, refer the link\n> <https://stackoverflow.com/questions/47143289/tensorflow-attributeerror-module-object-has-no-attribute-data>\n> ,#34547 <https://github.com/tensorflow/tensorflow/issues/34547> and see\n> if it helps you.\n> If the issue still persists please share colab link or simple standalone\n> code with supporting files to reproduce the issue in our environment.It\n> helps us in localizing the issue faster. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46175#issuecomment-754634070>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABBFRZUNHAE4NTRCRHAH42DSYMHLJANCNFSM4VVACYDA>\n> .\n>\n", "@mbhoshen \r\n\r\nDid you try with Latest stable TF version 2.4 ot nightly version and see if you are facing the same issue?\r\n\r\nIt will be great if you share sample data to reproduce the issue in our environment. It helps us in debugging the issue faster.Thanks!", "Thanks\nI now used the nightly and it seems to work fine. It seems the problem was\nwith versions 2.1 and 2.2. It is not data dependent, but rather with the\ninstallation itself, as the problem arose before loading my data\n\n\u05d1\u05ea\u05d0\u05e8\u05d9\u05da \u05d9\u05d5\u05dd \u05d3\u05f3, 6 \u05d1\u05d9\u05e0\u05d5\u05f3 2021, 08:17, \u05de\u05d0\u05ea ravikyram \u200f<notifications@github.com\n>:\n\n> @mbhoshen <https://github.com/mbhoshen>\n>\n> Did you try with Latest stable TF version 2.4 ot nightly version and see\n> if you are facing the same issue?\n>\n> It will be great if you share sample data to reproduce the issue in our\n> environment. It helps us in debugging the issue faster.Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46175#issuecomment-755106382>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABBFRZWWYQCG2N2ZBYDHXULSYP557ANCNFSM4VVACYDA>\n> .\n>\n", "@mbhoshen \r\n\r\nPlease, close this thread as this issue was resolved in recent TF versions. Thanks!", "Will do, thanks!\n\n\u05d1\u05ea\u05d0\u05e8\u05d9\u05da \u05d9\u05d5\u05dd \u05d3\u05f3, 6 \u05d1\u05d9\u05e0\u05d5\u05f3 2021, 10:27, \u05de\u05d0\u05ea ravikyram \u200f<notifications@github.com\n>:\n\n> @mbhoshen <https://github.com/mbhoshen>\n>\n> Please, close this thread as this issue was resolved in recent TF\n> versions. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/46175#issuecomment-755158807>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABBFRZWPDHJBD42NW2OAKWLSYQNFJANCNFSM4VVACYDA>\n> .\n>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46175\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46175\">No</a>\n"]}, {"number": 46174, "title": "Failed to build tensorflow 2.3.0 from source on Nvidia Jetson Nano", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetpack 4.4 (Ubuntu 18.04 LTS)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 2.7.17 and 3.6.9\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): Build label: 3.1.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2.89 / 8.0.0.180\r\n- GPU model and memory: Nvidia Jetson Nano\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nERROR: /home/minhduc/src/tensorflow-2.3.0/tensorflow/core/kernels/BUILD:6109:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4)\r\naarch64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10499.940s, Critical Path: 654.48s\r\nINFO: 5016 processes: 5016 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed this [tutorial](https://jkjung-avt.github.io/build-tensorflow-2.0.0/) ([github](https://github.com/jkjung-avt/jetson_nano)). I set up my virtual screen, start the build (NOT install) script (build_libtensorflow-2.3.0.sh). Go to school and come back home with the error log above\r\n\r\n**Any other info / logs**\r\nI also install both protobuf 3.8.0 and bazel using shell scripts from the above github repo\r\n", "comments": ["@minhduc66532,\r\nTensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6. For more information, please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nAlso, please follow the [official guide](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_110) while building TensorFlow from source and check if you are facing the same issue. Thanks!", "@amahendrakar I have downgraded Jetpack 4.4 to Jetpack 4.3\r\n\r\n![image](https://user-images.githubusercontent.com/66398066/103753111-36599600-503d-11eb-9c2a-b1597d2118fc.png)\r\n\r\nStill the getting error:\r\n```\r\nERROR: /home/minhduc/src/tensorflow-2.3.0/tensorflow/core/kernels/BUILD:2828:1: C++ compilation of rule '//tensorflow/core/kernels:padding_fifo_queue' failed (Exit 4)\r\naarch64-linux-gnu-gcc-7: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 9190.224s, Critical Path: 1217.73s\r\nINFO: 5456 processes: 5456 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@minhduc66532,\r\nNow you have CUDA 10.0 with cuDNN 7.6, which is not compatible with any of the TensorFlow 2.x versions. As shown in the table below, please try building either \r\n\r\nTF v2.4 with CUDA 11.0 and cuDNN 8 \r\n\r\nor\r\n\r\nTF v2.3 with CUDA 10.1 and cuDNN 7.6\r\n\r\n\r\n \r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow-2.2.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 2.0.0 | 7.6 | 10.1\r\ntensorflow-2.1.0 | 2.7, 3.5-3.7 | GCC 7.3.1 | Bazel 0.27.1 | 7.6 | 10.1\r\ntensorflow-2.0.0 | 2.7, 3.3-3.7 | GCC 7.3.1 | Bazel 0.26.1 | 7.4 | 10.0\r\n\r\nThanks!", "Wow, so it has to be EXACTLY cudnn 7.6.0 and CUDA 10.1 ? Ok i'm gonna try again tomorrow and report the result", "Problem solved. This [guy ](https://qengineering.eu/install-tensorflow-2.3.1-on-jetson-nano.html) already built it on Jetpack 4.4 you can just download and use it.\r\nBut anyway thank you for answering all of my question, really appreciated it. Have a great day", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46174\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46174\">No</a>\n"]}, {"number": 46173, "title": "Failed to build with CUDA 11.1 + TensorRT 7.2", "body": "**System information**\r\nUbuntu 20.04\r\nBuilding branch r2.4\r\nPython 3.8.5 (conda)\r\nBazel 3.1.0\r\nGCC 9\r\nCUDA 11.1\r\nTensorRT 7.2.2\r\nGPU RTX 2080ti\r\n\r\nI was able to build r2.4 a week ago, with a similar config (but in docker, with nvidia cuda-11.1 base image, without tensorrt).  \r\nI tried with or without MKL (but I would like to build both cuda and mkl support).\r\n\r\n**Bazel command**\r\n```bash\r\nexport GCC_HOST_COMPILER_PATH=$(which gcc)\r\nexport PYTHON_BIN_PATH=$(which python)\r\nexport PYTHON_LIB_PATH=\"$($PYTHON_BIN_PATH -c 'import site; print(site.getsitepackages()[0])')\"\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_COMPUTECPP=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\n\r\n# GPU\r\nexport CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:$CUDA_TOOLKIT_PATH/lib64:$CUDA_TOOLKIT_PATH/lib64/stubs\"\r\nexport TF_CUDA_VERSION=11.1\r\nexport TF_CUDA_CLANG=0\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\nexport TF_NEED_CUDA=1\r\nexport CUDNN_INSTALL_PATH=/usr/\r\nexport TF_CUDNN_VERSION=8\r\nexport TF_NCCL_VERSION=2\r\nexport TF_NEED_TENSORRT=1\r\nexport TF_TENSORRT_VERSION=7\r\n\r\n# Set search path: /usr,/usr/local/cuda-11.1,/opt/TensorRT-7.2.2.3\r\n./configure\r\n\r\nexport TMP=/tmp/bazel\r\nbazel build --jobs 15 --compilation_mode opt --verbose_failures --copt='-march=native' --copt='-mfpmath=both' --config=nohdfs --config=mkl --config=opt \\\r\n    //tensorflow:libtensorflow_framework.so //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Bazel output (verbose failure)**\r\n```\r\nConfiguration finished\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=271\r\nINFO: Reading rc options for 'build' from /home/vidlb/Code/git/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/vidlb/Code/git/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/vidlb/Code/git/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin/python --action_env PYTHON_LIB_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/lib/python3.8/site-packages --python_path=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin/python --config=xla --config=tensorrt --action_env TF_CUDA_VERSION=11.1 --action_env TF_CUDNN_VERSION=8 --action_env TF_TENSORRT_VERSION=7 --action_env TF_NCCL_VERSION=2 --action_env TF_CUDA_PATHS=/usr,/usr/local/cuda-11.1,/opt/TensorRT-7.2.2.3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 --action_env CUDNN_INSTALL_PATH=/usr/ --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env LD_LIBRARY_PATH=/opt/TensorRT-7.2.2.3/lib::/opt/tensorflow/lib:/usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64/stubs --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/vidlb/Code/git/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:tensorrt in file /home/vidlb/Code/git/tensorflow/.bazelrc: --action_env TF_NEED_TENSORRT=1\r\nINFO: Found applicable config definition build:cuda in file /home/vidlb/Code/git/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:nohdfs in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=no_hdfs_support=true\r\nINFO: Found applicable config definition build:mkl in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt\r\nINFO: Found applicable config definition build:opt in file /home/vidlb/Code/git/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:linux in file /home/vidlb/Code/git/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/vidlb/Code/git/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/vidlb/.cache/bazel/_bazel_vidlb/60f888ef78f7b073d0fc0a98193e0965/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nWARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  /home/vidlb/.cache/bazel/_bazel_vidlb/60f888ef78f7b073d0fc0a98193e0965/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2020/sqlite-amalgamation-3340000.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/f402e682d0ef5598eeffc9a21a691b03e602ff58.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed 3 targets (414 packages loaded, 33212 targets configured).\r\nINFO: Found 3 targets...\r\nERROR: /home/vidlb/Code/git/tensorflow/tensorflow/stream_executor/cuda/BUILD:254:1: C++ compilation of rule '//tensorflow/stream_executor/cuda:cublas_lt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /home/vidlb/.cache/bazel/_bazel_vidlb/60f888ef78f7b073d0fc0a98193e0965/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.1 \\\r\n    CUDNN_INSTALL_PATH=/usr/ \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \\\r\n    LD_LIBRARY_PATH=/opt/TensorRT-7.2.2.3/lib::/opt/tensorflow/lib:/usr/local/cuda-11.1/lib64:/usr/local/cuda-11.1/lib64/stubs \\\r\n    PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin:/opt/tensorflow/bin:/home/vidlb/Applications/anaconda3/condabin:/home/vidlb/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/bin/python \\\r\n    PYTHON_LIB_PATH=/home/vidlb/Applications/anaconda3/envs/tensorflow/lib/python3.8/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \\\r\n    TF_CUDA_PATHS=/usr,/usr/local/cuda-11.1,/opt/TensorRT-7.2.2.3 \\\r\n    TF_CUDA_VERSION=11.1 \\\r\n    TF_CUDNN_VERSION=8 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_TENSORRT=1 \\\r\n    TF_TENSORRT_VERSION=7 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cublas_lt_stub/cublasLt_stub.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cublas_lt_stub/cublasLt_stub.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt/bin -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/local_config_cuda/cuda/cublas/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cublas/include -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=native' '-mfpmath=both' '-march=native' -Wno-sign-compare '-std=c++14' -c tensorflow/stream_executor/cuda/cublasLt_stub.cc -o bazel-out/k8-opt/bin/tensorflow/stream_executor/cuda/_objs/cublas_lt_stub/cublasLt_stub.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:\r\nbazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:576:5: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  576 |     cublasComputeType_t computeType,\r\n      |     ^~~~~~~~~~~~~~~~~~~\r\nbazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:586:5: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  586 |     cublasComputeType_t computeType,\r\n      |     ^~~~~~~~~~~~~~~~~~~\r\nbazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:597:60: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  597 | cublasLtMatmulDescCreate(cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType, cudaDataType_t scaleType);\r\n      |                                                            ^~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:15:\r\nbazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1069:5: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n 1069 |     cublasComputeType_t computeType,\r\n      |     ^~~~~~~~~~~~~~~~~~~\r\nbazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual/third_party/gpus/cuda/include/cublasLt.h:1087:26: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n 1087 |                          cublasComputeType_t computeType,\r\n      |                          ^~~~~~~~~~~~~~~~~~~\r\nIn file included from tensorflow/stream_executor/cuda/cublasLt_stub.cc:58:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:135:5: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  135 |     cublasComputeType_t computeType, cudaDataType_t scaleType) {\r\n      |     ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function \u2018cublasStatus_t cublasLtMatmulDescInit_internal(cublasLtMatmulDesc_t, size_t, int, cudaDataType_t)\u2019:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:137:37: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  137 |       cublasLtMatmulDesc_t, size_t, cublasComputeType_t, cudaDataType_t);\r\n      |                                     ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:144:39: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  144 |     cublasLtMatmulDesc_t *matmulDesc, cublasComputeType_t computeType,\r\n      |                                       ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function \u2018cublasStatus_t cublasLtMatmulDescCreate(cublasLtMatmulDescOpaque_t**, int, cudaDataType_t)\u2019:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:147:31: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  147 |       cublasLtMatmulDesc_t *, cublasComputeType_t, cudaDataType_t);\r\n      |                               ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:309:35: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  309 |     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,\r\n      |                                   ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function \u2018cublasStatus_t cublasLtMatmulAlgoGetIds(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, int*, int*)\u2019:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:314:25: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  314 |       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,\r\n      |                         ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: At global scope:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:323:35: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  323 |     cublasLtHandle_t lightHandle, cublasComputeType_t computeType,\r\n      |                                   ^~~~~~~~~~~~~~~~~~~\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc: In function \u2018cublasStatus_t cublasLtMatmulAlgoInit(cublasLtHandle_t, int, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, cudaDataType_t, int, cublasLtMatmulAlgo_t*)\u2019:\r\n./tensorflow/stream_executor/cuda/cublasLt_11_0.inc:328:25: error: \u2018cublasComputeType_t\u2019 has not been declared\r\n  328 |       cublasLtHandle_t, cublasComputeType_t, cudaDataType_t, cudaDataType_t,\r\n      |                         ^~~~~~~~~~~~~~~~~~~\r\nINFO: Elapsed time: 1574.977s, Critical Path: 135.25s\r\nINFO: 10072 processes: 10072 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nDo I really need to build with 11.0, even if I was able to build with v11.1 inside a docker container ?\r\nOr is it possible to patch some files in order to build with latest CUDA drivers ?\r\n", "comments": ["@vidlb \r\nCan you check the path for ./configuration. The correct path is /local/usr/cuda-11.0/ and wherever your tensorrt path is. please refer to similar issue : #44106 ", "I'm trying to build with cuda 11.1, but is it possible ? It's strange because I was able to complete a docker build with the same version (11.1.1).\r\nConfiguration is OK, my cuda path is right (**/usr/local/cuda-11.1**), same for tensorrt. Every info is in the post. You will see that I set env variables before running ./configure (then again I set search path when asked because the script can't find tensorrt on its own). It seems to build well but stops around step 22000/36000.", "The error isn't due to CUDA but TensorRT. I was able to build without it.", "@vidlb\r\nplease move this to closed status as resolved.", "If you want, but it is not resolved...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46173\">No</a>\n", "We haven't built or tested with 11.1, so building with 11.1 might require some elbow grease.  However, I'm happy to review PRs with obvious fixes.", "I just read on nvidia's website that tensorrt does not support TF2.x.\r\nEven if I was able to build with v11.1, I guess it is not a good idea if it hasn't been tested yet.\r\nI think I will go back to v11.0 for now... I'd be happy if I can contribute by any mean, but I don't think I'll be able to help you here..", "Closing the issue. I don't think there is anything actionable here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46173\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46173\">No</a>\n"]}, {"number": 46172, "title": "Add GPU kernel for SparseFillEmptyRows[Grad]", "body": "Follow-up to https://github.com/tensorflow/tensorflow/pull/44719\r\n\r\ncc @nluehr ", "comments": ["I fixed a few small issues I noticed since submitting this. I'm not sure why the Windows GPU build is complaining about std::complex on the device.", "@benbarsdell  Can you please check @sanjoy's comments and keep us posted ? Thanks!", "(Fixed Ubuntu CI issues).", "I've cleaned this up in https://github.com/tensorflow/tensorflow/pull/46172/commits/78353b59aa7f0b2bc88ad50f642a8044c1201468 using `gpu_prim_helpers.h` from the recently-merged https://github.com/tensorflow/tensorflow/pull/47082/. It addresses the three requested changes.\r\n\r\n", "Ben, this PR looks functionally correct to me, but I believe these ops only work on sparse matrices, which means `rank` is always 2.  If you agree, do you mind sending a separate PR simplifying the kernel and adding the runtime check?\r\n\r\nCC @penpornk @ebrevdo ", "Thanks Sanjoy.\r\n\r\nRegarding rank, the CPU implementation does currently support rank>2, and it is possible to pass a rank>2 `SparseTensor` to `sparse_fill_empty_rows` without getting any errors. The only indication I see of it being limited to rank=2 is this message in the documentation:`A SparseTensor with shape [N, M].`\r\n\r\nExample with rank=3:\r\n```\r\ntf.sparse.fill_empty_rows(tf.sparse.SparseTensor(indices=[[0, 1, 2]], values=[3.14], dense_shape=[3, 4, 5]), default_value=1.23)\r\n```", "> Thanks Sanjoy.\r\n> \r\n> Regarding rank, the CPU implementation does currently support rank>2, and it is possible to pass a rank>2 `SparseTensor` to `sparse_fill_empty_rows` without getting any errors. The only indication I see of it being limited to rank=2 is this message in the documentation:`A SparseTensor with shape [N, M].`\r\n\r\nI suspect that's just a missing runtime check.  Otherwise the spec of the op (which talks in terms of \"rows\") does not make sense to me.\r\n\r\n> \r\n> Example with rank=3:\r\n> \r\n> ```\r\n> tf.sparse.fill_empty_rows(tf.sparse.SparseTensor(indices=[[0, 1, 2]], values=[3.14], dense_shape=[3, 4, 5]), default_value=1.23)\r\n> ```\r\n\r\n", "@benbarsdell  Can you please check @sanjoy's comments and keep us posted ? Thanks!", "Any info on why this was reverted?"]}, {"number": 46171, "title": "Conda_Verification_Error for installing Tensor_flow 2.3.0", "body": "Hello Folks, \r\nI am new to Deep Learning Libraries and stuff. I tried installing tensorflow with conda in a new environment and \r\nI am getting the following error while verifying.\r\n\r\nCondaVerificationError: The package for tensorflow-base located at C:\\Users\\yuvid\\Downloads\\Softs\\Installed\\Miniconda3\\pkgs\\tensorflow-base-2.3.0-eigen_py37h17acbac_0\r\nappears to be corrupted. The path 'Lib/site-packages/tensorflow/include/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen/mlir/Dialect/Affine/IR/AffineMemoryOpInterfaces.cpp.inc'\r\nspecified in the package manifest cannot be found.\r\n\r\n\r\nClobberError: The package 'defaults/win-64::six-1.15.0-py37haa95532_0' cannot be installed due to a\r\npath collision for 'lib/site-packages/wheel/__pycache__/__init__.cpython-37.pyc'.\r\nThis path already exists in the target prefix, and it won't be removed\r\nby an uninstall action in this transaction. The path is one that conda\r\ndoesn't recognize. It may have been created by another package manager.\r\n\r\n\r\nClobberError: The package 'defaults/win-64::six-1.15.0-py37haa95532_0' cannot be installed due to a\r\npath collision for 'lib/site-packages/wheel/__pycache__/bdist_wheel.cpython-37.pyc'.\r\nThis path already exists in the target prefix, and it won't be removed\r\nby an uninstall action in this transaction. The path is one that conda\r\ndoesn't recognize. It may have been created by another package manager.\r\n\r\n\r\nClobberError: The package 'defaults/win-64::markdown-3.3.3-py37haa95532_0' cannot be installed due to a\r\npath collision for 'lib/site-packages/wheel/__pycache__/pkginfo.cpython-37.pyc'.\r\nThis path already exists in the target prefix, and it won't be removed\r\nby an uninstall action in this transaction. The path is one that conda\r\ndoesn't recognize. It may have been created by another package manager.\r\n\r\nClobberError: This transaction has incompatible packages due to a shared path.\r\n  packages: defaults/win-64::six-1.15.0-py37haa95532_0, defaults/win-64::win_inet_pton-1.1.0-py37haa95532_0, defaults/win-64::cffi-1.14.4-py37hcd4344a_0, defaults/win-64::yarl-1.6.3-py37h2bbff1b_0, defaults/win-64::markdown-3.3.3-py37haa95532_0\r\n  path: 'lib/site-packages/wheel/__pycache__/wheelfile.cpython-37.pyc'\r\n\r\nN many more similar errors just there is change in the file name of pyc\r\n\r\n\r\nHope anyone can help me out.", "comments": ["@Yuvraj-Dhepe \r\n\r\nSorry, but we don't provide support for issues with the conda environment.\r\nThis issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!", "> \r\n> \r\n> @Yuvraj-Dhepe\r\n> \r\n> Sorry, but we don't provide support for issues with the conda environment.\r\n> This issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\n> Please post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!\r\n\r\nThank you I will do the same. I will close the issue.\r\n"]}, {"number": 46170, "title": "Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate", "body": "Apologies if this is in the incorrect category, but is more clarification that I am looking for rather than an issue with the implemented code. \r\n\r\n**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.1 LTS**\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\nTensorFlow installed from (source or binary): **Source**\r\nTensorFlow version (use command below): **v2.3.1**\r\nPython version: N/A\r\nBazel version (if compiling from source): **3.1.0**\r\nGCC/Compiler version (if compiling from source): **6.3.0**\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI'm currently looking into benchmarking a custom delegate against XNNPACK. I've done so through the second option specified in the tensorflow documentation (using the external delegate options within the benchmark tool). \r\n\r\nI downloaded the binary for the benchmarking tool itself here (linux aarch64) rather than building source: https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary\r\n\r\nWhen I run the benchmarking tool for the custom delegate, I get the info message specified in the title: \r\n_\"Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate\"_\r\n\r\nI'm not sure what this is telling me. I've looked through the the tensorflow codebase and, from where I've looked, these output messages are no where in **_tensorflow/tensorflow/lite_**. Going up a level or two, I still can't find anything of similar. \r\n\r\nWhen running with xnnpack set to true, I get the output message _**\"Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\"**_. This is a little more self explanatory, and I can see that some of the code for this message was generated at tensorflow/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc:666.\r\n\r\nI added some debug information within the custom delegate and I can see that, when running the benchmark_model tool, I am creating the optimized delegate graph I expect. I am also seeing the numbers that I expect to see for the benchmarking against XNNPACK.\r\n\r\nHowever, the message seems to make it out as if the custom delegate has been created, but is not used during the benchmarking. Is this true? I would appreciate any clarity at your earliest convenience. Thank you!", "comments": ["Bump on this. I could really do with some information today if at all possible", "> Apologies if this is in the incorrect category, but is more clarification that I am looking for rather than an issue with the implemented code.\r\n> \r\n> **System information**\r\n> \r\n> Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n> OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 20.04.1 LTS**\r\n> Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n> TensorFlow installed from (source or binary): **Source**\r\n> TensorFlow version (use command below): **v2.3.1**\r\n> Python version: N/A\r\n> Bazel version (if compiling from source): **3.1.0**\r\n> GCC/Compiler version (if compiling from source): **6.3.0**\r\n> CUDA/cuDNN version: N/A\r\n> GPU model and memory: N/A\r\n> \r\n> **Describe the current behavior**\r\n> \r\n> I'm currently looking into benchmarking a custom delegate against XNNPACK. I've done so through the second option specified in the tensorflow documentation (using the external delegate options within the benchmark tool).\r\n> \r\n> I downloaded the binary for the benchmarking tool itself here (linux aarch64) rather than building source: https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary\r\n> \r\n> When I run the benchmarking tool for the custom delegate, I get the info message specified in the title:\r\n> _\"Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate\"_\r\n\r\nI believe this means the external delegate is applied successfully based on the return value of ModifyGraphWithDelegate invoke, however, no ops in the TFLite model graph are found that could be delegated by this EXTERNAL delegate.\r\n\r\n> \r\n> I'm not sure what this is telling me. I've looked through the the tensorflow codebase and, from where I've looked, these output messages are no where in **_tensorflow/tensorflow/lite_**. Going up a level or two, I still can't find anything of similar.\r\n\r\nThe above info message is emitted from this part: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L626-L659\r\n\r\n> \r\n> When running with xnnpack set to true, I get the output message _**\"Explicitly applied XNNPACK delegate, and the model graph will be partially executed by the delegate w/ 4 delegate kernels.\"**_. This is a little more self explanatory, and I can see that some of the code for this message was generated at tensorflow/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc:666.\r\n> \r\n> I added some debug information within the custom delegate and I can see that, when running the benchmark_model tool, I am creating the optimized delegate graph I expect. I am also seeing the numbers that I expect to see for the benchmarking against XNNPACK.\r\n> \r\n> However, the message seems to make it out as if the custom delegate has been created, but is not used during the benchmarking. Is this true? I would appreciate any clarity at your earliest convenience. Thank you!\r\n\r\n", "Thank you very much for the response - I appreciate it.\r\n\r\nCould you tell me is there somewhere I can download a specific version of the benchmark_model tool? I downloaded from here: \r\nhttps://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary\r\n\r\nBut I have a mismatch on tensorflow versioning, which may or may not be interfering with the logging that I am supposed to be getting. My tensorflow version is 2.3.1, whereas the benchmark_tool is nightly of course", "> Thank you very much for the response - I appreciate it.\r\n> \r\n> Could you tell me is there somewhere I can download a specific version of the benchmark_model tool? I downloaded from here:\r\n> https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary\r\n> \r\n\r\nAcked. Which version (like the date of the binary was built) do you want to download? Such pre-built binaries were only enabled  recently. So, I'm not sure whether we will have the version you'd like to use.\r\n\r\n> But I have a mismatch on tensorflow versioning, which may or may not be interfering with the logging that I am supposed to be getting. My tensorflow version is 2.3.1, whereas the benchmark_tool is nightly of course\r\n\r\nAcked. Could you build (or backport) the benchmark_tool with your repo? Or update the EXTERNAL delegate w/ latest TF repo?\r\n\r\n", "We are currently looking into building the benchmark_model tool with our repo, however building with bazel can be quite tedious with our codebase. So that's a process underway. \r\n\r\nAlso, in relation to updating TF, we recently updated the external delegate and the rest of the code base to 2.3.1. It is a relatively lengthy process to align our code so unfortunately it is not possible under my time constraint.\r\n\r\nNo worries, ideally I could have the latest binary that aligned with the tensorflow 2.3.1 release, which appears to be this commit from the **21st September 2020 at 18:57pm** fcc4b966f1265f466e82617020af93670141b009 (HEAD, tag: v2.3.1).\r\n\r\nThank you", "> We are currently looking into building the benchmark_model tool with our repo, however building with bazel can be quite tedious with our codebase. So that's a process underway.\r\n\r\nAcked. An alternative to bazel might be backporting [TFLite's CMake support](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#step-6-build-tensorflow-lite-benchmark-tool) to your repo?\r\n> \r\n> Also, in relation to updating TF, we recently updated the external delegate and the rest of the code base to 2.3.1. It is a relatively lengthy process to align our code so unfortunately it is not possible under my time constraint.\r\n> \r\n> No worries, ideally I could have the latest binary that aligned with the tensorflow 2.3.1 release, which appears to be this commit from the **21st September 2020 at 18:57pm** [fcc4b96](https://github.com/tensorflow/tensorflow/commit/fcc4b966f1265f466e82617020af93670141b009) (HEAD, tag: v2.3.1).\r\n\r\nThe closest one I could find is this [one](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/android_aarch64_benchmark_model).\r\n\r\n> \r\n> Thank you\r\n\r\n", "Thank you for all the information. It was really helpful!\r\n\r\nWe have managed to build on x86 and can clarify that the peculiar message observed was due to the mismatch in tensorflow versions from the binary vs. our tensorflow source", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46170\">No</a>\n", "> Acked. An alternative to bazel might be backporting [TFLite's CMake support](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#step-6-build-tensorflow-lite-benchmark-tool) to your repo?\r\n> \r\n> The closest one I could find is this [one](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/android_aarch64_benchmark_model).\r\n\r\nCould I please get the android arm 64 version of this as well please? ", "> > Acked. An alternative to bazel might be backporting [TFLite's CMake support](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#step-6-build-tensorflow-lite-benchmark-tool) to your repo?\r\n> > The closest one I could find is this [one](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/android_aarch64_benchmark_model).\r\n> \r\n> Could I please get the android arm 64 version of this as well please?\r\n\r\nThe above one is built for android arm64. Did you mean 32bit? Then it's this [link](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/android_arm_benchmark_model). Based on the URL, I guess one could spot the pattern.\r\n", "Oh sorry apologies - I meant linux aarch64", "> > > Acked. An alternative to bazel might be backporting [TFLite's CMake support](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md#step-6-build-tensorflow-lite-benchmark-tool) to your repo?\r\n> > > The closest one I could find is this [one](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/android_aarch64_benchmark_model).\r\n> > \r\n> > \r\n> > Could I please get the android arm 64 version of this as well please?\r\n> \r\n> The above one is built for android arm64. Did you mean 32bit? Then it's this [link](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/android_arm_benchmark_model). Based on the URL, I guess one could spot the pattern.\r\n\r\n\r\n\r\n> Oh sorry apologies - I meant linux aarch64\r\n\r\nNP. [Here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/lite/release/tools/nightly/41/20200921-215507/linux_aarch64_benchmark_model) you go.", "Thank you! Could you tell me which revision that is based on as well please? ", "> Thank you! Could you tell me which revision that is based on as well please?\r\n\r\nIt's based on https://github.com/tensorflow/tensorflow/commit/1a2b1d0ae199dd328bff8eb6c6dfb74ac8b79c61", "Great thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46170\">No</a>\n", "Hi there,\r\n\r\nWe looked a bit more into this issue and it seems at this point:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L631\r\n\r\ndelegate.get() is returning the address of the ExternalDelegateWrapper rather than the external delegate itself,\r\nwhere as node.delegate correctly contains the address of the external delegate.\r\n\r\nThis mismatch is then causing the benchmark_model to believe the delegate isn't doing anything.", "Reopening due to Finn's comment", "> Hi there,\r\n> \r\n> We looked a bit more into this issue and it seems at this point:\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L631\r\n> \r\n> delegate.get() is returning the address of the ExternalDelegateWrapper rather than the external delegate itself,\r\n> where as node.delegate correctly contains the address of the external delegate.\r\n> \r\n> This mismatch is then causing the benchmark_model to believe the delegate isn't doing anything.\r\n\r\nAcked. Is it possible that your \"external delegate\" has a different implementation from the semantics of the external delegate that could be consumed by the benchmark model tool (including other TFLite tools and unit tests)? Could you share how is your external delegate is implemented? \r\n\r\nNote that we have a simple [\"external delegate\" example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/external_delegate_adaptor.cc) that we believe could be successfully applied in the tooling as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/utils/dummy_delegate/README.md#option-2-utilize-tensorflow-lite-external-delegate). Meanwhile, I'll look into this \"mismatch\" issue w.r.t. \"external delegate\" with a more sophisticated example.", "Thank you for the response, the header file we are using is here: https://review.mlplatform.org/c/ml/armnn/+/4728/6/delegate/src/armnn_external_delegate.cpp\r\nIt has the same function signature as the example you linked, from our own tests the delegate is working normally with the exception of this message.", "> Thank you for the response, the header file we are using is here: https://review.mlplatform.org/c/ml/armnn/+/4728/6/delegate/src/armnn_external_delegate.cpp\r\n> It has the same function signature as the example you linked, from our own tests the delegate is working normally with the exception of this message.\r\n\r\nThx for providing this info! After looking into the issue with a more sophisticated example, I did see there's a problem as you mentioned earlier. We will get this fixed asap.", "> > Thank you for the response, the header file we are using is here: https://review.mlplatform.org/c/ml/armnn/+/4728/6/delegate/src/armnn_external_delegate.cpp\r\n> > It has the same function signature as the example you linked, from our own tests the delegate is working normally with the exception of this message.\r\n> \r\n> Thx for providing this info! After looking into the issue with a more sophisticated example, I did see there's a problem as you mentioned earlier. We will get this fixed asap.\r\n\r\n\r\n\r\n> > Thank you for the response, the header file we are using is here: https://review.mlplatform.org/c/ml/armnn/+/4728/6/delegate/src/armnn_external_delegate.cpp\r\n> > It has the same function signature as the example you linked, from our own tests the delegate is working normally with the exception of this message.\r\n> \r\n> Thx for providing this info! After looking into the issue with a more sophisticated example, I did see there's a problem as you mentioned earlier. We will get this fixed asap.\r\n\r\nAfter digging into the code, I think the output message \"Though EXTERNAL delegate is explicitly applied, the model will not be executed by the delegate\" is likely a false alarm here. Similar to what's mentioned [above](https://github.com/tensorflow/tensorflow/issues/46170#issuecomment-765334686), basically, when checking how many delegate kernels have been created by a delegate, the benchmark model tool compared the pointer of the external delegate wrapper with the pointer of the actual external delegate to tell whether there's a newly created delegate kernel ([code](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L631-L636)). Therefore, even when the actual external delegate is successfully applied, this comparison will return false and miss counting the actually to-be-delegated node.\r\n\r\nI've made a fix, and verifies it w/ the xnnpack delegate (but loading it as an external delegate) on the mobilenet-v2 float model. Here's the output I've got, and now you can see the \"EXTERNAL\" delegate is yielding 1 partition (i.e. one delegate kernel).\r\n\"\r\n.....\r\nGraph: [/tmp/m2.tflite]\r\nExternal delegate path: [/tmp/xnnpack_as_external_delegate.so]\r\nLoaded model /tmp/m2.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Replacing 66 node(s) with delegate (TfLiteXNNPackDelegate) node, yielding 1 partitions.\r\nExplicitly applied EXTERNAL delegate, and the model graph will be completely executed by the delegate.\r\n......\r\n\"", "Just tried your patch and it works! Thanks very much for the quick fix :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46170\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46170\">No</a>\n"]}, {"number": 46169, "title": "The training job is stuck because the rpc server is not started in the evaluator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\ntf version: 2.4\r\ncluster spec: \r\n```\r\n{\r\n    \"ps\":[\r\n        \"localhost:13344\"\r\n    ],\r\n    \"chief\":[\r\n        \"localhost:15881\"\r\n    ],\r\n    \"worker\":[\r\n        \"localhost:37309\",\r\n        \"localhost:30812\"\r\n    ],\r\n    \"evaluator\":[\r\n        \"localhost:22944\"\r\n    ]\r\n}\r\n```\r\nexample code:\r\n```python\r\ndef main(argv):\r\n    cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\r\n    if cluster_resolver.task_type in ('ps', 'worker'):\r\n        logging.info(\"[{}] Start {}({})...\".format(get_cur_time(), cluster_resolver.task_type, cluster_resolver.task_id))\r\n        server = tf.distribute.Server(\r\n            cluster_resolver.cluster_spec(),\r\n            job_name=cluster_resolver.task_type,\r\n            task_index=cluster_resolver.task_id,\r\n            protocol=cluster_resolver.rpc_layer or \"grpc\",\r\n            start=True)\r\n        server.join()\r\n\r\n    if cluster_resolver.task_type == 'evaluator':\r\n        ...\r\n        checkpoint = tf.train.Checkpoint(model=model)\r\n        ...\r\n\r\n    if cluster_resolver.task_type == 'chief':\r\n        logging.info(\"[{}] Start {}({})...\".format(get_cur_time(), cluster_resolver.task_type, cluster_resolver.task_id))\r\n\r\n        variable_partitioner = (\r\n            tf.distribute.experimental.partitioners.FixedShardsPartitioner(\r\n                num_shards=NUM_PS))\r\n\r\n        strategy = tf.distribute.experimental.ParameterServerStrategy(\r\n            cluster_resolver,\r\n            variable_partitioner=variable_partitioner)\r\n\r\n        ...\r\n```\r\nWhen i run tensorflow2.4 job with Custom Training Loop using ParameterServerStrategy on Yarn and use the same cluster spec on every process, it is stucked. \r\nFinally, I found that the GRPC Server was not started by the **evaluator** process, which caused the **coordinator** to wait all the time.\r\n\r\n**Describe the expected behavior**\r\nAccording to the problem described above, there are two solutions\uff1a\r\n1. in evaluator process, it start the grpc server. like the following code. Besides, it should add some related doc.\r\n```python\r\nif cluster_resolver.task_type == 'evaluator':\r\n        server = tf.distribute.Server(\r\n            cluster_resolver.cluster_spec(),\r\n            job_name=cluster_resolver.task_type,\r\n            task_index=cluster_resolver.task_id,\r\n            protocol=cluster_resolver.rpc_layer or \"grpc\",\r\n            start=True)\r\n        # dont need to call server.join()\r\n        checkpoint = tf.train.Checkpoint(model=model)\r\n        ...\r\n```\r\n2. In the PS strategy code of tf2.4, the **evaluator** type should be dynamically ignored to avoid waiting for a long time in **coordinator**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @zuston, just to clarify a few things--does your code run if you remove the evaluator from the cluster? And are there any logs you can share? If I understand correctly, you're suggesting that the even if the evaluator fails to start, the training process should go on?", "Thanks reply @nikitamaia . \r\n\r\n> does your code run if you remove the evaluator from the cluster? \r\n\r\nYes. when i remove the evaluator from cluster spec, it can run. \r\nI want to clarify that during the process, the evaluator did not start the rpc server in my code.\r\n\r\nSo I make extra expriments on PSS. I found that when evaluator in cluster spec and starting the rpc server in my code about evaluator, the training job can run normally.\r\n\r\nSo i think it's unreasonable. According to the TF documentation ([link](https://www.tensorflow.org/tutorials/distribute/parameter_server_training#cluster_setup)), the evaluator is not a member of the entire training job, and it is only responsible for checking checkpoints and evaluating. Therefore coordinator should ignore cluster spec about evaluator. Currently, i think coordinator will wait to create rpc channel with evaluator, so it will hang.\r\n\r\n> And are there any logs you can share?\r\n\r\nOK. i will attach it later.\r\n\r\n> If I understand correctly, you're suggesting that the even if the evaluator fails to start, the training process should go on?\r\n\r\nYes, but not exactly. I think that evaluator don't need to start rpc server.", "Attach related logs. \r\n\r\nCoordinator (chief) logs\r\n```\r\nI0105 12:12:10.884592 140192272869184 train.py:53] [2021-01-05 12:12:10] Create variable partitioner...\r\nI0105 12:12:10.885219 140192272869184 train.py:55] [2021-01-05 12:12:10] Create strategy...\r\n2021-01-05 12:12:10.886007: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 12:12:10.886360: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native:/usr/java/default/jre/lib/amd64/server\r\n2021-01-05 12:12:10.886390: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-01-05 12:12:10.886414: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (localhost): /proc/driver/nvidia/version does not exist\r\n2021-01-05 12:12:10.887107: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-05 12:12:10.897641: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\r\nI0105 12:12:10.909158 140192272869184 parameter_server_strategy.py:323] ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\r\nINFO:tensorflow:`tf.distribute.experimental.ParameterServerStrategy` is initialized with cluster_spec: ClusterSpec({'chief': ['localhost:11524'], 'evaluator': ['localhost:36867'], 'ps': ['localhost:35270'], 'worker': ['localhost:24572', 'localhost:10920']})\r\nI0105 12:12:10.909924 140192272869184 parameter_server_strategy_v2.py:430] `tf.distribute.experimental.ParameterServerStrategy` is initialized with cluster_spec: ClusterSpec({'chief': ['localhost:11524'], 'evaluator': ['localhost:36867'], 'ps': ['localhost:35270'], 'worker': ['localhost:24572', 'localhost:10920']})\r\nINFO:tensorflow:ParameterServerStrategyV2 is now connecting to cluster with cluster_spec: ClusterSpec({'chief': ['localhost:11524'], 'evaluator': ['localhost:36867'], 'ps': ['localhost:35270'], 'worker': ['localhost:24572', 'localhost:10920']})\r\nI0105 12:12:10.911811 140192272869184 parameter_server_strategy_v2.py:461] ParameterServerStrategyV2 is now connecting to cluster with cluster_spec: ClusterSpec({'chief': ['localhost:11524'], 'evaluator': ['localhost:36867'], 'ps': ['localhost:35270'], 'worker': ['localhost:24572', 'localhost:10920']})\r\n2021-01-05 12:12:10.912644: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 12:12:10.938734: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief -> {0 -> localhost:11524}\r\n2021-01-05 12:12:10.938802: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job evaluator -> {0 -> localhost:36867}\r\n2021-01-05 12:12:10.938813: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job ps -> {0 -> localhost:35270}\r\n2021-01-05 12:12:10.938820: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:24572, 1 -> localhost:10920}\r\n```\r\n\r\nworker and ps:\r\n```\r\nI0105 12:11:59.622877 140338002704192 train.py:46] [2021-01-05 12:11:59] NUM_WORKERS: 2, global_batch_size: 2048\r\nI0105 12:11:59.623015 140338002704192 train.py:198] [2021-01-05 12:11:59] Start worker(0)...\r\n2021-01-05 12:11:59.623634: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-05 12:11:59.659345: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 12:11:59.659649: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native:/usr/java/default/jre/lib/amd64/server\r\n2021-01-05 12:11:59.659702: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-01-05 12:11:59.659733: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (localhost): /proc/driver/nvidia/version does not exist\r\n2021-01-05 12:11:59.714894: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job chief -> {0 -> localhost:11524}\r\n2021-01-05 12:11:59.714962: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job evaluator -> {0 -> localhost:36867}\r\n2021-01-05 12:11:59.714973: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job ps -> {0 -> localhost:35270}\r\n2021-01-05 12:11:59.714980: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:24572, 1 -> localhost:10920}\r\n2021-01-05 12:11:59.795589: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:24572\r\n```\r\n\r\nevaluator log (not start rpc server):\r\n```\r\n2021-01-05 12:11:45.403783: E tensorflow/core/platform/hadoop/hadoop_file_system.cc:115] HadoopFileSystem load error: /usr/lib/hadoop-hdfs/lib/native/libhdfs.so: cannot open shared object file: No such file or directory\r\n2021-01-05 12:11:47.924755: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-01-05 12:11:47.924980: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/hadoop/lib/native:/usr/java/default/jre/lib/amd64/server\r\n2021-01-05 12:11:47.924998: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-01-05 12:11:47.925016: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (localhost): /proc/driver/nvidia/version does not exist\r\n2021-01-05 12:11:47.925418: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-01-05 12:11:47.928078: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO:tensorflow:Waiting for new checkpoint at hdfs://namenode03-xxxx.hadoop/data/xxx/ranking/xxxxxx/2020-12-25-00/checkpoint\r\nI0105 12:11:58.545091 140620859508544 checkpoint_utils.py:139] Waiting for new checkpoint at hdfs://namenode03-xxxx.hadoop/data/xxx/ranking/xxxxxx/2020-12-25-00/checkpoint\r\n\r\n```", "Any update on it? @nikitamaia ", "Hi, the strategy is probably not at the best position to remove a job from cluster resolvers. As described in the tutorial, training and evaluation clusters are two separate clusters and they should not connect to each other. See https://www.tensorflow.org/tutorials/distribute/parameter_server_training#cluster_setup.\r\n\r\nAlternatively, we would recommend you try out inline evaluation and give us feedbacks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46169\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46169\">No</a>\n", "Thanks your reply. @yuefengz \r\nAs stated in the above TF doc, the evaluator is indeed not part of the training cluster. But it does not have this problem in TF1.x PS strategy, and related kubeflow/tf-operator and linkedin/tony are also used in this way.\r\n\r\nNow it seems that we need to adapt to TF2.x single-client PS strategy in related project.", "With V1 PSStrategy, we indeed had some hacks to split the training and evaluation cluster. It makes sense for V2 ParameterServerStrategy to strip the evaluator. But one concern I have is the likelihood of unifying the interfaces of ParameterServerStrategy and TPUStrategy in the future. In that case, we will probably require users to call `tf.config.experimental_connect_to_cluster` before creating the strategy instance and then users will have to strip the evaluators from TF_CONFIG themselves. The transition would be not smooth if we have this magic right now. Therefore, we started with the least magical way and once we the API becomes more stable, we can consider adding these magics.", "Got it. Thank you for your patience~"]}, {"number": 46167, "title": "[PluggableDevice] Enable DefaultDevice for ops who only have host code.", "body": "Enable some ops with DefaultDevice for PluggableDevice.", "comments": ["@penpornk @annarev  This PR is for some ops with DefaultDevice in plugin, please help to review. Thanks.", "> This change looks good to me. The only question I have is whether it makes sense to have both `DEVICE_CPU` and `DEVICE_DEFAULT` registrations or whether these should be merged into one. @sanjoy, @gunan in case you know preference in this case.\r\n\r\nSeems we can't merge `DEVICE_CPU` and `DEVICE_DEFAULT` into one. Because of the colocation mechanism, some unittests will fail due to lack of colocated operators.", "@penpornk This PR is about register kernels with default device in Core TensorFlow. could you also help to review it? Thanks very much. ", "@quintinwang5 Can you please resolve conflicts? Thanks!", "@gbaned I've resolved conflicts. Please review.", "@penpornk @allenlavoie  can you have a review on this PR? some ops(constant op) as `DEVICE_DEFAULT` may help inference performance .Thanks. ", "@jzhoulon Sorry for the delay. I'll try to get to this PR today.", "> @jzhoulon Sorry for the delay. I'll try to get to this PR today.\r\n\r\nThanks @penpornk and @jzhoulon!!"]}, {"number": 46166, "title": "[PluggableDevice] Allow quantized type registration.", "body": "Support quantized types for PluggableDevice's kernel.", "comments": ["@penpornk @annarev  This PR is for the quantized data type support for plugin, please help to review. Thanks.", "@penpornk This PR is related with kernel C API, could you help to review or suggest other reviewers? Thanks very much.", "@saxenasaurabh Could you please help suggest a reviewer for this PR as well? Thank you very much! "]}, {"number": 46164, "title": "Missing header when building Android CMake TF Lite 2.4 ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Android\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.8.3\r\n- Installed using virtualenv? pip? conda?: git/source\r\n- Bazel version (if compiling from source):3.1.0 (not used)\r\n- GCC/Compiler version (if compiling from source): clang 12.0.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A, 32GB ram\r\n\r\n\r\nI am not sure but I think the tensorflow 2.4 branch is missing the file `tensorflow/lite/delegates/gpu/cl/serialization_generated.h`?\r\n\r\nFollowing steps from https://www.tensorflow.org/lite/guide/build_cmake I get the error messages\r\n\r\n```\r\ngmake[2]: *** [CMakeFiles/tensorflow-lite.dir/build.make:960: CMakeFiles/tensorflow-lite.dir/delegates/gpu/cl/kernels/softmax1x1.cc.o] Error 1\r\ngmake[2]: *** [CMakeFiles/tensorflow-lite.dir/build.make:895: CMakeFiles/tensorflow-lite.dir/delegates/gpu/cl/kernels/relu.cc.o] Error 1\r\n1 warning and 1 error generated.\r\nIn file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/softmax.cc:16:\r\nIn file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/softmax.h:20:\r\nIn file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/kernels/gpu_operation.h:22:\r\nIn file included from /Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/arguments.h:24:\r\n/Users/mng/Repositories/tensorflow/tensorflow/lite/delegates/gpu/cl/gpu_object.h:26:10: fatal error:\r\n      'tensorflow/lite/delegates/gpu/cl/serialization_generated.h' file not found\r\n#include \"tensorflow/lite/delegates/gpu/cl/serialization_generated.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\nAnd many others. On master I can see the header exist, I assume the CMakeLists file isn't downloading or generating this file on compilation.\r\n\r\n", "comments": ["OpenCL support is only enabled on master (r2.5)\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#opencl_gpu_delegate", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46164\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46164\">No</a>\n"]}, {"number": 46163, "title": "GPU-delegate null EGLDisplay: lack of Wayland support?", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Mendel Linux 5.0 Eagle** ([Mendel Linux Versions](https://coral.ai/software/#mendel-dev-board))\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Coral Dev Board 1GB** ([details](https://coral.ai/products/dev-board/))\r\n- TensorFlow installed from (source or binary): **source, TensorFlow Lite only**\r\n- TensorFlow version (use command below): **released version 2.3.1**\r\n- Python version: **N/A**\r\n- Bazel version (if compiling from source): **3.10** (installed via `sudo apt install bazel-3.10`)\r\n- GCC/Compiler version (if compiling from source): **gcc 7.5.0**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **Vivante GC7000Lite, ~1GB**\r\n\r\n**Describe the current behavior**\r\n\r\nWhen attempting to run a C++ program linked against `libtensorflowlite_gpu_delegate.so`, an EGLDisplay is\r\nnot properly assigned nor initialized, thus the overall gpu-delegate fails to run.\r\n\r\nThis is the pertinent error message:\r\n```\r\nEGL: Warning: No default display support on wayland\r\nERROR: TfLiteGpuDelegate Init: eglGetDisplay returned nullptr\r\n```\r\nwhich I believe is generated via [line 35 of egl_environment.cc](https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/delegates/gpu/gl/egl_environment.cc#L35)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe Coral Dev Board runs Mendel Linux, which as I understand it runs the Wayland compositor.  Thus _I believe_\r\n`wl_display_connect()` (and possibly other functions) must be called to properly initialize EGL\r\nwithin the TensorFlow Lite gpu-delegate.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe following code specifically shows the above error:\r\n```\r\n#include <stdio.h>\r\n#include <assert.h>\r\n#include <math.h>\r\n\r\n#include <EGL/egl.h>\r\n#include <EGL/eglext.h>\r\n\r\n// gcc simple-egl.cpp -lEGL -lGLESv2 -o simple-egl-display\r\n    \r\nint main(int argc, char *argv[]) {\r\n\r\n    EGLDisplay display;\r\n    EGLSurface surface;\r\n    EGLContext context;\r\n    EGLConfig config;\r\n    EGLBoolean result;\r\n\r\n    display = eglGetDisplay(EGL_DEFAULT_DISPLAY);\r\n    \r\n    if (display == EGL_NO_DISPLAY) {\r\n        fprintf(stderr, \"EGL_NO_DISPLAY...\\n\");\r\n        return 1;\r\n    } \r\n\r\n    // initialize the EGL display connection\r\n    result = eglInitialize(display, NULL, NULL);\r\n    \r\n    if (result == EGL_FALSE) {\r\n        fprintf(stderr, \"Cannot initialize EGL via the default display...\\n\");\r\n        return 1;\r\n    }        \r\n    return 0;\r\n}   \r\n```\r\nVersus this simple program with Wayland support:\r\n```\r\n#include <stdio.h>\r\n#include <assert.h>\r\n#include <math.h>\r\n\r\n#include <EGL/egl.h>\r\n#include <EGL/eglext.h>\r\n\r\n#include <errno.h>\r\n#include <cstring>\r\n#include <wayland-egl.h>\r\n\r\n// gcc simple-egl.cpp -lEGL -lGLESv2 -lwayland-egl -lwayland-client -o simple-egl-display\r\n\r\nint main(int argc, char *argv[]) {\r\n\r\n    EGLNativeDisplayType egl_native_display;\r\n    EGLDisplay egl_display_wayland;\r\n    EGLint major_version, minor_version;\r\n\r\n    EGLDisplay display;\r\n    EGLSurface surface;\r\n    EGLContext context;\r\n    EGLConfig config;\r\n    EGLBoolean result;\r\n\r\n    display = eglGetDisplay(EGL_DEFAULT_DISPLAY);\r\n\r\n    if (display == EGL_NO_DISPLAY) {\r\n        printf(\"EGL NO DISPLAY (Wayland support?)\\n\");\r\n        //return 1;\r\n    } else {\r\n\r\n        // initialize the EGL display connection\r\n        result = eglInitialize(display, NULL, NULL);\r\n\r\n        if (result == EGL_FALSE) {\r\n            fprintf(stderr, \"Can't initialise EGL via the default display...\\n\");\r\n            // return 1;\r\n        }\r\n    }\r\n\r\n    // Requires wayland\r\n    egl_native_display = EGLNativeDisplayType(wl_display_connect(NULL));\r\n    if (egl_native_display == NULL) {\r\n        printf(\"wl_display_connect failed: %s\\n\", strerror(errno));\r\n        return 1;\r\n    } else {\r\n\r\n        // Now initialize EGL\r\n        egl_display_wayland = eglGetDisplay(egl_native_display);\r\n\r\n        if (egl_display_wayland == EGL_NO_DISPLAY){\r\n            printf(\"Could not obtain EGL display (wayland)\\n\");\r\n            return 1;\r\n        }\r\n\r\n        if (!eglInitialize(egl_display_wayland, &major_version, &minor_version))\r\n        {\r\n            printf(\"Could not initialize EGL (wayland)\");\r\n            egl_display_wayland = EGL_NO_DISPLAY;\r\n            return 1;\r\n        }\r\n    }\r\n    return 0;\r\n}\r\n```\r\n", "comments": ["@jfinken Linux-based GPU support is still experimental, and we can't accommodate all Linux-based platforms due to limited resources and testing. However, if there are simple fixes to improve the support story, without requiring API changes, then pull requests are welcome.\r\n", "@impjdi Understood, _many_ thanks for the reply.  If I am able to make progress with simple fixes to potentially improve general GPU support under linux I'll definitely submit a PR.  Thanks again.", "@jfinken \r\nIs this still an issue, could you please verify on latest tf and let us know.", "This is still an issue at TF 2.3.1, so I'll certainly take a look.  Do you have a TF commit or tag that I should target?  Thanks.", "For what it's worth, here is the diff to `tensorflow/lite/delegates/gpu/gl/egl_environment.cc` that adds modest Wayland support.  Note you presumably would want some preprocessor directive to guard if/when _not_ on a Wayland system.  \r\n\r\n```\r\ndiff --git a/tensorflow/lite/delegates/gpu/gl/egl_environment.cc b/tensorflow/lite/delegates/gpu/gl/egl_environment.cc\r\nindex 8ae75acd93..25bf9ab7be 100644\r\n--- a/tensorflow/lite/delegates/gpu/gl/egl_environment.cc\r\n+++ b/tensorflow/lite/delegates/gpu/gl/egl_environment.cc\r\n@@ -20,6 +20,9 @@ limitations under the License.\r\n #include \"tensorflow/lite/delegates/gpu/gl/gl_call.h\"\r\n #include \"tensorflow/lite/delegates/gpu/gl/request_gpu_info.h\"\r\n\r\n+// TODO preprocessor guards if not on a Wayland system\r\n+#include <wayland-egl.h>\r\n+\r\n namespace tflite {\r\n namespace gpu {\r\n namespace gl {\r\n@@ -29,10 +32,20 @@ namespace {\r\n // and OpenGL ES is reinitialized. See eglMakeCurrent\r\n\r\n absl::Status InitDisplay(EGLDisplay* egl_display) {\r\n+\r\n+  // call wl_display_connect to get the wl display that is then passed into eglGetDisplay\r\n+  EGLDisplay wl_disp = EGLNativeDisplayType(wl_display_connect(NULL));\r\n+  if (wl_disp == NULL) {\r\n+    return absl::InternalError(\"[JF] EGLNativeDisplayType(wl_display_connect(NULL)) returned NULL\");\r\n+  }\r\n+\r\n   RETURN_IF_ERROR(\r\n-      TFLITE_GPU_CALL_EGL(eglGetDisplay, egl_display, EGL_DEFAULT_DISPLAY));\r\n+      TFLITE_GPU_CALL_EGL(eglGetDisplay, egl_display, wl_disp));\r\n   if (*egl_display == EGL_NO_DISPLAY) {\r\n-    return absl::UnavailableError(\"eglGetDisplay returned nullptr\");\r\n+    return absl::UnavailableError(\r\n+        \"eglGetDisplay returned nullptr with a Wayland display\");\r\n   }\r\n```\r\nThis does work for me, with the gpu-delegate, on a Wayland system, with an approved GPU (IMG PowerVR in my case).", "@jfinken \r\nIt looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Can you please execute your code using Latest stable version of TF (2.5) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46163\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46163\">No</a>\n"]}, {"number": 46162, "title": "micro: port op ADD_N from lite", "body": "@tensorflow/micro\r\n\r\nThis issue tracks my work porting operator ADD_N from lite to micro.\r\n\r\nThe port will be submitted in a number of PRs. Here's a rough flight plan per @advaitjain and @petewarden:\r\n\r\nPR 1: Extract the code for parsing the op from a flatbuffer out of ParseOpDataTfLite in tensorflow/lite/core/api/flatbuffer_conversions.cc into a standalone function that can be called from micro's op resolver\r\nPR 2: Extract the reference implementation out of tensorflow/lite/kernels/internal/reference/reference_ops.h into its own header which can be included without dragging in reference_ops.h's dependences\r\nPR 3: Copy operator from lite to micro making minimal changes and not including in the build\r\nPR 4: Delete extra code from the micro copy of the operator\r\nPR 5: Port micro copy of operator as necessary and add a corresponding test\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46162\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46162\">No</a>\n"]}]