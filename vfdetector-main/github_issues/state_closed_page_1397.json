[{"number": 11147, "title": "having problem to identify porn images -> especaly with penises (NO its not a joke!!!)", "body": "i m running a website where a mass on photos are uploaded (upload without registration)... so i get aaaaa lot of penis-trolls or whatever makes them post their private parts... anyhow...\r\n\r\ni tried to train inception model with tensorflow (newest version 1.x) - so i made a   folder with \"penises\" (approx 160 pics) and one with different images with person how dont show their penis (approx 160 pics).... \r\n\r\ntraining accuracy is quit good -> over 90 % - but testing the model with other pictures - it fails really bad on detecting penises.... \r\n\r\nhmmmm, i know.. 160 pics are not that much for training, but i thing the problem is:\r\n\r\nguy on the beach in shorts is: ok\r\nguy on the beach with penis lurking out of his pants: is not okay.. \r\n\r\nbut the difference is quite small between the pictures... because a penis is (mostly) quite a small part of the human body ... so hard to detect... \r\n\r\nanybody could help? and no, its sounds funny, buts no joke.. i suffering under all the uploads i have to review manualy.... it's really not funny to see over 1000 penises a day :(\r\n\r\n\r\ncheers, puck", "comments": ["This sounds like a really hard problem.", "hoho ;)\r\n", "Hopefully you'll get some good tips from people in the community.", "I mean tensorflow is really a head of the curve in solving this type of thing.", "Just hoping we'll get a comment from the members of this  repo.", "hm, trying it again with 1000 pictures per category...  ", "btw_ why does python classify_image.py does not classify humans? ", "even with 1000 pics per category it totally fails on this task", "Growing your sample size should definitely help.", "It should only be a concern if it makes your script run for more than four hours.", "Fuzziness could be a problem, try to make sure your sample images are smooth.", "Also try softer backgrounds to make sure the subject really pops.", "> penis is (mostly) quite a small part of the human body\r\n\r\n#humblebrag", "Sorry, I'm really just unloading my ideas here.", "Are your sample images too similar? You have to make sure there's enough deferens between them to train the script.", "If you get stuck though, don't be afraid to pull-out the big guns and use Watson instead.", "Gotta head to bed. I'll get up in the morning and see if you've whipped out a solution.", "@puckpuck85 i've a working solution for this problem. give me your email will be happy to share some insights. sorry can't upload it publicly. \r\nPS. To build it yourself and from scratch you will need much more than 1000 images per category.", "Maybe https://github.com/yahoo/open_nsfw is helpful? ", "_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\n@vade has posted a good answer for Caffe. Shoutout to @xdumaine for filling the issue tracker with unfunny comments instead of actually helping.\r\n\r\n@puckpuck85 You have a couple of ways of attacking this. Your current method of classifying between different image classes has potential but will need much more training data. As you've said, the difference between many of these images can be rather subtle in some instances and more obvious in others. NSFW content can take so many forms that just 160 images won't cover the breadth of the different types of images you are likely to come across. Looking at the repo @vade posted it appears that they used a large dataset but it is private. This was labelled manually of course.\r\n\r\nIf you want to quickly bolster your dataset you can always scrape images from google images and train on that with inception. I've had surprising success with this! You can then implement a function on your website that if there is a certain uncertainty, it goes to manual review and if it is blocked it gives the user a chance to appeal or whatever. \r\n\r\nYour alternative option (which may be slightly overkill for this situation) is to implement an object detector with annotated objects. Then you can filter out images with have detections. This is a lot of work however as i'm 99% sure that dataset does not already exist.\r\n\r\nFor the meantime I would suggest increasing your dataset. Perhaps you have a repository of \"banned\" images from your website that you could use to train? \r\n\r\nI suggest you move this to Stack Overflow as, whilst your question is extremely valid and the basics of image training, it is neither a bug or feature request and i'm not sure how the devs will feel about this on their issue tracker! I'm sure it will make them chuckle.\r\n\r\nIf you decide to move this issue to Stack Overflow, please post the link here so that myself (or others that stumble upon this issue) can find the S/O thread.", "@jubjamie I made jokes because this is clearly a troll issue. It's a brand new account with 0 activity who's profile picture is [a meme](http://knowyourmeme.com/memes/hide-the-pain-harold). He got you and others to spend time on a fake issue about dick pics. Congrats. You've been bamboozled.", "@xdumaine fair enough. It's not a brand new account and has activity in the area over the last few months. It's still an interesting question and I don't really regret using my saved replies and 2 minutes to reply to it haha. Regardless of context it's still an issue that's seen a number of times both on this issue tracker and stack overflow. I'd rather help someone that might need it than spam the issue tracker with nonsense. \r\nWill leave this to be closed off. Cheers", "i'm not trolling at all - caffe with yahoo NSFW works pretty well, but fails sometimes when it comes to close up shots from the penis. \r\n\r\nbut, i guess, it will filter 90 % - i will take this route and the rest has to be done by user reporting unwanted pics.... when it works good enough for yahoo - i'm fine.\r\n\r\nthread can be closed - i will start the next on stackoverflow. thx all for your help so far.", "@Dim25 please write me the email to -> puckpuck85@flymail24.com\r\n\r\n:)", "For now:\r\n\r\nhttps://github.com/infinitered/nsfwjs", "Yup!  @RWOverdijk mentioned my TFJS lib\r\n\r\npowered by\r\nhttps://github.com/GantMan/nsfw_model"]}, {"number": 11146, "title": "Branch 160538962", "body": "", "comments": ["Jenkins, test this please.", "Jenkins, test this please.\r\n\r\nYeah, somehow that BUILD rule was duplicated.", "Jenkins, test this please.", "Jenkins, test this please.", "Yay! All passed."]}, {"number": 11145, "title": "Fix for contrib.layers test that was raising an IndexError (#11069)", "body": "* variable name change and documentation clarification\r\n\r\n* Updated a test to eliminate an error it raises", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 11144, "title": "MultiRNNCell cannot stack PhasedLSTMCell", "body": "### System information\r\n **TensorFlow version (use command below)**: 1.2\r\n\r\n### Describe the problem\r\ntf.contrib.rnn.PhasedLSTMCell takes a tuple of tensors as inputs (time and features).\r\n\r\nbut the tf.nn.rnn_cell.MultiRNNCell reuse the output of a cell to feed the next one:\r\n\r\n`cur_inp, new_state = cell(cur_inp, cur_state)`\r\n\r\n### Source code / logs\r\n\r\nSomething like that works.\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py)\r\n\r\n```\r\n902c902,903\r\n<     cur_inp = inputs\r\n---\r\n>     times = inputs[0]\r\n>     cur_inp = inputs[1]\r\n916c917\r\n<         cur_inp, new_state = cell(cur_inp, cur_state)\r\n---\r\n>         cur_inp, new_state = cell((times, cur_inp), cur_state)\r\n```\r\n\r\nNot sure whether it is a bug report or a feature request (for a dedicated MultiPhasedLSTMCell) though...", "comments": ["I don't quite understand the question. What is the problem? Could you please elaborate further? ", "Sure.\r\n\r\nFor the reason explained in my first description of the issue, stacking PhasedLSTMCells with MultiRNNCell, e.g.\r\n\r\n```\r\ninputs = tf.placeholder(tf.float32, shape=(batch_size, max_seq_len, 1))            \r\ntimes = tf.placeholder(tf.float32, shape=(batch_size, max_seq_len, 1))\r\n\r\ntargets = tf.placeholder(tf.float32, [batch_size, n_classes])\r\n\r\n# Stacking rnn cells\r\ncells = []\r\nfor i in range(n_layers):            \r\n    cells.append(tf.contrib.rnn.PhasedLSTMCell(n_hidden))\r\n        \r\nstack = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\r\noutputs, _ = tf.nn.dynamic_rnn(stack, (times, inputs), seq_len, dtype=tf.float32)\r\n```\r\n\r\n\r\n\r\n\r\n raises the following error : \r\n\r\n ```\r\nFile \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 206, in _rnn_step\r\n    new_output, new_state = call_cell()\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 708, in <lambda>\r\n    call_cell = lambda: cell(input_t, state)\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 916, in call\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 441, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\", line 1867, in call\r\n    (time, x) = inputs\r\n  File \"/home/jul/.miniconda3/envs/plstm/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 541, in __iter__\r\n    raise TypeError(\"'Tensor' object is not iterable.\")\r\nTypeError: 'Tensor' object is not iterable.\r\n```\r\n\r\nA (maybe not very elegant) option to solve this issue is to write a dedicated MultiPhasedLSTMCell with the patch I provided.", "Hi @julj, so I think I'm still stuck on @ali01's original question: bug or feature? \r\n\r\nAre you saying this is a bug, because you think your code should work, and MultiRNNCell ought to be able to successfully wrap a PhasedLSTMCell? So your exhibitor ought to work but doesn't? \r\n\r\nOr is there some aspect of PhaseLSTMCell that makes it difficult to wrap (perhaps it needs an additional time input to indicate what the phase is?), so that you can't just wrap it? And you're asking for a MultiPhaseLSTMCell that would automatically plug in the time signal? ", "If the MultiRNNCell is supposed to be standard, then I would say it's a bug because the PhaseLSTMCell cannot be stacked using it.", "Could you take a look at this @ebrevdo, please.", "I encountered the same issue using both `BasicLSTMCell` and `LSTMCell`. [This line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L566) errors because `state` is a tensor instead of the expected tuple. I verified that all cells involved had `_state_is_tuple` set to `True`.\r\n\r\nThe documentation is quite a bit off, but it works if you pass in an array of cell states, with a number of elements equal to the number of stacked cells. So something like `multicell(x, [(c1, h1), (c2, h2),...])` works. It doesn't confirm to the recurrent API though. From the top of my head, one fix would be to stack states and let the MultiRNNCell unstack them, so from a user perspective it's still just a tuple state.", "The phased lstm cell should probably pass the time through in the state tuple, with zero_state initializing this value to 0 and each time iteration incrementing it by Delta (a configurable parameter).", "Can we modify MultiRNNCell to handle tuple input? or maybe add a MultiRNNCell specifically for PhraseLSTM or any other similar type of LSTMCells has tuple inputs?", "MultiRNNCell should be able to handle tuple inputs just fine.  It's just\nthat here you want special handling of one element in the tuple.\n\nNo; the solution is to make the time a part of the state rather than an\ninput.\n\nOn Thu, Sep 28, 2017 at 8:17 PM, bingo619 <notifications@github.com> wrote:\n\n> Can we modify MultiRNNCell to handle tuple input? or maybe add a\n> MultiRNNCell specifically for PhraseLSTM or any other similar type of\n> LSTMCells has tuple inputs?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11144#issuecomment-333020793>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimy6LCpAw1lsg_nJLslnyq0Dp_Tw7ks5snGFHgaJpZM4OJm39>\n> .\n>\n", "```\r\n//MultiRNNCell\r\ndef call(self, inputs, state):\r\n    \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\r\n    cur_state_pos = 0\r\n    cur_inp = inputs\r\n    new_states = []\r\n    for i, cell in enumerate(self._cells):\r\n      with vs.variable_scope(\"cell_%d\" % i):\r\n        if self._state_is_tuple:\r\n          if not nest.is_sequence(state):\r\n            raise ValueError(\r\n                \"Expected state to be a tuple of length %d, but received: %s\" %\r\n                (len(self.state_size), state))\r\n          cur_state = state[i]\r\n        else:\r\n          cur_state = array_ops.slice(state, [0, cur_state_pos],\r\n                                      [-1, cell.state_size])\r\n          cur_state_pos += cell.state_size\r\n        cur_inp, new_state = cell(cur_inp, cur_state)\r\n        new_states.append(new_state)\r\n\r\n    new_states = (tuple(new_states) if self._state_is_tuple else\r\n                  array_ops.concat(new_states, 1))\r\n\r\n    return cur_inp, new_states\r\n```\r\n\r\nfrom 2nd layer the time bit is missing, can we feed that?\r\n\r\nI think your solution will work, it is just that the time delta is not always the same.\r\n\r\nSorry if I understand it wrong.", "Then add a time delta argument to the initializer of the PhasedLSTM cell;\nand possibly even an optional start time argument.\n\nOn Thu, Sep 28, 2017 at 9:45 PM, bingo619 <notifications@github.com> wrote:\n\n> //MultiRNNCell\n> def call(self, inputs, state):\n>     \"\"\"Run this multi-layer cell on inputs, starting from state.\"\"\"\n>     cur_state_pos = 0\n>     cur_inp = inputs\n>     new_states = []\n>     for i, cell in enumerate(self._cells):\n>       with vs.variable_scope(\"cell_%d\" % i):\n>         if self._state_is_tuple:\n>           if not nest.is_sequence(state):\n>             raise ValueError(\n>                 \"Expected state to be a tuple of length %d, but received: %s\" %\n>                 (len(self.state_size), state))\n>           cur_state = state[i]\n>         else:\n>           cur_state = array_ops.slice(state, [0, cur_state_pos],\n>                                       [-1, cell.state_size])\n>           cur_state_pos += cell.state_size\n>         cur_inp, new_state = cell(cur_inp, cur_state)\n>         new_states.append(new_state)\n>\n>     new_states = (tuple(new_states) if self._state_is_tuple else\n>                   array_ops.concat(new_states, 1))\n>\n>     return cur_inp, new_states\n>\n> from 2nd layer the time bit is missing, can we feed that?\n>\n> I think your solution will work, it is just that the time delta is not\n> always the same.\n>\n> Sorry if I understand it wrong.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11144#issuecomment-333029692>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8OX8rAQhVHoVOvkBMNrTU1pZPWsks5snHXWgaJpZM4OJm39>\n> .\n>\n", "Sorry, do you mean instead of feeding the time in a tuple, feed it to the initializer? I think if we can do that, that should work.\r\n\r\nI find feeding tuple input is a disaster anyway, it does't work with bidirectional_dynamic_rnn either, I had to write my own. \r\n\r\nAnd it is the same problem when implementing other time variance RNNs, like this one https://www.ijcai.org/proceedings/2017/504", "Right.  CC'ing author of PhasedLSTMCell to discuss feasibility of moving\nthe time component into the state; and modifying the initializer to accept\noptional initial time & time delta arguments.\n\nOn Thu, Sep 28, 2017 at 10:21 PM, bingo619 <notifications@github.com> wrote:\n\n> Sorry, do you mean instead of feeding the time in a tuple, feed it to the\n> initializer? I think if we can do that, that should work.\n>\n> I find feeding tuple input is a disaster anyway, it does't work with\n> bidirectional_dynamic_rnn either, I had to write my own.\n>\n> And it is the same problem when implementing other time variance RNNs,\n> like this one https://www.ijcai.org/proceedings/2017/504\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11144#issuecomment-333033585>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4uqM53gqiWiXtJM_n3wo-tKhtDKks5snH5wgaJpZM4OJm39>\n> .\n>\n", "I don't think feeding the time to the initializer would work since the time\nchanges every time step.\nwe can concatenate the time with the feature tensor and pass a single input\ntensor.\nin practice we pass the time as a feature.\n\nOn Fri, Sep 29, 2017 at 5:12 PM, Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> Right.  CC'ing author of PhasedLSTMCell to discuss feasibility of moving\n> the time component into the state; and modifying the initializer to accept\n> optional initial time & time delta arguments.\n>\n> On Thu, Sep 28, 2017 at 10:21 PM, bingo619 <notifications@github.com>\n> wrote:\n>\n>> Sorry, do you mean instead of feeding the time in a tuple, feed it to the\n>> initializer? I think if we can do that, that should work.\n>>\n>> I find feeding tuple input is a disaster anyway, it does't work with\n>> bidirectional_dynamic_rnn either, I had to write my own.\n>>\n>> And it is the same problem when implementing other time variance RNNs,\n>> like this one https://www.ijcai.org/proceedings/2017/504\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/11144#issuecomment-333033585>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/ABtim4uqM53gqiWiXtJM_n3wo-tKhtDKks5snH5wgaJpZM4OJm39>\n>> .\n>>\n>\n>\n", "No, I mean feed the initial time to the initializer and have the state\ncarry forward a time tensor, adding Delta at every call.\n\nOn Oct 2, 2017 2:45 AM, \"ebrevdo\" <notifications@github.com> wrote:\n\n> I don't think feeding the time to the initializer would work since the time\n> changes every time step.\n> we can concatenate the time with the feature tensor and pass a single input\n> tensor.\n> in practice we pass the time as a feature.\n>\n> On Fri, Sep 29, 2017 at 5:12 PM, Eugene Brevdo <ebrevdo@google.com> wrote:\n>\n> > Right. CC'ing author of PhasedLSTMCell to discuss feasibility of moving\n> > the time component into the state; and modifying the initializer to\n> accept\n> > optional initial time & time delta arguments.\n> >\n> > On Thu, Sep 28, 2017 at 10:21 PM, bingo619 <notifications@github.com>\n> > wrote:\n> >\n> >> Sorry, do you mean instead of feeding the time in a tuple, feed it to\n> the\n> >> initializer? I think if we can do that, that should work.\n> >>\n> >> I find feeding tuple input is a disaster anyway, it does't work with\n> >> bidirectional_dynamic_rnn either, I had to write my own.\n> >>\n> >> And it is the same problem when implementing other time variance RNNs,\n> >> like this one https://www.ijcai.org/proceedings/2017/504\n> >>\n> >> \u2014\n> >> You are receiving this because you were mentioned.\n> >> Reply to this email directly, view it on GitHub\n> >> <https://github.com/tensorflow/tensorflow/issues/\n> 11144#issuecomment-333033585>,\n> >> or mute the thread\n> >> <https://github.com/notifications/unsubscribe-\n> auth/ABtim4uqM53gqiWiXtJM_n3wo-tKhtDKks5snH5wgaJpZM4OJm39>\n> >> .\n> >>\n> >\n> >\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11144#issuecomment-333488062>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim4FC-mhFjN5p-NC86w7Bvaxclq_Wks5soLDVgaJpZM4OJm39>\n> .\n>\n", "I have this problem too when trying to stack multiple `PhasedLSTMCell`s. What's the current status of this issue?", "Contributions welcome to move the time to the state tuple.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "cc += @qlzh727 for context.", "`tf.contrib` module has been depreciated and you can check the fate of the API [here](https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md#list-of-projects).\r\n[MultiRNNCell](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/rnn_cell/MultiRNNCell) and the Tensorflow version 1 behavior and is unlikely to get modified, please reopen this feature request with Tensorfllow 2 compatibility. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 11143, "title": "Parameter server of distributed Tensorflow computes unexpected training operations when using Estimator", "body": "Hi, I am trying to use multi GPUs for the Google's seq2seq training (https://github.com/google/seq2seq) through distributed Tensorflow (data parallelism). \r\n\r\nI launched a parameter server (PS) and three worker processes on a machine equipped with 4 GPUs. \r\nI have each process (including PS) to run on separate GPU through the CUDA_VISIBLE_DEVICES. \r\nI successfully trained the model faster than the single-node version; however, I noticed a weird behavior. \r\n\r\nThe way of enabling data parallelism was to set the ClusterConfig like below:  \r\n`# ps_hosts, worker_hosts, job_name, and task_index are given as program arguments`\r\n`ps_hosts = FLAGS.ps_hosts.split(\",\")`\r\n`worker_hosts = FLAGS.worker_hosts.split(\",\")`\r\n`cluster = {\"ps\": ps_hosts, \"worker\": worker_hosts}`\r\n`os.environ['TF_CONFIG'] = json.dumps({`\r\n`    'cluster': cluster,`\r\n`    'task': {`\r\n`        'type': FLAGS.job_name,`\r\n`        'index': FLAGS.task_index`\r\n`    }`\r\n`})`\r\n`config = run_config.RunConfig( ..... ) `\r\n`estimator = tf.contrib.learn.Estimator(....., config=config)`\r\n`experiment = tf.contrib.learn.Experiment(estimator=estimator, .....)\r\nlearn_runner.run(experiment=experiment, .....)`\r\n\r\nI profiled the training of this machine using nvprof and noticed that the parameter server process also uses its GPU for training. I looked into the device placement log messages and there is no MatMul ops associated with the /job:ps but for some reason, GPU calls many gemm calls. \r\n\r\nI think this issue is not specific to GPU because I also ran the same experiment with PS mapped to CPU but the PS also computes training operations. \r\n\r\nIs there anybody else who has experienced this? Is this the Estimator's bug? \r\n[_get_replica_device_setter](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L201) seems to pass /job:ps/task:%d as worker_device and maybe this makes this problem? \r\n\r\nI also thought about the possibility that I did something wrong in deploying distributed Tensorflow but as mentioned earlier, the model is trained successfully with higher performance so I am confused if this is a bug or a feature. \r\n\r\nI would really appreciate any feedback/comments/help. Please let me know if I am misunderstanding anything.  \r\n\r\n", "comments": ["@jongsae Hello!\r\n~~Could you please provide more details about distributed training for TF.Estimator?\r\nCause for now, many docs for that are already deprecated.~~\r\n\r\nOkay, I made it. But it would be cool, if you share your solution for this.", "I'm also facing same problem, while doing distributed training using ESTIMATOR, my parameter server is also doing training. Although they are just supposed to update parameters.\r\n "]}, {"number": 11142, "title": "Quantization causes some operations to be missing in the graph?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Bazel version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 8.0.61, 5.1\r\n- **GPU model and memory**: Titan X Pascal\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI am currently trying to evaluate the performance of a quantized imagenet pretrained models based on what is available from TF-slim. For my inception models, the quantization works ok (except that if we use quantize_nodes, the performance becomes 20x worse and accuracy drops. Totally the opposite of what we want). However, for the resnet and VGG architectures, it gave me this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"evaluate_from_pb.py\", line 82, in <module>\r\n    tf.import_graph_def(graph_def)\r\n  File \"/home/kwotsin/.local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 331, in import_graph_def\r\n    op_to_bind_to, node.name))\r\nValueError: Specified colocation to an op that does not exist during import: ToFloat_2 in cond/truediv/Switch\r\n\r\n```\r\n\r\nWhich I did not encounter when evaluating my quantized inception models. I am suspecting this might be due to a conversion error during quantization that does not work for some operations that are contained within the VGG_16 and resnet_v1_50 architectures, as defined here:\r\n\r\nVGG_16: https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py#L131\r\nResnet_v1_50: https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py#L241\r\n\r\nYet, due to the limited error traceback produced, I can't seem to identify what went wrong. Because the op name is very strange - something I never named or seen in the model's definition, I suspect it is a quantization op. I don't really know how to reproduce or find this op if I cannot import my graph_def to analyze the graph variables at all.\r\n\r\nThank you.\r\n\r\n", "comments": ["@kwotsin would you please add the sequence of commands you issued to get the error?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11141, "title": "Error while executing the imagenet_train model", "body": "# bazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=256 --train_dir=/tmp --data_dir=/root/kits/dataset\r\nTraceback (most recent call last):\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py\", line 41, in <module>\r\n    tf.app.run()\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py\", line 37, in main\r\n    inception_train.train(dataset)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py\", line 217, in train\r\n    num_preprocess_threads=num_preprocess_threads)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py\", line 136, in distorted_inputs\r\n    num_readers=FLAGS.num_readers)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py\", line 490, in batch_inputs\r\n    example_serialized)\r\n  File \"/root/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py\", line 397, in parse_example_proto\r\n    bbox = tf.concat(0, [ymin, xmin, ymax, xmax])\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1029, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 639, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 704, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 113, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 102, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 370, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/root/anaconda2/envs/tensor/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n\r\nUsing Tensorflow 1.1 inside Anaconda\r\nPython version 3.6.1\r\nMachine - Power 8\r\nOS - Ubuntu 16.04\r\n\r\n**Can someone tell me why am I getting this error?**", "comments": ["Did you mean to file this in https://github.com/tensorflow/models?", "Have you tried the solutions previously posted in these issues:\r\n- https://github.com/tensorflow/tensorflow/issues/8267\r\n- https://github.com/tensorflow/tensorflow/issues/8267", "@skye yes, that is the file.", "I have re-installed tensorflow=1.0.1 and it gave me the same error. But this time I installed it without anaconda environment and with python2.7.\r\nFurther, I came across that the the tensorflow api has changed and so I had to do a \r\n`git checkout master` \r\nin /models/ directory \r\nand this worked."]}, {"number": 11140, "title": "Arbitrary dim for slice", "body": "Add arbitrary dim support for slice op(https://github.com/tensorflow/tensorflow/issues/8873)\r\nThere is still a compiling problem. When build //tensorflow/core/kernel:slice_op_test\uff0cI got the following errors:\r\n```\r\nERROR: /home/parallels/Desktop/github/tensorflow/tensorflow/core/kernels/BUILD:1153:1: Linking of rule '//tensorflow/core/kernels:slice_op_test' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local-opt/bin/tensorflow/core/kernels/slice_op_test '-Wl,-rpath,$ORIGIN/../../../_solib_k8/' -Lbazel-out/local-opt/bin/_solib_k8 -pthread '-fuse-ld=gold' -Wl,-no-as-needed ... (remaining 6 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/slice_op/tensorflow/core/kernels/slice_op.o:slice_op.cc:function tensorflow::SliceOp<Eigen::ThreadPoolDevice, Eigen::QInt8>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::internal::SliceSimple<Eigen::ThreadPoolDevice, Eigen::QInt8>(Eigen::ThreadPoolDevice const&, tensorflow::Tensor*, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<long long> const&)'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/slice_op/tensorflow/core/kernels/slice_op.o:slice_op.cc:function tensorflow::SliceOp<Eigen::ThreadPoolDevice, Eigen::QUInt8>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::internal::SliceSimple<Eigen::ThreadPoolDevice, Eigen::QUInt8>(Eigen::ThreadPoolDevice const&, tensorflow::Tensor*, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<long long> const&)'\r\nbazel-out/local-opt/bin/tensorflow/core/kernels/_objs/slice_op/tensorflow/core/kernels/slice_op.o:slice_op.cc:function tensorflow::SliceOp<Eigen::ThreadPoolDevice, Eigen::QInt32>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::internal::SliceSimple<Eigen::ThreadPoolDevice, Eigen::QInt32>(Eigen::ThreadPoolDevice const&, tensorflow::Tensor*, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<long long> const&)'\r\n```\r\nIt looks like there are some types which haven't be instantiated. However, I can't find that where are these type used. And simply add these instantiation is not a good idea. Could you give some help? @girving", "comments": ["Can one of the admins verify this patch?", "I think the `SliceSimple` problem is that you are declaring instantiations for `Slice` in a file that defines `SliceSimple`.  But the place that uses `Slice` doesn't know that you've already instantiated it, so it instantiates it again (wasting code).  And the file that declares `SliceSimple` doesn't know that `SliceSimple` needs to be used outside of that file, so it does a locally visible instantiation (or perhaps inlines everything.\r\n\r\nBottom line: You should declare instantiations of stuff you define in that file, not other stuff.", "Jenkins, test this please.", "Why are we merging the `cpu_impl` files?  As I understand it, they take a long time so that's why we split them.", "@girving fixed compiling problem and pass python unit test", "@girving could you test this code please?", "Unfortunately I no longer work at Google, and am currently on vacation. \r\n@drpngx: Can you take over?", "Jenkins, test this please.\r\n\r\n@girving yes, on it!", "```\r\nERROR: /workspace/tensorflow/stream_executor/BUILD:39:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:cuda_platform':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_event.cc':\r\n  'bazel-out/local_linux-opt/genfiles/external/local_config_cuda/cuda/include/cuda.h'.\r\n```\r\n@drpngx seems the error is about latest changes in 'tensorflow/stream_executor/cuda/cuda_event.cc'. I update my code to master, Could you test it again? Thank you.", "Jenkins, test this please.", "@drpngx add impl_*.cc files back, pls review and run test", "Jenkins, test this please.", "@aselle could you take a look? Are there benchmarks we can suggest?", "Is there anything I need to do?", "@aselle I've added some benchmark, Please take a look.", "@aselle @drpngx can you please take another look?", "@aselle @drpngx friendly ping", "@aselle @drpngx ping?", "Jenkins, test this please.", "@drpngx uncollapse the for loop by Duff's device, pls take a look. Thank you.", "Jenkins, test this please.", "If you had some benchmarks for before/after, please report them here. That would help.", "Yes, but I can't find the output of benchmark result. I read the rest log, there isn't benchmark result there. Could you tell me where to get the benchmark result? thanks.", "@drpngx ping", "We don't have a benchmark for slice. Can you create a program to run on a large number of random values, exercising both cases when the outer and inner loop have different sizes?", "Jenkins, test this please.", "Could you rebase this PR on top of master?\r\nAlso, was the comment by @drpngx addressed?", "sorry, removed line \">>>>>>>\"", "Jenkins, test this please."]}, {"number": 11139, "title": "No module named '_pywrap_tensorflow_internal'", "body": "---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     17         try:\r\n---> 18             return importlib.import_module(mname)\r\n     19         except ImportError:\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     40     sys.setdlopenflags(_default_dlopen_flags | ctypes.RTLD_GLOBAL)\r\n---> 41   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     42   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 21     _pywrap_tensorflow_internal = swig_import_helper()\r\n     22     del swig_import_helper\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     19         except ImportError:\r\n---> 20             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     21     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127\r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-a649b509054f> in <module>()\r\n----> 1 import tensorflow\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22\r\n     23 # pylint: disable=wildcard-import\r\n---> 24 from tensorflow.python import *\r\n     25 # pylint: enable=wildcard-import\r\n     26\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48\r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50\r\n     51 # Protocol buffers\r\n\r\nD:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     50 for some common reasons and solutions.  Include the entire stack trace\r\n     51 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 52   raise ImportError(msg)\r\n     53\r\n     54 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: \u627e\u4e0d\u5230\u6307\u5b9a\u7684\u6a21\u5757\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\Users\\laiyi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\Users\\laiyi\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["additional information: pytorch seems to [work](https://img.vim-cn.com/1a/2cd097681a938130249c2cd21cdda990101561.jpg) correctly\r\n", "already solved problem\uff0c convert cudnn version from 6.0 to 5.1", "Hi ,\r\ncan you tell me ,how to solved this problem .I got the same problme now  . it is make me crazy ..", "@summerlove66 you should try @laiyi55 's advice! switching to `cudnn-8.0-windows10-x64-v5.1` was the solution for me. even on CUDA 9.0 RC1", "Solved my problem. Note that the cudnn version depends on you tensorflow version. In tensorflow1.3.0 on windows the version of cudnn should be v6.0 or v6.1.", "On Win7, W7, Windows 7 try to install   Microsoft Visual C++ 2015 Redistributable Update 3 \r\n    https://www.microsoft.com/en-us/download/details.aspx?id=53587\r\nfirst. It helped for me.\r\nsee: https://github.com/tensorflow/tensorflow/issues/5949\r\n"]}, {"number": 11138, "title": "Hang when fitting tensorflow learn model which contains crossed sparse_column_with_* columns and the data is loaded with pandas_input_fn", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Run on Databricks GPU cluster (Ubuntu 16.04.1 LTS)\r\n- **TensorFlow installed from (source or binary)**: pip installed tensorflow-gpu\r\n- **TensorFlow version (use command below)**: v1.2.0-rc2-21-g12f033d 1.2.0\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: CUDA: Version 8.0, cuDNN: Version 5.1 for CUDA 8.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nWhen building Linear models with the tensorflow learn libraries the program hangs if the pandas input function (tensorflow.estimator.inputs.pandas_input_fn) is used and a crossed_column (tensorflow.contrib.layers.crossed_column) is constructed from a sparse_column_with_\\*. This issue is not seen in the tensorflow tutorials as they use constant tensor's to input the data which does not scale to large datasets. This also only occurs if crossing sparse_column_with_\\* columns, bucketized continuous columns do not cause a hang.\r\n\r\nThe final line of output before the code hangs is: `INFO:tensorflow:Create CheckpointSaverHook.`\r\n\r\n### Source code / logs\r\n\r\nMinimum working example of bug (with example of code that works and code that breaks):\r\n\r\n```python\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\n# Some sample data\r\ndf = pd.DataFrame({'label' : [0,1,1,0], 'con1' : [10,20,30,40], 'con2' : [1,4,9,16], 'cat1' : ['a','a','b','b'], 'cat2' : ['c','d','c','d']})\r\n\r\n# Sparse base columns\r\nfrom tensorflow.contrib.layers import sparse_column_with_keys, sparse_column_with_hash_bucket\r\nsparse_cat1 = sparse_column_with_keys('cat1', keys=['a','b'])\r\nsparse_cat2 = sparse_column_with_hash_bucket('cat2', hash_bucket_size=10)\r\n\r\n# Bucketised columns\r\nfrom tensorflow.contrib.layers import real_valued_column, bucketized_column\r\nbucket_con1 = bucketized_column(real_valued_column('con1'), boundaries=[5,15,25,35])\r\nbucket_con2 = bucketized_column(real_valued_column('con2'), boundaries=[5,15,25,35])\r\n\r\n# Crossed columns\r\nfrom tensorflow.contrib.layers import crossed_column\r\n# This works with both inputs:\r\ncross_bb = crossed_column([bucket_con1, bucket_con2], hash_bucket_size=100)\r\n# Both these hang with pandas input:\r\ncross_cc = crossed_column([sparse_cat1, sparse_cat2], hash_bucket_size=100)\r\ncross_bc = crossed_column([bucket_con1, sparse_cat1], hash_bucket_size=100)\r\n\r\nfeature_columns = [sparse_cat1, sparse_cat2, cross_bb, cross_cc, cross_bc]\r\nmodel = tf.contrib.learn.LinearClassifier(feature_columns=feature_columns)\r\n\r\n# Use pandas input - doesn't work\r\ntrain_gen_fun = tf.estimator.inputs.pandas_input_fn(df, batch_size=len(df), num_epochs=None, shuffle=True)\r\n# Split out the label from the features\r\ndef input_fn1(gen) :\r\n  features = gen()\r\n  target = features.pop('label')\r\n  return features, target\r\n\r\n# Use constant input - does work\r\ndef input_fn2(df):\r\n  feature_cols = {\r\n    'con1' : tf.constant(df['con1'].values),\r\n    'con2' : tf.constant(df['con2'].values),\r\n    'cat1' : tf.SparseTensor(indices=[[i, 0] for i in range(df['cat1'].size)],\r\n                             values=df['cat1'].values, dense_shape=[df['cat1'].size, 1]),\r\n    'cat2' : tf.SparseTensor(indices=[[i, 0] for i in range(df['cat2'].size)],\r\n                             values=df['cat2'].values, dense_shape=[df['cat2'].size, 1]),\r\n  }\r\n  label = tf.constant(df['label'].values)\r\n  return feature_cols, label\r\n\r\n# This hangs\r\nmodel.fit(input_fn=lambda:input_fn1(train_gen_fun), steps=10)\r\n\r\n# This works\r\nmodel.fit(input_fn=lambda:input_fn2(df), steps=10)\r\n```\r\n", "comments": ["I tried running your script inside the latest nightly docker image, and got: \r\nNameError: name 'real_valued_column' is not defined\r\n\r\nSo I tried patching it with \r\nfrom tensorflow.contrib.layers import real_valued_column\r\n\r\nand got some errors that suggest that wasn't the right thing to do:\r\nWARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\r\n\r\nI'll go look for the v1.2.0-rc2 image, but is there any chance that your script is missing a piece?", "I am sorry I missed the include for real_valued_column I accidentally removed it when simplifying the code to share, as you say simply adding the line `from tensorflow.contrib.layers import real_valued_column` fixes this. I will edit the original to fix this.\r\n\r\nThe warnings you see are warnings not errors and as such are not expected to prevent the algorithm from correctly training. These are also related to the use of the pandas_input_fn I believe but I have not managed to fully understand their source when investigating. They do not prevent the successful running of the above code with the crossed_columns removed and I believe they are unrelated to this problem.\r\n\r\nI have run this script on multiple systems and encountered the same issue so I don't believe it is missing anything. Please also note there are two `model.fit` lines at the bottom, the script will not complete successfully if both these are uncommented I provided both to demonstrate a working case and a not working case. In my running I have found that the bug in the first prevents the running of the second but in the case the first is commented out the second will run. Also if the line `feature_columns = [sparse_cat1, sparse_cat2, cross_bb, cross_cc, cross_bc]` is changed to `feature_columns = [sparse_cat1, sparse_cat2, cross_bb]` then the first will run and it will crash on the second if left uncommented.\r\n\r\nIn your testing did the script complete successfully or did it hang as I saw in my testing? You simply state you saw a warning message not what happened after the warning message.", "@ispirmustafa are you able to take a look or reassign to someone who can?", "@xiejw could you please take a look? it may be related to `pandas_input_fn`.", "@bensowden is this still an issue?", "Nagging Assignee @xiejw: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 11137, "title": "wide_n_deep Tutorial example not working", "body": "**cd** tensorflow/tensorflow/examples/learn/\r\n**python** wide_n_deep_tutorial.py\r\n## it shows that :\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_tutorial.py\", line 36, in <module>\r\n    gender = tf.feature_column.categorical_column_with_vocabulary_list(\r\nAttributeError: 'module' object has no attribute 'feature_column'\r\nMy tensorflow is '1.0.0-rc2', and do i need to upgrade?\r\nthank you! ", "comments": ["Yes you need to upgrade ", "Hi, @terrytangyuan ,  after upgrade the tf, I got another problem while running wide_n_deep_tutorial.py, they say that:\r\n  File \"wide_n_deep_tutorial.py\", line 149, in \r\n    m = tf.estimator.DNNLinearCombinedClassifier(\r\nAttributeError: 'module' object has no attribute 'DNNLinearCombinedClassifier'", "A lot of changes have been merged in today. I think it should work once you try the new nightly build tomorrow or so.", "ok, tomorrow i will try the newest~~~ :D ", "@terrytangyuan  I try the newest wide_n_deep tutorial , but I get the msg like this:\r\n  File \"wide_n_deep_tutorial.py\", line 147, in build_estimator\r\n    m = tf.estimator.DNNLinearCombinedClassifier(\r\nAttributeError: 'module' object has no attribute 'DNNLinearCombinedClassifier'\r\nhow can i run it ?"]}, {"number": 11136, "title": "Windows: Add missing source files declaration for Bazel build", "body": "Fix http://ci.tensorflow.org/job/tf-master-win-bzl/1169/\r\n@gunan ", "comments": ["Testing http://ci.tensorflow.org/job/tensorflow-pr-win-bazel/30/console"]}, {"number": 11135, "title": "TF API r1.2 Keras TimeDistributed wrapper error", "body": "\r\nimport tensorflow.contrib.keras as K\r\nmodel = K.models.Sequential()\r\nmodel.add(K.layers.LSTM(150, batch_input_shape=(batch_size, n_in, encoded_length), stateful=True))\r\nmodel.add(K.layers.RepeatVector(n_out))\r\nmodel.add(K.layers.LSTM(150, return_sequences=True, stateful=True))\r\nmodel.add(K.layers.TimeDistributed(K.layers.Dense(encoded_length, activation='softmax')))\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n\r\n\r\nAttributeError: module 'tensorflow.contrib.keras.api.keras.layers' has no attribute 'TimeDistributed'\r\n\r\nIs there way to avoid that error?\r\n\r\nSystem info:\r\n---------\r\nWindows 10\r\nTensorFlow r1.2\r\nPython 3.6\r\nMiniconda3\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@fchollet ", "`TimeDistributed` is available in `tf.contrib.keras.layers`: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/keras/api/keras/layers/__init__.py#L141\r\n\r\nCan someone verify that this does in fact not work with the latest version? If it doesn't work, can someone who understands API exports in TF advise on what is wrong...", "It works perfectly alright in Python 3.5 and TF (version 1.0.1) and Keras 2.0.1 ", "I have the same problem in TF r1.2 and Python 3.5. If I try to use tf.contrib.keras.layers.TimeDistributed, the same error happens. But if I add the code\r\n`from tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed`\r\nI can use TimeDistributed.\r\n\r\nWhen I use TimeDistributed, I meet another trouble.\r\n`from tensorflow.contrib.keras import layers as KL`\r\n`x = KL.Input(batch_shape=[1,5,64,64,3])`\r\n`y = TimeDistributed(KL.Conv2D(16, kernel_size=[3,3], padding='same', activation='relu'))(x)`\r\n\r\nThen an error happen\r\nTypeError: rnn() got an unexpected keyword argument 'input_length'\r\n\r\nIf the Input layer have batch_shape=[None,5,64,64,3] or just shape=[5,64,64,3], the error will not happen. But I just want to use a stateful ConvLSTM2D layer after conv frames in each time step, so the batch size is neccesery. I don't know how to deal with this problem.\r\n\r\n\r\n", "With TF v1.2.1 I have to use:\r\n```\r\nfrom tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed\r\n```\r\n\r\n@fchollet It looks like the import did not make it into the latest release, but is in master: https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/keras/api/keras/layers/__init__.py\r\n", "Unfortunately, using suggested:\r\nfrom tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed\r\n\r\nPerfectly alright working in older version code gives, the following error message:\r\n\r\n  File \"C:\\Users\\natlun\\AppData\\Local\\Continuum\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\layers\\wrappers.py\", line 194, in call\r\n    unroll=False)\r\n\r\nTypeError: rnn() got an unexpected keyword argument 'input_length'\r\n\r\ni.e., just the same as @WYF-Learning mentioned...", "@martinwicke it looks like something is wrong with the imports/sealing", "Looks like this is resolved on master. It will be fixed in 1.3.", "I am still getting this error in 1.3.0, using example by @WYF-Learning: [1]\r\n\r\nI have compared the argument list of `rnn` in `keras.backend.tensorflow_backend` https://github.com/fchollet/keras/blob/7c7d73530c1ab1b47f9fb5f0612ec13fef1a26c6/keras/backend/tensorflow_backend.py#L2316\r\n\r\nand in `tensorflow.contrib.keras.python.keras.backend` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/keras/python/keras/backend.py#L2471\r\n\r\nthe latter does not have `input_length`, which in Keras is marked as not relevant for Tensorflow backend,\r\nbut it still present in the call to `rnn` in the `tensorflow.contrib.keras.python.keras.layers.wrappers` https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/keras/python/keras/layers/wrappers.py#L203\r\n\r\nremoving fixes it:\r\n\r\n```python\r\n      _, outputs, _ = K.rnn(\r\n          step,\r\n          inputs,\r\n          initial_states=[],\r\n          #input_length=input_shape[1],\r\n          unroll=False)\r\n      y = outputs\r\n    else:\r\n```\r\n\r\n[1]\r\n```bash\r\n$ python\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.3.0'\r\n>>> \r\n>>> from tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed\r\n>>> from tensorflow.contrib.keras import layers as KL\r\n>>> x = KL.Input(batch_shape=[1,5,64,64,3])\r\n>>> y = TimeDistributed(KL.Conv2D(16, kernel_size=[3,3], padding='same', activation='relu'))(x)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/alexeys/.local/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/engine/topology.py\", line 396, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/home/alexeys/.local/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/home/alexeys/.local/lib/python3.6/site-packages/tensorflow/contrib/keras/python/keras/layers/wrappers.py\", line 208, in call\r\n    unroll=False)\r\nTypeError: rnn() got an unexpected keyword argument 'input_length'\r\n```"]}, {"number": 11134, "title": "\"ImportError: No module named '_pywrap_tensorflow'\" while trying to run TensorFlow", "body": "`Traceback (most recent call last):\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"c:\\Python35-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"c:\\Python35-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"c:\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.`\r\n\r\n**SYSTEM DETAILS:**\r\n1. NVIDIA GeForce 920M\r\n2. Cuda Toolkit 8.0\r\n3.  cudNN 5.1 version\r\n4.  Visual Studio 2015, also Visual Studio C++ Redistributable 2015\r\n5. Windows 10", "comments": ["What happens when you follow the advice given at the end of the trace back?", "There's no change whatsoever. I've edited the parent branch and the error persists. ", "_Warning: As you've not filled in the required info above you may not get the support you're looking for from the devs. Perhaps this is an issue for Stack Overflow?_\r\n\r\nPlease can you submit the info that is required by the template when you file an issue. Without it, it is very hard to help further. You can find [the template here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md). \r\n\r\nIn the meantime, see whether [this S/O question](https://stackoverflow.com/q/42011070) helps you.", "Sorry for the irregularities, I've updated the details. Please let me know if anything else is missing\r\n", "@SarthakJShetty Can you try to run the interpreter from a location other than the directory in which TensorFlow is installed?", "As @printdhruv mentioned, you shouldn't run python interpreter from directory where tensorflow is installed. Try, for example, to open new terminal, and type:\r\n\r\n`cd ~`\r\n`python`\r\n`import tensorflow`\r\n\r\nI've also faced similar issue while was launching python in a directory where I've compiled and installed tensorflow. After changing folder problem has disappeared.", "Hey! just fixed the problem. Bash on windows comes with Python 3.x 32 bit in built and had to install Python 3.x 64 bit to fix the issue. TensorFlow working perfectly fine. Thanks for the help!", "Use python3.5 version it solves all the problems. I spent almost weeks on these errors."]}, {"number": 11133, "title": "Add Sublime Tensorflow project in welcome.md", "body": "Sublime Tensorflow is a Sublime Text plugin. It offers you:\r\n\r\n- Autocompletion from a list scrapped from official Tensorflow API documentation.\r\n- Shortcut to check the official doc", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 11132, "title": "Go: SIGABRT when executing the same node more than once", "body": "## Problem\r\n\r\nIn Go, when we pass the same node to the `fetches` list more then once SIGABRT is raised.\r\n\r\n### Source code / logs\r\n\r\n```go\r\npackage poc_test\r\n\r\nimport (\r\n        \"fmt\"\r\n        tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n        \"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n        \"testing\"\r\n)\r\n\r\nfunc TestFunc(t *testing.T) {\r\n        // Create root scope\r\n        root := op.NewScope()\r\n\r\n        // Define graph\r\n\r\n        // Create a constant matrix\r\n        A := op.Const(root.SubScope(\"A\"), [2][2]int32{{1, 2}, {-1, -2}})\r\n        // Create a constant column vector\r\n        b := op.Const(root.SubScope(\"b\"), [2][1]int32{{10}, {100}})\r\n        // Create a matmul operation\r\n        mul := op.MatMul(root.SubScope(\"MatMul\"), A, b)\r\n\r\n        // Finalize the graph\r\n        graph, _ := root.Finalize()\r\n\r\n        // Create the session\r\n        var sess *tf.Session\r\n        sess, _ = tf.NewSession(graph, &tf.SessionOptions{})\r\n        // Run\r\n        var results []*tf.Tensor\r\n        var err error\r\n        if results, err = sess.Run(nil, []tf.Output{mul, mul}, nil); err != nil {\r\n                t.Errorf(err.Error())\r\n        }\r\n        fmt.Println(results[0].Value())\r\n}\r\n```\r\n\r\nHere's the output:\r\n\r\n```out\r\ngo test poc_test.go \r\n2017-06-29 10:46:09.154744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-29 10:46:09.155330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:03:00.0\r\nTotal memory: 10.91GiB\r\nFree memory: 249.38MiB\r\n2017-06-29 10:46:09.267778: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x1f22910 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n2017-06-29 10:46:09.268001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-06-29 10:46:09.268357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] Found device 1 with properties: \r\nname: GeForce GTX 1060 6GB\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.34GiB\r\n2017-06-29 10:46:09.268390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:830] Peer access not supported between device ordinals 0 and 1\r\n2017-06-29 10:46:09.268399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:830] Peer access not supported between device ordinals 1 and 0\r\n2017-06-29 10:46:09.268409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:959] DMA: 0 1 \r\n2017-06-29 10:46:09.268415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:969] 0:   Y N \r\n2017-06-29 10:46:09.268421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:969] 1:   N Y \r\n2017-06-29 10:46:09.268433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1028] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0)\r\n2017-06-29 10:46:09.268440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1028] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)\r\n2017-06-29 10:46:09.290295: F tensorflow/c/c_api.cc:488] Check failed: nelems == 0 (2 vs. 0)\r\nSIGABRT: abort\r\nPC=0x7fa8684dc670 m=0 sigcode=18446744073709551610\r\nsignal arrived during cgo execution\r\n\r\ngoroutine 5 [syscall, locked to thread]:\r\nruntime.cgocall(0x50d580, 0xc420043d68, 0x530100)\r\n        /usr/lib/go/src/runtime/cgocall.go:131 +0xe2 fp=0xc420043d20 sp=0xc420043ce0\r\ngithub.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0x2beb8a0, 0x0, 0x0, 0x0, 0x0, 0xc42000ce40, 0xc420011040, 0xc400000002, 0x0, 0x0, ...)\r\n        github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:703 +0x45 fp=0xc420043d68 sp=0xc420043d20\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0x2beb8a0, 0x0, 0x0, 0x0, 0x0, 0xc42000ce40, 0xc420011040, 0xc400000002, 0x0, 0x0, ...)\r\n        /home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a fp=0xc420043dd8 sp=0xc420043d68\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000ce20, 0x0, 0xc420043f50, 0x2, 0x2, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\r\n        /home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:91 +0x243 fp=0xc420043e70 sp=0xc420043dd8\r\ncommand-line-arguments_test.TestFunc(0xc4200665b0)\r\n        /home/pgaleone/projects/go/src/github.com/galeone/asd/poc_test.go:32 +0x35d fp=0xc420043fa8 sp=0xc420043e70\r\ntesting.tRunner(0xc4200665b0, 0x562278)\r\n        /usr/lib/go/src/testing/testing.go:657 +0x96 fp=0xc420043fd0 sp=0xc420043fa8\r\nruntime.goexit()\r\n        /usr/lib/go/src/runtime/asm_amd64.s:2197 +0x1 fp=0xc420043fd8 sp=0xc420043fd0\r\ncreated by testing.(*T).Run\r\n        /usr/lib/go/src/testing/testing.go:697 +0x2ca\r\n\r\ngoroutine 1 [chan receive]:\r\ntesting.(*T).Run(0xc4200664e0, 0x559659, 0x8, 0x562278, 0xc420053d20)\r\n        /usr/lib/go/src/testing/testing.go:698 +0x2f4\r\ntesting.runTests.func1(0xc4200664e0)\r\n        /usr/lib/go/src/testing/testing.go:882 +0x67\r\ntesting.tRunner(0xc4200664e0, 0xc420053de0)\r\n        /usr/lib/go/src/testing/testing.go:657 +0x96\r\ntesting.runTests(0xc42000cd80, 0x7ecf80, 0x1, 0x1, 0x4131ac)\r\n        /usr/lib/go/src/testing/testing.go:888 +0x2c1\r\ntesting.(*M).Run(0xc420053f20, 0xc420053f20)\r\n        /usr/lib/go/src/testing/testing.go:822 +0xfc\r\nmain.main()\r\n        command-line-arguments/_test/_testmain.go:42 +0xf7\r\n\r\ngoroutine 17 [syscall, locked to thread]:\r\nruntime.goexit()\r\n        /usr/lib/go/src/runtime/asm_amd64.s:2197 +0x1\r\n\r\nrax    0x0\r\nrbx    0x6\r\nrcx    0x7fa8684dc670\r\nrdx    0x0\r\nrdi    0x2\r\nrsi    0x7ffe4515bf50\r\nrbp    0x7ffe4515c1a0\r\nrsp    0x7ffe4515bf50\r\nr8     0x0\r\nr9     0x7ffe4515bf50\r\nr10    0x8\r\nr11    0x246\r\nr12    0x2\r\nr13    0x2\r\nr14    0x2bf8160\r\nr15    0x20\r\nrip    0x7fa8684dc670\r\nrflags 0x246\r\ncs     0x33\r\nfs     0x0\r\ngs     0x0\r\nFAIL    command-line-arguments  0.544s\r\n```\r\n\r\nThe same logic,  in python, works without any issue:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nA = tf.constant([[1,2], [-1, -2]])\r\nb = tf.constant([[10], [100]])\r\n\r\nmul = tf.matmul(A, b)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run([mul, mul]))\r\n\r\n```\r\noutputs\r\n```out\r\n[array([[ 210],\r\n       [-210]], dtype=int32), array([[ 210],\r\n       [-210]], dtype=int32)]\r\n```\r\n\r\nas expected.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Archlinux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.2.0\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: cuda 8, cudnn 5.1\r\n- **GPU model and memory**:  GeForce GTX 1080\r\n- **Exact command to reproduce**: `go test`\r\n", "comments": ["Thanks for the report, it certainly shouldn't fail with a SIGABRT.\r\n\r\n(That said though, am curious about the use case for fetching the same value multiple times).\r\n\r\n", "I was showing that in [`tfgo`](https://github.com/galeone/tfgo) when one assigns a go variable to another go variable it needs to \"clone\" it before the assignment in order to create a different and new node in the graph.\r\nOtherwise, the assignment only exists in Go but the underlying reference points to the same node in the graph.\r\n\r\n(In short, I was showing how to use `tf.assing` and not the assignment operator of the lanuage used).\r\n\r\nTo empathize this, I'd like to show that those 2 Go variables when evaluated contain the same value.\r\nBut I can't because of that bug, thus I fallback showing it in another way. The first example here: https://github.com/galeone/tfgo#getting-started\r\n"]}, {"number": 11131, "title": "how use tensorflow model in optimization model", "body": "hi\r\ni know how to save, restore and feed a tensorflow model but how can i use tf model in optimization model (like the genetic algorithm, python deep package). my problem is in transforming (convert) tf value to python. if I want to use\r\n\r\n`python_variable=sess.run(tf_variable)`\r\n\r\nor\r\n\r\n`python_variable=tf_variable.eval()`\r\n\r\nfor each population (genetic run) I should initialize sess and .... this process is very time-consuming. what is the solution?\r\nis there any better solution or tensorflow have any optimization algorithm?", "comments": ["_Warning: As this doesn't appear to be a bug with Tensorflow, the devs may ask for this to be moved to Stack Overflow._\r\n\r\nIt's not very clear what you're after here. You want to get values of tensors for what purpose? For training in something else? I think this is better asked on Stack Overflow. Good luck!", "yes, it's not a bug but maybe can be as feature request \r\ni asked this question in stackoverflow and also find some similar question without even one answer. so i think i can ask here \r\nabout my question: i fitted tf model on some laboratory test (there is some variable like time and etc).\r\nnow i want to find the optimum value of each variable", "Unfortunately we don't answer usage questions here. Please file a new issue if you think of a specific feature that would help you do this."]}, {"number": 11130, "title": "Add bazel build configs for cpu type 'x64_windows_msvc'", "body": "Without this fix, the final generated whl file will contain a '_pywrap_tensorflow_internal.so' instead of '_pywrap_tensorflow_internal.pyd'", "comments": ["Can one of the admins verify this patch?", "@snnn This change looks good to me, but after https://github.com/tensorflow/tensorflow/commit/08ed32dbb9e8f67eec9efce3807b5bdb3933eb2f, with Bazel 0.5.0 or later version you don't need `--cpu=x64_windows_msvc` to build TensorFlow anymore.\r\nAnd it will be deprecated in future.", "got it."]}, {"number": 11129, "title": "Fix typos", "body": "This PR fixes some typos: `succesfully`, `optimzations`, `unecessary`, `accross`, `trainning`, `beacuse`, and `dont`.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11128, "title": "Make consistent author information format", "body": "This PR makes formatting consistent. All the other formats for author information are described as `et al.` not `et. al.`.", "comments": ["Can one of the admins verify this patch?", "Thanks!\r\n\r\nJenkins, test this please.", "Jenkins, test this please."]}, {"number": 11127, "title": "tf.contrib.keras modules can't Open HDFS file", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:CentOS 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:5.1.10\r\n\r\nI use keras.preprocessing.image.ImageDataGenerator class,I invoke flow_from_directory function,when train_data_dir is a HDFS directory. the script throw exception. I refer the source code.I found the  ImageDataGenerator class not use GFile class to open path.\r\nSo,it's possible to use GFile to open file in tf.contrib.keras modules?\r\n", "comments": ["@fchollet can you comment or redirect? Thanks!", "@fchollet can you give any feedback about it? plz, I have the same problem.", "Keras can open HDF5 files accessible on local filesystem / network. It cannot open HDF5 files on GFS, because GFS does not support HDF5. We have been talking to the HDF group about possibly making that happen in the future.\r\n\r\nTo open HDFS files that aren't on a locally-accessible filesystem, first copy them locally."]}, {"number": 11126, "title": "ios compile failed : checking whether the C compiler works... no", "body": "\r\n\r\n\r\n\r\npls help..\r\n\r\n\r\n\r\n\r\nmy config.log file:\r\n\r\n\r\nThis file contains any messages produced by compilers while\r\nrunning configure, to aid debugging if configure makes a mistake.\r\n\r\nIt was created by Protocol Buffers configure 3.2.0, which was\r\ngenerated by GNU Autoconf 2.69.  Invocation command line was\r\n\r\n  $ ./configure --host=i386-apple-darwin14.0.0 --disable-shared --enable-cross-compile --with-protoc=/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc --prefix=/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386 --exec-prefix=/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386 CFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk CXX= CXXFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -std=c++11 -stdlib=libc++ -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk LDFLAGS=-arch i386 -fembed-bitcode -mios-simulator-version-min=8.0 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/system LIBS=-lc++ -lc++abi\r\n\r\n## --------- ##\r\n## Platform. ##\r\n## --------- ##\r\n\r\nhostname = jakiedeMac.local\r\nuname -m = x86_64\r\nuname -r = 14.0.0\r\nuname -s = Darwin\r\nuname -v = Darwin Kernel Version 14.0.0: Fri Sep 19 00:26:44 PDT 2014; root:xnu-2782.1.97~2/RELEASE_X86_64\r\n\r\n/usr/bin/uname -p = i386\r\n/bin/uname -X     = unknown\r\n\r\n/bin/arch              = unknown\r\n/usr/bin/arch -k       = unknown\r\n/usr/convex/getsysinfo = unknown\r\n/usr/bin/hostinfo      = Mach kernel version:\r\n\t Darwin Kernel Version 14.0.0: Fri Sep 19 00:26:44 PDT 2014; root:xnu-2782.1.97~2/RELEASE_X86_64\r\nKernel configured for up to 4 processors.\r\n4 processors are physically available.\r\n4 processors are logically available.\r\nProcessor type: x86_64h (Intel x86-64h Haswell)\r\nProcessors active: 0 1 2 3\r\nPrimary memory available: 8.00 gigabytes\r\nDefault processor set: 102 tasks, 394 threads, 4 processors\r\nLoad average: 2.19, Mach factor: 1.88\r\n/bin/machine           = unknown\r\n/usr/bin/oslevel       = unknown\r\n/bin/universe          = unknown\r\n\r\nPATH: /usr/local/bin\r\nPATH: /usr/bin\r\nPATH: /bin\r\nPATH: /usr/sbin\r\nPATH: /sbin\r\n\r\n\r\n## ----------- ##\r\n## Core tests. ##\r\n## ----------- ##\r\n\r\nconfigure:2601: checking whether to enable maintainer-specific portions of Makefiles\r\nconfigure:2610: result: yes\r\nconfigure:2685: checking build system type\r\nconfigure:2699: result: x86_64-apple-darwin14.0.0\r\nconfigure:2719: checking host system type\r\nconfigure:2732: result: i386-apple-darwin14.0.0\r\nconfigure:2752: checking target system type\r\nconfigure:2765: result: i386-apple-darwin14.0.0\r\nconfigure:2808: checking for a BSD-compatible install\r\nconfigure:2876: result: /usr/bin/install -c\r\nconfigure:2887: checking whether build environment is sane\r\nconfigure:2942: result: yes\r\nconfigure:3001: checking for i386-apple-darwin14.0.0-strip\r\nconfigure:3031: result: no\r\nconfigure:3041: checking for strip\r\nconfigure:3057: found /usr/bin/strip\r\nconfigure:3068: result: strip\r\nconfigure:3093: checking for a thread-safe mkdir -p\r\nconfigure:3132: result: ./install-sh -c -d\r\nconfigure:3139: checking for gawk\r\nconfigure:3169: result: no\r\nconfigure:3139: checking for mawk\r\nconfigure:3169: result: no\r\nconfigure:3139: checking for nawk\r\nconfigure:3169: result: no\r\nconfigure:3139: checking for awk\r\nconfigure:3155: found /usr/bin/awk\r\nconfigure:3166: result: awk\r\nconfigure:3177: checking whether make sets $(MAKE)\r\nconfigure:3199: result: yes\r\nconfigure:3228: checking whether make supports nested variables\r\nconfigure:3245: result: yes\r\nconfigure:3334: checking whether UID '501' is supported by ustar format\r\nconfigure:3337: result: yes\r\nconfigure:3344: checking whether GID '20' is supported by ustar format\r\nconfigure:3347: result: yes\r\nconfigure:3355: checking how to create a ustar tar archive\r\nconfigure:3366: tar --version\r\nbsdtar 2.8.3 - libarchive 2.8.3\r\nconfigure:3369: $? = 0\r\nconfigure:3409: tardir=conftest.dir && eval tar --format=ustar -chf - \"$tardir\" >conftest.tar\r\nconfigure:3412: $? = 0\r\nconfigure:3416: tar -xf - <conftest.tar\r\nconfigure:3419: $? = 0\r\nconfigure:3421: cat conftest.dir/file\r\nGrepMe\r\nconfigure:3424: $? = 0\r\nconfigure:3437: result: gnutar\r\nconfigure:3515: checking for i386-apple-darwin14.0.0-gcc\r\nconfigure:3545: result: no\r\nconfigure:3555: checking for gcc\r\nconfigure:3571: found /usr/bin/gcc\r\nconfigure:3582: result: gcc\r\nconfigure:3811: checking for C compiler version\r\nconfigure:3820: gcc --version >&5\r\nApple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)\r\nTarget: x86_64-apple-darwin14.0.0\r\nThread model: posix\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nconfigure:3831: $? = 0\r\nconfigure:3820: gcc -v >&5\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)\r\nTarget: x86_64-apple-darwin14.0.0\r\nThread model: posix\r\nconfigure:3831: $? = 0\r\nconfigure:3820: gcc -V >&5\r\nclang: error: argument to '-V' is missing (expected 1 value)\r\nclang: error: no input files\r\nconfigure:3831: $? = 1\r\nconfigure:3820: gcc -qversion >&5\r\nclang: error: unknown argument: '-qversion'\r\nclang: error: no input files\r\nconfigure:3831: $? = 1\r\nconfigure:3851: checking whether the C compiler works\r\nconfigure:3873: gcc -DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk  -arch i386 -fembed-bitcode -mios-simulator-version-min=8.0 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/system conftest.c -lc++ -lc++abi >&5\r\nclang: error: unknown argument: '-fembed-bitcode'\r\nclang: error: unknown argument: '-fembed-bitcode'\r\nconfigure:3877: $? = 1\r\nconfigure:3915: result: no\r\nconfigure: failed program was:\r\n| /* confdefs.h */\r\n| #define PACKAGE_NAME \"Protocol Buffers\"\r\n| #define PACKAGE_TARNAME \"protobuf\"\r\n| #define PACKAGE_VERSION \"3.2.0\"\r\n| #define PACKAGE_STRING \"Protocol Buffers 3.2.0\"\r\n| #define PACKAGE_BUGREPORT \"protobuf@googlegroups.com\"\r\n| #define PACKAGE_URL \"\"\r\n| #define PACKAGE \"protobuf\"\r\n| #define VERSION \"3.2.0\"\r\n| /* end confdefs.h.  */\r\n| \r\n| int\r\n| main ()\r\n| {\r\n| \r\n|   ;\r\n|   return 0;\r\n| }\r\nconfigure:3920: error: in `/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure:3922: error: C compiler cannot create executables\r\nSee `config.log' for more details\r\n\r\n## ---------------- ##\r\n## Cache variables. ##\r\n## ---------------- ##\r\n\r\nac_cv_build=x86_64-apple-darwin14.0.0\r\nac_cv_env_CCC_set=\r\nac_cv_env_CCC_value=\r\nac_cv_env_CC_set=\r\nac_cv_env_CC_value=\r\nac_cv_env_CFLAGS_set=set\r\nac_cv_env_CFLAGS_value='-DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk'\r\nac_cv_env_CPPFLAGS_set=\r\nac_cv_env_CPPFLAGS_value=\r\nac_cv_env_CPP_set=\r\nac_cv_env_CPP_value=\r\nac_cv_env_CXXCPP_set=\r\nac_cv_env_CXXCPP_value=\r\nac_cv_env_CXXFLAGS_set=set\r\nac_cv_env_CXXFLAGS_value='-DNDEBUG -Os -pipe -fPIC -fno-exceptions -std=c++11 -stdlib=libc++ -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk'\r\nac_cv_env_CXX_set=set\r\nac_cv_env_CXX_value=\r\nac_cv_env_DIST_LANG_set=\r\nac_cv_env_DIST_LANG_value=\r\nac_cv_env_LDFLAGS_set=set\r\nac_cv_env_LDFLAGS_value='-arch i386 -fembed-bitcode -mios-simulator-version-min=8.0 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/system'\r\nac_cv_env_LIBS_set=set\r\nac_cv_env_LIBS_value='-lc++ -lc++abi'\r\nac_cv_env_LT_SYS_LIBRARY_PATH_set=\r\nac_cv_env_LT_SYS_LIBRARY_PATH_value=\r\nac_cv_env_OBJCFLAGS_set=\r\nac_cv_env_OBJCFLAGS_value=\r\nac_cv_env_OBJC_set=\r\nac_cv_env_OBJC_value=\r\nac_cv_env_build_alias_set=\r\nac_cv_env_build_alias_value=\r\nac_cv_env_host_alias_set=set\r\nac_cv_env_host_alias_value=i386-apple-darwin14.0.0\r\nac_cv_env_target_alias_set=\r\nac_cv_env_target_alias_value=\r\nac_cv_host=i386-apple-darwin14.0.0\r\nac_cv_path_install='/usr/bin/install -c'\r\nac_cv_prog_AWK=awk\r\nac_cv_prog_ac_ct_CC=gcc\r\nac_cv_prog_ac_ct_STRIP=strip\r\nac_cv_prog_make_make_set=yes\r\nac_cv_target=i386-apple-darwin14.0.0\r\nam_cv_make_support_nested_variables=yes\r\nam_cv_prog_tar_ustar=gnutar\r\n\r\n## ----------------- ##\r\n## Output variables. ##\r\n## ----------------- ##\r\n\r\nACLOCAL='${SHELL} /Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/missing aclocal-1.15'\r\nAMDEPBACKSLASH=''\r\nAMDEP_FALSE=''\r\nAMDEP_TRUE=''\r\nAMTAR='$${TAR-tar}'\r\nAM_BACKSLASH='\\'\r\nAM_DEFAULT_V='$(AM_DEFAULT_VERBOSITY)'\r\nAM_DEFAULT_VERBOSITY='1'\r\nAM_V='$(V)'\r\nAR=''\r\nAUTOCONF='${SHELL} /Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/missing autoconf'\r\nAUTOHEADER='${SHELL} /Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/missing autoheader'\r\nAUTOMAKE='${SHELL} /Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/missing automake-1.15'\r\nAWK='awk'\r\nBUILD_EXEEXT=''\r\nBUILD_OBJEXT=''\r\nCC='gcc'\r\nCCDEPMODE=''\r\nCC_FOR_BUILD=''\r\nCFLAGS='-DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk'\r\nCFLAGS_FOR_BUILD=''\r\nCPP=''\r\nCPPFLAGS=''\r\nCPPFLAGS_FOR_BUILD=''\r\nCPP_FOR_BUILD=''\r\nCXX=''\r\nCXXCPP=''\r\nCXXCPPFLAGS_FOR_BUILD=''\r\nCXXCPP_FOR_BUILD=''\r\nCXXDEPMODE=''\r\nCXXFLAGS='-DNDEBUG -Os -pipe -fPIC -fno-exceptions -std=c++11 -stdlib=libc++ -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk'\r\nCXXFLAGS_FOR_BUILD=''\r\nCXX_FOR_BUILD=''\r\nCYGPATH_W='echo'\r\nDEFS=''\r\nDEPDIR=''\r\nDIST_LANG='all'\r\nDLLTOOL=''\r\nDSYMUTIL=''\r\nDUMPBIN=''\r\nECHO_C='\\c'\r\nECHO_N=''\r\nECHO_T=''\r\nEGREP=''\r\nEXEEXT=''\r\nFGREP=''\r\nGCC_FALSE=''\r\nGCC_TRUE=''\r\nGREP=''\r\nHAVE_CXX11=''\r\nHAVE_PTHREAD_FALSE=''\r\nHAVE_PTHREAD_TRUE=''\r\nHAVE_ZLIB_FALSE=''\r\nHAVE_ZLIB_TRUE=''\r\nINSTALL_DATA='${INSTALL} -m 644'\r\nINSTALL_PROGRAM='${INSTALL}'\r\nINSTALL_SCRIPT='${INSTALL}'\r\nINSTALL_STRIP_PROGRAM='$(install_sh) -c -s'\r\nISAINFO=''\r\nLD=''\r\nLDFLAGS='-arch i386 -fembed-bitcode -mios-simulator-version-min=8.0 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/system'\r\nLDFLAGS_FOR_BUILD=''\r\nLIBOBJS=''\r\nLIBS='-lc++ -lc++abi'\r\nLIBTOOL=''\r\nLIPO=''\r\nLN_S=''\r\nLTLIBOBJS=''\r\nLT_SYS_LIBRARY_PATH=''\r\nMAINT=''\r\nMAINTAINER_MODE_FALSE='#'\r\nMAINTAINER_MODE_TRUE=''\r\nMAKEINFO='${SHELL} /Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/missing makeinfo'\r\nMANIFEST_TOOL=''\r\nMKDIR_P='./install-sh -c -d'\r\nNM=''\r\nNMEDIT=''\r\nOBJC=''\r\nOBJCDEPMODE=''\r\nOBJCFLAGS=''\r\nOBJC_CONFORMANCE_TEST_FALSE=''\r\nOBJC_CONFORMANCE_TEST_TRUE=''\r\nOBJDUMP=''\r\nOBJEXT=''\r\nOTOOL64=''\r\nOTOOL=''\r\nPACKAGE='protobuf'\r\nPACKAGE_BUGREPORT='protobuf@googlegroups.com'\r\nPACKAGE_NAME='Protocol Buffers'\r\nPACKAGE_STRING='Protocol Buffers 3.2.0'\r\nPACKAGE_TARNAME='protobuf'\r\nPACKAGE_URL=''\r\nPACKAGE_VERSION='3.2.0'\r\nPATH_SEPARATOR=':'\r\nPOW_LIB=''\r\nPROTOBUF_OPT_FLAG=''\r\nPROTOC=''\r\nPTHREAD_CC=''\r\nPTHREAD_CFLAGS=''\r\nPTHREAD_LIBS=''\r\nRANLIB=''\r\nSED=''\r\nSET_MAKE=''\r\nSHELL='/bin/sh'\r\nSTRIP='strip'\r\nUSE_EXTERNAL_PROTOC_FALSE=''\r\nUSE_EXTERNAL_PROTOC_TRUE=''\r\nVERSION='3.2.0'\r\nac_ct_AR=''\r\nac_ct_CC='gcc'\r\nac_ct_CC_FOR_BUILD=''\r\nac_ct_CXX=''\r\nac_ct_CXX_FOR_BUILD=''\r\nac_ct_DUMPBIN=''\r\nac_ct_OBJC=''\r\nacx_pthread_config=''\r\nam__EXEEXT_FALSE=''\r\nam__EXEEXT_TRUE=''\r\nam__fastdepCC_FALSE=''\r\nam__fastdepCC_TRUE=''\r\nam__fastdepCXX_FALSE=''\r\nam__fastdepCXX_TRUE=''\r\nam__fastdepOBJC_FALSE=''\r\nam__fastdepOBJC_TRUE=''\r\nam__include=''\r\nam__isrc=''\r\nam__leading_dot='.'\r\nam__nodep=''\r\nam__quote=''\r\nam__tar='tar --format=ustar -chf - \"$$tardir\"'\r\nam__untar='tar -xf -'\r\nbindir='${exec_prefix}/bin'\r\nbuild='x86_64-apple-darwin14.0.0'\r\nbuild_alias=''\r\nbuild_cpu='x86_64'\r\nbuild_os='darwin14.0.0'\r\nbuild_vendor='apple'\r\ndatadir='${datarootdir}'\r\ndatarootdir='${prefix}/share'\r\ndocdir='${datarootdir}/doc/${PACKAGE_TARNAME}'\r\ndvidir='${docdir}'\r\nexec_prefix='/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386'\r\nhost='i386-apple-darwin14.0.0'\r\nhost_alias='i386-apple-darwin14.0.0'\r\nhost_cpu='i386'\r\nhost_os='darwin14.0.0'\r\nhost_vendor='apple'\r\nhtmldir='${docdir}'\r\nincludedir='${prefix}/include'\r\ninfodir='${datarootdir}/info'\r\ninstall_sh='${SHELL} /Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf/install-sh'\r\nlibdir='${exec_prefix}/lib'\r\nlibexecdir='${exec_prefix}/libexec'\r\nlocaledir='${datarootdir}/locale'\r\nlocalstatedir='${prefix}/var'\r\nmandir='${datarootdir}/man'\r\nmkdir_p='$(MKDIR_P)'\r\noldincludedir='/usr/include'\r\npdfdir='${docdir}'\r\nprefix='/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386'\r\nprogram_transform_name='s,x,x,'\r\npsdir='${docdir}'\r\nsbindir='${exec_prefix}/sbin'\r\nsharedstatedir='${prefix}/com'\r\nsubdirs=''\r\nsysconfdir='${prefix}/etc'\r\ntarget='i386-apple-darwin14.0.0'\r\ntarget_alias=''\r\ntarget_cpu='i386'\r\ntarget_os='darwin14.0.0'\r\ntarget_vendor='apple'\r\n\r\n## ----------- ##\r\n## confdefs.h. ##\r\n## ----------- ##\r\n\r\n/* confdefs.h */\r\n#define PACKAGE_NAME \"Protocol Buffers\"\r\n#define PACKAGE_TARNAME \"protobuf\"\r\n#define PACKAGE_VERSION \"3.2.0\"\r\n#define PACKAGE_STRING \"Protocol Buffers 3.2.0\"\r\n#define PACKAGE_BUGREPORT \"protobuf@googlegroups.com\"\r\n#define PACKAGE_URL \"\"\r\n#define PACKAGE \"protobuf\"\r\n#define VERSION \"3.2.0\"\r\n\r\nconfigure: exit 77\r\n", "comments": ["control console outputs: \r\n\r\n\r\nMaking distclean in benchmarks\r\n rm -f generate-datasets cpp-benchmark\r\ntest -z \"benchmarks.pb.cc benchmarks.pb.h benchmark_messages_proto3.pb.cc benchmark_messages_proto3.pb.h benchmark_messages_proto2.pb.cc benchmark_messages_proto2.pb.h protoc_middleman protoc_middleman2 dataset.*\" || rm -f benchmarks.pb.cc benchmarks.pb.h benchmark_messages_proto3.pb.cc benchmark_messages_proto3.pb.h benchmark_messages_proto2.pb.cc benchmark_messages_proto2.pb.h protoc_middleman protoc_middleman2 dataset.*\r\nrm -rf .libs _libs\r\nrm -f *.o\r\nrm -f *.lo\r\nrm -f *.tab.c\r\ntest -z \"\" || rm -f \r\ntest . = \".\" || test -z \"\" || rm -f \r\nrm -f TAGS ID GTAGS GRTAGS GSYMS GPATH tags\r\nrm -rf ./.deps\r\nrm -f Makefile\r\nrm -rf .libs _libs\r\nCleaning any ObjC pyc files\r\nrm -f *.lo\r\ntest -z \"protobuf.pc protobuf-lite.pc\" || rm -f protobuf.pc protobuf-lite.pc\r\ntest . = \".\" || test -z \"\" || rm -f \r\nrm -f config.h stamp-h1\r\nrm -f libtool config.lt\r\nrm -f TAGS ID GTAGS GRTAGS GSYMS GPATH tags\r\nrm -f cscope.out cscope.in.out cscope.po.out cscope.files\r\nrm -f config.status config.cache config.log configure.lineno config.status.lineno\r\nrm -f Makefile\r\n+ ./configure --host=i386-apple-darwin14.0.0 --disable-shared --enable-cross-compile --with-protoc=/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc --prefix=/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386 --exec-prefix=/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_386 'CFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk' CXX= 'CXXFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -std=c++11 -stdlib=libc++ -mios-simulator-version-min=8.0 -arch i386 -fembed-bitcode -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk' 'LDFLAGS=-arch i386 -fembed-bitcode -mios-simulator-version-min=8.0 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator8.4.sdk/usr/lib/system' 'LIBS=-lc++ -lc++abi'\r\nchecking whether to enable maintainer-specific portions of Makefiles... yes\r\nchecking build system type... x86_64-apple-darwin14.0.0\r\nchecking host system type... i386-apple-darwin14.0.0\r\nchecking target system type... i386-apple-darwin14.0.0\r\nchecking for a BSD-compatible install... /usr/bin/install -c\r\nchecking whether build environment is sane... yes\r\nchecking for i386-apple-darwin14.0.0-strip... no\r\nchecking for strip... strip\r\nchecking for a thread-safe mkdir -p... ./install-sh -c -d\r\nchecking for gawk... no\r\nchecking for mawk... no\r\nchecking for nawk... no\r\nchecking for awk... awk\r\nchecking whether make sets $(MAKE)... yes\r\nchecking whether make supports nested variables... yes\r\nchecking whether UID '501' is supported by ustar format... yes\r\nchecking whether GID '20' is supported by ustar format... yes\r\nchecking how to create a ustar tar archive... gnutar\r\nchecking for i386-apple-darwin14.0.0-gcc... no\r\nchecking for gcc... gcc\r\nchecking whether the C compiler works... no\r\nconfigure: error: in `/Users/jakie/tensorflow-master/tensorflow/contrib/makefile/downloads/protobuf':\r\nconfigure: error: C compiler cannot create executables\r\nSee `config.log' for more details", "jakiedeMac:~ jakie$ /usr/bin/gcc -v\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)\r\nTarget: x86_64-apple-darwin14.0.0\r\nThread model: posix\r\njakiedeMac:~ jakie$ g++ -v\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 6.1.0 (clang-602.0.53) (based on LLVM 3.6.0svn)\r\nTarget: x86_64-apple-darwin14.0.0", "my mac os version is 10.10\r\nthere seems a solution that brew install apple-gcc42 , but faild on my osx:\r\n\r\n\r\njakiedeMac:~ jakie$ brew install apple-gcc42\r\nUpdating Homebrew...\r\n\r\napple-gcc42: This formula either does not compile or function as expected on macOS\r\nversions newer than Mavericks due to an upstream incompatibility.\r\nError: An unsatisfied requirement failed this build.", "@jakiechris I had this problem as well!\r\n\r\nI had `/usr/bin/gcc` symlinked to a `homebrew` installation `/usr/local/bin/gcc` which was `gcc4.9`. The `homebrew` distribution of `gcc` threw the same error you're getting. \r\n\r\nI fixed this by unlinking the `brew` installation from `/usr/local/bin/gcc` and kept the `clang` that is originally installed on **macOS Sierra**  (`/usr/bin/gcc`).\r\n\r\n```sh\r\n$ gcc --version\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.0.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n\r\n**Note:** It may be necessary, but not recommended to use\r\n```sh\r\n$ sudo tensorflow/contrib/makefile/build_all_ios.sh\r\n```", "@jakiechris As a precursor, ensure you have updated `xcode` tools from the `Terminal` or `App Store`.\r\n\r\n```sh\r\n$ xcode-select --install\r\n```", "@wenkesj \r\nHi Sam\uff1a\r\n      thks 4 reply\r\n      \r\n      problem solved~  i used osx 10.10 and a low version of xcode, which cannot work well. \r\n      \r\n      solution:\r\n      1. osx 10.12 on vmware 12\r\n      2. xcode 8.3.2\r\n      3. CommandLineToolsforXcode8.3.2.dmg\r\n      \r\n\r\n", "@jakiechris good to hear!", "Glad this was resolved!"]}, {"number": 11125, "title": "Fix typos", "body": "", "comments": ["Can one of the admins verify this patch?", "CC: @fchollet \r\n\r\nJenkins, test this please."]}, {"number": 11124, "title": "[iOS] Bug: No OpKernel was registered to support Op 'LessEqual'", "body": "### System information\r\nRunning OS `macOS Sierra` building for `iOS` specifically.\r\n```\r\n$ gcc --version\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.0.0\r\n```\r\n\r\nTensorFlow version `1.2.0`, installed through `pip`.\r\n\r\n### Describe the problem\r\n\r\n### Case: Build From Source (1.2.0)\r\nClone the tensorflow repository and build for `iOS`\r\n\r\n```sh\r\ncd tensorflow\r\nsh tensorflow/contrib/makefile/build_all_ios.sh\r\n```\r\n\r\nInstalls with no problem. Follow the [iOS example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/ios#creating-your-own-app-from-your-source-libraries).\r\n\r\n### Case: Pod Install (1.1)\r\nUse `CocoaPods`\r\n\r\n```sh\r\npod install TensorFlow-experimental\r\n```\r\n\r\n### Status Not `ok`\r\nFor both of the cases, the status from loading and creating session from a frozen graph (binary proto- file) is not `ok` when using `LessEqual` op in a `seq2seq` based model.\r\n\r\n**Note**: This example runs without any problems on the `python` (`pip install tensorflow`) distribution.\r\n\r\n### Source code / logs\r\nThis error occurs when loading from a `.pb` context:\r\n\r\n```obj-c\r\n// ...else-where\r\n// ReadBinaryProto(tensorflow::Env::Default(), path.fileSystemRepresentation, &graph)\r\n// ...\r\nstatus = session->Create(graph);\r\nif (!status.ok()) {\r\n  RCTLogInfo(@\"Error adding graph to session: %s\", status.error_message().c_str());\r\n  return false;\r\n}\r\n```\r\n\r\nIssue is logged here:\r\n```\r\nNo OpKernel was registered to support Op 'LessEqual' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: decode/decoder_1/LessEqual = LessEqual[T=DT_INT32](max_target_len, decode/decoder_1/LessEqual/y)]]\r\n```\r\n\r\nChecked `tf_op_files.txt` and saw that `LessEqual` op was not included:\r\n```diff\r\ntensorflow/core/kernels/cwise_op_less.cc\r\n+tensorflow/core/kernels/cwise_op_less_equal.cc\r\ntensorflow/core/kernels/cwise_op_isfinite.cc\r\n```\r\n\r\nMaybe this has something to do with it?\r\n\r\n**Update** it seems that this did have something to do with it.\r\n\r\nI rebuilt after adding `/cwise_op_less_equal.cc` to `tf_op_files.txt`:\r\n```sh\r\n$ tensorflow/contrib/makefile/build_all_ios.sh \r\n```\r\n\r\nI re-froze the graph:\r\n```sh\r\n$ python ../tensorflow/tensorflow/python/tools/freeze_graph.py \\\r\n--input_graph=./pb/input_graph.pb --input_checkpoint=./save/model \\\r\n--output_node_names=predictions --input_binary \\\r\n--output_graph=./tmp/frozen.pb\r\n```\r\n\r\nThen optimized...\r\n```sh\r\n$ python ../tensorflow/tensorflow/python/tools/optimize_for_inference.py \\\r\n--input=./tmp/frozen.pb --output=./tmp/inference.pb \\\r\n--input_names=input,target_sequence_length,source_sequence_length,keep_prob \\\r\n--output_names=predictions \\\r\n--frozen_graph=True\r\n```\r\n\r\nAnd the issue is no-longer there.\r\n\r\n#### BUT\r\nThere's still an error, which I assume is on my end.\r\n```\r\n2017-06-30 14:44:18.410032: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd\r\n2017-06-30 14:44:18.410300: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseAnd\r\n2017-06-30 14:44:18.410421: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseAnd\r\n2017-06-30 14:44:18.410519: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseAnd\r\n2017-06-30 14:44:18.410641: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseAnd\r\n2017-06-30 14:44:18.410740: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseAnd\r\n2017-06-30 14:44:18.410833: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr\r\n2017-06-30 14:44:18.410921: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseOr\r\n2017-06-30 14:44:18.410998: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseOr\r\n2017-06-30 14:44:18.411104: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseOr\r\n2017-06-30 14:44:18.411194: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseOr\r\n2017-06-30 14:44:18.411290: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseOr\r\n2017-06-30 14:44:18.411408: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor\r\n2017-06-30 14:44:18.411505: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseXor\r\n2017-06-30 14:44:18.411613: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseXor\r\n2017-06-30 14:44:18.411706: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseXor\r\n2017-06-30 14:44:18.411806: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseXor\r\n2017-06-30 14:44:18.411898: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseXor\r\n2017-06-30 14:44:18.412014: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert\r\n2017-06-30 14:44:18.412106: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: Invert\r\n2017-06-30 14:44:18.412194: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: Invert\r\n2017-06-30 14:44:18.412297: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: Invert\r\n2017-06-30 14:44:18.412406: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: Invert\r\n2017-06-30 14:44:18.412499: E tensorflow/core/framework/op_kernel.cc:1141] OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: Invert\r\n...\r\n```\r\n\r\nAnd all the way at the bottom...\r\n```\r\nInput 0 of node rnn/Shape_1 was passed float from source_sequence_length:0 incompatible with expected int32.\r\n```", "comments": ["This may be similar to #4863 but I think some light needs to be shed on it", "@petewarden, could you confirm that the underlying cause of this problem is the same as that for #4863 so that  we can dedup? Thank you!", "@ali01 @petewarden  So, adding the `cwise_op_less_equal.cc` fixed the first issue, but then `Missing op... \"RandomUniform\"` came up and this was due to the use of `dropout` ops. Removing these fixed the issue. This time, I skipped the inference optimization step on the frozen graph to be safe and it built and works."]}, {"number": 11123, "title": "Correction to Getting Started", "body": "Hello,\r\n\r\nI would like to submit that there is a programming bug on line 29 of the \"custom model tutorial\" within Getting Started with Tensorflow. \r\n\r\nThe line should read:\r\n\r\ninput_fn = tf.contrib.learn.io.numpy_input_fn({\"x\":x_train}, y_train, batch_size=4, num_epochs=1000)\r\n\r\n\r\n\u200bBest regards.\r\nJeff\u200b", "comments": ["That line works fine for me; however eval_input_fn isn't defined at all. Probably just needs to be:\r\neval_input_fn = tf.contrib.learn.io.numpy_input_fn(\r\n    {\"x\":x_eval}, y_eval, batch_size=4, num_epochs=1000)\r\n\r\nas it is in the previous example", "@dr4b @wolffg ", "This is fixed in the master branch...\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/get_started.md", "Thank you for the update.\r\n\r\n\r\nJeff\r\n\r\nOn Thu, Jul 6, 2017 at 12:39 PM, Andrew Selle <notifications@github.com>\r\nwrote:\r\n\r\n> This is fixed in the master branch...\r\n> https://github.com/tensorflow/tensorflow/blob/master/\r\n> tensorflow/docs_src/get_started/get_started.md\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/11123#issuecomment-313482699>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AOMZhvaBtOf26dsPPErQg-Rxjk2ylEJFks5sLSnNgaJpZM4OIguk>\r\n> ."]}, {"number": 11122, "title": "Building Tensorflow from source with XLA enabled and without GPU", "body": "Hi,\r\nI built and installed tensorflow from source with XLA enabled and GPU disabled (basically I opted N for everything while configuring via ./config except XLA enabling as Y). There were lot of warnings regrding deprecated syntax while building. but the build was successful.\r\nI am able to import tensorflow and run basic print command in session. But while I try to do some computation (for eg. simple addition) it gives me following error:\r\n**2017-06-28 15:09:22.366052: F tensorflow/compiler/xla/statusor.cc:41] Attempting to fetch value instead of handling error Not found: could not find registered computation placer for platform Executor -- check target linkage\r\nAborted**\r\n\r\nI did a bit of debugging and this error comes just after the call from client/sessions.py:1262  to pywrap_tensorflow:\r\n**tf_session.TF_Run(session, options,\r\n                                   feed_dict, fetch_list, target_list,\r\n                                   status, run_metadata)**\r\nso I believe it's because it is unable to link to _pywrap_tensorflow_internal.so. \r\nCan you please provide any fix to this or is there something am doing wrong here?\r\nThis is blocking my further task so any kind of help is appreciated!\r\n\r\nThanks & Regards\r\n", "comments": ["I encounter this also having built on MacOS.  I noticed that the error goes away if running in a  test block if that helps identify what is happening.\r\n\r\nThis code does not error:\r\n\r\n    import tensorflow as tf\r\n\r\n    class BugTest(tf.test.TestCase):\r\n      def bug_test(self):\r\n  \r\n  \t    with tf.Session() as sess:  #this can also use self.test_session()\r\n\t\tx1 = tf.random_normal(shape=[64, 64, 32, 32], seed=1.)\r\n\t\tres = sess.run(x1)\r\n\r\n    if __name__ == '__main__':\r\n\t  tf.test.main()\r\n\r\nThis code gives the error:\r\n\r\n    import tensorflow as tf\r\n    with tf.Session() as sess:\r\n       x1 = tf.random_normal(shape=[64, 64, 32, 32], seed=1.)\r\n       res = sess.run(x1)\r\n\r\n", "I meet this error when running [cnn benchmark](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py). I build tensorflow from source, enabled both gpu and xla, latest version is [this commit](https://github.com/tensorflow/tensorflow/commit/cf18c6d384a96a53b448bd51a90c117af0ed7c96). I'm in a cloud environment using nfs. @mikowals both programs are ok in my environment.", "@mikowals are you running the test with bazel?\r\n", "@suiyuan2009 I think there's issue when we do it without GPU support. Have you tried that?", "Faced the same thing running cifar10 example from https://github.com/tensorflow/models.\r\nHere is the snippet from @yaroslavvb, [link](https://gist.github.com/yaroslavvb/53052184e50cdfec35f0a127dd6df843)\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES']=''\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.compiler import jit\r\ntf.reset_default_graph()\r\njit_scope = jit.experimental_jit_scope\r\nwith jit_scope(compile_ops=True):\r\n    N = 500*1000*1000\r\n    x = tf.Variable(tf.random_uniform(shape=(N,)))\r\n    y = 0.1*x*x*x*x*x-0.5*x*x*x*x+.25*x*x*x+.75*x*x-1.5*x-2\r\n    y0 = y[0]\r\nimport time\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(y.op)\r\nstart_time = time.time()\r\nprint(sess.run(y0))\r\nend_time = time.time()\r\nprint(\"%.2f sec\"%(end_time-start_time))\r\n```\r\ngives: \r\n```\r\n2017-06-29 14:37:05.431926: F tensorflow/compiler/xla/statusor.cc:41] Attempting to fetch value instead of handling error Not found: could not find registered computation placer for platform Executor -- check target linkage\r\n```\r\nAfter adding `with tf.device('/cpu')` it works:\r\n\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES']=''\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.compiler import jit\r\ntf.reset_default_graph()\r\njit_scope = jit.experimental_jit_scope\r\nwith jit_scope(compile_ops=True):\r\n    N = 500*1000*1000\r\n    with tf.device(\"/cpu\"):\r\n        x = tf.Variable(tf.random_uniform(shape=(N,)))\r\n        y = 0.1*x*x*x*x*x-0.5*x*x*x*x+.25*x*x*x+.75*x*x-1.5*x-2\r\n        y0 = y[0]\r\nimport time\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(y.op)\r\nstart_time = time.time()\r\nprint(sess.run(y0))\r\nend_time = time.time()\r\nprint(\"%.2f sec\"%(end_time-start_time))\r\n```\r\n\r\n```\r\n2017-06-29 14:52:30.979254: I tensorflow/compiler/xla/service/service.cc:193] XLA service 0x7ff440159ca0 executing computations on platform Host. Devices:\r\n2017-06-29 14:52:30.979277: I tensorflow/compiler/xla/service/service.cc:201]   StreamExecutor device (0): <undefined>, <undefined>\r\n-2.7442\r\n0.68 sec\r\n```\r\nBuilt from sources with XLA and CUDA, commit: 270a3e8886e352ad563a3431963296e3dc6dae06\r\n\r\n", "Forcing ops on the CPU seems to be a workaround for my reproduction also on a CPU only build.  \r\n\r\n@ankitachandak I am not testing with bazel, just running with python.\r\n\r\nThis code works:\r\n\r\n    import tensorflow as tf\r\n    with tf.device('/cpu'):\r\n        x1 = tf.random_normal(shape=[64, 64, 32, 32], seed=1.)\r\n \r\n    with tf.Session() as sess: \r\n        res = sess.run(x)\r\n\t\t", "@ankitachandak , I didn't try cpu version, our training data is very large.", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@suiyuan2009 No need to \"try cpu version\". The snippet @mikowals provided works fine without the explicit specification of the device if gpus are available. But if they are not available for some reason (e.g. CUDA_VISIBLE_DEVICES='') - the code requires such explicit specification. Interesting, that this breaks cifar10 code. \r\n\r\n@ali01\r\n```\r\n$ cat tf_env.txt\r\n== cat /etc/issue ===============================================\r\nLinux sv 4.8.0-56-generic #61~16.04.1-Ubuntu SMP Wed Jun 14 11:58:22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n\r\n== uname -a =====================================================\r\nLinux sv800478lx 4.8.0-56-generic #61~16.04.1-Ubuntu SMP Wed Jun 14 11:58:22 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.0)\r\ntensorflow-tensorboard (0.1.2)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0\r\ntf.GIT_VERSION = b'v1.2.0-1367-g270a3e8'\r\ntf.COMPILER_VERSION = b'v1.2.0-1367-g270a3e8'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Jun 30 12:01:38 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.51                 Driver Version: 375.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN X (Pascal)    On   | 0000:03:00.0      On |                  N/A |\r\n| 27%   49C    P0    59W / 250W |   1017MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN X (Pascal)    On   | 0000:04:00.0     Off |                  N/A |\r\n| 23%   39C    P0    56W / 250W |      1MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a\r\n```\r\n```\r\n$ cat .tf_configure.bazelrc\r\n................\r\nbuild --define with_jemalloc=true\r\nbuild --define with_xla_support=true\r\nbuild:opt --cxxopt=-march=native --copt=-march=native\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env TF_NEED_OPENCL=\"0\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda\"\r\nbuild --action_env TF_CUDA_VERSION=\"8.0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild --action_env TF_CUDNN_VERSION=\"\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"/usr/local/cuda-8.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"6\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\n```\r\n\r\nWithout explicit device specification, example.py:\r\n```\r\nimport tensorflow as tf\r\nx = tf.random_normal(shape=[64, 64, 32, 32], seed=1.)\r\nwith tf.Session() as sess: \r\n    res = sess.run(x)\r\n```\r\n`CUDA_VISIBLE_DEVICES='' python example.py` crashes with `Attempting to fetch value instead of handling error Not found: could not find registered computation placer for platform Executor -- check target linkage\r\nAborted (core dumped)`\r\n\r\nAt the same time the version from [g8bbec0b](https://github.com/yaroslavvb/tensorflow-community-wheels/issues/21) works. \r\n\r\nI would assume that something happened in between. ", "@ali01 I compiled from the source\r\nI followed the instructions from https://www.tensorflow.org/install/install_sources\r\nSo cloned the latest Tensorflow version\r\nOS Platform and Distribution: Linux Ubuntu 16.04\r\nPython version: 2.7\r\nBazel: 0.5.2\r\nCUDA/cuDNN version: None (As I said I am installing CPU version)\r\nGPU model and memory: N/A\r\n\r\nExact command I ran which gave the error were as simple:\r\n\r\na = tf.placeholder(tf.float32)\r\nb = tf.placeholder(tf.float32)\r\nadder_node = a + b  # + provides a shortcut for tf.add(a, b)\r\nprint(sess.run(adder_node, {a: 3, b:4.5}))\r\n\r\nI placed it in a file and ran python example.py\r\nI debugged and found out that it goes till pywrap_tensorflow run method:\r\n**tf_session.TF_Run(session, options,\r\nfeed_dict, fetch_list, target_list,\r\nstatus, run_metadata)**\r\n\r\n\r\n\r\n", "@ramanishka @mikowals Yes force setting CPU works! Thanks for the hack.\r\n\r\nAlthough I hope tensorflow resolves this issue as the workaround won't be of much help later as I am trying to play around LLVM and I would prefer to run it without any hacks.", "Used the following command, trying to guarantee that it uses the SSE4.[1,2] and AVX:\r\n```shell\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-msse4.1 --copt=-msse3 --copt=-mfma -k //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nAfter the **pip installation**, tryed to run a Keras project and got this error:\r\n\r\n*\"Using TensorFlow backend.\r\n2017-07-04 10:30:20.681180: F tensorflow/compiler/xla/statusor.cc:41] Attempting to fetch value instead of handling error Not found: could not find registered computation placer for platform Executor -- check target linkage*\r\n\r\nTried this way and the standard way from TensorFlow website.\r\n\r\nMy computer info:\r\nOS: Ubuntu 16.04.2 LTS (Linux 4.8.0-51-generic)\r\nProcessor: i3-2100 CPU\r\nMemory: 16.4 GB\r\n", "Looks like this came in with the `ComputationPlacer` in 7d3497a6. Although it might have just been exposed by that commit.\r\n\r\nSpecifically, [this check](https://github.com/tensorflow/tensorflow/commit/7d3497a639670d9c31d09185ff97b852f0fbe101#diff-91c1769c96df8265caf0c5f0ea83666fR99) fails for the Executor plugin. The `TransferManager` registers  itself in the executor plugin ([like this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/plugin/executor/transfer_manager.cc#L183)) so bodging in a similar thing for `ComputationPlacer` fixes the network I see the reported error on. eg:\r\n\r\n```\r\ndiff --git a/tensorflow/compiler/plugin/executor/transfer_manager.cc b/tensorflow/compiler/plugin/executor/transfer_manager.cc\r\nindex 51c5dee..39d443e 100644\r\n--- a/tensorflow/compiler/plugin/executor/transfer_manager.cc\r\n+++ b/tensorflow/compiler/plugin/executor/transfer_manager.cc\r\n@@ -23,6 +23,7 @@ limitations under the License.\r\n #include \"tensorflow/compiler/xla/types.h\"\r\n #include \"tensorflow/compiler/xla/util.h\"\r\n #include \"tensorflow/compiler/xla/xla_data.pb.h\"\r\n+#include \"tensorflow/compiler/xla/service/computation_placer.h\"\r\n #include \"tensorflow/core/lib/core/errors.h\"\r\n #include \"tensorflow/core/platform/logging.h\"\r\n #include \"tensorflow/core/platform/stream_executor_no_cuda.h\"\r\n@@ -179,9 +180,15 @@ static std::unique_ptr<xla::TransferManager> CreateExecutorTransferManager() {\r\n   return xla::MakeUnique<xla::executorplugin::ExecutorTransferManager>();\r\n }\r\n\r\n+static std::unique_ptr<xla::ComputationPlacer> CreateExecutorComputationPlacer() {\r\n+  return xla::MakeUnique<xla::ComputationPlacer>();\r\n+}\r\n+\r\n static bool InitModule() {\r\n   xla::TransferManager::RegisterTransferManager(sep::kExecutorPlatformId,\r\n                                                 &CreateExecutorTransferManager);\r\n+  xla::ComputationPlacer::RegisterComputationPlacer(sep::kExecutorPlatformId,\r\n+                                                &CreateExecutorComputationPlacer);\r\n   return true;\r\n }\r\n static bool module_initialized = InitModule();\r\n```\r\n\r\nHowever unsurprisingly this bodge doesn't fix the other test cases in this ticket. For instance the simple test case:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.random_normal(shape=[64, 64, 32, 32], seed=1.)\r\nwith tf.Session() as sess: \r\n    res = sess.run(x)\r\n```\r\nNow provokes:\r\n```\r\ntensorflow.python.framework.errors_impl.UnimplementedError: unhandled HLO ops for HloEvaluator: rng.\r\n\t [[Node: cluster_0/_0/_1 = _XlaLaunch[Nresources=0, Targs=[], Tconstants=[], Tresults=[DT_FLOAT], function=cluster_0[_XlaCompiledKernel=true, _XlaNumConstantArgs=0, _XlaNumResourceArgs=0], _device=\"/job:localhost/replica:0/task:0/device:XLA_EXEC:0\"]()]]\r\n\t [[Node: cluster_0/_0/_1/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/device:XLA_EXEC:0\", send_device_incarnation=1, tensor_name=\"edge_6_cluster_0/_0/_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]```", "I have the same problem (built from source, XLA and GPU enabled).    \r\n(Note that this error doesn't occur in the binary version from the official website.)\r\n\r\n### Code to reproduce the issue:\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.ones([2, 1])\r\ny = tf.layers.dense(x, 3)\r\n\r\nsv = tf.train.Supervisor()\r\nsess = sv.prepare_or_wait_for_session()  # this triggers error\r\n```\r\n\r\nThe error message:\r\n```\r\n2017-07-06 14:41:11.341941: F tensorflow/compiler/xla/statusor.cc:41] Attempting to fetch value instead of handling error Not found: could not find registered computation placer for platform Executor -- check target linkage\r\nAborted (core dumped)\r\n```\r\n\r\nHowever, replacing the last 2 lines with the following won't trigger the error.\r\n```python\r\nsess = tf.Session()\r\nsess.run(tf.initialize_all_variables())\r\nsess.run(y)\r\n```\r\nThis yielded normal output and did not trigger core dump. \r\n\r\n\r\nUnlike what @ramanishka mentioned, using `with tf.device('/cpu')` cannot prevent the error in my case.\r\n\r\n\r\n### System Info\r\nOS: Ubuntu 16.04\r\nTensorflow: 1.2.1, built from source  \r\n  commit ee4259a271f56a32f046cb840215f1370acaf186\r\n  (and commit 43a819e1386195f7010f8ca9c74ed96ab81913d8)\r\nPython: 3.5.3\r\nBazel: release 0.5.2\r\nCUDA/cuDNN: 8.0/6.0\r\nGPU: GeForce GTX TITAN X\r\n", "@hyouklee it looks as if you added the ComputationPlacer code: would you take a look?", "I will take a look at this, since @hyouklee is away this week.", "Have a fix: the priority of XLA_EXEC is set too high in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/plugin/executor/device.cc#L50 changing it to lower (40 for eg.) fixes the problem.\r\n\r\nA fix should be merged upstream soon.", "Thanks @frankchn for the merge. This should now be fixed, but please feel free to reopen if the same symptom remains."]}, {"number": 11121, "title": "reset_default_graph awkwardly breaks graph nesting.", "body": "\r\n### System information\r\n- **Linux Ubuntu 16.04**:\r\n- **TF version 1.1.0, though it should still be present in master**:\r\n\r\n### Bug\r\nRunning this:\r\n```python\r\nimport tensorflow as tf\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n  tf.reset_default_graph()\r\n  # Build something...\r\n  a = tf.constant(1.0)\r\n  b = tf.constant(1.0)\r\n  c = a + b\r\n  # Bug happens on exit of the with statement\r\n```\r\nCauses this\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-1-af7c5ed96704> in <module>()\r\n      7   a = tf.constant(1.0)\r\n      8   b = tf.constant(1.0)\r\n----> 9   c = a + b\r\n\r\n/usr/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     64         if type is None:\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n     68                 return\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_controller(self, default)\r\n   3626     finally:\r\n   3627       if self._enforce_nesting:\r\n-> 3628         if self.stack[-1] is not default:\r\n   3629           raise AssertionError(\r\n   3630               \"Nesting violated for default stack of %s objects\"\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\nThis bug causes a confusing error message. The stack should either include the new default graph created by reset_default_graph or a warning/error message should happen when you reset the default within nested graphs.", "comments": ["The default graph is a property of the current thread. `tf.reset_default_graph()` function applies only to the current thread. Calling this function while a tf.Session or tf.InteractiveSession is active will result in undefined behavior. Using any previously created tf.Operation or tf.Tensor objects after calling this function will result in undefined behavior.\r\n\r\n```\r\nimport tensorflow as tf\r\ng = tf.Graph()\r\nwith g.as_default():\r\n  a = tf.constant(1.0)\r\n  b = tf.constant(1.0)\r\n  c = a + b\r\n```", "Right, but you don't need an active session or any previously created ops/tensors to cause the error. Just calling reset_default_graph within nested graphs causes a confusing error.\r\n\r\nJust this:\r\n```python\r\nimport tensorflow as tf\r\n\r\ng = tf.Graph()\r\nwith g.as_default():\r\n  tf.reset_default_graph()\r\n```\r\n\r\nAlso breaks with the same error. \r\n```\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-1-8377d035bd16> in <module>()\r\n      3 g = tf.Graph()\r\n      4 with g.as_default():\r\n----> 5   tf.reset_default_graph()\r\n\r\n/usr/lib/python3.5/contextlib.py in __exit__(self, type, value, traceback)\r\n     64         if type is None:\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n     68                 return\r\n\r\n/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in get_controller(self, default)\r\n   3813     finally:\r\n   3814       if self._enforce_nesting:\r\n-> 3815         if self.stack[-1] is not default:\r\n   3816           raise AssertionError(\r\n   3817               \"Nesting violated for default stack of %s objects\"\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\nI think it would be better if it actually threw the AssertionError rather than break on an index error. I'll see if I can make a PR for the fix tonight.", "For historical context, `tf.reset_default_graph()` was never designed to be used with `with g.as_default():` context managers. I think the proper fix here is to make `tf.reset_default_graph()` fail with an informative error message when used inside a `with g.as_default():` context. I think this could be done by checking that `ops._default_graph_stack` is empty before resetting.", "@mrry You are right. Many errors in tensorflow should be written in human friendly manner so that they don't have to research so hard for its cause.\r\n", "I agree we should fail with a more informative error message. @Thenerdstation @printdhruv would either of you be willing to create a PR for this?", "@skye It would be so easy for author of that specific code do this. I don't have specific guidelines but let me see how can I approach this?\r\nAlso, I would be happy if you have some suggestions to achieve success?", "@skye Ill gladly make the PR tonight", "Great, thank you @Thenerdstation! I realize getting started with TensorFlow development is non-trivial, and  there are many small issues like this that collectively would take up a lot of the team's time, so we really appreciate external contributions!\r\n\r\nAs Derek said, you can detect if you're in a default graph context by checking if `_default_graph_stack` is empty. This will probably involve adding a new method to the `_DefaultGraph` class. Don't forget to add a unit test :)", "@skye Let me try in your direction. I am enjoying this work! Oh! @Thenerdstation already did it!", "As far as I understand `graph.as_default` adds graph on the top of stack and `tf.reset_default_graph` resets whole stack. Is it unintuitively? Would it be better to make `tf.reset_default_graph` remove graph from top of stack? So that it would work with nested graphs. @mrry "]}, {"number": 11120, "title": "fix link in README.md in Using TensorFlow via Docker", "body": "correctly link parameterized_docker_build.sh to its actual location, instead of README.md", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "> Once you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\r\n\r\nSigned CLA!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the fix, @chunjy92 !"]}, {"number": 11119, "title": "Fix to TensorFlow-Slim README", "body": "Same TF Slim README fix as earlier today. \r\nThis is a resubmit with my email added to git config to pass your CLA check. ", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I'm not sure why @googlebot can't verify my CLA (I am both the commit author and sender of the pull request). I set user.email in my git config before resubmitting this pull request. \r\nPlease let me know if I need to update a config elsewhere.  \r\n\r\n", "I think you have to say something like \"I signed it!\". It's somewhere in the instructions.", "Thanks @drpngx , I'll try that and see if it resolves the issue.", "I signed it!", "OK, I'm able to see the CLA with your gmail address. Not clear why the bot is confused.", "Jenkins, test this please.", "Ignoring unrelated failure.\r\n\r\n```\r\n==================== Test output for //bazel_pip/tensorflow/python/kernel_tests:sparse_reshape_op_test:\r\n2017-06-28 18:29:35.102138: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-28 18:29:35.102177: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-28 18:29:35.102184: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-28 18:29:35.102187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-28 18:29:35.102190: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nF..................\r\n======================================================================\r\nFAIL: testFeedDenseReshapeSemantics (__main__.SparseReshapeTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/bin/bazel_pip/tensorflow/python/kernel_tests/sparse_reshape_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/kernel_tests/sparse_reshape_op_test.py\", line 277, in testFeedDenseReshapeSemantics\r\n    self.assertAllEqual(output_val.indices, new_indices)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/tensorflow/python/framework/test_util.py\", line 725, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b)\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/numpy/testing/utils.py\", line 854, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/workspace/pip_test/venv/lib/python3.4/site-packages/numpy/testing/utils.py\", line 778, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\n(mismatch 66.95152019231006%)\r\n x: array([[  0,   0,   0,   0],\r\n       [  0,   0,   0,   0],\r\n       [  0,   0,   0,   0],...\r\n y: array([[  0,   0,   0,   2],\r\n       [  0,   0,   0,   3],\r\n       [  0,   0,   0,   9],...\r\n\r\n----------------------------------------------------------------------\r\nRan 19 tests in 0.774s\r\n\r\nFAILED (failures=1)\r\nnot equal where =  (array([     0,      1,      2, ..., 405178, 405178, 405178]), array([3, 3, 3, ..., 1, 2, 3]))\r\nnot equal lhs =  [  0   0   0 ...,   7  47 413]\r\nnot equal rhs =  [  2   3   9 ...,   9  59 448]\r\n================================================================================\r\n```"]}, {"number": 11118, "title": "minor fix to TF slim README", "body": "Removing duplicated parameter in TensorFlow-Slim documentation.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed the CLA.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Sorry, email was not set when I committed the fix. I'll close this PR and resubmit."]}]