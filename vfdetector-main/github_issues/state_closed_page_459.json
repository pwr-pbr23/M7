[{"number": 40052, "title": "Format issue in `tf.ragged.constant` documentation", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/ragged/constant\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nFormat issue. In the \"Args\" section, format of description of `ragged_rank` is problematic. The default value should be `max(0, K-1-len(inner_shape))`\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n", "comments": ["The issue was that in docstring `max(0, K - 1 - len(inner_shape))` crossed two lines and doc rendering doesn't work anymore. Created a PR #40057 for the fix.", "@yongtang  Thanks!"]}, {"number": 40051, "title": "`tf.hessians` documentation refers `colocate_gradients_with_ops` as an argument", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/hessians\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIn the \"Args\" section, there is an input `colocate_gradients_with_ops`, but it is not in the signature, and the function doesn't accept the argument.\r\n\r\nRunning code:\r\n\r\n~~~python\r\ntf.hessians(1,1,colocate_gradients_with_ops=None)\r\n~~~\r\n\r\ngot exception:\r\n\r\n~~~python\r\nTypeError: HessiansV2() got an unexpected keyword argument 'colocate_gradients_with_ops'\r\n~~~\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n", "comments": ["Was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/14cf7c3e867823290e14641cdab5be6a/40051.ipynb). Thanks!", "Hi guys, I just submit a PR https://github.com/tensorflow/tensorflow/pull/40109 fixing this.", "@HughKu  Thanks!", "Closing this issue as it has been fixed. Thanks @HughKu "]}, {"number": 40050, "title": "Unclear shape dependency of `value` in  `tf.keras.backend.moving_average_update` documentation", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/backend/moving_average_update\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nUnclear shape dependency of input `value`. According to the document, `value` should have the same shape as `variable`, but it is unclear what is `variable`.\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nNo", "comments": ["@DNXie \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@Saduf2019 \r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.2.0-rc3\r\n- **Python version**: 3.8.2\r\n"]}, {"number": 40049, "title": "Usage of secure grpc functions breaks builds using unsecure grpc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04 on s390x\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nMaster\r\n- Python version:\r\n3.7.6\r\n- Installed using virtualenv? pip? conda?:\r\nBuilding from source\r\n- Bazel version (if compiling from source):\r\n2.0.0\r\n- GCC/Compiler version (if compiling from source):\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n- CUDA/cuDNN version:\r\nN/A disabled in configure\r\n- GPU model and memory:\r\nN/A not using GPU for tensorflow\r\n\r\n\r\n**Describe the problem**\r\nOn s390x grpc_unsecure build is used:\r\n\r\n```\r\ncc_library(\r\n    name = \"grpc\",\r\n    visibility = [\"//visibility:public\"],\r\n    deps = select({\r\n        \":linux_s390x\": [\"@com_github_grpc_grpc//:grpc_unsecure\"],\r\n        \"//conditions:default\": [\"@com_github_grpc_grpc//:grpc\"],\r\n    }),\r\n)\r\n\r\ncc_library(\r\n    name = \"grpc++\",\r\n    visibility = [\"//visibility:public\"],\r\n    deps = select({\r\n        \":linux_s390x\": [\"@com_github_grpc_grpc//:grpc++_unsecure\"],\r\n        \"//conditions:default\": [\"@com_github_grpc_grpc//:grpc++\"],\r\n    }),\r\n)\r\n```\r\n\r\nWhen building tensorflow master on s390x the following error occurs:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN9grpc_impl12experimental22LocalServerCredentialsE23grpc_local_connect_type\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: /home/alfred/jenkins/workspace/TensorFlow_IBMZ_CI/Tensorflow_tmp/_bazel_jenkins/f9a4b527679ac8529256cd6c97867507/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN9grpc_impl12experimental22LocalServerCredentialsE23grpc_local_connect_type\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\nThe issue is caused because the built pywrap internal shared library is missing a few grpc secure symbols. The reason is tensorflow master has started to use secure grpc functions which breaks the build.\r\n\r\nIs this perhaps a mistake in the code? Should unsecure grpc functions be used instead?\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBuild tensorflow on s390x from source. \r\n\r\n```\r\nbazel --host_jvm_args=\"-Xms1024m\" --host_jvm_args=\"-Xmx2048m\" build  --define=tensorflow_mkldnn_contraction_kernel=0 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nA possible solution is to replace the use of secure grpc functions with unsecure grpc functions, which can be fixed by the below patch:\r\n\r\n```diff --git a/tensorflow/compiler/xla/python/tpu_driver/grpc_tpu_driver.cc b/tensorflow/compiler/xla/python/tpu_driver/grpc_tpu_driver.cc\r\nindex 7632f21..db91ea2 100644\r\n--- a/tensorflow/compiler/xla/python/tpu_driver/grpc_tpu_driver.cc\r\n+++ b/tensorflow/compiler/xla/python/tpu_driver/grpc_tpu_driver.cc\r\n@@ -1083,14 +1083,8 @@ REGISTER_TPU_DRIVER(\r\n     \"grpc://\",\r\n     [](const TpuDriverConfig& config)\r\n         -> xla::StatusOr<std::unique_ptr<TpuDriver>> {\r\n-      if (absl::StartsWith(config.worker(), \"grpc://localhost\")) {\r\n-        LOG(INFO) << \"Using local credentials for localhost: connection.\";\r\n-        return CreateGrpcTpuDriver(\r\n-            config, ::grpc::experimental::LocalCredentials(LOCAL_TCP));\r\n-      } else {\r\n-        return CreateGrpcTpuDriver(config,\r\n-                                   ::grpc::InsecureChannelCredentials());\r\n-      }\r\n+      return CreateGrpcTpuDriver(config,\r\n+                                 ::grpc::InsecureChannelCredentials());\r\n     });\r\n }  // namespace tpu_driver\r\ndiff --git a/tensorflow/core/data/service/local_credentials_factory.cc b/tensorflow/core/data/service/local_credentials_factory.cc\r\nindex 136bf49..2fd9202 100644\r\n--- a/tensorflow/core/data/service/local_credentials_factory.cc\r\n+++ b/tensorflow/core/data/service/local_credentials_factory.cc\r\n@@ -24,13 +24,13 @@ class LocalCredentialsFactory : public CredentialsFactory {\r\n   Status CreateServerCredentials(\r\n       std::shared_ptr<::grpc::ServerCredentials>* out) override {\r\n-    *out = grpc::experimental::LocalServerCredentials(LOCAL_TCP);\r\n+    *out = grpc::InsecureServerCredentials();\r\n     return Status::OK();\r\n   }\r\n   Status CreateClientCredentials(\r\n       std::shared_ptr<::grpc::ChannelCredentials>* out) override {\r\n-    *out = grpc::experimental::LocalCredentials(LOCAL_TCP);\r\n+    *out = grpc::InsecureChannelCredentials();\r\n     return Status::OK();\r\n   }\r\n };```", "comments": ["@aaudiber Just wondering regarding the use of the secure credentials (LocalServerCredentials/LocalCredentials), is that something Tensorflow plans on going forward with now in the future? ", "@cdavoudian The local credentials in `core/data/service` are only used for testing, so in theory they could be moved outside of `_pywrap_tensorflow_internal.so` and only depended on by the tests. The data service unit tests wouldn't work in your grpc_unsecure build, but importing `_pywrap_tensorflow_internal` wouldn't cause this error. Would that work for you?", "@cdavoudian the issue should be fixed now. Can you confirm whether it's working on your end?", "> @cdavoudian the issue should be fixed now. Can you confirm whether it's working on your end?\r\n\r\nI should be able to get back to you tomorrow, the build is still running on my end.", "I can confirm that its working on my end- so I suppose that Tensorflow plans on continuing to support unsecure grpc builds in the future as well?", "Thanks for confirming.\r\n\r\nI'm not sure about plans regarding unsecure grpc builds. It seemed to work before my change, but there weren't any tests to enforce that unsecure grpc builds would work. If we're really planning to support unsecure grpc, we ought to have a CI test to enforce the support.", "Closing this issue now since it's fixed. Feel free to reopen if required. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40049\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40049\">No</a>\n"]}, {"number": 40048, "title": "Test case //tensorflow/python/eager:def_function_test_cpu_only fails on TF2.2 due to inconsistent XLA flag.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nThe test case `//tensorflow/python/eager:def_function_test_cpu_only` fails with the following error log:\r\n```\r\n======================================================================\r\nERROR: testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported (__main__.DefFunctionCpuOnlyTest)\r\ntestExperimentalCompileRaisesExceptionWhenXlaIsUnsupported (__main__.DefFunctionCpuOnlyTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function_test_cpu_only.py\", line 46, in testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported\r\n    fn([1, 1, 2, 3])\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function.py\", line 650, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\r\n    return self._call_flat(\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 1745, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/function.py\", line 593, in call\r\n    outputs = execute.execute(\r\n  File \"/home/sidong/.cache/bazel/_bazel_sidong/9ef871a29c692fc6a18121463529e145/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_fn_7}} = __inference_fn_7[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0002\\002J\\0008\\001\", executor_type=\"\"]().\r\nUncompilable nodes:\r\nUnique: unsupported op: No registered 'Unique' OpKernel for XLA_CPU_JIT devices compatible with node {{node Unique}}\r\n        Stacktrace:\r\n                Node: __inference_fn_7, function:\r\n                Node: Unique, function: __inference_fn_7\r\n [Op:__inference_fn_7]\r\n\r\n----------------------------------------------------------------------\r\nRan 2 tests in 0.318s\r\n\r\nFAILED (errors=1, skipped=1)\r\n```\r\n**Describe the expected behavior**\r\nThe test case should raise a ValueError \"Attempting to use experimental_compile, but XLA support is not linked in. Rebuild with --define=with_xla_support=true.\", and captured by `self.assertRaisesRegexp()`\r\nThe test case should pass.\r\n\r\n**Standalone code to reproduce the issue**\r\nI modified the test case a little bit, and it could be reproduced from this [gist](https://colab.research.google.com/drive/1qweKRl1ZxEPWtN-qlz6DsNCVR0J8M8fg?usp=sharing)\r\n\r\n\r\n**Other info / logs** \r\nI looked into this issue, it seems that it is caused by inconsistent internal APIs. When debugging this test file, I get the following results:\r\n```\r\n> /home/sidong/.cache/bazel/_bazel_sidong/338a466d2403fbfe3413e7ca6003e4cf/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/eager/def_function_test_cpu_only.runfiles/org_tensorflow/tensorflow/python/eager/def_function_test_cpu_only.py(47)testExperimentalCompileRaisesExceptionWhenXlaIsUnsupported()\r\n-> fn([1, 1, 2, 3])\r\n(Pdb) p test_util.is_xla_enabled()\r\nFalse\r\n(Pdb) p pywrap_tfe.TF_IsXlaEnabled()\r\nTrue\r\n```\r\nBasically, the test proceeds if `test_util.is_xla_enabled()` is false, and it will raise the exception as intended when `pywrap_tfe.TF_IsXlaEnabled()` is false. The inconsistency caused the exception not correctly raised.\r\nOne way to fix it is to discard `test_util.is_xla_enabled()` and only use `pywrap_tfe.TF_IsXlaEnabled()`, but I think maybe it's better to solve this inconsistency issue. I noticed that the `pywrap_tfe.TF_IsXlaEnabled()` API and this test case was added from the same [commit](https://github.com/tensorflow/tensorflow/commit/96e0b87d1e23ac1dd7a7aa984e3f479647267b32), and I assume the cause of inconsistency is that the function `SetXlaIsEnabled()` was called incorrectly. Could you look into this issue and check why the function was called when xla was not enabled? Thanks.\r\n\r\nSidong", "comments": ["Was able to reproduce the issue with [TF v2.2](https://colab.research.google.com/gist/amahendrakar/8434fed6f34cd6ea642fbba585c8ba4c/40048.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8ffb1016c0291672d7b9d0508874e083/40048-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Hi @cheshire , I noticed that you contributed to this part of xla code and specifically the `pywrap_tfe.TF_IsXlaEnabled()` flag, could you take a look into this issue and provides some insights? Thank you.", "@Sidong-Wei looks like a bug, thanks, I wonder why it passes internally on our CI.", "Looks like it has been fixed by this [commit](https://github.com/tensorflow/tensorflow/commit/c90cf2e18f808f93d315ba35e6f9103065cd0384), close for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40048\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40048\">No</a>\n"]}, {"number": 40047, "title": "InvalidArgumentError:  Incompatible shapes: [200,10] vs. [200,5] [[node LogicalAnd_3 (defined at <ipython-input-11-ea6e7bc7fc28>:1) ]]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, I have written custom code. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nI am running into this problem on Google Colab, which has Tensorflow 2.2.0\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNA\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nGoogle Colab setup, Tensorflow version 2.2.0\r\n\r\n- **TensorFlow version (use command below)**:\r\nTensorFlow 2.2.0\r\n\r\n- **Python version**: Python 3.6.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhile the model is getting trained, it randomly fails in any of the iteration of epoch with an Incompatible shape Error for **LogicalAnd** node, with the shapes differing as [200, 10] and [200, 5].\r\nThe difference in axis 1 here is actually : 2*Batch_size and Batch_size. So, if I change Batch_Size, the shapes change accordingly. Also, I'm not seeing this error on my local machine if I'm running this same code, Tensorflow version 2.2.0_rc1on a CPU. But, on Google Colab, I tried it both on CPU and GPU, and I face this error.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nDeep Network containing Pretrained Bert and  ResNet50V2 networks. The final layer outputs of both networks are concatenated and connected to a final Softmax layer of 2 neurons. There are 4 Input layers for complete network : 3 for BERT and 1 for ResNet50V2.\r\n\r\n**Here's the architecture code for network** : \r\n`\r\n    def complete_model():\r\n    input_word_ids = tf.keras.layers.Input(\r\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\r\n    input_masks = tf.keras.layers.Input(\r\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\r\n    input_segments = tf.keras.layers.Input(\r\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\r\n    \r\n    img_input = tf.keras.layers.Input((256, 256, 3))\r\n    \r\n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\r\n    \r\n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\r\n    bert_out = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\r\n    \r\n    resnet_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_tensor=img_input, pooling='avg')\r\n    resnet_out = resnet_model.output\r\n    \r\n    x = tf.keras.layers.Concatenate()([bert_out, resnet_out])\r\n    x = tf.keras.layers.Dropout(0.4)(x)\r\n    out = tf.keras.layers.Dense(2, activation='softmax')(x)\r\n    \r\n    model = tf.keras.models.Model(inputs=[img_input, input_word_ids, input_masks, input_segments], outputs=out)\r\n    return model\r\n`\r\n\r\nThe model is created and then compiled and trained with the tf.keras.fit API. I created a tf.keras.Sequence object for generating inputs in batches in format :\r\n**<batch of images for ResNet50V2 of shape (256, 256, 3)>, <batch for Input IDs for BERT of shape (512)>, <batch for Attention Masks for BERT of shape (512)>, <batch for Segment IDs for BERT of shape (512)> and output <batch of class labels (batch_size, 2)>**\r\n\r\n**Below is the Error Trace which I get** :\r\n\r\nEpoch 1/5\r\n  18/1700 [..............................] - ETA: 1:31:14 - loss: 1.6233 - accuracy: 0.5889 - auc: 0.5584\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-14-ea6e7bc7fc28> in <module>()\r\n----> 1 hist = model.fit(train_dataseq, epochs=5, verbose=1, steps_per_epoch=8500//5, validation_data=valid_dataseq, validation_steps=500//5)\r\n\r\n8 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    609       # In this case we have created variables on the first call, so we run the\r\n    610       # defunned version which is guaranteed to never create variables.\r\n--> 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    612     elif self._stateful_fn is not None:\r\n    613       # Release the lock early so that multiple threads can perform the call\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Incompatible shapes: [200,10] vs. [200,5]\r\n\t [[node LogicalAnd (defined at <ipython-input-14-ea6e7bc7fc28>:1) ]]\r\n\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_2738/_167]]\r\n  (1) Invalid argument:  Incompatible shapes: [200,10] vs. [200,5]\r\n\t [[node LogicalAnd (defined at <ipython-input-14-ea6e7bc7fc28>:1) ]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_64050]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n", "comments": ["@anktplwl91 \r\nCan you please share simple stand alone code to replicate the issue faced, or share a colab gist for us to analyse the error. Also include your TensorFlow version.\r\n\r\n", "Hi @Saduf2019 \r\nHere's my Github [Gist](https://gist.github.com/anktplwl91/a0b6612cb3ed649d4e9036933c228134)\r\nTensorflow version used in Colab is : 2.2.0 GPU and CPU both (facing issue on both)", "@anktplwl91 \r\nI ran the code shared and face [this issue](https://colab.sandbox.google.com/gist/Saduf2019/29d965d7786ca7d8813bb3e896173c76/untitled208.ipynb), please share all dependencies", "Hi @Saduf2019 \r\nHere's the link to my [colab](https://colab.research.google.com/drive/1J5oXtKy9I1ynpx1lbM4X9jiVufyRM3bi?usp=sharing)\r\n", "An update, I ran into this same issue on my local PC. But for some reason, the issue is seen in 5th epoch. I'm using same code there also, but using smaller version of BERT from Transformers repo\r\nTensorFlow Version : 2.2.0_rc1 CPU\r\nPython : 3.6.5\r\n\r\nError Logtrace : \r\n\r\nEpoch 1/5\r\n\r\nW0601 21:38:38.055769 17108 optimizer_v2.py:1180] Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\r\nW0601 21:38:43.361550 17108 optimizer_v2.py:1180] Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\r\n\r\n531/531 [==============================] - ETA: 0s - loss: 0.7343 - auc: 0.6148 - accuracy: 0.5972 \r\nEpoch 00001: saving model to model_checkpoint\r\n531/531 [==============================] - 10835s 20s/step - loss: 0.7343 - auc: 0.6148 - accuracy: 0.5972 - val_loss: 148.2352 - val_auc: 0.5020 - val_accuracy: 0.5020\r\nEpoch 2/5\r\n531/531 [==============================] - ETA: 0s - loss: 0.6840 - auc: 0.6270 - accuracy: 0.6176 \r\nEpoch 00002: saving model to model_checkpoint\r\n531/531 [==============================] - 9856s 19s/step - loss: 0.6840 - auc: 0.6270 - accuracy: 0.6176 - val_loss: 0.8900 - val_auc: 0.4815 - val_accuracy: 0.4960\r\nEpoch 3/5\r\n531/531 [==============================] - ETA: 0s - loss: 0.6715 - auc: 0.6392 - accuracy: 0.6228 \r\nEpoch 00003: saving model to model_checkpoint\r\n531/531 [==============================] - 9444s 18s/step - loss: 0.6715 - auc: 0.6392 - accuracy: 0.6228 - val_loss: 0.7030 - val_auc: 0.4856 - val_accuracy: 0.4859\r\nEpoch 4/5\r\n531/531 [==============================] - ETA: 0s - loss: 0.6715 - auc: 0.6364 - accuracy: 0.6289 \r\nEpoch 00004: saving model to model_checkpoint\r\n531/531 [==============================] - 13278s 25s/step - loss: 0.6715 - auc: 0.6364 - accuracy: 0.6289 - val_loss: 45.8197 - val_auc: 0.4939 - val_accuracy: 0.4980\r\nEpoch 5/5\r\n376/531 [====================>.........] - ETA: 48:52 - loss: 0.6696 - auc: 0.6402 - accuracy: 0.6330\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-14-b5f6dfea3b4c> in <module>()\r\n----> 1 hist = transformer_model.fit(train_dataseq, epochs=5, verbose=1, steps_per_epoch=8500//16, validation_data=valid_dataseq, validation_steps=500//16, callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='model_checkpoint', save_weights_only=True, verbose=1, save_freq='epoch')])\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n     63   def _method_wrapper(self, *args, **kwargs):\r\n     64     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 65       return method(self, *args, **kwargs)\r\n     66 \r\n     67     # Running inside `run_distribute_coordinator` already.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    781                 batch_size=batch_size):\r\n    782               callbacks.on_train_batch_begin(step)\r\n--> 783               tmp_logs = train_function(iterator)\r\n    784               # Catch OutOfRangeError for Datasets of unknown size.\r\n    785               # This blocks until the batch has finished executing.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    609       # In this case we have created variables on the first call, so we run the\r\n    610       # defunned version which is guaranteed to never create variables.\r\n--> 611       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    612     elif self._stateful_fn is not None:\r\n    613       # Release the lock early so that multiple threads can perform the call\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  Incompatible shapes: [200,32] vs. [200,16]\r\n\t [[node LogicalAnd (defined at <ipython-input-14-b5f6dfea3b4c>:1) ]] [Op:__inference_train_function_24857]\r\n\r\nFunction call stack:\r\ntrain_function", "@anktplwl91 \r\nI ran the code shared by you, every cell has error in it, can you please share code that is complete with all dependencies so we could replicate it.", "Hi @Saduf2019 \r\nCan you please download following files from [here](https://drive.google.com/drive/folders/1igDo4noUI-d1SQFUwOfa7JMtQS6vd1tR?usp=sharing)\r\n\r\n- img, bert-base-from-tfhub directories\r\n\r\n- train.jsonl, dev.jsonl, tfv2_tokenization.py and fb_meme_compeition.py files\r\n\r\n- Also, make sure you have tqdm and tensorflow_hub installed\r\n\r\nThis should run the code without any errors. Please let me know if you face anymore issues. \r\nThanks", "Hi @Saduf2019 @jvishnuvardhan \r\n\r\nI tried to run the model by removing metric **tf.keras.metrics.AUC** from model.compile, and have run the model for quite sometime now, around 10 epochs. I haven't seen this bug pop until now. Maybe the issue is coming from somehwere in AUC metrics' code", "The code sample is fairly complex, and it would need to be simplified before we could reliably debug. Since it looks like you resolved the issue in any case, I will close this. If you find you can reproduce the problem with a straightforward snippet, please file a new issue or reopen.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40047\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40047\">No</a>\n", "I also faced the same issue and found that there was issue with my pooling layer. It was giving out incorrect output. Changing maxpool2d to globalaevaragepool solved the issue. If you want to stick to maxpool, then change it's output shape. "]}, {"number": 40046, "title": "TypeError: '_TupleWrapper' object is not callable", "body": "```\r\ndef run_train(dataset, num_epochs=2):\r\n    start_time = time.perf_counter()\r\n\r\n    model = VGGBase()\r\n\r\n    for _ in tf.data.Dataset.range(num_epochs):\r\n        for image,target in dataset: # (batch_size (N), 300, 300, 3)\r\n            image = np.array(image)\r\n            target = np.array(target)\r\n            print(target)\r\n            print(type(image), type(target),image.shape,target.shape)\r\n            predicted_locs, predicted_socres = model(image)# (N, 8732, 4), (N, 8732, n_classes)\r\n            print(predicted_locs,predicted_socres)\r\n            pass\r\n            break\r\n        pass\r\n\r\ndef train():\r\n    if isprint:print(tf.__version__)\r\n    batch_size= 256\r\n\r\n    #dataset test0\r\n    images,boxes,labels,difficulties= PascalVOCDataset()\r\n    boxes = tf.ragged.constant(boxes)\r\n    dataset = tf.data.Dataset.from_tensor_slices((images,boxes))\r\n    run_train(dataset.map(resize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(1).prefetch(tf.data.experimental.AUTOTUNE))\r\n```\r\n\r\nI got an error of \r\n\r\n```\r\n File \"/home/jake/Gits/ssd_tensorflow/model.py\", line 56, in call\r\n    x = self.conv1_1(x)# (N, 64, 300, 300)\r\nTypeError: '_TupleWrapper' object is not callable\r\n```\r\n\r\nmy model looks like \r\n```\r\nclass  VGGBase(Model):\r\n    def __init__(self):\r\n        super(VGGBase,self).__init__()\r\n        self.padding_1 = tf.keras.layers.ZeroPadding2D(padding=(1, 1))  # put this before your conv layer\r\n        self.conv1_1 = tf.keras.layers.Conv2D(3, kernel_size=3,padding='same',strides=1, activation='relu'),\r\n        self.conv1_2 = tf.keras.layers.Conv2D(64, kernel_size=3, padding='same',strides=1,activation='relu'),\r\n        self.pool1 = tf.keras.layers.MaxPool2D(2,2),\r\n\r\n        self.conv2_1  =  tf.keras.layers.Conv2D(128, kernel_size=3, padding='same',strides= 1,activation='relu'),\r\n        self.conv2_2 = tf.keras.layers.Conv2D(128, kernel_size=3,padding='same',strides= 1,activation='relu'),\r\n        self.pool2 = tf.keras.layers.MaxPool2D(2,2),\r\n\r\n        self.conv3_1 =  tf.keras.layers.Conv2D(256, kernel_size=3, padding='same',strides= 1,activation='relu'),\r\n        self.conv3_2 =  tf.keras.layers.Conv2D(256, kernel_size=3, padding='same',strides= 1,activation='relu'),\r\n        self.conv3_3 =  tf.keras.layers.Conv2D(256, kernel_size=3, padding='same',strides= 1,activation='relu'),\r\n        self.pool3 = tf.keras.layers.MaxPool2D(2,2),\r\n\r\n        self.conv4_1 = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', strides=1,activation='relu'),\r\n        self.conv4_2 = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', strides=1,activation='relu'),\r\n        self.conv4_3 = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', strides=1,activation='relu'),\r\n        self.pool4 = tf.keras.layers.MaxPool2D(2, 2),\r\n\r\n        self.conv5_1 = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', strides=1,activation='relu'),\r\n        self.conv5_2 = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', strides=1,activation='relu'),\r\n        self.conv5_3 = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', strides=1,activation='relu'),\r\n        self.pool5 = tf.keras.layers.MaxPool2D(2, 2),\r\n\r\n        self.padding6 = tf.keras.layers.ZeroPadding2D(padding=(6, 6))  # put this before your conv layer\r\n        self.conv6 = tf.keras.layers.Conv2D(1024, kernel_size=3, padding='same',dilation_rate=6,activation='relu') # atrous convolution\r\n        self.conv7 = tf.keras.layers.Conv2D(1024, kernel_size=1,activation='relu')\r\n        #self.load_weights()\r\n    def call(self,x):\r\n        x = self.padding_1(x)\r\n        x = self.conv1_1(x)# (N, 64, 300, 300)\r\n        x = self.conv1_2(x)# (N, 64, 300, 300)\r\n        x = self.pool1(x) # (N, 64, 150, 150)\r\n\r\n        x = self.conv2_1(x) # (N, 128, 150, 150)\r\n        x = self.conv2_2(x) # (N, 128, 150, 150)\r\n        x = self.pool2(x)# (N, 128, 75, 75)\r\n\r\n        x = self.conv3_1(x) # (N, 256, 75, 75)\r\n        x = self.conv3_2(x)# (N, 256, 75, 75)\r\n        x = self.conv3_3(x)# (N, 256, 75, 75)\r\n        x = self.pool3(x) #(N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\r\n\r\n        x = self.conv4_1(x)# (N, 512, 38, 38)\r\n        x = self.conv4_2(x)# (N, 512, 38, 38)\r\n        x = self.conv4_3(x)# (N, 512, 38, 38)\r\n        conv4_3_feats = x# (N, 512, 38, 38)\r\n        x = self.pool4(x)# (N, 512, 19, 19)\r\n\r\n        x = self.conv5_1(x) # (N, 512, 19, 19)\r\n        x = self.conv5_2(x) # (N, 512, 19, 19)\r\n        x = self.conv5_3(x) # (N, 512, 19, 19)\r\n        x = self.pool5(x) # (N, 512, 19, 19), pool5 does not reduce dimensions\r\n\r\n        x = self.padding6(x)\r\n        x = self.conv6(x) # (N, 1024, 19, 19)\r\n        x = self.conv7(x) # (N, 1024, 19, 19)\r\n        conv7_feats = x\r\n\r\n        return conv4_3_feats, conv7_feats\r\n\r\n```\r\n", "comments": ["@SlowMonk \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@SlowMonk I just ran into this same error, and found that the issue was in the `__init__` method of my model. Looking at the snippet you provided, the trailing commas when you're defining the layers of your model leads to them being interpreted as tuples. Hope this helps!"]}, {"number": 40043, "title": "gcc: internal compiler error: Killed (program cc1plus)", "body": "**System information**\r\n- OS Platform: host: mac 14.10 \r\n- docker image:  tensorflow/tensorflow:latest-devel\r\n- TensorFlow version: 2.2\r\n- Python version: 3.6.9\r\n- building tensorflow from source using docker on mac with ubuntu in docker image\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NONE ( cpu only build)\r\n- GPU model and memory: NONE \r\n\r\n**the problem**\r\n```\r\nERROR: /tensorflow_src/tensorflow/core/BUILD:2193:1: C++ compilation of rule '//tensorflow/core:framework_internal_impl' failed (Exit 4): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/_objs/framework_internal_impl/batch_util.pic.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/_objs/framework_internal_impl/batch_util.pic.o' -fPIC -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/k8-opt-exec-50AE0418/bin -iquote external/com_google_protobuf -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/local_config_sycl -iquote external/gif -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/libjpeg_turbo -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -iquote external/double_conversion -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/snappy -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src -isystem external/nsync/public -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/gif -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -isystem external/double_conversion -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/double_conversion -g0 '-march=native' -g0 '-std=c++14' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -msse3 -pthread '-DTENSORFLOW_USE_XLA=1' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/util/batch_util.cc -o bazel-out/k8-opt-exec-50AE0418/bin/tensorflow/core/_objs/framework_internal_impl/batch_util.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,\r\n                 from ./tensorflow/core/framework/numeric_types.h:24,\r\n                 from ./tensorflow/core/framework/allocator.h:26,\r\n                 from ./tensorflow/core/framework/tensor.h:23,\r\n                 from ./tensorflow/core/util/batch_util.h:18,\r\n                 from tensorflow/core/util/batch_util.cc:16:\r\n\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;\r\n                                         ^\r\n./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]\r\n typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;\r\n                                         ^\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1314.325s, Critical Path: 521.70s\r\nINFO: 469 processes: 469 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n- I am trying to build tensorflow from source to enable the AVX and FMA instruction. i followed the exact set of instructions inorder to start building. As given [here](https://www.tensorflow.org/install/source#docker_linux_builds) i am building the cpu only version.\r\n- Since i am building from the latest one, I should be building from master branch. so I initiated using the below command: \r\n\r\n`bazel build --config=opt --local_ram_resources=4096  --verbose_failures //tensorflow/tools/pip_package:build_pip_packag`\r\n\r\n- after this the analysis phase was done. The error occurred during compilation.\r\n\r\nunable to diagnose the error. Please help\r\n", "comments": ["The compiler can be killed if Docker runs out of RAM -- Docker for Mac probably limits it. Once you've configured the memory to give to the container, configure the build to limit RAM usage with [these instructions](https://www.tensorflow.org/install/source#bazel_build_options).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40043\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40043\">No</a>\n"]}, {"number": 40042, "title": "Tensorflow getting slower and slower", "body": "I am benchmarking tensorflow with the cifar10_train.py script from https://github.com/tensorflow/models (v.1.13.0)\r\n\r\nRunning for 100000 steps, in the first  190 steps\r\n```\r\n2020-06-01 03:26:25.877249: step 0, loss = 4.68 (387.6 examples/sec; 0.330 sec/batch)\r\n2020-06-01 03:26:26.113249: step 10, loss = 4.60 (5423.4 examples/sec; 0.024 sec/batch)\r\n2020-06-01 03:26:26.251981: step 20, loss = 4.53 (9227.0 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:26.391191: step 30, loss = 4.36 (9194.3 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:26.532910: step 40, loss = 4.52 (9032.0 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:26.671122: step 50, loss = 4.30 (9261.1 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:26.810607: step 60, loss = 4.22 (9176.7 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:26.948641: step 70, loss = 4.18 (9273.1 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:27.084231: step 80, loss = 4.16 (9440.1 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:27.221754: step 90, loss = 4.20 (9307.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.5952\r\nI0601 03:26:27.554355 140231070107392 basic_session_run_hooks.py:692] global_step/sec: 59.5952\r\n2020-06-01 03:26:27.555495: step 100, loss = 4.20 (3835.3 examples/sec; 0.033 sec/batch)\r\n2020-06-01 03:26:27.698553: step 110, loss = 4.02 (8947.4 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:27.838008: step 120, loss = 3.89 (9178.6 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:27.980759: step 130, loss = 4.19 (8966.7 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:28.117689: step 140, loss = 3.95 (9347.8 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:28.253787: step 150, loss = 3.97 (9405.1 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:28.394747: step 160, loss = 3.93 (9080.5 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:28.536241: step 170, loss = 3.99 (9046.3 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:28.675036: step 180, loss = 3.81 (9222.2 examples/sec; 0.014 sec/batch)\r\n2020-06-01 03:26:28.812280: step 190, loss = 3.95 (9326.5 examples/sec; 0.014 sec/batch)\r\n```\r\nthen it progressively goes downhill.\r\n\r\nin the last 200 steps;\r\n```2020-06-01 04:39:15.068430: step 99800, loss = 0.61 (1638.6 examples/sec; 0.078 sec/batch)\r\n2020-06-01 04:39:15.610507: step 99810, loss = 0.69 (2361.4 examples/sec; 0.054 sec/batch)\r\n2020-06-01 04:39:16.214997: step 99820, loss = 0.60 (2117.5 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:16.808370: step 99830, loss = 0.58 (2157.2 examples/sec; 0.059 sec/batch)\r\n2020-06-01 04:39:17.416628: step 99840, loss = 0.74 (2104.4 examples/sec; 0.061 sec/batch)\r\n2020-06-01 04:39:18.010663: step 99850, loss = 0.79 (2154.8 examples/sec; 0.059 sec/batch)\r\n2020-06-01 04:39:18.617192: step 99860, loss = 0.67 (2110.4 examples/sec; 0.061 sec/batch)\r\n2020-06-01 04:39:19.218318: step 99870, loss = 0.58 (2129.3 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:19.815205: step 99880, loss = 0.57 (2144.5 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:20.411179: step 99890, loss = 0.79 (2147.7 examples/sec; 0.060 sec/batch)\r\nINFO:tensorflow:global_step/sec: 16.3652\r\nI0601 04:39:21.177534 139750045267712 basic_session_run_hooks.py:692] global_step/sec: 16.3652\r\n2020-06-01 04:39:21.178711: step 99900, loss = 0.71 (1667.6 examples/sec; 0.077 sec/batch)\r\n2020-06-01 04:39:21.718429: step 99910, loss = 0.72 (2371.8 examples/sec; 0.054 sec/batch)\r\n2020-06-01 04:39:22.323965: step 99920, loss = 0.77 (2113.8 examples/sec; 0.061 sec/batch)\r\n2020-06-01 04:39:22.923835: step 99930, loss = 0.66 (2133.8 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:23.516449: step 99940, loss = 0.59 (2159.9 examples/sec; 0.059 sec/batch)\r\n2020-06-01 04:39:24.106146: step 99950, loss = 0.73 (2170.6 examples/sec; 0.059 sec/batch)\r\n2020-06-01 04:39:24.705905: step 99960, loss = 0.54 (2134.2 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:25.302246: step 99970, loss = 0.71 (2146.4 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:25.897399: step 99980, loss = 0.63 (2150.7 examples/sec; 0.060 sec/batch)\r\n2020-06-01 04:39:26.500119: step 99990, loss = 0.66 (2123.7 examples/sec; 0.060 sec/batch)\r\n```\r\nThere is a steady drop in examples/sec\r\nis this normal?\r\n\r\nOS Ubuntu-16.04\r\nTensorflow 1.15.3 compiled from  source\r\npython-3.7.6\r\ntested for both cuda-10.0 and 10.1\r\nNvidia 418.56 driver\r\n\r\nNot sure if  these warnings are relevant\r\n```\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:130: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:123: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nW0601 03:30:25.042990 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:123: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:124: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.\r\n\r\nW0601 03:30:25.043155 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:124: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:125: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0601 03:30:25.045882 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:125: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:65: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW0601 03:30:25.046512 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:65: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nW0601 03:30:25.050264 139750045267712 deprecation.py:323] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nW0601 03:30:25.053800 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\nW0601 03:30:25.054384 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0601 03:30:25.055722 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0601 03:30:25.056567 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\r\nW0601 03:30:25.060451 139750045267712 deprecation.py:323] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:172: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\r\n\r\nW0601 03:30:25.071967 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:172: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nW0601 03:30:25.104054 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\r\nW0601 03:30:25.105133 139750045267712 deprecation.py:323] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:135: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nW0601 03:30:25.113710 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_input.py:135: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:203: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nW0601 03:30:25.115518 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:203: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:135: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0601 03:30:25.115780 139750045267712 deprecation.py:506] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:135: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:111: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nW0601 03:30:25.115960 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:111: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:93: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nW0601 03:30:25.125622 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:93: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:94: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nW0601 03:30:25.126545 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:94: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:215: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nW0601 03:30:25.138711 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:215: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:138: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\r\n\r\nW0601 03:30:25.170348 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:138: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:295: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\r\n\r\nW0601 03:30:25.234809 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:295: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:343: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\r\n\r\nW0601 03:30:25.235462 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:343: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\r\n\r\nINFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\r\nI0601 03:30:25.261295 139750045267712 summary_op_util.py:66] Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\r\nINFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\r\nI0601 03:30:25.262904 139750045267712 summary_op_util.py:66] Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\r\nINFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\r\nI0601 03:30:25.264466 139750045267712 summary_op_util.py:66] Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\r\nINFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\r\nI0601 03:30:25.266004 139750045267712 summary_op_util.py:66] Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:355: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nW0601 03:30:25.267595 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:355: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:362: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\r\n\r\nW0601 03:30:25.333956 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10.py:362: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nW0601 03:30:25.350221 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:84: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\r\n\r\nW0601 03:30:25.504043 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:84: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:110: The name tf.train.MonitoredTrainingSession is deprecated. Please use tf.compat.v1.train.MonitoredTrainingSession instead.\r\n\r\nW0601 03:30:25.504228 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:110: The name tf.train.MonitoredTrainingSession is deprecated. Please use tf.compat.v1.train.MonitoredTrainingSession instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:112: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\r\n\r\nW0601 03:30:25.504341 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:112: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:113: The name tf.train.NanTensorHook is deprecated. Please use tf.estimator.NanTensorHook instead.\r\n\r\nW0601 03:30:25.504446 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:113: The name tf.train.NanTensorHook is deprecated. Please use tf.estimator.NanTensorHook instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:115: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nW0601 03:30:25.504545 139750045267712 module_wrapper.py:139] From /home/bernard/python-dev/test/models-master/tutorials/image/cifar10/cifar10_train.py:115: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nI0601 03:30:25.504671 139750045267712 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\r\nWARNING:tensorflow:From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0601 03:30:25.617982 139750045267712 deprecation.py:323] From /home/bernard/opt/cuda-10.1/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n```\r\n", "comments": ["I encounter exactly same problem, but with a different test case. The case is exactly same as the tensorflow's first tutorial: \r\nhttps://www.tensorflow.org/tutorials/quickstart/beginner\r\n\r\nI copied its code as the following:\r\n\r\n```\r\nimport tensorflow as tf\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\npredictions = model(x_train[:1]).numpy()\r\npredictions\r\ntf.nn.softmax(predictions).numpy()\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\nloss_fn(y_train[:1], predictions).numpy()\r\nmodel.compile(optimizer='adam',\r\n              loss=loss_fn,\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=5)\r\n\r\n```\r\n\r\nMy becoming slower process is much faser than @beew test, the following is my test output when executing model.fit(), the last line in above codes:\r\n```\r\n>>> model.fit(x_train, y_train, epochs=5)\r\nEpoch 1/5\r\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2963 - accuracy: 0.9147\r\nEpoch 2/5\r\n1875/1875 [==============================] - 14s 7ms/step - loss: 0.1440 - accuracy: 0.9572\r\nEpoch 3/5\r\n1875/1875 [==============================] - 32s 17ms/step - loss: 0.1072 - accuracy: 0.9677\r\nEpoch 4/5\r\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.0884 - accuracy: 0.9725\r\nEpoch 5/5\r\n1875/1875 [==============================] - 25s 13ms/step - loss: 0.0745 - accuracy: 0.9768\r\n<tensorflow.python.keras.callbacks.History object at 0x7fd6590f5a90>\r\n>>> \r\n>>> \r\n>>> \r\n>>> model.fit(x_train, y_train, epochs=20)\r\nEpoch 1/20\r\n1875/1875 [==============================] - 87s 46ms/step - loss: 0.0649 - accuracy: 0.9794\r\nEpoch 2/20\r\n1875/1875 [==============================] - 173s 92ms/step - loss: 0.0581 - accuracy: 0.9818\r\nEpoch 3/20\r\n1875/1875 [==============================] - 177s 94ms/step - loss: 0.0522 - accuracy: 0.9836\r\nEpoch 4/20\r\n1875/1875 [==============================] - 195s 104ms/step - loss: 0.0486 - accuracy: 0.9841\r\nEpoch 5/20\r\n1875/1875 [==============================] - 209s 112ms/step - loss: 0.0436 - accuracy: 0.9857\r\nEpoch 6/20\r\n1875/1875 [==============================] - 208s 111ms/step - loss: 0.0407 - accuracy: 0.9862\r\n\r\n```\r\n\r\nI found this bug after upgrading to Tensorflow 2.2. At first, I suspect it's caused by my tensorflow program's mistake, but when I only testing Tensorflow's official tuturial codes, it shows the same wrong results(only wrong on performance data, the result is not wrong). So I suspect it's caused by TF2.2 or maybe other factors in my TF environment factors(e.g. my Nvidia driver) \r\nI'll try tf session(non-eager-mode) to see the performance, because I suspect the root cause is some change on the eager-mode implementation. I'll update it when it's done.\r\n\r\n", "I've recorded my test training time increasing results after a freshly bootup, it shows a clear gradual time increasing for these training sessions:\r\n\r\n```\r\n>>> model.compile(optimizer='adam',\r\n...               loss=loss_fn,\r\n...               metrics=['accuracy'])\r\n>>> model.fit(x_train, y_train, epochs=5)\r\nEpoch 1/5\r\n1875/1875 [==============================] - 3s 1ms/step - loss: 0.2945 - accuracy: 0.9139\r\nEpoch 2/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1452 - accuracy: 0.9568\r\nEpoch 3/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1078 - accuracy: 0.9665\r\nEpoch 4/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0890 - accuracy: 0.9726\r\nEpoch 5/5\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0772 - accuracy: 0.9753\r\n<tensorflow.python.keras.callbacks.History object at 0x7f65ae876470>\r\n>>> model.fit(x_train, y_train, epochs=50)\r\nEpoch 1/50\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0668 - accuracy: 0.9785\r\nEpoch 2/50\r\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0612 - accuracy: 0.9802\r\nEpoch 3/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0530 - accuracy: 0.9828\r\nEpoch 4/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0491 - accuracy: 0.9836\r\nEpoch 5/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0466 - accuracy: 0.9842\r\nEpoch 6/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0383 - accuracy: 0.9873\r\nEpoch 7/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0399 - accuracy: 0.9868\r\nEpoch 8/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0400 - accuracy: 0.9865\r\nEpoch 9/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0346 - accuracy: 0.9884\r\nEpoch 10/50\r\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0327 - accuracy: 0.9886\r\nEpoch 11/50\r\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.0329 - accuracy: 0.9886\r\nEpoch 12/50\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0300 - accuracy: 0.9892\r\nEpoch 13/50\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 0.0292 - accuracy: 0.9901\r\nEpoch 14/50\r\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0276 - accuracy: 0.9908\r\nEpoch 15/50\r\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0252 - accuracy: 0.9915\r\nEpoch 16/50\r\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0258 - accuracy: 0.9915\r\nEpoch 17/50\r\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0247 - accuracy: 0.9916\r\nEpoch 18/50\r\n1875/1875 [==============================] - 6s 3ms/step - loss: 0.0237 - accuracy: 0.9918\r\nEpoch 19/50\r\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.0240 - accuracy: 0.9918\r\nEpoch 20/50\r\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.0218 - accuracy: 0.9925\r\nEpoch 21/50\r\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.0207 - accuracy: 0.9927\r\nEpoch 22/50\r\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0233 - accuracy: 0.9926\r\nEpoch 23/50\r\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0222 - accuracy: 0.9923\r\nEpoch 24/50\r\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0224 - accuracy: 0.9928\r\nEpoch 25/50\r\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.0204 - accuracy: 0.9928\r\nEpoch 26/50\r\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.0208 - accuracy: 0.9934\r\nEpoch 27/50\r\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.0201 - accuracy: 0.9935\r\nEpoch 28/50\r\n1875/1875 [==============================] - 13s 7ms/step - loss: 0.0189 - accuracy: 0.9935\r\nEpoch 29/50\r\n1875/1875 [==============================] - 16s 8ms/step - loss: 0.0189 - accuracy: 0.9935\r\nEpoch 30/50\r\n1875/1875 [==============================] - 13s 7ms/step - loss: 0.0193 - accuracy: 0.9936\r\nEpoch 31/50\r\n1875/1875 [==============================] - 19s 10ms/step - loss: 0.0177 - accuracy: 0.9936\r\nEpoch 32/50\r\n1875/1875 [==============================] - 14s 8ms/step - loss: 0.0177 - accuracy: 0.9942\r\nEpoch 33/50\r\n1875/1875 [==============================] - 13s 7ms/step - loss: 0.0176 - accuracy: 0.9939\r\nEpoch 34/50\r\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0199 - accuracy: 0.9932\r\nEpoch 35/50\r\n1875/1875 [==============================] - 15s 8ms/step - loss: 0.0174 - accuracy: 0.9940\r\nEpoch 36/50\r\n1875/1875 [==============================] - 14s 8ms/step - loss: 0.0171 - accuracy: 0.9940\r\nEpoch 37/50\r\n1875/1875 [==============================] - 17s 9ms/step - loss: 0.0177 - accuracy: 0.9940\r\nEpoch 38/50\r\n1875/1875 [==============================] - 21s 11ms/step - loss: 0.0182 - accuracy: 0.9941\r\nEpoch 39/50\r\n1875/1875 [==============================] - 16s 8ms/step - loss: 0.0148 - accuracy: 0.9952\r\nEpoch 40/50\r\n1875/1875 [==============================] - 22s 12ms/step - loss: 0.0167 - accuracy: 0.9940\r\nEpoch 41/50\r\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.0171 - accuracy: 0.9944\r\nEpoch 42/50\r\n1875/1875 [==============================] - 24s 13ms/step - loss: 0.0167 - accuracy: 0.9943\r\nEpoch 43/50\r\n1875/1875 [==============================] - 20s 11ms/step - loss: 0.0170 - accuracy: 0.9948\r\nEpoch 44/50\r\n1875/1875 [==============================] - 26s 14ms/step - loss: 0.0156 - accuracy: 0.9948\r\nEpoch 45/50\r\n1875/1875 [==============================] - 52s 28ms/step - loss: 0.0161 - accuracy: 0.9946\r\nEpoch 46/50\r\n1875/1875 [==============================] - 42s 22ms/step - loss: 0.0149 - accuracy: 0.9948\r\nEpoch 47/50\r\n1875/1875 [==============================] - 22s 12ms/step - loss: 0.0135 - accuracy: 0.9952\r\nEpoch 48/50\r\n1875/1875 [==============================] - 63s 33ms/step - loss: 0.0156 - accuracy: 0.9950\r\nEpoch 49/50\r\n1875/1875 [==============================] - 64s 34ms/step - loss: 0.0143 - accuracy: 0.9952\r\nEpoch 50/50\r\n1875/1875 [==============================] - 239s 127ms/step - loss: 0.0160 - accuracy: 0.9948\r\n\r\n```", "@beew,\r\nCould you please the provide the complete code or link the `cifar10_train.py` file you are using.\r\nI was unable to find that file in the link you have given. Thanks!", "@amahendrakar \r\n\r\nHere is the link, https://github.com/tensorflow/models/releases, download V1.13.0 (between official models 1.11.0 and 2.0) untar it, go to /models-1.13.0/tutorials/image/cifar10/ the file is called cifar10_train.py. The models have changed since 2.0 that's why you couldn't find it in master branch.", "@beew,\r\nI did not observe any difference, even after running for 11000 steps. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3eda65c1b3870daa83150294514fca51/40042.ipynb#scrollTo=GfguuksO1epZ). Thanks!", "@amahendrakar \r\n\r\nHi, doesn't seem like you were using GPU. Also in my test the first 10,000 steps actually the slow down wasn't that drastic, my test ran 100,000 steps.", "@amahendrakar \r\n\r\nThe first 11000 steps don't show appreciable deterioration for a fresh reboot, but it deteriorates when the number of steps increases. So I think you need to let it run for longer to see it and it may also be GPU related. Here are my 11000 steps:\r\n```\r\n$ time python /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py -max_steps=11000\r\n....\r\n2020-06-11 04:27:32.955302: step 0, loss = 4.67 (381.8 examples/sec; 0.335 sec/batch)\r\n2020-06-11 04:27:33.186719: step 10, loss = 4.63 (5530.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:27:33.327838: step 20, loss = 4.53 (9070.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:33.467326: step 30, loss = 4.43 (9176.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:33.610375: step 40, loss = 4.49 (8948.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:33.753143: step 50, loss = 4.52 (8965.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:33.892402: step 60, loss = 4.25 (9191.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:34.034290: step 70, loss = 4.19 (9021.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:34.175995: step 80, loss = 4.23 (9032.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:34.319638: step 90, loss = 4.16 (8911.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.6415\r\nI0611 04:27:34.631107 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.6415\r\n2020-06-11 04:27:34.632395: step 100, loss = 4.16 (4092.6 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:27:34.767608: step 110, loss = 4.22 (9466.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:34.904086: step 120, loss = 3.99 (9379.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.042077: step 130, loss = 3.97 (9276.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.181722: step 140, loss = 4.19 (9166.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.321593: step 150, loss = 4.01 (9151.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.461718: step 160, loss = 3.89 (9134.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.601510: step 170, loss = 3.88 (9156.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.739833: step 180, loss = 3.81 (9253.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:35.877067: step 190, loss = 3.79 (9327.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.1852\r\nI0611 04:27:36.165189 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.1852\r\n2020-06-11 04:27:36.166333: step 200, loss = 3.72 (4424.9 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:27:36.302399: step 210, loss = 3.68 (9407.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:36.443810: step 220, loss = 3.80 (9051.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:36.584247: step 230, loss = 3.86 (9114.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:36.728274: step 240, loss = 3.59 (8887.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:36.867066: step 250, loss = 3.64 (9222.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:37.008673: step 260, loss = 3.63 (9039.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:37.150891: step 270, loss = 3.64 (9000.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:37.294436: step 280, loss = 3.50 (8917.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:37.438387: step 290, loss = 3.45 (8891.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.9654\r\nI0611 04:27:37.753268 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 62.9654\r\n2020-06-11 04:27:37.754348: step 300, loss = 3.60 (4051.1 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:27:37.894518: step 310, loss = 3.89 (9131.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.034058: step 320, loss = 3.59 (9173.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.171716: step 330, loss = 3.58 (9298.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.312762: step 340, loss = 3.56 (9075.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.449541: step 350, loss = 3.58 (9358.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.587536: step 360, loss = 3.50 (9275.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.726921: step 370, loss = 3.41 (9183.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:38.865896: step 380, loss = 3.40 (9210.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:39.012082: step 390, loss = 3.26 (8756.0 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.4734\r\nI0611 04:27:39.328744 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.4734\r\n2020-06-11 04:27:39.329746: step 400, loss = 3.35 (4029.4 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:27:39.465220: step 410, loss = 3.38 (9448.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:39.606768: step 420, loss = 3.34 (9043.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:39.752638: step 430, loss = 3.36 (8775.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:39.897195: step 440, loss = 3.43 (8854.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:40.040860: step 450, loss = 3.31 (8909.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:40.185227: step 460, loss = 3.27 (8866.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:40.331379: step 470, loss = 3.25 (8758.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:40.476755: step 480, loss = 3.98 (8804.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:40.618876: step 490, loss = 3.12 (9006.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.2509\r\nI0611 04:27:40.935133 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 62.2509\r\n2020-06-11 04:27:40.936228: step 500, loss = 3.24 (4033.3 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:27:41.073313: step 510, loss = 3.18 (9337.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:41.210720: step 520, loss = 3.37 (9315.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:41.352873: step 530, loss = 2.98 (9004.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:41.489803: step 540, loss = 2.90 (9347.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:41.626561: step 550, loss = 3.20 (9359.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:41.765570: step 560, loss = 2.99 (9208.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:41.903532: step 570, loss = 3.08 (9277.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:42.041490: step 580, loss = 3.06 (9278.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:42.180293: step 590, loss = 2.91 (9221.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.8768\r\nI0611 04:27:42.476512 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.8768\r\n2020-06-11 04:27:42.477675: step 600, loss = 3.06 (4304.2 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:27:42.617074: step 610, loss = 3.08 (9182.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:42.761314: step 620, loss = 3.17 (8874.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:42.906056: step 630, loss = 2.98 (8843.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:43.053027: step 640, loss = 2.88 (8709.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:43.197936: step 650, loss = 3.03 (8833.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:43.344622: step 660, loss = 2.66 (8726.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:43.487085: step 670, loss = 2.78 (8984.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:43.630279: step 680, loss = 2.90 (8938.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:43.775820: step 690, loss = 2.92 (8794.8 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.5962\r\nI0611 04:27:44.074067 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 62.5962\r\n2020-06-11 04:27:44.075155: step 700, loss = 2.69 (4276.0 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:27:44.210478: step 710, loss = 3.01 (9459.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:44.351301: step 720, loss = 3.01 (9089.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:44.491551: step 730, loss = 2.88 (9126.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:44.634785: step 740, loss = 2.79 (8936.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:44.776931: step 750, loss = 2.83 (9004.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:44.922725: step 760, loss = 2.80 (8779.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:45.064207: step 770, loss = 2.63 (9047.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:45.208257: step 780, loss = 2.69 (8885.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:45.354158: step 790, loss = 2.59 (8773.0 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.8281\r\nI0611 04:27:45.640747 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.8281\r\n2020-06-11 04:27:45.641766: step 800, loss = 2.80 (4450.4 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:27:45.783063: step 810, loss = 2.74 (9059.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:45.920605: step 820, loss = 2.65 (9306.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.057333: step 830, loss = 2.91 (9361.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.196795: step 840, loss = 2.51 (9178.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.338435: step 850, loss = 2.62 (9037.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.475961: step 860, loss = 2.62 (9307.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.617566: step 870, loss = 2.67 (9039.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.755489: step 880, loss = 2.46 (9280.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:46.892917: step 890, loss = 2.51 (9314.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.7268\r\nI0611 04:27:47.185714 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.7268\r\n2020-06-11 04:27:47.186674: step 900, loss = 2.47 (4357.3 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:27:47.329730: step 910, loss = 2.66 (8947.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:47.468070: step 920, loss = 2.50 (9252.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:47.608738: step 930, loss = 2.44 (9099.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:47.747665: step 940, loss = 2.57 (9213.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:47.889006: step 950, loss = 2.80 (9056.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:48.029397: step 960, loss = 2.31 (9117.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:48.168861: step 970, loss = 2.48 (9178.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:48.308221: step 980, loss = 2.39 (9184.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:48.448668: step 990, loss = 2.28 (9113.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.0453\r\nI0611 04:27:48.747141 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.0453\r\n2020-06-11 04:27:48.748180: step 1000, loss = 2.25 (4273.6 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:27:48.883347: step 1010, loss = 2.39 (9469.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.020023: step 1020, loss = 2.51 (9365.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.159531: step 1030, loss = 2.37 (9175.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.296901: step 1040, loss = 2.39 (9317.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.433832: step 1050, loss = 2.45 (9347.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.571829: step 1060, loss = 2.28 (9275.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.714108: step 1070, loss = 2.43 (8996.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.853466: step 1080, loss = 2.46 (9185.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:49.991116: step 1090, loss = 2.39 (9298.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.6978\r\nI0611 04:27:50.292757 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.6978\r\n2020-06-11 04:27:50.293838: step 1100, loss = 2.13 (4228.3 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:27:50.434275: step 1110, loss = 2.41 (9114.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:50.575171: step 1120, loss = 2.44 (9084.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:50.716085: step 1130, loss = 2.52 (9083.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:50.856620: step 1140, loss = 2.43 (9108.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:50.994910: step 1150, loss = 2.37 (9255.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:51.134072: step 1160, loss = 2.20 (9197.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:51.274212: step 1170, loss = 2.38 (9133.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:51.414515: step 1180, loss = 2.28 (9123.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:51.555359: step 1190, loss = 2.26 (9088.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.0496\r\nI0611 04:27:51.854050 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.0496\r\n2020-06-11 04:27:51.855126: step 1200, loss = 2.27 (4270.0 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:27:51.999235: step 1210, loss = 2.21 (8882.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:52.137403: step 1220, loss = 2.12 (9264.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:52.279425: step 1230, loss = 2.14 (9012.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:52.420257: step 1240, loss = 2.09 (9088.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:52.554711: step 1250, loss = 2.43 (9520.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:27:52.695310: step 1260, loss = 2.08 (9103.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:52.833946: step 1270, loss = 2.19 (9232.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:52.975038: step 1280, loss = 2.08 (9072.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:53.113730: step 1290, loss = 2.19 (9229.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.765\r\nI0611 04:27:53.398098 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.765\r\n2020-06-11 04:27:53.399148: step 1300, loss = 2.19 (4484.6 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:27:53.534662: step 1310, loss = 2.08 (9445.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:53.684459: step 1320, loss = 2.01 (8545.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:53.832489: step 1330, loss = 2.19 (8646.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:53.977629: step 1340, loss = 1.91 (8819.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:54.125005: step 1350, loss = 2.24 (8685.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:54.272478: step 1360, loss = 1.97 (8679.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:54.422133: step 1370, loss = 2.15 (8553.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:54.567433: step 1380, loss = 2.14 (8809.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:54.717551: step 1390, loss = 2.05 (8526.6 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 60.6707\r\nI0611 04:27:55.046334 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 60.6707\r\n2020-06-11 04:27:55.047359: step 1400, loss = 2.07 (3881.0 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:27:55.188989: step 1410, loss = 2.07 (9037.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:55.328665: step 1420, loss = 2.06 (9164.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:55.470714: step 1430, loss = 2.09 (9011.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:55.609635: step 1440, loss = 2.11 (9213.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:55.753310: step 1450, loss = 1.89 (8909.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:55.889236: step 1460, loss = 2.08 (9416.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:56.027725: step 1470, loss = 2.15 (9242.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:56.169722: step 1480, loss = 1.86 (9014.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:56.308020: step 1490, loss = 2.06 (9255.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.2145\r\nI0611 04:27:56.579751 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.2145\r\n2020-06-11 04:27:56.580824: step 1500, loss = 2.03 (4691.9 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:27:56.720696: step 1510, loss = 2.06 (9151.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:56.860493: step 1520, loss = 2.02 (9156.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:56.998201: step 1530, loss = 1.93 (9295.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:57.136820: step 1540, loss = 1.83 (9234.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:57.275500: step 1550, loss = 2.02 (9229.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:57.413880: step 1560, loss = 1.98 (9249.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:57.556714: step 1570, loss = 2.01 (8961.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:57.694371: step 1580, loss = 1.92 (9298.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:57.833212: step 1590, loss = 1.82 (9219.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.774\r\nI0611 04:27:58.123554 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.774\r\n2020-06-11 04:27:58.124589: step 1600, loss = 1.80 (4392.9 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:27:58.263127: step 1610, loss = 1.80 (9239.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:58.406863: step 1620, loss = 2.00 (8905.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:58.553307: step 1630, loss = 1.85 (8740.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:58.696889: step 1640, loss = 1.92 (8914.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:27:58.850209: step 1650, loss = 1.92 (8348.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:58.996674: step 1660, loss = 1.91 (8739.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:59.144877: step 1670, loss = 1.58 (8637.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:59.294643: step 1680, loss = 1.79 (8546.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:27:59.446091: step 1690, loss = 1.92 (8451.8 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 61.2116\r\nI0611 04:27:59.757245 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 61.2116\r\n2020-06-11 04:27:59.758321: step 1700, loss = 1.99 (4099.5 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:27:59.895976: step 1710, loss = 1.71 (9298.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:00.041210: step 1720, loss = 1.67 (8813.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:00.182081: step 1730, loss = 1.69 (9086.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:00.321197: step 1740, loss = 1.77 (9200.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:00.463599: step 1750, loss = 1.68 (8988.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:00.604432: step 1760, loss = 1.74 (9088.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:00.741476: step 1770, loss = 1.88 (9340.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:00.879643: step 1780, loss = 1.69 (9264.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:01.020593: step 1790, loss = 1.90 (9081.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.085\r\nI0611 04:28:01.293693 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.085\r\n2020-06-11 04:28:01.294802: step 1800, loss = 1.63 (4667.9 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:01.429302: step 1810, loss = 1.57 (9516.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:01.573563: step 1820, loss = 1.68 (8873.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:01.716833: step 1830, loss = 1.92 (8934.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:01.860717: step 1840, loss = 1.83 (8896.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:02.006614: step 1850, loss = 1.72 (8773.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:02.146983: step 1860, loss = 1.59 (9119.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:02.291140: step 1870, loss = 1.82 (8878.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:02.431464: step 1880, loss = 1.83 (9121.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:02.576683: step 1890, loss = 1.83 (8814.3 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.8771\r\nI0611 04:28:02.859189 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.8771\r\n2020-06-11 04:28:02.860233: step 1900, loss = 1.95 (4514.1 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:02.995343: step 1910, loss = 1.71 (9473.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.135482: step 1920, loss = 1.88 (9133.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.276372: step 1930, loss = 1.80 (9085.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.413311: step 1940, loss = 1.59 (9347.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.553943: step 1950, loss = 1.66 (9101.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.697181: step 1960, loss = 1.61 (8936.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.836250: step 1970, loss = 1.52 (9204.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:03.972775: step 1980, loss = 1.44 (9375.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:04.113037: step 1990, loss = 1.70 (9125.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.1035\r\nI0611 04:28:04.395224 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.1035\r\n2020-06-11 04:28:04.396266: step 2000, loss = 1.79 (4519.2 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:04.534131: step 2010, loss = 1.64 (9284.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:04.680229: step 2020, loss = 1.60 (8761.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:04.827545: step 2030, loss = 1.39 (8688.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:04.972897: step 2040, loss = 1.68 (8806.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:05.120024: step 2050, loss = 1.62 (8700.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:05.266260: step 2060, loss = 1.53 (8753.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:05.413150: step 2070, loss = 1.66 (8714.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:05.561829: step 2080, loss = 1.71 (8609.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:05.708762: step 2090, loss = 1.53 (8711.4 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 61.3788\r\nI0611 04:28:06.024471 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 61.3788\r\n2020-06-11 04:28:06.025542: step 2100, loss = 1.58 (4040.6 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:28:06.162031: step 2110, loss = 1.78 (9378.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:06.303639: step 2120, loss = 1.51 (9039.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:06.445495: step 2130, loss = 1.69 (9023.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:06.586894: step 2140, loss = 1.61 (9052.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:06.725113: step 2150, loss = 1.64 (9260.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:06.865765: step 2160, loss = 1.80 (9100.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:07.004880: step 2170, loss = 1.43 (9201.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:07.144426: step 2180, loss = 1.55 (9172.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:07.282302: step 2190, loss = 1.43 (9283.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.3064\r\nI0611 04:28:07.555678 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.3064\r\n2020-06-11 04:28:07.556626: step 2200, loss = 1.60 (4666.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:07.694056: step 2210, loss = 1.45 (9313.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:07.834168: step 2220, loss = 1.72 (9135.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:07.977550: step 2230, loss = 1.55 (8927.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:08.120431: step 2240, loss = 1.58 (8958.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:08.260520: step 2250, loss = 1.47 (9137.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:08.400789: step 2260, loss = 1.38 (9125.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:08.544630: step 2270, loss = 1.42 (8898.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:08.686751: step 2280, loss = 1.65 (9006.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:08.829914: step 2290, loss = 1.42 (8940.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.8162\r\nI0611 04:28:09.122696 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.8162\r\n2020-06-11 04:28:09.123646: step 2300, loss = 1.55 (4357.6 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:09.260340: step 2310, loss = 1.59 (9364.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:09.401968: step 2320, loss = 1.44 (9037.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:09.545429: step 2330, loss = 1.61 (8922.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:09.685622: step 2340, loss = 1.84 (9130.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:09.825876: step 2350, loss = 1.48 (9126.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:09.969780: step 2360, loss = 1.47 (8894.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:10.111342: step 2370, loss = 1.66 (9041.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:10.256228: step 2380, loss = 1.52 (8834.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:10.401204: step 2390, loss = 1.62 (8829.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.986\r\nI0611 04:28:10.685530 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.986\r\n2020-06-11 04:28:10.686505: step 2400, loss = 1.59 (4486.4 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:10.825400: step 2410, loss = 1.59 (9215.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:10.968483: step 2420, loss = 1.46 (8946.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:11.111696: step 2430, loss = 1.31 (8937.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:11.255595: step 2440, loss = 1.37 (8895.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:11.400304: step 2450, loss = 1.49 (8845.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:11.546790: step 2460, loss = 1.30 (8738.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:11.691654: step 2470, loss = 1.38 (8835.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:11.834162: step 2480, loss = 1.45 (8982.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:11.977326: step 2490, loss = 1.52 (8940.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.5637\r\nI0611 04:28:12.258768 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.5637\r\n2020-06-11 04:28:12.259867: step 2500, loss = 1.65 (4530.2 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:12.397086: step 2510, loss = 1.46 (9328.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:12.531783: step 2520, loss = 1.51 (9502.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:12.668618: step 2530, loss = 1.46 (9354.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:12.807123: step 2540, loss = 1.29 (9241.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:12.943662: step 2550, loss = 1.34 (9374.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:13.086319: step 2560, loss = 1.42 (8972.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:13.225585: step 2570, loss = 1.54 (9191.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:13.367393: step 2580, loss = 1.32 (9026.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:13.506406: step 2590, loss = 1.42 (9207.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.9977\r\nI0611 04:28:13.797260 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.9977\r\n2020-06-11 04:28:13.798250: step 2600, loss = 1.42 (4385.8 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:13.939400: step 2610, loss = 1.67 (9068.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.078362: step 2620, loss = 1.38 (9211.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.216121: step 2630, loss = 1.52 (9291.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.357172: step 2640, loss = 1.45 (9074.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.499353: step 2650, loss = 1.35 (9002.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.641697: step 2660, loss = 1.51 (8992.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.780187: step 2670, loss = 1.46 (9242.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:14.918353: step 2680, loss = 1.49 (9264.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:15.057329: step 2690, loss = 1.41 (9210.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.2934\r\nI0611 04:28:15.328820 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.2934\r\n2020-06-11 04:28:15.329948: step 2700, loss = 1.46 (4695.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:15.465095: step 2710, loss = 1.29 (9471.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:15.600768: step 2720, loss = 1.39 (9434.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:15.736356: step 2730, loss = 1.37 (9440.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:15.872066: step 2740, loss = 1.52 (9431.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:16.010805: step 2750, loss = 1.37 (9226.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:16.154693: step 2760, loss = 1.24 (8895.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:16.293328: step 2770, loss = 1.26 (9232.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:16.432735: step 2780, loss = 1.45 (9181.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:16.571279: step 2790, loss = 1.30 (9239.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4137\r\nI0611 04:28:16.857556 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.4137\r\n2020-06-11 04:28:16.858511: step 2800, loss = 1.27 (4456.3 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:16.993377: step 2810, loss = 1.27 (9490.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:17.131934: step 2820, loss = 1.36 (9238.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:17.274722: step 2830, loss = 1.47 (8964.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:17.416327: step 2840, loss = 1.22 (9039.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:17.553998: step 2850, loss = 1.45 (9297.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:17.694849: step 2860, loss = 1.22 (9087.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:17.834606: step 2870, loss = 1.42 (9158.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:17.979715: step 2880, loss = 1.52 (8821.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:18.122385: step 2890, loss = 1.28 (8971.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.8022\r\nI0611 04:28:18.424891 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.8022\r\n2020-06-11 04:28:18.425996: step 2900, loss = 1.53 (4215.9 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:18.564849: step 2910, loss = 1.27 (9218.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:18.704870: step 2920, loss = 1.44 (9141.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:18.843796: step 2930, loss = 1.31 (9213.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:18.986989: step 2940, loss = 1.38 (8939.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:19.125146: step 2950, loss = 1.24 (9264.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:19.265147: step 2960, loss = 1.36 (9142.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:19.403843: step 2970, loss = 1.36 (9228.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:19.546371: step 2980, loss = 0.99 (8980.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:19.684841: step 2990, loss = 1.13 (9243.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4162\r\nI0611 04:28:19.953567 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.4162\r\n2020-06-11 04:28:19.954669: step 3000, loss = 1.29 (4743.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:20.089759: step 3010, loss = 1.29 (9475.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:20.229243: step 3020, loss = 1.25 (9177.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:20.369262: step 3030, loss = 1.14 (9141.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:20.512574: step 3040, loss = 1.33 (8931.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:20.657727: step 3050, loss = 1.32 (8818.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:20.802909: step 3060, loss = 1.44 (8816.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:20.946857: step 3070, loss = 1.33 (8892.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:21.089091: step 3080, loss = 1.18 (8999.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:21.231081: step 3090, loss = 1.30 (9014.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.0428\r\nI0611 04:28:21.515031 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.0428\r\n2020-06-11 04:28:21.516033: step 3100, loss = 1.12 (4491.9 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:21.651908: step 3110, loss = 1.30 (9420.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:21.794006: step 3120, loss = 1.22 (9007.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:21.935569: step 3130, loss = 1.28 (9041.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:22.077376: step 3140, loss = 1.22 (9026.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:22.216491: step 3150, loss = 1.18 (9201.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:22.360299: step 3160, loss = 1.25 (8900.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:22.498276: step 3170, loss = 1.50 (9276.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:22.636762: step 3180, loss = 1.34 (9242.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:22.772411: step 3190, loss = 1.08 (9436.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.5685\r\nI0611 04:28:23.040149 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.5685\r\n2020-06-11 04:28:23.041331: step 3200, loss = 1.19 (4759.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:23.179428: step 3210, loss = 1.32 (9269.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:23.316756: step 3220, loss = 1.31 (9320.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:23.458216: step 3230, loss = 1.10 (9048.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:23.599998: step 3240, loss = 1.30 (9028.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:23.739673: step 3250, loss = 1.34 (9164.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:23.880285: step 3260, loss = 1.18 (9103.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:24.017651: step 3270, loss = 1.20 (9318.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:24.154063: step 3280, loss = 1.31 (9383.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:24.294760: step 3290, loss = 1.05 (9097.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.1152\r\nI0611 04:28:24.575889 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.1152\r\n2020-06-11 04:28:24.576966: step 3300, loss = 1.30 (4535.6 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:24.712383: step 3310, loss = 0.99 (9452.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:24.847641: step 3320, loss = 1.16 (9463.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:24.986500: step 3330, loss = 1.32 (9218.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:25.126814: step 3340, loss = 1.33 (9122.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:25.268971: step 3350, loss = 1.27 (9004.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:25.408569: step 3360, loss = 1.26 (9169.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:25.545173: step 3370, loss = 1.15 (9370.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:25.681361: step 3380, loss = 1.09 (9398.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:25.821738: step 3390, loss = 1.37 (9118.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4914\r\nI0611 04:28:26.102803 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.4914\r\n2020-06-11 04:28:26.103923: step 3400, loss = 1.11 (4536.0 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:26.241826: step 3410, loss = 1.09 (9282.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:26.381757: step 3420, loss = 0.97 (9147.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:26.521165: step 3430, loss = 1.37 (9181.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:26.662651: step 3440, loss = 1.14 (9046.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:26.800103: step 3450, loss = 1.51 (9312.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:26.935983: step 3460, loss = 1.21 (9420.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:27.073539: step 3470, loss = 1.17 (9305.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:27.212940: step 3480, loss = 1.25 (9182.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:27.353444: step 3490, loss = 1.38 (9110.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.0128\r\nI0611 04:28:27.640963 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.0128\r\n2020-06-11 04:28:27.641972: step 3500, loss = 0.92 (4436.3 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:27.780322: step 3510, loss = 1.46 (9252.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:27.920031: step 3520, loss = 1.17 (9162.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.062014: step 3530, loss = 1.04 (9015.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.204176: step 3540, loss = 1.34 (9003.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.343141: step 3550, loss = 1.15 (9211.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.486005: step 3560, loss = 1.12 (8959.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.630519: step 3570, loss = 1.22 (8857.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.773704: step 3580, loss = 1.22 (8939.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:28.918011: step 3590, loss = 1.14 (8869.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.6172\r\nI0611 04:28:29.212862 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.6172\r\n2020-06-11 04:28:29.213846: step 3600, loss = 1.16 (4326.7 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:29.350958: step 3610, loss = 1.15 (9335.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:29.493637: step 3620, loss = 1.17 (8971.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:29.634805: step 3630, loss = 1.20 (9067.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:29.779139: step 3640, loss = 1.15 (8868.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:29.915922: step 3650, loss = 1.11 (9357.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:30.056244: step 3660, loss = 1.16 (9121.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:30.198455: step 3670, loss = 1.09 (9001.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:30.341215: step 3680, loss = 1.27 (8965.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:30.483455: step 3690, loss = 1.33 (8999.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.8116\r\nI0611 04:28:30.755818 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.8116\r\n2020-06-11 04:28:30.756994: step 3700, loss = 1.17 (4679.3 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:30.895410: step 3710, loss = 1.30 (9247.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:31.035213: step 3720, loss = 1.06 (9156.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:31.177477: step 3730, loss = 1.00 (8997.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:31.317096: step 3740, loss = 1.06 (9167.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:31.464287: step 3750, loss = 0.99 (8696.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:31.610320: step 3760, loss = 1.14 (8765.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:31.756157: step 3770, loss = 1.41 (8776.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:31.896461: step 3780, loss = 1.08 (9123.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:32.038738: step 3790, loss = 1.07 (8996.5 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.2452\r\nI0611 04:28:32.336962 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.2452\r\n2020-06-11 04:28:32.338079: step 3800, loss = 1.05 (4276.0 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:32.471314: step 3810, loss = 1.18 (9607.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:32.617415: step 3820, loss = 1.22 (8761.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:32.767214: step 3830, loss = 1.14 (8544.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:32.915587: step 3840, loss = 1.11 (8626.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:33.067249: step 3850, loss = 1.07 (8439.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:33.213853: step 3860, loss = 1.18 (8731.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:33.363682: step 3870, loss = 1.05 (8543.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:33.509409: step 3880, loss = 1.18 (8783.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:33.659703: step 3890, loss = 1.05 (8516.6 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 61.6655\r\nI0611 04:28:33.958612 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 61.6655\r\n2020-06-11 04:28:33.959817: step 3900, loss = 1.32 (4265.0 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:34.093364: step 3910, loss = 1.15 (9584.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:34.233465: step 3920, loss = 1.02 (9136.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:34.374543: step 3930, loss = 0.98 (9073.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:34.515375: step 3940, loss = 1.14 (9088.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:34.652730: step 3950, loss = 1.11 (9319.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:34.789525: step 3960, loss = 0.98 (9357.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:34.929568: step 3970, loss = 1.12 (9140.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:35.073438: step 3980, loss = 1.01 (8896.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:35.212464: step 3990, loss = 1.12 (9206.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.7714\r\nI0611 04:28:35.502488 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.7714\r\n2020-06-11 04:28:35.503415: step 4000, loss = 0.89 (4399.3 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:35.641377: step 4010, loss = 1.04 (9278.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:35.784687: step 4020, loss = 1.07 (8931.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:35.930436: step 4030, loss = 1.03 (8782.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:36.076689: step 4040, loss = 1.10 (8752.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:36.216968: step 4050, loss = 1.09 (9124.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:36.360508: step 4060, loss = 1.07 (8917.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:36.502828: step 4070, loss = 1.22 (8993.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:36.645480: step 4080, loss = 1.20 (8973.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:36.785748: step 4090, loss = 0.91 (9125.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.0775\r\nI0611 04:28:37.087844 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.0775\r\n2020-06-11 04:28:37.088970: step 4100, loss = 1.30 (4221.3 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:37.226947: step 4110, loss = 1.10 (9276.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:37.366920: step 4120, loss = 1.16 (9144.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:37.510434: step 4130, loss = 0.86 (8918.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:37.651038: step 4140, loss = 1.14 (9103.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:37.797042: step 4150, loss = 1.07 (8766.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:37.942526: step 4160, loss = 1.03 (8798.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:38.086202: step 4170, loss = 1.11 (8908.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:38.228968: step 4180, loss = 1.17 (8965.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:38.377712: step 4190, loss = 1.20 (8605.4 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.9122\r\nI0611 04:28:38.652506 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.9122\r\n2020-06-11 04:28:38.653576: step 4200, loss = 1.00 (4639.9 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:38.790640: step 4210, loss = 1.03 (9338.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:38.926854: step 4220, loss = 1.04 (9397.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.066868: step 4230, loss = 1.10 (9142.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.208131: step 4240, loss = 1.14 (9061.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.349261: step 4250, loss = 1.12 (9069.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.489423: step 4260, loss = 1.08 (9132.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.630424: step 4270, loss = 1.03 (9078.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.770747: step 4280, loss = 1.24 (9121.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:39.915855: step 4290, loss = 0.94 (8821.1 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.0567\r\nI0611 04:28:40.189612 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.0567\r\n2020-06-11 04:28:40.190642: step 4300, loss = 1.06 (4658.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:40.327216: step 4310, loss = 1.09 (9372.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:40.463900: step 4320, loss = 0.92 (9364.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:40.599018: step 4330, loss = 1.08 (9473.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:40.736278: step 4340, loss = 1.09 (9325.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:40.873922: step 4350, loss = 1.03 (9299.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:41.008753: step 4360, loss = 1.12 (9493.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:41.141947: step 4370, loss = 1.17 (9610.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:41.283063: step 4380, loss = 1.04 (9070.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:41.424973: step 4390, loss = 0.99 (9019.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.5582\r\nI0611 04:28:41.692068 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 66.5582\r\n2020-06-11 04:28:41.693271: step 4400, loss = 0.97 (4770.8 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:41.827903: step 4410, loss = 0.96 (9507.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 04:28:41.967284: step 4420, loss = 0.89 (9183.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.106960: step 4430, loss = 1.19 (9164.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.248048: step 4440, loss = 1.06 (9072.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.385953: step 4450, loss = 1.23 (9281.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.524341: step 4460, loss = 1.02 (9249.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.661220: step 4470, loss = 1.10 (9351.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.800011: step 4480, loss = 1.06 (9222.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:42.942207: step 4490, loss = 1.16 (9001.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.7713\r\nI0611 04:28:43.212461 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 65.7713\r\n2020-06-11 04:28:43.213604: step 4500, loss = 1.13 (4716.3 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:43.349898: step 4510, loss = 1.10 (9391.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:43.496617: step 4520, loss = 0.91 (8724.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:43.640609: step 4530, loss = 0.89 (8889.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:43.781794: step 4540, loss = 1.11 (9066.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:43.928034: step 4550, loss = 1.08 (8752.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:44.073007: step 4560, loss = 1.04 (8829.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:44.218695: step 4570, loss = 1.18 (8785.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:44.362785: step 4580, loss = 1.09 (8883.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:44.509187: step 4590, loss = 1.07 (8743.0 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.1848\r\nI0611 04:28:44.795183 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.1848\r\n2020-06-11 04:28:44.796327: step 4600, loss = 1.15 (4457.7 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:44.933491: step 4610, loss = 1.09 (9332.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.070090: step 4620, loss = 1.15 (9370.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.209318: step 4630, loss = 1.20 (9193.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.353969: step 4640, loss = 0.91 (8848.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.492352: step 4650, loss = 1.07 (9249.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.633749: step 4660, loss = 1.16 (9052.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.776541: step 4670, loss = 1.17 (8964.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:45.920096: step 4680, loss = 1.07 (8916.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:46.059900: step 4690, loss = 0.85 (9155.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.052\r\nI0611 04:28:46.356371 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.052\r\n2020-06-11 04:28:46.357520: step 4700, loss = 0.82 (4300.8 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:46.495333: step 4710, loss = 1.09 (9287.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:46.635435: step 4720, loss = 1.01 (9136.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:46.775404: step 4730, loss = 1.08 (9145.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:46.916338: step 4740, loss = 1.07 (9082.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:47.058588: step 4750, loss = 1.10 (8998.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:47.200477: step 4760, loss = 1.04 (9021.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:47.345021: step 4770, loss = 0.97 (8855.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:47.487414: step 4780, loss = 0.93 (8989.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:47.627615: step 4790, loss = 0.98 (9129.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.8009\r\nI0611 04:28:47.899583 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.8009\r\n2020-06-11 04:28:47.900822: step 4800, loss = 0.93 (4685.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:48.043012: step 4810, loss = 1.07 (9002.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:48.188872: step 4820, loss = 1.31 (8775.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:48.334453: step 4830, loss = 1.19 (8792.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:48.480093: step 4840, loss = 0.91 (8788.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:48.622449: step 4850, loss = 0.85 (8991.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:48.766637: step 4860, loss = 1.10 (8877.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:48.915383: step 4870, loss = 1.02 (8605.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:49.060064: step 4880, loss = 1.26 (8847.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:49.204838: step 4890, loss = 1.00 (8841.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.6704\r\nI0611 04:28:49.495191 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 62.6704\r\n2020-06-11 04:28:49.496288: step 4900, loss = 1.08 (4391.7 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:28:49.634705: step 4910, loss = 0.94 (9247.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:49.772729: step 4920, loss = 1.22 (9273.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:49.912489: step 4930, loss = 0.98 (9159.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:50.051589: step 4940, loss = 0.96 (9201.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:50.194589: step 4950, loss = 1.25 (8951.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:50.334693: step 4960, loss = 1.09 (9136.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:50.477783: step 4970, loss = 0.93 (8945.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:50.620334: step 4980, loss = 0.95 (8979.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:50.762221: step 4990, loss = 1.23 (9021.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.8958\r\nI0611 04:28:51.036150 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.8958\r\n2020-06-11 04:28:51.037312: step 5000, loss = 1.16 (4652.9 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:28:51.174001: step 5010, loss = 0.96 (9364.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:51.320247: step 5020, loss = 1.09 (8752.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:51.461243: step 5030, loss = 0.90 (9078.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:51.602449: step 5040, loss = 1.11 (9064.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:51.742862: step 5050, loss = 1.17 (9115.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:51.884834: step 5060, loss = 1.12 (9015.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:52.027949: step 5070, loss = 0.85 (8944.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:52.172298: step 5080, loss = 0.95 (8867.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:52.317726: step 5090, loss = 0.90 (8801.6 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.3761\r\nI0611 04:28:52.589532 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.3761\r\n2020-06-11 04:28:52.590742: step 5100, loss = 1.09 (4688.3 examples/sec; 0.027 sec/batch)\r\n2020-06-11 04:28:52.729794: step 5110, loss = 1.00 (9205.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:52.873323: step 5120, loss = 0.95 (8918.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.014204: step 5130, loss = 1.11 (9085.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.156937: step 5140, loss = 1.11 (8967.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.297837: step 5150, loss = 1.11 (9084.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.442100: step 5160, loss = 1.02 (8872.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.583716: step 5170, loss = 0.97 (9038.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.726076: step 5180, loss = 0.90 (8991.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:53.866492: step 5190, loss = 1.04 (9115.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.541\r\nI0611 04:28:54.163300 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.541\r\n2020-06-11 04:28:54.164526: step 5200, loss = 1.09 (4294.8 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:54.304599: step 5210, loss = 1.21 (9138.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:54.443212: step 5220, loss = 0.99 (9234.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:54.583651: step 5230, loss = 1.07 (9114.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:54.725674: step 5240, loss = 0.94 (9012.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:54.867300: step 5250, loss = 0.97 (9037.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:55.009128: step 5260, loss = 1.01 (9025.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:55.152028: step 5270, loss = 1.04 (8957.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:55.296263: step 5280, loss = 1.09 (8874.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:55.438956: step 5290, loss = 1.21 (8970.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.1454\r\nI0611 04:28:55.746946 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.1454\r\n2020-06-11 04:28:55.748136: step 5300, loss = 0.99 (4140.0 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:28:55.886758: step 5310, loss = 1.03 (9233.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:56.031847: step 5320, loss = 1.08 (8822.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:56.172719: step 5330, loss = 1.06 (9086.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:56.314219: step 5340, loss = 0.96 (9045.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:56.458531: step 5350, loss = 1.00 (8869.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:56.603175: step 5360, loss = 0.93 (8849.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:56.744966: step 5370, loss = 1.27 (9027.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:56.890709: step 5380, loss = 1.00 (8782.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:28:57.030102: step 5390, loss = 0.93 (9182.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.3991\r\nI0611 04:28:57.324237 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.3991\r\n2020-06-11 04:28:57.325327: step 5400, loss = 0.90 (4335.6 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:28:57.466788: step 5410, loss = 0.96 (9048.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:57.607039: step 5420, loss = 0.90 (9126.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:57.745767: step 5430, loss = 1.06 (9226.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:57.890086: step 5440, loss = 0.95 (8869.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:58.031647: step 5450, loss = 1.00 (9042.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:58.171840: step 5460, loss = 1.06 (9130.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:58.311110: step 5470, loss = 0.89 (9190.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:58.450791: step 5480, loss = 0.90 (9163.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:58.592509: step 5490, loss = 1.01 (9032.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.2479\r\nI0611 04:28:58.905340 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.2479\r\n2020-06-11 04:28:58.906441: step 5500, loss = 0.99 (4077.3 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:28:59.047117: step 5510, loss = 0.81 (9098.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:59.185782: step 5520, loss = 0.93 (9230.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:59.328060: step 5530, loss = 1.06 (8996.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:59.470339: step 5540, loss = 1.00 (8996.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:59.610466: step 5550, loss = 1.03 (9134.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:59.748504: step 5560, loss = 0.99 (9272.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:28:59.889700: step 5570, loss = 0.91 (9065.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:00.030456: step 5580, loss = 0.93 (9093.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:00.173677: step 5590, loss = 1.04 (8937.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.3204\r\nI0611 04:29:00.460036 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.3204\r\n2020-06-11 04:29:00.461047: step 5600, loss = 1.05 (4454.1 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:29:00.601134: step 5610, loss = 0.98 (9137.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:00.749165: step 5620, loss = 1.09 (8646.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:00.895856: step 5630, loss = 1.08 (8725.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:01.044711: step 5640, loss = 0.98 (8599.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:01.189395: step 5650, loss = 0.90 (8846.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:01.335436: step 5660, loss = 1.02 (8764.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:01.479553: step 5670, loss = 0.93 (8881.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:01.626285: step 5680, loss = 0.96 (8723.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:01.770861: step 5690, loss = 0.89 (8853.5 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.4644\r\nI0611 04:29:02.060945 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 62.4644\r\n2020-06-11 04:29:02.061963: step 5700, loss = 0.85 (4397.0 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:29:02.199642: step 5710, loss = 0.98 (9297.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:02.341400: step 5720, loss = 1.05 (9029.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:02.482096: step 5730, loss = 1.17 (9097.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:02.622803: step 5740, loss = 1.03 (9096.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:02.763469: step 5750, loss = 1.05 (9099.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:02.906546: step 5760, loss = 0.93 (8946.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:03.045852: step 5770, loss = 1.26 (9188.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:03.183166: step 5780, loss = 0.95 (9321.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:03.323395: step 5790, loss = 0.96 (9128.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.5353\r\nI0611 04:29:03.610489 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 64.5353\r\n2020-06-11 04:29:03.611525: step 5800, loss = 1.12 (4442.4 examples/sec; 0.029 sec/batch)\r\n2020-06-11 04:29:03.751369: step 5810, loss = 0.90 (9153.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:03.895062: step 5820, loss = 1.12 (8908.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.035537: step 5830, loss = 0.81 (9112.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.176901: step 5840, loss = 1.08 (9054.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.317411: step 5850, loss = 1.01 (9109.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.460180: step 5860, loss = 0.97 (8965.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.604336: step 5870, loss = 0.84 (8879.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.745767: step 5880, loss = 0.98 (9050.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:04.887268: step 5890, loss = 1.07 (9046.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.325\r\nI0611 04:29:05.189643 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.325\r\n2020-06-11 04:29:05.190605: step 5900, loss = 1.05 (4219.7 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:29:05.331743: step 5910, loss = 1.03 (9069.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:05.478437: step 5920, loss = 1.01 (8725.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:05.622929: step 5930, loss = 0.91 (8858.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:05.764906: step 5940, loss = 1.03 (9015.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:05.912548: step 5950, loss = 0.91 (8669.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:06.055037: step 5960, loss = 1.02 (8983.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:06.199113: step 5970, loss = 0.97 (8884.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:06.340978: step 5980, loss = 1.06 (9022.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:06.492262: step 5990, loss = 0.99 (8460.9 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.2095\r\nI0611 04:29:06.797138 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 62.2095\r\n2020-06-11 04:29:06.798399: step 6000, loss = 1.03 (4181.1 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:06.941487: step 6010, loss = 0.88 (8945.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:07.083924: step 6020, loss = 1.03 (8986.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:07.228954: step 6030, loss = 0.93 (8825.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:07.371323: step 6040, loss = 1.04 (8990.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:07.513568: step 6050, loss = 0.86 (8998.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:07.656036: step 6060, loss = 1.07 (8984.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:07.799418: step 6070, loss = 0.83 (8927.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:07.945365: step 6080, loss = 0.98 (8770.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:08.091845: step 6090, loss = 1.01 (8738.4 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 63.6123\r\nI0611 04:29:08.369139 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 63.6123\r\n2020-06-11 04:29:08.370085: step 6100, loss = 1.13 (4600.3 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:29:08.513417: step 6110, loss = 1.02 (8930.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:08.655916: step 6120, loss = 0.95 (8982.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:08.800815: step 6130, loss = 0.88 (8833.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:08.944610: step 6140, loss = 0.89 (8901.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:09.097017: step 6150, loss = 1.08 (8398.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:09.262697: step 6160, loss = 1.06 (7725.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:09.419950: step 6170, loss = 1.04 (8139.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:09.587154: step 6180, loss = 1.02 (7655.3 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:09.743447: step 6190, loss = 0.73 (8189.7 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 57.8665\r\nI0611 04:29:10.097275 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 57.8665\r\n2020-06-11 04:29:10.098525: step 6200, loss = 1.08 (3604.8 examples/sec; 0.036 sec/batch)\r\n2020-06-11 04:29:10.254590: step 6210, loss = 0.94 (8201.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:10.409293: step 6220, loss = 1.08 (8274.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:10.570895: step 6230, loss = 0.94 (7920.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:10.739580: step 6240, loss = 0.89 (7588.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:10.900843: step 6250, loss = 1.02 (7937.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:11.062997: step 6260, loss = 0.91 (7894.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:11.231594: step 6270, loss = 1.21 (7591.4 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:11.400354: step 6280, loss = 0.94 (7584.7 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:11.561650: step 6290, loss = 0.97 (7935.7 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 54.4714\r\nI0611 04:29:11.933100 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 54.4714\r\n2020-06-11 04:29:11.934324: step 6300, loss = 0.83 (3434.6 examples/sec; 0.037 sec/batch)\r\n2020-06-11 04:29:12.086124: step 6310, loss = 0.79 (8432.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:12.243710: step 6320, loss = 1.14 (8122.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:12.407192: step 6330, loss = 0.92 (7829.7 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:12.571197: step 6340, loss = 0.84 (7804.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:12.732667: step 6350, loss = 0.99 (7927.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:12.896018: step 6360, loss = 0.89 (7835.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:13.056501: step 6370, loss = 0.91 (7976.0 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:13.221374: step 6380, loss = 0.94 (7763.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:13.390018: step 6390, loss = 0.86 (7589.9 examples/sec; 0.017 sec/batch)\r\nINFO:tensorflow:global_step/sec: 54.8449\r\nI0611 04:29:13.756429 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 54.8449\r\n2020-06-11 04:29:13.757727: step 6400, loss = 1.04 (3480.9 examples/sec; 0.037 sec/batch)\r\n2020-06-11 04:29:13.915965: step 6410, loss = 0.70 (8089.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:14.074195: step 6420, loss = 0.80 (8089.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:14.235134: step 6430, loss = 1.11 (7953.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:14.404232: step 6440, loss = 0.98 (7569.5 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:14.564361: step 6450, loss = 0.88 (7993.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:14.726878: step 6460, loss = 0.97 (7876.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:14.892979: step 6470, loss = 1.04 (7706.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:15.057371: step 6480, loss = 1.13 (7786.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:15.224717: step 6490, loss = 0.89 (7648.8 examples/sec; 0.017 sec/batch)\r\nINFO:tensorflow:global_step/sec: 54.3869\r\nI0611 04:29:15.595106 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 54.3869\r\n2020-06-11 04:29:15.596354: step 6500, loss = 1.19 (3444.2 examples/sec; 0.037 sec/batch)\r\n2020-06-11 04:29:15.751030: step 6510, loss = 1.09 (8275.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:15.911509: step 6520, loss = 0.88 (7976.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:16.070132: step 6530, loss = 1.04 (8069.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:16.238482: step 6540, loss = 1.00 (7603.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:16.406252: step 6550, loss = 0.92 (7629.5 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:16.577237: step 6560, loss = 0.87 (7486.0 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:16.757863: step 6570, loss = 0.94 (7086.5 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:16.926769: step 6580, loss = 1.01 (7578.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:17.095260: step 6590, loss = 0.78 (7596.9 examples/sec; 0.017 sec/batch)\r\nINFO:tensorflow:global_step/sec: 54.5958\r\nI0611 04:29:17.426740 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 54.5958\r\n2020-06-11 04:29:17.427920: step 6600, loss = 0.79 (3847.6 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:29:17.571608: step 6610, loss = 0.83 (8908.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:17.717559: step 6620, loss = 0.94 (8770.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:17.872197: step 6630, loss = 0.94 (8277.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:18.021707: step 6640, loss = 1.02 (8561.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:18.175363: step 6650, loss = 1.02 (8330.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:18.334470: step 6660, loss = 0.87 (8044.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:18.496347: step 6670, loss = 0.88 (7907.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:18.656030: step 6680, loss = 0.98 (8015.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:18.805373: step 6690, loss = 0.93 (8571.0 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.9125\r\nI0611 04:29:19.124180 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.9125\r\n2020-06-11 04:29:19.125346: step 6700, loss = 0.86 (4000.3 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:29:19.274261: step 6710, loss = 1.00 (8595.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:19.426892: step 6720, loss = 0.91 (8386.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:19.578814: step 6730, loss = 0.89 (8425.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:19.733836: step 6740, loss = 1.01 (8256.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:19.883770: step 6750, loss = 0.92 (8537.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:20.038616: step 6760, loss = 1.20 (8266.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:20.192701: step 6770, loss = 1.14 (8307.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:20.345028: step 6780, loss = 0.81 (8403.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:20.497100: step 6790, loss = 1.04 (8417.0 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.9502\r\nI0611 04:29:20.820531 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.9502\r\n2020-06-11 04:29:20.821713: step 6800, loss = 0.84 (3943.1 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:29:20.967351: step 6810, loss = 1.02 (8788.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:21.113832: step 6820, loss = 0.75 (8738.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:21.255079: step 6830, loss = 0.86 (9062.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:21.399422: step 6840, loss = 0.84 (8867.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:21.549708: step 6850, loss = 0.97 (8517.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:21.697999: step 6860, loss = 1.02 (8631.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:21.847001: step 6870, loss = 0.93 (8590.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:21.992235: step 6880, loss = 1.16 (8813.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:22.137010: step 6890, loss = 0.96 (8841.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 61.5221\r\nI0611 04:29:22.445959 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 61.5221\r\n2020-06-11 04:29:22.447150: step 6900, loss = 0.89 (4127.1 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:22.589990: step 6910, loss = 1.07 (8961.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:22.742728: step 6920, loss = 1.26 (8380.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:22.892733: step 6930, loss = 0.96 (8533.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:23.039005: step 6940, loss = 0.92 (8750.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:23.192599: step 6950, loss = 0.83 (8333.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:23.340472: step 6960, loss = 1.03 (8656.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:23.498090: step 6970, loss = 0.95 (8120.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:23.644964: step 6980, loss = 0.84 (8714.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:23.794249: step 6990, loss = 0.89 (8574.2 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.9362\r\nI0611 04:29:24.114395 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.9362\r\n2020-06-11 04:29:24.115498: step 7000, loss = 0.99 (3984.4 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:29:24.259296: step 7010, loss = 1.02 (8901.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:24.404437: step 7020, loss = 1.04 (8819.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:24.558095: step 7030, loss = 1.15 (8330.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:24.710800: step 7040, loss = 0.84 (8382.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:24.861876: step 7050, loss = 0.88 (8472.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:25.015871: step 7060, loss = 1.06 (8312.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:25.172813: step 7070, loss = 0.98 (8155.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:25.328166: step 7080, loss = 0.98 (8239.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:25.478403: step 7090, loss = 0.98 (8519.9 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.8407\r\nI0611 04:29:25.785489 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.8407\r\n2020-06-11 04:29:25.786513: step 7100, loss = 0.91 (4154.3 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:25.935546: step 7110, loss = 0.96 (8589.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:26.085191: step 7120, loss = 0.89 (8553.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:26.241204: step 7130, loss = 0.84 (8204.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:26.393701: step 7140, loss = 0.86 (8393.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:26.548588: step 7150, loss = 0.86 (8264.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:26.703655: step 7160, loss = 1.10 (8255.0 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:26.861519: step 7170, loss = 0.98 (8107.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:27.014717: step 7180, loss = 0.82 (8355.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:27.169792: step 7190, loss = 0.89 (8254.1 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.4931\r\nI0611 04:29:27.495098 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.4931\r\n2020-06-11 04:29:27.496043: step 7200, loss = 0.94 (3923.3 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:29:27.644120: step 7210, loss = 1.08 (8644.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:27.799519: step 7220, loss = 1.12 (8236.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:27.952824: step 7230, loss = 1.00 (8349.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:28.108180: step 7240, loss = 0.99 (8239.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:28.261801: step 7250, loss = 0.83 (8332.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:28.418298: step 7260, loss = 0.96 (8179.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:28.575532: step 7270, loss = 0.77 (8140.7 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:28.729849: step 7280, loss = 0.85 (8294.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:28.886380: step 7290, loss = 1.03 (8177.4 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.1435\r\nI0611 04:29:29.185891 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.1435\r\n2020-06-11 04:29:29.186831: step 7300, loss = 0.79 (4260.1 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:29:29.329142: step 7310, loss = 1.08 (8994.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:29.480918: step 7320, loss = 0.73 (8433.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:29.627061: step 7330, loss = 0.87 (8758.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:29.779569: step 7340, loss = 0.99 (8393.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:29.927548: step 7350, loss = 1.01 (8649.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:30.077244: step 7360, loss = 0.96 (8550.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:30.227349: step 7370, loss = 0.96 (8527.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:30.378283: step 7380, loss = 0.80 (8480.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:30.529890: step 7390, loss = 0.87 (8443.4 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 60.8515\r\nI0611 04:29:30.829236 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 60.8515\r\n2020-06-11 04:29:30.830357: step 7400, loss = 0.92 (4259.8 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:29:30.984084: step 7410, loss = 0.93 (8326.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:31.133145: step 7420, loss = 1.05 (8587.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:31.280562: step 7430, loss = 0.83 (8682.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:31.430575: step 7440, loss = 0.95 (8532.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:31.581409: step 7450, loss = 1.00 (8486.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:31.730607: step 7460, loss = 0.96 (8579.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:31.885036: step 7470, loss = 0.94 (8288.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:32.034275: step 7480, loss = 0.92 (8576.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:32.183821: step 7490, loss = 0.84 (8559.8 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.9445\r\nI0611 04:29:32.497474 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.9445\r\n2020-06-11 04:29:32.498654: step 7500, loss = 0.94 (4065.5 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:32.643946: step 7510, loss = 0.92 (8809.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:32.792667: step 7520, loss = 0.93 (8606.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:32.942144: step 7530, loss = 0.93 (8563.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:33.095310: step 7540, loss = 0.83 (8356.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:33.248340: step 7550, loss = 0.94 (8364.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:33.407768: step 7560, loss = 1.05 (8028.7 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:33.561488: step 7570, loss = 0.94 (8326.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:33.714144: step 7580, loss = 0.89 (8384.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:33.872105: step 7590, loss = 0.91 (8103.2 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.9054\r\nI0611 04:29:34.195103 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.9054\r\n2020-06-11 04:29:34.196369: step 7600, loss = 1.17 (3947.4 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:29:34.344567: step 7610, loss = 0.94 (8637.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:34.497121: step 7620, loss = 0.97 (8390.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:34.650492: step 7630, loss = 1.12 (8345.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:34.812265: step 7640, loss = 1.08 (7912.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:34.967097: step 7650, loss = 0.96 (8267.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:35.123039: step 7660, loss = 0.74 (8208.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:35.276722: step 7670, loss = 0.80 (8328.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:35.432404: step 7680, loss = 1.15 (8221.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:35.589305: step 7690, loss = 0.88 (8158.0 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.9759\r\nI0611 04:29:35.890701 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.9759\r\n2020-06-11 04:29:35.891735: step 7700, loss = 0.94 (4232.3 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:29:36.044568: step 7710, loss = 0.92 (8375.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:36.196477: step 7720, loss = 0.94 (8426.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:36.349226: step 7730, loss = 0.89 (8379.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:36.502424: step 7740, loss = 0.96 (8355.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:36.657662: step 7750, loss = 0.85 (8245.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:36.808788: step 7760, loss = 0.91 (8469.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:36.962536: step 7770, loss = 0.84 (8325.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:37.117674: step 7780, loss = 0.97 (8250.7 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:37.271770: step 7790, loss = 0.96 (8306.6 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.0757\r\nI0611 04:29:37.583456 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.0757\r\n2020-06-11 04:29:37.584627: step 7800, loss = 0.74 (4091.3 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:37.729032: step 7810, loss = 1.05 (8864.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:37.873372: step 7820, loss = 0.79 (8868.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:38.020184: step 7830, loss = 0.79 (8718.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:38.173024: step 7840, loss = 0.82 (8374.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:38.327618: step 7850, loss = 0.95 (8280.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:38.476574: step 7860, loss = 0.87 (8592.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:38.626781: step 7870, loss = 0.88 (8521.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:38.779876: step 7880, loss = 0.84 (8360.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:38.930989: step 7890, loss = 1.05 (8471.2 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 60.4555\r\nI0611 04:29:39.237566 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 60.4555\r\n2020-06-11 04:29:39.238668: step 7900, loss = 0.96 (4160.0 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:39.387221: step 7910, loss = 0.97 (8616.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:39.535124: step 7920, loss = 0.97 (8654.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:39.684235: step 7930, loss = 1.02 (8584.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:39.831493: step 7940, loss = 0.96 (8692.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:39.982842: step 7950, loss = 1.24 (8457.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:40.133010: step 7960, loss = 0.96 (8523.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:40.279886: step 7970, loss = 0.93 (8714.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:40.427303: step 7980, loss = 0.78 (8682.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:40.574319: step 7990, loss = 0.86 (8706.5 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 60.0693\r\nI0611 04:29:40.902301 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 60.0693\r\n2020-06-11 04:29:40.903330: step 8000, loss = 0.87 (3890.4 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:29:41.051418: step 8010, loss = 0.89 (8643.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:41.208525: step 8020, loss = 0.81 (8147.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:41.360692: step 8030, loss = 0.88 (8411.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:41.517031: step 8040, loss = 0.95 (8187.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:41.674541: step 8050, loss = 0.89 (8126.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:41.827193: step 8060, loss = 0.93 (8385.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:41.986030: step 8070, loss = 0.80 (8058.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:42.143255: step 8080, loss = 0.96 (8141.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:42.299191: step 8090, loss = 0.87 (8208.6 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.6331\r\nI0611 04:29:42.607816 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.6331\r\n2020-06-11 04:29:42.608788: step 8100, loss = 0.84 (4134.3 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:42.762614: step 8110, loss = 0.98 (8321.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:42.929344: step 8120, loss = 0.80 (7677.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:43.117978: step 8130, loss = 0.91 (6785.7 examples/sec; 0.019 sec/batch)\r\n2020-06-11 04:29:43.304667: step 8140, loss = 0.85 (6856.3 examples/sec; 0.019 sec/batch)\r\n2020-06-11 04:29:43.482870: step 8150, loss = 0.94 (7182.8 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:43.666987: step 8160, loss = 0.87 (6952.7 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:43.842357: step 8170, loss = 1.19 (7298.2 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:44.021303: step 8180, loss = 0.96 (7153.0 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:44.208123: step 8190, loss = 0.78 (6851.5 examples/sec; 0.019 sec/batch)\r\nINFO:tensorflow:global_step/sec: 51.265\r\nI0611 04:29:44.558461 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 51.265\r\n2020-06-11 04:29:44.559414: step 8200, loss = 0.86 (3643.6 examples/sec; 0.035 sec/batch)\r\n2020-06-11 04:29:44.706633: step 8210, loss = 0.88 (8694.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:44.883124: step 8220, loss = 1.05 (7252.8 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:45.074728: step 8230, loss = 0.87 (6680.4 examples/sec; 0.019 sec/batch)\r\n2020-06-11 04:29:45.260627: step 8240, loss = 0.98 (6885.5 examples/sec; 0.019 sec/batch)\r\n2020-06-11 04:29:45.446782: step 8250, loss = 0.86 (6876.0 examples/sec; 0.019 sec/batch)\r\n2020-06-11 04:29:45.625472: step 8260, loss = 0.97 (7163.2 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:45.807131: step 8270, loss = 0.81 (7046.2 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:45.990231: step 8280, loss = 0.77 (6990.7 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:46.169929: step 8290, loss = 1.01 (7123.1 examples/sec; 0.018 sec/batch)\r\nINFO:tensorflow:global_step/sec: 50.9059\r\nI0611 04:29:46.522884 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 50.9059\r\n2020-06-11 04:29:46.524126: step 8300, loss = 0.85 (3613.7 examples/sec; 0.035 sec/batch)\r\n2020-06-11 04:29:46.673949: step 8310, loss = 1.13 (8543.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:46.845606: step 8320, loss = 0.98 (7456.9 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:47.027745: step 8330, loss = 1.09 (7027.6 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:47.227658: step 8340, loss = 0.71 (6402.9 examples/sec; 0.020 sec/batch)\r\n2020-06-11 04:29:47.415586: step 8350, loss = 0.81 (6811.1 examples/sec; 0.019 sec/batch)\r\n2020-06-11 04:29:47.594265: step 8360, loss = 0.79 (7163.7 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:47.768811: step 8370, loss = 0.81 (7333.3 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:47.951501: step 8380, loss = 1.01 (7006.4 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:48.133980: step 8390, loss = 1.13 (7014.5 examples/sec; 0.018 sec/batch)\r\nINFO:tensorflow:global_step/sec: 51.3239\r\nI0611 04:29:48.471276 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 51.3239\r\n2020-06-11 04:29:48.472370: step 8400, loss = 0.78 (3782.5 examples/sec; 0.034 sec/batch)\r\n2020-06-11 04:29:48.622214: step 8410, loss = 0.83 (8542.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:48.796529: step 8420, loss = 0.93 (7343.4 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:29:48.975962: step 8430, loss = 1.03 (7133.6 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:49.154337: step 8440, loss = 0.83 (7175.9 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:49.337995: step 8450, loss = 1.00 (6969.5 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:49.519369: step 8460, loss = 1.02 (7057.2 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:49.704029: step 8470, loss = 1.00 (6931.7 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:49.884902: step 8480, loss = 1.07 (7076.8 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:29:50.076298: step 8490, loss = 0.95 (6687.7 examples/sec; 0.019 sec/batch)\r\nINFO:tensorflow:global_step/sec: 50.8945\r\nI0611 04:29:50.436153 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 50.8945\r\n2020-06-11 04:29:50.437342: step 8500, loss = 0.90 (3545.2 examples/sec; 0.036 sec/batch)\r\n2020-06-11 04:29:50.584474: step 8510, loss = 0.84 (8699.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:50.737218: step 8520, loss = 0.74 (8380.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:50.887564: step 8530, loss = 0.69 (8513.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:51.038279: step 8540, loss = 1.10 (8492.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:51.191035: step 8550, loss = 0.90 (8379.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:51.346542: step 8560, loss = 1.29 (8231.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:51.501173: step 8570, loss = 0.85 (8277.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:51.659958: step 8580, loss = 0.84 (8061.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:51.819801: step 8590, loss = 0.96 (8007.8 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 59.1889\r\nI0611 04:29:52.125639 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 59.1889\r\n2020-06-11 04:29:52.126777: step 8600, loss = 0.81 (4169.6 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:29:52.273207: step 8610, loss = 0.81 (8741.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:52.428241: step 8620, loss = 1.03 (8256.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:52.581582: step 8630, loss = 0.88 (8347.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:52.733058: step 8640, loss = 0.98 (8450.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:52.892723: step 8650, loss = 0.97 (8016.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:53.047942: step 8660, loss = 0.94 (8246.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:53.202639: step 8670, loss = 0.83 (8274.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:53.357303: step 8680, loss = 0.87 (8276.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:53.513492: step 8690, loss = 0.85 (8195.3 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.6456\r\nI0611 04:29:53.830806 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.6456\r\n2020-06-11 04:29:53.832081: step 8700, loss = 0.91 (4017.7 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:29:53.976477: step 8710, loss = 0.94 (8864.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:54.119798: step 8720, loss = 0.98 (8931.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:29:54.269958: step 8730, loss = 0.97 (8524.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:54.421659: step 8740, loss = 0.89 (8437.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:54.568131: step 8750, loss = 0.89 (8738.9 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:54.720795: step 8760, loss = 0.98 (8384.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:54.872290: step 8770, loss = 0.95 (8449.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:55.024741: step 8780, loss = 0.98 (8396.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:55.178559: step 8790, loss = 0.85 (8321.5 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 61.3674\r\nI0611 04:29:55.460316 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 61.3674\r\n2020-06-11 04:29:55.461201: step 8800, loss = 0.90 (4528.6 examples/sec; 0.028 sec/batch)\r\n2020-06-11 04:29:55.606233: step 8810, loss = 0.85 (8825.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:55.756623: step 8820, loss = 1.02 (8511.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:55.907543: step 8830, loss = 0.85 (8481.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:56.059001: step 8840, loss = 1.14 (8451.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:56.208574: step 8850, loss = 0.88 (8558.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:56.359626: step 8860, loss = 0.91 (8473.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:56.512510: step 8870, loss = 0.91 (8372.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:56.660264: step 8880, loss = 0.93 (8663.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:56.823156: step 8890, loss = 0.85 (7858.0 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.8037\r\nI0611 04:29:57.160899 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.8037\r\n2020-06-11 04:29:57.162051: step 8900, loss = 0.92 (3776.9 examples/sec; 0.034 sec/batch)\r\n2020-06-11 04:29:57.313210: step 8910, loss = 0.86 (8468.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:57.469329: step 8920, loss = 0.77 (8198.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:57.625157: step 8930, loss = 0.78 (8214.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:57.786194: step 8940, loss = 1.04 (7948.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:57.939349: step 8950, loss = 0.98 (8357.5 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:58.090679: step 8960, loss = 0.96 (8458.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:58.242838: step 8970, loss = 1.04 (8412.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:58.402480: step 8980, loss = 0.98 (8018.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:58.557804: step 8990, loss = 0.93 (8240.2 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.2092\r\nI0611 04:29:58.878855 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.2092\r\n2020-06-11 04:29:58.880202: step 9000, loss = 0.99 (3970.2 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:29:59.031085: step 9010, loss = 0.82 (8484.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:59.184771: step 9020, loss = 0.70 (8328.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:59.340462: step 9030, loss = 0.82 (8221.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:59.501134: step 9040, loss = 0.80 (7966.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:59.658903: step 9050, loss = 0.73 (8113.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:29:59.813547: step 9060, loss = 0.93 (8277.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:29:59.969320: step 9070, loss = 0.88 (8217.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:00.124511: step 9080, loss = 0.81 (8247.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:00.278491: step 9090, loss = 1.11 (8312.8 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 58.1532\r\nI0611 04:30:00.598448 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 58.1532\r\n2020-06-11 04:30:00.599590: step 9100, loss = 0.80 (3986.3 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:30:00.819149: step 9110, loss = 0.94 (5830.6 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:01.413841: step 9120, loss = 0.93 (2152.4 examples/sec; 0.059 sec/batch)\r\n2020-06-11 04:30:01.988051: step 9130, loss = 0.86 (2229.2 examples/sec; 0.057 sec/batch)\r\n2020-06-11 04:30:02.300564: step 9140, loss = 0.78 (4095.6 examples/sec; 0.031 sec/batch)\r\n2020-06-11 04:30:02.474997: step 9150, loss = 0.89 (7338.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:02.649158: step 9160, loss = 0.73 (7348.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:02.814078: step 9170, loss = 0.94 (7761.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:02.980223: step 9180, loss = 0.76 (7704.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:03.147056: step 9190, loss = 0.82 (7672.3 examples/sec; 0.017 sec/batch)\r\nINFO:tensorflow:global_step/sec: 34.9115\r\nI0611 04:30:03.462838 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 34.9115\r\n2020-06-11 04:30:03.464070: step 9200, loss = 0.89 (4037.5 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:30:03.608782: step 9210, loss = 0.84 (8845.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 04:30:03.756259: step 9220, loss = 0.89 (8679.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:03.913298: step 9230, loss = 0.88 (8150.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:04.063919: step 9240, loss = 0.92 (8498.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:04.214898: step 9250, loss = 0.93 (8478.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:04.361112: step 9260, loss = 1.14 (8754.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:04.509436: step 9270, loss = 0.84 (8629.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:04.660148: step 9280, loss = 0.71 (8493.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:04.802272: step 9290, loss = 0.86 (9006.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 60.1196\r\nI0611 04:30:05.126176 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 60.1196\r\n2020-06-11 04:30:05.127268: step 9300, loss = 0.74 (3938.5 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:30:05.276658: step 9310, loss = 0.82 (8568.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:05.430175: step 9320, loss = 0.90 (8337.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:05.584628: step 9330, loss = 0.92 (8287.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:05.739680: step 9340, loss = 1.01 (8255.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:05.893051: step 9350, loss = 1.02 (8345.8 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:06.053761: step 9360, loss = 0.91 (7964.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:06.215919: step 9370, loss = 0.79 (7893.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:06.381978: step 9380, loss = 0.89 (7708.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:06.550065: step 9390, loss = 1.00 (7615.1 examples/sec; 0.017 sec/batch)\r\nINFO:tensorflow:global_step/sec: 57.1809\r\nI0611 04:30:06.875036 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 57.1809\r\n2020-06-11 04:30:06.876375: step 9400, loss = 0.85 (3922.6 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:30:07.030156: step 9410, loss = 0.95 (8323.6 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:07.196005: step 9420, loss = 0.71 (7718.0 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:07.364497: step 9430, loss = 1.01 (7596.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:07.528426: step 9440, loss = 0.82 (7808.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:07.696939: step 9450, loss = 0.89 (7595.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:07.858962: step 9460, loss = 0.93 (7900.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:08.021018: step 9470, loss = 0.91 (7898.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:08.185490: step 9480, loss = 0.72 (7782.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:08.347635: step 9490, loss = 0.93 (7894.2 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 55.5692\r\nI0611 04:30:08.674582 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 55.5692\r\n2020-06-11 04:30:08.675719: step 9500, loss = 0.98 (3901.4 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:30:08.825967: step 9510, loss = 0.95 (8519.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:08.992848: step 9520, loss = 1.01 (7670.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:09.153532: step 9530, loss = 0.75 (7965.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:09.324072: step 9540, loss = 1.10 (7505.5 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:09.485284: step 9550, loss = 0.90 (7939.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:09.647651: step 9560, loss = 1.02 (7883.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:09.815781: step 9570, loss = 1.00 (7613.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:09.983819: step 9580, loss = 0.82 (7617.3 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:10.144509: step 9590, loss = 0.78 (7965.6 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 55.8843\r\nI0611 04:30:10.463994 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 55.8843\r\n2020-06-11 04:30:10.465137: step 9600, loss = 0.93 (3992.2 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:30:10.613698: step 9610, loss = 1.03 (8616.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:10.770423: step 9620, loss = 0.81 (8167.3 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:10.930230: step 9630, loss = 0.77 (8009.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:11.086541: step 9640, loss = 1.00 (8188.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:11.252848: step 9650, loss = 0.85 (7696.6 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:11.416409: step 9660, loss = 1.02 (7826.0 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:11.584401: step 9670, loss = 0.99 (7619.4 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:11.746331: step 9680, loss = 0.93 (7904.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:11.913281: step 9690, loss = 0.91 (7667.0 examples/sec; 0.017 sec/batch)\r\nINFO:tensorflow:global_step/sec: 57.1373\r\nI0611 04:30:12.214133 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 57.1373\r\n2020-06-11 04:30:12.214989: step 9700, loss = 0.77 (4242.8 examples/sec; 0.030 sec/batch)\r\n2020-06-11 04:30:12.365806: step 9710, loss = 0.96 (8486.0 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:12.525240: step 9720, loss = 0.89 (8028.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:12.690934: step 9730, loss = 0.98 (7725.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:12.858711: step 9740, loss = 0.96 (7629.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:13.020299: step 9750, loss = 0.85 (7921.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:13.189081: step 9760, loss = 0.79 (7583.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:13.351406: step 9770, loss = 0.90 (7885.4 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:13.515204: step 9780, loss = 0.92 (7814.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:13.674489: step 9790, loss = 0.97 (8035.9 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 56.2199\r\nI0611 04:30:13.992879 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 56.2199\r\n2020-06-11 04:30:13.994014: step 9800, loss = 0.92 (4005.9 examples/sec; 0.032 sec/batch)\r\n2020-06-11 04:30:14.154040: step 9810, loss = 0.90 (7998.9 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:14.320519: step 9820, loss = 0.92 (7688.7 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:14.485891: step 9830, loss = 0.83 (7740.1 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:14.651043: step 9840, loss = 1.00 (7750.5 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:14.813693: step 9850, loss = 0.80 (7869.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:14.982987: step 9860, loss = 0.98 (7561.4 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:15.149363: step 9870, loss = 0.78 (7692.8 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:15.318508: step 9880, loss = 0.80 (7567.5 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:15.483466: step 9890, loss = 0.96 (7759.6 examples/sec; 0.016 sec/batch)\r\nINFO:tensorflow:global_step/sec: 54.8998\r\nI0611 04:30:15.814391 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 54.8998\r\n2020-06-11 04:30:15.815709: step 9900, loss = 0.89 (3852.6 examples/sec; 0.033 sec/batch)\r\n2020-06-11 04:30:15.969482: step 9910, loss = 0.91 (8324.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 04:30:16.135517: step 9920, loss = 0.68 (7709.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:16.299051: step 9930, loss = 0.83 (7827.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:16.480448: step 9940, loss = 0.86 (7056.4 examples/sec; 0.018 sec/batch)\r\n2020-06-11 04:30:16.718892: step 9950, loss = 0.80 (5368.3 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:16.954830: step 9960, loss = 1.09 (5425.2 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:17.190616: step 9970, loss = 0.94 (5428.6 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:17.430386: step 9980, loss = 0.72 (5338.4 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:17.669378: step 9990, loss = 0.84 (5355.8 examples/sec; 0.024 sec/batch)\r\nINFO:tensorflow:global_step/sec: 44.4694\r\nI0611 04:30:18.063112 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 44.4694\r\n2020-06-11 04:30:18.064062: step 10000, loss = 1.08 (3243.0 examples/sec; 0.039 sec/batch)\r\n2020-06-11 04:30:18.228066: step 10010, loss = 0.81 (7805.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:18.447680: step 10020, loss = 1.21 (5828.5 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:18.673094: step 10030, loss = 0.77 (5678.4 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:18.908407: step 10040, loss = 0.93 (5439.7 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:19.144155: step 10050, loss = 0.84 (5429.5 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:19.381538: step 10060, loss = 0.86 (5392.1 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:19.618174: step 10070, loss = 0.91 (5409.1 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:19.846687: step 10080, loss = 0.78 (5601.4 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:20.075522: step 10090, loss = 0.96 (5593.5 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 41.16\r\nI0611 04:30:20.492682 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 41.16\r\n2020-06-11 04:30:20.493903: step 10100, loss = 0.89 (3059.3 examples/sec; 0.042 sec/batch)\r\n2020-06-11 04:30:20.663241: step 10110, loss = 0.92 (7559.4 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:20.891713: step 10120, loss = 0.91 (5602.4 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:21.128343: step 10130, loss = 0.87 (5409.3 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:21.356519: step 10140, loss = 0.88 (5609.7 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:21.590308: step 10150, loss = 0.94 (5475.0 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:21.827315: step 10160, loss = 0.98 (5400.7 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:22.057116: step 10170, loss = 0.77 (5570.0 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:22.290925: step 10180, loss = 0.69 (5474.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:22.520570: step 10190, loss = 1.00 (5573.9 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 41.0889\r\nI0611 04:30:22.926417 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 41.0889\r\n2020-06-11 04:30:22.927638: step 10200, loss = 0.94 (3144.3 examples/sec; 0.041 sec/batch)\r\n2020-06-11 04:30:23.086583: step 10210, loss = 0.82 (8053.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:23.317602: step 10220, loss = 0.85 (5540.8 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:23.544268: step 10230, loss = 1.03 (5647.1 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:23.768369: step 10240, loss = 0.79 (5711.7 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:23.995980: step 10250, loss = 0.91 (5623.6 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:24.221059: step 10260, loss = 0.87 (5686.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:24.456436: step 10270, loss = 0.97 (5438.1 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:24.682039: step 10280, loss = 0.97 (5673.7 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:24.911709: step 10290, loss = 0.73 (5573.2 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 42.1014\r\nI0611 04:30:25.301628 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 42.1014\r\n2020-06-11 04:30:25.302617: step 10300, loss = 0.93 (3274.4 examples/sec; 0.039 sec/batch)\r\n2020-06-11 04:30:25.462072: step 10310, loss = 0.84 (8027.2 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:25.682001: step 10320, loss = 0.86 (5820.1 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:25.907160: step 10330, loss = 0.95 (5684.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:26.145427: step 10340, loss = 0.93 (5372.2 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:26.377405: step 10350, loss = 0.82 (5517.8 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:26.610731: step 10360, loss = 0.89 (5485.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:26.847051: step 10370, loss = 0.82 (5416.4 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:27.074940: step 10380, loss = 0.97 (5616.8 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:27.307925: step 10390, loss = 0.88 (5493.9 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 41.3705\r\nI0611 04:30:27.718812 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 41.3705\r\n2020-06-11 04:30:27.720041: step 10400, loss = 0.91 (3105.8 examples/sec; 0.041 sec/batch)\r\n2020-06-11 04:30:27.878766: step 10410, loss = 0.88 (8064.6 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:28.094201: step 10420, loss = 0.90 (5941.6 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:28.318623: step 10430, loss = 0.99 (5703.5 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:28.537039: step 10440, loss = 0.94 (5860.4 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:28.766578: step 10450, loss = 0.84 (5576.4 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:29.000478: step 10460, loss = 0.97 (5472.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:29.232405: step 10470, loss = 0.85 (5519.0 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:29.471732: step 10480, loss = 0.91 (5348.3 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:29.704470: step 10490, loss = 0.87 (5499.7 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 42.0659\r\nI0611 04:30:30.096016 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 42.0659\r\n2020-06-11 04:30:30.096972: step 10500, loss = 0.86 (3261.0 examples/sec; 0.039 sec/batch)\r\n2020-06-11 04:30:30.262787: step 10510, loss = 0.75 (7719.9 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:30.497629: step 10520, loss = 0.81 (5450.6 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:30.732651: step 10530, loss = 0.92 (5446.3 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:30.966644: step 10540, loss = 0.73 (5470.2 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:31.204006: step 10550, loss = 0.95 (5392.6 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:31.440368: step 10560, loss = 0.90 (5415.4 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:31.674396: step 10570, loss = 0.83 (5469.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:31.906395: step 10580, loss = 0.99 (5517.2 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:32.145344: step 10590, loss = 0.93 (5356.8 examples/sec; 0.024 sec/batch)\r\nINFO:tensorflow:global_step/sec: 40.8423\r\nI0611 04:30:32.544492 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 40.8423\r\n2020-06-11 04:30:32.545619: step 10600, loss = 0.89 (3197.7 examples/sec; 0.040 sec/batch)\r\n2020-06-11 04:30:32.704516: step 10610, loss = 0.82 (8055.8 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:32.912513: step 10620, loss = 0.83 (6154.0 examples/sec; 0.021 sec/batch)\r\n2020-06-11 04:30:33.139093: step 10630, loss = 1.06 (5649.3 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:33.366031: step 10640, loss = 1.00 (5640.3 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:33.591101: step 10650, loss = 0.86 (5687.1 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:33.820035: step 10660, loss = 0.81 (5591.2 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:34.047856: step 10670, loss = 0.77 (5618.4 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:34.272700: step 10680, loss = 0.80 (5692.8 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:34.496647: step 10690, loss = 0.87 (5715.7 examples/sec; 0.022 sec/batch)\r\nINFO:tensorflow:global_step/sec: 42.8927\r\nI0611 04:30:34.875855 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 42.8927\r\n2020-06-11 04:30:34.876825: step 10700, loss = 1.21 (3366.7 examples/sec; 0.038 sec/batch)\r\n2020-06-11 04:30:35.036240: step 10710, loss = 0.98 (8029.7 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:35.261938: step 10720, loss = 0.81 (5671.4 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:35.487286: step 10730, loss = 0.95 (5680.1 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:35.717149: step 10740, loss = 0.81 (5568.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:35.944277: step 10750, loss = 0.97 (5635.6 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:36.173631: step 10760, loss = 0.86 (5580.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:36.402343: step 10770, loss = 0.84 (5596.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:36.632744: step 10780, loss = 0.88 (5555.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:36.866655: step 10790, loss = 0.99 (5472.2 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 42.1439\r\nI0611 04:30:37.248675 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 42.1439\r\n2020-06-11 04:30:37.249791: step 10800, loss = 0.78 (3340.7 examples/sec; 0.038 sec/batch)\r\n2020-06-11 04:30:37.418304: step 10810, loss = 0.93 (7596.2 examples/sec; 0.017 sec/batch)\r\n2020-06-11 04:30:37.642139: step 10820, loss = 0.94 (5718.8 examples/sec; 0.022 sec/batch)\r\n2020-06-11 04:30:37.880458: step 10830, loss = 0.82 (5370.9 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:38.107623: step 10840, loss = 0.89 (5634.7 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:38.341544: step 10850, loss = 0.84 (5471.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:38.582125: step 10860, loss = 0.85 (5320.5 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:38.815055: step 10870, loss = 0.77 (5495.2 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:39.046204: step 10880, loss = 0.87 (5537.5 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:39.280606: step 10890, loss = 0.85 (5460.7 examples/sec; 0.023 sec/batch)\r\nINFO:tensorflow:global_step/sec: 40.8687\r\nI0611 04:30:39.695564 140592134350592 basic_session_run_hooks.py:692] global_step/sec: 40.8687\r\n2020-06-11 04:30:39.696846: step 10900, loss = 1.01 (3075.0 examples/sec; 0.042 sec/batch)\r\n2020-06-11 04:30:39.859937: step 10910, loss = 0.97 (7848.7 examples/sec; 0.016 sec/batch)\r\n2020-06-11 04:30:40.106025: step 10920, loss = 0.86 (5201.7 examples/sec; 0.025 sec/batch)\r\n2020-06-11 04:30:40.348884: step 10930, loss = 0.87 (5270.5 examples/sec; 0.024 sec/batch)\r\n2020-06-11 04:30:40.580772: step 10940, loss = 0.75 (5519.9 examples/sec; 0.023 sec/batch)\r\n2020-06-11 04:30:40.795236: step 10950, loss = 0.98 (5968.0 examples/sec; 0.021 sec/batch)\r\n2020-06-11 04:30:40.994191: step 10960, loss = 1.07 (6433.6 examples/sec; 0.020 sec/batch)\r\n2020-06-11 04:30:41.194209: step 10970, loss = 0.88 (6399.4 examples/sec; 0.020 sec/batch)\r\n2020-06-11 04:30:41.398873: step 10980, loss = 1.04 (6254.2 examples/sec; 0.020 sec/batch)\r\n2020-06-11 04:30:41.612530: step 10990, loss = 0.80 (5990.9 examples/sec; 0.021 sec/batch)\r\nINFO:tensorflow:Saving checkpoints for 11000 into /tmp/cifar10_train/model.ckpt.\r\nI0611 04:30:41.818451 140592134350592 basic_session_run_hooks.py:606] Saving checkpoints for 11000 into /tmp/cifar10_train/model.ckpt.\r\n\r\nreal\t3m14.660s\r\nuser\t11m16.581s\r\nsys\t1m22.507s\r\nbernard@Netra2:~$ time python /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py -max_steps=11000\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:127: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:120: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nW0611 12:30:29.483493 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:120: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:121: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.\r\n\r\nW0611 12:30:29.483694 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:121: The name tf.gfile.DeleteRecursively is deprecated. Please use tf.io.gfile.rmtree instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:122: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nW0611 12:30:29.486235 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:122: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:62: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nW0611 12:30:29.486796 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:62: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nW0611 12:30:29.489988 139727290783488 deprecation.py:323] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:158: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nW0611 12:30:29.493128 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\nW0611 12:30:29.493753 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0611 12:30:29.495112 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0611 12:30:29.495923 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\r\nW0611 12:30:29.498490 139727290783488 deprecation.py:323] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:79: FixedLengthRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.FixedLengthRecordDataset`.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:172: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\r\n\r\nW0611 12:30:29.507227 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:172: The name tf.random_crop is deprecated. Please use tf.image.random_crop instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nW0611 12:30:29.535539 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nDeprecated in favor of operator or tf.math.divide.\r\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\r\nW0611 12:30:29.536478 139727290783488 deprecation.py:323] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:126: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nQueue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:135: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nW0611 12:30:29.543221 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_input.py:135: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:203: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nW0611 12:30:29.544828 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:203: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:135: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nW0611 12:30:29.545093 139727290783488 deprecation.py:506] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:135: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the constructor\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:111: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nW0611 12:30:29.545320 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:111: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:93: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nW0611 12:30:29.553547 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:93: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:94: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nW0611 12:30:29.554385 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:94: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:215: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nW0611 12:30:29.564506 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:215: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:138: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\r\n\r\nW0611 12:30:29.592494 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:138: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:295: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\r\n\r\nW0611 12:30:29.645589 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:295: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:343: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\r\n\r\nW0611 12:30:29.646215 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:343: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\r\n\r\nINFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\r\nI0611 12:30:29.667191 139727290783488 summary_op_util.py:66] Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\r\nINFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\r\nI0611 12:30:29.668576 139727290783488 summary_op_util.py:66] Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\r\nINFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\r\nI0611 12:30:29.669985 139727290783488 summary_op_util.py:66] Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\r\nINFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\r\nI0611 12:30:29.671325 139727290783488 summary_op_util.py:66] Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:355: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nW0611 12:30:29.673135 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:355: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:362: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\r\n\r\nW0611 12:30:29.729470 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10.py:362: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nW0611 12:30:29.742350 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:81: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\r\n\r\nW0611 12:30:29.872007 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:81: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:107: The name tf.train.MonitoredTrainingSession is deprecated. Please use tf.compat.v1.train.MonitoredTrainingSession instead.\r\n\r\nW0611 12:30:29.872260 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:107: The name tf.train.MonitoredTrainingSession is deprecated. Please use tf.compat.v1.train.MonitoredTrainingSession instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:109: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\r\n\r\nW0611 12:30:29.872472 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:109: The name tf.train.StopAtStepHook is deprecated. Please use tf.estimator.StopAtStepHook instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:110: The name tf.train.NanTensorHook is deprecated. Please use tf.estimator.NanTensorHook instead.\r\n\r\nW0611 12:30:29.872640 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:110: The name tf.train.NanTensorHook is deprecated. Please use tf.estimator.NanTensorHook instead.\r\n\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:112: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nW0611 12:30:29.872808 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:112: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\r\n\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nI0611 12:30:29.872956 139727290783488 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0611 12:30:30.001923 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nINFO:tensorflow:Graph was finalized.\r\nI0611 12:30:30.053723 139727290783488 monitored_session.py:240] Graph was finalized.\r\n2020-06-11 12:30:30.058675: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz\r\n2020-06-11 12:30:30.058971: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ebb650 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-11 12:30:30.059002: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-11 12:30:30.060724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-06-11 12:30:30.136275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 12:30:30.136851: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f4a680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-11 12:30:30.136871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1\r\n2020-06-11 12:30:30.137061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 12:30:30.137559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.695\r\npciBusID: 0000:01:00.0\r\n2020-06-11 12:30:30.137813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-06-11 12:30:30.138813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-06-11 12:30:30.139621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-06-11 12:30:30.139874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-06-11 12:30:30.140925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-06-11 12:30:30.141827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-06-11 12:30:30.144418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-11 12:30:30.144571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 12:30:30.145116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 12:30:30.145596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\r\n2020-06-11 12:30:30.145660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-06-11 12:30:30.146507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-11 12:30:30.146518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \r\n2020-06-11 12:30:30.146541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \r\n2020-06-11 12:30:30.146810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 12:30:30.147493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 12:30:30.148067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7450 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:Running local_init_op.\r\nI0611 12:30:30.792880 139727290783488 session_manager.py:500] Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nI0611 12:30:30.802895 139727290783488 session_manager.py:502] Done running local_init_op.\r\nWARNING:tensorflow:From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0611 12:30:30.833667 139727290783488 deprecation.py:323] From /home/bernard/opt/python37/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nINFO:tensorflow:Saving checkpoints for 0 into /tmp/cifar10_train/model.ckpt.\r\nI0611 12:30:31.232494 139727290783488 basic_session_run_hooks.py:606] Saving checkpoints for 0 into /tmp/cifar10_train/model.ckpt.\r\nWARNING:tensorflow:From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:90: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\r\n\r\nW0611 12:30:31.334620 139727290783488 module_wrapper.py:139] From /home/bernard/python-dev/test/models-1.13.0/tutorials/image/cifar10/cifar10_train.py:90: The name tf.train.SessionRunArgs is deprecated. Please use tf.estimator.SessionRunArgs instead.\r\n\r\n2020-06-11 12:30:31.473473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-06-11 12:30:32.186745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-11 12:30:33.234021: step 0, loss = 4.67 (389.3 examples/sec; 0.329 sec/batch)\r\n2020-06-11 12:30:33.462534: step 10, loss = 4.62 (5601.1 examples/sec; 0.023 sec/batch)\r\n2020-06-11 12:30:33.602542: step 20, loss = 4.58 (9142.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:33.737598: step 30, loss = 4.58 (9477.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:33.874267: step 40, loss = 4.30 (9365.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:34.009428: step 50, loss = 4.22 (9470.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:34.144271: step 60, loss = 4.29 (9492.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:34.277412: step 70, loss = 4.28 (9613.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:34.413972: step 80, loss = 4.27 (9373.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:34.548104: step 90, loss = 4.19 (9542.8 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 62.2117\r\nI0611 12:30:34.840493 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 62.2117\r\n2020-06-11 12:30:34.841766: step 100, loss = 4.28 (4358.9 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:30:34.976211: step 110, loss = 4.01 (9520.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:35.113858: step 120, loss = 4.03 (9299.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:35.250274: step 130, loss = 3.97 (9383.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:35.385845: step 140, loss = 3.93 (9441.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:35.523816: step 150, loss = 3.98 (9277.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:35.660895: step 160, loss = 4.29 (9337.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:35.797337: step 170, loss = 3.98 (9381.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:35.931412: step 180, loss = 3.84 (9546.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:36.063176: step 190, loss = 3.73 (9714.4 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.5293\r\nI0611 12:30:36.343569 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.5293\r\n2020-06-11 12:30:36.344536: step 200, loss = 4.05 (4549.3 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:30:36.473926: step 210, loss = 3.77 (9892.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:36.606017: step 220, loss = 3.77 (9690.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:36.739453: step 230, loss = 3.85 (9592.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:36.872614: step 240, loss = 3.74 (9612.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:37.004938: step 250, loss = 3.67 (9673.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:37.137834: step 260, loss = 3.56 (9631.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:37.270413: step 270, loss = 3.61 (9654.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:37.406533: step 280, loss = 3.88 (9403.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:37.543310: step 290, loss = 3.57 (9358.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.1977\r\nI0611 12:30:37.854175 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.1977\r\n2020-06-11 12:30:37.855300: step 300, loss = 3.62 (4102.7 examples/sec; 0.031 sec/batch)\r\n2020-06-11 12:30:37.987512: step 310, loss = 3.55 (9681.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:38.118031: step 320, loss = 3.49 (9807.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:38.252989: step 330, loss = 3.33 (9484.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:38.388165: step 340, loss = 3.36 (9469.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:38.522673: step 350, loss = 3.37 (9516.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:38.658536: step 360, loss = 3.75 (9421.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:38.795214: step 370, loss = 3.52 (9365.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:38.929673: step 380, loss = 3.55 (9519.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:39.066156: step 390, loss = 3.29 (9378.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.6432\r\nI0611 12:30:39.377566 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.6432\r\n2020-06-11 12:30:39.378666: step 400, loss = 3.48 (4095.9 examples/sec; 0.031 sec/batch)\r\n2020-06-11 12:30:39.510432: step 410, loss = 3.39 (9714.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:39.645229: step 420, loss = 3.36 (9495.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:39.780542: step 430, loss = 3.62 (9459.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:39.914108: step 440, loss = 3.29 (9583.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:40.048489: step 450, loss = 3.38 (9525.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:40.186050: step 460, loss = 3.21 (9304.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:40.321851: step 470, loss = 3.30 (9425.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:40.457794: step 480, loss = 3.31 (9415.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:40.593497: step 490, loss = 3.12 (9432.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.6855\r\nI0611 12:30:40.899949 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.6855\r\n2020-06-11 12:30:40.901050: step 500, loss = 3.41 (4161.8 examples/sec; 0.031 sec/batch)\r\n2020-06-11 12:30:41.031748: step 510, loss = 3.12 (9793.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:41.164212: step 520, loss = 3.20 (9663.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:41.297816: step 530, loss = 3.03 (9580.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:41.435340: step 540, loss = 3.19 (9307.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:41.569850: step 550, loss = 3.19 (9516.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:41.706524: step 560, loss = 3.14 (9365.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:41.841228: step 570, loss = 2.99 (9502.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:41.974581: step 580, loss = 3.06 (9598.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:42.109036: step 590, loss = 2.93 (9519.9 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4268\r\nI0611 12:30:42.383056 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4268\r\n2020-06-11 12:30:42.383960: step 600, loss = 2.98 (4655.8 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:42.516347: step 610, loss = 3.05 (9668.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:42.650267: step 620, loss = 2.93 (9558.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:42.789975: step 630, loss = 2.94 (9161.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:42.929347: step 640, loss = 2.91 (9184.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:43.069134: step 650, loss = 3.03 (9156.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:43.206432: step 660, loss = 2.80 (9322.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:43.344658: step 670, loss = 2.87 (9260.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:43.481463: step 680, loss = 2.91 (9356.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:43.620878: step 690, loss = 2.73 (9181.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.8882\r\nI0611 12:30:43.900795 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.8882\r\n2020-06-11 12:30:43.901879: step 700, loss = 2.79 (4555.0 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:30:44.034358: step 710, loss = 2.88 (9662.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:44.166549: step 720, loss = 2.73 (9683.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:44.301567: step 730, loss = 2.69 (9480.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:44.436645: step 740, loss = 2.69 (9475.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:44.574547: step 750, loss = 2.84 (9281.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:44.709039: step 760, loss = 3.06 (9517.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:44.846095: step 770, loss = 3.03 (9339.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:44.984041: step 780, loss = 2.72 (9279.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:45.121425: step 790, loss = 2.69 (9316.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.4937\r\nI0611 12:30:45.404688 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.4937\r\n2020-06-11 12:30:45.405727: step 800, loss = 2.71 (4502.2 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:30:45.538600: step 810, loss = 2.49 (9633.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:45.673766: step 820, loss = 2.47 (9469.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:45.809043: step 830, loss = 2.54 (9462.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:45.946199: step 840, loss = 2.73 (9332.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:46.081976: step 850, loss = 2.66 (9427.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:46.218611: step 860, loss = 2.55 (9368.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:46.354132: step 870, loss = 2.75 (9445.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:46.490801: step 880, loss = 2.53 (9365.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:46.625004: step 890, loss = 2.49 (9537.8 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.2182\r\nI0611 12:30:46.892363 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.2182\r\n2020-06-11 12:30:46.893280: step 900, loss = 2.62 (4771.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:47.022043: step 910, loss = 2.42 (9940.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:47.154590: step 920, loss = 2.45 (9657.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:47.293995: step 930, loss = 2.50 (9181.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:47.429794: step 940, loss = 2.42 (9425.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:47.564162: step 950, loss = 2.42 (9526.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:47.696801: step 960, loss = 2.35 (9650.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:47.837249: step 970, loss = 2.52 (9113.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:47.969040: step 980, loss = 2.36 (9712.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:48.102033: step 990, loss = 2.63 (9624.5 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4873\r\nI0611 12:30:48.374131 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4873\r\n2020-06-11 12:30:48.375321: step 1000, loss = 2.47 (4683.6 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:48.508260: step 1010, loss = 2.65 (9628.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:48.645143: step 1020, loss = 2.45 (9351.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:48.782786: step 1030, loss = 2.35 (9299.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:48.917829: step 1040, loss = 2.60 (9478.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:49.055541: step 1050, loss = 2.40 (9294.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:49.192602: step 1060, loss = 2.29 (9338.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:49.330454: step 1070, loss = 2.52 (9285.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:49.463783: step 1080, loss = 2.37 (9600.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:49.595705: step 1090, loss = 2.29 (9702.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.5507\r\nI0611 12:30:49.876749 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.5507\r\n2020-06-11 12:30:49.877840: step 1100, loss = 2.28 (4536.8 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:30:50.010959: step 1110, loss = 2.25 (9615.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:50.146875: step 1120, loss = 2.43 (9417.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:50.282573: step 1130, loss = 2.40 (9432.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:50.416805: step 1140, loss = 2.24 (9535.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:50.551982: step 1150, loss = 2.18 (9469.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:50.685008: step 1160, loss = 2.29 (9622.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:50.820304: step 1170, loss = 2.27 (9460.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:50.957357: step 1180, loss = 2.22 (9339.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:51.093802: step 1190, loss = 2.40 (9381.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1727\r\nI0611 12:30:51.365444 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1727\r\n2020-06-11 12:30:51.366566: step 1200, loss = 2.28 (4692.6 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:51.496607: step 1210, loss = 2.20 (9843.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:51.631116: step 1220, loss = 2.21 (9516.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:51.765730: step 1230, loss = 2.24 (9508.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:51.898899: step 1240, loss = 2.04 (9611.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:52.033126: step 1250, loss = 2.25 (9536.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:52.171114: step 1260, loss = 2.23 (9276.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:52.308278: step 1270, loss = 2.11 (9331.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:52.443591: step 1280, loss = 2.19 (9459.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:52.578019: step 1290, loss = 2.22 (9521.8 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.725\r\nI0611 12:30:52.864135 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.725\r\n2020-06-11 12:30:52.865299: step 1300, loss = 2.00 (4455.6 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:30:53.002653: step 1310, loss = 2.01 (9319.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:53.139203: step 1320, loss = 2.11 (9373.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:53.275746: step 1330, loss = 1.93 (9374.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:53.409860: step 1340, loss = 2.11 (9544.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:53.546335: step 1350, loss = 2.01 (9379.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:53.683417: step 1360, loss = 2.14 (9337.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:53.816835: step 1370, loss = 2.01 (9593.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:53.952995: step 1380, loss = 2.18 (9400.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:54.088171: step 1390, loss = 2.07 (9469.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.9051\r\nI0611 12:30:54.358805 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.9051\r\n2020-06-11 12:30:54.359848: step 1400, loss = 2.33 (4711.4 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:54.486437: step 1410, loss = 2.11 (10111.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:54.621066: step 1420, loss = 1.89 (9507.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:54.757783: step 1430, loss = 1.96 (9362.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:54.892049: step 1440, loss = 1.88 (9533.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:55.026844: step 1450, loss = 2.03 (9495.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:55.163515: step 1460, loss = 2.03 (9365.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:55.298806: step 1470, loss = 1.83 (9461.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:55.433881: step 1480, loss = 2.08 (9476.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:55.570034: step 1490, loss = 1.89 (9401.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.6104\r\nI0611 12:30:55.837844 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.6104\r\n2020-06-11 12:30:55.838988: step 1500, loss = 2.08 (4759.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:55.971336: step 1510, loss = 2.04 (9671.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:56.109061: step 1520, loss = 1.82 (9294.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:56.244159: step 1530, loss = 2.01 (9474.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:56.381264: step 1540, loss = 2.18 (9335.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:56.517140: step 1550, loss = 1.81 (9420.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:56.652882: step 1560, loss = 2.04 (9429.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:56.786299: step 1570, loss = 1.87 (9594.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:56.919256: step 1580, loss = 1.78 (9627.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:57.055219: step 1590, loss = 2.10 (9414.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.408\r\nI0611 12:30:57.321357 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.408\r\n2020-06-11 12:30:57.322507: step 1600, loss = 2.22 (4788.8 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:30:57.455471: step 1610, loss = 1.96 (9626.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:57.590339: step 1620, loss = 1.75 (9490.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:57.724421: step 1630, loss = 1.84 (9546.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:57.861018: step 1640, loss = 1.70 (9370.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:57.996709: step 1650, loss = 2.01 (9433.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:58.131548: step 1660, loss = 1.83 (9492.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:58.265154: step 1670, loss = 1.92 (9580.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:58.400128: step 1680, loss = 1.85 (9483.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:58.535425: step 1690, loss = 1.96 (9460.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.7012\r\nI0611 12:30:58.798447 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.7012\r\n2020-06-11 12:30:58.799608: step 1700, loss = 2.01 (4845.1 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:30:58.931313: step 1710, loss = 1.75 (9718.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:59.065213: step 1720, loss = 1.67 (9559.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:59.201871: step 1730, loss = 1.89 (9366.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:59.336518: step 1740, loss = 1.73 (9506.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:59.472752: step 1750, loss = 1.72 (9395.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:59.606758: step 1760, loss = 1.83 (9551.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:30:59.744591: step 1770, loss = 1.74 (9286.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:30:59.878602: step 1780, loss = 1.70 (9551.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:00.013040: step 1790, loss = 1.82 (9521.2 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.6462\r\nI0611 12:31:00.276727 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.6462\r\n2020-06-11 12:31:00.277814: step 1800, loss = 2.08 (4834.3 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:00.409482: step 1810, loss = 1.63 (9721.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:00.542472: step 1820, loss = 1.70 (9624.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:00.679599: step 1830, loss = 1.71 (9334.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:00.815963: step 1840, loss = 1.62 (9386.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:00.949928: step 1850, loss = 1.64 (9554.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:01.084127: step 1860, loss = 1.76 (9538.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:01.220557: step 1870, loss = 1.77 (9382.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:01.355590: step 1880, loss = 1.64 (9479.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:01.491598: step 1890, loss = 1.57 (9411.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.9988\r\nI0611 12:31:01.769292 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.9988\r\n2020-06-11 12:31:01.770518: step 1900, loss = 1.53 (4589.1 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:01.901104: step 1910, loss = 1.66 (9802.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:02.037116: step 1920, loss = 1.96 (9411.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:02.173033: step 1930, loss = 1.70 (9417.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:02.306970: step 1940, loss = 1.79 (9556.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:02.442939: step 1950, loss = 1.63 (9413.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:02.579475: step 1960, loss = 1.76 (9374.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:02.716324: step 1970, loss = 1.49 (9353.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:02.854876: step 1980, loss = 1.68 (9238.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:02.990221: step 1990, loss = 1.56 (9457.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.144\r\nI0611 12:31:03.258613 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.144\r\n2020-06-11 12:31:03.259694: step 2000, loss = 1.57 (4750.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:03.390165: step 2010, loss = 1.58 (9810.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:03.522066: step 2020, loss = 1.77 (9704.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:03.659904: step 2030, loss = 1.60 (9286.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:03.796631: step 2040, loss = 1.54 (9361.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:03.931608: step 2050, loss = 1.68 (9483.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:04.066514: step 2060, loss = 1.73 (9488.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:04.201861: step 2070, loss = 1.58 (9457.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:04.338102: step 2080, loss = 1.84 (9395.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:04.473076: step 2090, loss = 1.47 (9483.3 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.0379\r\nI0611 12:31:04.750305 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.0379\r\n2020-06-11 12:31:04.751397: step 2100, loss = 1.64 (4598.9 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:04.883691: step 2110, loss = 2.01 (9675.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:05.015472: step 2120, loss = 1.67 (9713.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:05.151165: step 2130, loss = 1.47 (9433.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:05.284237: step 2140, loss = 1.49 (9618.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:05.417760: step 2150, loss = 1.33 (9586.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:05.552094: step 2160, loss = 1.55 (9528.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:05.688237: step 2170, loss = 1.42 (9401.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:05.823982: step 2180, loss = 1.67 (9429.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:05.958847: step 2190, loss = 1.56 (9491.0 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.9287\r\nI0611 12:31:06.222437 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.9287\r\n2020-06-11 12:31:06.223501: step 2200, loss = 1.66 (4836.4 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:06.356726: step 2210, loss = 1.52 (9607.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:06.490046: step 2220, loss = 1.69 (9601.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:06.624956: step 2230, loss = 1.54 (9487.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:06.760657: step 2240, loss = 1.57 (9432.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:06.896917: step 2250, loss = 1.69 (9393.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:07.030899: step 2260, loss = 1.74 (9553.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:07.166917: step 2270, loss = 1.77 (9410.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:07.305916: step 2280, loss = 1.48 (9208.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:07.443032: step 2290, loss = 1.50 (9335.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1474\r\nI0611 12:31:07.711683 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1474\r\n2020-06-11 12:31:07.712787: step 2300, loss = 1.51 (4745.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:07.847202: step 2310, loss = 1.54 (9522.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:07.979160: step 2320, loss = 1.57 (9700.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:08.112909: step 2330, loss = 1.48 (9570.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:08.249367: step 2340, loss = 1.51 (9380.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:08.383627: step 2350, loss = 1.38 (9533.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:08.517883: step 2360, loss = 1.27 (9534.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:08.653768: step 2370, loss = 1.61 (9419.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:08.789469: step 2380, loss = 1.38 (9432.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:08.923743: step 2390, loss = 1.39 (9532.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.0397\r\nI0611 12:31:09.203377 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.0397\r\n2020-06-11 12:31:09.204551: step 2400, loss = 1.35 (4558.2 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:09.338671: step 2410, loss = 1.57 (9544.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:09.474387: step 2420, loss = 1.49 (9431.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:09.610288: step 2430, loss = 1.47 (9418.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:09.744419: step 2440, loss = 1.48 (9542.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:09.878576: step 2450, loss = 1.55 (9541.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:10.015181: step 2460, loss = 1.40 (9370.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:10.150029: step 2470, loss = 1.47 (9492.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:10.288562: step 2480, loss = 1.29 (9239.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:10.423227: step 2490, loss = 1.38 (9505.1 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.0266\r\nI0611 12:31:10.695295 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.0266\r\n2020-06-11 12:31:10.696274: step 2500, loss = 1.51 (4687.8 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:10.827524: step 2510, loss = 1.38 (9752.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:10.958960: step 2520, loss = 1.34 (9738.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:11.092784: step 2530, loss = 1.39 (9564.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:11.227685: step 2540, loss = 1.40 (9488.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:11.363363: step 2550, loss = 1.29 (9434.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:11.497657: step 2560, loss = 1.49 (9531.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:11.630962: step 2570, loss = 1.57 (9602.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:11.767666: step 2580, loss = 1.43 (9363.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:11.901804: step 2590, loss = 1.46 (9542.5 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4188\r\nI0611 12:31:12.178562 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4188\r\n2020-06-11 12:31:12.179689: step 2600, loss = 1.36 (4606.2 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:12.312903: step 2610, loss = 1.24 (9608.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:12.448568: step 2620, loss = 1.41 (9435.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:12.582856: step 2630, loss = 1.54 (9531.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:12.720107: step 2640, loss = 1.37 (9326.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:12.857648: step 2650, loss = 1.58 (9306.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:12.996441: step 2660, loss = 1.31 (9222.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:13.135098: step 2670, loss = 1.40 (9231.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:13.271276: step 2680, loss = 1.32 (9399.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:13.410017: step 2690, loss = 1.43 (9225.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.954\r\nI0611 12:31:13.672140 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.954\r\n2020-06-11 12:31:13.673330: step 2700, loss = 1.17 (4861.1 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:13.804682: step 2710, loss = 1.27 (9745.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:13.938618: step 2720, loss = 1.35 (9556.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:14.074927: step 2730, loss = 1.47 (9390.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:14.207722: step 2740, loss = 1.39 (9639.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:14.341677: step 2750, loss = 1.43 (9555.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:14.477967: step 2760, loss = 1.15 (9391.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:14.616646: step 2770, loss = 1.39 (9230.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:14.756050: step 2780, loss = 1.12 (9181.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:14.890791: step 2790, loss = 1.33 (9499.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.1456\r\nI0611 12:31:15.183952 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.1456\r\n2020-06-11 12:31:15.185054: step 2800, loss = 1.40 (4349.8 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:31:15.315762: step 2810, loss = 1.34 (9792.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:15.447801: step 2820, loss = 1.50 (9694.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:15.580975: step 2830, loss = 1.32 (9611.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:15.715018: step 2840, loss = 1.14 (9549.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:15.851358: step 2850, loss = 1.35 (9388.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:15.986999: step 2860, loss = 1.35 (9436.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:16.123911: step 2870, loss = 1.46 (9349.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:16.259248: step 2880, loss = 1.16 (9458.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:16.393706: step 2890, loss = 1.16 (9520.3 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4197\r\nI0611 12:31:16.667205 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4197\r\n2020-06-11 12:31:16.668332: step 2900, loss = 1.44 (4660.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:16.800784: step 2910, loss = 1.39 (9664.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:16.936412: step 2920, loss = 1.23 (9437.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:17.069588: step 2930, loss = 1.42 (9611.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:17.205490: step 2940, loss = 1.14 (9418.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:17.343844: step 2950, loss = 1.35 (9251.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:17.479473: step 2960, loss = 1.24 (9437.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:17.617095: step 2970, loss = 1.35 (9300.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:17.752765: step 2980, loss = 1.35 (9434.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:17.888861: step 2990, loss = 1.27 (9405.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1276\r\nI0611 12:31:18.156912 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1276\r\n2020-06-11 12:31:18.158077: step 3000, loss = 1.04 (4754.5 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:18.289797: step 3010, loss = 1.33 (9717.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:18.425231: step 3020, loss = 1.17 (9451.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:18.559348: step 3030, loss = 1.34 (9543.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:18.697193: step 3040, loss = 1.32 (9285.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:18.831434: step 3050, loss = 1.24 (9535.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:18.964532: step 3060, loss = 1.26 (9617.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:19.100688: step 3070, loss = 1.12 (9401.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:19.236916: step 3080, loss = 1.22 (9396.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:19.372872: step 3090, loss = 1.35 (9414.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4912\r\nI0611 12:31:19.638550 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4912\r\n2020-06-11 12:31:19.639702: step 3100, loss = 1.25 (4797.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:19.772075: step 3110, loss = 1.39 (9669.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:19.904610: step 3120, loss = 1.35 (9658.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:20.042596: step 3130, loss = 1.15 (9276.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:20.177547: step 3140, loss = 1.35 (9484.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:20.311414: step 3150, loss = 1.26 (9561.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:20.448521: step 3160, loss = 1.30 (9335.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:20.585880: step 3170, loss = 1.15 (9318.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:20.721229: step 3180, loss = 1.21 (9457.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:20.857685: step 3190, loss = 1.19 (9380.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.5098\r\nI0611 12:31:21.119842 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.5098\r\n2020-06-11 12:31:21.120867: step 3200, loss = 1.21 (4863.5 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:21.254115: step 3210, loss = 1.07 (9606.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:21.385461: step 3220, loss = 1.25 (9745.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:21.521675: step 3230, loss = 1.30 (9396.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:21.657147: step 3240, loss = 1.19 (9448.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:21.792788: step 3250, loss = 1.17 (9436.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:21.927277: step 3260, loss = 1.11 (9517.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:22.064060: step 3270, loss = 1.18 (9357.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:22.200326: step 3280, loss = 1.14 (9393.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:22.335850: step 3290, loss = 1.28 (9444.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4112\r\nI0611 12:31:22.603260 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4112\r\n2020-06-11 12:31:22.604348: step 3300, loss = 1.32 (4767.2 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:22.737476: step 3310, loss = 1.25 (9614.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:22.868875: step 3320, loss = 0.97 (9741.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:23.002355: step 3330, loss = 1.15 (9589.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:23.136425: step 3340, loss = 1.41 (9547.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:23.276217: step 3350, loss = 1.08 (9156.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:23.411159: step 3360, loss = 1.10 (9485.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:23.547053: step 3370, loss = 0.98 (9419.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:23.683770: step 3380, loss = 1.08 (9362.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:23.818291: step 3390, loss = 1.02 (9515.3 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.483\r\nI0611 12:31:24.107402 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.483\r\n2020-06-11 12:31:24.108436: step 3400, loss = 1.26 (4411.5 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:31:24.244577: step 3410, loss = 1.31 (9402.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:24.377834: step 3420, loss = 1.13 (9605.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:24.512576: step 3430, loss = 1.30 (9499.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:24.649716: step 3440, loss = 1.17 (9334.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:24.787117: step 3450, loss = 1.33 (9315.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:24.924836: step 3460, loss = 1.15 (9294.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:25.061905: step 3470, loss = 1.28 (9338.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:25.198092: step 3480, loss = 1.31 (9398.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:25.337464: step 3490, loss = 1.19 (9184.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.908\r\nI0611 12:31:25.602010 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.908\r\n2020-06-11 12:31:25.603072: step 3500, loss = 1.27 (4819.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:25.734884: step 3510, loss = 1.07 (9711.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:25.872748: step 3520, loss = 1.19 (9284.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.008044: step 3530, loss = 1.16 (9460.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.145034: step 3540, loss = 1.19 (9343.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.281167: step 3550, loss = 1.16 (9402.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.416540: step 3560, loss = 1.15 (9455.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.555337: step 3570, loss = 1.26 (9222.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.691652: step 3580, loss = 1.21 (9390.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:26.823143: step 3590, loss = 1.02 (9734.5 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.936\r\nI0611 12:31:27.095952 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.936\r\n2020-06-11 12:31:27.097054: step 3600, loss = 1.21 (4673.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:27.230816: step 3610, loss = 1.06 (9569.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:27.368434: step 3620, loss = 1.29 (9301.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:27.504987: step 3630, loss = 0.94 (9373.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:27.646594: step 3640, loss = 0.94 (9039.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:27.785783: step 3650, loss = 1.20 (9196.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:27.925784: step 3660, loss = 1.05 (9142.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:28.063940: step 3670, loss = 1.15 (9264.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:28.206496: step 3680, loss = 1.05 (8978.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:28.345108: step 3690, loss = 1.07 (9234.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.7589\r\nI0611 12:31:28.616667 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.7589\r\n2020-06-11 12:31:28.617780: step 3700, loss = 1.45 (4694.2 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:28.748543: step 3710, loss = 1.12 (9788.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:28.884614: step 3720, loss = 1.15 (9407.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.024595: step 3730, loss = 1.17 (9144.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.165780: step 3740, loss = 1.22 (9066.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.302386: step 3750, loss = 1.12 (9370.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.441083: step 3760, loss = 1.10 (9228.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.580783: step 3770, loss = 1.17 (9162.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.720650: step 3780, loss = 1.09 (9151.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:29.859386: step 3790, loss = 1.01 (9226.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.0583\r\nI0611 12:31:30.130482 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.0583\r\n2020-06-11 12:31:30.131563: step 3800, loss = 1.08 (4702.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:30.265864: step 3810, loss = 1.11 (9530.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:30.399303: step 3820, loss = 1.07 (9592.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:30.537503: step 3830, loss = 1.06 (9261.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:30.670106: step 3840, loss = 1.31 (9652.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:30.807236: step 3850, loss = 1.11 (9334.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:30.940391: step 3860, loss = 1.05 (9612.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:31.075838: step 3870, loss = 1.13 (9450.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:31.211603: step 3880, loss = 1.13 (9428.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:31.347659: step 3890, loss = 1.09 (9407.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.5661\r\nI0611 12:31:31.610528 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.5661\r\n2020-06-11 12:31:31.611605: step 3900, loss = 1.10 (4849.4 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:31.746684: step 3910, loss = 1.10 (9476.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:31.882555: step 3920, loss = 0.98 (9420.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:32.017316: step 3930, loss = 1.08 (9498.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:32.153625: step 3940, loss = 1.07 (9390.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:32.292509: step 3950, loss = 1.23 (9216.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:32.429955: step 3960, loss = 1.22 (9312.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:32.565405: step 3970, loss = 1.15 (9450.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:32.697650: step 3980, loss = 1.01 (9679.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:32.835317: step 3990, loss = 1.16 (9297.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.096\r\nI0611 12:31:33.100918 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.096\r\n2020-06-11 12:31:33.102145: step 4000, loss = 0.98 (4797.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:33.233800: step 4010, loss = 1.07 (9722.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:33.366965: step 4020, loss = 1.11 (9612.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:33.504542: step 4030, loss = 1.19 (9303.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:33.642436: step 4040, loss = 1.18 (9282.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:33.778075: step 4050, loss = 0.98 (9436.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:33.913423: step 4060, loss = 1.13 (9457.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:34.049164: step 4070, loss = 1.11 (9429.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:34.184070: step 4080, loss = 1.19 (9488.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:34.319962: step 4090, loss = 1.21 (9419.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.9183\r\nI0611 12:31:34.595281 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.9183\r\n2020-06-11 12:31:34.596368: step 4100, loss = 1.21 (4630.8 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:34.726054: step 4110, loss = 0.95 (9870.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:34.865437: step 4120, loss = 1.41 (9183.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.005705: step 4130, loss = 1.02 (9126.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.141818: step 4140, loss = 1.10 (9403.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.283808: step 4150, loss = 1.26 (9014.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.420933: step 4160, loss = 1.16 (9334.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.561509: step 4170, loss = 1.00 (9105.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.700907: step 4180, loss = 1.23 (9182.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:35.839284: step 4190, loss = 1.16 (9250.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.8877\r\nI0611 12:31:36.113004 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.8877\r\n2020-06-11 12:31:36.114016: step 4200, loss = 1.10 (4659.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:36.244466: step 4210, loss = 0.94 (9812.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:36.378184: step 4220, loss = 1.00 (9572.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:36.514319: step 4230, loss = 1.03 (9402.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:36.648648: step 4240, loss = 1.13 (9528.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:36.782757: step 4250, loss = 1.03 (9544.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:36.919774: step 4260, loss = 1.10 (9341.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:37.055626: step 4270, loss = 1.12 (9422.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:37.191686: step 4280, loss = 1.08 (9407.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:37.327338: step 4290, loss = 1.13 (9436.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.6726\r\nI0611 12:31:37.590724 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.6726\r\n2020-06-11 12:31:37.591888: step 4300, loss = 1.08 (4838.3 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:37.726307: step 4310, loss = 0.99 (9522.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:37.858530: step 4320, loss = 1.09 (9680.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:38.001082: step 4330, loss = 1.22 (8979.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:38.140660: step 4340, loss = 1.00 (9170.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:38.279482: step 4350, loss = 0.91 (9220.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:38.420250: step 4360, loss = 1.04 (9093.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:38.561237: step 4370, loss = 1.09 (9078.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:38.699804: step 4380, loss = 1.04 (9237.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:38.839964: step 4390, loss = 0.96 (9132.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.7571\r\nI0611 12:31:39.111459 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.7571\r\n2020-06-11 12:31:39.112586: step 4400, loss = 1.21 (4695.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:39.245469: step 4410, loss = 1.00 (9632.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:39.387437: step 4420, loss = 1.01 (9016.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:39.531287: step 4430, loss = 1.15 (8898.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:39.677018: step 4440, loss = 0.94 (8783.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:39.817289: step 4450, loss = 0.97 (9125.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:39.961615: step 4460, loss = 1.06 (8868.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:40.108567: step 4470, loss = 0.97 (8710.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:40.248911: step 4480, loss = 0.97 (9120.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:40.393821: step 4490, loss = 1.12 (8833.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.0187\r\nI0611 12:31:40.673491 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 64.0187\r\n2020-06-11 12:31:40.674548: step 4500, loss = 0.99 (4559.5 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:40.805725: step 4510, loss = 1.09 (9757.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:40.935198: step 4520, loss = 0.97 (9886.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:41.072422: step 4530, loss = 1.06 (9327.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:41.211428: step 4540, loss = 1.12 (9208.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:41.347386: step 4550, loss = 1.10 (9414.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:41.487261: step 4560, loss = 0.85 (9151.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:41.624984: step 4570, loss = 0.92 (9294.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:41.761529: step 4580, loss = 0.85 (9374.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:41.896289: step 4590, loss = 0.97 (9498.4 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1913\r\nI0611 12:31:42.161828 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1913\r\n2020-06-11 12:31:42.162911: step 4600, loss = 0.98 (4800.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:42.293100: step 4610, loss = 0.98 (9832.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:42.429772: step 4620, loss = 0.98 (9365.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:42.563990: step 4630, loss = 1.01 (9536.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:42.700380: step 4640, loss = 1.04 (9384.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:42.839813: step 4650, loss = 1.09 (9180.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:42.974518: step 4660, loss = 1.13 (9502.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:43.110167: step 4670, loss = 0.99 (9436.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:43.245979: step 4680, loss = 0.81 (9424.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:43.380693: step 4690, loss = 1.09 (9501.6 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.3803\r\nI0611 12:31:43.645921 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.3803\r\n2020-06-11 12:31:43.646984: step 4700, loss = 0.77 (4806.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:43.779638: step 4710, loss = 1.07 (9649.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:43.912184: step 4720, loss = 1.00 (9657.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:44.044650: step 4730, loss = 0.96 (9662.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:44.180610: step 4740, loss = 0.95 (9414.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:44.315702: step 4750, loss = 0.76 (9475.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:44.452757: step 4760, loss = 0.86 (9339.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:44.586514: step 4770, loss = 1.16 (9569.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:44.722228: step 4780, loss = 0.87 (9431.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:44.855656: step 4790, loss = 1.02 (9593.1 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.9861\r\nI0611 12:31:45.116812 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.9861\r\n2020-06-11 12:31:45.117924: step 4800, loss = 1.09 (4880.4 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:45.249717: step 4810, loss = 1.05 (9712.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:45.382485: step 4820, loss = 0.91 (9640.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:45.517958: step 4830, loss = 1.04 (9448.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:45.652364: step 4840, loss = 0.91 (9523.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:45.785869: step 4850, loss = 0.95 (9587.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:45.921368: step 4860, loss = 0.97 (9446.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:46.058780: step 4870, loss = 1.00 (9315.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:46.193583: step 4880, loss = 1.02 (9495.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:46.331214: step 4890, loss = 0.91 (9300.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4459\r\nI0611 12:31:46.599469 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4459\r\n2020-06-11 12:31:46.600618: step 4900, loss = 0.99 (4751.2 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:46.735987: step 4910, loss = 1.01 (9455.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:46.868030: step 4920, loss = 0.95 (9693.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:47.003445: step 4930, loss = 0.91 (9452.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:47.137406: step 4940, loss = 0.87 (9555.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:47.273021: step 4950, loss = 1.05 (9439.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:47.410757: step 4960, loss = 1.09 (9292.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:47.548280: step 4970, loss = 1.08 (9307.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:47.682648: step 4980, loss = 1.15 (9526.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:47.818180: step 4990, loss = 1.13 (9444.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.4657\r\nI0611 12:31:48.104013 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.4657\r\n2020-06-11 12:31:48.105111: step 5000, loss = 1.12 (4460.9 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:31:48.237654: step 5010, loss = 1.04 (9657.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:48.375870: step 5020, loss = 1.01 (9261.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:48.515238: step 5030, loss = 0.87 (9184.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:48.652823: step 5040, loss = 0.95 (9303.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:48.790911: step 5050, loss = 0.93 (9269.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:48.930452: step 5060, loss = 1.01 (9173.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:49.072221: step 5070, loss = 1.01 (9029.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:49.209514: step 5080, loss = 0.88 (9322.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:49.350594: step 5090, loss = 0.88 (9072.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.3637\r\nI0611 12:31:49.633915 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.3637\r\n2020-06-11 12:31:49.635064: step 5100, loss = 1.07 (4499.5 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:49.769740: step 5110, loss = 0.91 (9504.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:49.905993: step 5120, loss = 0.96 (9394.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:50.044989: step 5130, loss = 0.96 (9208.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:50.179262: step 5140, loss = 0.90 (9532.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:50.317917: step 5150, loss = 1.07 (9231.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:50.456594: step 5160, loss = 0.95 (9230.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:50.594316: step 5170, loss = 0.98 (9294.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:50.731378: step 5180, loss = 1.12 (9338.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:50.867838: step 5190, loss = 0.86 (9380.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.7414\r\nI0611 12:31:51.132236 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.7414\r\n2020-06-11 12:31:51.133225: step 5200, loss = 1.02 (4823.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:31:51.266494: step 5210, loss = 0.90 (9604.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:51.402149: step 5220, loss = 0.85 (9435.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:51.538240: step 5230, loss = 0.95 (9405.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:51.677524: step 5240, loss = 1.04 (9189.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:51.812670: step 5250, loss = 1.03 (9471.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:51.948397: step 5260, loss = 0.91 (9430.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:52.085806: step 5270, loss = 0.97 (9315.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:52.221237: step 5280, loss = 0.96 (9451.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:52.357660: step 5290, loss = 1.07 (9382.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1473\r\nI0611 12:31:52.621484 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1473\r\n2020-06-11 12:31:52.622554: step 5300, loss = 0.97 (4832.0 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:52.781188: step 5310, loss = 1.00 (8069.5 examples/sec; 0.016 sec/batch)\r\n2020-06-11 12:31:52.938471: step 5320, loss = 0.92 (8138.1 examples/sec; 0.016 sec/batch)\r\n2020-06-11 12:31:53.092002: step 5330, loss = 1.04 (8337.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:53.242796: step 5340, loss = 0.91 (8488.3 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:53.393667: step 5350, loss = 1.29 (8484.1 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:53.540047: step 5360, loss = 0.89 (8744.4 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:53.689237: step 5370, loss = 0.85 (8579.7 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:53.834704: step 5380, loss = 1.06 (8799.2 examples/sec; 0.015 sec/batch)\r\n2020-06-11 12:31:53.986756: step 5390, loss = 0.78 (8418.2 examples/sec; 0.015 sec/batch)\r\nINFO:tensorflow:global_step/sec: 60.6461\r\nI0611 12:31:54.270397 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 60.6461\r\n2020-06-11 12:31:54.271505: step 5400, loss = 1.05 (4495.0 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:54.405270: step 5410, loss = 1.09 (9569.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:54.537385: step 5420, loss = 1.06 (9688.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:54.671988: step 5430, loss = 1.03 (9509.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:54.807235: step 5440, loss = 1.01 (9464.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:54.944894: step 5450, loss = 1.03 (9298.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:55.080552: step 5460, loss = 1.06 (9435.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:55.217559: step 5470, loss = 0.91 (9342.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:55.354414: step 5480, loss = 0.91 (9352.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:55.491784: step 5490, loss = 0.91 (9317.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4831\r\nI0611 12:31:55.752263 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4831\r\n2020-06-11 12:31:55.753414: step 5500, loss = 1.31 (4892.3 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:55.887346: step 5510, loss = 1.23 (9557.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:56.024491: step 5520, loss = 0.96 (9333.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:56.165946: step 5530, loss = 1.16 (9048.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:56.303297: step 5540, loss = 1.11 (9319.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:56.442407: step 5550, loss = 1.03 (9201.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:56.583927: step 5560, loss = 1.09 (9044.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:56.727575: step 5570, loss = 0.92 (8910.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:56.865051: step 5580, loss = 1.12 (9310.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:57.006191: step 5590, loss = 0.94 (9069.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.1231\r\nI0611 12:31:57.287802 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.1231\r\n2020-06-11 12:31:57.289005: step 5600, loss = 0.90 (4525.9 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:31:57.420781: step 5610, loss = 0.89 (9713.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:57.556625: step 5620, loss = 1.01 (9422.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:57.692833: step 5630, loss = 1.10 (9397.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:57.829495: step 5640, loss = 1.07 (9366.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:57.966494: step 5650, loss = 0.88 (9343.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:58.103009: step 5660, loss = 0.96 (9376.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:58.236538: step 5670, loss = 1.06 (9586.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:58.371433: step 5680, loss = 0.96 (9488.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:58.508362: step 5690, loss = 1.01 (9347.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4893\r\nI0611 12:31:58.769541 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4893\r\n2020-06-11 12:31:58.770793: step 5700, loss = 1.03 (4877.4 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:31:58.903993: step 5710, loss = 1.00 (9609.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:59.036688: step 5720, loss = 0.92 (9646.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:59.168574: step 5730, loss = 0.92 (9705.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:59.302130: step 5740, loss = 0.94 (9583.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:59.436119: step 5750, loss = 0.98 (9553.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:31:59.571136: step 5760, loss = 0.92 (9480.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:59.708889: step 5770, loss = 0.94 (9292.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:59.845413: step 5780, loss = 0.89 (9375.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:31:59.982624: step 5790, loss = 1.03 (9328.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.7163\r\nI0611 12:32:00.246281 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.7163\r\n2020-06-11 12:32:00.247336: step 5800, loss = 0.91 (4835.4 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:00.380527: step 5810, loss = 1.09 (9610.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:00.516927: step 5820, loss = 0.87 (9384.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:00.653517: step 5830, loss = 0.78 (9371.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:00.792749: step 5840, loss = 0.85 (9193.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:00.929407: step 5850, loss = 0.95 (9366.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:01.065875: step 5860, loss = 0.95 (9379.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:01.203870: step 5870, loss = 1.00 (9275.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:01.344186: step 5880, loss = 1.02 (9122.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:01.479619: step 5890, loss = 1.05 (9451.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.9191\r\nI0611 12:32:01.763275 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.9191\r\n2020-06-11 12:32:01.764404: step 5900, loss = 0.97 (4494.5 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:01.897042: step 5910, loss = 1.00 (9650.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:02.030801: step 5920, loss = 0.82 (9569.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:02.165299: step 5930, loss = 0.94 (9516.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:02.299819: step 5940, loss = 0.94 (9515.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:02.436456: step 5950, loss = 1.00 (9368.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:02.570545: step 5960, loss = 0.96 (9546.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:02.709250: step 5970, loss = 0.86 (9228.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:02.847939: step 5980, loss = 1.09 (9229.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:02.983523: step 5990, loss = 1.08 (9440.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.2735\r\nI0611 12:32:03.249803 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.2735\r\n2020-06-11 12:32:03.250950: step 6000, loss = 1.02 (4786.3 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:03.386811: step 6010, loss = 1.22 (9421.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:03.525678: step 6020, loss = 0.88 (9217.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:03.664236: step 6030, loss = 0.92 (9237.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:03.804119: step 6040, loss = 1.02 (9150.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:03.944829: step 6050, loss = 0.91 (9096.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:04.082812: step 6060, loss = 1.06 (9276.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:04.224240: step 6070, loss = 1.05 (9050.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:04.362218: step 6080, loss = 0.83 (9276.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:04.501440: step 6090, loss = 0.85 (9194.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.2398\r\nI0611 12:32:04.782556 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.2398\r\n2020-06-11 12:32:04.783783: step 6100, loss = 0.93 (4533.4 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:04.915969: step 6110, loss = 0.84 (9683.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:05.052604: step 6120, loss = 0.96 (9368.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:05.193069: step 6130, loss = 1.02 (9112.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:05.331083: step 6140, loss = 0.96 (9274.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:05.469399: step 6150, loss = 1.03 (9254.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:05.611998: step 6160, loss = 1.08 (8976.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:05.751572: step 6170, loss = 0.93 (9170.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:05.888835: step 6180, loss = 1.22 (9325.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:06.028463: step 6190, loss = 0.98 (9167.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.9384\r\nI0611 12:32:06.299111 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.9384\r\n2020-06-11 12:32:06.300222: step 6200, loss = 1.07 (4710.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:06.432601: step 6210, loss = 1.00 (9669.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:06.570180: step 6220, loss = 1.00 (9304.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:06.708793: step 6230, loss = 0.94 (9234.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:06.849486: step 6240, loss = 0.92 (9097.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:06.988259: step 6250, loss = 0.95 (9223.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:07.127899: step 6260, loss = 0.98 (9166.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:07.266964: step 6270, loss = 1.09 (9204.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:07.407066: step 6280, loss = 0.93 (9136.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:07.546689: step 6290, loss = 1.03 (9167.5 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4383\r\nI0611 12:32:07.827293 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.4383\r\n2020-06-11 12:32:07.828392: step 6300, loss = 0.84 (4543.7 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:07.960283: step 6310, loss = 0.98 (9705.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:08.100166: step 6320, loss = 0.87 (9150.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:08.235256: step 6330, loss = 1.05 (9475.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:08.373830: step 6340, loss = 0.88 (9236.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:08.512644: step 6350, loss = 0.88 (9220.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:08.655456: step 6360, loss = 0.97 (8962.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:08.793086: step 6370, loss = 1.04 (9300.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:08.931743: step 6380, loss = 1.04 (9231.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:09.071510: step 6390, loss = 0.95 (9158.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.8746\r\nI0611 12:32:09.368697 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 64.8746\r\n2020-06-11 12:32:09.369864: step 6400, loss = 0.98 (4290.1 examples/sec; 0.030 sec/batch)\r\n2020-06-11 12:32:09.505182: step 6410, loss = 0.92 (9459.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:09.647394: step 6420, loss = 1.02 (9000.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:09.788033: step 6430, loss = 1.16 (9101.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:09.927408: step 6440, loss = 0.86 (9183.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:10.066184: step 6450, loss = 0.95 (9223.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:10.205228: step 6460, loss = 0.84 (9205.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:10.344744: step 6470, loss = 0.97 (9174.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:10.484999: step 6480, loss = 1.09 (9126.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:10.624853: step 6490, loss = 0.84 (9152.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4232\r\nI0611 12:32:10.897231 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.4232\r\n2020-06-11 12:32:10.898211: step 6500, loss = 0.90 (4682.4 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:11.029941: step 6510, loss = 1.05 (9716.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:11.165163: step 6520, loss = 1.17 (9466.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:11.301832: step 6530, loss = 0.94 (9365.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:11.440656: step 6540, loss = 0.99 (9220.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:11.583050: step 6550, loss = 0.88 (8989.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:11.721010: step 6560, loss = 0.96 (9278.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:11.863250: step 6570, loss = 1.06 (8998.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:12.001014: step 6580, loss = 1.07 (9291.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:12.140476: step 6590, loss = 0.94 (9178.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.577\r\nI0611 12:32:12.422148 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.577\r\n2020-06-11 12:32:12.423309: step 6600, loss = 0.80 (4525.5 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:12.557194: step 6610, loss = 0.81 (9560.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:12.691691: step 6620, loss = 0.98 (9517.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:12.826784: step 6630, loss = 1.09 (9474.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:12.960870: step 6640, loss = 0.93 (9546.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:13.096604: step 6650, loss = 0.98 (9430.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:13.232154: step 6660, loss = 0.93 (9443.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:13.367931: step 6670, loss = 1.07 (9427.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:13.502333: step 6680, loss = 0.97 (9523.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:13.639212: step 6690, loss = 1.10 (9351.3 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.646\r\nI0611 12:32:13.900447 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.646\r\n2020-06-11 12:32:13.901518: step 6700, loss = 0.84 (4879.7 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:14.035394: step 6710, loss = 1.04 (9561.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:14.166674: step 6720, loss = 0.80 (9750.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:14.302234: step 6730, loss = 1.17 (9442.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:14.439948: step 6740, loss = 0.87 (9294.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:14.574041: step 6750, loss = 1.02 (9545.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:14.710217: step 6760, loss = 0.89 (9399.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:14.848501: step 6770, loss = 1.02 (9256.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:14.984093: step 6780, loss = 1.06 (9440.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:15.117600: step 6790, loss = 1.12 (9587.5 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.6073\r\nI0611 12:32:15.379590 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.6073\r\n2020-06-11 12:32:15.380659: step 6800, loss = 0.91 (4865.7 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:15.519400: step 6810, loss = 1.00 (9226.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:15.655929: step 6820, loss = 1.11 (9375.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:15.795637: step 6830, loss = 1.01 (9161.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:15.938765: step 6840, loss = 0.82 (8943.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:16.074062: step 6850, loss = 0.83 (9460.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:16.212569: step 6860, loss = 0.92 (9241.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:16.353563: step 6870, loss = 0.97 (9078.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:16.495627: step 6880, loss = 0.91 (9010.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:16.636923: step 6890, loss = 0.93 (9058.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4978\r\nI0611 12:32:16.906322 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.4978\r\n2020-06-11 12:32:16.907501: step 6900, loss = 0.88 (4730.5 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:17.040139: step 6910, loss = 1.00 (9650.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:17.175477: step 6920, loss = 1.21 (9457.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:17.313300: step 6930, loss = 0.93 (9287.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:17.450280: step 6940, loss = 0.92 (9344.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:17.589582: step 6950, loss = 0.99 (9188.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:17.726582: step 6960, loss = 0.80 (9343.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:17.860602: step 6970, loss = 0.96 (9550.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:17.997622: step 6980, loss = 0.98 (9341.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:18.133833: step 6990, loss = 0.98 (9397.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.9773\r\nI0611 12:32:18.399393 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.9773\r\n2020-06-11 12:32:18.400493: step 7000, loss = 1.11 (4800.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:18.535643: step 7010, loss = 0.98 (9471.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:18.674494: step 7020, loss = 0.83 (9218.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:18.813658: step 7030, loss = 1.10 (9197.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:18.957656: step 7040, loss = 0.97 (8889.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:19.094175: step 7050, loss = 0.82 (9376.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:19.234282: step 7060, loss = 1.10 (9135.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:19.372961: step 7070, loss = 1.01 (9230.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:19.514297: step 7080, loss = 0.74 (9056.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:19.655434: step 7090, loss = 0.87 (9069.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4642\r\nI0611 12:32:19.926943 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.4642\r\n2020-06-11 12:32:19.928062: step 7100, loss = 0.92 (4695.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:20.063338: step 7110, loss = 0.88 (9462.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:20.199214: step 7120, loss = 1.08 (9420.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:20.332988: step 7130, loss = 0.83 (9568.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:20.469602: step 7140, loss = 1.01 (9369.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:20.606280: step 7150, loss = 0.87 (9365.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:20.742028: step 7160, loss = 1.06 (9429.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:20.875689: step 7170, loss = 1.04 (9576.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:21.008059: step 7180, loss = 0.95 (9669.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:21.142446: step 7190, loss = 0.91 (9524.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.2371\r\nI0611 12:32:21.414221 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.2371\r\n2020-06-11 12:32:21.415356: step 7200, loss = 1.00 (4690.1 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:21.548764: step 7210, loss = 0.97 (9594.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:21.686636: step 7220, loss = 1.07 (9284.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:21.828015: step 7230, loss = 0.93 (9053.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:21.970563: step 7240, loss = 0.95 (8979.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:22.109585: step 7250, loss = 1.09 (9207.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:22.248317: step 7260, loss = 0.77 (9226.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:22.388745: step 7270, loss = 0.89 (9115.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:22.530782: step 7280, loss = 0.97 (9011.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:22.670279: step 7290, loss = 1.00 (9175.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.5936\r\nI0611 12:32:22.962361 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 64.5936\r\n2020-06-11 12:32:22.963463: step 7300, loss = 0.81 (4365.8 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:32:23.096809: step 7310, loss = 0.85 (9599.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:23.229037: step 7320, loss = 0.93 (9680.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:23.362447: step 7330, loss = 1.00 (9594.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:23.498528: step 7340, loss = 0.90 (9406.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:23.630748: step 7350, loss = 0.68 (9680.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:23.763664: step 7360, loss = 0.99 (9630.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:23.896904: step 7370, loss = 0.84 (9606.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:24.033037: step 7380, loss = 0.90 (9402.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:24.170249: step 7390, loss = 0.94 (9328.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4296\r\nI0611 12:32:24.445374 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4296\r\n2020-06-11 12:32:24.446437: step 7400, loss = 1.04 (4634.4 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:24.578564: step 7410, loss = 0.87 (9687.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:24.714645: step 7420, loss = 1.07 (9406.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:24.850777: step 7430, loss = 0.84 (9402.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:24.985483: step 7440, loss = 0.98 (9502.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:25.123397: step 7450, loss = 0.88 (9281.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:25.256159: step 7460, loss = 0.83 (9641.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:25.397328: step 7470, loss = 1.00 (9067.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:25.532595: step 7480, loss = 1.07 (9462.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:25.671348: step 7490, loss = 0.86 (9225.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.0253\r\nI0611 12:32:25.937356 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.0253\r\n2020-06-11 12:32:25.938553: step 7500, loss = 0.86 (4790.3 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:26.072623: step 7510, loss = 0.95 (9547.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:26.208656: step 7520, loss = 0.88 (9409.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:26.346193: step 7530, loss = 0.91 (9306.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:26.483391: step 7540, loss = 0.76 (9329.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:26.622192: step 7550, loss = 1.06 (9221.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:26.757071: step 7560, loss = 0.84 (9490.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:26.891262: step 7570, loss = 0.77 (9538.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:27.026513: step 7580, loss = 0.93 (9463.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:27.163224: step 7590, loss = 0.86 (9362.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.0032\r\nI0611 12:32:27.429811 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.0032\r\n2020-06-11 12:32:27.430835: step 7600, loss = 0.97 (4783.0 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:27.563912: step 7610, loss = 0.75 (9618.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:27.700397: step 7620, loss = 1.07 (9378.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:27.836628: step 7630, loss = 1.07 (9395.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:27.971534: step 7640, loss = 0.87 (9488.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:28.108451: step 7650, loss = 1.10 (9348.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:28.246106: step 7660, loss = 0.93 (9298.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:28.382786: step 7670, loss = 0.81 (9364.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:28.518251: step 7680, loss = 0.92 (9449.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:28.654786: step 7690, loss = 1.10 (9374.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.3967\r\nI0611 12:32:28.913567 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.3967\r\n2020-06-11 12:32:28.914729: step 7700, loss = 0.89 (4924.1 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:29.046362: step 7710, loss = 1.00 (9724.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:29.180490: step 7720, loss = 0.89 (9543.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:29.320002: step 7730, loss = 0.97 (9174.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:29.459379: step 7740, loss = 0.94 (9183.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:29.595513: step 7750, loss = 1.07 (9402.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:29.737047: step 7760, loss = 0.98 (9043.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:29.870925: step 7770, loss = 0.86 (9561.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:30.009114: step 7780, loss = 0.98 (9262.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:30.149309: step 7790, loss = 0.86 (9130.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.183\r\nI0611 12:32:30.424540 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.183\r\n2020-06-11 12:32:30.425647: step 7800, loss = 0.84 (4631.9 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:30.558818: step 7810, loss = 0.80 (9611.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:30.693204: step 7820, loss = 1.05 (9524.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:30.828521: step 7830, loss = 0.90 (9459.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:30.963383: step 7840, loss = 0.76 (9491.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:31.097295: step 7850, loss = 0.91 (9558.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:31.233383: step 7860, loss = 1.04 (9405.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:31.368443: step 7870, loss = 0.85 (9477.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:31.504848: step 7880, loss = 1.02 (9383.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:31.641666: step 7890, loss = 0.86 (9355.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.2528\r\nI0611 12:32:31.911473 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.2528\r\n2020-06-11 12:32:31.912466: step 7900, loss = 1.10 (4726.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:32.048433: step 7910, loss = 0.80 (9414.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:32.183275: step 7920, loss = 0.99 (9492.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:32.319940: step 7930, loss = 0.86 (9365.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:32.457199: step 7940, loss = 0.83 (9325.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:32.593773: step 7950, loss = 0.89 (9372.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:32.729571: step 7960, loss = 0.83 (9425.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:32.863992: step 7970, loss = 0.90 (9522.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:32.998500: step 7980, loss = 0.93 (9516.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:33.135673: step 7990, loss = 0.91 (9331.2 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.4054\r\nI0611 12:32:33.395033 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.4054\r\n2020-06-11 12:32:33.396026: step 8000, loss = 1.08 (4916.3 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:33.528622: step 8010, loss = 1.02 (9653.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:33.664458: step 8020, loss = 1.00 (9423.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:33.800884: step 8030, loss = 0.84 (9382.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:33.937381: step 8040, loss = 0.89 (9377.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:34.073227: step 8050, loss = 0.84 (9422.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:34.207584: step 8060, loss = 0.98 (9526.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:34.342755: step 8070, loss = 0.96 (9469.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:34.479439: step 8080, loss = 0.99 (9364.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:34.613425: step 8090, loss = 0.95 (9553.2 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.776\r\nI0611 12:32:34.892561 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.776\r\n2020-06-11 12:32:34.893584: step 8100, loss = 0.94 (4568.8 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:35.028369: step 8110, loss = 0.88 (9496.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:35.161825: step 8120, loss = 0.97 (9591.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:35.294237: step 8130, loss = 0.77 (9666.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:35.430291: step 8140, loss = 0.95 (9408.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:35.565220: step 8150, loss = 1.00 (9486.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:35.702484: step 8160, loss = 0.92 (9325.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:35.836298: step 8170, loss = 0.88 (9565.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:35.974933: step 8180, loss = 0.99 (9232.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:36.109761: step 8190, loss = 0.98 (9493.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.5041\r\nI0611 12:32:36.396227 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.5041\r\n2020-06-11 12:32:36.397309: step 8200, loss = 0.82 (4451.4 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:32:36.530455: step 8210, loss = 0.82 (9613.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:36.666298: step 8220, loss = 0.88 (9422.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:36.803102: step 8230, loss = 0.84 (9357.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:36.940064: step 8240, loss = 0.94 (9345.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:37.074545: step 8250, loss = 0.92 (9518.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:37.210526: step 8260, loss = 0.89 (9413.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:37.346379: step 8270, loss = 0.83 (9421.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:37.482590: step 8280, loss = 0.77 (9397.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:37.614651: step 8290, loss = 0.84 (9692.5 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.5345\r\nI0611 12:32:37.876987 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.5345\r\n2020-06-11 12:32:37.878137: step 8300, loss = 0.87 (4857.9 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:38.013688: step 8310, loss = 0.94 (9443.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:38.146507: step 8320, loss = 0.91 (9637.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:38.280329: step 8330, loss = 0.91 (9564.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:38.414040: step 8340, loss = 0.93 (9572.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:38.549772: step 8350, loss = 0.91 (9430.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:38.687135: step 8360, loss = 0.80 (9318.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:38.821362: step 8370, loss = 0.80 (9536.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:38.958016: step 8380, loss = 1.03 (9366.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:39.091711: step 8390, loss = 0.78 (9574.1 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.737\r\nI0611 12:32:39.353262 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.737\r\n2020-06-11 12:32:39.354267: step 8400, loss = 0.85 (4875.1 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:39.485305: step 8410, loss = 0.85 (9768.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:39.622022: step 8420, loss = 1.08 (9362.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:39.765281: step 8430, loss = 0.97 (8934.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:39.906556: step 8440, loss = 0.96 (9060.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:40.048121: step 8450, loss = 0.92 (9041.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:40.188182: step 8460, loss = 0.91 (9138.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:40.330534: step 8470, loss = 0.98 (8991.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:40.470275: step 8480, loss = 0.90 (9159.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:40.611135: step 8490, loss = 0.89 (9087.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4185\r\nI0611 12:32:40.881870 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.4185\r\n2020-06-11 12:32:40.882917: step 8500, loss = 0.96 (4709.6 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:41.015781: step 8510, loss = 0.94 (9634.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:41.147687: step 8520, loss = 0.88 (9704.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:41.286417: step 8530, loss = 1.01 (9226.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:41.421195: step 8540, loss = 1.01 (9497.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:41.558289: step 8550, loss = 0.87 (9336.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:41.692324: step 8560, loss = 0.98 (9549.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:41.830506: step 8570, loss = 0.95 (9263.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:41.967433: step 8580, loss = 0.86 (9348.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:42.102011: step 8590, loss = 0.90 (9511.2 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.0473\r\nI0611 12:32:42.373353 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.0473\r\n2020-06-11 12:32:42.374493: step 8600, loss = 0.84 (4697.5 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:42.512270: step 8610, loss = 1.00 (9290.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:42.648610: step 8620, loss = 0.89 (9388.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:42.785211: step 8630, loss = 0.92 (9370.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:42.919291: step 8640, loss = 0.96 (9546.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:43.054140: step 8650, loss = 0.78 (9492.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:43.190013: step 8660, loss = 0.99 (9420.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:43.325887: step 8670, loss = 0.83 (9420.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:43.458580: step 8680, loss = 0.88 (9646.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:43.595949: step 8690, loss = 0.85 (9318.0 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.3785\r\nI0611 12:32:43.857521 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.3785\r\n2020-06-11 12:32:43.858737: step 8700, loss = 0.96 (4870.8 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:43.990919: step 8710, loss = 0.89 (9683.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:44.125216: step 8720, loss = 1.02 (9531.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:44.262338: step 8730, loss = 0.73 (9334.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:44.397037: step 8740, loss = 0.93 (9502.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:44.530819: step 8750, loss = 0.75 (9567.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:44.668498: step 8760, loss = 0.96 (9297.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:44.806125: step 8770, loss = 0.94 (9300.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:44.943547: step 8780, loss = 1.01 (9314.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:45.077878: step 8790, loss = 0.91 (9528.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.2936\r\nI0611 12:32:45.343522 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.2936\r\n2020-06-11 12:32:45.344548: step 8800, loss = 0.82 (4799.9 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:45.478329: step 8810, loss = 1.01 (9568.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:45.613810: step 8820, loss = 0.89 (9447.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:45.746826: step 8830, loss = 0.83 (9622.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:45.881836: step 8840, loss = 0.86 (9480.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:46.018019: step 8850, loss = 0.87 (9399.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:46.156458: step 8860, loss = 0.87 (9245.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:46.293076: step 8870, loss = 0.95 (9369.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:46.426567: step 8880, loss = 1.05 (9588.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:46.561540: step 8890, loss = 0.77 (9483.4 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1382\r\nI0611 12:32:46.833002 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1382\r\n2020-06-11 12:32:46.834184: step 8900, loss = 0.93 (4694.7 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:46.966758: step 8910, loss = 0.94 (9656.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:47.104363: step 8920, loss = 0.78 (9301.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:47.239760: step 8930, loss = 1.01 (9453.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:47.377635: step 8940, loss = 0.83 (9283.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:47.514138: step 8950, loss = 0.84 (9377.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:47.647374: step 8960, loss = 0.79 (9606.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:47.785661: step 8970, loss = 0.87 (9256.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:47.921322: step 8980, loss = 1.00 (9435.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:48.056504: step 8990, loss = 0.82 (9468.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.1223\r\nI0611 12:32:48.322828 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.1223\r\n2020-06-11 12:32:48.323916: step 9000, loss = 0.95 (4786.6 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:48.456880: step 9010, loss = 0.92 (9626.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:48.594123: step 9020, loss = 0.90 (9326.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:48.730054: step 9030, loss = 0.91 (9416.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:48.865088: step 9040, loss = 0.87 (9479.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:49.001564: step 9050, loss = 0.84 (9379.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:49.137381: step 9060, loss = 1.05 (9424.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:49.271163: step 9070, loss = 0.91 (9567.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:49.405304: step 9080, loss = 0.79 (9542.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:49.539894: step 9090, loss = 0.95 (9510.3 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.6453\r\nI0611 12:32:49.801126 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.6453\r\n2020-06-11 12:32:49.802314: step 9100, loss = 0.70 (4877.6 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:49.934920: step 9110, loss = 0.82 (9652.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:50.067174: step 9120, loss = 0.89 (9678.5 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:50.205688: step 9130, loss = 0.83 (9241.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:50.340890: step 9140, loss = 1.03 (9467.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:50.480300: step 9150, loss = 0.95 (9181.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:50.617209: step 9160, loss = 0.91 (9349.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:50.754082: step 9170, loss = 0.84 (9351.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:50.888892: step 9180, loss = 0.92 (9494.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:51.025974: step 9190, loss = 0.86 (9337.5 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.1855\r\nI0611 12:32:51.312022 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.1855\r\n2020-06-11 12:32:51.313220: step 9200, loss = 0.84 (4456.1 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:32:51.444666: step 9210, loss = 0.86 (9737.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:51.578105: step 9220, loss = 0.86 (9592.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:51.712563: step 9230, loss = 0.94 (9519.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:51.846215: step 9240, loss = 0.99 (9577.1 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:51.981971: step 9250, loss = 0.98 (9428.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:52.118675: step 9260, loss = 0.86 (9363.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:52.256658: step 9270, loss = 0.85 (9276.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:52.392007: step 9280, loss = 0.85 (9457.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:52.528040: step 9290, loss = 0.76 (9409.5 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.7815\r\nI0611 12:32:52.787353 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.7815\r\n2020-06-11 12:32:52.788632: step 9300, loss = 0.80 (4911.8 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:52.922802: step 9310, loss = 0.94 (9540.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:53.054278: step 9320, loss = 0.95 (9735.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:53.193636: step 9330, loss = 0.93 (9185.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:53.327266: step 9340, loss = 0.76 (9578.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:53.466127: step 9350, loss = 0.97 (9217.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:53.602137: step 9360, loss = 0.82 (9411.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:53.738415: step 9370, loss = 0.84 (9392.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:53.872003: step 9380, loss = 0.89 (9581.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:54.007753: step 9390, loss = 0.90 (9429.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.3553\r\nI0611 12:32:54.272015 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.3553\r\n2020-06-11 12:32:54.273166: step 9400, loss = 0.75 (4822.6 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:32:54.405253: step 9410, loss = 0.87 (9690.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:54.540599: step 9420, loss = 0.79 (9457.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:54.675239: step 9430, loss = 0.82 (9506.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:54.811505: step 9440, loss = 0.90 (9393.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:54.949173: step 9450, loss = 0.65 (9297.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:55.084793: step 9460, loss = 0.92 (9438.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:55.217772: step 9470, loss = 0.86 (9625.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:55.350954: step 9480, loss = 0.85 (9610.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:55.487317: step 9490, loss = 1.04 (9386.8 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.8029\r\nI0611 12:32:55.746880 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.8029\r\n2020-06-11 12:32:55.747962: step 9500, loss = 0.96 (4910.8 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:32:55.880683: step 9510, loss = 0.93 (9644.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:56.017188: step 9520, loss = 0.77 (9377.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.155243: step 9530, loss = 0.89 (9271.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.292633: step 9540, loss = 0.97 (9316.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.431777: step 9550, loss = 0.85 (9199.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.574196: step 9560, loss = 0.83 (8987.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.712140: step 9570, loss = 0.77 (9279.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.852710: step 9580, loss = 0.91 (9105.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:56.994911: step 9590, loss = 0.93 (9001.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.3593\r\nI0611 12:32:57.276890 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.3593\r\n2020-06-11 12:32:57.278092: step 9600, loss = 0.94 (4520.0 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:57.412530: step 9610, loss = 1.08 (9521.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:32:57.549596: step 9620, loss = 0.94 (9338.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:57.685831: step 9630, loss = 0.98 (9395.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:57.822095: step 9640, loss = 0.86 (9393.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:57.960709: step 9650, loss = 0.95 (9234.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:58.099222: step 9660, loss = 0.87 (9241.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:58.237524: step 9670, loss = 0.68 (9255.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:58.373260: step 9680, loss = 0.91 (9430.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:58.507973: step 9690, loss = 0.87 (9501.8 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.1694\r\nI0611 12:32:58.788166 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.1694\r\n2020-06-11 12:32:58.789331: step 9700, loss = 0.81 (4549.3 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:32:58.925858: step 9710, loss = 1.03 (9375.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.064389: step 9720, loss = 0.86 (9240.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.205259: step 9730, loss = 0.67 (9086.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.343261: step 9740, loss = 0.79 (9275.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.480184: step 9750, loss = 0.86 (9348.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.620473: step 9760, loss = 1.01 (9124.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.759542: step 9770, loss = 0.83 (9204.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:32:59.901047: step 9780, loss = 0.71 (9045.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:00.035642: step 9790, loss = 0.69 (9510.0 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.1605\r\nI0611 12:33:00.299664 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.1605\r\n2020-06-11 12:33:00.300746: step 9800, loss = 0.85 (4828.2 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:33:00.433158: step 9810, loss = 1.01 (9666.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:00.567914: step 9820, loss = 1.11 (9498.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:00.702608: step 9830, loss = 0.85 (9502.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:00.836505: step 9840, loss = 0.81 (9559.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:00.971190: step 9850, loss = 0.74 (9503.6 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:01.106094: step 9860, loss = 0.98 (9488.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:01.240316: step 9870, loss = 0.86 (9536.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:01.377876: step 9880, loss = 0.85 (9304.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:01.511871: step 9890, loss = 0.79 (9552.7 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 67.8967\r\nI0611 12:33:01.772441 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 67.8967\r\n2020-06-11 12:33:01.773529: step 9900, loss = 1.00 (4891.8 examples/sec; 0.026 sec/batch)\r\n2020-06-11 12:33:01.906502: step 9910, loss = 0.90 (9626.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:02.042271: step 9920, loss = 0.84 (9428.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:02.185280: step 9930, loss = 0.93 (8950.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:02.322566: step 9940, loss = 0.95 (9323.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:02.460631: step 9950, loss = 0.84 (9271.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:02.600381: step 9960, loss = 0.86 (9159.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:02.738845: step 9970, loss = 0.96 (9244.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:02.875437: step 9980, loss = 0.87 (9370.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:03.016253: step 9990, loss = 0.83 (9089.9 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.128\r\nI0611 12:33:03.284673 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.128\r\n2020-06-11 12:33:03.285621: step 10000, loss = 0.88 (4751.8 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:33:03.420649: step 10010, loss = 0.88 (9479.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:03.558887: step 10020, loss = 0.73 (9259.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:03.702539: step 10030, loss = 0.88 (8910.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:03.843944: step 10040, loss = 0.96 (9051.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:03.984277: step 10050, loss = 0.93 (9121.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:04.123361: step 10060, loss = 0.78 (9203.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:04.265064: step 10070, loss = 0.71 (9032.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:04.407668: step 10080, loss = 0.86 (8975.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:04.548846: step 10090, loss = 0.79 (9066.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.5339\r\nI0611 12:33:04.834228 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 64.5339\r\n2020-06-11 12:33:04.835257: step 10100, loss = 0.98 (4469.0 examples/sec; 0.029 sec/batch)\r\n2020-06-11 12:33:04.967911: step 10110, loss = 0.95 (9649.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:05.105820: step 10120, loss = 0.89 (9281.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:05.242376: step 10130, loss = 1.12 (9373.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:05.379732: step 10140, loss = 0.84 (9318.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:05.514042: step 10150, loss = 0.83 (9530.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:05.651629: step 10160, loss = 0.98 (9303.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:05.787682: step 10170, loss = 1.00 (9408.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:05.925552: step 10180, loss = 0.81 (9284.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:06.058564: step 10190, loss = 1.02 (9623.3 examples/sec; 0.013 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.9342\r\nI0611 12:33:06.328234 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.9342\r\n2020-06-11 12:33:06.329317: step 10200, loss = 0.98 (4727.5 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:33:06.463926: step 10210, loss = 0.84 (9509.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:06.605525: step 10220, loss = 0.84 (9039.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:06.744207: step 10230, loss = 0.85 (9229.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:06.885514: step 10240, loss = 0.82 (9058.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:07.027519: step 10250, loss = 0.95 (9013.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:07.169738: step 10260, loss = 0.70 (9000.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:07.308332: step 10270, loss = 1.02 (9235.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:07.448706: step 10280, loss = 0.91 (9118.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:07.590722: step 10290, loss = 1.11 (9013.1 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.1583\r\nI0611 12:33:07.862955 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.1583\r\n2020-06-11 12:33:07.863903: step 10300, loss = 0.80 (4685.4 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:33:07.996524: step 10310, loss = 1.07 (9651.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:08.133841: step 10320, loss = 0.98 (9321.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:08.274430: step 10330, loss = 0.96 (9104.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:08.415176: step 10340, loss = 1.00 (9094.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:08.557414: step 10350, loss = 1.04 (8999.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:08.694627: step 10360, loss = 0.82 (9328.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:08.835599: step 10370, loss = 0.91 (9079.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:08.975382: step 10380, loss = 0.90 (9157.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:09.111925: step 10390, loss = 1.05 (9374.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.8389\r\nI0611 12:33:09.381858 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.8389\r\n2020-06-11 12:33:09.382856: step 10400, loss = 0.87 (4724.4 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:33:09.519794: step 10410, loss = 0.96 (9347.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:09.656033: step 10420, loss = 0.97 (9395.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:09.794166: step 10430, loss = 0.94 (9266.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:09.932321: step 10440, loss = 0.71 (9264.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:10.071225: step 10450, loss = 0.77 (9215.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:10.215764: step 10460, loss = 0.76 (8855.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:10.354399: step 10470, loss = 0.74 (9232.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:10.496824: step 10480, loss = 0.78 (8987.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:10.636203: step 10490, loss = 0.77 (9183.6 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.4742\r\nI0611 12:33:10.909162 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.4742\r\n2020-06-11 12:33:10.910141: step 10500, loss = 1.04 (4672.5 examples/sec; 0.027 sec/batch)\r\n2020-06-11 12:33:11.043764: step 10510, loss = 0.86 (9579.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:11.179018: step 10520, loss = 0.83 (9464.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:11.319394: step 10530, loss = 0.87 (9118.3 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:11.458524: step 10540, loss = 0.93 (9200.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:11.596922: step 10550, loss = 0.83 (9248.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:11.739194: step 10560, loss = 0.98 (8996.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:11.876821: step 10570, loss = 0.84 (9300.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:12.014564: step 10580, loss = 0.68 (9292.7 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:12.155837: step 10590, loss = 0.79 (9060.5 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 64.8342\r\nI0611 12:33:12.451553 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 64.8342\r\n2020-06-11 12:33:12.452720: step 10600, loss = 1.05 (4311.4 examples/sec; 0.030 sec/batch)\r\n2020-06-11 12:33:12.587702: step 10610, loss = 1.02 (9482.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:12.729799: step 10620, loss = 0.91 (9008.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:12.873395: step 10630, loss = 0.90 (8913.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:13.016006: step 10640, loss = 0.77 (8975.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:13.153510: step 10650, loss = 0.85 (9308.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:13.291965: step 10660, loss = 0.81 (9244.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:13.432935: step 10670, loss = 0.85 (9079.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:13.573641: step 10680, loss = 1.12 (9096.9 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:13.711552: step 10690, loss = 0.82 (9281.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 65.0948\r\nI0611 12:33:13.987750 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 65.0948\r\n2020-06-11 12:33:13.988806: step 10700, loss = 1.08 (4616.6 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:33:14.119436: step 10710, loss = 0.87 (9798.8 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:14.253961: step 10720, loss = 0.90 (9515.0 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:14.391637: step 10730, loss = 0.79 (9297.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:14.528603: step 10740, loss = 0.90 (9345.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:14.663061: step 10750, loss = 0.88 (9519.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:14.798156: step 10760, loss = 0.84 (9474.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:14.935410: step 10770, loss = 0.80 (9325.8 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:15.071277: step 10780, loss = 0.80 (9421.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:15.206975: step 10790, loss = 0.84 (9432.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.6286\r\nI0611 12:33:15.488617 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.6286\r\n2020-06-11 12:33:15.489821: step 10800, loss = 0.81 (4525.4 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:33:15.623977: step 10810, loss = 0.88 (9541.3 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:15.756893: step 10820, loss = 0.90 (9630.2 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:15.891787: step 10830, loss = 0.85 (9488.9 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:16.028274: step 10840, loss = 0.85 (9378.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:16.169358: step 10850, loss = 0.81 (9072.6 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:16.307073: step 10860, loss = 0.78 (9294.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:16.443694: step 10870, loss = 0.89 (9369.0 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:16.577786: step 10880, loss = 0.87 (9545.7 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:16.713758: step 10890, loss = 0.77 (9413.7 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:global_step/sec: 66.292\r\nI0611 12:33:16.997109 139727290783488 basic_session_run_hooks.py:692] global_step/sec: 66.292\r\n2020-06-11 12:33:16.998194: step 10900, loss = 0.90 (4500.1 examples/sec; 0.028 sec/batch)\r\n2020-06-11 12:33:17.132943: step 10910, loss = 0.90 (9499.4 examples/sec; 0.013 sec/batch)\r\n2020-06-11 12:33:17.271647: step 10920, loss = 0.78 (9228.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:17.412475: step 10930, loss = 0.81 (9089.2 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:17.553514: step 10940, loss = 0.98 (9075.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:17.691669: step 10950, loss = 0.93 (9265.1 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:17.831797: step 10960, loss = 0.98 (9134.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:17.973054: step 10970, loss = 0.81 (9061.4 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:18.110919: step 10980, loss = 0.86 (9284.5 examples/sec; 0.014 sec/batch)\r\n2020-06-11 12:33:18.248592: step 10990, loss = 0.83 (9297.4 examples/sec; 0.014 sec/batch)\r\nINFO:tensorflow:Saving checkpoints for 11000 into /tmp/cifar10_train/model.ckpt.\r\nI0611 12:33:18.374850 139727290783488 basic_session_run_hooks.py:606] Saving checkpoints for 11000 into /tmp/cifar10_train/model.ckpt.\r\n\r\nreal\t2m50.690s\r\nuser\t9m45.761s\r\nsys\t1m10.234s\r\n \r\n```", "Could it be something environmental, like perhaps the GPU is getting boosted during the initial steps?  Can you try [disabling GPU boost](https://developer.nvidia.com/blog/increase-performance-gpu-boost-k80-autoboost/) to see if that reduces the variance?", "I figured it out. Turned out the GPU got overheated and slows down over time. Got a cooling pad and now no degradation over 100,000 steps. Thanks for everyone who tried to help and apologies for the false alarm.", "@beew Thanks for your information. \r\nAfter checking on my testing environment, I believe the same root cause triggers my platform's performance issue also. I've used the following cmd to monitor my GPU's temperature:\r\n`nvidia-smi --query-gpu=temperature.gpu --format=csv\r\n`\r\nDuring my training process, when the GPU's temperature is above 90 degree, the training performance start to decrease largely. \r\nAnd I've used many other nvidia's utilties to fix the performance in my laptop ubuntu environment, but all failed, e.g. the following commands:\r\nSet one frequencies setting for GPU&Memory:\r\n`sudo nvidia-smi -ac 2505,1202 -i 0`\r\nFAILED! Performance still going down when temperature is above 90 degree\r\nEnable persistency mode:\r\n`nvidia-smi -pm 1`\r\nStill FAILED, the performance still not persistant\r\nAnyway, it's a good HW design to prevent the GPU burning, and what I need do is just switching to another better HW platform when scaling-up my project data process. This issue took me above one month time to locate the root cause, it's finally clear, Thanks again!!!!\r\n", "You also could use the following commands to monitor GPU's working frequencies:\r\n## for GPU working frequency \r\n`watch -n 0.5 nvidia-smi --query-gpu=clocks.gr  --format=csv    `\r\n## For GPU's memory frequncy\r\n`watch -n 0.5 nvidia-smi --query-gpu=clocks.mem  --format=csv`\r\n\r\nThe following command could force your GPU working at 2.505Ghz for GPU and 1.006Ghz for GPU memory:\r\n`nvidia-smi --applications-clocks=2505,1006 -i 0`\r\nThe frequencies options could be listed by command:\r\n`nvidia-smi  -q -i 0 -d SUPPORTED_CLOCKS`\r\n\r\nIf your GPU is not toooooo hot, it could work at the frequencies as you request with command:\"--applications-clocks\", but if too hot, it'll decrease the two working frequencies. So cooling down your GPU is also an important part on your working platform."]}, {"number": 40041, "title": "Customizing what happens in fit()", "body": "I read the guide document of keras, and run codes of 'Customizing what happens in fit() -> Going lower-level', just copy code on the guide and run it, but I get this error message:\r\n\r\n\"ValueError: The model cannot be compiled because it has no loss to optimize.\"\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): manjaro\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0 cpu\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nI run code copied from here \"https://keras.io/guides/customizing_what_happens_in_fit/#going-lowerlevel\"\r\n\r\n\r\n**Describe the expected behavior**\r\nAccording to the doc, I can just skip passing loss function when I call model.compile, but it does not work, It seems like when I call model.fit, the overrided method \"train_step\" is not called.\r\n", "comments": ["@labusi \r\nPlease share simple stand alone code to replicate the issue faced or if possible share a colab gist to analyse the error faced.", "@Saduf2019 \r\n'https://github.com/labusi/tfbugs/blob/master/01_train_step.ipynb'\r\nHere is the stand alone code that replicates the issue. By the way both tf2.0.0 and tf2.1.0 have this issue, but it works well in tf2.2.0", "I am able to replicate this issue , please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/8b95f953d32dd8c74c13406460210eb6/untitled214.ipynb)", "@Saduf2019 Ok, thank you, hope that this issue will be fixed soon", "This [guide](https://github.com/keras-team/keras-io/blob/master/guides/customizing_what_happens_in_fit.py) was published in TF 2.2 dev cycle.\r\nTherefore it works with TF 2.2 onwards. TF 2.1 was released in early January which is much older than recently released TF 2.2", "Yes, I konw this, and I am using TF2.2 now, but it's still an issue for people who have not upgrade to TF 2.2, right?\r\nMaybe you can let other guys konw this when they read that guide. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40041\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40041\">No</a>\n", "So what is the solution for those of us forced to backport code from 2.2+ to 2.1 ?", "The same error still occurs in TF 2.4 when one turns off eager mode using `tf.compat.v1.disable_eager_execution()`. The error being:\r\n```\r\nValueError: The model cannot be compiled because it has no loss to optimize.\r\n```\r\n\r\nThis can be fixed by passing the loss function to `compile` and then using `self.compiled_loss` to compute the loss, as done in the \"A first simple example\" part of this tutorial.\r\n\r\nSee this [notebook](https://colab.research.google.com/gist/bwolfson97/72ffa724d990ec82b0c9f00272cd7f33/untitled214.ipynb) for an example.", "> The same error still occurs in TF 2.4 when one turns off eager mode using `tf.compat.v1.disable_eager_execution()`. The error being:\r\n> \r\n> ```\r\n> ValueError: The model cannot be compiled because it has no loss to optimize.\r\n> ```\r\n> \r\n> This can be fixed by passing the loss function to `compile` and then using `self.compiled_loss` to compute the loss, as done in the \"A first simple example\" part of this tutorial.\r\n> \r\n> See this [notebook](https://colab.research.google.com/gist/bwolfson97/72ffa724d990ec82b0c9f00272cd7f33/untitled214.ipynb) for an example.\r\n\r\nAre there any workarounds for 2.4 with eager mode off for custom losses like in the keras VAE example?\r\n\r\nhttps://keras.io/examples/generative/vae/"]}, {"number": 40040, "title": "Fix package name", "body": "", "comments": []}, {"number": 40039, "title": "Re-enable __builtin_expect when building with NVCC", "body": "NVCC supports this construct as of CUDA 9.0 (older versions will not work; let me know if this is a problem).\r\n\r\ncc @reedwm @nluehr ", "comments": []}, {"number": 40038, "title": "Warnings in tf.autodiff.ForwardAccumulator", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GTX 2080TI/11GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI run the first example in [doc for ForwardAccumulator](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator). It gives the following warning message:\r\n\r\n```\r\nWARNING:tensorflow:5 out of the last 5 calls to <function _jvp_helper at 0x00000152FA90A288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\nWARNING:tensorflow:6 out of the last 6 calls to <function _jvp_helper at 0x00000152FA90A288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warning.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.constant([[2.0, 3.0], [1.0, 4.0]])\r\ndense = tf.keras.layers.Dense(1)\r\ndense.build([None, 2])\r\nwith tf.autodiff.ForwardAccumulator(\r\n   primals=dense.kernel,\r\n   tangents=tf.constant([[1.], [0.]])) as acc:\r\n  loss = tf.reduce_sum((dense(x) - tf.constant([1., -1.])) ** 2.)\r\nacc.jvp(loss)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@YinTat \r\nI have tried in colab with TF nightly version(`2.3.0-dev20200531`) and i am not seeing any issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/2e0550ac41f691ddc5371a467d9edf6f/untitled941.ipynb).You could use tf-nightly for now and in the next couple of months new stable version will be released. Please, verify once and close the issue. Thanks!", "It is indeed fixed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40038\">No</a>\n"]}, {"number": 40037, "title": "How am I supposed to debug an error with this complexity? - Shapes mismatch YoloV4 - tensorflow 2.2", "body": "I recently added a new feature to my [yolo implementation in tensorflow 2.2](https://github.com/emadboctorx/yolov3-keras-tf2) is models are currently loaded directly from [DarkNet](https://github.com/AlexeyAB/darknet) cfg files for convenience, I tested the code with yolov3 configuration as well as yolov4 configuration they both work just fine except for v4 training. Shortly after I start training I get a shapes mismatch error and I'll be very grateful if someone can help me get rid of the error and get to finally complete my project. Please let me know in the comments and I will provide you with any resources you need to help me with fixing the problem and thank you in advance...\r\n\r\nThis is what you would want to do in order to reproduce:\r\n\r\n* clone the repo above\r\n* download the files necessary for the code to run below\r\n* run the following in train.py\r\n\r\n```\r\nif __name__ == '__main__':\r\n    tr = Trainer((608, 608, 3),\r\n                 '../Config/yolo4.cfg',\r\n                 '../Config/beverly_hills.txt',\r\n                 1344, 756, score_threshold=0.1,\r\n                 train_tf_record='../Data/TFRecords/beverly_hills_train.tfrecord',\r\n                 valid_tf_record='../Data/TFRecords/beverly_hills_test.tfrecord')\r\n\r\n    tr.train(\r\n        100,\r\n        8,\r\n        1e-3,\r\n        dataset_name='beverly_hills',\r\n        merge_evaluation=False,\r\n        n_epoch_eval=10,\r\n        clear_outputs=True\r\n    )\r\n```\r\n\r\nlinks to files you need:\r\n\r\n[bh_labels.csv](https://drive.google.com/file/d/13WHpo6qBB4LsFvr4jydDcxEmyQYROEI8/view?usp=sharing)\r\n[beverly_hills.txt](https://drive.google.com/file/d/1-WWwrDa-QoqK0GqjANMqX3tcwsuVH7Bd/view?usp=sharing)\r\n[beverly_hills_train.tfrecord](https://drive.google.com/file/d/13uJWnOqMdbvwWv6EttLN8U6G8F6eL7bB/view?usp=sharing)\r\n[beverly_hills_test.tfrecord](https://drive.google.com/file/d/13nesAJRryuzE09i0FxBQsmixM51QRjIL/view?usp=sharing)\r\n\r\nThe error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"trainer.py\", line 629, in <module>\r\n    clear_outputs=True\r\n  File \"../Helpers/utils.py\", line 62, in wrapper\r\n    result = func(*args, **kwargs)\r\n  File \"trainer.py\", line 490, in train\r\n    validation_data=valid_dataset,\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 1090, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 766, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 826, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2811, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1838, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1914, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 549, in call\r\n    ctx=ctx)\r\n  File \"/root/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [4,76,76,3,1] vs. [4,19,19,3,1]\r\n     [[node yolo_loss/logistic_loss/mul (defined at ../Helpers/utils.py:260) ]] [Op:__inference_train_function_38735]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node yolo_loss/logistic_loss/mul:\r\n yolo_loss/split_1 (defined at ../Helpers/utils.py:222) \r\n yolo_loss/split (defined at ../Helpers/utils.py:196)\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nAnd when I change the batch_size to 8 instead of 4, the error mutates into the following(the source of the error changes)\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/emadboctor/Desktop/Code/yolov3-keras-tf2/Main/trainer.py\", line 693, in <module>\r\n    clear_outputs=True,\r\n  File \"/Users/emadboctor/Desktop/Code/yolov3-keras-tf2/Helpers/utils.py\", line 62, in wrapper\r\n    result = func(*args, **kwargs)\r\n  File \"/Users/emadboctor/Desktop/Code/yolov3-keras-tf2/Main/trainer.py\", line 526, in train\r\n    validation_data=valid_dataset,\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\r\n    tmp_logs = train_function(iterator)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 598, in call\r\n    ctx=ctx)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [8,13,13,3,2] vs. [8,52,52,3,2]\r\n\t [[node gradient_tape/yolo_loss/sub_5/BroadcastGradientArgs (defined at Users/emadboctor/Desktop/Code/yolov3-keras-tf2/Main/trainer.py:526) ]] [Op:__inference_train_function_42744]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```", "comments": ["Your class number is different from yolov4 weight.", "@fsx950223 The problem has nothing to do with weights, i dont even load the weights to produce the error just run the code using the code snippet and the data i provided, you'll get the same error and try again using yolo3.cfg it will run perfectly fine and since there is no clear indication of which tensors that are incompatible, i can't trace the source of the error", "> @fsx950223 The problem has nothing to do with weights, i dont even load the weights to produce the error just run the code using the code snippet and the data i provided, you'll get the same error and try again using yolo3.cfg it will run perfectly fine and since there is no clear indication of which tensors that are incompatible, i can't trace the source of the error\r\n\r\nThe log shows ```node yolo_loss/logistic_loss/mul (defined at ../Helpers/utils.py:260)```\r\nCould you check it?", "@fsx950223 I ran the program in pycharm debugger, the inputs of the loss function are the same for yolo3 and yolo4 configuration or maybe i overlooked something however, the error happens in some tensorflow module that is named execute.py or a similar name. In other words if you wrap the whole loss function in a try and except blocks, the error will not be caught and will occur in the tensorflow module i indicated. I'm getting really frustrated, it's been almost a week and i can't get to fix the problem because of my inability to locate the source of the problem, i'll be very thankful if you could help me with it", "@emadboctorx,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code sample to reproduce the issue reported here. Thanks!", "@amahendrakar the minimum code sample is indicated above, clone my repo and run the code snippet above in trainer.py and use the links I provided as the code will not run without them"]}, {"number": 40036, "title": "Tensorflow 2.2 takes much more time than 2.1/2.0 to start training with \"keras.fit\"", "body": "**System information**\r\n- OS Platform and Distribution:Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):conda (I trided pip, same)\r\n- TensorFlow version:2.2\r\n- Python version:3.7\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:  Titan RTX * 2  = 48GB\r\n\r\nTensorflow 2.2 takes much more time than 2.1/2.0 to start training, after called \"keras.fit\".\r\n\r\n```\r\n2020-06-01 10:16:44.991459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-01 10:16:46.235945: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256\r\n2020-06-01 10:16:46.328871: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \r\nRelying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n2020-06-01 10:16:48.148004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-01 10:23:36.473814: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\n```\r\n\r\nIt stucks about 7 mins to start training.\r\n\r\nHowever, in 2.1\r\n\r\n```\r\nINFO:tensorflow:batch_all_reduce: 436 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nI0531 20:55:03.956965 139684401002304 cross_device_ops.py:760] batch_all_reduce: 436 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nINFO:tensorflow:batch_all_reduce: 436 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\nI0531 20:55:14.695299 139684401002304 cross_device_ops.py:760] batch_all_reduce: 436 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\r\n2020-05-31 20:55:39.932592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-31 20:55:41.811100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-31 20:55:48.718710: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\r\n```\r\n", "comments": ["I have same issue. https://github.com/tensorflow/tensorflow/issues/35346#issuecomment-636463855\r\n\r\nI think it cause by AutoGraph compiled slow.", "@edwardyehuang \r\nPlease provide with simple indented stand alone code for us to replicate the issue faced.", "Same issue here in TF 2.2.0. That makes each model.fit only start after 1 minitue waiting which not occurs in TF 2.1.0. I think the suspicious output is below message:\r\n\r\n_Relying on driver to perform ptx compilation. \r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once._\r\n\r\nWhich also not appear in TF 2.1.0 for the same code.", "[code for TF 2.2 ](https://colab.research.google.com/drive/1YDTrSobTvRCx2IJOWANVrKTMvkG7qnK6?usp=sharing)\r\nget_concrete_function Cost  155.99647617340088\r\nRun First Time Cost  115.89516520500183\r\nRun Second Time Cost  20.85046076774597\r\n\r\n[code for TF 2.1 ](https://colab.research.google.com/drive/1RJ_owqFyNyerfFfrHxy8u9thkwVzQCnK?usp=sharing)\r\nget_concrete_function Cost  136.64350962638855\r\nRun First Time Cost  127.72333002090454\r\nRun Second Time Cost  19.65573525428772\r\n\r\n@Saduf2019 Can you check my code ? My model may be a little complicated, but every time I start training, I have to wait for tf.function to compile for a long time, which is unbearable.\r\n\r\nI want to know is there any way to speed up the compilation of tf.function, or is it necessary for me to modify the code?", "I am able to replicate this issue, please find the gist for [tf2.2](https://colab.sandbox.google.com/gist/Saduf2019/76937519e979fc5a7794b9ca3f17709f/gou2_2.ipynb) and [tf 2.1](https://colab.sandbox.google.com/gist/Saduf2019/3e0ace846be0178c03c33311c477f176/untitled215.ipynb).", "@rohan100jain this is an example of a painfully slow tf.function tracing + compilation", "Hi for what it's worth I also get this with a much simpler model, although only really noticeably when using tf.distribute.MirroredStrategy where I'm seeing it take about 8 mins to spin up a VGG16 on 4 GPUS. I'm not aware of a way to attach multiple GPU's to a colab, and when I paste my script into colab with a GPU runtime the lag before training start is just fine, but here is is a self-contained [gist](https://gist.github.com/nathanin/c2fe9fd8a5af341997bed35ce9a96f5e) anyway. I can also say that running with the `strategy.experimental_run_v2` function in TF 2.1 mitigates the issue in my case.\r\n\r\nIn both cases I'm running Docker containers built on top of `rapidsai/rapidsai:cuda10.1-base-ubuntu18.04` where TensorFlow was installed via pip, and using 2080Ti's.", "Any update ? @Saduf2019 @ymodak @tomerk @rohan100jain  Thanks!!", "> Same issue here in TF 2.2.0. That makes each model.fit only start after 1 minitue waiting which not occurs in TF 2.1.0. I think the suspicious output is below message:\r\n> \r\n> _Relying on driver to perform ptx compilation. \r\n> Modify $PATH to customize ptxas location.\r\n\r\nI'd also suspect that. To me, this looks like (cu)bins for the different `sm_XX` levels (compute capabilities for the different NVIDIA GPUs) are not included in the `cmake` builds (anymore?), thus TensorFlow needs to contact the driver and compile stuff \"on the fly\", which takes forever.\r\n\r\nCan someone official confirm that?\r\n\r\n**EDIT** I can also reproduce this issue.\r\n\r\n", "Does anyone have a fix for this?", "@tkoch96 @edwardyehuang @nathanin @MakeCent \r\n\r\nI was having a look into the problem and definitely suspect kernels, cubins, and libraries missing to be the root of the problem.\r\n\r\nFor example, if you base your docker image on `rapidsai/rapidsai:cuda10.1-base-ubuntu18.04`, the `-base` will not include some cuda binaries, including `ptxas` for the \"just in time\" compilation of kernels and whatever else TensorFlow might need. If TensorFlow itself now does not include pre-compiled kernels for the requested operations / your GPU, then you run into the problem of having neither pre-compiled kernels to execute nor ptxas itself and the necessary cubins to let it compile itself during runtime.\r\nThis is why TensorFlow then delegates this responsibility to the driver. Takes very long and, at least for me, this fails always.\r\n\r\n**My current workaround**:\r\n- build a docker container based on `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04`\r\nThis (`-devel`) includes `ptxas` binary and all the other cuda bins we need to compile stuff\r\n- build TensorFlow 2.2 from source in this container\r\n- enable all necessary optimization and cuda flags\r\n- include all cuda compute capabilities you need for your GPUs\r\n\r\nThis works and then TensorFlow 2.2 GPU runs **smooth again**. Tested on a compute server with 4x TITAN RTX.\r\n\r\n---------------------\r\n\r\nYour custom TensorFlow docker images should look something like this:\r\n\r\nGet bazel 2.0.0 up and running\r\n```\r\nWORKDIR /build/tmp\r\nRUN wget https://github.com/bazelbuild/bazel/releases/download/2.0.0/bazel-2.0.0-installer-linux-x86_64.sh -O bazel.sh\r\nRUN chmod +x bazel.sh\r\n\r\nRUN ./bazel.sh --prefix=/tmp/bazel\r\n\r\nENV PATH=\"$PATH:/tmp/bazel/bin\"\r\n```\r\nThen you'll download TensorFlow and checkout to `r2.2`\r\n```\r\nWORKDIR /build-tf\r\nRUN git clone https://github.com/tensorflow/tensorflow.git tensorflow\r\nWORKDIR /build-tf/tensorflow\r\nRUN git checkout r2.2\r\n```\r\n\r\nAnd do not forget to set all necessary variables for TF compilation and later on for your container:\r\n```\r\nENV GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\nENV CI_BUILD_PYTHON=python\r\nENV PYTHON_PATH=\"/path/to/python/bin/python\"\r\nENV PYTHON_BIN_PATH=\"/path/to/python/bin/python\"\r\nENV PYTHON_LIB_PATH=\"/path/to/python/lib/python3.8/site-packages\"\r\nENV LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\nENV CUDA_TOOLKIT_PATH=\"/usr/local/cuda-10.1\"\r\nENV TF_NEED_CUDA=1\r\nENV TF_NEED_TENSORRT=0\r\nENV TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5\r\nENV TF_CUDA_VERSION=10.1\r\nENV TF_CUDNN_VERSION=7\r\n```\r\n\r\nMost import step here is:\r\n```\r\nENV TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.0,7.5\r\n```\r\nThis includes kernels for all GTX 1000, RTX 2000, RTX Titan GPUs.\r\n\r\nThen compile and build wheel\r\n```\r\nRUN bazel build --config=v2 --config=cuda --config=xla //tensorflow/tools/pip_package:build_pip_package\r\n\r\nRUN ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n```\r\n\r\nMake sure that all CUDA paths are set correctly. I noticed that even the NVIDIA image itself links stuff to `/lib/nvidia/` and other paths that **do not** exist in the container. For `nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04` the `LD_LIBRARY_PATH` is `/usr/local/cuda...`.\r\n\r\nAdditionally, to have `ptxas` and other cuda bins, you need to add the cuda bin path to your `$PATH`:\r\n```\r\nENV PATH=\"/usr/local/cuda/bin:$PATH\"\r\n```", "Adding @sanjoy who may have more familiarity with the cuda setup than I do.\r\n\r\nIt also seems related to https://github.com/tensorflow/tensorflow/issues/33002", "@dspyrhsu @tkoch96 @edwardyehuang @nathanin @MakeCent\r\nInterestingly, self-compiled container with `TF 2.3.0` starts trainings fast with 1 GPU, but takes very long to start with 4 GPUs (4x TITAN RTX). Any experiences already with that? Can you reproduce this behavior?\r\n\r\nI will test this a little more with 1-4 GPUs, different scripts and also vanilla TF GPU docker / conda TF GPU setups (as soon as available) and report.", "I did not try the single GPU. I tried 2 or 8 GPUs (Titan RTX or V100). Both are slow. I even feel it is slower than 2.2", "@Saduf2019 @zhen8838 @tomerk @rohan100jain \r\nIn my recent experiments, I observed some insteresting result.\r\nI have 5 machines. Seems slow start of first epoch issue only happened on 2 of them.\r\n\r\n```\r\nMachine 0: Intel i9 9820X, 64GB, 2 * Titan RTX W Nvlink\r\nMachine 1: Intel Xeon ?, 186GB, 2 * RTX 6000 W/O Nvlink\r\nMachine 2: Intel Xeon ?, 32GB, 2 * V100 32GB\r\nMachine 3: Intel Xeon Gold 6126, 98GB, 2 * V100 32GB\r\nMachine 4: Intel Xeon ?, 120GB, 2 * P100\r\n```\r\n\r\nThe issue only occur on machine = 0 or 1\r\nMachine 2, 3 and 4 can start training immediately \r\nTested on same configs.\r\n\r\n", "Does anyone have a fix for this? \r\nIt takes about 8~10mins before each epoch\r\nI'm using 8 GPUs (Titan Xp) with TF 2.3", "@viet2411  same GPU  Titan Xp with TF 2.3\r\nI use about 5min before the first epoch.", "The method provided by @Daniel451 is worked. Tested on RTX 2080.\r\nThe only problem is the built docker image is too large. Make it hard to deploy.\r\nI already try my best to optimize the Dockerfile, however, the image size is still about 20GB...\r\nBTW, Thank you @Daniel451", "@sanjoy \r\n\r\nI think I found the root cause of the slowdown.\r\n\r\nHere is the problematic block of code in TF 2.2.0 PTX compiler (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/stream_executor/gpu/asm_compiler.cc#L174): \r\n```cpp\r\n  if (!env->FileExists(ptxas_path).ok()) {\r\n    // Rely on subprocess invocation to find the correct binary.\r\n    ptxas_path = \"ptxas\";\r\n  }\r\n  VLOG(2) << \"Using ptxas at \" << ptxas_path;\r\n```\r\n\r\nWhile this is the exact same place, but in TF 2.1.0 (https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/stream_executor/gpu/asm_compiler.cc#L173) is:\r\n```cpp\r\n  TF_RETURN_IF_ERROR(env->FileExists(ptxas_path));\r\n  VLOG(2) << \"Using ptxas at \" << ptxas_path;\r\n```\r\n\r\nNow here is the workflow of what is happening in the TF 2.2.0:\r\n1. `CheckRedzones` method is being invoked for hundred times during initialization/inference on a CUDA GPU (e.g. when autotuning cuDNN kernels).\r\n2. Each invocation leads to redzone checker PTX code blob compilation or retrieving a precompiled blob from cache. Normally, this PTX code should be compiled once and then cached, and it really happens when `ptxas` is installed to the system within a CUDA toolkit. However, if if is not installed (e.g. if only CUDA runtime libraries are presented), then ptxas binary name resolution is being delegated to the OS at the asm_compiler.cc:L174.\r\n3. After ptxas binary name resolution was delegated to the OS, the asm_compiler code does some actions trying to compile the PTX code: **it writes the PTX blob to the filesystem temporary folder**, constructs the compilation command, invokes the command, obviously fails (since ptxas is not installed), deletes the temporary file and then returns with an error.\r\n\r\nThus, during the GPU model initialization/inference, this workflow repeats _hundreds_ or _thousands_ of times leading to _hundreds_ or _thousands_ of IO operations. You can make sure of it by setting the TF logging environment variable `TF_CPP_VMODULE=asm_compiler=2` and then running GPU stuff with TF. You'll see a continuous log of unsuccessful attempts to compile PTX for redzone checking consisting of continuous writes to the disk.\r\n\r\nNow let's take a look at the TF 2.1.0 code (see above). The major difference is that TF 2.1.0 does not rely on subprocess invocation to find the correct ptxas binary and immediately returns with an error if it fails to find the binary itself. By this simple action, TF 2.1.0 avoid doing a lot of repeating and useless stuff which TF 2.2.0 does. I was able to fix the slowdown in TF 2.2.0 by changing this tiny piece of code back to 2.1.0 version and it worked like a charm.\r\n\r\nHope this helps!", "Wow, good catch!  Would you be willing to send a PR?  It seems it'd be sufficient to cache the first failure (to find `ptxas`) and short circuit the subsequent invocations?\r\n\r\nCC @cheshire ", "@dev0x13 Thanks a lot for finding this!\r\n\r\nThe change you've mentioned was introduced to fix another bug (https://github.com/tensorflow/tensorflow/issues/33375), so it's not that easy to rollback (users have complained that `ptxas` is not found in their `PATH`).\r\n\r\nAs far as I understand, the gist of the problem is that we cache the success, but not the failure of the lookup. Would you be willing to write a PR fixing this?", "Subprocess invocation is hard/resource consuming to do in general.\r\n\r\nNote that this should also be fixed for good with CUDA 11.1 which bundles ptxas as a library which can be linked in instead, cf. https://docs.nvidia.com/cuda/ptx-compiler-api/index.html", "@sanjoy @cheshire \r\n\r\nThank you, guys. I am going to submit a PR fixing this within a week or so.\r\nThat's a good point about CUDA 11.1, hopefully this feature will be employed in the future TF versions.", "Can confirm upgrading to CUDA toolkit 11.1 solved this for me.", "I am sorry to open this issue again, but I was experiencing a similar problem when installing tensorflow-gpu from conda. I tried all the fixes given in this issue but none of them worked. I found out that ptxas and some more executables are missing in the cudatoolkit packages provided by conda (I've tried nvidia and conda-forge channels). After installing the package cudatoolkit-dev from the conda-forge channel (`conda install -c conda-forge cudatoolkit-dev`), the binary ptxas appeared in my conda environment and finally the problem was solved :)", "I hit the same problem (major slowdown) after upgrading to `CUDA 11.2` and `tf 2.4.1`. Downgrading to `CUDA 11.1` and then also to `tf 2.4.0` did not help. But then I noticed that @xehartnort is speaking about missing ptxas executables. I checked that the executables are in place:\r\n\r\n```\r\n[user.user@gauss local]$ find . -name ptxas\r\n./cuda-10.2/bin/ptxas\r\n./cuda-10.0/bin/ptxas\r\n./cuda-10.1/bin/ptxas\r\n./cuda-11.2/bin/ptxas\r\n./cuda-11.1/bin/ptxas\r\n```\r\n\r\nbut the PATH variable was not set, as required. Adding these lines to `~/.bash_profile` solved the problem:\r\n\r\n```\r\nPATH=$PATH:/usr/local/cuda-11.1/bin\r\nLD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.1/lib64\r\n```\r\n\r\nNow I am happy man again, until the next tensorflow issue.", "> I am sorry to open this issue again, but I was experiencing a similar problem when installing tensorflow-gpu from conda. I tried all the fixes given in this issue but none of them worked. I found out that ptxas a some more executables are missing in the cudatoolkit packages provided by conda (I've tried nvidia and conda-forge channels). After installing the package cudatoolkit-dev from the conda-forge channel (`conda install -c conda-forge cudatoolkit-dev`), the binary ptxas appeared in my conda environment and finally the problem was solved :)\r\n\r\nWorked for me (cuda 10.1), thank you", "\r\n\"relying on driver to perform ptx compilation. modify $path to customize ptxas location\"\r\n\r\nIn ubuntu, the following codes fixed my issue:\r\n\r\n```console\r\nptxas --version\r\nsudo apt install nvidia-cuda-toolkit\r\n\r\n```", "> \r\n> \r\n> I am sorry to open this issue again, but I was experiencing a similar problem when installing tensorflow-gpu from conda. I tried all the fixes given in this issue but none of them worked. I found out that ptxas and some more executables are missing in the cudatoolkit packages provided by conda (I've tried nvidia and conda-forge channels). After installing the package cudatoolkit-dev from the conda-forge channel (`conda install -c conda-forge cudatoolkit-dev`), the binary ptxas appeared in my conda environment and finally the problem was solved :)\r\n\r\nWorked for me too ! Thanks \r\nTensorflow 2.4.1\r\nCUDA Version: 11.2", "In the case that there is no cudatoolkit-dev version available, for example for cuda 11. I have found that it is possible to simply copy that file from the cuda docker to conda and it works like charm.\r\n\r\n```\r\ndocker run nvidia/cuda:11.2.2-devel-ubuntu20.04 cat /usr/local/cuda/bin/ptxas > /home/gbarbadillo/miniconda3/envs/goose_dev/bin/ptxas\r\nchmod u+x /home/gbarbadillo/miniconda3/envs/goose_dev/bin/ptxas\r\n```\r\n", "CUDA11.0 + tf2.4.0 with the ptxas executable in the right place, still suffer from slow start. I observed that more gpus cause slower training start up .... \ud83c\udd98  \ud83d\ude22 "]}, {"number": 40035, "title": "Training error on Cloud TPUs", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian OS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI am using a custom-modified code which works well on CPUs to train on TPUs and I am getting the error below\r\n```\r\n2020-05-31 22:06:00.162510: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-05-31 22:06:00.169480: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2000179999 Hz\r\n2020-05-31 22:06:00.170574: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x41a8240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-31 22:06:00.170610: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-31 22:06:00.180728: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.240.1.2:8470}\r\n2020-05-31 22:06:00.180770: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:56139}\r\n2020-05-31 22:06:00.195639: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.240.1.2:8470}\r\n2020-05-31 22:06:00.195683: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:56139}\r\n2020-05-31 22:06:00.196179: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:56139\r\nAll devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\r\n2020/05/31 22:06:05 Network Started\r\nNumber of devices: 8\r\nTraceback (most recent call last):\r\n  File \"check_tpu_distribute_strategy_trainer.py\", line 166, in <module>\r\n    for x in train_dist_dataset:\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 296, in __next__\r\n    return self.get_next()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 328, in get_next\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 192, in _get_next_as_optional\r\n    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py\", line 1150, in get_next_as_list\r\n    strict=True,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1204, in cond\r\n    if pred:\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 884, in __bool__\r\n    return bool(self._numpy())\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 929, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1590962770.862373470\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1590962770.862371521\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n2020-05-31 22:06:10.972287: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1590962770.862373470\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1590962770.862371521\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\t [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n\t [[RemoteCall]]\r\n```\r\n**Describe the expected behavior**\r\nThe code works without any problem with CPUs. The data is stored on the VM not on the GCS bucket.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@caffeine-lab \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "@ravikyram \r\nMy code is based on. : https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\r\n\r\ncode snippet which remodified\r\n\r\n```\r\n\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='tpu-node1')\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))\r\n\r\nPATH = \"Train_Images/\"\r\n\r\nimg_train,cap_train = I2S_Data.data_loader(PATH)\r\n\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n\t# loading the numpy files\r\n\tdef map_func(img_name, cap):\r\n\t\timg_tensor = np.load(img_name.decode('utf-8')+'.npy')\r\n\t\treturn img_tensor, cap\r\n\tdef get_dataset(batch_size,img_name_train,cap_train):\r\n\t\tdataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\r\n\r\n\t\t# using map to load the numpy files in parallel\r\n\t\tdataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n\t\t# shuffling and batching\r\n\t\tdataset = dataset.shuffle(BUFFER_SIZE)\r\n\t\tdataset = dataset.batch(batch_size)\r\n\r\n\t\treturn dataset\r\n\t#dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n\r\n\ttrain_dist_dataset = strategy.experimental_distribute_datasets_from_function(\r\n    lambda _: get_dataset(BATCH_SIZE_PER_REPLICA, img_name_train,cap_train))\r\n\r\n\tcheckpoint_path = \"./checkpoints/\"\r\n\t#Network Parameters\r\n\toptimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\r\n\tloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\r\n\r\n\tdef loss_function(real, pred):\r\n\t\tmask = tf.math.logical_not(tf.math.equal(real, 0))\r\n\t\tloss_ = loss_object(real, pred)\r\n\r\n\t\tmask = tf.cast(mask, dtype=loss_.dtype)\r\n\t\tloss_ *= mask\r\n\r\n\t\treturn tf.reduce_mean(loss_)\r\n\tencoder = I2S_Model.CNN_Encoder(embedding_dim)\r\n\tdecoder = I2S_Model.RNN_Decoder(embedding_dim, units, vocab_size)\r\n\tckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer = optimizer)\r\n\tckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=50)\r\n\tstart_epoch = 0\r\n\tif ckpt_manager.latest_checkpoint:\r\n\t\tckpt.restore(tf.train.latest_checkpoint(checkpoint_path))\r\n\t\tstart_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\r\n\r\n\t#the loss_plot array will be reset many times\r\n\tloss_plot = []\r\n\r\n\t@tf.function\r\n\tdef train_step(dist_inputs):\r\n\t\tdef step_fn(inputs):\r\n\t\t\timg_tensor, target = inputs\r\n\t\t\tprint(target.shape[0])\r\n\t\t\tloss = 0\r\n\r\n\t\t\t# initializing the hidden state for each batch because the captions are not related from image to image\r\n\t\t\thidden = decoder.reset_state(batch_size=target.shape[0])\r\n\r\n\t\t\tdec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\r\n\r\n\t\t\twith tf.GradientTape() as tape:\r\n\t\t\t\tfeatures = encoder(img_tensor)\r\n\r\n\t\t\t\tfor i in range(1, target.shape[1]):\r\n\t\t\t\t\t# passing the features through the decoder\r\n\t\t\t\t\tpredictions, hidden, _ = decoder(dec_input, features, hidden)\r\n\r\n\t\t\t\t\tloss += loss_function(target[:, i], predictions)\r\n\r\n\t\t\t\t\t# using teacher forcing\r\n\t\t\t\t\tdec_input = tf.expand_dims(target[:, i], 1)\r\n\r\n\t\t\ttotal_loss = (loss / int(target.shape[1]))\r\n\r\n\t\t\ttrainable_variables = encoder.trainable_variables + decoder.trainable_variables\r\n\r\n\t\t\tgradients = tape.gradient(loss, trainable_variables)\r\n\r\n\t\t\toptimizer.apply_gradients(zip(gradients, trainable_variables))\r\n\r\n\t\t\t\r\n\t\t\treturn loss, total_loss\r\n\r\n\t\tper_replica_losses, l_loss = strategy.run(step_fn, args=(dist_inputs,))\r\n\t\treturn strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, l_loss,axis=None)\r\n\r\n\r\n\tfor epoch in range(start_epoch, EPOCHS):\r\n\t\tstart = time.time()\r\n\t\ttotal_loss = 0.0\r\n\t\tbatch = 0\r\n\t\tfor x in train_dist_dataset:\r\n\t\t\tbatch_loss, t_loss = train_step(x)\r\n\t\t\ttotal_loss += t_loss\r\n\t\t\tbatch +=1\r\n\r\n\t\t\tif batch % 100 == 0:\r\n\t\t\t\tprint ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / BATCH_SIZE), flush=True)\r\n\t\ttrain_loss = total_loss / batch\r\n\t\tprint(\"Total Batches: \",batch)\r\n\t\t# storing the epoch end loss value to plot later\r\n\t\tloss_plot.append(total_loss / num_steps)\r\n\t\t\r\n\t\tif epoch % 1 == 0:\r\n\t\t\tckpt_manager.save()\r\n\r\n\t\tprint ('Epoch {} Loss {:.6f} Train Loss {:.6f}'.format(epoch + 1,total_loss/num_steps,train_loss), flush=True)\r\n\t\tprint (datetime.now().strftime('%Y/%m/%d %H:%M:%S'),'Time taken for 1 epoch {} sec\\n'.format(time.time() - start), flush=True)\r\n```\r\n\r\nRest of the code remains the same as in the notebook\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "@caffeine-lab Can you Please modify the code on colab and share the gist. Thanks!", "@gowthamkpr \r\n\r\nhttps://gist.github.com/caffeine-lab/bd623547b086322aa82832ce915931cf\r\n\r\nYou can find the edited gist on above link", "Hello!\r\n\r\nI'm going through the same issue and I wanted to know if there was any update on this? Will it be fixed in the forthcoming 2.3 release?", "We don't currently support loading local files this way, although it'll work at some point in the near future. For now, you can load them outside of tf.data as numpy arrays and feed them instead.", "@caffeine-lab Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40035\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40035\">No</a>\n"]}, {"number": 40034, "title": "Update Keras application docs", "body": "- added lost cautions\r\n- fixed references according to https://github.com/tensorflow/tensorflow/commit/71964116c54e3f1ad9686b9fb4987d526a15d8a7", "comments": ["@Molkree This PR is in draft, any update on this? Please. Thanks!", "> @Molkree This PR is in draft, any update on this? Please. Thanks!\r\n\r\nHi, @gbaned!\r\nSorry for the abandoned PR, I updated the rest of the docs (added more missing cautions in DenseNet and NASNet and standardized all references according to https://github.com/tensorflow/tensorflow/commit/71964116c54e3f1ad9686b9fb4987d526a15d8a7).\r\n\r\nWhile at it I can also remove `from __future__ import`s if you are willing to accept it in the same PR (and if it's actually okay to do but I presume so as TF no longer supports Python 2)."]}, {"number": 40033, "title": "Update CODE_OF_CONDUCT.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40033) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40033) for more info**.\n\n<!-- ok -->", "Can you add a few more spelling fixes to this PR? We should not run multiple hours of CI for just a single word change", "That was it \r\nthere aren't any more", "From [`CONTRIBUTING.md`](https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#general-guidelines-and-philosophy-for-contribution),\r\n> As every PR requires several CPU/GPU hours of CI testing, we discourage submitting PRs to fix one typo, one warning,etc. We recommend fixing the same issue at the file level at least (e.g.: fix all typos in a file, fix all compiler warning in a file, etc.)", "Please look in other files too, in that case", "Good catches!! Thanks for your contribution"]}, {"number": 40032, "title": "Regression on tf-nightly when using Cloud TPU and writing to GCS", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 4.19.118-2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly==2.3.0.dev20200531\r\n- Python version: 3.7.3\r\n- GPU model and memory: v3-8 TPU on Google Cloud\r\n\r\n**Describe the current behavior**\r\nFollowing the [MNIST with TPUs tutorial](https://cloud.google.com/tpu/docs/quickstart) using tf-nightly + tf-models-nightly I get the following exception when training the model with a TPU (see full log at end of issue):\r\n`tensorflow.python.framework.errors_impl.InternalError: Unexpected response from GCS when writing to gs://bert-data-for-grover-daniel-191017/mnist/: 'Location' header not returned.`\r\n\r\n- *Using a CPU/GPU the above code works fine.*\r\n- *Using TF=2.2.0 and its respective TF-Models' tag works fine as well.*\r\n\r\nI should stress that I'm linking to the tutorial because it's a simple script that reproduces the issue. This also affects my own project, and I have spent the last 24 hours going through my own code before thinking that it might be a bug in tensorflow / TPU drivers. I would also like to add that it might still be a misconfiguration on my part related to my Google Cloud setup (though I've already went through the basics of setting the correct permissions), but even if that's the case I believe TF's exceptions should be more relevant than the cryptic \"Location header not returned\" string, which seems too low-level.\r\n\r\nLooking through other issues, the only one in the past that seems relevant is #29304, though there the exception is always thrown when trying to write to the bucket, while here it's only when using TPUs (as mentioned above, it works fine with CPU/GPU).\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above tutorial for reproducing error while using the latest tf-nightly and tf-models-nightly.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nFull log when running mnist_main.py (replacing my actual bucket name with <bucket name>):\r\n```\r\n(venv) or@instance-6-tf-nightly:~/models/official/vision/image_classification$ python3 mnist_main.py \\\r\n>   --tpu=$TPU_NAME \\\r\n>   --model_dir=$MODEL_DIR \\\r\n>   --data_dir=$DATA_DIR \\\r\n>   --train_epochs=10 \\\r\n>   --distribution_strategy=tpu \\\r\n>   --download\r\n2020-05-31 18:12:25.684123: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-05-31 18:12:25.684174: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nI0531 18:12:27.662264 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token\r\nI0531 18:12:27.766179 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token\r\n2020-05-31 18:12:27.836011: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-05-31 18:12:27.836062: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-05-31 18:12:27.836083: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-6-tf-nightly): /proc/driver/nvidia/version does not exist\r\nI0531 18:12:27.865279 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token\r\nI0531 18:12:27.951599 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token\r\nI0531 18:12:28.011914 139671896655680 remote.py:218] Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\r\n2020-05-31 18:12:28.012461: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-05-31 18:12:28.020191: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\r\n2020-05-31 18:12:28.020458: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59b9fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-31 18:12:28.020490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-31 18:12:28.030120: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.149.197.2:8470}\r\n2020-05-31 18:12:28.030185: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34872}\r\n2020-05-31 18:12:48.223171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.149.197.2:8470}\r\n2020-05-31 18:12:48.223227: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:34872}\r\n2020-05-31 18:12:48.223761: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:422] Started server with target: grpc://localhost:34872\r\nINFO:tensorflow:Initializing the TPU system: node-2-test\r\nI0531 18:12:48.225129 139671896655680 tpu_strategy_util.py:72] Initializing the TPU system: node-2-test\r\nINFO:tensorflow:Clearing out eager caches\r\nI0531 18:12:48.369003 139671896655680 tpu_strategy_util.py:100] Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nI0531 18:12:54.483889 139671896655680 tpu_strategy_util.py:123] Finished initializing TPU system.\r\nI0531 18:12:54.511550 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token\r\nI0531 18:12:54.605852 139671896655680 transport.py:157] Attempting refresh to obtain initial access_token\r\nINFO:tensorflow:Found TPU system:\r\nI0531 18:12:54.659763 139671896655680 tpu_system_metadata.py:159] Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nI0531 18:12:54.659997 139671896655680 tpu_system_metadata.py:160] *** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nI0531 18:12:54.660263 139671896655680 tpu_system_metadata.py:161] *** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nI0531 18:12:54.660351 139671896655680 tpu_system_metadata.py:163] *** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nI0531 18:12:54.660465 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nI0531 18:12:54.660613 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nI0531 18:12:54.660671 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nI0531 18:12:54.660733 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nI0531 18:12:54.660809 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nI0531 18:12:54.660887 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nI0531 18:12:54.660967 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nI0531 18:12:54.661044 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nI0531 18:12:54.661120 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nI0531 18:12:54.661195 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nI0531 18:12:54.661268 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nI0531 18:12:54.661341 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nI0531 18:12:54.661427 139671896655680 tpu_system_metadata.py:165] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nI0531 18:12:54.974422 139671896655680 dataset_info.py:430] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: mnist/3.0.1\r\nI0531 18:12:55.118813 139671896655680 dataset_info.py:361] Load dataset info from /tmp/tmpq933cjqntfds\r\nI0531 18:12:55.120393 139671896655680 dataset_info.py:401] Field info.citation from disk and from code do not match. Keeping the one from code.\r\nI0531 18:12:55.181865 139671896655680 dataset_builder.py:333] Generating dataset mnist (gs://<bucket name>/data/mnist/3.0.1)\r\nDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to gs://<bucket name>/data/mnist/3.0.1...\r\nW0531 18:12:56.779903 139671896655680 dataset_builder.py:357] Dataset mnist is hosted on GCS. It will automatically be downloaded to your\r\nlocal data directory. If you'd instead prefer to read directly from our public\r\nGCS bucket (recommended if you're running on GCP), you can instead pass\r\n`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\r\n\r\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  4.35 file/s]\r\nDl Completed...: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.19 file/s]\r\nI0531 18:12:57.596850 139671896655680 dataset_info.py:361] Load dataset info from gs://<bucket name>/data/mnist/3.0.1.incompleteIAR91Z\r\nI0531 18:12:57.898775 139671896655680 dataset_info.py:401] Field info.citation from disk and from code do not match. Keeping the one from code.\r\nDataset mnist downloaded and prepared to gs://<bucket name>/data/mnist/3.0.1. Subsequent calls will reuse this data.\r\nI0531 18:12:58.717931 139671896655680 dataset_builder.py:478] Constructing tf.data.Dataset for split ['train', 'test'], from gs://<bucket name>/data/mnist/3.0.1\r\n2020-05-31 18:12:59.741571: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.\r\nTraceback (most recent call last):\r\n  File \"mnist_main.py\", line 171, in <module>\r\n    app.run(main)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"mnist_main.py\", line 164, in main\r\n    stats = run(flags.FLAGS)\r\n  File \"mnist_main.py\", line 135, in run\r\n    validation_freq=flags_obj.epochs_between_evals)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1066, in fit\r\n    steps=data_handler.inferred_steps)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 226, in __init__\r\n    self.set_model(model)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 277, in set_model\r\n    callback.set_model(model)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1951, in set_model\r\n    self._write_keras_model_graph()\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1989, in _write_keras_model_graph\r\n    summary_ops_v2.keras_model('keras', self.model, step=0)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 1155, in keras_model\r\n    metadata=summary_metadata)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py\", line 681, in write\r\n    _should_record_summaries_v2(), record, _nothing, name=\"summary_cond\")\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py\", line 51, in smart_cond\r\n    pred_value = smart_constant_value(pred)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/smart_cond.py\", line 75, in smart_constant_value\r\n    pred_value = tensor_util.constant_value(pred)\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 829, in constant_value\r\n    return tensor.numpy()\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1071, in numpy\r\n    maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1039, in _numpy\r\n    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Unexpected response from GCS when writing to gs://<bucket name>/mnist/: 'Location' header not returned.\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 2270, in async_wait\r\n    context().sync_executors()\r\n  File \"/home/or/models/venv/lib/python3.7/site-packages/tensorflow/python/eager/context.py\", line 652, in sync_executors\r\n    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\ntensorflow.python.framework.errors_impl.InternalError: Unexpected response from GCS when writing to gs://<bucket name>/mnist/: 'Location' header not returned.\r\n```\r\n", "comments": ["This is fixed with latest tf-nightly 2.4.0-dev20200712 version \r\nPlease can you confirm at your end? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40032\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40032\">No</a>\n"]}, {"number": 40031, "title": "Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/transpose.cc:", "body": "I have trained an audio classification model using Keras & Tensorflow and converted it to Tensorflow lite, it is converted fine but when I run on Android I get this error:\r\n\r\n```\r\n Process: org.tensorflow.lite.examples.speech, PID: 15170\r\n    java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/transpose.cc:56 op_context->perm->dims->data[0] != dims (3 != 2)\r\n    Node number 1 (TRANSPOSE) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:145)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)\r\n```\r\n\r\n**Usage:**\r\n\r\n```\r\ntfLite = try {\r\n    tfliteOptions.setNumThreads(4)\r\n\r\n    Interpreter(loadModelFile(assets, actualModelFilename), tfliteOptions)\r\n} catch (e: Exception) {\r\n    throw RuntimeException(e)\r\n}\r\n\r\nfor (i in 0 until RECORDING_LENGTH) {\r\n    floatInputBuffer[i][0] = inputBuffer[i] / 32767.0f\r\n}\r\n\r\nval inputArray = arrayOf<Any>(floatInputBuffer)\r\nval outputMap: MutableMap<Int, Any> = HashMap()\r\noutputMap[0] = outputScores\r\n\r\n// Run the model.\r\ntfLite?.runForMultipleInputsOutputs(inputArray, outputMap)\r\n```\r\n\r\n\r\n**Android Dependency:**\r\n\r\n` implementation 'org.tensorflow:tensorflow-lite:2.2.0'`\r\n       \r\n**Python Requirements:**\r\n\r\n```\r\ntensorboard==2.1.1\r\ntensorflow==2.1.0\r\ntensorflow-estimator==2.1.0\r\n```\r\n\r\n**Model Conversion:**\r\n\r\n    # Create a converter\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.allow_custom_ops = True\r\n    converter.experimental_new_converter = True\r\n\r\n    # Convert the model\r\n    tflite_model = converter.convert()\r\n\r\n    # Create the tflite model file\r\n    tflite_model_name = \"android/app/src/main/assets/converted_model.tflite\"\r\n    open(tflite_model_name, \"wb\").write(tflite_model)\r\n\r\n\r\n**Model Summary:**\r\n\r\nModel: \"long_short_term_memory\"\r\n\r\n|Layer (type)|Output Shape|Param #| Connected to|\r\n\r\n|input (InputLayer) |             [(None, 1, 16000)]|   0    |                                        \r\nmelbands (Melspectrogram)       (None, 128, 100, 1)  296064      input[0][0]                      \r\nbatch_norm (Normalization2D)    (None, 128, 100, 1)  0           melbands[0][0]                   \r\npermute (Permute)               (None, 100, 128, 1)  0           batch_norm[0][0]                 \r\nreshape (TimeDistributed)       (None, 100, 128)     0           permute[0][0]                    \r\nd_dense_tanh (TimeDistributed) (None, 100, 64)      8256        reshape[0][0]                    \r\nbidirectional_lstm (Bidirection (None, 100, 64)      24832       td_dense_tanh[0][0]              \r\nskip_connection (Concatenate)   (None, 100, 128)     0           td_dense_tanh[0][0]              \r\n                                                                 bidirectional_lstm[0][0]         \r\ndense_1_relu (Dense)            (None, 100, 64)      8256        skip_connection[0][0]            \r\nmax_pool_1d (MaxPooling1D)      (None, 50, 64)       0           dense_1_relu[0][0]               \r\ndense_2_relu (Dense)            (None, 50, 32)       2080        max_pool_1d[0][0]                \r\nflatten (Flatten)               (None, 1600)         0           dense_2_relu[0][0]               \r\ndropout (Dropout)               (None, 1600)         0           flatten[0][0]                    \r\ndense_3_relu (Dense)            (None, 32)           51232       dropout[0][0]                    \r\nsoftmax (Dense)                 (None, 2)            66          dense_3_relu[0][0]               \r\n\r\nTotal params: 390,786\r\nTrainable params: 390,786\r\nNon-trainable params: 0\r\n\r\n**Complete Python & Android Code can be found here:**\r\n[TensorFlow Lite Audio Classification](https://github.com/umair13adil/Audio-Classification)", "comments": ["@umair13adil \r\nI ran the code shared by you there is syntax error, can you please share simple indented stand alone code such that we can replicate the issue or if possible share a colab gist to analyse the issue.\r\n\r\nCan you please refer to issues with similar error message: #36555 #38926 if it helps.", "@Saduf2019 The Issue is on the Android side. Tensorflow lite library is crashing when I run it, If you run the Android code in Android studio, you will see the exception details. \r\n\r\nCan you tell me what I can do to optimize my model so Tensorflow lite can run without crashing? ", "I just updated these dependencies in python:\r\n\r\n    tensorboard==2.2.2\r\n    tensorflow==2.2.0\r\n    tensorflow-estimator==2.2.0\r\n\r\nAfter I built my model, it started working on Android. So the issue was related to the incompatible version on the Python side.", "I'm facing the same issue on the latest version of tensorflow\r\nCan you guide me how to debug it?", "@PatroxGaurab This issue is old and already got closed. Can you please create a new issue with simple standalone code to reproduce the issue? Thanks!"]}, {"number": 40030, "title": "models.metrics", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/ V10.1.105\r\n- GPU model and memory: Geforce MX110 2GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI created a simple model using tf.keras Sequential API and printed its metrics but it is outputting an empty list. No matter what I try, It returns an empty list.\r\n```\r\n[]\r\n```\r\n**Describe the expected behavior**\r\nIt should return \r\n```\r\n[<tensorflow.python.keras.metrics.BinaryAccuracy at 0x7f3e5e218320>,\r\n <tensorflow.python.keras.metrics.MeanAbsoluteError at 0x7f3e5c063a20>]\r\n```\r\n**Standalone code to reproduce the issue**\r\n```python3\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, MaxPool2D\r\nmodel = Sequential([\r\n    Conv2D(16,(3,3),padding='same', input_shape=(1,28,28),data_format='channels_first'),\r\n    MaxPooling2D((3,3), data_format='channels_first')\r\n])\r\nopt = tf.keras.optimizers.Adam(learning_rate=0.005)\r\nmodel.compile(optimizer=opt,\r\n              loss=tf.keras.losses.BinaryCrossentropy(),\r\n              metrics=[tf.keras.metrics.BinaryAccuracy(),\r\n              tf.keras.metrics.MeanAbsoluteError()]\r\n              )\r\nprint(model.metrics)\r\n```\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2, nightly version and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/596178d7832c35f15db66df49febf958/untitled939.ipynb).However i am not seeing any issue with TF version 2.1.0 .Thanks!", "@ahmadmustafaanis This is expected and was notified in the `Breaking Changes` of [2.2.0](https://github.com/tensorflow/tensorflow/releases).\r\n\r\n> Keras compile/fit behavior for functional and subclassed models have been unified. Model properties such as metrics, metrics_names will now be available only after training/evaluating the model on actual data for functional models. metrics will now include model loss and output losses.loss_functions property has been removed from the model. This was an undocumented property that was accidentally public and has now been removed.\r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "I tried this in [this](https://colab.research.google.com/gist/ravikyram/596178d7832c35f15db66df49febf958/untitled939.ipynb#scrollTo=kLDcDwDEWAsl) notebook but still, it is not showing metrics.", "@ahmadmustafaanis As mentioned in the Release notes (Breaking changes), some functionality was unified. It will show the metrics only after training/evaluating the model on **actual data**. Try to run training (`model.fit`) or `model.train_on_batch` and let us know whether you can see the metrics or not. Thanks!", "Yes, It is showing on ```model.fit```. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40030\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40030\">No</a>\n", "I still can reproduce the error on tf 2.4.0, using author's code. Any updates on this?", "@\r\n\r\n> @ahmadmustafaanis As mentioned in the Release notes (Breaking changes), some functionality was unified. It will show the metrics only after training/evaluating the model on **actual data**. Try to run training (`model.fit`) or `model.train_on_batch` and let us know whether you can see the metrics or not. Thanks!\r\n\r\n@callzhang As mentioned in the above comment, you need to run `model.fit` or `model.train_on_batch` with actual or toy data. Thanks!", "Well, this still looks like an issue to me, I would like to get the metrics without the need to train the model. "]}, {"number": 40029, "title": "Update pywrap_tensorflow.py", "body": "indentation correction", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F40029) for more info**.\n\n<!-- need_sender_cla -->", "@yoosoomin  Thank you for your contribution. Can you please sign CLA? Thanks!", "Can you check my CLA again?"]}, {"number": 40028, "title": "Tensorflow after built from source in High Sierra won't import in Jupyter notebook", "body": "Hello and i would to apologise if this is not the right place to ask the question i have but it is my first time writing to ask for help.\r\n\r\nGetting to my question, i built tensorflow from source in my macbook pro (mid 10) which is running on High Sierra 10.13.6. After installing tensorflow it imports without problems in python from terminal but when i open jupyter notebook and i try to import it from there it does not recognise tensorflow giving the impression as it was never installed. I looked it up before asking here and with sys.executable notebook and python use the same executables. \r\n\r\nAn odd but interesting thing is that when i create a jupyter .ipynb file in the \"/Users/mac\" directory where tensorflow directory lies after source installation it loads tensorflow without problems.Elsewhere when creating a .ipynb in my laptop, tensorflow does not import.\r\n\r\nFinally the \"tf.__ version __ \" won't show me my tensorflow version nowhere not even when i import it througth the terminal.\r\n\r\nThank you in advance for your answers.", "comments": ["@thanoscs,\r\nCould you please double check if you have built and installed TensorFlow as per [this document](https://www.tensorflow.org/install/source#install_the_package).\r\n\r\nAlso, make sure both the terminal and Jupyter notebook are using the same Python environment. Thanks!", "> @thanoscs,\r\n> Could you please double check if you have built and installed TensorFlow as per [this document](https://www.tensorflow.org/install/source#install_the_package).\r\n> \r\n> Also, make sure both the terminal and Jupyter notebook are using the same Python environment. Thanks!\r\n\r\n\r\nhi and thanks for the answer,\r\n\r\nI rebuilt it and after \"./configure\" command when i insert this command \"bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\"  it pops me these errors\r\n\r\n\r\n![Screen Shot 2020-06-02 at 13 33 03](https://user-images.githubusercontent.com/33914304/83510407-a3201700-a4d5-11ea-8c77-07f8bc569142.png)\r\n\r\nI should add that my cpu does not has AVX instructions and this is the reason that i am building it from source.", "Did you install Jupyter in the TensorFlow environment?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Did you install Jupyter in the TensorFlow environment?\r\n\r\nIt's not an environment as i understand when you build it from source but a directory. Am i mistaken?", "Apologies for the delay in response. You may try raising this issue on [jupyter notebook repo](https://github.com/jupyter/notebook/issues)", "> Apologies for the delay in response. You may try raising this issue on [jupyter notebook repo](https://github.com/jupyter/notebook/issues)\r\n\r\nOk thanks for your answer, i will raise it there too.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40028\">No</a>\n"]}, {"number": 40027, "title": "TF2.2 HALT TRAINING ON 2 2080TI+NVLINK BRIDGE", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): LINUX UBUNTU 20.04LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): pip3 install tensorflow\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8 \r\n- Bazel version (if compiling from source): NO \r\n- GCC/Compiler version (if compiling from source): GCC7\r\n- CUDA/cuDNN version: 10.1/7.6.5/  NCCL  2.6.4\r\n- GPU model and memory: 2 X 2080ti 11GB + NVLINK BRIDGE\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nthe training halt at \"model.fit()\", and then the Linux halt, no response to my mouse, keyboard, I have to reboot.\r\n**Describe the expected behavior**\r\nTraining should be running.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCODE is TF2.2 official example :\r\nhttps://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/distribute/keras.ipynb\r\nWith TF2.2, CUDA 10.1, CUDNN NEWEST WITH CUDA 10.1 ,7.6.5 \r\nNCCL  2.6.4 \r\n Linux Ubuntu 20.04 LTS.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nIf I remove NVLINK, reboot , the above code runs fine, and 2 GPU have load.\r\n\r\nand I tested NVLINK bridge with CUDA's utility code for NVLINK speed , I think NVLINK is functional and the 2 GPU's peer to peer speed is fast.\r\n\r\nthe LOG can't be obtained ,because I have to reboot my computer ,the whole Linux system halt ,and had no response .", "comments": ["@AlexWang1900 \r\nI am unable to open the file shared, can you please provide a colab gist or paste the code here that replicates the issue.", "@Saduf2019\r\nIt's also here in Tensorflow's GitHub \r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/keras.ipynb\r\n\r\nYou can tell me what test  I should ran , what version I should try, otherwise I may install the whole system to Ubuntu 18.06 lts.", "@AlexWang1900\r\nCode shared is too huge, please share the code where the error is faced so i could replicate the error. else paste the code here. or if possible paste your code in colab and share its gist when you face an error.", "@Saduf2019 \r\nI don't know exactly which part cause the problem.\r\nthe code is written and released by you Tensorflow team, uploaded on Tensorflow github ,released with TF2.2 as official document , intend to show the end user like regular user the keras  distributed function.\r\n\r\nHaven't you guys run it?????  you tell me you cannot run it???? but it is released as a part of TF2.2!!!!\r\n\r\nthe code is already minimal to show the distributed GPU training function ,I don't think I can shrink it anymore. \r\n\r\nI guess it's the NCCL part , \"strategy = tf.distribute.MirroredStrategy()\" part which calls the NCCL functional api to achieve GPU communication.\r\n\r\nBy the way as a software engineer/project manager with PMP Certification, I doubt the Tensorflow's release process, CR process ,and the unprofessional question as you asked, \"Code shared is too huge\"  , it is your official code to show the user how to run a basic multi-gpu training code.\r\n\r\n", "@Saduf2019\r\nAND COLAB is not the environment you can replicate the error, haven't you seen it's 2 2080ti with NVLINK bridge , the specified hardware.\r\nalso I saw another guy posted here with 2 RTX TITAN, he have a similar problem, he calls model.fit then after 7 mins,training starts, where I encounter Linux halt.\r\n\r\nSo I highly recommend you setup a consumer GPUs PC to replicate the error!", "@AlexWang1900 we have tested that tutorial, as have many other users, on multiple GPU systems.\r\n\r\nAs you said, likely the problem you're facing is due to NCCL on your system. I would recommend the following: \r\n\r\n1. Run nccl_tests (https://github.com/NVIDIA/nccl-tests) to verify NCCL runs fine in your system\r\n2. Try `strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())` which will skip using NCCL in the training.  \r\n\r\nBased on the above, we can debug further.\r\n\r\n\r\n\r\n", "> @AlexWang1900 we have tested that tutorial, as have many other users, on multiple GPU systems.\r\n> \r\n> As you said, likely the problem you're facing is due to NCCL on your system. I would recommend the following:\r\n> \r\n>     1. Run nccl_tests (https://github.com/NVIDIA/nccl-tests) to verify NCCL runs fine in your system\r\n> \r\n>     2. Try `strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())` which will skip using NCCL in the training.\r\n> \r\n> \r\n> Based on the above, we can debug further.\r\n\r\n@guptapriya  THX very much! I will do the test asap! ", "@guptapriya  \r\n1, I ran all nccl_tests, it seems NCCL is working. But when each test running(about 30 min for each test), the system freezes, I can't switch to browser or doing anything, I can only move the mouse, but the system doesn't respond to mouse-clicking or keyboard input.\r\n```\r\n\r\n#  ./all_reduce_perf -b 8 -e 128M -f 2 -g 2\r\n# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1 \r\n#\r\n# Using devices\r\n#   Rank  0 Pid   3795 on w-system device  0 [0x01] GeForce RTX 2080 Ti\r\n#   Rank  1 Pid   3795 on w-system device  1 [0x02] GeForce RTX 2080 Ti\r\n#\r\n#                                                     out-of-place                       in-place          \r\n#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \r\n           8             2   float     sum     7.18    0.00    0.00  0e+00     7.02    0.00    0.00  0e+00\r\n          16             4   float     sum     7.00    0.00    0.00  0e+00     7.02    0.00    0.00  0e+00\r\n          32             8   float     sum     7.28    0.00    0.00  0e+00     7.19    0.00    0.00  0e+00\r\n          64            16   float     sum     7.20    0.01    0.01  0e+00     7.05    0.01    0.01  0e+00\r\n         128            32   float     sum     7.30    0.02    0.02  0e+00     7.19    0.02    0.02  0e+00\r\n         256            64   float     sum     7.30    0.04    0.04  0e+00     7.20    0.04    0.04  0e+00\r\n         512           128   float     sum     7.47    0.07    0.07  0e+00     7.12    0.07    0.07  0e+00\r\n        1024           256   float     sum     8.14    0.13    0.13  0e+00     7.92    0.13    0.13  0e+00\r\n        2048           512   float     sum     8.56    0.24    0.24  0e+00     8.43    0.24    0.24  0e+00\r\n        4096          1024   float     sum     9.72    0.42    0.42  0e+00     9.49    0.43    0.43  0e+00\r\n        8192          2048   float     sum    11.99    0.68    0.68  0e+00    11.92    0.69    0.69  0e+00\r\n       16384          4096   float     sum    14.36    1.14    1.14  0e+00    14.21    1.15    1.15  0e+00\r\n       32768          8192   float     sum    16.79    1.95    1.95  0e+00    16.64    1.97    1.97  0e+00\r\n       65536         16384   float     sum    21.14    3.10    3.10  0e+00    20.55    3.19    3.19  0e+00\r\n      131072         32768   float     sum    35.56    3.69    3.69  0e+00    35.43    3.70    3.70  0e+00\r\n      262144         65536   float     sum    41.23    6.36    6.36  0e+00    41.21    6.36    6.36  0e+00\r\n      524288        131072   float     sum    50.66   10.35   10.35  0e+00    50.82   10.32   10.32  0e+00\r\n     1048576        262144   float     sum    72.54   14.45   14.45  0e+00    72.45   14.47   14.47  0e+00\r\n     2097152        524288   float     sum    120.7   17.37   17.37  0e+00    118.4   17.71   17.71  0e+00\r\n     4194304       1048576   float     sum    215.2   19.49   19.49  0e+00    214.7   19.53   19.53  0e+00\r\n     8388608       2097152   float     sum    411.3   20.39   20.39  0e+00    399.1   21.02   21.02  0e+00\r\n    16777216       4194304   float     sum    865.3   19.39   19.39  0e+00    779.6   21.52   21.52  0e+00\r\n    33554432       8388608   float     sum   1547.9   21.68   21.68  0e+00   1699.3   19.75   19.75  0e+00\r\n    67108864      16777216   float     sum   3115.1   21.54   21.54  0e+00   3007.4   22.31   22.31  0e+00\r\n   134217728      33554432   float     sum   5994.3   22.39   22.39  0e+00   5991.9   22.40   22.40  0e+00\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 7.43886 \r\n\r\n/all_gather_perf -b 8 -e 128M -f 2 -g 2\r\n# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1 \r\n#\r\n# Using devices\r\n#   Rank  0 Pid   9119 on w-system device  0 [0x01] GeForce RTX 2080 Ti\r\n#   Rank  1 Pid   9119 on w-system device  1 [0x02] GeForce RTX 2080 Ti\r\n#\r\n#                                             out-of-place                       in-place          \r\n#       size         count    type     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)             (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \r\n           8             1   float     7.14    0.00    0.00  0e+00     7.06    0.00    0.00  0e+00\r\n          16             2   float     7.03    0.00    0.00  0e+00     7.00    0.00    0.00  0e+00\r\n          32             4   float     6.96    0.00    0.00  0e+00     7.07    0.00    0.00  0e+00\r\n          64             8   float     7.10    0.00    0.00  0e+00     7.07    0.00    0.00  0e+00\r\n         128            16   float     7.10    0.01    0.01  0e+00     7.14    0.01    0.01  0e+00\r\n         256            32   float     7.18    0.02    0.02  0e+00     7.23    0.02    0.02  0e+00\r\n         512            64   float     7.49    0.03    0.03  0e+00     7.47    0.03    0.03  0e+00\r\n        1024           128   float     7.03    0.07    0.07  0e+00     6.96    0.07    0.07  0e+00\r\n        2048           256   float     6.97    0.15    0.15  0e+00     6.97    0.15    0.15  0e+00\r\n        4096           512   float     7.41    0.28    0.28  0e+00     7.00    0.29    0.29  0e+00\r\n        8192          1024   float     9.59    0.43    0.43  0e+00     8.80    0.47    0.47  0e+00\r\n       16384          2048   float    11.41    0.72    0.72  0e+00    10.78    0.76    0.76  0e+00\r\n       32768          4096   float    13.39    1.22    1.22  0e+00    11.85    1.38    1.38  0e+00\r\n       65536          8192   float    16.57    1.98    1.98  0e+00    13.83    2.37    2.37  0e+00\r\n      131072         16384   float    23.07    2.84    2.84  0e+00    18.39    3.56    3.56  0e+00\r\n      262144         32768   float    31.38    4.18    4.18  0e+00    30.27    4.33    4.33  0e+00\r\n      524288         65536   float    36.00    7.28    7.28  0e+00    35.30    7.43    7.43  0e+00\r\n     1048576        131072   float    47.38   11.06   11.06  0e+00    46.84   11.19   11.19  0e+00\r\n     2097152        262144   float    70.44   14.89   14.89  0e+00    69.77   15.03   15.03  0e+00\r\n     4194304        524288   float    120.1   17.46   17.46  0e+00    115.5   18.16   18.16  0e+00\r\n     8388608       1048576   float    212.5   19.73   19.73  0e+00    210.2   19.95   19.95  0e+00\r\n    16777216       2097152   float    418.5   20.05   20.05  0e+00    414.0   20.26   20.26  0e+00\r\n    33554432       4194304   float    817.8   20.51   20.51  0e+00    785.1   21.37   21.37  0e+00\r\n    67108864       8388608   float   1568.3   21.40   21.40  0e+00   1560.9   21.50   21.50  0e+00\r\n   134217728      16777216   float   3298.6   20.34   20.34  0e+00   3070.3   21.86   21.86  0e+00\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 6.6972 \r\n\r\n./broadcast_perf -b 8 -e 128M -f 2 -g 2\r\n# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1 \r\n#\r\n# Using devices\r\n#   Rank  0 Pid  26256 on w-system device  0 [0x01] GeForce RTX 2080 Ti\r\n#   Rank  1 Pid  26256 on w-system device  1 [0x02] GeForce RTX 2080 Ti\r\n#\r\n#                                                     out-of-place                       in-place          \r\n#       size         count    type    root     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \r\n           8             2   float       0     7.24    0.00    0.00  0e+00     7.50    0.00    0.00  0e+00\r\n          16             4   float       0     8.31    0.00    0.00  0e+00     7.69    0.00    0.00  0e+00\r\n          32             8   float       0     8.15    0.00    0.00  0e+00     8.23    0.00    0.00  0e+00\r\n          64            16   float       0     7.19    0.01    0.01  0e+00     7.13    0.01    0.01  0e+00\r\n         128            32   float       0     7.25    0.02    0.02  0e+00     7.45    0.02    0.02  0e+00\r\n         256            64   float       0     7.08    0.04    0.04  0e+00     7.16    0.04    0.04  0e+00\r\n         512           128   float       0     7.47    0.07    0.07  0e+00     7.39    0.07    0.07  0e+00\r\n        1024           256   float       0     7.19    0.14    0.14  0e+00    32.19    0.03    0.03  0e+00\r\n        2048           512   float       0     7.36    0.28    0.28  0e+00     7.03    0.29    0.29  0e+00\r\n        4096          1024   float       0     7.25    0.57    0.57  0e+00     7.07    0.58    0.58  0e+00\r\n        8192          2048   float       0     9.11    0.90    0.90  0e+00     8.10    1.01    1.01  0e+00\r\n       16384          4096   float       0    10.97    1.49    1.49  0e+00    10.52    1.56    1.56  0e+00\r\n       32768          8192   float       0    13.36    2.45    2.45  0e+00    11.73    2.79    2.79  0e+00\r\n       65536         16384   float       0    17.03    3.85    3.85  0e+00    14.24    4.60    4.60  0e+00\r\n      131072         32768   float       0    22.66    5.78    5.78  0e+00    22.60    5.80    5.80  0e+00\r\n      262144         65536   float       0    28.48    9.21    9.21  0e+00    28.45    9.21    9.21  0e+00\r\n      524288        131072   float       0    40.26   13.02   13.02  0e+00    40.08   13.08   13.08  0e+00\r\n     1048576        262144   float       0    63.48   16.52   16.52  0e+00    63.19   16.59   16.59  0e+00\r\n     2097152        524288   float       0    110.1   19.04   19.04  0e+00    109.3   19.19   19.19  0e+00\r\n     4194304       1048576   float       0    205.7   20.39   20.39  0e+00    237.1   17.69   17.69  0e+00\r\n     8388608       2097152   float       0    425.1   19.73   19.73  0e+00    386.7   21.69   21.69  0e+00\r\n    16777216       4194304   float       0    815.0   20.59   20.59  0e+00    824.0   20.36   20.36  0e+00\r\n    33554432       8388608   float       0   1536.8   21.83   21.83  0e+00   1508.2   22.25   22.25  0e+00\r\n    67108864      16777216   float       0   3139.2   21.38   21.38  0e+00   3124.3   21.48   21.48  0e+00\r\n   134217728      33554432   float       0   6283.5   21.36   21.36  0e+00   5873.1   22.85   22.85  0e+00\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 7.99748 \r\n\r\n$ ./reduce_perf -b 8 -e 128M -f 2 -g 2\r\n# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1 \r\n#\r\n# Using devices\r\n#   Rank  0 Pid   4810 on w-system device  0 [0x01] GeForce RTX 2080 Ti\r\n#   Rank  1 Pid   4810 on w-system device  1 [0x02] GeForce RTX 2080 Ti\r\n#\r\n#                                                     out-of-place                       in-place          \r\n#       size         count    type   redop    root     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                             (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \r\n           8             2   float     sum       0     7.16    0.00    0.00  0e+00     7.35    0.00    0.00  0e+00\r\n          16             4   float     sum       0     7.74    0.00    0.00  0e+00     7.67    0.00    0.00  0e+00\r\n          32             8   float     sum       0     7.08    0.00    0.00  0e+00     7.07    0.00    0.00  0e+00\r\n          64            16   float     sum       0     7.13    0.01    0.01  0e+00     7.14    0.01    0.01  0e+00\r\n         128            32   float     sum       0     7.15    0.02    0.02  0e+00     7.06    0.02    0.02  0e+00\r\n         256            64   float     sum       0     7.14    0.04    0.04  0e+00     7.12    0.04    0.04  0e+00\r\n         512           128   float     sum       0     7.14    0.07    0.07  0e+00     7.11    0.07    0.07  0e+00\r\n        1024           256   float     sum       0     7.09    0.14    0.14  0e+00     7.09    0.14    0.14  0e+00\r\n        2048           512   float     sum       0     7.11    0.29    0.29  0e+00     7.12    0.29    0.29  0e+00\r\n        4096          1024   float     sum       0     7.28    0.56    0.56  0e+00     7.20    0.57    0.57  0e+00\r\n        8192          2048   float     sum       0     8.72    0.94    0.94  0e+00     8.59    0.95    0.95  0e+00\r\n       16384          4096   float     sum       0    10.80    1.52    1.52  0e+00    10.78    1.52    1.52  0e+00\r\n       32768          8192   float     sum       0    12.89    2.54    2.54  0e+00    12.64    2.59    2.59  0e+00\r\n       65536         16384   float     sum       0    16.42    3.99    3.99  0e+00    15.88    4.13    4.13  0e+00\r\n      131072         32768   float     sum       0    23.17    5.66    5.66  0e+00    23.27    5.63    5.63  0e+00\r\n      262144         65536   float     sum       0    29.13    9.00    9.00  0e+00    28.88    9.08    9.08  0e+00\r\n      524288        131072   float     sum       0    40.93   12.81   12.81  0e+00    40.93   12.81   12.81  0e+00\r\n     1048576        262144   float     sum       0    64.30   16.31   16.31  0e+00    64.25   16.32   16.32  0e+00\r\n     2097152        524288   float     sum       0    110.5   18.98   18.98  0e+00    110.6   18.97   18.97  0e+00\r\n     4194304       1048576   float     sum       0    202.1   20.76   20.76  0e+00    202.1   20.76   20.76  0e+00\r\n     8388608       2097152   float     sum       0    386.5   21.70   21.70  0e+00    386.3   21.71   21.71  0e+00\r\n    16777216       4194304   float     sum       0    752.6   22.29   22.29  0e+00    752.5   22.30   22.30  0e+00\r\n    33554432       8388608   float     sum       0   1485.2   22.59   22.59  0e+00   1529.3   21.94   21.94  0e+00\r\n    67108864      16777216   float     sum       0   2947.4   22.77   22.77  0e+00   2945.2   22.79   22.79  0e+00\r\n   134217728      33554432   float     sum       0   5873.8   22.85   22.85  0e+00   5873.8   22.85   22.85  0e+00\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 8.22671 \r\n$ ./reduce_scatter_perf -b 8 -e 128M -f 2 -g 2\r\n# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1 \r\n#\r\n# Using devices\r\n#   Rank  0 Pid   5435 on w-system device  0 [0x01] GeForce RTX 2080 Ti\r\n#   Rank  1 Pid   5435 on w-system device  1 [0x02] GeForce RTX 2080 Ti\r\n#\r\n#                                                     out-of-place                       in-place          \r\n#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \r\n           8             1   float     sum     7.21    0.00    0.00  0e+00     7.28    0.00    0.00  0e+00\r\n          16             2   float     sum     7.12    0.00    0.00  0e+00     7.18    0.00    0.00  0e+00\r\n          32             4   float     sum     7.14    0.00    0.00  0e+00     7.22    0.00    0.00  0e+00\r\n          64             8   float     sum     7.20    0.00    0.00  0e+00     7.15    0.00    0.00  0e+00\r\n         128            16   float     sum     7.14    0.01    0.01  0e+00     7.12    0.01    0.01  0e+00\r\n         256            32   float     sum     7.16    0.02    0.02  0e+00     7.12    0.02    0.02  0e+00\r\n         512            64   float     sum     7.18    0.04    0.04  0e+00     7.12    0.04    0.04  0e+00\r\n        1024           128   float     sum     7.53    0.07    0.07  0e+00     7.27    0.07    0.07  0e+00\r\n        2048           256   float     sum     7.28    0.14    0.14  0e+00     7.23    0.14    0.14  0e+00\r\n        4096           512   float     sum     7.64    0.27    0.27  0e+00     7.57    0.27    0.27  0e+00\r\n        8192          1024   float     sum     9.35    0.44    0.44  0e+00     9.24    0.44    0.44  0e+00\r\n       16384          2048   float     sum    11.33    0.72    0.72  0e+00    11.23    0.73    0.73  0e+00\r\n       32768          4096   float     sum    12.66    1.29    1.29  0e+00    12.62    1.30    1.30  0e+00\r\n       65536          8192   float     sum    15.39    2.13    2.13  0e+00    15.31    2.14    2.14  0e+00\r\n      131072         16384   float     sum    21.02    3.12    3.12  0e+00    21.35    3.07    3.07  0e+00\r\n      262144         32768   float     sum    32.36    4.05    4.05  0e+00    31.98    4.10    4.10  0e+00\r\n      524288         65536   float     sum    39.63    6.61    6.61  0e+00    39.76    6.59    6.59  0e+00\r\n     1048576        131072   float     sum    57.11    9.18    9.18  0e+00    56.88    9.22    9.22  0e+00\r\n     2097152        262144   float     sum    92.96   11.28   11.28  0e+00    92.54   11.33   11.33  0e+00\r\n     4194304        524288   float     sum    166.4   12.60   12.60  0e+00    165.9   12.64   12.64  0e+00\r\n     8388608       1048576   float     sum    308.5   13.59   13.59  0e+00    504.4    8.32    8.32  0e+00\r\n    16777216       2097152   float     sum   1050.1    7.99    7.99  0e+00    693.5   12.10   12.10  0e+00\r\n    33554432       4194304   float     sum   1533.4   10.94   10.94  0e+00   1414.8   11.86   11.86  0e+00\r\n    67108864       8388608   float     sum   2529.2   13.27   13.27  0e+00   2314.2   14.50   14.50  0e+00\r\n   134217728      16777216   float     sum   5619.2   11.94   11.94  0e+00   4905.4   13.68   13.68  0e+00\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 4.44552 \r\n```\r\n\r\n2. when   strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice()) \r\nit runs smoothly , and the linux system does not freeze,  the log is here: \r\n\r\n```\r\n`model.fit(train_dataset, epochs=10, callbacks=callbacks)\r\nEpoch 1/10\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n469/469 [==============================] - ETA: 0s - accuracy: 0.9232 - loss: 0.2688\r\nLearning rate for epoch 1 is 0.0010000000474974513\r\n469/469 [==============================] - 2s 4ms/step - accuracy: 0.9232 - loss: 0.2688 - lr: 0.0010\r\nEpoch 2/10\r\n456/469 [============================>.] - ETA: 0s - accuracy: 0.9739 - loss: 0.0879\r\nLearning rate for epoch 2 is 0.0010000000474974513\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9741 - loss: 0.0874 - lr: 0.0010\r\nEpoch 3/10\r\n452/469 [===========================>..] - ETA: 0s - accuracy: 0.9832 - loss: 0.0574\r\nLearning rate for epoch 3 is 0.0010000000474974513\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9830 - loss: 0.0577 - lr: 0.0010\r\nEpoch 4/10\r\n467/469 [============================>.] - ETA: 0s - accuracy: 0.9900 - loss: 0.0368\r\nLearning rate for epoch 4 is 9.999999747378752e-05\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9900 - loss: 0.0369 - lr: 1.0000e-04\r\nEpoch 5/10\r\n466/469 [============================>.] - ETA: 0s - accuracy: 0.9910 - loss: 0.0340\r\nLearning rate for epoch 5 is 9.999999747378752e-05\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9910 - loss: 0.0340 - lr: 1.0000e-04\r\nEpoch 6/10\r\n457/469 [============================>.] - ETA: 0s - accuracy: 0.9914 - loss: 0.0325\r\nLearning rate for epoch 6 is 9.999999747378752e-05\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9914 - loss: 0.0324 - lr: 1.0000e-04\r\nEpoch 7/10\r\n459/469 [============================>.] - ETA: 0s - accuracy: 0.9920 - loss: 0.0309\r\nLearning rate for epoch 7 is 9.999999747378752e-05\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9920 - loss: 0.0307 - lr: 1.0000e-04\r\nEpoch 8/10\r\n456/469 [============================>.] - ETA: 0s - accuracy: 0.9930 - loss: 0.0282\r\nLearning rate for epoch 8 is 9.999999747378752e-06\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9929 - loss: 0.0284 - lr: 1.0000e-05\r\nEpoch 9/10\r\n458/469 [============================>.] - ETA: 0s - accuracy: 0.9930 - loss: 0.0282\r\nLearning rate for epoch 9 is 9.999999747378752e-06\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9930 - loss: 0.0281 - lr: 1.0000e-05\r\nEpoch 10/10\r\n454/469 [============================>.] - ETA: 0s - accuracy: 0.9929 - loss: 0.0281\r\nLearning rate for epoch 10 is 9.999999747378752e-06\r\n469/469 [==============================] - 1s 3ms/step - accuracy: 0.9929 - loss: 0.0279 - lr: 1.0000e-05\r\n\r\n```\r\nFrom the log I can't see if multi-GPUs  are involved. \r\nbut with nvidia-smi it shows about 20~% utilization for both GPUs.\r\n\r\nAnd as I mentioned before , when I remove the NVLINK BRIDGE hardware between the 2 GPUs, with tf.distribute.MirroredStrategy() default strategy, it runs smoothly too.\r\n\r\n\r\n\r\nThanks !!!!!! \r\n\r\n", "Thanks for the updates. So it seems that if running NCCl tests causes your system to hang, then there must be some issue on that side. Can you file a bug with NVIDIA and they should be able to help you debug?\r\n\r\n`strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())` does run with 2 GPUs, but it would be less efficient than the default option which is NCCL. So I would not advise this as a solution - I was just asking to test this to confirm that NCCL is the problem. \r\n\r\nI think you should work with NVIDIA to figure out how to fix the root cause why NCCL tests make the computer hang. \r\n", "@guptapriya\r\nThanks a lot !  I will file a bug at NVIDIA side. \r\n\r\nAnd I may ask , if I remove the NVLINK bridge, and using strategy = tf.distribute.MirroredStrategy() default option, the code runs well, is it also using NCCL? and is that efficient as with NVLINK on? \r\nin that case ,the GPU is communicating through PCIE .\r\n\r\nAnd from the docs and names of the tf.distribute.ReductionToOneDevice(), one cannot conclude it uses NCCL or not, \r\nI search through internet and the distributed method is changing so fast, very confusing to understand the newest methods.\r\nSo I need a more updated and precise document, to tell me under 2 gpu situation, should I use a NVLINK, and which strategy should I use when training with them??\r\n", "@guptapriya  Hi!!!! \r\nI reboot the system without a display cable connected to the 2080ti\r\nthen I SSH to it from another PC through intranet.\r\n\r\nthen I start jupyter server remotely , and runs NCCL-TEST from another session also remotely\r\nusing : $ ./all_reduce_perf -b 8 -e 16M -f 2 -g 2\r\n\r\nat the same time, before NCCL-TEST finished running ,I ran some simple code \r\n```\r\nimport numpy as np\r\nprint(2+4)\r\n```\r\nthrough jupyter notebook remotely , it is fine , it returns immediately:\r\n6\r\n\r\nit shows at least the cpu-dram-dmi-pch-usb-network(I am using a usb wifi card) is not hanging when remotely running NCCL-TEST\r\n\r\nbut I started another session remotely, fire command nvidia-smi, it never returns anything. \r\n\r\nafter a while , NCCL-TEST session returned:\r\n```\r\n# nThread 1 nGpus 2 minBytes 8 maxBytes 16777216 step: 2(factor) warmup iters: 5 iters: 20 validation: 1\r\n#\r\n# Using devices\r\n#   Rank  0 Pid   1711 on w-system device  0 [0x01] GeForce RTX 2080 Ti\r\n#   Rank  1 Pid   1711 on w-system device  1 [0x02] GeForce RTX 2080 Ti\r\n#\r\n#                                                     out-of-place                       in-place\r\n#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)\r\n           8             2   float     sum     7.46    0.00    0.00  0e+00     7.37    0.00    0.00  0e+00\r\n          16             4   float     sum     7.43    0.00    0.00  0e+00     7.45    0.00    0.00  0e+00\r\n          32             8   float     sum     7.39    0.00    0.00  0e+00     7.38    0.00    0.00  0e+00\r\n          64            16   float     sum     7.52    0.01    0.01  0e+00     7.36    0.01    0.01  0e+00\r\n         128            32   float     sum     7.55    0.02    0.02  0e+00     7.45    0.02    0.02  0e+00\r\n         256            64   float     sum     7.57    0.03    0.03  0e+00     7.44    0.03    0.03  0e+00\r\n         512           128   float     sum     7.61    0.07    0.07  0e+00     7.54    0.07    0.07  0e+00\r\n        1024           256   float     sum     8.20    0.12    0.12  0e+00     8.06    0.13    0.13  0e+00\r\n        2048           512   float     sum     8.54    0.24    0.24  0e+00     8.44    0.24    0.24  0e+00\r\n        4096          1024   float     sum     9.92    0.41    0.41  0e+00     9.78    0.42    0.42  0e+00\r\n        8192          2048   float     sum    12.10    0.68    0.68  0e+00    11.92    0.69    0.69  0e+00\r\n       16384          4096   float     sum    14.77    1.11    1.11  0e+00    14.38    1.14    1.14  0e+00\r\n       32768          8192   float     sum    16.97    1.93    1.93  0e+00    16.64    1.97    1.97  0e+00\r\n       65536         16384   float     sum    21.45    3.06    3.06  0e+00    20.51    3.20    3.20  0e+00\r\n      131072         32768   float     sum    35.61    3.68    3.68  0e+00    35.85    3.66    3.66  0e+00\r\n      262144         65536   float     sum    42.46    6.17    6.17  0e+00    41.73    6.28    6.28  0e+00\r\n      524288        131072   float     sum    50.70   10.34   10.34  0e+00    52.69    9.95    9.95  0e+00\r\n     1048576        262144   float     sum    72.64   14.44   14.44  0e+00    72.58   14.45   14.45  0e+00\r\n     2097152        524288   float     sum    121.7   17.23   17.23  0e+00    118.7   17.66   17.66  0e+00\r\n     4194304       1048576   float     sum    215.4   19.47   19.47  0e+00    215.1   19.50   19.50  0e+00\r\n     8388608       2097152   float     sum    400.1   20.96   20.96  0e+00    399.7   20.99   20.99  0e+00\r\n    16777216       4194304   float     sum    779.8   21.51   21.51  0e+00    780.7   21.49   21.49  0e+00\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 5.53153\r\n```\r\nthen I ran nvidia-smi again ,it returns:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   38C    P0    62W / 260W |      0MiB / 11016MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  Off  | 00000000:02:00.0 Off |                  N/A |\r\n|  0%   38C    P0    28W / 260W |      0MiB / 11019MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nIt is clear that no XORG or any other program is using GPUs.\r\n\r\nthen I ran the TF code mentioned above again , using NCCL, through Jupyter notebook remotely .\r\n**it is still hanging**   \r\n\r\n```\r\nEpoch 1/20\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n\r\n```\r\n\r\nAs showed above , NCCL-TEST runs well remotely, it may have conflict when locally connected to a moniter, or blocks some GPU operatrion.\r\n**But Tensorflow stops remotely as bad as locally.**\r\n\r\n", "### Test case 2\r\n**I ran the test above again with these changes:** \r\nI turn nvidia-smi -pm 1 according to an NVIDIA egineer,\r\nI set NCCL variables like this:\r\nNCCL_DEBUG=INFO\r\nNCCL_MAX_NCHANNELS=1\r\nNCCL_P2P_DISABLE=0\r\nNCCL_ALGO=Ring\r\nNCCL_IGNORE_CPU_AFFINITY=0\r\nNCCL_NTHREADS=256\r\nNCCL_DEBUG_SUBSYS=ALL\r\n\r\n**results:**\r\n1)   nvidia-smi blocked ,and  this time ,I waited the NCCL tests finished, then nvidia-smi returned result after NCCL test finished:\r\n\r\n```\r\nThu Jun 11 11:48:00 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.82       Driver Version: 440.82       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  On   | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   34C    P8    18W / 260W |    578MiB / 11016MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  On   | 00000000:02:00.0 Off |                  N/A |\r\n|  0%   35C    P8    12W / 260W |    580MiB / 11019MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1762      C   ./all_reduce_perf                            569MiB |\r\n|    1      1762      C   ./all_reduce_perf                            579MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\nthis shows ./all_reduce_perf  blocks nvidia-smi to return.\r\n\r\n2) simple code \"print(2+4)\" still **not** blocked.\r\n3) TF code still hangs:\r\n\r\n```\r\nEpoch 1/2\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n```\r\nbut from the jupyter console, I found something strange :\r\nafter start running the TF code, jupyter console is returning this before hanging:\r\n\r\n```\r\n\"\"\"\r\n2020-06-11 11:51:38.062792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n[I 11:51:49.184 NotebookApp] Saving file at /Projects/projects_emotion/2GPUtest.ipynb\r\n2020-06-11 11:52:10.157455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.157946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-11 11:52:10.158035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.158483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-11 11:52:10.158666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-11 11:52:10.177880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-11 11:52:10.189116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-11 11:52:10.192011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-11 11:52:10.212995: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-11 11:52:10.217748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-11 11:52:10.268347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-11 11:52:10.268658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.270592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.272525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.274385: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.276123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-11 11:52:10.302103: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-06-11 11:52:10.331471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2899885000 Hz\r\n2020-06-11 11:52:10.332315: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8b0c000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-11 11:52:10.332379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-11 11:52:10.721908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.731219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.731791: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ae5ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-11 11:52:10.731807: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-06-11 11:52:10.731812: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2020-06-11 11:52:10.732502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.732981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-11 11:52:10.733028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.733490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties:\r\npciBusID: 0000:02:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-06-11 11:52:10.733516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-11 11:52:10.733528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-11 11:52:10.733540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-06-11 11:52:10.733551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-06-11 11:52:10.733563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-06-11 11:52:10.733574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-06-11 11:52:10.733585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-06-11 11:52:10.733621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.734107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.734594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.735081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.735539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\r\n2020-06-11 11:52:10.735774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-06-11 11:52:10.736833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-11 11:52:10.736845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1\r\n2020-06-11 11:52:10.736852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y\r\n2020-06-11 11:52:10.736856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N\r\n2020-06-11 11:52:10.737195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.737690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.738180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.738649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10201 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-06-11 11:52:10.739231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-06-11 11:52:10.739710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10203 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)\r\n2020-06-11 11:52:12.123844: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\n2020-06-11 11:52:12.124277: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 2 GPUs\r\n2020-06-11 11:52:12.128452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\n2020-06-11 11:52:12.229408: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-06-11 11:52:12.231351: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-06-11 11:52:12.231641: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n2020-06-11 11:52:13.036926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-06-11 11:52:13.649714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\nwangying-system:2441:2494 [1] NCCL INFO Bootstrap : Using [0]wlx90e6ba5a46fa:192.168.31.167<0>\r\nwangying-system:2441:2494 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n\r\nwangying-system:2441:2494 [1] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nwangying-system:2441:2494 [1] NCCL INFO NET/Socket : Using [0]wlx90e6ba5a46fa:192.168.31.167<0>\r\nNCCL version 2.5.7+cudaCUDA_MAJOR.CUDA_MINOR\r\nwangying-system:2441:2569 [1] NCCL INFO NCCL_IGNORE_CPU_AFFINITY set by environment to 0.\r\nwangying-system:2441:2569 [1] NCCL INFO Setting affinity for GPU 1 to 3f\r\nwangying-system:2441:2568 [0] NCCL INFO Setting affinity for GPU 0 to 3f\r\n\"\"\"\r\n```\r\n\r\n**it shows TF is calling NCCL version 2.5.7+cudaCUDA_MAJOR.CUDA_MINOR, \r\nI only download NCCL once for 2.6.4, and where is this 2.5.7 come from??? \r\nand each time I run NCCL-TEST ,it shows NCCL version 2.6.4**\r\n\r\n", "### update: good news \r\nI was editing post, and leave it hanging \r\nafter about 30 mins, the TF CODE finished running !!!!\r\n\r\nthe jupyter cosole returned more after information above: \r\n```\r\nwangying-system:2441:2568 [0] NCCL INFO Intel CPU (PCI 12, InterCpu 8)\r\nwangying-system:2441:2568 [0] NCCL INFO /sys/devices/pci0000:00/0000:00:14.0/usb1/1-12/1-12:1.0 -> 0/0/0/0\r\nwangying-system:2441:2568 [0] NCCL INFO NCCL_P2P_DISABLE set by environment to 0.\r\nwangying-system:2441:2568 [0] NCCL INFO === System : maxWidth 21 maxSpeed 21 ===\r\nwangying-system:2441:2568 [0] NCCL INFO CPU/FFFFFFFFFFFFFFFF\r\nwangying-system:2441:2568 [0] NCCL INFO + PCI[12] - GPU/1000 (0)\r\nwangying-system:2441:2568 [0] NCCL INFO             + NVL[21] - GPU/2000\r\nwangying-system:2441:2568 [0] NCCL INFO + PCI[12] - GPU/2000 (1)\r\nwangying-system:2441:2568 [0] NCCL INFO             + NVL[21] - GPU/1000\r\nwangying-system:2441:2568 [0] NCCL INFO + PCI[12] - PCI/0\r\nwangying-system:2441:2568 [0] NCCL INFO             + PCI[12] - NIC/0\r\nwangying-system:2441:2568 [0] NCCL INFO ==========================================\r\nwangying-system:2441:2568 [0] NCCL INFO GPU/1000 :GPU/1000 (0/5000/0) GPU/2000 (1/21/1) CPU/FFFFFFFFFFFFFFFF (1/12/2)\r\nwangying-system:2441:2568 [0] NCCL INFO GPU/2000 :GPU/1000 (1/21/1) GPU/2000 (0/5000/0) CPU/FFFFFFFFFFFFFFFF (1/12/2)\r\nwangying-system:2441:2568 [0] NCCL INFO Pattern 2, crossNic 0, nChannels 1, speed 21/21, nvlink 1, type 1, sameChannels 1\r\nwangying-system:2441:2568 [0] NCCL INFO  0 : GPU/0 GPU/1\r\nwangying-system:2441:2568 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, speed 21/21, nvlink 1, type 1, sameChannels 1\r\nwangying-system:2441:2568 [0] NCCL INFO  0 : GPU/0 GPU/1\r\nwangying-system:2441:2569 [1] NCCL INFO /sys/devices/pci0000:00/0000:00:14.0/usb1/1-12/1-12:1.0 -> 0/0/0/0\r\nwangying-system:2441:2569 [1] NCCL INFO === System : maxWidth 21 maxSpeed 21 ===\r\nwangying-system:2441:2569 [1] NCCL INFO CPU/FFFFFFFFFFFFFFFF\r\nwangying-system:2441:2569 [1] NCCL INFO + PCI[12] - GPU/1000 (0)\r\nwangying-system:2441:2569 [1] NCCL INFO             + NVL[21] - GPU/2000\r\nwangying-system:2441:2569 [1] NCCL INFO + PCI[12] - GPU/2000 (1)\r\nwangying-system:2441:2569 [1] NCCL INFO             + NVL[21] - GPU/1000\r\nwangying-system:2441:2569 [1] NCCL INFO + PCI[12] - PCI/0\r\nwangying-system:2441:2569 [1] NCCL INFO             + PCI[12] - NIC/0\r\nwangying-system:2441:2569 [1] NCCL INFO ==========================================\r\nwangying-system:2441:2569 [1] NCCL INFO GPU/1000 :GPU/1000 (0/5000/0) GPU/2000 (1/21/1) CPU/FFFFFFFFFFFFFFFF (1/12/2)\r\nwangying-system:2441:2569 [1] NCCL INFO GPU/2000 :GPU/1000 (1/21/1) GPU/2000 (0/5000/0) CPU/FFFFFFFFFFFFFFFF (1/12/2)\r\nwangying-system:2441:2569 [1] NCCL INFO Pattern 2, crossNic 0, nChannels 1, speed 21/21, nvlink 1, type 1, sameChannels 1\r\nwangying-system:2441:2569 [1] NCCL INFO  0 : GPU/0 GPU/1\r\nwangying-system:2441:2569 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 1, speed 21/21, nvlink 1, type 1, sameChannels 1\r\nwangying-system:2441:2569 [1] NCCL INFO  0 : GPU/0 GPU/1\r\nwangying-system:2441:2569 [1] NCCL INFO NCCL_MAX_NCHANNELS set by environment to 1.\r\nwangying-system:2441:2569 [1] NCCL INFO NCCL_NTHREADS set by environment to 256.\r\nwangying-system:2441:2569 [1] NCCL INFO Threads per block : 256/640/256\r\nwangying-system:2441:2568 [0] NCCL INFO Channel 00/01 :    0   1\r\nwangying-system:2441:2569 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64\r\nwangying-system:2441:2568 [0] NCCL INFO Threads per block : 256/640/256\r\nwangying-system:2441:2569 [1] NCCL INFO Trees [0] -1/-1/-1->1->0|0->1->-1/-1/-1\r\nwangying-system:2441:2568 [0] NCCL INFO Latency/AlgBw | Tree/    LL | Tree/ LL128 | Tree/Simple | Ring/    LL | Ring/ LL128 | Ring/Simple |\r\nwangying-system:2441:2568 [0] NCCL INFO     Broadcast |    0.0/  0.0|    0.0/  0.0|    0.0/  0.0|    4.0/  5.2|    6.1/  0.0|   14.1/ 21.0|\r\nwangying-system:2441:2568 [0] NCCL INFO        Reduce |    0.0/  0.0|    0.0/  0.0|    0.0/  0.0|    4.0/  5.2|    6.1/  0.0|   14.1/ 21.0|\r\nwangying-system:2441:2568 [0] NCCL INFO     AllGather |    0.0/  0.0|    0.0/  0.0|    0.0/  0.0|    4.0/ 10.5|    6.1/  0.0|   14.1/ 42.0|\r\nwangying-system:2441:2568 [0] NCCL INFO ReduceScatter |    0.0/  0.0|    0.0/  0.0|    0.0/  0.0|    4.0/ 10.5|    6.1/  0.0|   14.1/ 42.0|\r\nwangying-system:2441:2568 [0] NCCL INFO     AllReduce |    5.4/  0.0|    8.2/  0.0|   56.0/  0.0|    4.4/  5.2|    8.6/  0.0|   19.8/ 21.0|\r\nwangying-system:2441:2568 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64\r\nwangying-system:2441:2568 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1\r\nwangying-system:2441:2569 [1] NCCL INFO Ring 00 : 1[2000] -> 0[1000] via P2P/direct pointer\r\nwangying-system:2441:2568 [0] NCCL INFO Ring 00 : 0[1000] -> 1[2000] via P2P/direct pointer\r\nwangying-system:2441:2569 [1] NCCL INFO comm 0x7f8a5c0aa760 rank 1 nranks 2 cudaDev 1 busId 2000 - Init COMPLETE\r\nwangying-system:2441:2568 [0] NCCL INFO comm 0x7f8a582d31f0 rank 0 nranks 2 cudaDev 0 busId 1000 - Init COMPLETE\r\nwangying-system:2441:2565 [0] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8a80c00000 recvbuff 0x7f8a80c00000 count 347146 datatype 7 op 0 root 0 comm 0x7f8a582d31f0 [nranks=2] stream 0x7f898f832140\r\nwangying-system:2441:2565 [0] NCCL INFO Launch mode Group/CGMD\r\nwangying-system:2441:2566 [1] NCCL INFO AllReduce: opCount 0 sendbuff 0x7f8a816a4000 recvbuff 0x7f8a816a4000 count 347146 datatype 7 op 0 root 0 comm 0x7f8a5c0aa760 [nranks=2] stream 0x7f898f8443d0\r\n2020-06-11 12:18:16.116142: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\n2020-06-11 12:18:16.116204: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_NOT_INITIALIZED\r\n2020-06-11 12:18:16.116223: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_NOT_INITIALIZED\r\nwangying-system:2441:2565 [0] NCCL INFO AllReduce: opCount 1 sendbuff 0x7f8a80c00000 recvbuff 0x7f8a80c00000 count 347146 datatype 7 op 0 root 0 comm 0x7f8a582d31f0 [nranks=2] stream 0x7f898f832140\r\nwangying-system:2441:2566 [1] NCCL INFO AllReduce: opCount 1 sendbuff 0x7f8a816a4000 recvbuff 0x7f8a816a4000 count 347146 datatype 7 op 0 root 0 comm 0x7f8a5c0aa760 [nranks=2] stream 0x7f898f8443d0\r\n2020-06-11 12:18:16.122365: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n2020-06-11 12:18:16.123972: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n2020-06-11 12:18:16.130630: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: ./logs/train/plugins/profile/2020_06_11_12_18_16\r\n2020-06-11 12:18:16.134600: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to ./logs/train/plugins/profile/2020_06_11_12_18_16/wangying-system.trace.json.gz\r\n2020-06-11 12:18:16.136308: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0 ms\r\n\r\n2020-06-11 12:18:16.136868: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: ./logs/train/plugins/profile/2020_06_11_12_18_16Dumped tool data for overview_page.pb to ./logs/train/plugins/profile/2020_06_11_12_18_16/wangying-system.overview_page.pb\r\nDumped tool data for input_pipeline.pb to ./logs/train/plugins/profile/2020_06_11_12_18_16/wangying-system.input_pipeline.pb\r\nDumped tool data for tensorflow_stats.pb to ./logs/train/plugins/profile/2020_06_11_12_18_16/wangying-system.tensorflow_stats.pb\r\nDumped tool data for kernel_stats.pb to ./logs/train/plugins/profile/2020_06_11_12_18_16/wangying-system.kernel_stats.pb\r\n..     many many lines about .recv buff,.....\r\n```\r\n\r\nthe jupyter web returns: \r\n```\r\nEpoch 1/2\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\nINFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n468/469 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.9256\r\nLearning rate for epoch 1 is 0.0010000000474974513\r\n469/469 [==============================] - 2s 5ms/step - loss: 0.2584 - accuracy: 0.9257 - lr: 0.0010\r\nEpoch 2/2\r\n469/469 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9742\r\nLearning rate for epoch 2 is 0.0010000000474974513\r\n469/469 [==============================] - 1s 3ms/step - loss: 0.0865 - accuracy: 0.9742 - lr: 0.0010\r\n```\r\nwhy it took so long????\r\n", "From the two test case above , there are these questions: (BOLD are for TF guys)\r\n**0 Why it took 30 mins for TF to run with NCCL, while the same code runs less than 30 seconds without NCCL.**\r\n1  why would  NCCL do a topology discovery every time it starts? it is like Windows doing a disk-scan every time I start playing minesweeper.\r\n**2 why the TF calls NCCL 2.5.7, while NCCL-TEST calls 2.6.4**\r\n3  why there are two topology discovery rounds???\r\n4 why NCCL detects node1 for cpu ? there is only one cpu in system.\r\n5  why tree algorithm tuning shows, when I specified only Ring algorithm\r\n\r\n**After all ,it is not clear to use NVLINK BRIDGE, MUST one use NCCL? \r\nsince I ran CUDA utility test, it shows great speed between 2 GPUs, NCCL uses CUDA kernels to perform inter-GPU communication\r\nand if I use \"reduce to one device strategy\" without NCCL, only CUDA, can I still get the benefit using NVLINK??**", "@dubey can you help answer some of the above questions related to NCCL?", "re: 0 - I'm not sure, I haven't seen such a long startup time with NCCL before.  Perhaps ask NVIDIA if this is a known issue?\r\nre: 2 - TF [bundles NCCL](https://github.com/tensorflow/tensorflow/tree/master/third_party/nccl) as a part of the pip package.  We are currently using 2.5.7.", "Hi~~~ Thanks everyone, \r\nI think I found a workaround to this problem, by changing some NCCL code. it takes 2s to finish NCCL-TEST. \r\nI need to test it on TF , how can I override TF's 2.5.7 nccl ??? or specify TF to use another NCCL?", "This issue is located inside NCCL and found a NVML NVIDIA library's bug . so I will close this issue now\r\n\r\nI think I have to build TF locally to use customized NCCL to test new NCCL. Thanks guys ~~~~ \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40027\">No</a>\n", "> This issue is located inside NCCL and found a NVML NVIDIA library's bug . so I will close this issue now\r\n> \r\n> I think I have to build TF locally to use customized NCCL to test new NCCL. Thanks guys ~~~~\r\n\r\n@AlexWang1900 How did you do this? I am struggling right now. I am testing a system with A100s and being forced to use:\r\n`tf.distribute.HierarchicalCopyAllReduce()` or `tf.distribute.ReductionToOneDevice()` to get my code to work. Doing this though makes my TF code behave very slowly compared to an older system I benchmark against.  "]}, {"number": 40026, "title": "Cannot build TensorFlow Lite iOS framework for benchmark tool due to '@XNNPACK//:neonv8_ukernels' rule compilation failure", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master (edbe5e189c1ec14d3a3386aa29e6118d807d9379)\r\n- Python version: 3.7.7 (also fails in the same way with 2.7.16)\r\n- Installed using virtualenv? pip? conda?: not installed\r\n- Bazel version (if compiling from source): 3.0.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.17)\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory: AMD Radeon R9 M370X 2 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nBuilding the benchmark framework for the TFLite iOS benchmark app (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/ios) fails with 'C++ compilation of rule '@XNNPACK//:neonv8_ukernels' failed'. XNNPack has been upgraded in April to fix an AArch64 compilation issue (#38400 #38436), perhaps this is the cause?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`./tensorflow/lite/tools/benchmark/ios/build_benchmark_framework.sh`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n$ ./configure \r\n\r\nYou have bazel 3.0.0 installed.\r\nPlease specify the location of python. [Default is /usr/local/opt/python/bin/python3.7]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: \r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: \r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nDo you wish to build TensorFlow with iOS support? [y/N]: y\r\niOS support will be enabled for TensorFlow.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n$ ./tensorflow/lite/tools/benchmark/ios/build_benchmark_framework.sh\r\n\r\n++ bazel info workspace\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'info' from /Users/valentinmiu/2019/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'info' from /Users/valentinmiu/2019/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'info' from /Users/valentinmiu/2019/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/usr/local/opt/python/bin/python3.7 --config=xla --action_env TF_CONFIGURE_IOS=1\r\nINFO: Found applicable config definition build:v2 in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:macos in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\n+ WORKSPACE_ROOT=/Users/valentinmiu/2019/tensorflow\r\n+ BENCHMARK_DIR=tensorflow/lite/tools/benchmark\r\n+ DEST_DIR=tensorflow/lite/tools/benchmark/ios/TFLiteBenchmark/TFLiteBenchmark/Frameworks\r\n+ FRAMEWORK_TARGET=TensorFlowLiteBenchmarkC_framework\r\n+ PROFILING_ARGS=\r\n+ getopts p opt_name\r\n+ shift 0\r\n+ pushd /Users/valentinmiu/2019/tensorflow\r\n~/2019/tensorflow ~/2019/tensorflow\r\n+ bazel build --config=ios_fat -c opt //tensorflow/lite/tools/benchmark/experimental/ios:TensorFlowLiteBenchmarkC_framework\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=272\r\nINFO: Reading rc options for 'build' from /Users/valentinmiu/2019/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/valentinmiu/2019/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /Users/valentinmiu/2019/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/usr/local/opt/python/bin/python3.7 --action_env PYTHON_LIB_PATH=/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/usr/local/opt/python/bin/python3.7 --config=xla --action_env TF_CONFIGURE_IOS=1\r\nINFO: Found applicable config definition build:v2 in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:ios_fat in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --config=ios --ios_multi_cpus=armv7,arm64,i386,x86_64\r\nINFO: Found applicable config definition build:ios in file /Users/valentinmiu/2019/tensorflow/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing --noenable_platform_specific_config --copt=-w --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Build option --action_env has changed, discarding analysis cache.\r\nINFO: Analyzed target //tensorflow/lite/tools/benchmark/experimental/ios:TensorFlowLiteBenchmarkC_framework (115 packages loaded, 9402 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_valentinmiu/94cd6c8db58787268db4435bda1fd58c/external/XNNPACK/BUILD.bazel:2048:1: C++ compilation of rule '@XNNPACK//:neonv8_ukernels' failed (Exit 1)\r\nexternal/XNNPACK/src/math/roundne-neonv8.c:24:23: error: initializing 'const float32x4_t' (vector of 4 'float32_t' values) with an expression of incompatible type 'int'\r\n    const float32x4_t vy = vrndnq_f32(vx);\r\n                      ^    ~~~~~~~~~~~~~~\r\n1 error generated.\r\nTarget //tensorflow/lite/tools/benchmark/experimental/ios:TensorFlowLiteBenchmarkC_framework failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1327.686s, Critical Path: 96.87s\r\nINFO: 5060 processes: 5060 local.\r\nFAILED: Build did NOT complete successfully```\r\n", "comments": ["@Maratyszcza I think this started to happen with a04c8be3e7086d9e14ba37c3c0945a3ea98414ce.\r\n\r\nCould you please check what's going on? Looks like it's trying to use NEON v8 operator for armv7 target.\r\n\r\nYou could easily reproduce this with iOS armv7 build with the following command, after enabling iOS build with `./configure` script.\r\n\r\n```sh\r\nbazel build -c opt --config=ios_armv7 //tensorflow/lite/tools/benchmark/experimental/ios:TensorFlowLiteBenchmarkC_framework\r\n```", "This should be fixed in master branch now.\r\n\r\n@user-vm Can you please try again after updating the repo, and close the issue if it's working for you? Thanks.", "Since this issue is confirmed to be fixed by multiple people, let me close it now. Feel free to reopen if it doesn't work.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40026\">No</a>\n"]}, {"number": 40025, "title": "Same labels and predictions, non-zero loss", "body": "```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense, Flatten\r\nfrom tensorflow.keras.models import Model\r\n\r\n#%%#######################################################\r\nipt = Input(batch_shape=(128, 28, 28, 1))\r\nx   = Flatten()(ipt)\r\nout = Dense(10, activation='softmax')(x)\r\nmodel = Model(ipt, out)\r\nmodel.compile('adam', 'categorical_crossentropy')\r\n\r\n#%%#######################################################\r\nx = np.random.uniform(0, 1, model.input_shape)\r\npred = model(x, training=True)  # =False also works\r\nloss = model.compiled_loss(pred, pred)\r\n\r\nprint(tf.__version__)\r\nprint(loss)\r\n```\r\n\r\n```python\r\n2.3.0-dev20200531  # Colab; also reproduced in 2.2.0, Win-10\r\ntf.Tensor(1.9904033, shape=(), dtype=float32)\r\n```\r\n\r\nWhat's the deal?", "comments": ["This is expected behavior - my mishap.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40025\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40025\">No</a>\n"]}, {"number": 40024, "title": "How to set batch_bize best. I get leakage in old Cuda versions, allocated memory consumption error in new  Cuda versions.", "body": "Cuda :10.0.0\r\nTensorflow :  1.14.0\r\nKeras :  2.3.1\r\nPython=3.6.4\r\nNvidia GeForce GTX 970\r\n\r\nKeras Model InceptionV3, Epoch 10, BatchSize=80, ImageSize=(170,170,3), Activation softplus, Optimizer Adam, Loss Function Categorical_crossentropy, Learning_rate=0.0001,\r\n\r\n...\r\n2020-05-31 15:32:23.841070: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 9 Chunks of size 12582912 totalling 108.00MiB\r\n2020-05-31 15:32:23.843941: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 14826240 totalling 14.14MiB\r\n2020-05-31 15:32:23.846970: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 18137088 totalling 17.30MiB\r\n2020-05-31 15:32:23.852014: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 18393856 totalling 17.54MiB\r\n2020-05-31 15:32:23.855604: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 21233664 totalling 20.25MiB\r\n2020-05-31 15:32:23.858570: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 3 Chunks of size 23887872 totalling 68.34MiB\r\n2020-05-31 15:32:23.862554: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 24526848 totalling 23.39MiB\r\n2020-05-31 15:32:23.865774: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 28164096 totalling 26.86MiB\r\n2020-05-31 15:32:23.871765: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 32768000 totalling 62.50MiB\r\n2020-05-31 15:32:23.874621: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 33293824 totalling 31.75MiB\r\n2020-05-31 15:32:23.879541: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 55083008 totalling 105.06MiB\r\n2020-05-31 15:32:23.886215: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 57802752 totalling 110.25MiB\r\n2020-05-31 15:32:23.891302: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 2 Chunks of size 70975488 totalling 135.38MiB\r\n2020-05-31 15:32:23.894429: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 71203584 totalling 67.90MiB\r\n2020-05-31 15:32:23.897584: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 83172096 totalling 79.32MiB\r\n2020-05-31 15:32:23.902251: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 98283008 totalling 93.73MiB\r\n2020-05-31 15:32:23.906031: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 105160704 totalling 100.29MiB\r\n2020-05-31 15:32:23.909807: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 110166016 totalling 105.06MiB\r\n2020-05-31 15:32:23.912743: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 206682368 totalling 197.11MiB\r\n2020-05-31 15:32:23.915783: I tensorflow/core/common_runtime/bfc_allocator.cc:812] 1 Chunks of size 220082688 totalling 209.89MiB\r\n2020-05-31 15:32:23.922903: I tensorflow/core/common_runtime/bfc_allocator.cc:816] Sum Total of in-use chunks: 2.93GiB\r\n2020-05-31 15:32:23.925769: I tensorflow/core/common_runtime/bfc_allocator.cc:818] total_region_allocated_bytes_: 3150400512 memory_limit_: 3150400716 available bytes: 204 curr_region_allocation_bytes_: 6300801536\r\n2020-05-31 15:32:23.931420: I tensorflow/core/common_runtime/bfc_allocator.cc:824] Stats:\r\nLimit:                  3150400716\r\nInUse:                  3148202496\r\nMaxInUse:               3150392320\r\nNumAllocs:                  545033\r\nMaxAllocSize:           2255688704\r\n\r\n2020-05-31 15:32:23.944922: W tensorflow/core/common_runtime/bfc_allocator.cc:319] **********************************xx*********xx**********x******************************************\r\n2020-05-31 15:32:23.951512: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at concat_op.cc:153 : Resource exhausted: OOM when allocating tensor with shape[64,2048,3,3] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n**************************************************************************\r\n**************************************************************************\r\nCuda :10.1.0\r\nTensorflow :  2.1.0\r\nThe others parameters same.\r\n\r\n2020-05-30 14:14:25.050571: I tensorflow/core/common_runtime/bfc_allocator.cc:958] 1 Chunks of size 166842368 totalling 159.11MiB\r\n2020-05-30 14:14:25.054540: I tensorflow/core/common_runtime/bfc_allocator.cc:958] 1 Chunks of size 177561600 totalling 169.34MiB\r\n2020-05-30 14:14:25.058475: I tensorflow/core/common_runtime/bfc_allocator.cc:958] 1 Chunks of size 194433024 totalling 185.43MiB\r\n2020-05-30 14:14:25.063757: I tensorflow/core/common_runtime/bfc_allocator.cc:962] Sum Total of in-use chunks: 2.60GiB\r\n2020-05-30 14:14:25.068505: I tensorflow/core/common_runtime/bfc_allocator.cc:964] total_region_allocated_bytes_: 3138866176 memory_limit_: 3138866380 available bytes: 204 curr_region_allocation_bytes_: 6277732864\r\n2020-05-30 14:14:25.074911: I tensorflow/core/common_runtime/bfc_allocator.cc:970] Stats:\r\nLimit:                  3138866380\r\nInUse:                  2792979456\r\nMaxInUse:               3138865664\r\nNumAllocs:                  790450\r\nMaxAllocSize:           2361825536\r\n\r\n2020-05-30 14:14:25.081733: W tensorflow/core/common_runtime/bfc_allocator.cc:429] **************************************************************__*****x*************x*_********xx*___\r\n2020-05-30 14:14:25.087205: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at conv_grad_input_ops.cc:1108 : Resource exhausted: OOM when allocating tensor with shape[16,64,170,170] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n2020-05-30 14:14:25.096432: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Resource exhausted: OOM when allocating tensor with shape[16,64,170,170] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node gradients_13/block1_conv2_13/convolution_grad/Conv2DBackpropInput}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\n\r\n\r\n", "comments": ["@omertortumlu \r\nPlease share simple stand alone code that replicates the issue faced.", "`import tensorflow as tf\r\nimport keras\r\nfrom keras import backend as K\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.metrics import categorical_crossentropy,binary_crossentropy,mean_squared_error,mean_absolute_error,mean_absolute_percentage_error,mean_squared_logarithmic_error,squared_hinge,hinge,categorical_hinge,logcosh,sparse_categorical_crossentropy,kullback_leibler_divergence,poisson,cosine_proximity\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Model, Sequential, model_from_json, load_model\r\nfrom keras.layers import Dense, GlobalAveragePooling2D, Dropout, SeparableConv2D, BatchNormalization, Activation, Conv2D, MaxPooling2D, Flatten, Input\r\nimport os\r\nimport random\r\nimport numpy as np\r\nimport skimage.io as io\r\nfrom skimage import color\r\nfrom skimage.transform import resize\r\nimport matplotlib.pyplot as plt\r\nimport time\r\n\r\nfrom sklearn.metrics import confusion_matrix\r\nimport itertools\r\nprint(\"\\n##############################    INFO    ####################################\\n\")\r\nprint(\"Tensorflow version: \", tf.__version__ ,\"\\nKeras version: \",keras.__version__)\r\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\nprint(\"\\n##############################################################################\\n\")\r\nclasses = []\r\noriginal_folds = []\r\nfolds = []\r\ninput_size = 3\r\nmdls = []\r\ntsts = []\r\ntstlbl = []\r\nfilename = \"\"\r\ndc=7 #:digit counts\r\n\r\ndef baseModel(self, mdl, a, o, loss, imagenet, lr, img_size, batch_size, epochs, convert_rgb, augment, r_r, h_s_r, w_s_r, z_r, s_r, h_f, f_m, datasetPath, fileStructure, original_k, original_percent):\r\n    mainPath = datasetPath\r\n    os.chdir(mainPath)\r\n    if(imagenet):\r\n        weight = 'imagenet'\r\n    else:\r\n        weight = None\r\n    global classes\r\n    global numberOfFiles\r\n    if(fileStructure):  # Filestructure value is true=folded structure\r\n        folds = sorted(os.listdir())\r\n        #print(folds)\r\n        os.chdir(folds[0]+'/train')\r\n        classes = sorted(os.listdir())\r\n        numberOfFiles = len(classes)\r\n    else:  # Original file structure\r\n        classes = []\r\n        classes = sorted(os.listdir())\r\n        print(classes)\r\n        numberOfFiles = len(classes)\r\n        # Original data is reading\r\n        foldedDataset = []\r\n        for j, k in enumerate(classes):\r\n            directory = mainPath+'/'+k\r\n            img_names = next(os.walk(directory))\r\n            print(directory+\" -> \" +\r\n                  str(len(img_names[2]))+\" images found.\")\r\n            tmpClass = []\r\n            for cs in img_names[2]:\r\n                imgLbl = []\r\n                tmpPath = ''+directory+'/'+cs\r\n                img = io.imread(tmpPath)\r\n                if convert_rgb == True:\r\n                    img2 = color.gray2rgb(img, alpha=None)\r\n                    img2 = resize(img2, (img_size, img_size, input_size),\r\n                                  anti_aliasing=True)\r\n                else:\r\n                    img2 = resize(img, (img_size, img_size, input_size),\r\n                                  anti_aliasing=True)\r\n                imgLbl.append(img2)\r\n                imgLbl.append(cs)\r\n                imgLbl.append(j)\r\n                tmpClass.append(imgLbl)\r\n            foldedDataset.append(tmpClass)\r\n\r\n        # Original dataset is split into file structure\r\n\r\n        folds = []\r\n        for f in range(original_k):  # fold1-fold2-fold3\r\n            fold = []\r\n            trainClasses = []\r\n            testClasses = []\r\n            for i in range(len(classes)):  # shuffle for everyclass\r\n                random.shuffle(foldedDataset[i])\r\n                tmpTrain = []\r\n                tmpTest = []\r\n                # print(len(foldedDataset[i]))\r\n                for j in range(len(foldedDataset[i])):\r\n                    if(j < int(len(foldedDataset[i])*(1-original_percent/100))):\r\n                        tmpTrain.append(foldedDataset[i][j])\r\n                    else:\r\n                        tmpTest.append(foldedDataset[i][j])\r\n                trainClasses.append(tmpTrain)\r\n                testClasses.append(tmpTest)\r\n\r\n            fold.append(trainClasses)\r\n            fold.append(testClasses)\r\n            folds.append(fold)\r\n    global original_folds\r\n    original_folds = folds\r\n    # io.imshow(path)\r\n    # plt.show()\r\n\r\n    def one_hot_encode(labels):\r\n        encoded = np.zeros((len(labels), numberOfFiles))\r\n        for idx, val in enumerate(labels):\r\n            encoded[idx][val] = 1\r\n        return encoded\r\n\r\n    def data_prep(v, type):  # type Train/Test # v fold1-fold2-fold3-fold4-fold5\r\n        try:\r\n            train_labels = []\r\n            image_dataset = []\r\n            print(classes)\r\n            for j, k in enumerate(classes):\r\n                directory = mainPath+'/'+v+'/'+type+'/'+k\r\n                class_names = next(os.walk(directory))\r\n                print(directory+\" -> \" +\r\n                      str(len(class_names[2]))+\" files found.\")\r\n                for cs in class_names[2]:\r\n                    path = ''+directory+'/'+cs\r\n                    # print(path)\r\n                    img = io.imread(path)\r\n                    img = resize(img, (img_size, img_size, input_size),\r\n                                 anti_aliasing=True)\r\n                    image_dataset.append(img)\r\n                    train_labels.append(j)\r\n            image_dataset = np.asarray(image_dataset)\r\n            encoded = one_hot_encode(train_labels)\r\n            print(type+' data read.')\r\n            return image_dataset, encoded\r\n        except Exception as err:\r\n            print(str(err)+\"\\nFault in data_prep.\")\r\n            pass\r\n\r\n    def original_data_prep(v):\r\n        global train_labels\r\n        global test_image_dataset\r\n        train_image_dataset = []\r\n        test_image_dataset = []\r\n        train_labels = []\r\n        test_labels = []\r\n        for j, tmpcls in enumerate(v[0]):  # train\r\n            for k, tmpimglbl in enumerate(tmpcls):\r\n                img = tmpimglbl[0]\r\n                train_image_dataset.append(img)\r\n                label = tmpimglbl[2]\r\n                train_labels.append(label)\r\n        for j, tmpcls in enumerate(v[1]):  # test\r\n            for k, tmpimglbl in enumerate(tmpcls):\r\n                img = tmpimglbl[0]\r\n                test_image_dataset.append(img)\r\n                label = tmpimglbl[2]\r\n                test_labels.append(label)\r\n        train_image_dataset = np.asarray(train_image_dataset)\r\n        encoded_train = one_hot_encode(train_labels)\r\n        test_image_dataset = np.asarray(test_image_dataset)\r\n        encoded_test = one_hot_encode(test_labels)\r\n        return train_image_dataset, encoded_train, test_image_dataset, encoded_test\r\n\r\n    def mdlcmp(opt, act):\r\n        try:\r\n            # setup model\r\n            if(mdl == 'MobileNet'):\r\n                from keras.applications.mobilenet import MobileNet\r\n                from keras.applications.mobilenet import preprocess_input\r\n                base_model = MobileNet(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'Xception'):\r\n                from keras.applications.xception import Xception\r\n                from keras.applications.xception import preprocess_input\r\n                base_model = Xception(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'VGG16'):\r\n                from keras.applications.vgg16 import VGG16\r\n                from keras.applications.vgg16 import preprocess_input\r\n                base_model = VGG16(weights=weight, include_top=False,\r\n                                   input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'VGG19'):\r\n                from keras.applications.vgg19 import VGG19\r\n                from keras.applications.vgg19 import preprocess_input\r\n                base_model = VGG19(weights=weight, include_top=False,\r\n                                   input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'ResNet'):\r\n                from keras.applications.resnet import ResNet152\r\n                from keras.applications.resnet import preprocess_input\r\n                base_model = ResNet152(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'ResNetV2'):\r\n                from keras.applications.resnet_v2 import ResNet152V2\r\n                from keras.applications.resnet_v2 import preprocess_input\r\n                base_model = ResNet152V2(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'InceptionResNetV2'):\r\n                from keras.applications.inception_resnet_v2 import InceptionResNetV2\r\n                from keras.applications.inception_resnet_v2 import preprocess_input\r\n                base_model = InceptionResNetV2(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'MobileNetV2'):\r\n                from keras.applications.mobilenet_v2 import MobileNetV2\r\n                from keras.applications.mobilenet_v2 import preprocess_input\r\n                base_model = MobileNetV2(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'DenseNet'):\r\n                from keras.applications.densenet import DenseNet\r\n                from keras.applications.densenet import preprocess_input\r\n                base_model = DenseNet(\r\n                    weights=weight, include_top=False, input_shape=(img_size, img_size, 3))\r\n            elif(mdl == 'NASnet'):\r\n                from keras.applications.nasnet import NASnet\r\n                from keras.applications.nasnet import preprocess_input\r\n                base_model = NASnet(weights=weight, include_top=False,\r\n                                    input_shape=(img_size, img_size, 3))\r\n            else:\r\n                from keras.applications.inception_v3 import InceptionV3\r\n                from keras.applications.inception_v3 import preprocess_input\r\n                \r\n                base_model = InceptionV3(\r\n                    weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\r\n\r\n            # Last layers flatten and activation\r\n            x = base_model.output\r\n            x = Flatten()(x)\r\n            predictions = Dense(numberOfFiles, activation=act)(x)\r\n            model = Model(inputs=base_model.input, outputs=predictions)\r\n            # Model Compile\r\n\r\n            model.compile(optimizer=rate_opt(opt, lr),\r\n                          loss=loss,\r\n                          metrics=['accuracy'])\r\n            #callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc', mode='max', patience=3, verbose=1)]\r\n            return model\r\n        except Exception as err:\r\n            print(str(err)+\"\\nFault in mdlcmp.\")\r\n            pass\r\n\r\n    def rate_opt(opt, lr):\r\n        try:\r\n            if(opt == 'SGD'):\r\n                from keras.optimizers import SGD\r\n                optim = SGD(learning_rate=lr)\r\n            elif(opt == 'RMSprop'):\r\n                from keras.optimizers import RMSprop\r\n                optim = RMSprop(learning_rate=lr)\r\n            elif(opt == 'Adagrad'):\r\n                from keras.optimizers import Adagrad\r\n                optim = Adagrad(learning_rate=lr)\r\n            elif(opt == 'Nadam'):\r\n                from keras.optimizers import Nadam\r\n                optim = Nadam(learning_rate=lr)\r\n            elif(opt == 'Adadelta'):\r\n                from keras.optimizers import Adadelta\r\n                optim = Adadelta(learning_rate=lr)\r\n            elif(opt == 'Adamax'):\r\n                from keras.optimizers import Adamax\r\n                optim = Adamax(learning_rate=lr)\r\n            else:\r\n                from keras.optimizers import Adam\r\n                optim = Adam(learning_rate=lr)\r\n            return optim\r\n            pass\r\n        except Exception as err:\r\n            print(\r\n                str(err)+\"\\nFault in rate_opt. Please your input like 0.1--0.01--0.0001\")\r\n            pass\r\n\r\n    def datagen():\r\n        try:\r\n            datagen = ImageDataGenerator(\r\n                rotation_range=r_r,\r\n                zoom_range=z_r,\r\n                width_shift_range=w_s_r,\r\n                height_shift_range=h_s_r,\r\n                shear_range=s_r,\r\n                horizontal_flip=h_f,\r\n                fill_mode=f_m)\r\n            return datagen\r\n        except Exception as err:\r\n            print(str(err)+\"\\nFault in datagen\")\r\n            pass\r\n\r\n    def train_base_model():\r\n        try:\r\n            scoreses = \"\"\r\n            t = 0\r\n            totalScore = 0\r\n            global mdls\r\n            global tsts\r\n            global tstlbl\r\n            mdls.clear()\r\n            tsts.clear()\r\n            tstlbl.clear()\r\n            for i, v in enumerate(folds):  # folds\r\n                start_time = time.time()\r\n                print(\"!!!!!!!!!!!!!!! \", mdl, a, o,\r\n                      \"fold\"+str(i+1)+\" training !!!!!!!!!!!!!!!!!!!\")\r\n                # Data preparing\r\n                if(fileStructure):                    \r\n                    train, train_label = data_prep(v, 'train')\r\n                    test, test_label = data_prep(v, 'test')\r\n                else:\r\n                    train, train_label, test, test_label = original_data_prep(\r\n                        v)\r\n                model = mdlcmp(o, a)\r\n                # Data augmentation yap\u0131l\u0131yor\r\n                aug = datagen()\r\n                epoch_step = ((len(train) * (augment)) // batch_size) + 1\r\n                print('Steps per epoch:', epoch_step,\r\n                      ' Train length:', len(train))\r\n\r\n                model.fit_generator(aug.flow(train, train_label, batch_size=batch_size),\r\n                                    steps_per_epoch=epoch_step,\r\n                                    epochs=epochs)\r\n                end_time = time.time()\r\n                print(a, o, 'TestFold {', i+1, '}')\r\n                scores = model.evaluate(test, test_label, verbose=1)\r\n                train_time=str(round((end_time-start_time),dc))\r\n                print('Train Time: ',train_time)\r\n                print('Test Loss: ', scores[0])\r\n                print('Test Accuracy: ', scores[1], '\\n')\r\n                totalScore += scores[1]\r\n                mdls.append(model)\r\n                pred = model.predict(test, verbose=0)\r\n                tsts.append(pred)\r\n                tstlbl.append(test_label)\r\n                t += 1\r\n                sc = \"{Fold \" + str(i+1)+\"} Accuracy: \" + str(\r\n                    round(scores[1],dc)) + \" Loss: \" + str(round(scores[0],dc))+\" Time: \"+train_time + \"\\n\"\r\n                self.allModels.addItem(\"Fold\"+str(i+1))\r\n                scoreses += sc\r\n                del model\r\n            self.scores_screen.setText(scoreses)\r\n            self.progressBar.setValue(int(totalScore*100)/t)\r\n            pass\r\n\r\n        except Exception as err:\r\n            print(str(err)+\"\\nFault in train_base_model\")\r\n            pass\r\n\r\n`\r\nI am developing an interface for transfer learning.", "@omertortumlu \r\nPlease provide indented code, indentation error faced while i ran the code shared. please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/deaa7a6a98309337946099a9de8e31d8/untitled210.ipynb) or provide a colab gost witht he error faced.", "The code works integrated with an interface written in PyQt. The interface only sends the parameters to be used. The purpose of the code is to work locally. :(  I am trying to integrate the code that runs in Colab into the local.", "Looks like your gpu memory is exhausted for current batch size. Ideally you want to lower your batch size on encountering memory allocation issues and use batch size which completes your training.\r\nYou may also try to  limit your gpu memory usage as well.\r\n```python\r\n#For TF 1.X\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n# your code\r\n```", "Thank you for your interest, I use this function to empty the GPU before starting each train operation.\r\n```\r\ndef reset_keras():\r\n    print(\"Start reset keras\")\r\n    from keras.backend.tensorflow_backend import set_session\r\n    from keras.backend.tensorflow_backend import clear_session\r\n    from keras.backend.tensorflow_backend import get_session\r\n    import tensorflow\r\n    import gc\r\n    sess = get_session()\r\n    clear_session()\r\n    sess.close()\r\n    sess = get_session()\r\n    print(\"********ZeroGPU**********\")\r\n    print(gc.collect()) # if it's done something you should see a number being outputted\r\n    print(\"*************************\")\r\n    # use the same config as you used to create the session\r\n    config = tensorflow.ConfigProto()\r\n    config.gpu_options.per_process_gpu_memory_fraction = 1\r\n    config.gpu_options.allow_growth = True\r\n    config.gpu_options.visible_device_list = \"0\"\r\n    set_session(tensorflow.Session(config=config))\r\n```", "I would recommend to use lower batch size for your computation.\r\nAlso you want to put gpu memory limitation code on the top of your script.\r\n```python\r\n# Top of code\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsess = tf.Session(config=config)\r\n# Rest of your code\r\n```\r\nThanks!\r\n\r\nFor further support please try [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow)"]}, {"number": 40023, "title": "OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed", "body": "**code below**\r\n```\r\n@tf.function\r\n  def train_step(self, batch):\r\n    with tf.GradientTape() as disc_tape, tf.GradientTape() as infe_tape, tf.GradientTape() as gener_tape:\r\n      loss_encoder, loss_decoder, loss_disc = self.compute_loss(batch)\r\n\r\n      grad_encoder = gener_tape.gradient(loss_encoder, self.model.inference_net.trainable_variables)\r\n      grad_decoder = infe_tape.gradient(loss_decoder, self.model.generative_net.trainable_variables)\r\n      grad_disc = disc_tape.gradient(loss_disc, self.model.discriminator.trainable_variables)\r\n\r\n      self.infe_optimizer.apply_gradients(zip(grad_encoder, self.model.inference_net.trainable_variables))\r\n      self.genr_optimizer.apply_gradients(zip(grad_decoder, self.model.generative_net.trainable_variables))\r\n      self.disc_optimizer.apply_gradients(zip(grad_disc, self.model.discriminator.trainable_variables))\r\n\r\n      return loss_encoder, loss_decoder, loss_disc\r\n\r\n  @tf.function\r\n  def train(self, epoch):\r\n    loss_history = tf.zeros([3,0])\r\n    for batch in self.train_dataset:\r\n      loss_encoder, loss_decoder, loss_disc = self.train_step(batch)\r\n      loss_each = tf.expand_dims(tf.stack([loss_encoder, loss_decoder, loss_disc], axis=0), 1)\r\n      loss_history = tf.concat([loss_history, loss_each], axis=1)\r\n\r\n    mean_loss_encoder, mean_loss_decoder, mean_loss_disc = tf.reduce_mean(loss_history, axis=1)\r\n    tf.summary.scalar('encoder loss', data=mean_loss_encoder, step=epoch, description='train_loss')\r\n    tf.summary.scalar('decoder loss', data=mean_loss_decoder, step=epoch, description='train_loss')\r\n    tf.summary.scalar('discriminator loss', data=mean_loss_disc, step=epoch, description='train_loss')\r\n    print('encoder training loss %.5f' % mean_loss_encoder, 'decoder training loss %.5f' % mean_loss_decoder, 'discriminator training loss %.5f' % mean_loss_disc)\r\n\r\n  @tf.function\r\n  def validation_step(self, batch):\r\n     return self.compute_loss(batch, training=tf.constant(False, tf.bool))\r\n\r\n  @tf.function\r\n  def validation(self, epoch):\r\n    '''produce mean loss for epoch and append to self.val_loss_monior'''\r\n\r\n    loss_history = tf.zeros([3,0])\r\n    for batch in self.validation_dataset:\r\n        loss_encoder, loss_decoder, loss_disc = self.validation_step(batch)\r\n        loss_each = tf.expand_dims(tf.concat([loss_encoder, loss_decoder, loss_disc], axis=0), 1)\r\n        loss_history = tf.concat([loss_history, loss_each], axis=1) \r\n    mean_loss_each = tf.reduce_mean(loss_history, axis=1)\r\n    print('encoder validation loss %.5f' % mean_loss_each[0], 'decoder validation loss %.5f' % mean_loss_each[1], 'discriminator validation loss %.5f' % mean_loss_each[2])\r\n\r\n    tf.summary.scalar('encoder loss', data=mean_loss_each[0], step=epoch, description='val_loss')\r\n    tf.summary.scalar('decoder loss', data=mean_loss_each[1], step=epoch, description='val_loss')\r\n    tf.summary.scalar('discriminator loss', data=mean_loss_each[2], step=epoch, description='val_loss')\r\n\r\n    # self.val_loss_monior: (3, self.patience), each row store history loss for each network\r\n    loss_each = tf.expand_dims(mean_loss_each, 1)\r\n    if epoch + 1 > self.patience:\r\n      is_less =  self.val_loss_monior < loss_each\r\n      is_less_encoder, is_less_decoder, is_less_discriminator = list(map(lambda x: tf.raw_ops.Any(input=tf.cast(tf.squeeze(x), tf.bool), axis=0), tf.split(is_less, 3, axis=0)))\r\n\r\n      if not is_less_encoder:\r\n         self.infe_optimizer.learning_rate *= self.decay_rate\r\n      if not is_less_decoder:\r\n         self.genr_optimizer.learning_rate *= self.decay_rate\r\n      if not is_less_discriminator:\r\n         self.disc_optimizer.learning_rate *= self.decay_rate\r\n    \r\n    self.val_loss_monior = tf.slice(self.val_loss_monior, [0, 1], [3, self.patience - 1])\r\n    self.val_loss_monior = tf.concat([self.val_loss_monior, loss_each], axis=1)\r\n\r\n    tf.summary.scalar('encoder learning rate', data=self.infe_optimizer.learning_rate, step=epoch)\r\n    tf.summary.scalar('decoder learning rate', data=self.genr_optimizer.learning_rate, step=epoch)\r\n    tf.summary.scalar('discriminator learning rate', data=self.disc_optimizer.learning_rate, step=epoch)\r\n\r\n  def generate_images(self, epoch):\r\n    predictions = self.model.sample(16, training=False)\r\n    fig = plt.figure(figsize=(4,4))\r\n    for i in range(predictions.shape[0]):\r\n        plt.subplot(4, 4, i+1)\r\n        plt.imshow(predictions[i])\r\n        plt.axis('off')\r\n    plt.savefig('image_at_epoch_{:03d}.png'.format(epoch))\r\n    plt.show()\r\n\r\n  @tf.function\r\n  def fit(self):\r\n    self.val_loss_monitor = tf.zeros([3, self.patience])\r\n    for epoch in range(self.epochs):\r\n      start = time.time()\r\n      epoch = tf.cast(epoch, tf.int64)\r\n      self.train(epoch)\r\n      self.validation(epoch)\r\n\r\n      print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))\r\n      display.clear_output(wait=True)\r\n      if (epoch + 1) % 5 == 0:\r\n        self.generate_images(epoch)\r\n```\r\n**error massage below**\r\n```\r\n\r\n\r\nOperatorNotAllowedInGraphError: in user code:\r\n\r\n    <ipython-input-5-8af39e045349>:152 fit  *\r\n        self.train(epoch)\r\n    <ipython-input-5-8af39e045349>:90 train  *\r\n        mean_loss_encoder, mean_loss_decoder, mean_loss_disc = tf.reduce_mean(loss_history, axis=1)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:561 __iter__\r\n        self._disallow_iteration()\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:554 _disallow_iteration\r\n        self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled\r\n        \" decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n```\r\nThe problem should be \r\n```\r\nmean_loss_encoder, mean_loss_decoder, mean_loss_disc = tf.reduce_mean(loss_history, axis=1)\r\n```\r\nError is about iterating over `tf.Tensor` is not allowed.\r\nIs reduce_mean not working in tf.function?", "comments": ["Use a = tf.reduce_mean(loss_history, axis=1) , if want to use tensor element, use a[0], a[1], ..., it works. It look like that tensor can not catch separately."]}, {"number": 40022, "title": "why pd  converted to cpkt, pd model detection effect is poor, nothing can be detected", "body": "\r\n", "comments": ["@DENESTY,\r\nIn order to expedite the trouble-shooting process, could you please fill in the issue template along with the complete code to reproduce the issue reported here and the TensorFlow version you are using. Thanks!", "@amahendrakar \r\nsorry,expression error. I have train ssd-mobilenetv2 in tensorflow object detection api with my dataset.During training,i saw the image can be detection through the tensorboard.Then i use export_inference_graph.py to convert ckpt to pd .But when i use frozen_inference_graph.pb to detection. the result is very very bad .I use tensorflow-gpu==1.13.very thanks"]}, {"number": 40021, "title": "Update doc of `global_norm`", "body": "", "comments": ["@findmyway: What's wrong with the documentation? It is correctly documenting squared operation vs multiplication.", "Emm, I thought it was `l2_loss`. Sorry to disturb."]}]