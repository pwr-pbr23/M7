[{"number": 32495, "title": "[r2.0.0-rc1] Converting to TFLite format: Invalid quantization params for op RESHAPE", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.2 LTS**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version (use command below): **2.0.0-rc1 commit 59bf33**\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: NVidia 1080Ti / 11G\r\n\r\n**Describe the current behavior**\r\n\r\nConversion of TF2.0 function containing `softmax` and `reshape` ops to TFLite format fails with the following RuntimeError. Source code of the program is listed in the `Code` section below.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-ec9775ede022> in <module>\r\n----> 1 run()\r\n\r\n~/mironov/hbtest/tflite_softmax_bug_v2.py in run()\r\n     20   converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n     21\r\n---> 22   tflite_model = converter.convert()\r\n     23   model_file = \"/tmp/tflite_softmax_bug_v2.tflite\"\r\n     24   open(model_file, \"wb\").write(tflite_model)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    448     if self._is_calibration_quantize():\r\n    449       result = self._calibrate_quantize_model(result, constants.FLOAT,\r\n--> 450                                               constants.FLOAT)\r\n    451\r\n    452     return result\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type)                                                                             \r\n    237     return calibrate_quantize.calibrate_and_quantize(\r\n    238         self.representative_dataset.input_gen, inference_input_type,\r\n--> 239         inference_output_type, allow_float)\r\n    240\r\n    241   def _get_base_converter_args(self):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float)                                                                   \r\n     76     return self._calibrator.QuantizeModel(\r\n     77         np.dtype(input_type.as_numpy_dtype()).num,\r\n---> 78         np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py in QuantizeModel(self, input_py_type, output_py_type, allow_float)                                                     \r\n    113\r\n    114     def QuantizeModel(self, input_py_type, output_py_type, allow_float):\r\n--> 115         return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)                                                                                            \r\n    116 CalibrationWrapper_swigregister = _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_swigregister\r\n    117 CalibrationWrapper_swigregister(CalibrationWrapper)\r\n\r\nRuntimeError: Invalid quantization params for op RESHAPE at index 1 in subgraph 0\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n`converter.convert()` call finishes without errors\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\nFile: `tflite_softmax_bug_v2.py`\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(shape=[10], dtype=tf.float32, name='inp0')])\r\ndef model(inp0):\r\n  res = tf.nn.softmax(inp0)\r\n  res = tf.reshape(res, [1, 10])\r\n  print(res.shape)\r\n  return res\r\n\r\ndef run():\r\n  def _representative_dataset_gen():\r\n    for i in range(10):\r\n      yield [np.random.random_integers(0, 9, size=[10]).astype('float32'),]\r\n\r\n  cfunc = model.get_concrete_function()\r\n  converter = tf.lite.TFLiteConverter.from_concrete_functions([cfunc])\r\n  converter.representative_dataset = _representative_dataset_gen\r\n  converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\n  tflite_model = converter.convert()\r\n  model_file = \"/tmp/tflite_softmax_bug_v2.tflite\"\r\n  open(model_file, \"wb\").write(tflite_model)\r\n\r\n  interpreter = tf.lite.Interpreter(model_path=model_file)\r\n  interpreter.allocate_tensors()\r\n\r\nif __name__ == '__main__':\r\n  run()\r\n\r\n```\r\n\r\n**Other info / logs**\r\nN/A", "comments": ["The error can be also reproduced in  tensorflow nightly gpu version 1.15,\r\n```\r\n2019-09-26 09:57:52.194571: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2019-09-26 09:57:52.194601: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4 nodes (-1), 3 edges (0), time = 1.698ms.\r\n2019-09-26 09:57:52.194611: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4 nodes (0), 3 edges (0), time = 0.198ms.\r\nTraceback (most recent call last):\r\n  File \"tflite_softmax_bug_v1.py\", line 33, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/home/hh1208-kang/venv_py2_tf_nightly/local/lib/python2.7/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Invalid quantization params for op RESHAPE at index 1 in subgraph 0\r\n```\r\nwith this code.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nexport_dir = 'test_saved_model'\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_dir)\r\n\r\nipts = tf.placeholder(dtype=tf.float32, shape=(10), name='x')\r\nres = tf.nn.softmax(ipts)\r\nres = tf.reshape(res, [1, 10])\r\nprint(res.shape)\r\n\r\ndef _representative_dataset_gen():\r\n    for i in range(10):\r\n      yield [np.random.random_integers(0, 9, size=[10]).astype('float32'),]\r\n\r\n\r\n\r\nwith tf.Session() as sess:\r\n\tsess.run(tf.global_variables_initializer())\r\n\r\n\tbuilder.add_meta_graph_and_variables(sess, [\"test\"])\r\n\tbuilder.save(as_text=True)\r\n\t\r\n\tconverter = tf.lite.TFLiteConverter.from_session(sess, [ipts], [res])\r\n\r\n\tconverter.representative_dataset = _representative_dataset_gen\r\n\tconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\t\r\n\ttflite_model = converter.convert()\r\n```\r\n\r\nWhen I look into the converting code, it seems that the error occurred during quantizing the output tensor of the op RESHAPE. The converter supposed to copy the quantization parameters of the input tensors, scale, zero_point, min and max. However, input tensor seems not to have the four quantization parameters.\r\n```\r\nInputs {\r\n  0: Softmax, float[10]\r\n  1: shape, int32[2] = {1, 10}\r\n}\r\nOutputs {\r\n  0: Reshape, float[1x10]\r\n}\r\nOptions {\r\n  new_shape\r\n}\r\n```\r\n", "Firstly, the converter set the quantization parameters of the output tensor of SOFTMAX op. Let's name it as A. The quantization parameter of the tensor A is set to {scale 0.003906, zero_point -128.000000}. But min and max are not set.\r\n\r\nWhen the converter tries to quantize the output tensor of the RESHAPE op, which follows the SOFTMAX op, it reads the tensor A as the input tensor. It tries to use the quantization parameters of tensor A, however, it fails since the min and max of the tensor A are not set.\r\n\r\nThe converter code says \"property.restriction_on_output\" is False for  RESHAPE op, and it copies the quantization parameters of the input tensor. \"property.restriction_on_output\" is True for SOFTMAX op and the code do not copies quantization parameters from the input tensor. But the code only provided scale and zero_point parameter. How can I set min and max values?", "I was currently working with modifying the tensorflow r1.14 version. Thus I'm not sure about other versions. \r\n\r\nWhen I changed the following code \r\nhttps://github.com/tensorflow/tensorflow/blob/be5eb917fdea8a99ec8d5eb9428f2e8866858560/tensorflow/lite/tools/optimize/quantize_model.cc#L550\r\n\r\n```\r\n  } else if (tensor_property.restriction) {\r\n    const auto scale_and_zp = tensor_property.restricted_value;\r\n    // Apply to output.\r\n    output_tensor->quantization = absl::make_unique<QuantizationParametersT>();\r\n    output_tensor->quantization->scale.push_back(scale_and_zp.first);\r\n    output_tensor->quantization->zero_point.push_back(scale_and_zp.second);\r\n    output_tensor->type = TensorType_INT8;\r\n  } else {\r\n```\r\nas\r\n```\r\n  } else if (property.restriction_on_output) {\r\n    const auto scale_and_zp = property.restricted_value_on_output;\r\n    // Apply to output.\r\n    output_tensor->quantization = absl::make_unique<QuantizationParametersT>();\r\n    output_tensor->quantization->scale.push_back(scale_and_zp.first);\r\n    output_tensor->quantization->zero_point.push_back(scale_and_zp.second);\r\n    output_tensor->type = TensorType_INT8;\r\n\r\n    // patch: read min/max from the input tensor\r\n    const int input_index = op->inputs[property.input_indexes[0]];\r\n    TensorT* input_tensor = subgraph->tensors[input_index].get();   \r\n    output_tensor->quantization->min.push_back(input_tensor->quantization->min[0]);\r\n    output_tensor->quantization->max.push_back(input_tensor->quantization->max[0]);\r\n  } else {\r\n```\r\n, providing the min and max values.\r\n\r\nThe converter completed the converting. However, I'm not sure this is safe or not.\r\n\r\nWhen I checked the parameter values of the SOFTMAX op, it shows \r\n```\r\nscale 0.003906, zp -128.000000\r\nmin 0.000000, max 9.000000\r\n```\r\n\r\n\r\nThe output is shown as follow.\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nNote the output min/max is different from the input min/max for op RESHAPE at index 1 in subgraph 0. This is legal but should happens rarely.\r\n```", "Considering the tensorflow quantization specification (https://www.tensorflow.org/lite/performance/quantization_spec), \r\n```\r\nSOFTMAX\r\n  Input 0:\r\n    data_type  : int8\r\n    range      : [-128, 127]\r\n    granularity: per-tensor\r\n  Output 0:\r\n    data_type  : int8\r\n    range      : [-128, 127]\r\n    granularity: per-tensor\r\n    restriction: (scale, zero_point) = (1.0 / 256.0, -128)\r\n```\r\nit seems better to use fixed min/max values as follows, rather than using the same min/max from the input tensor.\r\n```\r\n    const float scale = output_tensor->quantization->scale[0];\r\n    const int32_t zp = output_tensor->quantization->zero_point[0];\r\n    float min = (-128 - zp) * scale;\r\n    float max = (127 - zp) * scale;\r\n    output_tensor->quantization->min.push_back(min);\r\n    output_tensor->quantization->max.push_back(max);\r\n    printf(\"scale=%g, zero_point=%d, min=%g, max=%g\\n\",\r\n    output_tensor->quantization->scale[0], output_tensor->quantization->zero_point[0],\r\n    output_tensor->quantization->min[0], output_tensor->quantization->max[0]);\r\n```", "Just a little update.\r\nI'm working with the v2.0.0, and I think that following one line modification seems better.\r\nhttps://github.com/tensorflow/tensorflow/blob/64c3d382cadf7bbe8e7e99884bede8284ff67f56/tensorflow/lite/tools/optimize/quantize_model.cc#L528\r\n```\r\n    //output_tensor->quantization = absl::make_unique<QuantizationParametersT>();\r\n    if (!output_tensor->quantization){\r\n        output_tensor->quantization = absl::make_unique<QuantizationParametersT>();\r\n    }\r\n```\r\nWhen I checked with my model, it seems that tensors have min/max parameters of their own at this stage. So it seems enough to prevent the initialization of the quantization parameter, when it already had been initialized. I expect that this also works in TF1.1x versions.\r\n\r\np.s. \r\nI am still suspicious about my answer. I suspect that there is a graph_transformation which should prevent this before happening (tensorflow/lite/toco/graph_transformations/hardcode_min_max.cc). But it does not. My modification made tflite_softmax_bug_v2.py converted, however, it does not work with my model. \r\n\r\nWhen I tried to quantize my model, it made the same error. The modification also removed the error for my model, however, it made another run-time error in running the converted tflite model.\r\n\r\n```\r\nRuntimeError: tensorflow/lite/kernels/kernel_util.cc:119 std::abs(input_product_scale - bias_scale) <= 1e-6 * std::min(input_product_scale, bias_scale) was not true.Node number 458 (FULLY_CONNECTED) failed to prepare.\r\n```", "How is it going for now?", "> How is it going for now?\r\n\r\nContinued in this issue\r\nhttps://github.com/tensorflow/tensorflow/issues/33233", "Any update about this issue? Reshape is very common operation in DNN models -- very strange to see that this problem is just skipped by Tensorflow Team about six months...", "I have a similar problem, my model used MobileNet. When trying to optimize to int8 model under TF2.0, it raises: \r\n\r\n**return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Invalid quantization params for op RESHAPE at index 22 in subgraph 0**\r\n\r\nI found the error is came fome [quantize_model.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantize_model.cc#L638). It seems like the op before RESHAPE can not find max or min value, which are neccary for int quantilize. Howerer I had quantized this model(same pb file) to int8 tflite successfully. So I consider this error may an bug in TF2.0.\r\n\r\nAfter chaning to **TF2.1**, the error not rasized. And the output of tflite is correct.\r\n", "We struggled in tensorflow-gpu 2.0.0 also with this bug. For us, the issue was resolved with an update from version 2.0.0 to 2.2.0.", "@grwlf \r\nCould you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32495\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32495\">No</a>\n"]}, {"number": 32494, "title": "edit for clearity, changed \"implied\" to \"applied\"", "body": "", "comments": []}, {"number": 32493, "title": "How to determine the shape of input and output nodes in graph?  (C++) ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Operation system: Ubuntu 18.04\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the current behavior**\r\n\r\nAt first, I tried to implement the following pipeline:\r\n\r\n- 1a) I converted the model from keras (.hdf5) to tensorflow (.pb) and saved using the `tf.saved_model.simple_save` function in Python.\r\n- 2a) I loaded the model using `tensorflow::LoadSavedModel`\r\n- 3a) I determined a shape using the following code:\r\n\r\n```C++\r\nconst string export_dir = \"/home/user/MyBuild/build_tftest2/models\";\r\n SavedModelBundle bundle; RunOptions run_options;\r\nstatus = LoadSavedModel(sopt, run_options, export_dir,\r\n               {kSavedModelTagServe}, &bundle);\r\nif (status.ok())\r\n{\r\n    GraphDef graph = bundle.meta_graph_def.graph_def();\r\n    auto shape = graph.node().Get(0).attr().at(\"shape\").shape();\r\n    for (int i = 0; i < shape.dim_size(); i++) {\r\n        std::cout << shape.dim(i).size()<<std::endl;\r\n    }\r\n}\r\n```\r\nLater I was wishing to use optimal model, therefore I used another pipeline:\r\n\r\n - 1b) I converted the model from keras (.hdf5) to tensorflow (.pb) using \r\n```python\r\nfrozen_graph = convert_variables_to_constants(session, input_graph_def,\r\noutput_names, freeze_var_names)\r\n```\r\n and saved in Python using:\r\n\r\n```python\r\ntf.train.write_graph(frozen_graph, \"model\", \"tf_model.pb\", as_text=False)\r\n```\r\n- 2b) I optimized the model using the `transform_graph` utility from tensorflow package.\r\n- 3b) I loaded my graph using the following code (C++):\r\n```c++\r\nstatus = ReadBinaryProto(Env::Default(), \"/home/user/MySoftware/foreign code/netology_JN/Diplom/Models/optimized/optim_model.pb\", &graph_def);\r\n```\r\n- 4b) I tried to determine a shape using 3a) and an exception was thrown:\r\n\r\n\r\n```\r\n[libprotobuf FATAL /usr/local/include/google/protobuf/map.h:1064]\r\nCHECK failed: it != end(): key not found: shape terminate called after\r\nthrowing an instance of 'google::protobuf::FatalException' what():\r\nCHECK failed: it != end(): key not found: shape 14:21:36: The program\r\nhas unexpectedly finished.\r\n```\r\n", "comments": ["@Pin80 ,\r\nCan you please provide a simple and standalone code to reproduce the issue?Thanks!", "I solved this issue.\r\nTensorflow utility transform_graph didn't remove all unused tensors, when i called command: \r\n./transform_graph \\\r\n--in_graph='/home/user/MySoftware/foreign code/netology_JN/Diplom/Models/Raw/frozen_model.pb' \\\r\n--out_graph='/home/user/MySoftware/foreign code/netology_JN/Diplom/Models/optimized/optim_model.pb' \\\r\n--inputs='input_1' \\\r\n--outputs='conv2d_19_2/Sigmoid' \\\r\n--transforms='\r\nstrip_unused_nodes(type=float, shape=\"-1,192,512,1\")\r\nremove_nodes(op=Identity, op=CheckNumerics)\r\nfold_old_batch_norms\r\nThe first node of my graph is not input tensor, therefore it don't have \"shape\" attribute.\r\nI found input tensor as second tensor in my graph and problem was solved.\r\n\r\nP.S. This my first expirience of writing issue to tensorflow. Do  i need to close it ?"]}, {"number": 32491, "title": "Getting SIGSEGV when passing Tensor to sample_from_datasets during checkpoint save", "body": "**System information**\r\n- Have I written custom code: \r\nyes\r\n- OS Platform and Distribution:\r\nLinux Ubuntu 16.04\r\n- TensorFlow installed from:\r\n`pip`\r\n- TensorFlow version (use command below):\r\nExact Tensorflow version: v1.12.0-0-ga6d8ffae09 1.12.0\r\n*Update:* Same in v1.12.3-0-g41e0a4f56c 1.12.3\r\n```\r\n$ pip freeze | grep tensorflow\r\nmesh-tensorflow==0.0.5\r\ntensorflow-datasets==1.2.0\r\ntensorflow-gan==1.0.0.dev0\r\ntensorflow-gpu==1.12.0\r\ntensorflow-metadata==0.14.0\r\ntensorflow-probability==0.5.0\r\n```\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GeForce GTX 1080\r\n\r\n**Describe the current behavior**\r\n\r\nPassing a tensor to `sample_from_datasets` which depends on the global step causes a SIGSEGV.\r\n\r\n**Describe the expected behavior**\r\n\r\nEither don't produce a SIGSEGV but a more meaningful error message or simply don't fail at all.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n# ...\r\nelif pretrain_cfg.schedule == PretrainSchedule.CONVERGE_LINEARLY:\r\n    a = tf.minimum(tf.constant(1.0, dtype=tf.float64, shape=(1,)), global_step / max_pretrain_steps)\r\n    b = tf.maximum(tf.constant(0.0, dtype=tf.float64, shape=(1,)), 1 - global_step / max_pretrain_steps)\r\n    weights = a * const_task_weights + b * pretrain_task_weights\r\n\r\nreturn tf.data.experimental.sample_from_datasets(datasets, weights=weights)\r\n```\r\n\r\nThe following setup works in comparison:\r\n\r\n```python\r\nif pretrain_cfg.schedule == PretrainSchedule.CONSTANT:\r\n    weights = tf.cond(\r\n        tf.greater(global_step, max_pretrain_steps),\r\n        true_fn=lambda: const_task_weights,\r\n        false_fn=lambda: pretrain_task_weights\r\n    )\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n...\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into /data/itranslate-translation/multi-problem/hi2en/model/512-3-1-1024/de2en.hi2en/c19cfad259cad911/model.ckpt.\r\nbash: line 1: 14168 Segmentation fault      (core dumped) env \"CUDA_VISIBLE_DEVICES\"=\"0\" \"PYCHARM_MATPLOTLIB_PORT\"=\"49858\" \"JETBRAINS_REMOTE_RUN\"=\"1\" \"PYTHONIOENCODING\"=\"UTF-8\" \"PYTHONPATH\"=\"/home/sfalk/.pycharm_helpers/pycharm..\r\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\r\n```\r\n", "comments": ["It turns out the problem is not with `tensorflow` directly - actually not at all.\r\n\r\nThe problem was because `const_task_weights` and `pretrain_task_weights` did not have the same shape. I did not validate the input and had a bug somewhere else.\r\n\r\nJust be aware that you might get this kind of error if the shapes do not match.\r\n\r\nI guess this cannot be checked or determined by `tensorflow` so that will be something the user has to take care of (citation needed).\r\n\r\n> See also: https://stackoverflow.com/questions/57919530/getting-interrupted-by-signal-11-sigsegv"]}, {"number": 32490, "title": "//tensorflow/contrib/distributions/python/kernel_tests/independent_test.py test fails with Assertion error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04 s390x\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\nFAIL: testMnistLikeDynamicShape (__main__.ProductDistributionTest)\r\ntestMnistLikeDynamicShape (__main__.ProductDistributionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 275, in testMnistLikeDynamicShape\r\n    self._testMnistLike(static_shape=False)\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 269, in _testMnistLike\r\n    rtol=1e-6, atol=0.)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 1073, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2303, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2272, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2207, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 1501, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 827, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=0\r\nMismatched value: a is different from b.\r\nnot close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))\r\nnot close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]\r\nnot close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]\r\nnot close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]\r\nnot close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]\r\ndtype = float64, shape = (4, 5, 10)\r\nMismatch: 2.5%\r\nMax absolute difference: 0.00059155\r\nMax relative difference: 1.29257756e-06\r\n x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,\r\n         -456.784984, -448.14827 , -453.583166, -486.295655,\r\n         -468.533898, -481.740375],...\r\n y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,\r\n         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],\r\n        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...\r\n\r\n======================================================================\r\nFAIL: testMnistLikeStaticShape (__main__.ProductDistributionTest)\r\ntestMnistLikeStaticShape (__main__.ProductDistributionTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/home/test/.local/lib/python2.7/site-packages/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 272, in testMnistLikeStaticShape\r\n    self._testMnistLike(static_shape=True)\r\n  File \"tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\", line 269, in _testMnistLike\r\n    rtol=1e-6, atol=0.)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 1073, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2303, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2272, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/home/test/.local/lib/python2.7/site-packages/tensorflow/python/framework/test_util.py\", line 2207, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 1501, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/home/test/.local/lib/python2.7/site-packages/numpy/testing/_private/utils.py\", line 827, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=0\r\nMismatched value: a is different from b.\r\nnot close where = (array([1, 2, 2, 2, 3]), array([3, 1, 2, 4, 4]), array([6, 1, 1, 8, 6]))\r\nnot close lhs = [-463.41407785 -467.87059292 -444.45405599 -469.33429804 -457.64799854]\r\nnot close rhs = [-463.41458 -467.87006 -444.45358 -469.3348  -457.6486 ]\r\nnot close dif = [0.00050345 0.00053677 0.00047323 0.00051031 0.00059155]\r\nnot close tol = [0.00046341 0.00046787 0.00044445 0.00046933 0.00045765]\r\ndtype = float64, shape = (4, 5, 10)\r\nMismatch: 2.5%\r\nMax absolute difference: 0.00059155\r\nMax relative difference: 1.29257756e-06\r\n x: array([[[-465.912459, -448.916315, -457.207675, -486.805523,\r\n         -456.784984, -448.14827 , -453.583166, -486.295655,\r\n         -468.533898, -481.740375],...\r\n y: array([[[-465.9126 , -448.9159 , -457.20764, -486.8053 , -456.7849 ,\r\n         -448.1483 , -453.5835 , -486.2955 , -468.53412, -481.74048],\r\n        [-472.38965, -483.41187, -464.7721 , -467.14288, -478.4115 ,...\r\n\r\n----------------------------------------------------------------------\r\nRan 10 tests in 1.019s\r\n\r\nFAILED (failures=2)\r\n```\r\n**Describe the expected behavior**\r\n```\r\nThe test should pass on s390x.\r\n```\r\n**Code to reproduce the issue**\r\n\r\n```\r\npython tensorflow/contrib/distributions/python/kernel_tests/independent_test.py\r\n```\r\n\r\n", "comments": ["Hi,\r\n\r\nAs mentioned above the test fails due to difference in the values of `expected_log_prob` and `actual_log_prob_x` above standard tolerance.\r\nThe test case passes if we increase the value atol from 0. to 0.001 in the file `tensorflow/contrib/distributions/python/kernel_tests/independent_test.py`\r\n\r\n```diff\r\n@@ -264,9 +265,10 @@ class ProductDistributionTest(test.TestCase):\r\n       self.assertAllEqual(image_shape, ind_event_shape)\r\n       self.assertAllEqual(sample_shape + batch_shape + image_shape, x_shape)\r\n       self.assertAllEqual(sample_shape + batch_shape, log_prob_x_shape)\r\n+\r\n       self.assertAllClose(expected_log_prob(x_, logits),\r\n                           actual_log_prob_x,\r\n-                          rtol=1e-6, atol=0.)\r\n+                          rtol=1e-6, atol=0.001)\r\n\r\n```\r\nWhen I compared the values of `expected_log_prob` and `actual_log_prob_x ` . I observed that there is a minor difference in the precision. Please find attached the logs for the same.\r\n\r\nCan you please suggest if it is safe to increase the value of atol? \r\n\r\n[independent_test_logs_s390x.txt](https://github.com/tensorflow/tensorflow/files/3609525/independent_test_logs_s390x.txt)\r\n[independent_test_logs_x86.txt](https://github.com/tensorflow/tensorflow/files/3609526/independent_test_logs_x86.txt)\r\n\r\n", "`atol` is absolute tolerance. If you are ok with results differing by that much, it is ok.\r\n\r\nI would try to see if the behavior is still happening on 1.15 release candidate, just to be sure.", "@mihaimaruseac I tried running the test case on 1.15 release candidate as well. The test case is still failing for s390x with the same error. \r\nThe test passes on Intel. \r\n", "In that case, if you are ok with results differing by that much, please increase tolerance parameter.\r\n\r\nUnfortunately, I'm not in the know of that code, so I'm going to unassign myself.", "Hi @martinwicke \r\nCan you please comment on this. ", "This this would be merely fixing a test, and since contrib won't be distributed with TensorFlow going forward, I think we should not worry about this. \r\n\r\nThere is no user visible impact of any change to fix this. If we had a fix for the underlying problem that would be interesting. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32490\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32490\">No</a>\n"]}, {"number": 32489, "title": "tensorflow does not detect 2nd GPU", "body": "Hi,\r\n\r\nI am trying to use 2 GPUs, tensorflow does not recognise the 2nd one. the 2nd GPU is working fine (in widows environment).\r\n\r\nusing tensorflow example: tf.keras.utils.multi_gpu_model, https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model\r\n\r\nI see the follwing error\r\n\r\nValueError: To call `multi_gpu_model` with `gpus=2`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. However this machine only has: ['/cpu:0', '/gpu:0']. Try reducing `gpus`.\r\n\r\nI can switch between the two graphic cards, with CUDA VISIBLE DEVICE , which sets one of the GPUs to GPU 0. however both can not be used. device 0 and device 1 are recognised however one GPU is only recognised.\r\n\r\nThe main GPU is RTX2070 (8GB) and 2nd GPU is GTX1050 (2GB). Before i submit i spent sometime searching for solution and did whatever I could find on the internet. drivers are up to date, 64bit version and latest versions of the software are installed. I dont see any issue, beside not appearing the 2nd GPU. The codes are working fine on first GPU, both have > 3.5 computational capacity.", "comments": ["@cyrus2018 ,\r\nCan you please mention the TF version being used and also provide a simple standalone code to replicate the issue.Thanks!", "TF GPU version is 1.13.1 , I had downgraded from  1.14 (due to seeing another error running the same code).\r\n\r\nbelow is the exact code I use. Regradless of number of GPUs, you see the same error (I have actually 2 installed).\r\n\r\nimport tensorflow as tf\r\nfrom keras.applications import Xception\r\nfrom keras.utils import multi_gpu_model\r\nimport numpy as np\r\n\r\nnum_samples = 1000\r\nheight = 224\r\nwidth = 224\r\nnum_classes = 1000\r\n\r\nwith tf.device('/cpu:0'):\r\n       model = Xception(weights=None,\r\n                         input_shape=(height, width, 3),\r\n                         classes=num_classes)\r\n\r\nparallel_model = multi_gpu_model(model, gpus=2)\r\nparallel_model.compile(loss='categorical_crossentropy',\r\n                           optimizer='rmsprop')\r\n\r\nx = np.random.random((num_samples, height, width, 3))\r\ny = np.random.random((num_samples, num_classes))\r\n\r\n\r\nparallel_model.fit(x, y, epochs=20, batch_size=256)\r\n\r\nmodel.save('my_model.h5')\r\n\r\nValueError: To call `multi_gpu_model` with `gpus=2`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. However this machine only has: ['/cpu:0', '/gpu:0']. Try reducing `gpus`.\r\n\r\n\r\n![TF version](https://user-images.githubusercontent.com/45702408/64855383-5730f780-d652-11e9-8378-d98162f75ed4.JPG)\r\n", "@cyrus2018 I don't think this is a bug in Tensorflow. \r\nCan you try to run the following lines to see how many gpus are visible to TF.\r\n```\r\nimport tensorflow as tf\r\ntf.test.gpu_device_name()\r\n```\r\nPlease check this [resource](https://www.quora.com/If-you-have-2-graphics-cards-and-1-computer-how-can-you-use-them-at-the-same-time). \r\nTL/DR: this resource mentions that you need `SLI bridge` for cooperation between GPUs. Also, expects `your graphics cards are the same model` and mentions `Older boards often did not allow the presence of a second graphic card.`. Please go through that resource.\r\n\r\nThanks!", "Thanks for the info, please see below:\r\nI checked the link you shared. I dont think to use TF, the GPUs should be the same exact model/brand. Is that the case? I have Gigabyte Z390M gaming motherboard, with two PCIs. the thing is, I can run my models right now on any of the GPUs below, with TF, no issues at all. both GPUs are pretty good..\r\n\r\nbut when it comes to using two GPUs the system does not identify the 2nd one...which is weird.. can not get GPU:1 recognised. \r\n\r\nIn [1]: import tensorflow as tf\r\n   ...: tf.test.gpu_device_name()\r\nOut[1]: '/device:GPU:0'\r\n\r\n\r\nAlso when i set:\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\r\nphysical_device_desc: \"device: 0, name: GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='1'\r\nphysical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:04:00.0, compute capability: 6.1\"\r\n\r\n\r\n", "Please see the attachments. \r\n\r\nnow using TF 1.9, setting os.environ[\"CUDA_VISIBLE_DEVICES\"]='1' casues no reaction from Tpython/TF.. i changed os.environ[\"CUDA_VISIBLE_DEVICES\"]to different digits and combination, any attempt to input 1, causes the Tf stop responding. \r\n\r\n![1](https://user-images.githubusercontent.com/45702408/65198559-228bc880-dab6-11e9-831f-10d81522fe5a.JPG)\r\n[Qyarry.docx](https://github.com/tensorflow/tensorflow/files/3628814/Qyarry.docx)\r\n", "@cyrus2018 could you share the printed log messages when you run `tf.test.gpu_device_name()`?", "Also could you try with TF 1.15.0rc1?", "> @cyrus2018 could you share the printed log messages when you run `tf.test.gpu_device_name()`?\r\n\r\nThe thing is my TF works and recognises with 1 GPU (GPU 0) it can either be the RTX1070 or GTX1050 (both workwith no issue). so single GPU no problem, GPU 1 never recognised.\r\n![2](https://user-images.githubusercontent.com/45702408/65289208-647f4200-db7c-11e9-93f3-b96b8ccc1c65.JPG)\r\n", "> TF 1.15.0rc1\r\n\r\n![3](https://user-images.githubusercontent.com/45702408/65289838-fb4cfe00-db7e-11e9-8a0a-d66137f55c1c.JPG)\r\n", "@cyrus2018 did you disable TF logging? Normally it should print something like:\r\n```\r\n2019-09-20 06:59:20.328108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 11161 MB memory) -> physical GPU ...\r\n```", "Added below to top of the codes, issue is resolved\r\nNOTE:: TF_MIN_GPU_MULTIPROCESSOR_COUNT defauls value is 8\r\n\r\n\r\nimport os\r\nos.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"]=\"2\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\r\n", "@cyrus2018 I am currently facing the same issue where tensorflow is actually running with the GPU:0 that is an Intel Graphic one, while it is not recognizing the NVIDIA one which is the GPU:1. Could you please share the code?"]}, {"number": 32488, "title": "Issue: Structure of Python function inputs does not match input_signature while trying to save a subclassed model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.14.0-0-g87989f6959 1.14.0 / tensorflow-gpu==2.0.0-rc0\r\n- Python version: 3.6.8\r\n- GPU model and memory: Google Colab (Nvidia T4)\r\n\r\nSimilar to this issue #28165, I have defined the following Encoder class with a tf.function signature hoping to be able to save it using the SavedModel format. In my case, I'm using a LSTM layer though instead of GRUs:\r\n\r\n```\r\nclass Encoder(tf.keras.Model):\r\n    def __init__(self, vocab_size, embedding_size, lstm_size):\r\n        super(Encoder, self).__init__()\r\n        self.lstm_size = lstm_size\r\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\r\n        self.lstm = tf.keras.layers.LSTM(\r\n            lstm_size, return_sequences=True, return_state=True)\r\n\r\n    @tf.function(input_signature=(\r\n        tf.TensorSpec([None, None], tf.int32, name='sequence'),\r\n        (tf.TensorSpec([None, 64], tf.float32, name='states_1'), tf.TensorSpec([None, 64], tf.float32, name='states_2'))\r\n    )) \r\n    def call(self, sequence, states):\r\n        embed = self.embedding(sequence)\r\n        output, state_h, state_c = self.lstm(embed, initial_state=states)\r\n\r\n        return output, state_h, state_c\r\n\r\n    def init_states(self, batch_size):\r\n        return (tf.zeros([batch_size, self.lstm_size]),\r\n                tf.zeros([batch_size, self.lstm_size]))\r\n\r\n# Some more code for training the seq2seq model...\r\n\r\ntf.saved_model.save(\r\n    encoder,  # instance of Encoder\r\n    './some/directory/',\r\n    signatures=encoder.call\r\n)\r\n```\r\nHere's also the Colab notebook to reproduce the results: [Google Colab Link](https://colab.research.google.com/drive/11rgiI7oYT9uiRWG3ZT9NvRhuRJ5ubEQY)\r\n\r\nSince the issue was closed I hope it would work now but now I get this weird error that the Python function input does not match the input_signature although they clearly match each other:\r\n\r\n```\r\nValueError: Structure of Python function inputs does not match input_signature:\r\n  inputs: (\r\n    [<tf.Tensor 'sequence:0' shape=(None, None) dtype=int32>, (<tf.Tensor 'states_1:0' shape=(None, 64) dtype=float32>, <tf.Tensor 'states_2:0' shape=(None, 64) dtype=float32>)])\r\n  input_signature: (\r\n    TensorSpec(shape=(None, None), dtype=tf.int32, name='sequence'),\r\n    (TensorSpec(shape=(None, 64), dtype=tf.float32, name='states_1'), TensorSpec(shape=(None, 64), dtype=tf.float32, name='states_2')))\r\n```\r\n", "comments": ["Any updates on this issue, still happens on 2.0 release", "Have you solved it\uff1fI had the same problem.", "I use this method to solve it.\r\ncall fuction has only one param: inputs, package the multi inputs, eg: inputs=[x1, x2]\r\nand the input_signature is:\r\n`@tf.function(input_signature=[[tf.TensorSpec([None, 22], dtype=tf.int64, name='x1'),\r\n                                   tf.TensorSpec([None, 22], dtype=tf.float32, name='x2')]])`", "I encountered a similar issue with trying to provide an input signature for a method with multiple inputs. If I applied `@tf.function(intput_signature=...)` to the `call(...)` method with multiple inputs, it wouldn't work, but if i decorated `call(...)` with `@tf.function` and did not provide an `input_signature` kwarg, then `model.call.get_concrete_function(...<multiple tensor specs>...)` would work.\r\n\r\nAlso, another work around was that if I decorated a dummy method, say `call2(...)`, with the same  `@tf.function(intput_signature=...)` I planned on using on `call(...)`, it would work too.", "@datitran Can you please provide a simple standalone code to reproduce the issue? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@datitran \r\nplease update on the above comment", "@datitran\r\nplease update on the above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32488\">No</a>\n", "ValueError: Structure of Python function inputs does not match input_signature:\r\n  inputs: (\r\n    [<tf.Tensor 'transf_in:0' shape=(None, None) dtype=int32>, <tf.Tensor 'transf_out:0' shape=(None, None) dtype=int32>, <tf.Tensor 'enc_padding_mask:0' shape=(None, None, None) dtype=float32>, <tf.Tensor 'look_ahead_mask:0' shape=(None, None, None) dtype=float32>, <tf.Tensor 'dec_padding_mask:0' shape=(None, None, None) dtype=float32>, <tf.Tensor 'trainable:0' shape=() dtype=bool>],\r\n    False)\r\n  input_signature: (\r\n    TensorSpec(shape=(None, None), dtype=tf.int32, name='transf_in'),\r\n    TensorSpec(shape=(None, None), dtype=tf.int32, name='transf_out'),\r\n    TensorSpec(shape=(None, None, None), dtype=tf.float32, name='enc_padding_mask'),\r\n    TensorSpec(shape=(None, None, None), dtype=tf.float32, name='look_ahead_mask'),\r\n    TensorSpec(shape=(None, None, None), dtype=tf.float32, name='dec_padding_mask'),\r\n    TensorSpec(shape=(), dtype=tf.bool, name='trainable'))\r\n\r\ni also meet this error", "@taichuai Hello, I have a really same issue. I'm using transformer model and encounter that same error message! If you solved that problem, please give me what was wrong with that..."]}, {"number": 32487, "title": "[TF -2] Multi gpu training error", "body": "I am trying to train a keras model on two k80. \r\n**\r\n- Have I written custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SMP Debian 4.9.144-3.1\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: 10.1 \r\n- GPU model and memory: Tesla K80\r\n\r\nHere is the the keras model that I am trying to fit:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass SparseSlice(tf.keras.layers.Layer):\r\n    def __init__(self, feature_column):\r\n        super(SparseSlice, self).__init__()\r\n        self.fc = feature_column\r\n\r\n    def build(self, input_shape):\r\n\r\n        self.kernel = self.add_weight('{}_kernel'.format(self.fc.name), shape=(self.fc.num_buckets, ), dtype=tf.float32)\r\n\r\n    def call(self, input):\r\n        ids = self.fc._transform_input_tensor(input)\r\n        return tf.expand_dims(tf.gather(self.kernel, ids.values), axis=1)\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nwith strategy.scope():\r\n\r\n    batch_size = 10\r\n    sparse_col = tf.feature_column.categorical_column_with_hash_bucket('sparse_col', 10000, dtype=tf.int64)\r\n    dense_col = tf.feature_column.numeric_column('dense_col', dtype=tf.float32)\r\n    example_spec = tf.feature_column.make_parse_example_spec([sparse_col, dense_col])\r\n\r\n    sparse_inputs = tf.keras.layers.Input(name=sparse_col.name, shape=(None, ), batch_size=batch_size, sparse=True, dtype=tf.int64)\r\n    dense_inputs = {dense_col.name: tf.keras.layers.Input(name=dense_col.name, shape=(1, ), dtype=tf.float32)}\r\n\r\n    sparse_out = SparseSlice(sparse_col)(sparse_inputs)\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(sparse_out)\r\n    num = tf.keras.layers.DenseFeatures(dense_col)(dense_inputs)\r\n\r\n    concats = tf.keras.layers.Concatenate()([output, num])\r\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(concats)\r\n\r\n    model = tf.keras.Model([dense_inputs, {'sparse_output': sparse_inputs}], output)\r\n\r\n    model.compile(optimizer='adam',\r\n                  loss='mse')\r\n\r\n    np.random.random(())\r\n\r\n    features = {dense_col.name: tf.constant(np.random.random((batch_size, )))}\r\n    features.update({sparse_col.name: tf.sparse.SparseTensor(indices=[[i, 0] for i in range(batch_size)], values=np.random.randint(0, 1000, (batch_size, )), dense_shape=(batch_size, 1))})\r\n    ys = tf.constant(np.random.rand(batch_size), dtype=tf.float32)\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((features, ys)).batch(batch_size)\r\n\r\n    model.fit(x=dataset,\r\n              epochs=1\r\n              )\r\n```\r\nbut I am getting the following error:\r\n\r\n```\r\n2019-09-13 06:48:10.524592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.525159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:04.0\r\n2019-09-13 06:48:10.525252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.525673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:05.0\r\n2019-09-13 06:48:10.525737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2019-09-13 06:48:10.525763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2019-09-13 06:48:10.525798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2019-09-13 06:48:10.525835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2019-09-13 06:48:10.525869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-09-13 06:48:10.525904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-09-13 06:48:10.525937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-09-13 06:48:10.526033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.526541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.527021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.527491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.527907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2019-09-13 06:48:10.528002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-09-13 06:48:10.528023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2019-09-13 06:48:10.528036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y \r\n2019-09-13 06:48:10.528054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N \r\n2019-09-13 06:48:10.528240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.528714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.529244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.529670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\r\n2019-09-13 06:48:10.529763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-09-13 06:48:10.530226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 10805 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)\r\n      1/Unknown - 0s 75ms/stepTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-9c71ae70d829>\", line 33, in <module>\r\n    epochs=1\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 734, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 427, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 370, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\", line 320, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 66, in distributed_function\r\n    model, input_iterator, mode)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 112, in _prepare_feed_values\r\n    inputs, targets, sample_weights = _get_input_from_iterator(inputs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 132, in _get_input_from_iterator\r\n    next_element = next(iterator)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 275, in __next__\r\n    return self.get_next()\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 304, in get_next\r\n    global_has_value, replicas = _get_next_as_optional(self, self._strategy)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 200, in _get_next_as_optional\r\n    iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 878, in get_next_as_list\r\n    lambda: _dummy_tensor_fn(data.value_structure))\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1174, in cond\r\n    return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py\", line 91, in cond_v2\r\n    op_return_value=pred)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 878, in <lambda>\r\n    lambda: _dummy_tensor_fn(data.value_structure))\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 801, in _dummy_tensor_fn\r\n    result.append(create_dummy_tensor(feature_shape, feature_type))\r\n  File \"/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py\", line 784, in create_dummy_tensor\r\n    for dim in feature_shape.dims:\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\nEverything runs fine if I exclude the `with strategy.scope()`", "comments": ["Issue replicating with TF version-2.0rc0, please find the [gist](https://colab.research.google.com/gist/oanush/fa05103b0c9f82e26949f3ede67ffaf5/32487.ipynb) of the colab.Thanks!", "We have an internal fix for this pending. Meanwhile, adding\r\n\r\n```\r\nstrategy = tf.distribute.MirroredStrategy()\r\nstrategy.extended.experimental_enable_get_next_as_optional = False\r\n```\r\n\r\nshould work.", "It works indeed. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32487\">No</a>\n", "I also met this problem, and use above the internal fix, thanks ! \r\nstill I have few questions:\r\n1. is this a bug and will be fixed in the later version or this is the way to use it?\r\n2. what causes this ? \r\n3. if i don't set this option, but to do some changes to the dataset, will it work as expected ?"]}, {"number": 32486, "title": "[INTEL MKL] Enabled MIN_FIRST support and primitive caching for MKL-DNN Quantize OP", "body": "This PR will enable\r\n1. Min first mode support in the MKL-DNN quantize Op and\r\n2. Primitive Caching for the same Op\r\n\r\nCode changes involve\r\n1. Rewriting the rules in the mkl_layout_pass.cc to allow mkl-dnn quantize function being called for MIN_FIRST mode as well\r\n2. Appropriate code added in mkl_quantize_op.cc to support MIN_FIRST mode and primitive caching\r\n3. Added 2 more tests in mkl_quantize_op_test.cc specific to MIN_FIRST mode.", "comments": ["@rgomathi Can you please resolve conflicts? Thanks!", "Hi @penpornk , Thanks for the comments, will resolve them and get back to you ASAP", "Hi @penpornk , I'm extremely Sorry for the delay. Made the changes recommended. Please review and let me know your comments. Thanks.", "@rgomathi Can you please resolve conflicts? Thanks!", "Hi @penpornk , Thanks for the comments and I changed the code accordingly. Please review and let me know. Thanks", "Fixed them....", "@rgomathi Could you please address Ubuntu Sanity errors? Thanks!", "@rgomathi Never mind. Ubuntu Sanity build successful after re-trigger tests. Thanks!", "Thank you @penpornk "]}, {"number": 32485, "title": "[INTEL MKL]Fixing spurious omp thread spawning", "body": "In some models that use eager/imperative code where we don\u2019t rewrite a few operators to MKL, a lot (intra_op_threads*OMP_NUM_THREADS) of threads are spawned by Eigen matmul code. This is because -fopenmp flag is passed to Eigen as part of the build configuration. This PR fixes the configuration to not pass -fopenmp to Eigen under --config=mkl.", "comments": ["@penpornk This is the PR to fix openmp configuration. Please let me know of any changes.", "Thanks @penpornk.", "Hi @penpornk , is everything okay with the PR?", "@Srini511 Let's talk through email. :)"]}, {"number": 32484, "title": "tflite model inaccurate on python and not loading on android ", "body": "\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes I'm using a custom model (I can send you the original model and the converted tflite model)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen running the tflite model through python the results are very different to the original model when no quantisation has been applied. Also, the model does not load at all on android.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe same output as the original model\r\n**Code to reproduce the issue**\r\nI'm converting the original ckpt model to pb using frozen graph \r\n```\r\ntf.train.write_graph(graph_or_graph_def=sess.graph_def, logdir=checkpoint_dir+\"\\\\pb\", name=pbtxt_filename, as_text=True)\r\n\r\nfreeze_graph.freeze_graph(input_graph=pbtxt_filepath, input_saver='', input_binary=False,\r\n                          input_checkpoint=checkpoint_dir+\"model.ckpt\", output_node_names='g_conv10/BiasAdd',\r\n                          restore_op_name='save/restore_all', filename_tensor_name='save/Const:0',\r\n                          output_graph=checkpoint_dir+\"pb\\\\modelpy.pb\", clear_devices=True, initializer_nodes='')\r\n```\r\n\r\nThen I convert the .pb to tflite using :\r\n\r\n```\r\n converter = tf.lite.TFLiteConverter.from_frozen_graph(\"\"\"pb_model_dir\"\"\"\r\n                                                      ,input_arrays=[\"image_in\"]  ,output_arrays=[\"g_conv10/BiasAdd\"])\r\n ```\r\n\r\nAny help would be appreciated.\r\n", "comments": ["An error on my end.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32484\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32484\">No</a>\n"]}, {"number": 32483, "title": "link updated", "body": "Previous link throws 404 error. I updated that link.", "comments": ["Closing as we're not updating r1.14 branch anymore."]}, {"number": 32482, "title": "[r2.0-CherryPick]: Have LossScaleOptimizer better emulate the OptimizerV2 interface.", "body": "This allows LossScaleOptimizers to be saved in a SavedModel with Model.save().\r\n\r\nThe main addition is implementing LossScaleOptimizer.get_config(). This required adding serialization support to LossScales.\r\n\r\nAlso rename the \"opt\" argument in LossScaleOptimizer.__init__ to \"optimizer\".\r\n\r\nPiperOrigin-RevId: 268776796", "comments": []}, {"number": 32481, "title": "1.15.0 cherrypick request: Update tensorboard dependency to 1.15.x", "body": "This cherrypick is required to depend on a version of TensorBoard that\r\nis compatible with TensorFlow 1.15.x, such that `tensorboard`(1) starts\r\nsuccessfully.\r\n\r\nPiperOrigin-RevId: 268769044\r\n", "comments": []}, {"number": 32480, "title": "ODR violations between cuDNN and TensorRT", "body": "_This reports an issue with an NVIDIA library and **not** a bug in TensorFlow. This issue serves as a public description and permalink._\r\n\r\nThere are [One Definition Rule (ODR)](https://en.wikipedia.org/wiki/One_Definition_Rule) violations between TensorRT and cuDNN that can cause binaries that link in both these libraries to crash or misbehave.  We observed this with the combination of TensorRT 5.1.5 and cuDNN 7.6.2 linked statically. We don\u2019t know if builds that use shared libraries are affected.\r\n\r\nFor instance, both TensorRT 5.1.5 and cuDNN 7.6.2 define `_Z22first_layer_fwd_kernelILi4ELi7ELi7ELi64EEv19FirstLayerFwdParams` (demangled: `void first_layer_fwd_kernel<4, 7, 7, 64>(FirstLayerFwdParams)`) but SASS blobs corresponding to these kernels are not the same in TensorRT and cuDNN.  This means that in an application that links in both TensorRT and cuDNN, one of the two libraries will launch the incorrect kernel.  Observed failures manifest as GPU side crash due to misaligned memory access or data corruption.\r\n\r\nWe have informed NVIDIA and they are investigating the issue.", "comments": ["@nluehr @pooyadavoodi ", "TensorRT 6.0.1, which was released this week, and cuDNN 7.6.4, which is coming soon, both have fixes for this issue.  Thanks for reporting it!", "> TensorRT 6.0.1, which was released this week, and cuDNN 7.6.4, which is coming soon, both have fixes for this issue. Thanks for reporting it!\r\n\r\nClosing as per this comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32480\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32480\">No</a>\n"]}, {"number": 32479, "title": "[TF 2.0] AutoGraph: Jacobian docs example could not be transformed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian stretch in Crostini \u2764\ufe0f \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `pip install --upgrade tf-nightly-2.0-preview`\r\n- TensorFlow version (use command below): v1.12.1-10936-g3c7062c 2.0.0-dev20190912\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run the example code from https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape#example_usage_2\r\n\r\nI get this: \r\n\r\n```\r\n(libsbn) x86_64-conda_cos6-linux-gnu libsbn/python \u2039103-branch-length*\u203a \u00bb export AUTOGRAPH_VERBOSITY=10 && python autograph_issue.py \r\n2019-09-12 14:14:23.681818: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-12 14:14:23.689486: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1607975000 Hz\r\n2019-09-12 14:14:23.689916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c529e0a3cf0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-09-12 14:14:23.689979: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:Entity <function pfor.<locals>.f at 0x7ebbe2511e60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_core.estimator'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nTo compute the Jacobian without complaining.\r\n\r\n**Code to reproduce the issue**\r\n\r\nFrom the example, \r\n\r\n```python\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\nwith tf.GradientTape() as g:\r\n    x  = tf.constant([1.0, 2.0])\r\n    g.watch(x)\r\n    y = x * x\r\njacobian = g.jacobian(y, x)\r\n```\r\n\r\n**Other info / logs**\r\n", "comments": ["@matsen, I tried on colab. Please take a look at colab [gist here](https://colab.sandbox.google.com/gist/gadagashwini/43d83465c7deee26084727b745ddc7f2/untitled145.ipynb). \r\nWarning messages will not hamper the execution. Thanks!\r\n", "Thanks for having a look @gadagashwini ! \r\n\r\nI am also using `tfp-nightly`. When I add that to the pip install line in the colab notebook it does indeed throw an error.\r\n\r\nIt does give the correct answer, but does it not come with a speed penalty or something?", "I just did some testing and this does come with a performance penalty, so I would suggest that this is a bug.\r\n\r\nTo clarify the issue:\r\n```\r\npip install --upgrade tf-nightly-2.0-preview\r\n```\r\nworks great with no errors, while\r\n```\r\npip install --upgrade tf-nightly-2.0-preview tfp-nightly\r\n```\r\nthrows errors and is slower.\r\n", "I can't repro this with current nightlies in the given colab. I tried pinning the tf nightly to 20190912 and it still doesn't cause any errors. Is there still a reproducible example of this issue?\r\n\r\nThere were some bad tf-nightly-2.0-preview pushes in the last week, causing estimator not to be importable. It's possible that this only manifested alongside TFP because TF lazily loads modules and importing TFP was somehow causing TF to try to non-lazily load estimator.\r\n\r\nIt sounds like the failure to import estimator was causing issues for pfor/autograph/xla compilation, which could explain the slowness.\r\n\r\nIf someone can share a reproducible-as-of-today example (possibly pinning pip packages to earlier versions?) I'm happy to try to help debug.", "I just re-pulled from nightly and ISSUE FIXED! \ud83c\udf89 \r\n\r\nThanks for making my day, @csuter ! I owe you a \ud83c\udf7a , wherever you are. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32479\">No</a>\n"]}, {"number": 32478, "title": "optimizer.apply_gradients does not accept tape.gradient information", "body": "**System information**\r\nRunning inside colab\r\n\r\n**Describe the current behavior**\r\n\r\nI was expecting to be able to apply the same list of variables in apply_gradient as in t.gradient. But I get an error.\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\noptimizer = tf.train.GradientDescentOptimizer(0.001)\r\nx = tf.Variable(2*tf.ones((5,1)))  \r\nwith tf.GradientTape(persistent=True) as t:\r\n  y = x**2\r\n  dy_dx = t.gradient(y, x)\r\n  optimizer.apply_gradients(dy_dx,x)  # optimizer = Adam\r\n\r\n", "comments": ["@maclopes ,\r\nCan you please mention the TF version being used and also Error being faced ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32478\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32478\">No</a>\n"]}, {"number": 32477, "title": "BatchNormalization doesn't work in graph mode in tf2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow:\r\n- OS: MacOS\r\n- TensorFlow version (use command below): '2.0.0-rc0'\r\n- Python version: 3.7\r\n\r\ntf.keras.layers.BatchNormalization layer does not work in graph mode\r\n\r\ncode to reproduce:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\nimport numpy as np\r\n\r\nkeras = tf.keras\r\n\r\nclass check_bn_model(keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.bn = BatchNormalization()\r\n    \r\n    @tf.function\r\n    def call(self, x):\r\n        x = self.bn(x)\r\n        return x\r\n    \r\nX = np.ones((10,5)).astype('float32')\r\nmodel = check_bn_model()\r\nmodel.compile('adam', 'mse')\r\nmodel.fit(X, X, batch_size=2, epochs=2)\r\n```\r\n\r\nError stack:\r\n```\r\nInaccessibleTensorError                   Traceback (most recent call last)\r\n<ipython-input-142-0234ba2796e1> in <module>\r\n     12 model = check_bn_model()\r\n     13 model.compile('adam', 'mse')\r\n---> 14 model.fit(X, X, batch_size=2, epochs=2)\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    732         max_queue_size=max_queue_size,\r\n    733         workers=workers,\r\n--> 734         use_multiprocessing=use_multiprocessing)\r\n    735 \r\n    736   def evaluate(self,\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    222           validation_data=validation_data,\r\n    223           validation_steps=validation_steps,\r\n--> 224           distribution_strategy=strategy)\r\n    225 \r\n    226       total_samples = _get_total_number_of_samples(training_data_adapter)\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    545         max_queue_size=max_queue_size,\r\n    546         workers=workers,\r\n--> 547         use_multiprocessing=use_multiprocessing)\r\n    548     val_adapter = None\r\n    549     if validation_data:\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    591         batch_size=batch_size,\r\n    592         check_steps=False,\r\n--> 593         steps=steps)\r\n    594   adapter = adapter_cls(\r\n    595       x,\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2382     # First, we build the model on the fly if necessary.\r\n   2383     if not self.inputs:\r\n-> 2384       all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)\r\n   2385       is_build_called = True\r\n   2386     else:\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _build_model_with_inputs(self, inputs, targets)\r\n   2585     else:\r\n   2586       cast_inputs = inputs\r\n-> 2587     self._set_inputs(cast_inputs)\r\n   2588     return processed_inputs, targets, is_dict_inputs\r\n   2589 \r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)\r\n   2672           kwargs['training'] = training\r\n   2673       try:\r\n-> 2674         outputs = self(inputs, **kwargs)\r\n   2675       except NotImplementedError:\r\n   2676         # This Model or a submodel is dynamic and hasn't overridden\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    800                     not base_layer_utils.is_in_eager_or_tf_function()):\r\n    801                   with auto_control_deps.AutomaticControlDependencies() as acd:\r\n--> 802                     outputs = call_fn(cast_inputs, *args, **kwargs)\r\n    803                     # Wrap Tensors in `outputs` in `tf.identity` to avoid\r\n    804                     # circular dependencies.\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    437         # Lifting succeeded, so variables are initialized and we can run the\r\n    438         # stateless function.\r\n--> 439         return self._stateless_fn(*args, **kwds)\r\n    440     else:\r\n    441       canon_args, canon_kwds = \\\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1820     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1821     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1822     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1823 \r\n   1824   @property\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1139          if isinstance(t, (ops.Tensor,\r\n   1140                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1141         self.captured_inputs)\r\n   1142 \r\n   1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1228           {\"PartitionedCall\": gradient_name,\r\n   1229            \"StatefulPartitionedCall\": gradient_name}):\r\n-> 1230         flat_outputs = forward_function.call(ctx, args)\r\n   1231     if isinstance(flat_outputs, ops.Operation) or flat_outputs is None:\r\n   1232       # We only record function calls which have outputs.\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    538                 executing_eagerly=executing_eagerly,\r\n    539                 config=config,\r\n--> 540                 executor_type=executor_type)\r\n    541 \r\n    542     if executing_eagerly:\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/ops/functional_ops.py in partitioned_call(args, f, tout, executing_eagerly, config, executor_type)\r\n    857           f=f,\r\n    858           config_proto=config,\r\n--> 859           executor_type=executor_type)\r\n    860     else:\r\n    861       outputs = gen_functional_ops.partitioned_call(\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_functional_ops.py in stateful_partitioned_call(args, Tout, f, config, config_proto, executor_type, name)\r\n    670         \"StatefulPartitionedCall\", args=args, Tout=Tout, f=f, config=config,\r\n    671                                    config_proto=config_proto,\r\n--> 672                                    executor_type=executor_type, name=name)\r\n    673   _result = _op.outputs[:]\r\n    674   if not _result:\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    791         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\r\n    792                          input_types=input_types, attrs=attr_protos,\r\n--> 793                          op_def=op_def)\r\n    794       return output_structure, op_def.is_stateful, op\r\n    795 \r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in create_op(***failed resolving arguments***)\r\n    542       if ctxt is not None and hasattr(ctxt, \"AddValue\"):\r\n    543         inp = ctxt.AddValue(inp)\r\n--> 544       inp = self.capture(inp)\r\n    545       inputs[i] = inp\r\n    546     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\r\n\r\n~/anaconda3/envs/tf_2.x/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in capture(self, tensor, name)\r\n    601               \" explicit Python locals or TensorFlow collections to access\"\r\n    602               \" it. Defined in: %s; accessed from: %s.\\n\"\r\n--> 603               % (tensor, tensor.graph, self))\r\n    604         inner_graph = inner_graph.outer_graph\r\n    605       return self._capture_helper(tensor, name)\r\n\r\nInaccessibleTensorError: The tensor 'Tensor(\"batch_normalization_194/batch_normalization_194_trainable:0\", dtype=bool)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=call, id=5982930640); accessed from: FuncGraph(name=keras_graph, id=5269648784).\r\n```\r\n\r\nIf you remove the `@tf.function` decorator or if you pass `dynamic=True` to the model instantialization it will work. Otherwise it fails. Note that this is specific to BatchNormalization, if you replace it with any other layer it will work (even other normalization layers like layer norm/instancenorm)\r\n", "comments": ["Looks like code is incomplete. Request you to provide simple standalone code snippet to reproduce the issue in our environment.Thanks!", "> Looks like code is incomplete. Request you to provide simple standalone code snippet to reproduce the issue in our environment.Thanks!\r\n\r\nJust added the import statements. Sorry for the exclusion.", "I have tried on colab with TF version 2.0.0-rc0, 2.0.0-rc1 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/20ab0b70683608e4d33dd82b2934835d/untitled190.ipynb).Thanks!", "bump, also have this issue. code works with instance norm from TFA but fails with batch norm, using tf.function and functional API... ", "seems the issue only happens when custom layers or subclassed models use batch norm. if batch norm is used as a standalone layer, it works @jvishnuvardhan @robieta @mshlis @ravikyram", "@bionicles yeah i noticed that, i shifted to use the functional api (as it works there) and explicitly creating the block i was using with BN, but i feel its still important this gets fixed given that higher level layer abstraction and model sub-classing are some of convenient interfaces of the framework   ", "Any updates/news on this issue ?", "As a workaround you can set the class variable _USE_V2_BEHAVIOR to false.  \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import BatchNormalization\r\nBatchNormalization._USE_V2_BEHAVIOR = False\r\nimport numpy as np\r\n\r\nkeras = tf.keras\r\n\r\n\r\nclass check_bn_model(keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.bn = BatchNormalization()\r\n\r\n    @tf.function\r\n    def call(self, x, training=None, mask=None):\r\n        x = self.bn(x)\r\n        return x\r\n\r\n\r\nX = np.ones((10, 5)).astype('float32')\r\nmodel = check_bn_model()\r\nmodel.compile('adam', 'mse')\r\nmodel.fit(X, X, batch_size=2, epochs=2)\r\n```", "@kkimdev The original code is now failing due to the assert in `ops._override_gradient_function` which comes from https://github.com/tensorflow/tensorflow/commit/cafc2b641f62d00ba00c8fb7742f3c2ebc80c387. Can you take a look?", "Will this issue be fixed in tf2.1?", "> Will this issue be fixed in tf2.1?\r\n\r\nJust checked and this still appears in 2.1\r\n", "After a little more digging you can fix this by going into the batchnormalisation constructor and replacing `self._trainable_var = None` with  `self._trainable_var = K.freezable_variable(trainable, name=self.name + '_trainable')`. Not sure why but in the unaltered version this var only get created on the first call to the batch normalisation layer, which I guess messes with some .fit magic.\r\n", "Getting the same problem.", "@mshlis Is this still an issue? I am not able to reproduce the issue with `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/18aab600662fbcf67620aa4844bd1abd/32477.ipynb). Thanks!\r\n\r\nPlease close the issue if it was already resolved for you. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am closing this issue as it was resolved in recent `tf-nightly`. Please feel free to reopen if It was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32477\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32477\">No</a>\n"]}, {"number": 32476, "title": "Unexpected output shape on custom keras dynamic layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.6\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.0.0rc0\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nUpon attempting to create a custom dynamic keras layer, keras seems to incorrectly interpret the output of `compute_output_shape`.\r\n\r\n**Describe the expected behavior**\r\n\r\nIn the example code below, `model.summary()` outputs `[(None, (2,))]` for the output shape. According to the docs/examples, I would expect that to be `[(None, 2)]`. When attempting to place layers after this, it returns two placeholders, despite the output shape only defining one.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass Example(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        kwargs[\"dynamic\"] = True\r\n        super(Example, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        return inputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return [(None, 2)]\r\n\r\ninp = tf.keras.layers.Input(batch_shape=(None, 1))\r\ncomp = Example()(inp)\r\n\r\nmodel = tf.keras.models.Model(inputs=[inp], outputs=[comp])\r\nmodel.summary()\r\n```\r\nIn my code, the input layer's `batch_shape` and the content of `call` are arbitrary. If I remove `dynamic=True`, then it gives the expected shape based on the contents of `call`. \r\n\r\nThere seems to be no semantic difference in output if `compute_output_shapes` returns `[(None, 2)]`, `(None, 2)`, or `[None, 2]`\r\n\r\n**Other info / logs**\r\n\r\nHere's what I am seeing from model.summary()\r\n```\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 1)]               0\r\n_________________________________________________________________\r\nexample (Example)            [(None, (2,))]            0\r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n", "comments": ["I reproduced the issue on Colab with tf 2.0.0rc0. Find a Colab gist [here](https://colab.sandbox.google.com/gist/gadagashwini/56a82640d89c5c26555da2d6aa1f4a57/untitled149.ipynb). Thanks ", "It looks like using `tf.TensorShape` works properly. See:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass Example(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        kwargs[\"dynamic\"] = True\r\n        super(Example, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        return inputs\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return tf.TensorShape([None, 2])\r\n\r\ninp = tf.keras.layers.Input(batch_shape=(None, 1))\r\ncomp = Example()(inp)\r\n\r\nmodel = tf.keras.models.Model(inputs=[inp], outputs=[comp])\r\nmodel.summary()\r\n```\r\n\r\nThis outputs:\r\n```\r\nModel: \"model_5\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_9 (InputLayer)         [(None, 1)]               0         \r\n_________________________________________________________________\r\nexample_8 (Example)          (None, 2)                 0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nSee [updated Colab](https://colab.research.google.com/gist/porgull/c93dce7d1039b3ccacc1c9c16b956fa4/untitled149.ipynb).", "@porgull  this issue is no longer seen with 2.2.0-rc0. Can you please check and close this issue if it is resolved for you ?\r\n\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32476\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32476\">No</a>\n"]}, {"number": 32475, "title": "fpath = keras_utils.get_file( AttributeError: 'NoneType' object has no attribute 'get_file'", "body": "<em>Several Problems with importing ResNeXt101 from keras_applications in TF 2.0</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Following Instructions from Keras.io and this [Issue#54](https://github.com/keras-team/keras-applications/issues/54)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS with Kernel 5.2.10-050210-generic\r\n\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0b1\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\nFirst Issue was keras_application.resnext.py in `def preprocess_input(x, **kwargs):` was set hard to `return imagenet_utils.preprocess_input(x, mode='torch'), **kwargs) `\r\nsetting mode to 'tf' caused an error. \r\n\r\nMain Issue now is that in keras_applications.imagenet_utils.py on line 223 `fpath= keras_utils.get_file` collides with namespace of TF2, which is tf.keras.utils.\r\n\r\nError is\r\n`    fpath = keras_utils.get_file(\r\nAttributeError: 'NoneType' object has no attribute 'get_file'`\r\n\r\n\r\n**Code to reproduce the issue**\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom keras_applications.resnext import ResNeXt101\r\nfrom keras_applications.resnext import preprocess_input, decode_predictions\r\nfrom keras_preprocessing import image\r\n\r\n\r\nmodel = ResNeXt101(weights='imagenet',\r\n                   backend=tf.keras.backend,\r\n                   layers=tf.keras.layers,\r\n                   models=tf.keras.models,\r\n                   utils=tf.keras.utils)\r\n\r\nimg_path ='image.jpg'\r\nimg = image.load_img(img_path, target_size=(224, 224))\r\nx = image.img_to_array(img)\r\nx = np.expand_dims(x, axis=0)\r\nx = preprocess_input(x, backend=tf.keras.backend, utils=tf.keras.utils)\r\n\r\npreds = model.predict(x)\r\nprint('Predicted:', decode_predictions(preds, top=3)[0])\r\n\r\n\r\n**Other info / logs**\r\n`Traceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"<string>\", line 23, in <module>\r\n  File \"/venv/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\", line 224, in decode_predictions\r\n    fpath = keras_utils.get_file(\r\nAttributeError: 'NoneType' object has no attribute 'get_file'`\r\n", "comments": ["Setting `decode_predictions(preds, top=3, utils=tf.keras.utils)[0])`\r\nsolved the Issue"]}, {"number": 32474, "title": "[ROCM] Patch to enable rocm for r2.0 release branch", "body": "The goal for this PR is to patch Tensorflow r2.0 release, so it would fully enable ROCm\r\nnon-xla path support.\r\nMost of the PRs been cherry-picked in this patch have already been upstreamed in the\r\nupstream master branch.\r\ncc @deven-amd @whchung\r\n\r\nThe following were all the related commits been cherry-picked:\r\n\r\nCommits on Aug 20, 2019\r\ndeven-amd and sunway513\r\nadding/updating ROCm support in the ci_build scripts\r\nd5a0eee\r\ndeven-amd and sunway513\r\nupdating Dockerfile.rocm to pick a specific version of the rocm libra\u2026 \u2026\r\ne335575\r\ndeven-amd and sunway513\r\nadding a script for testing the ROCm Community Supported Build\r\nae83a20\r\n\r\nCommits on Aug 22, 2019\r\ndeven-amd and sunway513\r\nResolve merge conflicts for PR #31393\r\n73ff708\r\ndeven-amd and sunway513\r\nThe following PR/commit breaks the --config=rocm build \u2026\r\n614bdb5\r\ndeven-amd and sunway513\r\nupdating testcases to work correctly with ROCm\r\n1685240\r\njeffdaily and sunway513\r\nimprove concurrency between compute and nccl streams \u2026\r\n3fbb049\r\nwhchung and sunway513\r\n[ROCm] enable roll op on ROCm.\r\n1d5f440\r\nwhchung and sunway513\r\n[ROCm] enable InTopK op on ROCm.\r\n941f713\r\ndeven-amd and sunway513\r\nupdating README.md with information on ROCm Community Supported Builds\r\n73ce64e\r\n\r\nCommits on Aug 25, 2019\r\nhoutoms and sunway513\r\nfixed potential rocm breaks from use_padded_io\r\n0832b33\r\ndeven-amd and sunway513\r\nadding no_rocm tag on unit-tests that check features that are current\u2026 \u2026\r\n7aed626\r\ndeven-amd and sunway513\r\nAdding ROCm support for reduction ops\r\n82bd216\r\nsunway513\r\nFix ROCm path build error in rocm_dnn.h\r\n5dba305\r\n\r\nCommits on Aug 27, 2019\r\ndeven-amd\r\nfixing test failures by skipping parts that functionality not yet sup\u2026 \u2026\r\nbe6378c\r\nsunway513\r\nMerge pull request #616 from ROCmSoftwarePlatform/r2.0-rocm-upstream-\u2026 \u2026\r\nd98a943\r\nsunway513\r\nAdd no_rocm tag to //tensorflow/python:stateful_random_ops_test_gpu\r\nd05a47f\r\n\r\nCommits on Sep 04, 2019\r\nsunway513\r\nMerge branch 'r2.0-rocm-upstream' of https://github.com/ROCmSoftwareP\u2026 \u2026\r\nb1148e4\r\n\r\nCommits on Sep 06, 2019\r\ndeven-amd and sunway513\r\nadding ROCm support in the build_pip_package script\r\nb908324", "comments": ["This will not enable ROCm, right? It's still guarded by ifdefs? \r\n\r\nThis is a large change, and not one I would like to accept this late in the release cycle. What is the goal here?", "What is the ROCm build release plan at the moment? Are we releasing official ROCm builds? If not, is there a point in pulling changes into the 2.0 branch?", "Hi @martinwicke , the basic functionality for ROCm has been enabled in the r2.0 release branch, e.g. we can pass the basic TF 2.0 mnist sample using the docker container built with tensorflow r2.0 branch out of the box.\r\nHowever, there're 20ish unit tests failures for the test cases defined in the following script:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/linux/rocm/run_csb_tests.sh\r\n\r\nThe essential goal for this PR is to cherry-pick several missing PRs (already been upstreamed to tensorflow master branch) to the r2.0 release branch, so we can enable additional operators, e.g. reduction ops with commit https://github.com/tensorflow/tensorflow/commit/82bd216ebd98c6c85a221adadd8d5dee902e3f1d, as well as fixing the failed 20ish test cases. ", "Thanks @chsigg , we have addressed your comments with commit 2118059 and 41041a9, can you review the changes again? ", "Hi @chsigg , gentle ping.", "2.0 itself will not be built with this, it is too late for that. However, we can accept the cherry-pick once we have 2.0 binaries finalized, so that if people build from the 2.0 branch, they get ROCm capabilities.\r\n\r\n@goldiegadde does that sound sensible?", "Now that 2.0 is launched, I think we can merge this. CC @goldiegadde @martinwicke ", "Now we have all checks passed, can we merge the PR now? \r\nGentle ping @martinwicke @goldiegadde", "> Now we have all checks passed, can we merge the PR now?\r\n> Gentle ping @martinwicke @goldiegadde\r\n\r\nThanks @sunway513 for the reminder. I am merging this PR now, but we are not going to publish a new release with this, the branch of course will have the changes. \r\n"]}, {"number": 32473, "title": "Quantile Huber Loss to predict Specific Quantile Value", "body": "We know that we have huber loss function added in keras tf-2 already which can perform both kind of behaviour MSE and MAE depending on the scale of data. In most of the prediction and analysis models, we often do not need just median or mean predicted value, but we also need the specific quantile value of prediction. This is the loss function I am adding which is derived from **huber_loss** here\r\n[https://arxiv.org/pdf/1402.4624.pdf](https://arxiv.org/pdf/1402.4624.pdf) I name it **quantile_huber_loss** and the class which calls it is **QuantileHuber**. I have given great attention to the detail in implementing the function. function takes two extra arguements delta and quantile, where delta is common as the huber_loss and quantile is the float number between 0 and 1. Equation looks as below.\r\n![qhuber](https://user-images.githubusercontent.com/20843596/64808942-dd4b3080-d5b5-11e9-9549-2f98fb18a042.png)\r\n", "comments": ["Related: #32871", "I'm not sure this loss function works as intended. At low or high tau (quantile) and and for larger kappa values, the intersection of the linear segments moves away from from the y axis. I suspect that using this loss will significantly bias the predicted quantile to a less extreme value than intended.\r\n\r\nThese plots explain the problem better.\r\n![bokeh_plot(1)](https://user-images.githubusercontent.com/16913760/72537372-e6ccf980-3830-11ea-89f0-64da639844a1.png)\r\n\r\n![bokeh_plot(2)](https://user-images.githubusercontent.com/16913760/72537392-ecc2da80-3830-11ea-86d0-d87615ad7347.png)\r\n", "> I'm not sure this loss function works as intended. At low or high tau (quantile) and and for larger kappa values, the intersection of the linear segments moves away from from the y axis. I suspect that using this loss will significantly bias the predicted quantile to a less extreme value than intended.\r\n> \r\n> These plots explain the problem better.\r\n> ![bokeh_plot(1)](https://user-images.githubusercontent.com/16913760/72537372-e6ccf980-3830-11ea-89f0-64da639844a1.png)\r\n> \r\n> ![bokeh_plot(2)](https://user-images.githubusercontent.com/16913760/72537392-ecc2da80-3830-11ea-86d0-d87615ad7347.png)\r\n\r\n@stefanmeili, That is the sensitivity of this loss function. Training quality will largely depend upon tau and kappa values. But I have used this loss function to build forecasting engine. If this loss function is used with accurate parameters, then it helps extraordinarily training of very dynamic data to forecast. For more details, please read the paper mentioned in the first comment.", "Hi @ashutosh1919,\r\nThanks, I did look through the referenced paper and understand the motivation behind it. It just occurred to me that the quantile wouldn't be estimated correctly and thought it would be worth mentioning. I put together a quick numerical simulation via convolution to try to understand the magnitude of the discrepancy:\r\n![bokeh_plot(3)](https://user-images.githubusercontent.com/16913760/72589052-030f7b80-38af-11ea-8446-6ba2677aa94d.png)\r\n\r\nThe pinball loss correctly locates the 10% quantile at -1.2815. Quantile Huber loss estimates the 10% quantile at -0.9340 which translates to about the 17.5% quantile. This discrepancy grows for  quantiles closer to 0 or 1 and larger values of kappa.\r\n\r\nI wonder if a horizontal offset can be applied to the error term (x in the paper) to correct for this? It probably won't be a pretty solution."]}, {"number": 32472, "title": "Quantile Huber Loss to predict Specific Quantile Value", "body": "We know that we have huber loss function added in keras tf-2 already which can perform both kind of behaviour MSE and MAE depending on the scale of data. In most of the prediction and analysis models, we often do not need just median or mean predicted value, but we also need the specific quantile value of prediction. This is the loss function I am adding which is derived from **huber_loss** here\r\n[https://arxiv.org/pdf/1402.4624.pdf](https://arxiv.org/pdf/1402.4624.pdf) I name it **quantile_huber_loss** and the class which calls it is **QuantileHuber**. I have given great attention to the detail in implementing the function. function takes two extra arguements delta and quantile, where delta is common as the huber_loss and quantile is the float number between 0 and 1. Equation looks as below.\r\n![qhuber](https://user-images.githubusercontent.com/20843596/64808942-dd4b3080-d5b5-11e9-9549-2f98fb18a042.png)\r\n", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32472) for more info**.\n\n<!-- need_author_consent -->", "Please apply the PR to master, not 2.0."]}, {"number": 32471, "title": "Refactor {TensorSlice, Tensor, Cache, Concatenate}DatasetOpTest", "body": "This PR refactors `TensorSliceDatasetOpTest`, `TensorDatasetOpTest`, `CacheDatasetOpTest`, and `ConcatenateDatasetOpTest` using `DatasetParams`.\r\n\r\ncc: @jsimsa ", "comments": ["@jsimsa The conflicts with this CL(https://github.com/tensorflow/tensorflow/commit/409d58cf6f58fbb738eda8a45c66858abb47d560) has been resolved now. ", "```\r\nthird_party/tensorflow/core/kernels/data/experimental/assert_next_dataset_op_test.cc:15:10: error: module //third_party/tensorflow/core/kernels/data/experimental:assert_next_dataset_op_test does not depend on a module exporting 'third_party/tensorflow/core/kernels/data/take_dataset_op.h'\r\nsee http://go/cpp-features#layering_check; to fix run:\r\nbuild_cleaner //third_party/tensorflow/core/kernels/data/experimental:assert_next_dataset_op_test\r\n#include \"third_party/tensorflow/core/kernels/data/take_dataset_op.h\"\r\n```", "Thanks for checking the internal checks, @jsimsa ! The missing dependency is added [here](https://github.com/tensorflow/tensorflow/pull/32471/commits/ffd433aab68f485ae1fc64e115eca38becf31a14). Please take another look!"]}, {"number": 32470, "title": "Custom Optimizer keeps throwing `no attribute _create_slots` error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0rc1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: P100\r\n\r\n**Describe the current behavior**\r\nWhen creating a custom optimizer, the optimizer keeps throwing `object has no attribute _create_slots` error.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n!pip install tensorflow-gpu==2.0.0-rc1\r\nimport math\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import backend as K\r\n\r\nclass CustomOptimizer(tf.keras.optimizers.Optimizer):\r\n  def __init__(self, \r\n               optimizer, \r\n               cool_period = 10,\r\n               **kwargs):\r\n    super(CustomOptimizer, self).__init__(\"CustomOptimizer\", **kwargs)\r\n    self.optimizer = tf.keras.optimizers.get(optimizer)\r\n    \r\n    self.cool_period = K.variable(cool_period,\r\n                                  name = \"cool_period\",\r\n                                  dtype = K.floatx())\r\n    self.cool_period_slot = self.add_slot(self.cool_period, \"cool_period\")\r\n \r\n \r\n  def get_updates(self, loss, params):\r\n    if self.optimizer.iterations==self.get_slot(cool_period):\r\n      self.updates = self.optimizer.get_updates(loss, params)\r\n\r\n\r\n    \r\n  def get_config(self):\r\n    config = {\r\n              'cool_period': K.get_value(self.cool_period),\r\n              'optimizer': tf.keras.optimizers.serialize(self.optimizer),\r\n            }\r\n    base_config = super(CustomOptimizer, self).get_config()\r\n    return dict(list(base_config.items()) + list(config.items()))\r\n\r\nx = np.random.rand(100,100)\r\ny = np.random.randint(2, size=(100))\r\nop = CustomOptimizer('adam')\r\ninput_layer = tf.keras.layers.Input(shape=x.shape[1:])\r\nfc = tf.keras.layers.Dense(1)(input_layer)\r\n\r\nmodel = tf.keras.models.Model(input_layer, fc)\r\nmodel.compile(optimizer=op, loss='mse')\r\nmodel.fit(x, y, epochs=5)\r\n\r\n```\r\n", "comments": ["I could reproduce the issue with Tf 2.0.0.rc1. PTAL colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/ecbb46625cd07dc4f5657b0e375dbaf3/untitled144.ipynb). Thanks", "you can fix this by having the add_slot inside _create_slots:\r\ndef _create_slots(self, var_list):\r\n  self.cool_period_slot = self.add_slot(self.cool_period, \"cool_period\")\r\n\r\nBut I do agree we should probably not error out in this case", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32470\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32470\">No</a>\n"]}, {"number": 32469, "title": "backward compatible `TFLITE_SCHEMA_VERSION`", "body": "If the latest `TFLITE_SCHEMA_VERSION` supported by the library is version 3 for example, but we only have the model that has older schema version (2 for example), we should still be able to use it. \r\n\r\nI made this change on top of tag `v2.0.0`, please advise if I should make the change on top of master instead.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32469) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 32468, "title": "TF Micro requires CONV_2D version '2' when applying quantization", "body": "- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow version: 1.14, 2.0-rc0, nightly-preview 2.0.0.dev20190911\r\n- Python version: 3.7.4\r\n\r\nI am trying to run a simple Keras model with TensorFlow version for microcontrollers (modified micro_vision example with several convolutions, dense layers and batch normalization). While the model works correctly when using floats, the quantized version generated with `tf.lite.Optimize.DEFAULT` outputs wrong values and `'Didn't find op for builtin opcode 'CONV_2D' version '2' Invoke failed.'` error.\r\n\r\nAfter changing `CONV_2D` version with `AddBuiltin(BuiltinOperator_CONV_2D, Register_CONV_2D(), 1, 2)` in `all_ops_resolver.cc` the error disappeared, but now the model outputs NaN instead of normal output and I'm not sure if it actually uses a different micro kernel.\r\n\r\nAfter trying multiple conversion techniques (from .h5, model itself, SavedModel folder etc.) with multiple TF versions and BatchNormalization removal, the situation remains the same. As far as I understand, each way to quantize the model (post-training quantization, quantization aware training) requires this version of `CONV2D`, but it seems to work incorrectly or there is no such version in TF micro.\r\n\r\nIs there any way to deploy such a model with quantization?", "comments": ["Thanks for this information.  I believe there are several problems brought to light here.  First, the appropriate version of the convolution op that is generated by this quantization flow is not yet available in the micro framework.  Float and uint8 versions of the op are currently supported, but the new tf.lite quantization flow generates int8 ops.  There is a change in process to add this op to the micro framework, but it is not currently supported.\r\n\r\nSecond, if you were able to get the error to disappear by making your change, but the model just silently fails, this is a problem as well.  A proper error should still be generated indicating a mismatch between the tensor type and the kernel versions available.  I'll look into this a bit more and see if I can understand what is happening.\r\n", "The Eval(...) method in conv.cc is checking the input type of the op and generating an error message if the type is not one of {kTfLiteFloat32, kTfLiteUInt8}.  Do you see an error message like \"Type %d not currently supported\" when you run the code with your all_ops_resolver.cc changes?  The micro_interpreter code should also generate an error like \"Node %s (number %d) failed to invoke with status %d\" when the underlying invoke(...) method of an op returns an error.  The MicroInterpreter.Invoke(...) method should also return an error status.  Do you see this in your example code?", "@rockyrhodes No, I see only CONV2D version error when using post-training quantization.", "The error you're running into is definitely caused by us not having an int8 version (version 2) of CONV2D.  I'm working to add int8 per-channel quantization to DEPTHWISE_CONV and CONV2D, which should solve your problem.  This should land within the next week.  I would expect to see the type error that Rocky mentioned, so I'll have a quick look next week to make sure we are logging type errors correctly.", "@njeffrie \r\nI encountered the same problem with TF 1.14. And when I switched to TF 1.13.1 I have the following problem https://github.com/tensorflow/tensorflow/issues/32422\r\n\r\nCould you please have a look at it?", "Thanks for the reminder and apologies for the delay.  I'll have a look tomorrow.", "I see that @njeffrie pushed the int8 quantization support. I'm however still getting the same `'Didn't find op for builtin opcode'CONV_2D' version '2' Invoke failed.'` error with a simple convolutional network. \r\n\r\nI'm using the latest nightly build:\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow version: 1.14, 2.0-rc0, nightly-preview 2.0.0-dev20191002\r\n- Python version: 3.7.4\r\n\r\nAny news @dustedduke? Were you able to deploy the quantized model?", "@apparvi `CONV_2D` error doesn't appear in tf-nightly 2.1.0.dev20191007, but now I encounter different errors with recently added `QUANTIZE` layer and `DEPTHWISE_CONV_2D`.\r\n\r\nWhen I try to generate a fully integer model, as explained [here](https://www.tensorflow.org/lite/performance/post_training_integer_quant), converted CNN starts asking for `QUANTIZE` layer (`Didn't find op for builtin opcode...`), and asks to change the input to float32 after adding the layer, although I manually set `converter.inference_input_type = tf.int8` or `converter.inference_input_type = tf.uint8`. The situation is different, when using hybrid model with float32 input: it successfully uses `QUANTIZE` layer, but then asks for `DEPTHWISE_CONV_2D version 3`. \r\n\r\n@njeffrie I'm wondering if converter tries to replace basic convolution with depthwise version for better performance, because `DEPTHWISE_CONV_2D` dependency is not mentioned in any of the example files and isn't used in the model. ", "It's weird that you're seeing `DEPTHWISE_CONV_2D` if your model does not use it.  We do not support hybrid quantization on micro, so I would expect to see that fail.\r\n\r\nI notice that I forgot to increment the max op version in all_ops_resolver.  I will be working on streamlining how we do op versioning on micro soon.\r\n\r\n@suharshs may be better able to help with the conversion step.", "@suharshs As I understood, TF Lite conversion process replaces **first** `CONV_2D` with `DEPTHWISE_CONV_2D` with depth multiplier = number of output channels (16 in my case). Now, after running the model with added depthwise operation, I see the following:\r\n\r\n```\r\ntensorflow/lite/kernels/kernel_util.cc:54 affine_quantization->scale->size != filter->dims->data[affine_quantization->quantized_dimension] (16 != 1) \r\n```\r\nIs it a conversion issue, or I can set it manually somewhere?\r\n", "I also encountered the same problem.\r\nThe issue of Conv2d is solved by an update, but then requires QUANTIZE version '2'.\r\nI found that this problem seems to be happening because tfmicro does not have implementation of  \"requantization\"\u3000(introduced by commit a1d8d4a33529c1dff642177302dd19c2dfd06c1d).", "Any further updates on this, I'm seeing:\r\n\r\n> Didn't find op for builtin opcode 'CONCATENATION' version '1'\r\n> Failed to get registration from op code  d\r\n", "I am having the same issue. Please add missing ops for quantized model.", "I was able to overcome this by cloning the latest version of Tensorflow and building just the examples from TFLite Micro directories. Once I had that I could map my CMakeLists to the same makefile arrangement that was used there, copied the same source tree, and it worked.\r\n\r\nI think in the past I had been using TF 1.12, as that's what my model was builtin, but once I upgraded everything to the TF master copy it has been fine. The number of operations being added in just the last few iterations has been numerous.", "@AdamMoses-GitHub  Are you talking about CONCATENATION version '1' only or they have added  CONV_2D version '2', too?", "Has there been any updates on this? I'm running into the same error as @dustedduke . I ran the example found here: https://www.tensorflow.org/lite/performance/post_training_integer_quant to produce the *mnist_model_quant_io.tflite* model.\r\n\r\nI ran that model through this converter code to create a .h file with the model as constant bytes:\r\n\r\n```\r\n# Function: Convert some hex value into an array for C programming\r\ndef hex_to_c_array(hex_data, var_name):\r\n    \r\n    c_str = ''\r\n    \r\n    # Create header guard\r\n    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\r\n    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\r\n    \r\n    # Add array length at top of file\r\n    c_str += '\\nunsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\r\n    \r\n    # Declare C variable\r\n    c_str += 'unsigned char ' + var_name + '[] = {'\r\n    hex_array = []\r\n    for i, val in enumerate(hex_data):\r\n\r\n        # Construct string from hex\r\n        hex_str = format(val, '#04x')\r\n        \r\n        # Add formatting so each line stays within 80 characters\r\n        if (i + 1) < len(hex_data):\r\n            hex_str += ','\r\n        if (i + 1) % 12 == 0:\r\n            hex_str += '\\n '\r\n        hex_array.append(hex_str)\r\n        \r\n    # Add closing brace\r\n    c_str += '\\n  ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\r\n    \r\n    # Close out header guard\r\n    c_str += '#endif //' + var_name.upper() + '_H'\r\n    \r\n    return c_str\r\n\r\n# Write TFLite model to a C source file\r\nc_model_name = \"mnist_model_quant_io\"\r\nwith open(c_model_name + '.h', 'w') as file:\r\n    file.write(hex_to_c_array(tflite_model_quant, c_model_name))\r\n```\r\n\r\nI loaded the file on Arduino (TensorFlow library v1.15.0-ALPHA-precompiled) and attempted to run inference. I got the following error:\r\n\r\n```\r\n/home/arduino/workspace/Libraries - Google-Tensorflow scraper/Arduino/libraries/tensorflow_lite_mirror/src/tensorflow/lite/kernels/kernel_util.cpp:54 affine_quantization->scale->size != filter->dims->data[affine_quantization->quantized_dimension] (4 != 1)\r\nNode DEPTHWISE_CONV_2D (number 2) failed to invoke with status 1\r\n```\r\n\r\nHere is the model from the demo:\r\n\r\n```\r\nmodel = keras.Sequential([\r\n  keras.layers.InputLayer(input_shape=(28, 28)),\r\n  keras.layers.Reshape(target_shape=(28, 28, 1)),\r\n  keras.layers.Conv2D(filters=4, kernel_size=(2, 2), activation=tf.nn.relu),\r\n  keras.layers.MaxPooling2D(pool_size=(4, 4)),\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\n```\r\n\r\nAs you can see, there is no DepthwiseConv2D in the Keras model. It looks like the TFLite converter is replacing the first Conv2D layer with a DepthwiseConv2D layer and changing the depth_multiplier to the same number as the filters. This seems to break things, but only when we add quantization (it works without quantization).\r\n\r\nAny help with this would be appreciated!\r\n\r\nPython 3.7.6\r\nTensorFlow 2.1.0\r\nKeras 2.2.4-tf\r\nWindows 10 x64", "@dustedduke Could you please try on the latest stable TF v2.6.0 and let us know if it is still an issue ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32468\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32468\">No</a>\n"]}, {"number": 32467, "title": "Test if axis argument accepts tuples.", "body": "Unit test for bug fix in LayerNormalization https://github.com/tensorflow/tensorflow/pull/32463\r\n\r\n@robieta  I have no idea how the test framework for tensorflow works. I have not tested if this works or triggers the bug in the earlier version before the bug fix.", "comments": ["The unit test should live alongside the fix, so you can just add this there.\r\n\r\nEDIT: to https://github.com/tensorflow/tensorflow/pull/32463, I mean"]}, {"number": 32466, "title": "[ppc64le] Undo setting of OMP_NUM_THREADS", "body": "This check was added because libopenblas on Power had threading\r\nissues if OMP_NUM_THREADS was set higher than 1. This has been\r\nfixed in all the latest versions of libopenblas and this check\r\nis no longer needed.", "comments": ["I really appreciate coming back to remove the extra checks, and paying down the technical debt.\r\n", "Changes have been merged internally, waiting for auto-merge to happen."]}, {"number": 32465, "title": "[tflite] Build issue: failed by \\execroot\\org_tensorflow\\bin\\false in Windows", "body": "**System information**\r\n- Windows 10 x64:\r\n- TensorFlow installed from source:\r\n- TensorFlow version: master\r\n- Python version: 3.7\r\n- Bazel version: 0.29.1\r\n\r\n**Describe the problem**\r\n```\r\nERROR: E:/tools/tensorflow/tensorflow/lite/BUILD:89:1: C++ compilation of rule '//tensorflow/lite:external_cpu_backend_context' failed (Exit -1). Note: Remote connection/protocol failed with: execution failed\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(\"C:\\users\\???\\_bazel_???\\svyyfdgn\\execroot\\org_tensorflow\\bin\\false\" -MD -MF bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/_objs/external_cpu_backend_context/external_cpu_backend_context.pic.d -frandom-seed=bazel-out/armeabi-v7a-opt/bin/tensorflow/lite/_objs/external_cpu_backend_context/external_cpu_backend_context.pic.o -fPIC -iquote . -iquote bazel-out/armeabi-v7a-opt/bin -std=c++14 --std=c++11 -Wall -Wno-comment -Wno-extern-c-compat -c tensorflow/lite/external_cpu_backend_context.cc -o b(...)): The system cannot find the file specified.\r\n (error: 2)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel build -c opt --cxxopt=--std=c++11 --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a //tensorflow/lite/experimental/c:libtensorflowlite_c.so`\r\n", "comments": ["Thanks for flagging, are you still experiencing this issue with the latest master checkout?", "@tree1891 \r\n\r\nAny update on this issue please. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32465\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32465\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}]