[{"number": 31097, "title": "Make verbosity of fit generator available for validation", "body": "Fixing #31074.\r\nCreating another variable as an option to control the verbosity of the validation in `fit_generator` would probably be confusing -> `verbose` now would controll both", "comments": ["@gbaned  Could you possibly assign a new reviewer as the last update is past almost a week?", "@tanzhenyu could you please review again?"]}, {"number": 31096, "title": "Op request tensorflow lite ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, EXP, EXPAND_DIMS, FILL, GATHER, GREATER, GREATER_EQUAL, LESS, LOGICAL_AND, LOGISTIC, MAXIMUM, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, SELECT, SHAPE, SLICE, SPLIT, SQUEEZE, STRIDED_SLICE, SUB, SUM, TILE, TOPK_V2, TRANSPOSE, UNPACK, WHERE, ZEROS_LIKE. Here is a list of operators for which you will need custom implementations: Enter, Exit, LoopCond, Merge, NonMaxSuppressionV3, Switch, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@kishorekumarsingh,\r\nProvide the exact sequence of commands / steps that you executed before running into the problem. And also, provide Tensorflow version. Thanks!", "This is the command that I used:\r\n`tflite_convert --graph_def_file=frozen_inference_graph.pb --output_file=new_graph.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shape=1,224,224,3 --input_array=image_tensor --output_array=detection_boxes,detection_scores,detection_classes,num_detections --inference_input_type=QUANTIZED_UINT8 --inference_type=FLOAT --mean_values=128 --std_dev_values=128  `\r\n\r\nMy tensorflow version : \r\n1.14.0", "@kishorekumarsingh  \r\nIs this still an issue ", "Can you try this: https://www.tensorflow.org/lite/guide/ops_select\r\nFeel free to reopen if you run into any issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31096\">No</a>\n"]}, {"number": 31095, "title": "YOLO V3 Tensorflow Lite request", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, EXP, FILL, GATHER, GREATER_EQUAL, LEAKY_RELU, LOGISTIC, MUL, PAD, RESHAPE, RESIZE_NEAREST_NEIGHBOR, SHAPE, SPLIT_V, SQUEEZE, STRIDED_SLICE, SUB, WHERE. Here is a list of operators for which you will need custom implementations: NonMaxSuppressionV3.\r\n\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@hailigu Is this still an issue? Can you share *.h5 or .pb file that you have used? What version of Tensorflow you had used? Thanks!\r\n\r\nIf this was already resolved, then please close the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 31094, "title": "Compilation error with protoc", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Installed from source\r\n- TensorFlow version: 1.8\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.12\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n\r\n\r\n**Describe the problem**\r\nI'm following the instructions I've found in:\r\nhttps://medium.com/@fanzongshaoxing/use-tensorflow-c-api-with-opencv3-bacb83ca5683\r\nto build the shared library of libtensorflow_cc.so and trying to compile the code which follows the tutorial in \r\nhttps://github.com/lysukhin/tensorflow-object-detection-cpp?source=post_page---------------------------\r\nHowever, when I run cmake && make I get lots of compilation errors that tell me that the tensorflow library was generated using an old version of protoc and that I need to regenerate the file.\r\nHowever I have already built protobuf 3.6.1 from source and running the following command lines I am assured of this:\r\n\r\nprotoc --version\r\nlibprotoc 3.6.1\r\n\r\npip show protobuf\r\nName: protobuf\r\nVersion: 3.6.1\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: 3-Clause BSD License\r\nLocation: /home/user/.virtualenvs/cv/lib/python3.6/site-packages\r\nRequires: setuptools, six\r\nRequired-by: tensorflow, tensorboard\r\n\r\nBut I do note that in /usr/bin, there is a different version of protoc\r\n/usr/bin/protoc --version\r\nlibprotoc 3.0.0\r\n\r\nHow should I proceed? Thanks for your consideration.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nAfter performing the steps in the tutorial to build tensorflow_cc.so with bazel and move them to usr/local\r\ngit clone https://github.com/lysukhin/tensorflow-object-detection-cpp.git\r\ncd tensorflow-object-detection-cpp\r\nmkdir build\r\ncd build\r\ncmake ..\r\nmake\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nIn file included from /usr/local/include/tf/tensorflow/core/framework/variant.h:26:0,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,\r\n                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,\r\n                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,\r\n                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:17:2: error: #error This file was generated by an older version of protoc which is\r\n #error This file was generated by an older version of protoc which is\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please\r\n #error incompatible with your Protocol Buffer headers.  Please\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.\r\n #error regenerate this file with a newer version of protoc.\r\n\r\nIn file included from /usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:32:0,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,\r\n                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,\r\n                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,\r\n                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/resource_handle.pb.h:17:2: error: #error This file was generated by an older version of protoc which is\r\n #error This file was generated by an older version of protoc which is\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/resource_handle.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please\r\n #error incompatible with your Protocol Buffer headers.  Please\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/resource_handle.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.\r\n #error regenerate this file with a newer version of protoc.\r\n  ^~~~~\r\nIn file included from /usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:33:0,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,\r\n                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,\r\n                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,\r\n                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor_shape.pb.h:17:2: error: #error This file was generated by an older version of protoc which is\r\n #error This file was generated by an older version of protoc which is\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor_shape.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please\r\n #error incompatible with your Protocol Buffer headers.  Please\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor_shape.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.\r\n #error regenerate this file with a newer version of protoc.\r\n  ^~~~~\r\nIn file included from /usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/tensor.pb.h:34:0,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/variant.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/allocator.h:26,\r\n                 from /usr/local/include/tf/tensorflow/core/framework/tensor.h:20,\r\n                 from /usr/local/include/tf/tensorflow/cc/framework/ops.h:21,\r\n                 from /usr/local/include/tf/tensorflow/cc/ops/const_op.h:19,\r\n                 from /home/marcchoo/tensorflow-object-detection-cpp/main.cpp:6:\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/types.pb.h:17:2: error: #error This file was generated by an older version of protoc which is\r\n #error This file was generated by an older version of protoc which is\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/types.pb.h:18:2: error: #error incompatible with your Protocol Buffer headers. Please\r\n #error incompatible with your Protocol Buffer headers.  Please\r\n  ^~~~~\r\n/usr/local/include/tf/bazel-genfiles/tensorflow/core/framework/types.pb.h:19:2: error: #error regenerate this file with a newer version of protoc.\r\n #error regenerate this file with a newer version of protoc.\r\n\r\n...\r\n", "comments": ["Nvmind, I tried using the older version of protoc 3.5.1 that was verified to work in the tutorial and managed to compile. Guess 3.6.1 couldn't cut it. Where can we find the correct version of protoc for each tensorflow version?", "The TensorFlow team does not officially support cmake, sorry. Please try out building [from source with Bazel](https://www.tensorflow.org/install/source)."]}, {"number": 31093, "title": "Loading embeddings into the graph fails with: libprotobuf ERROR google/protobuf/io/zero_copy_stream_impl_lite.cc 164 Cannot allocate buffer larger than kint32max for StringOutputStream", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOS 10.14\r\n- TensorFlow installed from (source or binary):\r\nBinary \r\n- TensorFlow version (use command below):\r\n1.13.1\r\n- Python version:\r\nPython 3.6.7 |Anaconda\r\n\r\n**Describe the current behavior**\r\nLoading 1.4mil 100dim embeddings into the graph \r\nwords:1457657; dim:100\r\n\r\n**Describe the expected behavior**\r\nWant to be able to use TFRecord data set with words and graph to lookup indexes via TF table\r\nand then parallel lookup imbedding vectors; compute using 3 dim tensor.\r\nThis used to work with smaller set of embeddings. \r\nAnyway to overcome this problem without rewriting data feed?\r\n\r\n**Code to reproduce the issue**\r\nw_embedding_vocab = tf.constant(embDic.vocab, dtype=tf.string, shape=[embDic.vocab_size], name=\"w_embedding_vocab\")\r\n\r\n            w_embedding_vocab_table = lookup_ops.index_table_from_tensor(w_embedding_vocab, default_value=0, name=\"word_embidx_tbl\")\r\n\r\n            w_embeddings = tf.get_variable(name=\"word_embeddings\", shape=[embDic.vocab_size, embDic.dim],\r\n                                           initializer=tf.constant_initializer(np.asmatrix(embDic.embeddings)),\r\n                                           dtype=tf.float32, trainable=False)\r\n\r\n\r\n**Other info / logs**\r\nNo other logs, just one ERROR message\r\n\r\n[libprotobuf ERROR google/protobuf/io/zero_copy_stream_impl_lite.cc:164] Cannot allocate buffer larger than kint32max for StringOutputStream.\r\n", "comments": ["Work around so far was to reduce phrase vocabulary, which reduced graph size from 1.4G to 500Mb and above error doesn't appear, but it's not really scalable solution.  ", "@vitalyli, Provide us the full minimal code snippet. It will indeed help us to move faster.", "I can't share embeddings file, but the core issue is it's too large for the graph; The file is about 1.5G in size with words and word vectors of dim 100. \r\nMy guess is some phrase words may have ended up lengthy and would take more space.\r\n \r\nThe error prints while loading w_embeddings matrix. I don't remember seeing this in the past, but that could be because I have not crossed this limit. \r\nSo my question is if there is a better/scalable way to do this as to avoid this graph size limit or if there is a way to upgrade something and relax this limit. \r\nIt's definitely not machine memory problem as there is enough of memory to load many times that size.\r\nThe way I could solve this is by removing all phrase vectors, which prevented error from happening,\r\nhowever I'm looking for more general solution to this issue and not by dropping pre-trained embeddings. \r\nTraining data is being loaded from TFRecord files via dataset api. There I have a tensor with list of words. Below is to map word to word index and then to word embeddings.\r\n\r\nThe TFRecord are parsed this way, which works, but there seem to be no place to have external mapping of word->index->embedding unless it's being done as part of graph.\r\nThe only other alternative is to generate TFRecord with word indexes instead of words themselves, which will avoid hash table lookup in the graph, but it's operational headache and less efficient.\r\n\r\n```\r\ndef _parse_function(example_proto):\r\n\r\n    template = {\r\n\r\n        'qw': tf.FixedLenFeature([8], tf.string),\r\n        'qw_seq': tf.FixedLenFeature([1], tf.int64),\r\n\r\n        'ph_w': tf.FixedLenFeature([1], tf.string),\r\n        'ph_seq': tf.FixedLenFeature([1], tf.int64),\r\n\r\n        'qu_f': tf.FixedLenFeature([N_QU_F], tf.float32),\r\n\r\n        'lbl': tf.FixedLenFeature([2], tf.int64)\r\n    }\r\n\r\n    parsed_features = tf.parse_single_example(example_proto, template)\r\n\r\n    return parsed_features\r\n``` \r\n\r\nBelow is how graph embeddings are initialized, where:\r\nembDic.embeddings is array of 100dim numpy vectors loaded from Word2Vec file.\r\nembDic.vocab is corresponding vocab as array of words also loaded from Word2Vec file.\r\n\r\n```\r\n    with graph.as_default():\r\n\r\n        with tf.name_scope(\"EmbLoadScope\"):\r\n\r\n            w_embedding_vocab = tf.constant(embDic.vocab, dtype=tf.string, shape=[embDic.vocab_size], name=\"w_embedding_vocab\")\r\n\r\n            w_embedding_vocab_table = lookup_ops.index_table_from_tensor(w_embedding_vocab, default_value=0, name=\"word_embidx_tbl\")\r\n\r\n            w_embeddings = tf.get_variable(name=\"word_embeddings\", shape=[embDic.vocab_size, embDic.dim],\r\n                                           initializer=tf.constant_initializer(np.asmatrix(embDic.embeddings)),\r\n                                           dtype=tf.float32, trainable=False)\r\n\r\n    with tf.name_scope(\"InitVar\"):\r\n            init_variables = tf.group(tf.local_variables_initializer(),tf.global_variables_initializer(),tf.tables_initializer(), name='init_var_op')\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n   \r\n    with graph.as_default() as gg:\r\n\r\n        with tf.Session(graph=gg, config=config) as sess:\r\n\r\n            init_variables.run()\r\n\r\n            #etc. training loop here\r\n            while True:\r\n                        _, g_step = sess.run([train_op, global_step],\r\n                                 feed_dict={ds_handle: train_handle,\r\n                                            x_learning_rate: learn_rate,\r\n                                            x_pkeep: training_dropout})\r\n```", "This issue is not related to tf.data.", "Never mind solution to this is to avoid constant initializer and load vectors via placeholder, that keeps protobuf size small.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31093\">No</a>\n"]}, {"number": 31092, "title": "Tensorflow-lite gpu ios memory leak", "body": "I use TensorFlowLiteGpuExperimental  for inference ,I have 60 models, if i only use one model to inference ,it works well, but if i switch different models in running , than memory leak occurs, I just use code :\r\n            interpreter = nullptr;\r\n            DeleteGpuDelegate(delegate);\r\nto free resources\r\nso if i loss other free function ?\r\nif TensorFlowLiteGpuExperimental a bug?\r\nTensorFlowLiteGpuExperimental the newer version number is what? ", "comments": ["I assume you are storing interpreter in a std::unique_ptr so that interpreter=nullptr doesn't leak.\r\nCould you give a complete repro example. Reassigning @impjdi who has more insight.", "sorry about my english. I use tensorflow and i leak memory on ip. Does anyone have the same problem and how can i fix it", "@zuoshaobo Sorry, I missed this re-assignment... and this is already over a year old.  My apologies.\r\n\r\nWe don't have experience with multiple gpu delegates, but usually, deleting the GPU delegate and the interpreter (maybe the other way around... I forgot.  there is a particular order) should be clean without memory leaks.  Given that this bug is quite old without updates, I assume the OP solved the issue by him/herself and will close the bug.  Feel free to reopen.\r\n\r\n@NALanhnt2 I think it's better to create a new issue for this with additional information.  If you say \"it doesn't work on ip\", it doesn't tell us much.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31092\">No</a>\n"]}, {"number": 31091, "title": "[TF 2.0] tf.image.central_crop doesn't work", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-7164-gf2b5825 2.0.0-dev20190726\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\ntf.image.central_crop throws an error\r\n**Describe the expected behavior**\r\ntf.image.central_crop crops image\r\n**Code to reproduce the issue**\r\nI've tried two scenarios.\r\nFirst:\r\n```python\r\nfrom tensorflow.keras import Input\r\n\r\nimage = Input(shape=(512,512,3))\r\n\r\nimage = tf.image.central_crop(image, 0.8)\r\n```\r\n\r\nSecond:\r\n```python\r\nfrom tensorflow.keras import Input\r\n\r\nimage = Input(shape=(512,512,3))\r\n\r\n@tf.function\r\ndef central_crop_fn(image, fraction):\r\n    return tf.image.central_crop(image, fraction)\r\n\r\ncentral_crop_fn(image, 0.8)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nFirst log:\r\n\r\n```---------------------------------------------------------------------------\r\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\r\n<ipython-input-4-e5a8997e400a> in <module>\r\n      3 image = Input(shape=(512,512,3))\r\n      4 \r\n----> 5 image = tf.image.central_crop(image, 0.8)\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py in central_crop(image, central_fraction)\r\n    653       return image\r\n    654 \r\n--> 655     _AssertAtLeast3DImage(image)\r\n    656     rank = image.get_shape().ndims\r\n    657     if rank != 3 and rank != 4:\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py in _AssertAtLeast3DImage(image)\r\n    192   \"\"\"\r\n    193   return control_flow_ops.with_dependencies(\r\n--> 194       _CheckAtLeast3DImage(image, require_static=False), image)\r\n    195 \r\n    196 \r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py in _CheckAtLeast3DImage(image, require_static)\r\n    226         check_ops.assert_positive(\r\n    227             array_ops.shape(image),\r\n--> 228             [\"all dims of 'image.shape' \"\r\n    229              'must be > 0.']),\r\n    230         check_ops.assert_greater_equal(\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py in assert_positive(x, data, summarize, message, name)\r\n    266           'x (%s) = ' % name, x]\r\n    267     zero = ops.convert_to_tensor(0, dtype=x.dtype)\r\n--> 268     return assert_less(zero, x, data=data, summarize=summarize)\r\n    269 \r\n    270 \r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/check_ops.py in assert_less(x, y, data, summarize, message, name)\r\n    861       ]\r\n    862     condition = math_ops.reduce_all(math_ops.less(x, y))\r\n--> 863     return control_flow_ops.Assert(condition, data, summarize=summarize)\r\n    864 \r\n    865 \r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py in wrapped(*args, **kwargs)\r\n    196   \"\"\"\r\n    197   def wrapped(*args, **kwargs):\r\n--> 198     return _add_should_use_warning(fn(*args, **kwargs))\r\n    199   return tf_decorator.make_decorator(\r\n    200       fn, wrapped, 'should_use_result',\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py in Assert(condition, data, summarize, name)\r\n    147   \"\"\"\r\n    148   if context.executing_eagerly():\r\n--> 149     if not condition:\r\n    150       xs = ops.convert_n_to_tensor(data)\r\n    151       data_str = [_summarize_eager(x, summarize) for x in xs]\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in __bool__(self)\r\n    749       `TypeError`.\r\n    750     \"\"\"\r\n--> 751     self._disallow_bool_casting()\r\n    752 \r\n    753   def __nonzero__(self):\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _disallow_bool_casting(self)\r\n    530     else:\r\n    531       # Default: V1-style Graph execution.\r\n--> 532       self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n    533 \r\n    534   def _disallow_iteration(self):\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in _disallow_in_graph_mode(self, task)\r\n    519     raise errors.OperatorNotAllowedInGraphError(\r\n    520         \"{} is not allowed in Graph execution. Use Eager execution or decorate\"\r\n--> 521         \" this function with @tf.function.\".format(task))\r\n    522 \r\n    523   def _disallow_bool_casting(self):\r\n\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n```\r\nSecond log:\r\n\r\n```---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     60                                                op_name, inputs, attrs,\r\n---> 61                                                num_outputs)\r\n     62   except core._NotOkStatusException as e:\r\n\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: input_3:0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n_SymbolicException                        Traceback (most recent call last)\r\n<ipython-input-6-71417bdb3502> in <module>\r\n      7     return tf.image.central_crop(image, fraction)\r\n      8 \r\n----> 9 central_crop_fn(image, 0.8)\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    449               *args, **kwds)\r\n    450       # If we did not create any variables the trace we have is good enough.\r\n--> 451       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n    452 \r\n    453     def fn_with_cond(*inner_args, **inner_kwds):\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    663          if isinstance(t, (ops.Tensor,\r\n    664                            resource_variable_ops.BaseResourceVariable))),\r\n--> 665         self.captured_inputs)\r\n    666 \r\n    667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n    776     if executing_eagerly or not self.outputs:\r\n    777       outputs = self._inference_function.call(\r\n--> 778           ctx, args, cancellation_manager=cancellation_manager)\r\n    779     else:\r\n    780       self._register_gradient()\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    469               inputs=args,\r\n    470               attrs=(\"executor_type\", executor_type, \"config_proto\", config),\r\n--> 471               ctx=ctx)\r\n    472         else:\r\n    473           outputs = execute.execute_with_cancellation(\r\n\r\n~/tensorflow2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     73       raise core._SymbolicException(\r\n     74           \"Inputs to eager execution function cannot be Keras symbolic \"\r\n---> 75           \"tensors, but found {}\".format(keras_symbolic_tensors))\r\n     76     raise e\r\n     77   # pylint: enable=protected-access\r\n\r\n_SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'input_3:0' shape=(None, 512, 512, 3) dtype=float32>]\r\n```", "comments": ["I have tried on colab with TF version 2.0.0-dev20190726 and was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1h_Hr_QmSBJ445wTT_8srF-O4otMOhQUS) here.Thanks!", "Wrap the op in a tf.keras.layers.Lambda layer @lytkarinskiy\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Lambda\r\n\r\nprint('Tensorflow', tf.__version__)\r\n\r\nimage = Input(shape=(512,512,3))\r\n\r\n@tf.function\r\ndef central_crop_fn(image, fraction):\r\n    return tf.image.central_crop(image, fraction)\r\n\r\nx = Lambda(central_crop_fn, arguments={'fraction':0.8})(image)\r\nprint(x)\r\n```\r\n\r\nOutput\r\n```\r\nTensorflow 2.0.0-beta1\r\nTensor(\"lambda_3/Identity:0\", shape=(None, 410, 410, 3), dtype=float32)\r\n```\r\n", "This is fixed with TF 1.14. Thanks!\r\n```python\r\nfrom tensorflow.keras import Input\r\n\r\nimage = Input(shape=(512,512,3))\r\n\r\n@tf.function\r\ndef central_crop_fn(image, fraction):\r\n    return tf.image.central_crop(image, fraction)\r\n\r\ncentral_crop_fn(image, 0.8)\r\n```\r\noutput:\r\n```python\r\n<tf.Tensor 'StatefulPartitionedCall:0' shape=(?, 410, 410, 3) dtype=float32>\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31091\">No</a>\n"]}, {"number": 31090, "title": "[Intel MKL] Fix \"Missing 1-th output from\" Crash caused by MKL MaxPooling", "body": "node having an extra workspace tensor as auxiliary output. Such output should never be left unallocated (0x0) even for cases that the output tensors are empty. If not, code will fail at this check in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/executor.cc#L2034-L2042 (i.e. the if (val.tensor == nullptr) check ). In addition, only MKL MaxPool has such a workspace tensor. AvgPool does not and Quantized MaxPool does not.", "comments": []}, {"number": 31089, "title": "Can't set tf.keras.backend.variable(trainable=False)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13.1\r\n- Are you willing to contribute it (Yes/No): maybe, but I don't have time for tests/docs\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, I have to do this.\r\n```python\r\nimport tensorflow.keras.backend as K\r\n\r\nvariable = K.variable(5)\r\nvariable._trainable = False\r\n```\r\n\r\nBut changing private attributes always feels dodgy. Ideally, we should just be able to do this.\r\n```python\r\nvariable = K.variable(5, trainable=False)\r\n```\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt will add one parameter `trainable=True` to `tensorflow.keras.backend.variable`\r\n\r\n**Who will benefit with this feature?**\r\n\r\nPeople who don't like the feeling of having to modify private attributes.\r\n\r\n**Any Other info.**\r\nLooking here: https://github.com/tensorflow/tensorflow/blob/ec71eb1ac4986e145845372b44648a03ea0e7545/tensorflow/python/keras/backend.py#L739\r\n\r\n```python\r\ndef variable(value, dtype=None, name=None, constraint=None, trainable=True):\r\n    ...\r\n    v = variables_module.Variable(\r\n      value,\r\n      dtype=dtypes_module.as_dtype(dtype),\r\n      name=name,\r\n      trainable=trainable, # +++\r\n      constraint=constraint)\r\n    ...\r\n```\r\n\r\nNot sure what's going on with the sparse tensor, but you could always raise an exception if \r\n`hasattr(value, 'tocoo') and trainable == False` if that's a problem.", "comments": ["Any updates on this?", "It looks like this is still an unresolved issue: https://github.com/tensorflow/tensorflow/blob/c21a146b96a1aaf30f2849beac9458ed5c658633/tensorflow/python/keras/backend.py#L1052", "@beasteers,\r\nSorry for the delayed response. The functionality you are looking for, is implemented in [tf.Variable](https://www.tensorflow.org/guide/variable). Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/ccbb2d0eb8993887a480b9e6b7c23590/gh_31089.ipynb) of the working code. Thanks!", "This question was pertaining to `tf.keras.backend.variable` which is presumably different to `tf.Variable`. If I've missed something and they can in fact be used interchangeably, then it's perhaps that `tf.keras.backend.variable` is now unnecessary because of improving tf+keras core integrations and is just there for legacy purposes. \n\nBut in the keras version it's doing a bit more (`_keras_shape`, `track_variable`, etc.) which I assume is needed for working with keras models.", "@beasteers,\r\nThe documentation for tf.keras.backend.variable couldn't be seen in the documentation of [tf.keras.backend](https://www.tensorflow.org/api_docs/python/tf/keras/backend). Can you please provide references to that API? Thanks!\r\n ", "heres the implementation\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/backend.py#L1046-L1094\r\n\r\nif u look it now has a decorator called `@doc_controls.do_not_generate_docs`", "I do second what rmothukuru says.\n\nOn Wed, Jun 23, 2021 at 8:01 AM rmothukuru ***@***.***> wrote:\n\n> @beasteers <https://github.com/beasteers>,\n> The documentation for tf.keras.backend.variable couldn't be seen in the\n> documentation of tf.keras.backend\n> <https://www.tensorflow.org/api_docs/python/tf/keras/backend>. Can you\n> please provide references to that API? Thanks!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31089#issuecomment-866775895>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABLQT45VSEEIPR3TVFIHVTDTUHEKFANCNFSM4IHHJPXQ>\n> .\n>\n", "Looks like it was hidden in the docs in v2.4. But it was there in v2.3. Here's the docs for it:\r\nhttps://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/backend/variable\r\n\r\nTo reiterate - maybe using `tf.keras.backend.variable` is no longer necessary in tf2 - I have no idea because I haven't had to build a custom keras layer recently.\r\n\r\nI'm just going to assume that `tf.keras.backend.variable` is no longer useful and that the default `tf.Variable` handles any of the keras metadata it needs (`_keras_shape`, `track_variable`, etc.)\r\n"]}, {"number": 31088, "title": "import error for python3: _pywrap_tensorflow_internal.so: undefined symbol: _Py_ZeroStruct", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 6.9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: build from source\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): gcc 5.5.0\r\n- CUDA/cuDNN version: 10.1/7.6.2\r\n- GPU model and memory: GTX 1080\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI used a customized tool chain in $LINUXBREWHOME (which is ~/.linuxbrew) to build tensorflow from source using these commands (see below) and the build was successful. And then I built the whl for python3 and installed it using pip3 and they all worked. But when I tried importing tensorflow in python3 using ```python3 -c 'import tensorflow'``` I got an error:\r\n```\r\nImportError: /home/aznb/.linuxbrew/Cellar/python/3.7.4/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_ZeroStruct\r\n``` \r\nTensorflow was built using these commands:\r\n```\r\nexport GCC_HOST_COMPILER_PREFIX=$LINUXBREWHOME/bin\r\n\r\nexport PYTHON_BIN_PATH=$(which ${python3})\r\nexport PYTHON_LIB_PATH=$LINUXBREWHOME/Cellar/python/3.7.4/lib/python3.7/site-packages\r\nexport PYTHONPATH=$LINUXBREWHOME/opt/python3/lib/python3.7/site-packages\r\nexport PYTHON_ARG=$LINUXBREWHOME/opt/python3/lib/python3.7/site-packages\r\nexport CUDA_TOOLKIT_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/\r\nexport CUDNN_INSTALL_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/\r\n\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=\"$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\"\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5\r\nexport TF_NEED_HDFS=0\r\nexport TF_NEED_OPENCL=0\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_CUDA_CLANG=0\r\nexport TF_CUDNN_VERSION=\"$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)\"\r\nexport TF_NEED_MKL=0\r\nexport TF_DOWNLOAD_MKL=0\r\nexport TF_NEED_AWS=0\r\nexport TF_NEED_MPI=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_S3=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NEED_COMPUTECPP=0\r\nexport GCC_HOST_COMPILER_PATH=$LINUXBREWHOME/Cellar/gcc/5.5.0_4/bin/gcc\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n#export TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_TENSORRT=0\r\n\r\n# when using NCCL you need to install it own your own\r\n#export TF_NCCL_VERSION=1.3\r\n\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n\r\n#bazel clean --async\r\n#./configure\r\n\r\n#bazel build --jobs 16 --crosstool_top=@local_config_cuda//crosstool:toolchain --config=noaws --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package $TMPDIR/tensorflow-pkg\r\npip3 install $TMPDIR/tensorflow-pkg/tensorflow-1.14.0-cp37-cp37m-linux_x86_64.whl\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow](https://www.tensorflow.org/install/source) website .Please, let us know. Thanks!", "Yes, I did. \r\n\r\nHowever, I found the culprit is ```export PYTHON_BIN_PATH=$(which ${python3})``` which should be ```export PYTHON_BIN_PATH=$(which python3)```. Fixing this solves the problem"]}, {"number": 31087, "title": "Changed the order of auto-precision and layout optimizer", "body": "The current layout optimizer will use the info of data type to determine using NHWC/NCHW. On the other hand, auto-precision optimizer changes the data type to float16 if necessary. Therefore, it is preferable to put layout optimizer after auto-precision optimizer, considering they are both run-once optimizers. \r\n\r\nfyi @nluehr @benbarsdell ", "comments": []}, {"number": 31086, "title": "TensorFlow 2.0 using pretrained word embeddings as input to sequence model", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF 2.0 Beta\r\n- Are you willing to contribute it (Yes/No):\r\n**Describe the feature and the current behavior/state.**\r\nI've just started exploring TF2.0 and it appears that the word embeddings must be generated as part of the seq2seq model.  \r\n\r\n**Will this change the current api? How?**\r\nembedding layer would need to be modified to not train and permit inclusion of pretrained word embeddings\r\n\r\n**Who will benefit with this feature?**\r\nanyone who would like to use pretrained word embeddings**Any\r\n\r\n Other info.**\r\ninclusion of pretrained word embeddings would further expedite training so a huge win. \r\n", "comments": ["Hi @NeuralGirl,\r\n\r\nIf you already have pretrained embeddings, you should be able to build a model that takes pre-embedded data as inputs, and add a non-trainable lookup to your embedding matrix to your pipeline so as to conduct the embedding before feeding to the network.\r\n\r\nE.g. something in the vein of:\r\n```python\r\nclass PretrainedEmbedding(tf.keras.layers.Layer):\r\n    \"\"\"Non-trainable embedding layer.\"\"\"\r\n\r\n    def __init__(self, embeddings, rate=0.1, **kwargs):\r\n        \"\"\"\"Instantiate the layer using a pre-defined embedding matrix.\"\"\"\r\n        super().__init__(**kwargs)\r\n        self.embeddings = tf.constant(embeddings)\r\n        # if you want to add some dropout (or normalization, etc.)\r\n        self.dropout = tf.keras.layers.Dropout(rate=rate)\r\n\r\n    def call(self, inputs, training=None):\r\n        \"\"\"Embed some input tokens and optionally apply dropout.\"\"\"\r\n        output = tf.nn.embedding_lookup(self.embeddings, inputs)\r\n        return self.dropout(output, training=training)\r\n\r\n\r\nmy_model = tf.keras.models.Sequential([\r\n    tf.keras.Input(input_shape, dtype=tf.int64),\r\n    PretrainedEmbedding(embeddings_array),\r\n    # insert any model architecture here\r\n])\r\n```\r\n\r\nIf I am missing something or if you would like to indicate a more precise use case your issue arises from, please let me know :-)", "Thanks for the prompt followup! Yes, this is what I am looking to do in TF2.0. Thanks!", "You are welcome :-)", "Hi, I've tried to use the aforementioned class to load pretrained embeddings, but the an error message is thrown: AttributeError: 'PretrainedEmbedding' object has no attribute '_trainable'", "@luiz-resende The code I posted was lacking a `super` call in its `__init__` method; I edited it so that it should be workable. Note that to be rigorous in respecting the keras API you should also write a `get_config` method and a `compute_output_signature` one for that class, which I did not provide as this is a simple layer sketch.\r\n\r\nLet me know if you run into additional issues!", "Note that another take at the initial issue could be to assign the pretrained embeddings to an instantiated and built `Embedding` layer set to be non-trainable (_i.e._ whose weights would be frozen).\r\nSomething in the vein of:\r\n```python\r\nlayer = tf.keras.layers.Embedding(input_dim, output_dim, trainable=False)\r\nlayer.build((None, input_dim))  # replace None with a fixed batch size if appropriate\r\nlayer.set_weights([pretrained_embeddings_tensor])\r\n```\r\n\r\nNote that you can also freeze the layer after instantiating it, with `layer.trainable = False`.", "Thanks for followup and for the help! I've tried the second option and it worked perfectly and way cleaner in my model (still transitioning from TF 1.x to 2.0). Thanks again!", "You are very welcome :)", "You can do this through:\r\nembedding_layer = tf.keras.layers.Embedding(...)\r\nembedding_layer.set_weights(pretrained_ embeddings)"]}, {"number": 31085, "title": "Building Tensorflow with VS 2019", "body": "_My apologies for starting this conversation here. I didn't think the TensorFlow discussion forum or StackOverflow would be appropriate._\r\n\r\nHello,\r\n\r\nI work on the Visual Studio C++ Compiler team. We are interested in helping TensorFlow upgrade to Visual Studio 2019 while also making sure the best compiler switches are being used. We have seen significant throughput and code generation improvements between the Visual Studio 2017 and the 2019 compilers across various projects.\r\n\r\nI have several questions:\r\n\r\n1. From the [TensorFlow documents](https://www.tensorflow.org/install/source_windows), it looks like the only supported compiler is Visual Studio 2017. **Is VS 2017 being used in the TensorFlow continuous integration and release builds?** I couldn't verify from the logs [here](https://source.cloud.google.com/results/invocations/45f76066-3000-43cf-b55d-9317b4e6b083/targets/tensorflow%2Fgithub%2Fwindows%2Fbazel%2Fcontinuous/log).\r\n\r\n2. Are there any plans to move to another compiler toolchain for TensorFlow on Windows?\r\n\r\n3. Is there a better channel to ask any other questions that we might have?\r\n\r\nThank you for your time.", "comments": ["For CPU builds, as far as I know building with visual studio 2019 should be OK.\r\nWe cannot move our builds to visual studio 2019 because CUDA 10 does not support it.\r\nWe do definitely use VS 2017 to build TF on our side. Once we upgrade CUDA, we will reevaluate which compiler to base TF off of.\r\n\r\nI think you can reach out to @thirupalanisamy to help set up a more formal channel, and build special interest group to discuss build related concerns.\r\nhttps://groups.google.com/a/tensorflow.org/d/forum/build", "Thanks for the response, @gunan. I'll reach out separately to create that channel.\r\n\r\nI did a quick search for Cuda and Visual Studio and found this: https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html. It seems like in Cuda 10.1, they now have support for Visual Studio 2019. Do you happen to know if this is sufficient for Tensorflow?", "We will probably need to keep supporting Cuda 10 for a while. Until we move off of cuda 10, we will kepe the toolchains same across our build environment.", "That's understandable. If TensorFlow could continue using Cuda 10.0 but have that utilize Visual Studio 2019 instead of 2017, would your team consider upgrading to Visual Studio 2019 for both CPU and GPU setups on Windows? Also, would an upgrade to Cuda 10.1 be acceptable for the Tensorflow team or is that out of the question?\r\n\r\nIs there anything else that might block the TensorFlow team from upgrading to Visual Studio 2019?", "CUDA upgrades are usually a larger discussion for TF prebuilt binaries. Let's have a larger video call about that.", "Sure. Is there an email/contact I can reach out to, in order to set this up? Thanks again for the help", "@vitong I think contact was given in [this response](https://github.com/tensorflow/tensorflow/issues/31085#issuecomment-516064289). Also, you could join [Special interest group (SIG)](https://www.tensorflow.org/community/forums#special_interest_groups) related to build/install. There are also lots of other resources on TF website.  \r\nPlease close the issue, if you don't have any further questions. Thanks!", "Hi @vitong.\r\nFor now, let's continue the discussion through build@tensorflow.org mailing list.\r\n\r\nFor the questions posed:\r\nI think it is highly likely that TF will skip cuda 10.1 and move to the next release when it is available.\r\nAnd I can see no issues with using vs 2019, if cuda 10 headers are updated.", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 31084, "title": "Bug Issue: tf.case incompatible with list comprehension", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **custom**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n```\r\n$ uname -a\r\nLinux archlinux 5.1.5-arch1-2-ARCH #1 SMP PREEMPT Mon May 27 03:37:39 UTC 2019 x86_64 GNU/Linux\r\n```\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **None**\r\n- TensorFlow installed from (source or binary): **binary**\r\n- TensorFlow version (use command below):\r\n **v1.14.0-rc1-22-gaf24dc91b5**\r\n**1.14.0**\r\n- Python version: **3.6**\r\n- Bazel version (if compiling from source): **None**\r\n- GCC/Compiler version (if compiling from source): **None**\r\n- CUDA/cuDNN version: **None**\r\n- GPU model and memory: **None**\r\n\r\n**Describe the current behavior**\r\n\r\nBug 1: the function `random_choice` below has the correct behaviour only when using the decorator `@tf.function`. If the decorator is not used, `tf.case` no longer consider the predicate and always outputs the same value.\r\n\r\nBug 2: even if the decorator `@tf.function` is used, the function `random_choice` does not have the correct behaviour if I use list comprehension to create the list of pairs predicate / functions. Although, it has the correct behaviour when the list is populated iteratively with append (in a for loop). \r\n\r\n**Describe the expected behavior**\r\n\r\n1. Decorating the function `random_choice` with `@tf.function` should not affect `tf.case`, unless I am not aware of the intricacies of `tf.function` and `tf.case`.\r\n\r\n2. Choosing for loops or list comprehension to create the list of predicate / function pairs should not affect `tf.case`, unless (again) I am not aware of the intricacies of `tf.function` and `tf.case`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow_probability as tfp\r\n\r\nK.clear_session()\r\n\r\ndef fct1(x):\r\n  return x + 5\r\n\r\ndef fct2(x):\r\n  return x - 5\r\n\r\n# Bug 1: if you remove `@tf.function`, the function `random_choice` \r\n# no longer works, because `tf.case` no longer considers `idx`.  \r\n@tf.function\r\ndef random_choice(x, choices):\r\n  idx = tfp.distributions.Bernoulli(probs=0.5).sample(1)[0]\r\n  outputs = [c(x) for c in choices]\r\n \r\n# Bug 2: if you use `@tf.function`, but you replace this code:\r\n  fns = []\r\n  for o in outputs:\r\n    fns.append(lambda: o)\r\n# by this one:\r\n#   fns = [lambda:o for o in outputs]\r\n# then, `tf.case` no longer considers `idx`.\r\n\r\n  y = tf.case([(tf.equal(idx, i), fn) for (i, fn) in enumerate(fns)])\r\n  return outputs, idx, y\r\n\r\nx = tf.constant([4.0])\r\nchoices = [fct1, fct2]\r\noutputs, idx, y = random_choice(x, choices)\r\nsess = tf.Session()\r\nfor i in range(10):\r\n  outputs_np, idx_np, y_np = sess.run([outputs, idx, y])\r\n  print(y_np, outputs_np[idx_np])\r\n```\r\n\r\nExample outputs.\r\n\r\nFor the code above (this is the expected behaviour):\r\n```python\r\n[9.] [9.]\r\n[-1.] [-1.]\r\n[-1.] [-1.]\r\n[-1.] [-1.]\r\n[9.] [9.]\r\n[9.] [9.]\r\n[-1.] [-1.]\r\n[9.] [9.]\r\n[9.] [9.]\r\n[-1.] [-1.]\r\n```\r\n\r\nWhen removing `@tf.function`, or when using `@tf.function` but using list comprehension to create `fns` (this is the unexpected behaviour):\r\n```python\r\n[-1.] [9.]\r\n[-1.] [9.]\r\n[-1.] [-1.]\r\n[-1.] [9.]\r\n[-1.] [9.]\r\n[-1.] [9.]\r\n[-1.] [-1.]\r\n[-1.] [-1.]\r\n[-1.] [-1.]\r\n[-1.] [9.]\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI could reproduce this bug in tensorflow:\r\n**2.0.0-beta1**\r\n**v2.0.0-beta0-16-g1d91213fe7**\r\n \r\n", "comments": ["Was able to reproduce the issue on Colab with Tensorflow 1.14. Please see the [gist here](https://colab.research.google.com/drive/1aaE0CT0DSPlf2fQAPEzlPiT5d8cd8dtQ). Thanks!", "Thanks for the feedback. Let me know when this is fixed.", "@kkimdev can you help triage this? The implementation of tf.case with eager execution should be relatively straightforward to follow.", "As a side note, I tested `tf.switch_case` in colab 1.14, and I had the same problems:\r\n\r\n```\r\nidx = tf.constant([0], dtype=tf.int32)\r\no1 = tf.constant([2.])\r\no2 = tf.constant([4.])\r\n\r\nfns = []\r\nfor o in [o1,o2]:\r\n  fns.append(lambda: o)\r\n\r\ntf.switch_case(idx[0], fns)\r\n```\r\nOutputs:\r\n```\r\n<tf.Tensor: id=176, shape=(1,), dtype=float32, numpy=array([4.], dtype=float32)>\r\n```", "If I do this however, it works:\r\n```\r\nidx = tf.constant([0], dtype=tf.int32)\r\no1 = tf.constant([2.])\r\no2 = tf.constant([4.])\r\nfns = [lambda: o1, lambda: o2]\r\ntf.switch_case(idx[0], fns)\r\n```\r\n\r\nOutputs:\r\n```\r\n<tf.Tensor: id=223, shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>\r\n```", "@ltrottier Hi, sorry for getting back late.  Though it seems like this is just how Python works.\r\n\r\n```python\r\nfns = []\r\nfor o in 'abcdefg':\r\n  fns.append(lambda:o)\r\n\r\nprint(fns[0]())\r\n# prints 'g', not 'a'\r\n```\r\n\r\nMore info: https://stackoverflow.com/a/19837590\r\n\r\nI agree that it's quite confusing though :(\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31084\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31084\">No</a>\n", "Thanks for the link. It is indeed how python works. I can't believe I've never seen this before. "]}, {"number": 31083, "title": "Build for centos with toolchain in nonstandard location", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos 6.9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: build from source\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): gcc 5.5.0\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1080 \r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nBecause the OS's toolchain is too old to build tensorflow, I use the toolchain from linuxbrew installed in $LINUXBREWHOME=~/.linuxbrew (e.g., ~/.linuxbrew/bin/gcc) to build tensorflow. But the build failed with \r\n```\r\nERROR: /data/scratch/ssd/cache/_bazel_aznb/19c22cee7007ea5c8439dd1833ab51b5/external/nasm/BUILD.bazel:8:1: Linking of rule '@nasm//:nasm' failed (Exit 1)\r\n/usr/bin/ld: unrecognized option '-plugin'\r\n/usr/bin/ld: use the --help option for usage information\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\nas it can't find the linker in ~/.linuxbrew/bin/ld (which I confirm functional)\r\n\r\nThe commands I used to reproduce this:\r\n\r\n```\r\nexport VERSION=$1                                                                                                                                              \r\n                                                                                                                                                               \r\nexport TF_ROOT=$LINUXBREWHOME/Cellar/tensorflow/$VERSION                                                                                                       \r\n                                                                                                                                                               \r\nexport PYTHON_BIN_PATH=$(which ${python3})                                                                                                                     \r\nexport PYTHON_LIB_PATH=$LINUXBREWHOME/Cellar/python/3.7.4/lib/python3.7/site-packages                                                                          \r\nexport PYTHONPATH=${TF_ROOT}/lib                                                                                                                               \r\nexport PYTHON_ARG=${TF_ROOT}/lib                                                                                                                               \r\nexport CUDA_TOOLKIT_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/                                                                                                      \r\nexport CUDNN_INSTALL_PATH=$LINUXBREWHOME/Cellar/cuda/10.1/                                                                                                     \r\n                                                                                                                                                               \r\nexport TF_NEED_GCP=0                                                                                                                                           \r\nexport TF_NEED_CUDA=1                                                                                                                                          \r\nexport TF_CUDA_VERSION=\"$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\"                                                       \r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1,7.5                                                                                                                    \r\nexport TF_NEED_HDFS=0                                                                                                                                          \r\nexport TF_NEED_OPENCL=0                                                                                                                                        \r\nexport TF_NEED_JEMALLOC=1                                                                                                                                      \r\nexport TF_ENABLE_XLA=0                                                                                                                                         \r\nexport TF_NEED_VERBS=0                                                                                                                                         \r\nexport TF_CUDA_CLANG=0                                                                                                                                         \r\nexport TF_CUDNN_VERSION=\"$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)\"                                               \r\nexport TF_NEED_MKL=0                                                                                                                                           \r\nexport TF_DOWNLOAD_MKL=0\r\nexport TF_DOWNLOAD_MKL=0                                                                                                                                       \r\nexport TF_NEED_AWS=0                                                                                                                                           \r\nexport TF_NEED_MPI=1                                                                                                                                           \r\nexport MPI_HOME=$LINUXBREWHOME\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_S3=0\r\nexport TF_NEED_ROCM=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NEED_COMPUTECPP=0\r\nexport GCC_HOST_COMPILER_PATH=$LINUXBREWHOME/Cellar/gcc/5.5.0_4/bin/gcc\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\n#export TF_SET_ANDROID_WORKSPACE=0\r\nexport TF_NEED_KAFKA=0\r\nexport TF_NEED_TENSORRT=0\r\n\r\n# when using NCCL you need to install it own your own\r\n#export TF_NCCL_VERSION=1.3\r\n\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\nbazel clean --async\r\n./configure\r\n\r\nbazel build --crosstool_top=@local_config_cuda//crosstool:toolchain --config=noaws --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Found the solution here https://github.com/bazelbuild/bazel/issues/8715 by setting GCC_HOST_COMPILER_PREFIX to my customized toolchain\r\nWhy isn't GCC_HOST_COMPILER_PREFIX documented anywhere?", "As this is a bazel issue, I would recommend reaching out to bazel team. What you are raising is a good question, but it is a bazel feature that you may be able to use.\r\n\r\nClosing issue as this is a bazel issue."]}, {"number": 31082, "title": "Trivial conditions", "body": "Several `if`s have parts which are trivially true.", "comments": ["Adding @mihaimaruseac as requested on mailing list"]}, {"number": 31081, "title": "Uninitialized variables", "body": "Initializes some uninitialized variables. Some could lead to unexpected behavior when not zero-initialized, others produce compilation warnings. ", "comments": ["Adding @mihaimaruseac as requested on mailing list", "@sanjoy In this case, probably not, thanks for the review and the explanation."]}, {"number": 31080, "title": "Avoid potential undefined behavior", "body": "`nodes_map_.size()` could return different values because it can be called before or after `nodes_map_[node.name()]` is created.", "comments": ["Adding @mihaimaruseac as requested on mailing list"]}, {"number": 31079, "title": "Simplify code for TensorListConcatLists shape inference", "body": "This makes clearer that one condition is negation of another.", "comments": ["Adding @mihaimaruseac as requested on mailing list"]}, {"number": 31078, "title": "Avoid return without freeing a pointer", "body": "There are several places where a pointer is allocated with `new` and then there is return on some error without freeing it. I did not include places where `Unref()` is called.", "comments": ["Adding @mihaimaruseac as requested on mailing list", "@alextp To clarify: _are_ places where `Unref()` is called before return, but `delete` isn't, correct?"]}, {"number": 31077, "title": "Downgrade tensorflow 1.14 to tensorflow 1.13", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:9.0/7.4\r\n- GPU model and memory: Nivida GeForce 840m 3 Go\r\n\r\n\r\n\r\n**Describe the current behaviour**\r\nI training a Resnet Model ( CNN) that detects an eye region with landmarks on real-time using Tensorflow library \r\nFor the first time, I have used Tensorflow library ( 1.13.1), the model gives sometimes results and for others times not for the webcam ( and for video from my dataset ).\r\nI have upgrade Tensorflow to 1.14.1 recently and I have tested my webcam and I don't get any results contrary for a video the same result does not change when I upgrade the version of Tensorflow.\r\nI have a little doubt that the problem may be can related to the version of Tensorflow.\r\nMy question is :\r\n\r\n+ Can I downgrade the version of Tensorflow 1.14.1 to 1.13.1? ( all packages )\r\n\r\n\r\n\r\n\r\n", "comments": ["Yes. If you only used the binary package, you can uninstall and install with your package manager. Did you use pip or conda?\r\n\r\nFor example, for pip you can do:\r\n```shell\r\npip uninstall tensorflow-gpu -y\r\npip install tensorflow-gpu==1.13.1\r\n```", "Yes, I have used pip.\r\nThanks for your answer.", "When I tried this command I get this error:\r\n> pip install tensorflow-gpu==1.13.1\r\nCollecting tensorflow-gpu==1.13.1\r\n  Could not find a version that satisfies the requirement tensorflow-gpu==1.13.1 (from versions: )\r\nNo matching distribution found for tensorflow-gpu==1.13.1", "Can you please refer to the [software requirements](https://www.tensorflow.org/install/gpu#software_requirements) for tensorflow 1.13 and above.Thanks!", "I solved the issue by changing, pip is related to python 2.x and pip3 is related to python 3.6.8\r\n> pip install tensorflow-gpu==1.13.1\r\n> pip3 install tensorflow-gpu==1.13.1\r\n\r\n\r\n", "@abdou31 ,\r\nLooks like issue is resolved,Can i close the issue.?Thanks!", "Yes, I will close the issue.\r\nThanks.\r\n", "For Conda user do the following : \r\n\r\n`conda uninstall tensorflow`\r\n\r\n`conda install -c conda-forge tensorflow=1.13`\r\n", "ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1", "> ERROR: Could not find a version that satisfies the requirement tensorflow==1.13.1\r\n\r\nAre you using `conda env` or `python venv`?\r\n\r\nAlso, consider this:\r\n\r\n```\r\nTensorflow only supports the 64-bit version of Python\r\n\r\nTensorflow only supports Python 3.5 to 3.8\r\n\r\nSo, if you're using an out-of-range version of Python (older or newer) or a 32-bit version, then you'll need to use a different version.\r\n```", "I tried both pip and pip3 to re-install \r\ngot the \"no matching distribution\" error for both ", "Github should allow images on these forms "]}, {"number": 31076, "title": "Fix unique_ptr type", "body": "`std::unique_ptr<uint8_t>(new uint8_t[remainder_])` would be deleted using `delete` and not `delete[]`.", "comments": ["Adding @mihaimaruseac as requested on mailing list"]}, {"number": 31075, "title": "Typo fixes", "body": "", "comments": ["@mihaimaruseac Please review."]}, {"number": 31074, "title": "fit_generator validation progress bar", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0 beta\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently it is not possbile to see the progressbar of the valitation process when the `keras.fit_generator` is running in `verbose=1` or `verbose=2`.\r\n\r\n**Will this change the current api? How?**\r\nIn https://github.com/tensorflow/tensorflow/blob/456fbc0e498e3d10604973de9f46ca48d62267cc/tensorflow/python/keras/engine/training_generator.py#L320 \r\nthe control over the verbosity has to be controllable via the `fit_generator` setup.\r\n\r\n**Who will benefit with this feature?**\r\nPeople using the keras `fit_generator` on large datasets. Especially there the validation process can take long time and therefore seeing the progress is nice.\r\n", "comments": ["I thought you made a PR and I LGTMed it. Closing it for now."]}, {"number": 31073, "title": "tf.linalg.sqrtm() returns nan", "body": "I want to perform the function of matrix square root in my CNN, which can be automatically backpropagated. I used the function tf.linalg.sqrtm() to compute the square root of an invertible matrix. After running the session, it returns an array with all elements equal to \"nan\". Does anyone know about the reason for the incorrect result? Or any other methods to compute the matrix square root in the forward process and compute gradients automatically during the backward process?\r\nThe settings are:\r\n- Linux Ubuntu 16.04\r\n- TensorFlow version: 1. 14. 0\r\n- Python version: 3.6\r\n\r\n", "comments": ["@hathawayxxh \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 31072, "title": "torch.unfold function is needed..", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.14\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI think sending the function page in torch is the best. https://pytorch.org/docs/stable/_modules/torch/nn/modules/fold.html\r\n**Will this change the current api? How?**\r\nI guess it doesn't.\r\n**Who will benefit with this feature?**\r\nI think it will be popular in Vision Models, cause self attention is arising now to find out relationship in input pixels.(Stand-Alone Self-Attention in Vision Models)\r\n\r\n\r\n**Any Other info.**\r\n", "comments": ["unfold looks very similar to tf.extract_image_patches. Which of the differences in the API are important to you?", "i'm sorry to miss to check that function."]}, {"number": 31071, "title": "Error on model fit with stateful LSTM using Dataset", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 LTS\r\n- **TensorFlow installed from (source or binary)**: conda-forge\r\n- **TensorFlow version (use command below)**: unknown 1.14.0\r\n- **Python version**: Python 3.7.3\r\n- **CUDA/cuDNN version**: NVIDIA-SMI 418.67, Driver Version: 418.67, CUDA Version: 10.1\r\n- **GPU model and memory**: Quadro RTX 6000, 24190MiB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen feeding a `Dataset` to `Sequential.fit()` when a _stateful_ LSTM layer is included, a `TypeError` occurs:\r\n\r\n    TypeError: 'DatasetV1Adapter' object is not subscriptable\r\n\r\nThis does not happen with a stateless LSTM (Using `stateful=False` does not produce the error). Below is a minimal example to reproduce this issue.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#tf.enable_eager_execution()\r\n\r\ntrain_X = np.arange(1, 1001).reshape((200, 5))\r\ntrain_Y = np.array(list(map(\r\n    lambda x: np.array([1, 0]) if x == 0 else np.array([0, 1]),\r\n    np.random.randint(2, size=200))))\r\n\r\nds = tf.data.Dataset.from_tensor_slices((train_X, train_Y))\r\n\r\nws = 3\r\nsh = None\r\nst = 1\r\nbs = 10\r\nnu = 86\r\nne = 5\r\n\r\nds = ds.window(size=ws, shift=sh, stride=st, drop_remainder=True).flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(ws), y.batch(ws)))).batch(bs, drop_remainder=True)\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer(input_shape=(ws, len(train_X[0])), batch_size=bs),\r\n    tf.keras.layers.LSTM(nu, return_sequences=True, stateful=True),\r\n    tf.keras.layers.Dense(2), # categorical\r\n    tf.keras.layers.Activation('softmax'), # categorical\r\n])\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\r\nprint(model.summary())\r\n\r\nhistory = {}\r\nfor e in range(ne):\r\n    history[e] = model.fit(ds, epochs=1, shuffle=False)\r\n    model.reset_states()\r\n\r\n#history = model.fit(ds, epochs=ne, shuffle=False)\r\n```\r\nOutput:\r\n```\r\nModel: \"sequential_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nlstm_1 (LSTM)                (10, 3, 86)               31648     \r\n_________________________________________________________________\r\ndense_1 (Dense)              (10, 3, 2)                174       \r\n_________________________________________________________________\r\nactivation_1 (Activation)    (10, 3, 2)                0         \r\n=================================================================\r\nTotal params: 31,822\r\nTrainable params: 31,822\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n\r\n\r\nTypeErrorTraceback (most recent call last)\r\n<ipython-input-2-0cef961435e5> in <module>\r\n     30 history = {}\r\n     31 for e in range(ne):\r\n---> 32     history[e] = model.fit(ds, epochs=1, shuffle=False)\r\n     33     model.reset_states()\r\n     34 \r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    707         steps=steps_per_epoch,\r\n    708         validation_split=validation_split,\r\n--> 709         shuffle=shuffle)\r\n    710 \r\n    711     # Prepare validation data.\r\n\r\n/ws/miniconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2707       # Check that for stateful networks, number of samples is a multiple\r\n   2708       # of the static batch size.\r\n-> 2709       if x[0].shape[0] % batch_size != 0:\r\n   2710         raise ValueError('In a stateful network, '\r\n   2711                          'you should only pass inputs with '\r\n\r\nTypeError: 'DatasetV1Adapter' object is not subscriptable\r\n```", "comments": ["Issue is replicating with TF version-1.14, please find the gist of [Colab](https://colab.sandbox.google.com/drive/1N7agqrl9m2MTFK008TtJnxJA4vfBDBrF#scrollTo=-tSmJM4LQnUr). Thanks!", "@mimxrt I could reproduce the issue with tf-nightly. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/a2e382b13204b6852729ac48b1fae6d1/tf-31071_nightly.ipynb).\r\n\r\nHowever, TF2.0b1 is running without any issues. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/7547bf0796f78ea7f8078b1b396aab3c/tf20_31071.ipynb). Thanks!", "Thank you for testing this - good to know. However, I am unsure if I should migrate to TF2.0b1 now given it is still a beta version. To be fair, I was expecting 1.14 to be the more stable version of the two.", "I think the issue is fixed recently in beee660bfcc977bb65ca0c3bec4a3ea7756ee597.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31071\">No</a>\n"]}, {"number": 31070, "title": "Issue using tf.keras ModelCheckpoint when distributing under MultiWorkerMirroredStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-gpu 2.0.0b1\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10 / 7.4.1\r\n- GPU model and memory: 2 x GV100 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nRaises an error when using tf.keras.callbacks.ModelCheckpoint in the callbacks_list when training using keras under the MultiWorkerMirroredStrategy distribution strategy on a single machine.\r\n\r\nError is:\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: container=\"\", dtype=DT_INT32, shape=[], shared_name=\"cd2c89b7-88b7-44c8-ad83-06c2a9158347\"\r\n```\r\n\r\n**Describe the expected behavior**\r\nshould save a checkpoint model file and not crash\r\n\r\n**Code to reproduce the issue**\r\n\r\n**run script called - run_distributed_training_minimal_example.py**\r\n```\r\nimport json\r\nimport subprocess\r\n\r\nimport os\r\n\r\n\r\ndef create_TF_config(name, id):\r\n    return {\r\n        'cluster': {\r\n            'worker': ['localhost:9999']\r\n            ,'chief': ['localhost:9997']\r\n        },\r\n        'task': {'type': name, 'index': id}\r\n    }\r\n\r\n\r\ndef set_TF_CONFIG(id, name='worker'):\r\n    os.environ['TF_CONFIG'] = json.dumps(create_TF_config(name, id))\r\n\r\n\r\ndef start_processes(cluster_def, key, device=None):\r\n    process_list = []\r\n    if key in cluster_def:\r\n        for i, _ in enumerate(cluster_def[key]):\r\n            if device is not None:\r\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device)\r\n                device +=1\r\n            else:\r\n                os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\n\r\n            process_list.append(subprocess.Popen(['python', 'distributed_training_minimal_example.py', '--job-name='+key, '--job-id=' + str(i)]))\r\n    return process_list, device\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    cluster_def = create_TF_config(\"\",\"\")['cluster']\r\n\r\n    process_list = []\r\n    #this_list, device = start_processes(cluster_def, 'chief')\r\n    device=0\r\n    for key in ['chief','worker', 'ps']:\r\n        this_list, device = start_processes(cluster_def, key, device)\r\n        process_list.extend(this_list)\r\n\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\r\n    os.environ['TF_CONFIG'] = \"{}\"\r\n\r\n    for p in process_list:\r\n        p.wait()\r\n```\r\n\r\n**distributed worker script called - distributed_training_minimal_example.py**\r\n```\r\nfrom run_distributed_training_minimal_example import create_TF_config, set_TF_CONFIG\r\n\r\nuse_custom_check_point = False\r\n\r\ndef parse_arguments():\r\n    import argparse\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--job-name',\r\n                        type=str,\r\n                        default=\"worker\",\r\n                        help='type of job this process is running')\r\n    parser.add_argument('--job-id',\r\n                        type=int,\r\n                        default=0,\r\n                        help='id of this job type for this process to run')\r\n    return parser.parse_args()\r\n\r\nargs = parse_arguments()\r\n\r\ntf_config = create_TF_config(\"\", \"\")\r\ncluster_def = tf_config['cluster']\r\nset_TF_CONFIG(args.job_id, args.job_name)\r\n\r\nis_chief = args.job_name == 'chief'\r\nprint('is_chief:'+str(is_chief))\r\nbatchSize = len(cluster_def['worker'])\r\n\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nwith strategy.scope():\r\n\r\n    def create_simple_model():\r\n        return tf.keras.Sequential([\r\n            tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same',kernel_regularizer=tf.keras.regularizers.l2(0.04), input_shape = (128, 128, 1)),\r\n            tf.keras.layers.Conv2D(1, 3, activation='relu', padding='same',kernel_regularizer = tf.keras.regularizers.l2(0.04)),\r\n            tf.keras.layers.Dense(1, activation='softmax')\r\n        ])\r\n\r\n\r\n    def localised_cross_entropy(y_true, y_pred, ratio=1.0):\r\n        positive_error = ratio * y_true * tf.keras.backend.log(0.0000001 + y_pred)\r\n        negative_error = (1 - y_true) * tf.keras.backend.log(1.0000001 - y_pred)\r\n        errors = positive_error + negative_error\r\n        return tf.keras.backend.mean(errors)\r\n\r\n    def localised_cross_entropy_loss(y_true, y_pred, ratio=1.0):\r\n        return -localised_cross_entropy(y_true, y_pred, ratio)\r\n\r\n    def create_data():\r\n        import numpy as np\r\n        data_set = []\r\n        for i in range(20):\r\n            ip = np.random.random([128, 128, 1]).astype(np.float32)\r\n            op = np.random.randint(0, 2, [128, 128, 1]).astype(np.float32)\r\n            data_set.append((ip, op))\r\n        return data_set\r\n\r\n    model = create_simple_model()\r\n    model.summary()\r\n\r\n    trainingData = create_data()\r\n    model.compile('adam', loss=localised_cross_entropy_loss, metrics=[localised_cross_entropy])\r\n\r\n    split = int(len(trainingData)*0.8)\r\n    trainData, valData = trainingData[:split], trainingData[split:]\r\n\r\n    def create_RAM_generator(data):\r\n        while True:\r\n            for i in data:\r\n                yield i\r\n\r\n    def tensorflow_generator_training(data_getter, batchSize=None):\r\n        import tensorflow as tf\r\n\r\n        def __getter_generator():\r\n            while True:\r\n                item = next(data_getter)\r\n                yield item\r\n\r\n        shapes = ((None, None, 1), (None, None, 1))\r\n        dataset = tf.data.Dataset.from_generator(generator=__getter_generator, output_types=(tf.float32, tf.float32),output_shapes=shapes)\r\n        if batchSize is not None:\r\n            dataset = dataset.batch(batchSize)\r\n        return dataset\r\n\r\n    def generatorise(data):\r\n        train_gen = create_RAM_generator(data)\r\n        train_gen = tensorflow_generator_training(train_gen, batchSize=batchSize)\r\n        return train_gen\r\n\r\n    train_gen = generatorise(trainData)\r\n    val_gen = generatorise(valData)\r\n\r\nif not use_custom_check_point:\r\n    callbacks_list = [tf.keras.callbacks.ModelCheckpoint('tmp.hdf5')]\r\nelse:\r\n    from tensorflow.keras.callbacks import Callback\r\n\r\n\r\n    class CustomModelCheckpointCallback(Callback):\r\n        def __init__(self, path, model, is_chief_task):\r\n            super(CustomModelCheckpointCallback, self).__init__()\r\n            self.model = model\r\n            self.path = path\r\n            self.is_chief = is_chief_task\r\n\r\n        def on_epoch_end(self, epoch, logs=None):\r\n            if self.is_chief:\r\n                self.model.save(self.path)\r\n    callbacks_list = [CustomModelCheckpointCallback('tmp.hdf5', model, is_chief)]\r\n\r\nmodel.fit(train_gen, epochs=3, shuffle=False, callbacks=callbacks_list, validation_data=val_gen, steps_per_epoch=len(trainData), validation_steps=len(valData))\r\n```\r\n\r\nrunning first script will cause the issue.\r\nsetting use_custom_check_point to True in the second script will remove the error.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nF:\\ffa_dev\\deep-learning-dev-dist\\env\\Scripts\\python.exe F:/ffa_dev/deep-learning-dev-dist/run_distributed_training_minimal_example.py\r\nis_chief:False\r\nis_chief:True\r\n2019-07-26 12:00:27.116726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-07-26 12:00:27.117037: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-07-26 12:00:27.421095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:9e:00.0\r\n2019-07-26 12:00:27.421633: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:27.425381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:27.426214: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-07-26 12:00:27.443567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:9e:00.0\r\n2019-07-26 12:00:27.443960: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:27.448508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:27.497949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:5b:00.0\r\n2019-07-26 12:00:27.498407: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:27.502742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:27.503686: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-07-26 12:00:27.519979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:5b:00.0\r\n2019-07-26 12:00:27.520543: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:27.523778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:28.548291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:28.548686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:28.548929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:28.554809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)\r\n2019-07-26 12:00:28.569853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:9e:00.0\r\n2019-07-26 12:00:28.570305: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:28.573342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:28.573626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:28.573956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:28.574136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:28.576977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)\r\n2019-07-26 12:00:28.581376: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job chief -> {0 -> localhost:9997}\r\n2019-07-26 12:00:28.581735: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:9999}\r\n2019-07-26 12:00:28.599655: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:9999\r\n2019-07-26 12:00:28.602463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:9e:00.0\r\n2019-07-26 12:00:28.603395: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:28.607136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:28.607483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:28.607841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:28.608102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:28.611206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0726 12:00:28.611904 51176 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:GPU:0\r\n2019-07-26 12:00:28.613309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:28.613666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:28.613873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:28.618119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)\r\n2019-07-26 12:00:28.634094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:5b:00.0\r\n2019-07-26 12:00:28.634513: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:28.637145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:28.637447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:28.637741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:28.637928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:28.640533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:chief/replica:0/task:0/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)\r\n2019-07-26 12:00:28.646908: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job chief -> {0 -> localhost:9997}\r\n2019-07-26 12:00:28.647352: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:250] Initialize GrpcChannelCache for job worker -> {0 -> localhost:9999}\r\n2019-07-26 12:00:28.666115: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:9997\r\n2019-07-26 12:00:28.668862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:5b:00.0\r\n2019-07-26 12:00:28.669380: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:28.672907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:28.673317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:28.673744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:28.673983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:28.677930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0726 12:00:28.675424 29652 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nconv2d (Conv2D)              (None, 128, 128, 32)      320       \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 128, 128, 1)       289       \r\n_________________________________________________________________\r\nconv2d (Conv2D)              (None, 128, 128, 32)      320       \r\n_________________________________________________________________\r\ndense (Dense)                (None, 128, 128, 1)       2         \r\n=================================================================\r\nconv2d_1 (Conv2D)            (None, 128, 128, 1)       289       \r\n_________________________________________________________________\r\ndense (Dense)                (None, 128, 128, 1)       2         \r\n=================================================================\r\nTotal params: 611\r\nTrainable params: 611\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nTotal params: 611\r\nTrainable params: 611\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nW0726 12:00:30.992845 51176 deprecation.py:323] From F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n    \r\nW0726 12:00:30.992845 29652 deprecation.py:323] From F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ntf.py_func is deprecated in TF V2. Instead, there are two\r\n    options available in V2.\r\n    - tf.py_function takes a python function which manipulates tf eager\r\n    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\r\n    an ndarray (just call tensor.numpy()) but having access to eager tensors\r\n    means `tf.py_function`s can use accelerators such as GPUs as well as\r\n    being differentiable using a gradient tape.\r\n    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\r\n    (it is not differentiable, and manipulates numpy arrays). It drops the\r\n    stateful argument making all functions stateful.\r\n    \r\nW0726 12:00:31.031976 51176 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0726 12:00:31.031976 51176 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nW0726 12:00:31.031976 29652 distribute_coordinator.py:825] `eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nW0726 12:00:31.031976 29652 distribute_coordinator.py:829] `eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\n2019-07-26 12:00:31.036410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:92019-e07-:26 12:00.0\r\n00:31.2019-07-26 12:00:310.033668290: I tensor70: I flow/tensocorre/cflow/ommsotren_rauntime/gpm_execuu/gtorpu_/devpice.cc:16latfo4rm/0] defaultF/dlopeound dn_checkeevir_sce t0 wub.cc:25]ith pro perGPU libraries atiere sstati: \r\nnamcale: ly liQuadro nked, skiGV100 p dmajlor:open check.\r\n 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:5b:00.0\r\n2019-07-26 12:00:31.038318: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:31.042495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 2019-07-26 12:00:310\r\n.042772: I tensorflow/core/comm2019-07-26 12:on00:31.0429_runtime/gp9u/g2pu_: I devictene.cc:1763] sAorfdding visiblelow/co gpru deevi/ces: 0commo\r\nn_runtime/gpu/gpu_device.cc:1181] Device inte201rconnect9-0 StreamE7x-26 12ecutor wi:th 00:31.0437stren11: Igth 1  edge matritenx:\r\nsor2flo0w/cor19-07e/c-26 1o2:00:31mmon_ru.n044108time/:gpu /gpIu_device.cc: tensorflow/core/1c181ommon]_runtime/gpu /gpDevicu_devicee.c interconnect StreamExecutor c:1w1ith strength 1 8edge matrix:7\r\n]20 19-07-     0 \r\n2620 112:00:319-07-.20449886 12:00:31.:045 066I tensorflow/core:/co mmon_runtimIe/g tepnsorfu/gpu_devlow/cice.ccore/comm:on_1run187time/g]   p   0 \r\nu/g2019-pu_de0v7-26 1ice.cc2:00:31.0:456921200] 0: I: te   N \r\nnsorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:31.050217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)\r\nW0726 12:00:31.044387 51176 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:GPU:0\r\n2019-07-26 12:00:31.051178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)\r\n2019-07-26 12:00:31.054114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:9e:00.0\r\n2019-07-26 12:00:31.054942: I tensorflow/stream_executor/pW0726 12:00:31.054794 29652 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0\r\nlatform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:31.057696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: Quadro GV100 major: 7 minor: 0 memoryClockRate(GHz): 1.627\r\npciBusID: 0000:5b:00.0\r\n2019-07-26 12:00:31.058142: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-07-26 12:00:31.058764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:31.059078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:31.059395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:31.059590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:31.062322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-07-26 12:00:31.062664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-26 12:00:31.063059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-07-26 12:00:31.063278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-07-26 12:00:31.063637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:9e:00.0, compute capability: 7.0)\r\nW0726 12:00:31.062981 51176 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:worker/replica:0/task:0/device:GPU:0\r\n2019-07-26 12:00:31.066050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 25826 MB memory) -> physical GPU (device: 0, name: Quadro GV100, pci bus id: 0000:5b:00.0, compute capability: 7.0)\r\nW0726 12:00:31.065107 29652 cross_device_ops.py:1164] Some requested devices in `tf.distribute.Strategy` are not visible to TensorFlow: /job:chief/replica:0/task:0/device:GPU:0\r\n2019-07-26 12:00:31.075080: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:334] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n2019-07-26 12:00:31.076454: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:334] Cannot find shardable dataset, adding a shard node at the end of the dataset instead. This may have performance implications.\r\n2019-2019-07-26 12:00:31.126357: W tensorflow/core/grapp07-26 1ler2/op:00timizers/:d31.126335: W tensorfatal/auow/core/tgo_srappler/hard.cc:optim334i] Cannotzers/data/auto_shard fi.ccn:33d shardable4] C datasaet,n adnot fidnd ing a shashardabler d ndodeataset, add at theing a  sheardn nod ode at the end of the daftas the det insteaatadset. This  insteamay haved. This may have  performance implicapertions.f\r\normance implications.\r\nTrain on 16 steps, validate on 4 steps\r\nTrain on 16 steps, validate on 4 steps\r\nTraceback (most recent call last):\r\n  File \"distributed_training_minimal_example.py\", line 113, in <module>\r\nTraceback (most recent call last):\r\n  File \"distributed_training_minimal_example.py\", line 113, in <module>\r\n    model.fit(train_gen, epochs=3, shuffle=False, callbacks=callbacks_list, validation_data=val_gen, steps_per_epoch=len(trainData), validation_steps=len(valData))\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 643, in fit\r\n    model.fit(train_gen, epochs=3, shuffle=False, callbacks=callbacks_list, validation_data=val_gen, steps_per_epoch=len(trainData), validation_steps=len(valData))\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 776, in wrapper\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 776, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py\", line 360, in _run_single_worker\r\n    task_id, session_config, rpc_layer)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 771, in _worker_fn\r\n    return worker_fn(strategy)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 771, in _worker_fn\r\n    return fn(instance, model, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 681, in fit\r\n    return fn(instance, model, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_distributed.py\", line 681, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 252, in model_iteration\r\n    steps_name='steps_per_epoch')\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 252, in model_iteration\r\n    callbacks._call_begin_hook(mode)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 246, in _call_begin_hook\r\n    callbacks._call_begin_hook(mode)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 246, in _call_begin_hook\r\n    self.on_train_begin()\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 362, in on_train_begin\r\n    self.on_train_begin()\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 362, in on_train_begin\r\n    callback.on_train_begin(logs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 905, in on_train_begin\r\n    callback.on_train_begin(logs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 905, in on_train_begin\r\n    self.model, self.filepath))\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\distribute\\multi_worker_training_state.py\", line 60, in __init__\r\n    self.model, self.filepath))\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\keras\\distribute\\multi_worker_training_state.py\", line 60, in __init__\r\n    initial_value=CKPT_SAVED_EPOCH_UNUSED_VALUE, name='ckpt_saved_epoch')\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    initial_value=CKPT_SAVED_EPOCH_UNUSED_VALUE, name='ckpt_saved_epoch')\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 256, in _variable_v2_call\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 256, in _variable_v2_call\r\n    shape=shape)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 60, in getter\r\n    shape=shape)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 60, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1250, in creator_with_resource_vars\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1250, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 368, in _create_variable\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 368, in _create_variable\r\n    _real_mirrored_creator, *args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py\", line 251, in _create_mirrored_variable\r\n    _real_mirrored_creator, *args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py\", line 251, in _create_mirrored_variable\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 355, in _real_mirrored_creator\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py\", line 355, in _real_mirrored_creator\r\n    v = next_creator(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2551, in default_variable_creator_v2\r\n    v = next_creator(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2551, in default_variable_creator_v2\r\n    shape=shape)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 464, in __init__\r\n    shape=shape)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    shape=shape)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 618, in _init_from_args\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 464, in __init__\r\n    graph_mode=self._in_graph_mode)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 225, in eager_safe_variable_handle\r\n    shape=shape)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 618, in _init_from_args\r\n    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 141, in variable_handle_from_shape_and_dtype\r\n    container=container)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 1612, in var_handle_op\r\n    graph_mode=self._in_graph_mode)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 225, in eager_safe_variable_handle\r\n    shape, dtype, shared_name, name, graph_mode, initial_value)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 141, in variable_handle_from_shape_and_dtype\r\n    container=container)\r\n  File \"F:\\ffa_dev\\deep-learning-dev-dist\\env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\", line 1612, in var_handle_op\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: container=\"\", dtype=DT_INT32, shape=[], shared_name=\"cd2c89b7-88b7-44c8-ad83-06c2a9158347\"\r\n\t.  Registered:  device='CPU'\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n [Op:VarHandleOp] name: ckpt_saved_epoch/\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: container=\"\", dtype=DT_INT32, shape=[], shared_name=\"cd2c89b7-88b7-44c8-ad83-06c2a9158347\"\r\n\t.  Registered:  device='CPU'\r\n  device='GPU'; dtype in [DT_HALF]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n [Op:VarHandleOp] name: ckpt_saved_epoch/\r\n2019-07-26 12:00:31.325341: W tensorflow/core/common_runtime/eager/context.cc:232] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n2019-07-26 12:00:31.339402: W tensorflow/core/common_runtime/eager/context.cc:232] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n\r\nProcess finished with exit code 0\r\n```", "comments": ["I am experiencing the same issue.", "I am also experiencing the same issue.", "https://github.com/tensorflow/tensorflow/commit/6345ad553be2c23e09d7c3193533994eb522f635\r\n\r\nThis should be a fix, but not included in beta1 release", "Correct, https://github.com/tensorflow/tensorflow/commit/6345ad553be2c23e09d7c3193533994eb522f635 is the fix for this issue and should be available in the next release. Thanks for reporting the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31070\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31070\">No</a>\n", "I tried following 2 solutions\r\n1.I applied https://github.com/tensorflow/tensorflow/commit/6345ad553be2c23e09d7c3193533994eb522f635\r\nto my tensorflow installed code\r\n2. I install latest nightly dev build\r\n\r\nboth gave me following error, seems though previous commit change data type to int64, somewhere else still expects int32\r\n\r\n```\r\n2019-08-01 22:41:51.971726: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at collective_ops.cc:354 : Internal: RecvBufResponse returned 8 bytes where to_tensor expected 4\r\nTraceback (most recent call last):\r\n  File \"example_tf2.py\", line 124, in <module>\r\n    steps_per_epoch = parallel_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 643, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 776, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 771, in _worker_fn\r\n    return fn(instance, model, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_distributed.py\", line 681, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.py\", line 294, in model_iteration\r\n    batch_outs = f(actual_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\", line 813, in execution_function\r\n    return [out.numpy() for out in distributed_function(input_fn)]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py\", line 416, in __call__\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py\", line 359, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py\", line 1360, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py\", line 1648, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/function.py\", line 1541, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 716, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py\", line 309, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/func_graph.py\", line 706, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\ntensorflow.python.autograph.impl.api.StagingError: in converted code:\r\n\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py:804 distributed_function  *\r\n        outputs = strategy.experimental_run_v2(\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:708 experimental_run_v2\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1710 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:708 _call_for_each_replica\r\n        fn, args, kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:195 _call_for_each_replica\r\n        coord.join(threads)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py:389 join\r\n        six.reraise(*self._exc_info_to_raise)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py:297 stop_on_exception\r\n        yield\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:926 run\r\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.py:908 train_on_batch\r\n        output_loss_metrics=self._output_loss_metrics)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.py:307 train_on_batch\r\n        output_loss_metrics=output_loss_metrics))\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_eager.py:260 _process_single_batch\r\n        model.trainable_weights))\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:434 apply_gradients\r\n        self._create_hypers()\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:608 _create_hypers\r\n        aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:770 add_weight\r\n        aggregation=aggregation)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/tracking/base.py:713 _add_variable_with_custom_getter\r\n        **kwargs_for_getter)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:154 make_variable\r\n        shape=variable_shape if variable_shape else None)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:260 __call__\r\n        return cls._variable_v1_call(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call\r\n        shape=shape)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/shared_variable_creator.py:69 create_new_variable\r\n        v = next_creator(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1250 creator_with_resource_vars\r\n        return self._create_variable(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:368 _create_variable\r\n        _real_mirrored_creator, *args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:251 _create_mirrored_variable\r\n        value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:355 _real_mirrored_creator\r\n        v = next_creator(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:60 getter\r\n        return captured_getter(captured_previous, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope\r\n        lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py:264 __call__\r\n        return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/def_function.py:139 __init__\r\n        initial_value() if init_from_fn else initial_value,\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py:330 _overridden_initial_value_fn\r\n        group_key, collective_instance_key)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/collective_ops.py:161 broadcast_recv\r\n        instance_key=instance_key)\r\n    /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_collective_ops.py:66 collective_bcast_recv\r\n        _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    /root/.local/lib/python2.7/site-packages/six.py:737 raise_from\r\n        raise value\r\n\r\n    InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]\r\n\r\n\r\n```", "@jtang7, would you mind opening an issue with the steps to repro? Thanks."]}, {"number": 31069, "title": "Instructions for updating: Use standard file APIs to delete files with this prefix. While training chicken pose estimation algorithm", "body": "\r\n![11](https://user-images.githubusercontent.com/53338377/61946734-c710f300-afbc-11e9-8f08-b2eab9a06445.PNG)\r\n\r\nWhile training chicken pose estimation data on google colab using this pose estimation algorithm, this is the point where it stucks.  This is the command i am using;\r\n\"import os\r\nos.chdir(\"/content/gdrive/My Drive/pose-tensorflow-master/models/coco/train\")\r\n!ls ../../\r\n!TF_CUDNN_USE_AUTOTUNE=0 CUDA_VISIBLE_DEVICES=0 python3 ../../../train.py\"\r\nwhat should i do for that?", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Adding following line solved this issue for me.\r\n\r\ntf.compat.v1.disable_v2_behavior()"]}, {"number": 31068, "title": "CUDA_ERROR_ILLEGAL_ADDRESS but never on low power mode", "body": "**System information**\r\n- Windows 10\r\n- TensorFlow version 1.14 from PIP\r\n- python 3.6.8\r\n- cudatoolkit 10.0.130\r\n- cudnn 7.6.0\r\n- vs2015_runtime  14.15.26706\r\n- GPU model and memory: RTX 2070 - 6315 MB memory\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run TF code ildoonet/tf-pose-estimation on my RTX 2070 I have an issue about:\r\n\r\n`2019-07-26 11:57:11.701402: E tensorflow/stream_executor/cuda/cuda_driver.cc:1003] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-07-26 11:57:11.710473: E tensorflow/stream_executor/gpu/gpu_timer.cc:55] Internal: error destroying CUDA event in context 0x227ca310ac0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-07-26 11:57:11.717344: E tensorflow/stream_executor/gpu/gpu_timer.cc:60] Internal: error destroying CUDA event in context 0x227ca310ac0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2019-07-26 11:57:11.729093: F tensorflow/stream_executor/cuda/cuda_dnn.cc:189] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.`\r\nand... sometimes that crach windows.\r\n\r\nBut, If I disconnect the power plug in my laptop, It's works. \r\n\r\n\r\n**Code to reproduce the issue**\r\n- Install https://www.github.com/ildoonet/tf-pose-estimation\r\n- Execute the example about Web camera: `python .\\run_webcam.py --camera 0`\r\n\r\n\r\n\r\nThx :-)\r\n", "comments": ["@manuel-masiello ,\r\nCan you please provide minimal code to reproduce the issue.Thanks!", "Hello @anush-o,\r\n\r\nThe code to reproduce the error is not mine. You have to make a GIT clone on the project https://www.github.com/ildoonet/tf-pose-estimation, install the different libraries then launch the python script run_webcam.py\r\n\r\nThx for your help.", "@manuel-masiello ,\r\nWill it be possible to provide minimal code using which we can reproduce the same error?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]