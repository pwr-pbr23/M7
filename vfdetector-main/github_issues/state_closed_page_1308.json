[{"number": 13868, "title": "speech_commands svdf model does not work in android demo app.", "body": "== cat /etc/issue ===============================================\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.37)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.4.0rc1)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc1-3261-g934662e7b\r\ntf.COMPILER_VERSION = v1.3.0-rc1-3261-g934662e7b\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n/var/tmp/collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\n### Describe the problem\r\nunder tensorflow/examples/speech_commands, there are three models for speech commands. The \r\ndefault one(conv) and low-latency-conv works when I install them to android demo app(tensorflow/examples/android). \r\n\r\nlow-latency-svdf does not work in the android app, it works on pc though.\r\n\r\nIf i copy the svdf model into the app, app crashes when startup reporting model file error like this:\r\n\r\n`Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32, qint8, quint8, qint32\r\n; NodeDef: count_nonzero/Sum = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](count_nonzero/ToInt64, count_nonzero/Const); Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>`\r\n\r\n### Source code / logs\r\n\r\n> 10-20 17:08:30.889 23598-23598/org.tensorflow.demo.svdf D/AndroidRuntime: Shutting down VM\r\n10-20 17:08:30.894 23598-23598/org.tensorflow.demo.svdf E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                          Process: org.tensorflow.demo.svdf, PID: 23598\r\n                                                                          java.lang.RuntimeException: Unable to start activity ComponentInfo{org.tensorflow.demo.svdf/org.tensorflow.demo.SpeechActivity}: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/marvin_sheila_ll_svdf_graph.pb'\r\n                                                                              at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2817)\r\n                                                                              at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)\r\n                                                                              at android.app.ActivityThread.-wrap11(Unknown Source:0)\r\n                                                                              at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)\r\n                                                                              at android.os.Handler.dispatchMessage(Handler.java:105)\r\n                                                                              at android.os.Looper.loop(Looper.java:164)\r\n                                                                              at android.app.ActivityThread.main(ActivityThread.java:6541)\r\n                                                                              at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                              at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\r\n                                                                              at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\r\n                                                                           Caused by: java.lang.RuntimeException: Failed to load model from 'file:///android_asset/marvin_sheila_ll_svdf_graph.pb'\r\n                                                                              at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:113)\r\n                                                                              at org.tensorflow.demo.SpeechActivity.onCreate(SpeechActivity.java:151)\r\n                                                                              at android.app.Activity.performCreate(Activity.java:6975)\r\n                                                                              at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1213)\r\n                                                                              at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2770)\r\n                                                                              at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2892)\u00a0\r\n                                                                              at android.app.ActivityThread.-wrap11(Unknown Source:0)\u00a0\r\n                                                                              at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1593)\u00a0\r\n                                                                              at android.os.Handler.dispatchMessage(Handler.java:105)\u00a0\r\n                                                                              at android.os.Looper.loop(Looper.java:164)\u00a0\r\n                                                                              at android.app.ActivityThread.main(ActivityThread.java:6541)\u00a0\r\n                                                                              at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                                                                              at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:240)\u00a0\r\n                                                                              at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:767)\u00a0\r\n                                                                           Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32, qint8, quint8, qint32\r\n                                                                          \t; NodeDef: count_nonzero/Sum = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](count_nonzero/ToInt64, count_nonzero/Const); Op<name=Sum; signature=input:T, reduction_indices:Tidx -> output:T; attr=keep_dims:bool,default=false; attr=T:type,allowed=[DT_FLOAT, DT_INT32, DT_QINT8, DT_QUINT8, DT_QINT32]; attr=Tidx:type,default=DT_INT32,allowed=[DT_INT32, DT_INT64]>\r\n                                                                              at org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:535)\r\n                                                                              at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:105)\r\n                                                                              \t... 13 more\r\n", "comments": ["i met same issue...", "this issue is caused by the count_nonzero method in svdf mode. there is int64 cast in this method which won't be supported by _slim_android build. the other two models don't use count_nonzero, so they don't have this problem.\r\n\r\nafter fix the cast int64 issue, you also need to add not_equal op in bazel build. hope it helps.", "I've pinged the author of the SVDF model for more info.", "Any update regarding this issue ? I am facing the same problem with the SVDF model. Please rectify this.", "do you guys have any idea about locating the operation file, then changing the registry for supporting int64? i'm facing the same issue for days.", "+1", "Nagging Assignee @petewarden: It has been 90 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 13867, "title": "added CMake install targets", "body": "This way you can use cmake to install tensorflow into some location. It is easier to package tensorflow this way.", "comments": ["Can one of the admins verify this patch?", "Friendly ping for updates.", "@vrv I'm waiting for review from @petewarden and @mrry. Solution works, I can provide logs of tensorflow build and example program build.", "What about porting the examples (mainly tf_tutorials_example_trainer and label_image) from an in-source-example to an installing-example? (Providing an example based on the installed tensorflow)\r\n\r\nJoe Antognini provides an example in his blog. **https://joe-antognini.github.io/machine-learning/windows-tf-project**. ", "@PinkySan I think it is a good idea, though I want to merge this PR and few more pressing changes back to upstream first.", "@Slonegg What about providing an additional find_package module? It will be easier integrating tensorflow in existing cmake projects. (sorry for adding additional requrements. I was really looking forward to a PR like this. Good work!)", "@PinkySan I need to fix few things in tensorflow, so we can use it in our application.\r\n1. Add ability to use external GRPC, zlib, Eigen and mb some other packages. Because right now it is impossible to compile your project with tensorflow if you're using different version of GRPC. I did something similar to GRPC package whether you specify tensorflow_<LIB>_PROVIDER variable. It will determine if tensorflow needs to download dependency and use it as a module or use find_package(<LIB>) to find <LIB> package.\r\n2. Add tensorflow_BUILD_STATIC_LIB variable to create static library. With all kernels and no extra functionality tensorflow is several gigs in size. It's too heavy to deploy it as a shared library in microservice application. While if we link static tensorflow library, our final executable hopefully will be smaller.\r\n\r\nAs for what you are suggesting, I think tensorflow config will suffice. You will need to specify tensorflow_DIR variable to use find_package(tensorflow). We don't use find_package functionality in our projects a lot, since we're using conan package manager. And in conan recipe you can specify all necessary include dirs, libraries, etc. But in general, yes, package config or find module is desired.", "All of these sounds like great future contributions! Let's get this PR in as a starting point though :).\r\n\r\n@tensorflow-jenkins test this please."]}, {"number": 13866, "title": "Branch 172924803", "body": "", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Known bug with pending PR to fix, merging."]}, {"number": 13865, "title": "Feature Request: Support for None values in tf.contrib.data.Dataset", "body": "It would be very handy if the Dataset API supports `None` types. The idea is to be able to use the same `Iterator` object for the training and the test datasets. As the training dataset contains labels and the test dataset does not, the only workaround I know at the moment is to use some dummy labels in order to make the two datasets compatible with the same `Iterator`. This can waste a lot of memory though and is not a clean solution. Instead, maybe it can be possible to create a `Dataset` from `None`, that behaves in a way such that its `output_types` and `output_shapes` are compatible with any other type and shape, but does not consume so much memory. Here is a quick example:\r\n\r\n```\r\nX_train = tf.contrib.data.Dataset.from_tensor_slices(X_train_data)\r\ny_train = tf.contrib.data.Dataset.from_tensor_slices(y_train_data)\r\ndata_train = tf.conrib.Dataset.zip((X_train, y_train))\r\n\r\nX_test = tf.contrib.data.Dataset.from_tensor_slices(X_test_data)\r\ny_test = tf.contrib.data.Dataset.from_tensor_slices(None)\r\ndata_test = tf.conrib.Dataset.zip((X_test, y_test))\r\n\r\nassert data_train.output_types == data_test.output_types\r\nassert data_train.output_shapes == data_test.output_shapes\r\n\r\niterator = Iterator.from_structure(data_train.output_types, data_train.output_shapes)\r\n\r\ntrain_init_op = iterator.make_initializer(data_train)\r\ntest_init_op = iterator.make_initializer(data_test)\r\n\r\n# Build the graph ...\r\n\r\n# Train network\r\nwith tf.Session() as sess:\r\n  sess.run(train_init_op)\r\n  # Train ...\r\n\r\n# Run in prediction mode\r\nwith tf.Session() as sess:\r\n  sess.run(test_init_op)\r\n  # Get predictions ...\r\n\r\n```\r\n", "comments": ["@mrry WDYT?", "Hmm, I'm concerned that using `None` for this purpose doesn't give enough information. For example,  let's take the code fragment defining `data_test`:\r\n\r\n```python\r\nX_test = tf.contrib.data.Dataset.from_tensor_slices(X_test_data)\r\ny_test = tf.contrib.data.Dataset.from_tensor_slices(None)\r\ndata_test = tf.conrib.Dataset.zip((X_test, y_test))\r\n```\r\n\r\nAt this point, what is the value of `data_test.output_types[1]` and `data_test.output_shapes[1]`? It's only implied by code later in the snippet:\r\n\r\n```\r\niterator = Iterator.from_structure(data_train.output_types, data_train.output_shapes)\r\n# ...\r\ntest_init_op = iterator.make_initializer(data_test)\r\n```\r\n\r\n...and I don't see how the two `assert` statements would be able to pass. It seems like you want something \"stronger\" than an `assert`, which can go back a few lines in the code and cause `y_test` to have the appropriate type and shape.\r\n\r\nDid you have an approach in mind that would make this work?\r\n\r\n---\r\n\r\nFor the record though, you don't need to waste memory to tack on a dummy value to the test dataset. For example `Dataset.from_tensors(0).repeat(len(X_test_data))` only allocates a single tensor containing 0 and returns shallow copies of it for each element. This allows it to be used with much larger datasets than ones that fit in a NumPy array.", "In that case, seems like `Dataset.from_tensors(0).repeat(len(X_test_data))` can often do the job.\r\n\r\nAs far as the feature request goes, maybe something like this will be more clear\r\n```\r\ny_test = tf.contrib.data.Dataset.empty(output_types, output_shapes)\r\n# or\r\ny_test = tf.contrib.data.Dataset.empty(y_train_data)\r\n```\r\nThe idea is that just some empty Dataset is created, which can be compatible with `y_train` provided that the same `output_types` and `output_shapes` are supplied (or alternatively an example tensor for `output_types` and `output_shapes` is given). Not sure if this provides any sensible benefits other than maybe code clarity. Potentially it can be useful in some strange cases when the labels are vectors or something more than a single number (e.g. when learning some distributions for which not all the density is concentrated in a single bin). \r\n\r\nUp to you to close the issue if you think there will be no real benefit.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Catching up on old issues: In the interests of keeping the `tf.data` API as compact as possible, and since we have a workaround that's not too bad, I'm going to close this out."]}, {"number": 13864, "title": "updated unit test case", "body": "The following PR contains the required unit test case regarding issue https://github.com/tensorflow/tensorflow/issues/11673 \r\nand it also has a fix in another pull request https://github.com/tensorflow/tensorflow/pull/13829", "comments": ["Can one of the admins verify this patch?", "Can you please create one PR that contains both of them, so we can test them at the same time?  Thanks!", "please find the merged pull requests here https://github.com/tensorflow/tensorflow/pull/13877 ."]}, {"number": 13863, "title": "Fix crash when `int64` axis is passed to `tf.reduce_sum`", "body": "This fix tries to fix the crash triggered by `int64` axis passed to `tf.reduce_sum`:\r\n```\r\nubuntu@ubuntu:~/tensorflow2$ (cd && python)\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> v = tf.reduce_sum([1,2,3], tf.constant(0, tf.int64))\r\n2017-10-20 15:55:06.993430: F tensorflow/core/framework/tensor.cc:601] Check failed: dtype() == expected_dtype (9 vs. 3)\r\nubuntu@ubuntu:~/tensorflow2$\r\n```\r\n\r\nThe issue is caused by the fact that shape inference in `common_shape_fns.cc` only assumes int32 without proper handling of diffent types. In `math_ops.cc` both int32 and int64 are mentioned.\r\n\r\nNOTE that this fix does not address the issue that int64 is not supported. To allow int64 axis it is more than adding a template in `ReductionOp` as the type of the axis seems to be decided by some other ways in Eigen. Will investigate that later.\r\n\r\nThis fix merely fixed the crash so that an error message will return without exit from the python program \"No OpKernel was registered to support Op 'Sum' with these attrs\".\r\n\r\nStill, its worth to at least allow the program not to exit in case an error happens.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please\r\n\r\n(Surprised this never came up before! thanks for the fix :)"]}, {"number": 13862, "title": "Fix issues where int64 crops could not be passed to batch_to_space.", "body": "This fix tries to address the issue where int64 `crops` could not be passed to `batch_to_space` even though both int32 and int64 are specified as supported in the docs (tf.batch_to_space.__doc__)\r\n\r\nThe reason is that BatchToSpace kernel puts a constraint of int32 to crops data types.\r\n\r\nThis fix removed the constraint so that int64 `crops` could be supported.\r\n\r\nNOTE: Just removing the constraint should work and it is not necessary to add specification to the kernel class template, as `SubtleMustCopyFlat` called in the class already correctly handled both int32 and int64 cases. Besides, other data types (e.g., float or double) will not be passed to the kernel as they are guarded by the specification in `array_ops.cc`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 13861, "title": "Channel number of convolution output is unspecified when atrous rate > 1 ", "body": "Python 2.7\r\nTensorflow v1.2.0-rc2-21-g12f033d and v1.3.0-rc1-2361-gd1286ab\r\n\r\n\r\nEasily reproduced by the following code:\r\n``` Python\r\nx = tf.placeholder(tf.float32, [1, 10, None, None])\r\nw = tf.zeros([3, 3, 10, 20])\r\ny = tf.nn.convolution(x, w, 'VALID', [1]*2, [2]*2, data_format='NCHW')\r\nprint(y)\r\n```\r\nThe input has known channel number but unknown width and height, in NCHW format. The convolution has rate >= 2. The channel number of the output `y` is obviously known, `(1, 20, None, None)` in this example, depending on the shape of `w`. However the execution result gives \r\n```\r\nTensor(\"convolution/BatchToSpaceND:0\", shape=(1, ?, ?, ?), dtype=float32)\r\n```\r\nI believe this is related to the reshaping operation in the atrous convolution. The issue happens only when the spatial shape is unknown and with NCHW format. Not sure if this happens to the latest version.\r\n\r\n", "comments": ["It is reproduced on nightly version.\r\n\r\n```python\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.__version__\r\nOut[2]: '1.5.0-dev20171019'\r\n\r\nIn [3]: %paste\r\nx = tf.placeholder(tf.float32, [1, 10, None, None])\r\nw = tf.zeros([3, 3, 10, 20])\r\ny = tf.nn.convolution(x, w, 'VALID', [1]*2, [2]*2, data_format='NCHW')\r\nprint(y)\r\n\r\n## -- End pasted text --\r\nTensor(\"convolution/BatchToSpaceND:0\", shape=(1, ?, ?, ?), dtype=float32)\r\n```\r\n\r\nAs described by @vs-zhehangd, however NHWC is OK:\r\n```python\r\nIn [51]: x = tf.placeholder(tf.float32, [1, None, None, 10])\r\n\r\nIn [52]: w = tf.zeros([1, 3, 10, 20])\r\n\r\nIn [53]: tf.nn.convolution(x, w, 'VALID', dilation_rate=[2, 2], data_format='NHWC')\r\nOut[53]: <tf.Tensor 'convolution_29/BatchToSpaceND:0' shape=(1, ?, ?, 20) dtype=float32>\r\n```", "If I understand it correctly, since [`space_to_batch_nd`](https://www.tensorflow.org/versions/master/api_docs/python/tf/space_to_batch_nd) assumes that `input_shape = [batch] + spatial_shape + remaining_shape`, hence `paddings` is hacked as the tensor `[zeros, paddings]` to handle NC format. Because the value of tensor is hidden by itself, that's why we lost the channel information in both `space_to_batch_nd` and `batch_to_space_nd`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d7409d32bba5ffa89141ec5427780f68a3b6942d/tensorflow/python/ops/nn_ops.py#L485-L492\r\n\r\nThere are two solutions for the bug in my opinion:\r\n1. use some methods to rewrite the correct `num_output_channels` at the last if possible.\r\n2. modified `space_to_batch_nd` and `batch_to_space_nd` to handle `channle_at_the_first` case. Perhaps difficult.\r\n\r\nWhich one is better? Or other alternative solution? Thanks.", "For me, at application level I simply do\r\n```\r\nnout = ...\r\nformat = 'NCHW'\r\ny = tf.nn.convolution(x, ..., data_format=format)\r\nif format == 'NCHW':\r\n    y.set_shape([None, nout, None, None])\r\n```", "@ebrevdo WDYT? Should we encourage a contribution?", "The convolution function can call an appropriate set_shape in this special case.  Contributions welcome.", "Hi, @vs-zhehangd . I get a cold, would you like to take over the issue? The fix is pretty easy as you suggested, however, we'd better create a `nn_ops_test.py` and write more unit tests. It's not difficult, but perhaps a little cumbersome for new contributor. Thanks.", "Ok, let me take a look and see how to do it.\n\nOn Sun, 22 Oct 2017 at 2:23 AM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\nwrote:\n\n> Hi, @vs-zhehangd <https://github.com/vs-zhehangd> . I get a cold, would\n> you like to take over the issue? The fix is pretty easy as you suggested,\n> however, we'd better create a nn_ops_test.py and write more unit tests.\n> It's not difficult, but perhaps a little cumbersome for new contributor.\n> Thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13861#issuecomment-338463306>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Acfm2WIvGFGXL9_IwpSNlIIKJBMjlCZ4ks5suwl-gaJpZM4QBDL4>\n> .\n>\n", "There are conv unit tests in kernel_tests.\n\nOn Sun, Oct 22, 2017, 2:25 AM vs-zhehangd <notifications@github.com> wrote:\n\n> Ok, let me take a look and see how to do it.\n>\n> On Sun, 22 Oct 2017 at 2:23 AM Yan Facai (\u989c\u53d1\u624d) <notifications@github.com>\n> wrote:\n>\n> > Hi, @vs-zhehangd <https://github.com/vs-zhehangd> . I get a cold, would\n> > you like to take over the issue? The fix is pretty easy as you suggested,\n> > however, we'd better create a nn_ops_test.py and write more unit tests.\n> > It's not difficult, but perhaps a little cumbersome for new contributor.\n> > Thanks.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> https://github.com/tensorflow/tensorflow/issues/13861#issuecomment-338463306\n> >,\n> > or mute the thread\n> > <\n> https://github.com/notifications/unsubscribe-auth/Acfm2WIvGFGXL9_IwpSNlIIKJBMjlCZ4ks5suwl-gaJpZM4QBDL4\n> >\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13861#issuecomment-338463601>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3VmP7K84EzXBKYL9h3OFPzFDtlIks5suwoKgaJpZM4QBDL4>\n> .\n>\n", "@vs-zhehangd Hi, how's it going? Do you need help, or let me take it back?", "@facaiy  I found that I couldn't find a good time to deal with this problem. Probably it is better to return this to you. Sorry for the inconvenience.", "OK. Don't worry :-)"]}, {"number": 13860, "title": "cudnnGRU is_training placeholder", "body": "When creating a model with batch normalisation I can supply a place-holder for the is_training param like so:\r\n\r\n```\r\ntraining = tf.placeholder(tf.bool)  \r\nsym = create_symbol(training)\r\n# ....\r\n# Training: sess.run(model, feed_dict={X: data, y: label, training: True})\r\n# Inference: sess.run(pred, feed_dict={X: data, training: False})\r\n```\r\n\r\nHowever when I do this for a symbol that contains cudnnGRU (or cudnnLSTM), it doesn't like the place-holder:\r\n\r\n```\r\ncudnn_cell = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, \r\n                                           num_units=NUMHIDDEN, \r\n                                           input_size=EMBEDSIZE)    # Set params\r\nparams_size_t = cudnn_cell.params_size()\r\nparams = tf.Variable(tf.random_uniform([params_size_t]), validate_shape=False)   \r\ninput_h = tf.Variable(tf.zeros([1, BATCHSIZE, NUMHIDDEN]))\r\noutputs, states = cudnn_cell(is_training=training ,\r\n                             input_data=word_list,\r\n                             input_h=input_h,\r\n                             params=params)\r\n```\r\n\r\nError message:\r\n\r\n> \r\nTypeError: Expected bool for argument 'is_training' not <tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=bool>.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13859, "title": "Merge two GPU kernel launching to one in DiagOp", "body": "The PR is the following update for the [discussion](https://github.com/tensorflow/tensorflow/pull/13666#discussion_r146011795) of #13666, which tries to merge two GPU kernel launching to one launching in DiagOp, as @ekelsen suggested.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13858, "title": "Update the C++ API guide [Doc only, should be in 1.4]", "body": "- Includes a documentation fix for 1.4 (cc_binary -> tf_cc_binary to avoid undefined symbols).\r\n- Adds the standard warning at the top that people may want the master branch.\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/13855.", "comments": []}, {"number": 13857, "title": "Trying to import tensorflow but im getting this im using the cuda 9 and cudnn 7", "body": "Traceback (most recent call last):\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/mohammad/.local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/mohammad/.conda/envs/my_root/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["i have checked the cuda and cudnn instalations and they passed the tests\r\nbut i cant use tensorflow", "No prebuilt tenosrflow supports cuda 9.0 yet. You need to build it from source with cuda 9.", "We've uploaded prebuilt wheels: https://github.com/mind/wheels/releases/tag/tf1.4-gpu-cuda9", "You can build it from source using steps in following website http://www.python36.com/install-tensorflow141-gpu/ , let me know if it worked for you.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 13856, "title": "AttributeError: module 'tensorflow' has no attribute 'estimator'", "body": "Hi, for some reason 'estimator' isn't an attribute of 'tensorflow' when I test it on Mac. \r\n\r\nWhen I try:\r\n\r\n`tf.estimator.Estimator(model_fn)`\r\n\r\nI get this error \r\n\r\n`AttributeError: module 'tensorflow' has no attribute 'estimator'`\r\n\r\nThis works fine in Linux though. Upgrading pip/conda/tensorflow did not affect this behavior. Is there a workaround?\r\n\r\n-Thanks\r\n\r\n### System information\r\n- Mac OS X 10.9.5 (x86_64-apple-darwin13.4.0)\r\n- TensorFlow installed from binary\r\n- TensorFlow version v1.3.0-rc1-3628-g49f9c6f89 1.5.0-dev20171020\r\n- Python 3.6.3\r\n\r\n", "comments": ["This was a persistent error that somehow was resolved (after multiple restarts of terminal). Since I don't know how to reproduce it anymore I think this thread can be closed.", "Hello,\r\n\r\nI am having the same problem:\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nmy_optimizer= tf.train.GradientDescentOptimizer(learning_rate=0.00000001)\r\n\r\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-36-49765c0f1466>\", line 1, in <module>\r\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\r\n\r\nAttributeError: module 'tensorflow.contrib' has no attribute 'estimator'\r\n```\r\n\r\nBut estimator is in tensorflow.contrib:\r\n```\r\n\r\nhelp (tensorflow.contrib)\r\nHelp on package tensorflow.contrib in tensorflow:\r\n\r\nNAME\r\n    tensorflow.contrib\r\n\r\nPACKAGE CONTENTS\r\n    all_reduce (package)\r\n    batching (package)\r\n    boosted_trees (package)\r\n    cluster_resolver (package)\r\n    coder (package)\r\n    decision_trees (package)\r\n    eager (package)\r\n    estimator (package)\r\n    feature_column (package)\r\n    fused_conv (package)\r\n    ...\r\n```\r\n\r\nI am having the same problem:\r\n\r\nimport tensorflow as tf\r\n\r\nmy_optimizer= tf.train.GradientDescentOptimizer(learning_rate=0.00000001)\r\n\r\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-36-49765c0f1466>\", line 1, in <module>\r\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\r\n\r\nAttributeError: module 'tensorflow.contrib' has no attribute 'estimator'\r\nBut estimator is in tensorflow.contrib:\r\n\r\n```\r\n    help (tensorflow.contrib)\r\nHelp on package tensorflow.contrib in tensorflow:\r\n\r\nNAME\r\n    tensorflow.contrib\r\n\r\nPACKAGE CONTENTS\r\n    all_reduce (package)\r\n    batching (package)\r\n    boosted_trees (package)\r\n    cluster_resolver (package)\r\n    coder (package)\r\n    decision_trees (package)\r\n    eager (package)\r\n    estimator (package)\r\n    feature_column (package)\r\n    fused_conv (package)\r\nin fact this same error happens with any function from tensorflow.contrib.\r\n\r\nhelp (tensorflow.contrib.boosted_trees)\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-38-b173108b832b>\", line 1, in <module>\r\n    help (tensorflow.contrib.boosted_trees)\r\n\r\nAttributeError: module 'tensorflow.contrib' has no attribute 'boosted_trees'\r\n\r\n```\r\n\r\nI am using tensorflow 1.7.\r\nI am using anaconda, spider, windowns 10 .\r\nI am going through this tutorial, if more details of the code are important: https://colab.research.google.com/notebooks/mlcc/first_steps_with_tensor_flow.ipynb?hl=en#scrollTo=ubhtW-NGU802\r\n\r\nAny idea of what is wrong?\r\n\r\n", "I didn't encounter this error again. Contrib packages apparently are not very stable. So they may change between versions. My guess is that for me it was an installation issue. Have you tried uninstalling tensorflow and reinstalling it?", "Problem solved.\r\n\r\nThere was an environment issue, tensorflow was running in its own environment and not at base, at anaconda. A friend reinstalled a few packages at the base environment and deleted the tensorflow environmnet. I can't really say what he did, we had several trials before figuring out the problem and what to do.\r\n", "Good to hear that.", "I am having this now. Any hints on how to get rid of it. I have TF-1.7 from Anaconda and I just installe the current tensorflow-hub. I am running on linux.", "@mhlr seems like for me and Victor this was an installation/environment issue. I recommend uninstalling and reinstalling tensorflow.", "Solved. The problem was between the chair and the keyboard :)", "Haha, glad it's resolved.", "me too faced the same issue. Did the following steps\r\n1) uninstall tensor flow\r\n2) reinstall tensor flow and tensor flow hub\r\nand it worked :-)", "> I didn't encounter this error again. Contrib packages apparently are not very stable. So they may change between versions. My guess is that for me it was an installation issue. Have you tried uninstalling tensorflow and reinstalling it?\r\n@javadnoorb \r\nhi, javadnoorb, i have tried, but it still do not work for me. i am using python 3.6. any other idea?", "@jlinleung can you confirm that you don't have multiple versions in different paths? Maybe installing a clean version in a new conda/python environment would resolve it. Alternatively (and this can be dangerous) you might want to VERY CAREFULLY delete any folder in your path that contains 'tensorflow' in the name.", "FYI for anyone who comes across this in future - `tf.estimator` doesn't seem to exist prior to tensorflow r1.1. (Verify here using the branch dropdown at top left: https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/python/estimator/estimator.py)\r\n\r\nUsers above who resolved by uninstalling and reinstalling Tensorflow probably upgraded to most recent version.", "i've got the same problem : /", "I had the same problem, even with upgrading tensorflow.\r\nHowever, with 'pip freeze | grep tensorflow' I saw that I had a 'tensorflow-estimator' package that keeps being installed even if I uninstalled tensorflow. After uninstalling it and deleting some related folder in site-packages, and re-installing tensorflow, everything worked.\r\n(I'm not sure that this is the best way to solve this problem, but if you are stuck that could help)", "@morvan-s thanks for your suggestion: I renamed two estimator-related directories and now my code works! ", "Faced the same issue with tensorflow 2.3.0 and tensorflow-hub 0.10.0.\r\nI just solved it by uninstalling tensorflow-estimator, tensorflow-hub and tensorflow,\r\nthen installed tensorflow and tensorflow-hub.\r\n\r\nNow it's working :-)"]}, {"number": 13855, "title": "compile source code fail on mac", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**: \r\n1.3.0\r\n- **Python version**: \r\nPython 2.7.10\r\n- **Bazel version (if compiling from source)**:\r\nmacbookpro:tensorflow fredlee$ bazel version\r\nBuild label: 0.7.0-homebrew\r\nBuild target: bazel-out/darwin_x86_64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Oct 19 09:12:48 2017 (1508404368)\r\nBuild timestamp: 1508404368\r\nBuild timestamp as int: 1508404368\r\n- **CUDA/cuDNN version**:\r\nno\r\n- **GPU model and memory**:\r\nno \r\n- **Exact command to reproduce**:\r\n\r\n\r\nstep by step according to this guide\r\n   step1:\r\n       git clone https://github.com/tensorflow/tensorflow\r\n  step2:\r\n      cd tensorflow\r\n      ./configure\r\n  step3:\r\n     create an example as (https://tensorflow.google.cn/api_guides/cc/guide)\r\n  step4:\r\n    bazel run -c opt //tensorflow/cc/example:example\r\n\r\n  it return\r\n      \r\n\r\n> ld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/cc/example:example failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3083.675s, Critical Path: 306.19s\r\nERROR: Build failed. Not running target.\r\n\r\ncompile log:\r\n[compile.log.zip](https://github.com/tensorflow/tensorflow/files/1401503/compile.log.zip)\r\n\r\n\r\n****can you tell me how to make it work?** thanks**\r\n\r\n**best wishes.**\r\n", "comments": ["There are two issues. First, the documentation is out of date (cc_binary -> tf_cc_binary to link in libtensorflow_framework.so). Second, even if it were updated at head, that's the 1.3 version of the documentation (CC @MarkDaoust; I'll send you a CL for both).", "Thanks @allenlavoie !", "With `tf_cc_binary` in the `BUILD` file I get:\r\n\r\n    ERROR: /..../tensorflow/tensorflow/cc/example/BUILD:1:1: name 'tf_cc_binary' is not defined (did you mean 'cc_binary'?).\r\n", "Yes, you also need `load(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")` at the top to define it.", "The 1.3 docs will be replaced by 1.4 soon. The r1.4 branch is cut, so if you want this in the docs before 1.5 you will need to cherry-picked the commits onto that branch (docs changes can be cherry-picked with out affecting the release). ", "Ah that worked, thanks!", "The documentation should be fixed now. Thank you for the report!\r\n\r\n(Well, once the 1.4 branch gets merged into Master and the docs get regenerated)"]}, {"number": 13854, "title": "tf.train.SyncReplicasOptimizer training does not start", "body": "System information\r\n- **Have I written custom code **: Yes\r\n- **OS Platform and Distribution **: Linux Ubuntu 16.04\r\n- **TensorFlow installed from **: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==1.3.0\r\n- **Python version**: Python 3.6.2\r\n- **Bazel version (if compiling from source)**: not installed\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61\r\n- **GPU model and memory**: NVIDIA Quadro P5000 16GB\r\n- **Exact command to reproduce**:\r\nwrite actual IP addresses instead of  ip_address1 and ip_address2 \r\n\r\n( on machine 1 )\r\n$  python trainer.py \\\r\n    --replicas_num=1 \\\r\n    --ps_hosts=ip_address1:2222 \\\r\n    --worker_hosts=ip_address2:2223 \\\r\n    --job_name=ps --task_index=0\r\n\r\n( on machine 2 )\r\n$  python trainer.py \\\r\n    --replicas_num=1 \\\r\n    --ps_hosts=ip_address1:2222 \\\r\n    --worker_hosts=ip_address2:2223 \\\r\n    --job_name=worker --task_index=0\r\n\r\n### Describe the problem\r\nI'm trying to train an below model with distributed synchronized training.\r\nI tried, 1 ps and 1 worker, 1 ps and 2 workers, 1 ps and 3 workers.\r\nIt is a sample code similar to https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program\r\n\r\nIf I comment out line 71 shown below training became asynchronized trainig and there is no problem.\r\nopt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)\r\n\r\nBut if I run it with line 71 not commented out training will not start.\r\n(I followed https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage to make it synchronized.)\r\n\r\nThere is no error. But trainig will not continue. sess.run() (line 96) never ends.\r\nIs there any suggestions what might be the problem ?\r\n\r\n### Source code / logs\r\n#### Source code\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nflags = tf.flags\r\nflags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\nflags.DEFINE_integer(\"replicas_num\", 3, \"Number of replicas\")\r\nFLAGS = flags.FLAGS\r\n\r\ndef main(_):\r\n    # config\r\n    BATCH_SIZE = 10\r\n    TRAINING_STEPS = 5000\r\n    PRINT_EVERY = 100\r\n    LOG_DIR = \"/tmp/\"\r\n\r\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n    # cluster specification\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n    # start a server for a specific task\r\n    server = tf.train.Server(cluster,\r\n                             job_name=FLAGS.job_name,\r\n                             task_index=FLAGS.task_index)\r\n\r\n    mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\r\n\r\n    def net(x):\r\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\r\n        net = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')\r\n        net = slim.layers.max_pool2d(net, [2, 2], scope='pool1')\r\n        net = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')\r\n        net = slim.layers.max_pool2d(net, [2, 2], scope='pool2')\r\n        net = slim.layers.flatten(net, scope='flatten')\r\n        net = slim.layers.fully_connected(net, 500, scope='fully_connected')\r\n        net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='pred')\r\n        return net\r\n\r\n    print(\"job_name     = %s\" % FLAGS.job_name)\r\n    print(\"task_index   = %d\" % FLAGS.task_index)\r\n    print(\"replicas_num = %d\" % FLAGS.replicas_num)\r\n\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n        # Between-graph replication\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster)):\r\n            # count the number of updates\r\n            global_step = tf.get_variable('global_step', [],\r\n                                          initializer=tf.constant_initializer(0),\r\n                                          trainable=False)\r\n\r\n            x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\r\n            # target 10 output classes\r\n            y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\r\n            y = net(x)\r\n\r\n            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\r\n            #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n\r\n            opt = tf.train.AdamOptimizer(1e-4)\r\n\r\n            # for synchronous training\r\n            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)\r\n\r\n            train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n            #sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0))\r\n\r\n            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n            init_op = tf.global_variables_initializer()\r\n\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                                 logdir=LOG_DIR,\r\n                                 global_step=global_step,\r\n                                 init_op=init_op)\r\n\r\n        with sv.managed_session(server.target) as sess:\r\n            step = 0\r\n\r\n            test = 0\r\n            while not sv.should_stop() and step <= TRAINING_STEPS:\r\n                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\r\n\r\n                print(\"--------- run_step: test %d ----------\" % test)\r\n                test += 1\r\n\r\n                _, acc, step = sess.run([train_step, accuracy, global_step],\r\n                                        feed_dict={x: batch_x, y_: batch_y})\r\n                print(\"--------- run_step: test %d ----------\" % test)\r\n                test += 1\r\n\r\n                if step % PRINT_EVERY == 0:\r\n                    print(\"Worker : {}, Step: {}, Accuracy (batch): {}\".\\\r\n                          format(FLAGS.task_index, step, acc))\r\n\r\n            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\r\n            print(\"Test-Accuracy: {}\".format(test_acc))\r\n\r\n        sv.stop()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run(main=main)\r\n\r\n```\r\n#### Logs\r\nI got logs shown below from the parameter server and the worker \r\n\r\nfrom the parameter server:\r\n```bash\r\n...\r\njob_name     = ps\r\ntask_index   = 0\r\nreplicas_num = 1\r\n```\r\nfrom the worker:\r\n```bash\r\n...\r\njob_name     = worker\r\ntask_index   = 0\r\nreplicas_num = 1\r\n2017-10-20 17:54:43.891111: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 10bf51b4613ac80d with config:\r\n--------- run_step: test 0 ----------\r\n\r\n```\r\n", "comments": ["@zheng-xq anyone you would suggest for SyncReplica issues perhaps?\r\n\r\nOn a related note, Uber released their horovod framework.", "I got the same problem, the program hang up at session config init, log like this:\r\n`Training model...\r\n\u5f53\u524d\u8fd0\u884c\u6a21\u5f0f\u4e3a\u540c\u6b65\u6a21\u5f0f\r\n2017-11-03 09:49:13.819230: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 28f97991c51e14aa with config: `\r\n\r\nanyone can solve this problem\uff1f (ps: async mode can work well)", "@onyamaa  Do you solve this problem? I add some code block before ` with sv.managed_session(server.target) as sess:`, use this snippet ` chief_queue_runner = optimizer.get_chief_queue_runner()\r\n            sv.start_queue_runners(sess, [chief_queue_runner])`, then it works well. hope it can work for you.", "@terryKing1992 Thank you. It worked with below changes.\r\n```\r\n        chief_queue_runner = opt.get_chief_queue_runner()            # Added\r\n        with sv.managed_session(server.target) as sess:\r\n            if is_chief:                                             # Added\r\n                sv.start_queue_runners(sess, [chief_queue_runner])   # Added\r\n\r\n```\r\nBut I am trying to train synchronized RNN model and it still has same problem.\r\nhttps://www.tensorflow.org/tutorials/recurrent", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly."]}, {"number": 13853, "title": "how can i use \"TF_SetTarget or TF_SetConfig\" to set the number of threads in c code", "body": "### Source code / logs\r\nthis is my c code:\r\n\r\n\tTF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n\tstd::string str(\"intra_op_parallelism_threads = 4\");\r\n\tTF_SetConfig(sess_opts,(void *)str.c_str(),str.size(),status);\r\n\tif (TF_GetCode(status) != TF_OK) {\r\n\t\t  printf(\"ERROR: %s\\n\", TF_Message(status));\r\n\t}\r\n\r\nthis is the debug output:\r\n\r\n       Successfully imported graph\r\n       ERROR: Unparseable ConfigProto\r\n      ", "comments": ["Yes, you'll want to say:\r\n```\r\nconst std::string config_text(\"introa_op_parallelism_threads: 4\")\r\n```", "Is this correct? I just tried this (trying both \"intra_op_parallelism_threads: 4\" and \"\"intr**o**a_op_parallelism_threads: 4\") and neither seemed to work.", "I'm also having trouble with this when trying to set the gpu memory via the C API.  When I run the code below I get the following error message: \"Unparseable ConfigProto\".  \r\n\r\nTF_Status* status = TF_NewStatus();\r\nTF_SessionOptions* opts = TF_NewSessionOptions();\r\nstd::string str(\"per_process_gpu_memory_fraction: 0.5\");\r\n\r\nTF_SetConfig(opts, (void *)str.c_str(),str.size(), status);\r\n\r\nTF_Code code = TF_GetCode(status);\r\nif (code != TF_OK) {\r\n    std::cout << TF_Message(status) << std::endl;\r\n}\r\n\r\nAre there any documentation/examples/comments that show how to set session options via the C API?  I've been referencing the c_api_test.cc (link below) which has many examples of how to use the C API but it does not show how to format string for the TF_SetConfig function.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/c/c_api_test.cc\r\n\r\n", "@bgraf422 I believe `TF_SetConfig` wants you to provide the config as a serialized protobuf string. You can see what this looks like if you open a Python interpreter and create the config yourself:\r\n```python\r\n>>> import tensorflow as tf\r\n>>> gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\n>>> config = tf.ConfigProto(gpu_options=gpu_options)\r\n>>> serialized = config.SerializeToString()\r\n>>> list(map(hex, serialized))\r\n['0x32', '0x9', '0x9', '0x0', '0x0', '0x0', '0x0', '0x0', '0x0', '0xe0', '0x3f']\r\n```\r\nAnd you should use this serialized representation in the C code instead, like:\r\n```c\r\nuint8_t config[11] = {0x32, 0x09, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xe0, 0x3f};\r\nTF_SetConfig(opts, (void*)config, 11, status);\r\n```\r\n(Code off the top of my head, may not work but you can get the gist.)\r\n\r\nObviously this is not ideal and you should construct/serialize the Config object instead in your C code, but this seems to be the only solution if you want to rely solely on what's exposed by TensorFlow's C API.\r\n\r\n@drpngx do you have any thoughts on this? This seems like a hacky way to set this.", "Also: I've tried this in an Android NDK app to set the number of threads, and **while it makes a warning go away I've observed that this had no effect**. For me setting the number of threads to 1 or 8 through the C API had no effect on benchmark time, while setting threads using the equivalent code in C++ did. I'd be interested if this works for you or not.", "Yup that did the trick, thanks for the suggestion! I confirmed that only half of the gpu memory was being used via the nvidia-smi CLI tool.  \r\n\r\nI agree it would be best to construct the config object but I don't see any way of doing that without being dependent on the C++ API.  This will work for me for now, thanks for the quick response!", "Ah, I'm guessing you're on Linux then? Maybe I'm just experiencing an Android problem.\r\n\r\nBut yeah this would require some sort of wrapper around config in the C API, I'm not sure how much the TF devs want that though (it would definitely add more to the API surface).", "Yup I'm on Ubuntu 16.04.  Makes sense that they don't have all the functionality in the C API since it is mostly meant to be used for building bindings to other languages.  I hope they eventually add more features to the C API as it is pretty convenient for deployment.  It's a lot easier to just pull down the C API binaries than compiling the source code into a C++ shared library. \r\n ", "@zo7 I have tried your memory-limitation trick and it works! Thanks! ", "c++ code:\r\n```\r\n 57     TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n 58 \r\n 59     uint8_t config[11] = {0x32, 0x09, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xe0, 0x3f};\r\n 60     TF_SetConfig(sess_opts, (void*)config, 11, s);\r\n 61     assert(TF_GetCode(s) == TF_OK);\r\n 62 \r\n 63     TF_Session* session = TF_NewSession(graph, sess_opts, s);\r\n 64     assert(TF_GetCode(s) == TF_OK);\r\n```\r\n\r\npython2.7 code:\r\n```\r\nimport tensorflow as tf\r\n  \r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\nconfig = tf.ConfigProto(gpu_options=gpu_options)\r\nserialized = config.SerializeToString()\r\n  \r\nresult = [\"0x\"+c.encode('hex') for c in serialized]\r\n  \r\nprint(result) \r\n```", "I need. It's a binary proto. You want to use `TextFormat` to parse from text, rather than editing the binary strings.", "no need for hex-encoding, just use\r\n`[int(i) for i in serialized] `\r\nreplace the [] with {}\r\n`uint8_t config[] = { ....};`\r\nworks like a charm\r\n", "Hey guys, I'm late to the party but is this written anywhere in the API? \r\nI tried to set `intra_op_parallelism_threads` and `inter_op_parallelism_threads` but it does not seem to work in C. Here is my code:\r\n\r\n```\r\nconfig = tf.ConfigProto()\r\nconfig.intra_op_parallelism_threads = 1\r\nconfig.inter_op_parallelism_threads = 1\r\nconfig.use_per_session_threads=False\r\nserialized = config.SerializeToString()\r\nresult = [\"0x\"+c.encode('hex') for c in serialized]\r\n```\r\nCode in C:\r\n\r\n```\r\nuint8_t config[4] = {0x10, 0x01, 0x28, 0x01};\r\n    TF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\n    TF_SetConfig(sess_opts,(void *)config,4,s);\r\n    if (TF_GetCode(s) != TF_OK) {\r\n      printf(\"ERROR: %s\\n\", TF_Message(s));\r\n    }\r\n    TF_Session* session = TF_NewSession(graph, sess_opts, s);\r\n    assert(TF_GetCode(s) == TF_OK);\r\n```", "A slightly more advanced version of the code by @hoavt-54 ...\r\n\r\n```c\r\n    // reverse engineered the TF_SetConfig protocol from python code like:\r\n    // >> config = tf.ConfigProto();config.intra_op_parallelism_threads=7;config.SerializeToString()\r\n    // '\\x10\\x07'\r\n    uint8_t intra_op_parallelism_threads = 1;\r\n    uint8_t inter_op_parallelism_threads = 1;\r\n    uint8_t buf[]={0x10,intra_op_parallelism_threads,0x28,inter_op_parallelism_threads};\r\n    TF_SetConfig(sess_opts, buf,sizeof(buf),status);\r\n```\r\n\r\nBy default, TF assumes it can take over all CPUs.  Unfortunately, its threading is nowhere near linear. On a 16 core machine, processing a model single-threaded changed the `/bin/time` output from (real,user,sys)  = ( 1.61, 20.0, 6.46 ) to (8.36, 8.26, 0.07)\r\ni.e single-threaded took 5x longer wall(real) time, but used less than a third total CPU time (user+sys) as the default. One should seek ways to multiprocess at higher abstractions than this.\r\n\r\n", "To get `proto` parameter for `TF_SetConfig` we can use `tensorflow/core/protobuf/config.pb.h` and `tensorflow::ConfigProto` class\r\nThe implementation is included to `libtensorflow.so`\r\n\r\nCode example https://github.com/apivovarov/TF_C_API/blob/master/config.cc\r\n\r\nbuild command:\r\n```\r\ng++ -std=c++11 -o config.o -c -Itensorflow/bazel-tensorflow/external/com_google_protobuf/src -Itensorflow/bazel-bin -Ilibtensorflow/include config.cc\r\ng++ -std=c++11 -o config config.o -Llibtensorflow/lib -ltensorflow -ltensorflow_framework \r\nLD_LIBRARY_PATH=libtensorflow/lib: ./config\r\n0x10,0x2,0x28,0x3,0x32,0xb,0x9,0x9a,0x99,0x99,0x99,0x99,0x99,0xb9,0x3f,0x20,0x1\r\n```\r\nTF_SetConfig API: \r\nhttps://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/c/c_api.h#L147", "How can I restrict the tensorflow c api to use only and only one core of the cpu?", "How can I restrict the tensorflow c api to use only and only one core of the cpu?"]}, {"number": 13852, "title": "WIP: ENH: only master is allowed to export model", "body": "Fix #13849 \r\n\r\nIf the proposition is accepted, I'll add the corresponding test case later.\r\n\r\n### How to test\r\n\r\n+ [ ] add unit test\r\n+ [ ] pass all tests", "comments": ["Can one of the admins verify this patch?", "I looked at the issue #13849. If you counter that problem, very likely your TF_CONFIG is configured improperly (or in a way Experiment does not support). \r\n\r\nCan you list all TF_CONFIG environment vars for all jobs?", "Thank you, @xiejw . I'll post the information required next Monday. ", "@xiejw This is the TF_CONFIG:\r\n```bash\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \r\n  \"cluster\": {\r\n    \"ps\": [\"77-112-184-hadoop:2955\"], \r\n    \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"],\r\n    \"master\": [\"77-113-54-hadoop:5290\"]},\r\n  \"task\": {\"index\": 0, \"type\": \"worker\"}}\r\n```\r\n\r\nPlease note that the information is from tensorflow 1.3.0", "This is one TF_CONFIG for the first worker (as the index is 0). Is this the worker in question (which triggers the export)?\r\n\r\nBy default, the tf.contrib.learn.learn_runner will call Experiment.train for you (as this is worker), which does not trigger export by default. Did you modify the default behavior (by passing a different schedule)?", "Thank you. \r\n\r\nWe use `contrib.learn.estimator`, instead of `tf.estimator`. Will it be a problem? Code is pasted here.\r\n\r\n```python\r\n        estimator = config.get_estimator()\r\n\r\n        logging.info(\"generate export config...\")\r\n        columns = config._get_columns(only_used=True)\r\n        serving_input_fn = _get_serving_input_fn(columns)\r\n        export_strategies = make_export_strategy(\r\n            serving_input_fn=serving_input_fn)\r\n\r\n        logging.info(\"generate experiment...\")\r\n        ex = Experiment(estimator,\r\n                        train_input_fn=train_input_fn,\r\n                        eval_input_fn=test_input_fn,\r\n                        train_steps=self.fit_steps,\r\n                        eval_steps=self.evaluate_steps,\r\n                        export_strategies=export_strategies)\r\n\r\n        logging.info(\"train and evaluate....\")\r\n        ex.train_and_evaluate()\r\n\r\n```\r\n\r\n\r\nthis is all the logs:\r\n```bash\r\n##### master0\r\n\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \"cluster\": {\"ps\": [\"77-112-184-hadoop:2955\"], \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"], \"master\": [\"77-113-54-hadoop:5290\"]}, \"task\": {\"index\": 0, \"type\": \"master\"}}\r\n\r\nINFO:tensorflow:SavedModel written to: hdfs://77-16-121-hadoop:9000/user/zhongwei5/prometheus/output/export/Servo/1508406233/saved_model.pb\r\n\r\n\r\n##### worker0\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \"cluster\": {\"ps\": [\"77-112-184-hadoop:2955\"], \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"], \"master\": [\"77-113-54-hadoop:5290\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\r\n\r\nTraceback (most recent call last):\r\nAssertionError: Export directory already exists. Please specify a different export directory: hdfs://77-16-121-hadoop:9000/user/zhongwei5/prometheus/output/export/Servo/1508406233\r\n\r\n\r\n##### worker1\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \"cluster\": {\"ps\": [\"77-112-184-hadoop:2955\"], \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"], \"master\": [\"77-113-54-hadoop:5290\"]}, \"task\": {\"index\": 1, \"type\": \"worker\"}}\r\n\r\nAssertionError: Export directory already exists. Please specify a different export directory: hdfs://77-16-121-hadoop:9000/user/zhongwei5/prometheus/output/export/Servo/1508406233\r\n\r\n##### worker2\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \"cluster\": {\"ps\": [\"77-112-184-hadoop:2955\"], \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"], \"master\": [\"77-113-54-hadoop:5290\"]}, \"task\": {\"index\": 2, \"type\": \"worker\"}}\r\n\r\nINFO:tensorflow:SavedModel written to: hdfs://77-16-121-hadoop:9000/user/zhongwei5/prometheus/output/export/Servo/1508406234/saved_model.pb\r\n\r\n##### worker3\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \"cluster\": {\"ps\": [\"77-112-184-hadoop:2955\"], \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"], \"master\": [\"77-113-54-hadoop:5290\"]}, \"task\": {\"index\": 3, \"type\": \"worker\"}}\r\n\r\nAssertionError: Export directory already exists. Please specify a different export directory: hdfs://77-16-121-hadoop:9000/user/zhongwei5/prometheus/output/export/Servo/1508406234\r\n\r\n\r\n##### ps0\r\nINFO:tensorflow:TF_CONFIG:\r\n{\"environment\": \"cloud\", \"cluster\": {\"ps\": [\"77-112-184-hadoop:2955\"], \"worker\": [\"77-112-168-hadoop:7181\", \"77-112-174-hadoop:5564\", \"77-16-119-hadoop:6089\", \"77-112-184-hadoop:5587\"], \"master\": [\"77-113-54-hadoop:5290\"]}, \"task\": {\"index\": 0, \"type\": \"ps\"}}\r\n\r\nAssertionError: Export directory already exists. Please specify a different export directory: hdfs://77-16-121-hadoop:9000/user/zhongwei5/prometheus/output/export/Servo/1508406233\r\n\r\n```\r\n\r\n", "All TF_CONFIGs look good to me. \r\n\r\nI think the issue is you are calling the Experiment improperly. \r\n\r\nYour code runs on all jobs (master, worker and ps). The same code should behave differently. The code runs on master handling the eval and export in addtion to training. The code on worrk only trains the model. ps handles the data sharing. \r\n\r\nIn order to achieve that, Experiment class has multiple methods (called schedule). The \"schedule\" specifies the code to run. \r\n\r\nFor master, the schedule should be train_and_evaluate. For worker, the schedule should be train.  User does not need to set the schedule. tf.contrib.learn.learn_runner handles that for your automatically. \r\n\r\nSee you see the example in the docstring [1]. The schedule argument should be absent for most cases, as the default is good enough. So, the simplest way is\r\n\r\n    def _create_my_experiment(run_config, hparams):\r\n        # You can change a subset of the run_config properties as\r\n        #   run_config = run_config.replace(save_checkpoints_steps=500)\r\n        return tf.contrib.learn.Experiment(\r\n          estimator=my_estimator(config=run_config, hparams=hparams),\r\n          train_input_fn=my_train_input,\r\n          eval_input_fn=my_eval_input)\r\n\r\n    learn_runner.run(\r\n          experiment_fn=_create_my_experiment,\r\n          run_config=run_config_lib.RunConfig(model_dir=\"some/output/dir\"))\r\n\r\n[1] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/learn_runner.py#L119\r\n\r\n", "It looks like a change to the estimator is not needed to get this to work, so I'm going to close the PR for now; perhaps one could move this discussion to stackoverflow / github issue until it is clear a PR is needed.  Thanks!"]}, {"number": 13851, "title": "Fail in configuration due to different CUDA libraries path", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 49f9c6f890c938955fa2d448ac5b556b9a6d9aa0\r\n- **Python version**: Python 3.5.3\r\n- **Bazel version (if compiling from source)**: bazel release 0.6.1\r\n- **CUDA/cuDNN version**: CUDA 8, cuDNN 7\r\n- **GPU model and memory**: GeForce GTX 1080 \r\n\r\n### Describe the problem\r\nAs for now, TF assumes that CUDA liibs are located at `CUDA_PATH/lib64/`, however Ubuntu installs CUDA to `/usr/lib/x86_64-linux-gnu/`, which makes configuration impossible: I can't specify cuda path such that it'll find `/usr/lib/x86_64-linux-gnu/libcudart.so.8.0`\r\n", "comments": ["@ekelsen WDYT?", "As a temporary hack add you can add an environmental variable and a symlink to fix the problem. Works for me as on tf master, cuda 9, cudnn 7\r\n\r\nDockerfile example:\r\n```\r\nENV     CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu/\r\nENV     CUDA_TOOLKIT_PATH=/usr/local/cuda/\r\n\r\nENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH\r\nRUN mkdir -p /usr/local/nvidia/lib64/ \\\r\n              && cd /usr/local/nvidia/lib64/ \\\r\n              && ln -s /usr/local/cuda/targets/x86_64-linux/lib/stubs/libcuda.so libcuda.so.1 \\\r\n              && ldconfig /usr/local/cuda/lib64\r\n```\r\n", "Is the CUDA install path different in 17.04 vs. earlier versions of Ubuntu?  It seems like if this affected all versions it wouldn't have gone unnoticed until now.\r\n\r\nAnother workaround is installing CUDA from the NVIDIA installer rather than using Ubuntu; then it will be installed into the correct location.", "I believe it is, because there were no problem with earlier versions of Ubuntu (I haven't tried 16.10 though) . ", "Another option would be just to remove `lib64` at configuration script ( https://github.com/tensorflow/tensorflow/blob/master/configure.py#L617 ) and to ask users give a full path to libcudart", "However, the nvcc isn't install to the same dir anymore too (it sits at `/usr/bin/nvcc`)", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Is this still a problem in the latest version?", "@drpngx Well yeah, `libcudart` is still in `/usr/lib/x86_64-linux-gnu/` and Tensorflow still wants `/lib64/libcudart.so`, which isn't generic enough IMHO", "OK, so maybe send a PR to @gunan and CC @nluehr?", "@drpngx I don't have a clean fix, which probably should be allowing user to give full pathes to everything. However, I'm not familiar enough with the building system to make it work cleanly\r\n  ", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@Randl if you send a PR we can go from there.", "Hi @Randl!\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13851\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/13851\">No</a>\n"]}, {"number": 13850, "title": "Updated tf.svd documentation to state \" u * s * v' \"", "body": "Updated tf.svd documentation to state the index for V matrix is transposed.", "comments": ["Can one of the admins verify this patch?", "resolve #13298", "You change is not correct. V stores the singular vectors as stated. It is correct that A \\approx U \\Sigma V^H. Notice that this convention is unlike some other SVD libraries that return V^H.\r\n\r\nThe test that perhaps clarifies this best is here:\r\nhttps://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/kernel_tests/svd_op_test.py?rcl=171836140&l=120", "Also as stated in \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/svd\r\n\r\ntensor[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(v[..., :, :])\r\n\r\nWe should perhaps update that to \r\n\r\ntensor[..., :, :] = u[..., :, :] * diag(s[..., :, :]) * transpose(conj(v[..., :, :]))\r\n(although we currently only support complex SVD on CPU)", "Can one of the admins verify this patch?", "@donigian if you are up for changing the documentation as stated above, it would be helpful. Thanks!", "What's needed is a fix to \"compatibility\" section of the docs, there a difference in order and transposition from numpy result, whereas docs only mentions order difference\r\n\r\n```\r\n@compatibility(numpy)\r\n  Mostly equivalent to numpy.linalg.svd, except that the order of output\r\n  arguments here is `s`, `u`, `v` when `compute_uv` is `True`, as opposed to\r\n  `u`, `s`, `v` for numpy.linalg.svd.\r\n  @end_compatibility\r\n```", "oops @rmlarsen please see revised edits from what I was able to gather.", "@yaroslavvb the numpy api is just \"wrong\", as explained in the notes:\r\n\r\nhttps://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html\r\n\r\n\"The SVD is commonly written as a = U S V.H. The v returned by this function is V.H and u = U.\"", "Closing for lack of activity, and it seems @rmlarsen is the expert who knows how this could be clarified properly.", "FYI. I'll update the docstring internally. Should be pushed out in a day or two. Thanks for bringing up the issue @donigian and @yaroslavvb "]}, {"number": 13849, "title": "Feature request: only master is allowed to export for `tf.contrib.learn.Experiment`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0-nightly, 1.3.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nWhen using `tf.contrib.learn.Experiment` for distributed training, it seems that all workers, ps, and master try to export model when finished. However, this will cause write conflict when `model_dir` is on hdfs, because all write the same file to the same location.\r\n\r\nHence, I propose that only master is allowed to export model.\r\nI can work on it if tensorflowers're agreed.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nINFO:tensorflow:Saving dict for global step 30: accuracy = 0.339453, global_step = 30, loss = 70112.8\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 60, in <module>\r\n    tf.app.run()\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 56, in main\r\n    sess.run()\r\n  File \"/Users/facai/Workshop/sina/Prometheus/prometheus/python/session.py\", line 172, in run\r\n    ex.train_and_evaluate()\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 641, in train_and_evaluate\r\n    export_results = self._maybe_export(eval_result)\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 744, in _maybe_export\r\n    eval_result=eval_result))\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py\", line 87, in export\r\n    return self.export_fn(estimator, export_path, **kwargs)\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 454, in export_fn\r\n    checkpoint_path=checkpoint_path)\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1307, in export_savedmodel\r\n    builder = saved_model_builder.SavedModelBuilder(temp_export_dir)\r\n  File \"/Users/facai/Library/anaconda3/envs/py27_test/lib/python2.7/site-packages/tensorflow/python/saved_model/builder_impl.py\", line 88, in __init__\r\n    \"directory: %s\" % export_dir)\r\nAssertionError: Export directory already exists. Please specify a different export directory: file:///tmp/facai/prome/model/new/export/Servo/temp-1508482645\r\n```\r\n", "comments": ["asked question in #13852", "Thanks, @xiejw "]}, {"number": 13848, "title": "can not compile android demo for x86", "body": "I tried to use the following commands to build android demo for x86\r\nbazel build //tensorflow/examples/android:tensorflow_demo --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain  --force_pic --cpu=x86_64 --config=android_x86\r\n\r\nI always get the tensorflow_demo.apk for ARM device.\r\nIs there anything wrong?", "comments": ["@petewarden @andrewharp Can either of you comment?", "@kriszhangyf I think the flag you want is `--fat_apk_cpu`. IIRC `--cpu` merely refers to the host target.\r\n\r\nAlso, I'm not certain --config=android_x86 will do anything (that may have inadvertently come from Google-internal commands we use to build).", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 13847, "title": "tf.train.batch and shuffle_batch undetermined for multi queue input with multiple threads", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nCustom code, see below.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nOSX\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npip install tensorflow == 1.2\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-rc2-21-g12f033d', '1.2.0')\r\n\r\n- **Python version**: \r\nPython 2.7.10\r\n\r\n- **Bazel version (if compiling from source)**:\r\nn/a\r\n\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n\r\n- **GPU model and memory**:\r\nn/a\r\n\r\n- **Exact command to reproduce**:\r\n\r\n\r\n```\r\na_fifo = tf.train.slice_input_producer([np.arange(30)],\r\n                                        shuffle=False,\r\n                                        name='a')[0]\r\nb_fifo = tf.train.slice_input_producer([np.arange(30)],\r\n                                        shuffle=False,\r\n                                        name='b')[0]\r\ntrain_batch = tf.train.batch([a_fifo, b_fifo],\r\n                            batch_size=5,\r\n                            capacity=18,\r\n                            num_threads=2,\r\n                            name='batch')\r\nat, bt = train_batch\r\nwith tf.Session() as sess:\r\n    threads = tf.train.start_queue_runners(sess=sess)\r\n    sess.run(tf.global_variables_initializer())\r\n    try:\r\n        for i in range(150):\r\n            av, bv = sess.run([at, bt])\r\n            assert list(av) == list(bv), \"queues derailed!\"\r\n    except Exception as e:\r\n        print((\"Exception in training: {}\").format(e))\r\n        print(\"__\",i,\"__\")\r\n        print(av)\r\n        print(bv)\r\n```\r\n\r\nOutput:\r\n\r\n> Exception in training: queues derailed!\r\n> __ 11 __\r\n> [26 25 27 28 29]\r\n> [25 26 27 28 29]\r\n\r\n\r\n\r\n\r\n### Describe the problem\r\nBoth queues should be dequeued synchronously, as it does when num_threads =1 in the batch statement. With num_threads > 1 the two input queue's will derail, as shown in the example above. Same issue appears also in train.shuffle_batch.\r\n\r\n### Source code / logs\r\nSee code above.", "comments": ["@BastiaanBergman we are moving to `tf.data`, which is a much nicer interface. Would you be open to that?", "I really liked tf.data but found it not working for some other reason, therefore reverted back to the queues. Will retry some day.", "We are deprecating the queues in favor of `tf.data` and would really like to hear about what the problem is.\r\n\r\n@mrry maybe we can suggest a solution?", "Yes, the difficulty of getting deterministic results is one of the reasons why we are recommending that people use `tf.data` instead of the queue-based APIs. For example, here's what your program would look like in `tf.data`:\r\n\r\n```python\r\na_dataset = tf.data.Dataset.range(30).repeat()\r\nb_dataset = tf.data.Dataset.range(30).repeat()\r\nbatched_dataset = tf.data.Dataset.zip((a_dataset, b_dataset)).batch(5)\r\niterator = batched_dataset.make_one_shot_iterator()\r\nat, bt = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    try:\r\n        for i in range(150):\r\n            av, bv = sess.run([at, bt])\r\n            assert list(av) == list(bv), \"queues derailed!\"\r\n    except Exception as e:\r\n        print((\"Exception in training: {}\").format(e))\r\n        print(\"__\",i,\"__\")\r\n        print(av)\r\n        print(bv)\r\n```", "Yes its nice that way. I still have to look and see what made me move away from dataset. I think it was issues with switching between test and train sets on the same graph. But I had issues with that in queue's too and eventually settled with separate graphs for training and testing anyway.\r\n\r\nAre there multiple threads working on this dataset version? Would I be save with one thread on queue's? Or might I still have compromised 'deterministic results'? ;-) Thanks anyway, it's been awesome learning tf.\r\n", "There are a couple of ways to switch between different datasets with a single `Iterator` (search for \"reinitializable iterator\" and \"feedable iterator\" in [this guide](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator)), but most people find it easier to construct multiple graphs for these cases.\r\n\r\nThe code example I showed does not create additional threads, but you can add them wherever you'd like by adding a `.prefetch(buffer_size)` transformation. Although it doesn't make much sense for the concrete example, here's how you'd add prefetching threads to the `a_dataset` and `b_dataset` computations:\r\n\r\n```python\r\na_dataset = tf.data.Dataset.range(30).repeat().prefetch(10)\r\nb_dataset = tf.data.Dataset.range(30).repeat().prefetch(10)\r\nbatched_dataset = tf.data.Dataset.zip((a_dataset, b_dataset)).batch(5)\r\n```\r\n\r\nIf you have an expensive `Dataset.map()` in your program, you can also parallelize that by adding `num_parallel_calls=N` to process `N` elements at once.\r\n\r\nAll these options add parallelism without comprising on determinism. There is an explicitly \"sloppy\" transformation called `tf.contrib.data.sloppy_interleave()` that can produce non-determinstic results (but can be useful to make I/O blockages), and we've considered adding a non-deterministic parallel map (to deal with stragglers), but it's generally been more pleasant to work with deterministic APIs.\r\n\r\nBy contrast, the `num_threads > 1` versions of the queue-based APIs are inherently racy, because they issue multiple `Session.run()` calls in parallel, and mutate queue state (dequeuing from the source queue, enqueuing to the destination queue) without any coordination. Setting `num_threads = 1` ought to help, but it conflicts with `tf.train.shuffle_batch()`, because there can still be a race between a single producer thread and a single consumer thread when `capacity != min_after_dequeue`, leading to unrepeatable results.", "Closing due to lack of activity."]}, {"number": 13846, "title": "Fix `tf.py_func()` and `Dataset.from_generator()` on Python 3.", "body": "Cherry pick to make `Dataset.from_generator()` work on Python 3 when the generator yields (unicode) strings.", "comments": ["@tensorflow-jenkins test this please", "Hi folks... I realize this might be past the deadline for rc1, but I thought I'd send a PR just in case.\r\n\r\nThe bug it fixes is **not** severe enough to be a release blocker, because there's a workaround (i.e. explicitly yield `bytes` not `str` when using Python 3). However, if this PR is too late, would it be possible to update the release notes with a \"known issue\"?", "ping @case540; feel free to merge this if it makes sense, otherwise close the PR and add the 'known issue' to the RELEASE.md?  Thanks!", "This is unfortunately too late for RC1. I can update the release notes however. How would you word the issue..\r\n\r\nIs something like this good?\r\n\r\n* `Dataset.from_generator()` does not work in Python 3 when the generator yields (unicode) strings.", "How about \"In Python 3, `Dataset.from_generator()` does not support Unicode strings. You must convert any strings to `bytes` objects before yielding them from the generator.\"", "Sounds good. Will add to Release notes.", "Thanks!"]}, {"number": 13845, "title": "Fix ../makefile/download_dependencies.sh on OSX", "body": "wget expects parameters before the URL on OSX (tested on\r\nversion 1.16 and 1.19)\r\n\r\nIt would fail trying to use -P as a URL\r\n\r\nResolving -p... failed: nodename nor servname provided, or not known.\r\nwget: unable to resolve host address \u2018-p\u2019", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13844, "title": "Saving large graphs to S3 fails with InternalError: : Unable to connect to endpoint", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes; modified Tensorflow [save/load example code](https://www.tensorflow.org/programmers_guide/saved_model) to save a few large tensors to S3\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version**:\r\nGit version: `v1.3.0-rc1-3504-g27767d8`\r\nTensorflow version: `1.4.0-rc1`\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Nvidia Tesla K80 (12 GiB RAM)\r\n- **Exact command to reproduce**: Run the code in [this gist](https://gist.github.com/smurching/8766a9d91c148ef7d89292b6dd4da5b8) in a Python shell. You'll need access to an S3 bucket for which you have write permissions.\r\n\r\n### Describe the problem\r\nI'm trying to save a large (~380 MB) graph to S3, but my call to `tf.Saver.save()` crashes after ~1 min with what appears to be an AWS SDK error (`InternalError: : Unable to connect to endpoint`).\r\n\r\nIf the error is indeed AWS related, it'd be helpful to wrap it in something to indicate that the error isn't coming from tensorflow. Here's the stacktrace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<command-5036518> in <module>()\r\n----> 1 test_save(num_tensors=10, tensor_size=10000000, save_path=\"s3://<redacted_s3_bucket_name>/model.ckpt\")\r\n\r\n<command-5036204> in test_save(num_tensors, tensor_size, save_path)\r\n     18     sess.run(init_op)\r\n     19     # Save the variables to disk.\r\n---> 20     save_path = saver.save(sess, save_path)\r\n     21     print(\"Model saved in file: %s\" % save_path)\r\n\r\n/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc in save(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\r\n   1571           model_checkpoint_path = sess.run(\r\n   1572               self.saver_def.save_tensor_name,\r\n-> 1573               {self.saver_def.filename_tensor_name: checkpoint_file})\r\n   1574         else:\r\n   1575           self._build_eager(\r\n\r\n/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1119       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1120                              feed_dict_tensor, options, run_metadata)\r\n   1121     else:\r\n   1122       results = []\r\n\r\n/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1315     if handle is None:\r\n   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1317                            options, run_metadata)\r\n   1318     else:\r\n   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1334         except KeyError:\r\n   1335           pass\r\n-> 1336       raise type(e)(node_def, op, message)\r\n   1337 \r\n   1338   def _extend_graph(self):\r\n\r\nInternalError: : Unable to connect to endpoint\r\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, v0/_1, v1/_3, v2/_5, v3/_7, v4/_9, v5/_11, v6/_13, v7/_15, v8/_17, v9/_19)]]\r\n\r\nCaused by op u'save/SaveV2', defined at:\r\n  File \"/tmp/1509404428358-0/PythonShell.py\", line 990, in <module>\r\n    launch_process()\r\n  File \"/tmp/1509404428358-0/PythonShell.py\", line 986, in launch_process\r\n    shell.executor.run()\r\n  File \"/tmp/1509404428358-0/PythonShell.py\", line 263, in run\r\n    self.shell.shell.run_cell(command_id, cmd, store_history=True)\r\n  File \"/tmp/1509404428358-0/PythonShell.py\", line 572, in run_cell\r\n    super(IPythonShell, self).run_cell(raw_cell, store_history, silent, shell_futures)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2741, in run_cell\r\n    interactivity=interactivity, compiler=compiler)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2833, in run_ast_nodes\r\n    if self.run_code(code):\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2883, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<command-5036518>\", line 1, in <module>\r\n    test_save(num_tensors=10, tensor_size=10000000, save_path=\"s3://databricks-mllib/tmp/s3-save-failure/model.ckpt\")\r\n  File \"<command-5036204>\", line 13, in test_save\r\n    saver = tf.train.Saver()\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\r\n    self.build()\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1227, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1263, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 748, in _build_internal\r\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 296, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 239, in save_op\r\n    tensors)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1163, in save_v2\r\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInternalError (see above for traceback): : Unable to connect to endpoint\r\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, v0/_1, v1/_3, v2/_5, v3/_7, v4/_9, v5/_11, v6/_13, v7/_15, v8/_17, v9/_19)]]\r\n```\r\n\r\nIf I run the same code but checkpoint to my local filesystem the save op runs without error.\r\nThe error also only seems to occur for large graphs (running the [linked gist](https://gist.github.com/smurching/8766a9d91c148ef7d89292b6dd4da5b8) with smaller/fewer tensors works)\r\n\r\n\r\n\r\n", "comments": ["@asimshankar Can you comment on this one?", "@asimshankar I was able to reproduce this error locally on a simpler example with tensorflow 1.4rc1, I'll update the issue description!", "@smurching If you could reproduce it locally, could you remove the \"If I run the same code but checkpoint to my local filesystem the save op runs without error.\" from your initial description?\r\n\r\nIs the stack trace the same for the local error as well?\r\n\r\nLastly, are you running this on a relatively memory constrained device?", "My mistake, by \"locally\" I meant to say on a single machine (as opposed to a cluster) -- I'm still trying to checkpoint to S3, resulting in the stacktrace/error described above. I'm running my tensorflow code on a p2.xlarge AWS EC2 instance, which AFAIK has 61 GB RAM.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@smurching \r\nI am facing the same issue!\r\nTrying to train a Faster RCNN Nasnet and put the checkpoints into an S3 bucket.\r\nDid you find a solution yet?", "The same issue came up for me too.\r\nWhen running a toy example and using a Saver to save a small graph to an S3 bucket it works, but when trying to write a Faster R-CNN (through the export_inference_graph.py from the Object Detection API) to S3, I get the following error (I used the latest nightly build with @yongtang's AWS logging):\r\n\r\n```\r\n2018-01-03 12:24:08.336668: I tensorflow/core/platform/s3/aws_logging.cc:53] Creating HttpClient with max connections2 and scheme http\r\n2018-01-03 12:24:08.336680: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing CurlHandleContainer with size 2\r\n2018-01-03 12:24:08.336688: I tensorflow/core/platform/s3/aws_logging.cc:53] Creating Instance with default EC2MetadataClient and refresh rate 900000\r\n2018-01-03 12:24:08.336738: I tensorflow/core/platform/s3/aws_logging.cc:53] Successfully reloaded configuration.\r\n2018-01-03 12:24:08.336763: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing CurlHandleContainer with size 25\r\n2018-01-03 12:24:08.336906: I tensorflow/core/platform/s3/aws_logging.cc:53] Pool grown by 2\r\n2018-01-03 12:24:08.336924: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n2018-01-03 12:24:08.458925: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n2018-01-03 12:24:11.461615: E tensorflow/core/platform/s3/aws_logging.cc:59] Curl returned error code 28\r\n2018-01-03 12:24:11.461669: W tensorflow/core/platform/s3/aws_logging.cc:56] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\n2018-01-03 12:24:11.461681: W tensorflow/core/platform/s3/aws_logging.cc:56] Request failed, now waiting 0 ms before attempting again.\r\n2018-01-03 12:24:11.461953: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n2018-01-03 12:24:14.474854: E tensorflow/core/platform/s3/aws_logging.cc:59] Curl returned error code 28\r\n2018-01-03 12:24:14.474919: W tensorflow/core/platform/s3/aws_logging.cc:56] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\n2018-01-03 12:24:14.474932: W tensorflow/core/platform/s3/aws_logging.cc:56] Request failed, now waiting 50 ms before attempting again.\r\n2018-01-03 12:24:14.525228: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n...\r\n2018-01-03 12:24:51.335497: W tensorflow/core/platform/s3/aws_logging.cc:56] Request failed, now waiting 12800 ms before attempting again.\r\n2018-01-03 12:25:04.135817: I tensorflow/core/platform/s3/aws_logging.cc:53] Connection has been released. Continuing.\r\n2018-01-03 12:25:07.149093: E tensorflow/core/platform/s3/aws_logging.cc:59] Curl returned error code 28\r\n2018-01-03 12:25:07.149164: W tensorflow/core/platform/s3/aws_logging.cc:56] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.\r\n2018-01-03 12:25:07.149200: I tensorflow/core/platform/s3/aws_logging.cc:53] Cleaning up CurlHandleContainer.\r\n2018-01-03 12:25:07.149214: I tensorflow/core/platform/s3/aws_logging.cc:53] Cleaning up CurlHandleContainer.\r\n2018-01-03 12:25:07.149567: W tensorflow/core/framework/op_kernel.cc:1198] Internal: : Unable to connect to endpoint\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: : Unable to connect to endpoint\r\n\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File \"object_detection/export_inference_graph.py\", line 119, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/export_inference_graph.py\", line 115, in main\r\n    FLAGS.output_directory, input_shape)\r\n  File \"/home/ubuntu/train_and_serve/object_detection/exporter.py\", line 427, in export_inference_graph\r\n    input_shape, optimize_graph, output_collection_name)\r\n  File \"/home/ubuntu/train_and_serve/object_detection/exporter.py\", line 375, in _export_inference_graph\r\n    trained_checkpoint_prefix=checkpoint_to_use)\r\n  File \"/home/ubuntu/train_and_serve/object_detection/exporter.py\", line 321, in _write_graph_and_checkpoint\r\n    saver.save(sess, model_path)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1593, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: : Unable to connect to endpoint\r\n```\r\n", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "This is probably an issue with the default timeout the S3 client uses. Try setting `S3_REQUEST_TIMEOUT_MSEC` to a large value (say, `600000`).", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@smurching As @jkinkead mentioned, adjusting `S3_REQUEST_TIMEOUT_MSEC` could help addressing the timeout issue when the graph is too large. I will close this issue for now, but feel free to reopen and continue the discussion if the issue persists.", "@yongtang   where to set S3_REQUEST_TIMEOUT_MSEC ?   ", "@jasstionzyf via environment variables - see https://github.com/tensorflow/tensorflow/pull/15899/files", "#Training the model\r\nprint(f'Beginning Training!')\r\ncurrent_time = datetime.now()\r\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\nprint(\"Training took time \", datetime.now() - current_time)\r\n\r\nBeginning Training!\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\n/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\r\n  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n---------------------------------------------------------------------------\r\nAbortedError                              Traceback (most recent call last)\r\n<ipython-input-23-c162613f017e> in <module>\r\n      2 print(f'Beginning Training!')\r\n      3 current_time = datetime.now()\r\n----> 4 estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\r\n      5 print(\"Training took time \", datetime.now() - current_time)\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/util/smdebug.py in run(*args, **kwargs)\r\n     55     if pre_hook:\r\n     56       args, kwargs = pre_hook(function, *args, **kwargs)\r\n---> 57     return_value = function(*args, **kwargs)\r\n     58     if post_hook:\r\n     59       post_hook(function, return_value, *args, **kwargs)\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    368 \r\n    369       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 370       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    371       logging.info('Loss for final step: %s.', loss)\r\n    372       return self\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1159       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1160     else:\r\n-> 1161       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1162 \r\n   1163   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1193       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n   1194                                              hooks, global_step_tensor,\r\n-> 1195                                              saving_listeners)\r\n   1196 \r\n   1197   def _train_model_distributed(self, input_fn, hooks, saving_listeners):\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n   1488         config=self._session_config,\r\n   1489         max_wait_secs=self._config.session_creation_timeout_secs,\r\n-> 1490         log_step_count_steps=log_step_count_steps) as mon_sess:\r\n   1491       loss = None\r\n   1492       any_step_done = False\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py in MonitoredTrainingSession(master, is_chief, checkpoint_dir, scaffold, hooks, chief_only_hooks, save_checkpoint_secs, save_summaries_steps, save_summaries_secs, config, stop_grace_period_secs, log_step_count_steps, max_wait_secs, save_checkpoint_steps, summary_dir)\r\n    582       session_creator=session_creator,\r\n    583       hooks=all_hooks,\r\n--> 584       stop_grace_period_secs=stop_grace_period_secs)\r\n    585 \r\n    586 \r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py in __init__(self, session_creator, hooks, stop_grace_period_secs)\r\n   1018         hooks,\r\n   1019         should_recover=True,\r\n-> 1020         stop_grace_period_secs=stop_grace_period_secs)\r\n   1021 \r\n   1022 \r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/monitored_session.py in __init__(self, session_creator, hooks, should_recover, stop_grace_period_secs)\r\n    717 \r\n    718     for h in self._hooks:\r\n--> 719       h.begin()\r\n    720 \r\n    721     worker_context = distribute_coordinator_context.get_current_worker_context()\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/basic_session_run_hooks.py in begin(self)\r\n    555 \r\n    556   def begin(self):\r\n--> 557     self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)\r\n    558     self._global_step_tensor = training_util._get_or_create_global_step_read()  # pylint: disable=protected-access\r\n    559     if self._global_step_tensor is None:\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/summary/writer/writer_cache.py in get(logdir)\r\n     61       if logdir not in FileWriterCache._cache:\r\n     62         FileWriterCache._cache[logdir] = FileWriter(\r\n---> 63             logdir, graph=ops.get_default_graph())\r\n     64       return FileWriterCache._cache[logdir]\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/summary/writer/writer.py in __init__(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\r\n    365     else:\r\n    366       event_writer = EventFileWriter(logdir, max_queue, flush_secs,\r\n--> 367                                      filename_suffix)\r\n    368 \r\n    369     self._closed = False\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/summary/writer/event_file_writer.py in __init__(self, logdir, max_queue, flush_secs, filename_suffix)\r\n     65     self._logdir = str(logdir)\r\n     66     if not gfile.IsDirectory(self._logdir):\r\n---> 67       gfile.MakeDirs(self._logdir)\r\n     68     self._event_queue = six.moves.queue.Queue(max_queue)\r\n     69     self._ev_writer = pywrap_tensorflow.EventsWriter(\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py in recursive_create_dir(dirname)\r\n    436     errors.OpError: If the operation fails.\r\n    437   \"\"\"\r\n--> 438   recursive_create_dir_v2(dirname)\r\n    439 \r\n    440 \r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/lib/io/file_io.py in recursive_create_dir_v2(path)\r\n    451     errors.OpError: If the operation fails.\r\n    452   \"\"\"\r\n--> 453   pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path))\r\n    454 \r\n    455 \r\n\r\nAbortedError: All 10 retry attempts failed. The last failure: Unknown: : No response body.", "I am running the above script on sagemaker and getting the error. Please help me out.", "I'm using TFX on KubeFlow with Minio as S3 backend and I'm also encountering this issue. Any update?", "For us, this had to do with not running Minio in erasure mode: https://github.com/minio/minio/issues/11481."]}, {"number": 13843, "title": "Fixes build breakage", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13842, "title": "Cherry-pick to disable S3 on windows and renamed serving_input_fn arg.", "body": "", "comments": []}, {"number": 13841, "title": "Branch 172798007", "body": "", "comments": ["@tensorflow-jenkins test this please", "@case540, @gunan, @davidsoergel  there are some test failures that seem either flaky or unrelated, should I merge anyway?", "Test output for //tensorflow/contrib/framework:accumulate_n_v2_eager_test:\r\nTraceback (most recent call last):\r\n  File \"/tmp/botexec/bazel-out/linux_gnu_x86-py3-opt/bin/tensorflow/contrib/framework/accumulate_n_v2_eager_test.runfiles/org_tensorflow/tensorflow/contrib/framework/python/ops/accumulate_n_v2_eager_test.py\", line 82, in <module>\r\n    eager_context.enable_eager_execution()\r\nAttributeError: module 'tensorflow.python.eager.context' has no attribute 'enable_eager_execution'\r\n\r\nThis might be an actual failure? Maybe related to this CL? https://github.com/tensorflow/tensorflow/pull/13841/commits/502340916822f8cafade906c1c42acd842ddb7ed"]}, {"number": 13840, "title": "Improve resize_bicubic performance by reorganizing loops", "body": "This fix tries to address the issue raised in #13693 where performance of `resize_bicubic` is subpar compared with opencv.\r\n\r\nThis fix rearranges the loops so that the logic for num_channel=40 and num_channel=3 are similar.\r\n\r\nBefore this PR, `num_channel=3` is treated specially. It looks like manual unrolling will improve the performance for `num_channel=3`. So this PR keeps `num_channel=3` as a special case.\r\n\r\nPre-fix:\r\n```\r\nCHANNEL=40\r\nopencv: 145.08ms\r\ntf: 314.26ms\r\n\r\nCHANNEL=3\r\nopencv: 11.95ms\r\ntf: 8.95ms\r\n```\r\n\r\nPost-fix (without manual unrolling for `num_channel=3`):\r\n```\r\nCHANNEL=40\r\nopencv: 144.25ms\r\ntf: 214.55ms\r\n\r\nCHANNEL=3\r\nopencv: 11.78ms\r\ntf: 14.07ms\r\n```\r\n\r\n\r\nPost-fix (with manual unrolling for `num_channel=3`):\r\n```\r\nCHANNEL=40\r\nopencv: 144.80ms\r\ntf: 212.54ms\r\n\r\nCHANNEL=3\r\nopencv: 11.74ms\r\ntf: 9.46ms\r\n```\r\n\r\nThis fix fixes #13693.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks @cwhipkey for the suggestion. The PR has been updated. The benchmark is the following:\r\n```\r\nBEFORE PR:\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/core/kernels/resize_bicubic_op_test '--benchmarks=all' '--benchmark_min_time=20'\r\nRunning main() from test_main.cc\r\nBenchmark                      Time(ns) Iterations\r\n--------------------------------------------------\r\nBM_ResizeBicubic_8_32_3           45170      15505       544.1M items/s\r\nBM_ResizeBicubic_8_128_3         297522       2338       1321.6M items/s\r\nBM_ResizeBicubic_8_512_3        5093420        100       1235.2M items/s\r\nBM_ResizeBicubic_8_1024_3      20767140        100       1211.8M items/s\r\nBM_ResizeBicubic_16_32_3          63796      10000       770.4M items/s\r\nBM_ResizeBicubic_16_128_3        573589       1000       1371.1M items/s\r\nBM_ResizeBicubic_16_512_3      10392390        100       1210.8M items/s\r\nBM_ResizeBicubic_16_1024_3     48047780        100       1047.5M items/s\r\nBM_ResizeBicubic_32_32_3         100327       6961       979.8M items/s\r\nBM_ResizeBicubic_32_128_3       1130870        568       1390.8M items/s\r\nBM_ResizeBicubic_32_512_3      20679990        100       1216.9M items/s\r\nBM_ResizeBicubic_32_1024_3     97105890        100       1036.6M items/s\r\nBM_ResizeBicubicExpand12481     4472045        156       395.7M items/s\r\nBM_ResizeBicubicExpand12483     8055270        100       659.0M items/s\r\nBM_ResizeBicubicExpand124840  308084570        100       229.7M items/s\r\n\r\nAFTER PR:\r\n\r\nINFO: Running command line: bazel-bin/tensorflow/core/kernels/resize_bicubic_op_test '--benchmarks=all' '--benchmark_min_time=20'\r\nRunning main() from test_main.cc\r\nBenchmark                      Time(ns) Iterations\r\n--------------------------------------------------\r\nBM_ResizeBicubic_8_32_3           45280      15582       542.8M items/s\r\nBM_ResizeBicubic_8_128_3         295346       2353       1331.4M items/s\r\nBM_ResizeBicubic_8_512_3        5118330        100       1229.2M items/s\r\nBM_ResizeBicubic_8_1024_3      20398880        100       1233.7M items/s\r\nBM_ResizeBicubic_16_32_3          62595      10000       785.2M items/s\r\nBM_ResizeBicubic_16_128_3        570425       1000       1378.7M items/s\r\nBM_ResizeBicubic_16_512_3      10208090        100       1232.6M items/s\r\nBM_ResizeBicubic_16_1024_3     48496160        100       1037.8M items/s\r\nBM_ResizeBicubic_32_32_3         100469       6977       978.5M items/s\r\nBM_ResizeBicubic_32_128_3       1121702        564       1402.2M items/s\r\nBM_ResizeBicubic_32_512_3      20287840        100       1240.4M items/s\r\nBM_ResizeBicubic_32_1024_3     96245560        100       1045.9M items/s\r\nBM_ResizeBicubicExpand12481     6819970        100       259.5M items/s\r\nBM_ResizeBicubicExpand12483     7976640        100       665.5M items/s\r\nBM_ResizeBicubicExpand124840  212033740        100       333.8M items/s\r\n```", "The improvement in large `num_channels` (`= 40`)  is quite visible (`229.7M items/s` -> `333.8M items/s`)\r\n\r\nOn the other hand, for `num_channels = 1` there is a performance drop (`395.7M items/s` -> `259.5M items/s`). This is more or less expected as we have a for loop that is doing nothing.\r\n\r\nDepending on how frequent the `num_channels = 1`, it might worth the effort to treat `num_channels = 1` as a special case as well. Let me know if this is desired and I could update the PR accordingly.", "Thanks @cwhipkey. The PR has been updated. I think it probably is OK to merge this PR for now. Much appreciated for the kindly help.", "@tensorflow-jenkins test this please"]}, {"number": 13839, "title": "Add `int64` out_idx` support for `listdiff`/`list_diff`/`setdiff1d`", "body": "This fix tries to add `int64` `out_idx` support for `listdiff`/`list_diff`/`setdiff1d`.\r\n\r\nAs was specified in docs (`tf.setdiff1d.__doc__`), it is possible to specify `tf.int32` or `tf.int64` for the type of the output idx. However, the `tf.int64` kernel has not been registered. As a consequence, an error will be thrown out if `tf.int64` is used.\r\n\r\nThis fix adds `int64` out_idx` support for `listdiff`/`list_diff`/`setdiff1d`. Related test cases have been updated as well.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "As usual, very nice!\r\n\r\n@tensorflow-jenkins test this please", "github issues it seems, trying again.  @tensorflow-jenkins test this please", "Thanks @vrv for the review. The PR has been updated. Please take a look.", "@tensorflow-jenkins test this please"]}]