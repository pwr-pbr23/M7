[{"number": 47490, "title": "[ROCm] Add NHWC layout support when creating miopenTensorDescriptor for conv filter", "body": "This PR has two commits \r\n1. add code in `rocm_dnn.cc` and `dnn.[h,cc]` to correctly populate the `miopenTensorDescriptor` for the convolution filter for NHWC layout (it currently works only for NCHW layout) ... see commit message for details\r\n2. Remove dead code from `rocm_dnn.cc` w.r.t NHWC layout\r\n\r\n\r\n--------------------------------------------------\r\n\r\n\r\n/cc @cheshire @chsigg", "comments": ["@cheshire @chsigg gentle ping", "> Is it possible to test this?\r\n\r\nYes...you can force layout to NHWC mode in XLA via `XLA_FLAGS=--xla_gpu_force_conv_nhwc`, which in turn will trigger this code. Currently we are trying to find out all the changes required to enable the NHWC layout on AMDGPUs and that is how we stumbled upon this.", "So the testing plan is to eventually have  an integration test running with `--xla_gpu_force_conv_nhwc`? Maybe it's possible to make some more localized unit test?", "please hold off on merging this PR. the commit to use `absl::c_transform` does not work right on the ROCm platform...looking into why", "@cheshire found my bug and fixed it...please re-review + re-approve.\r\n\r\nw.r.t your question on testing...currently we cannot test this path since all the required pieces to make NHWC layout functional, are not yet in place. Once they are though, this path should get tested automatically as a consequence of unit-tests that test NHWC layout + FP16"]}, {"number": 47489, "title": "Fix for issue #47216", "body": "FIX TypeError if set the weights to the current weights via `set_weights, more details in #47216.\r\nThis new line improves the versatility of the set_weights() function. Now we can use .get_weights() or .weights of any existing layer to the set_weights() of a new layer to copy the existing weights to the new layer. @chenmoneygithub  I fixed the issue mentioned in #47255 ", "comments": ["It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "on it\r\n", "@shivaditya-meduri  Any update on this PR? Please. Thanks!", "> @shivaditya-meduri Any update on this PR? Please. Thanks!\r\nI will submit a new PR for this issue, thank you all for your guidance.\r\n"]}, {"number": 47488, "title": "Untraced function warning and Model parsing failure for Keras TCN Regressor (TF Lite)", "body": "**The error:** TF Lite converter throws an untraced function warning when trying to convert a temporal CNN (built using the widely used Keras TCN library: https://github.com/philipperemy/keras-tcn ), and throws in model parsing error when trying to do post-training quantization\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): Pip (python 3.8.8)\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0 (TF Base), 2.4.0 (TF-GPU)\r\n\r\n### 2. Code\r\nPart 1, converting pretrained TF model to TF Lite Model:\r\n```\r\nimport os\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\r\nfrom tensorflow.python.keras.backend import set_session\r\nimport tensorflow as tf\r\nconfig = tf.compat.v1.ConfigProto() \r\nconfig.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\r\nconfig.log_device_placement = True  # to log device placement (on which device the operation ran)\r\nsess = tf.compat.v1.Session(config=config)\r\nset_session(sess)  # set this TensorFlow session as the default\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom gtda.time_series import SlidingWindow\r\nimport matplotlib.pyplot as plt\r\nfrom math import atan2, pi, sqrt\r\n\r\nfrom tensorflow.keras.layers import Dense, MaxPooling1D, Flatten\r\nfrom tensorflow.keras import Input, Model\r\nfrom tensorflow.keras.callbacks import ModelCheckpoint\r\nfrom tcn import TCN, tcn_full_summary\r\nfrom tensorflow.keras.models import load_model\r\n\r\nmodel = load_model('best_joint_new.hdf5',custom_objects={'TCN':TCN})\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nmodel_no_quant_tflite = converter.convert()\r\nopen('best_joint.tflite', \"wb\").write(model_no_quant_tflite)\r\n```\r\nPart 2: Post training quantization\r\n\r\ntrainX is a nX200X6 matrix of floating point values, n can be any integer.\r\n```\r\ndef representative_dataset():\r\n    for i in range(trainX.shape[0]):\r\n        yield ([trainX[i]])\r\n```\r\nSame converter used as before.\r\n```\r\n# Set the optimization flag.\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n# Enforce integer only quantization\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n# Provide a representative dataset to ensure we quantize correctly.\r\nconverter.representative_dataset = representative_dataset\r\nmodel_quant_tflite = converter.convert()\r\n# Save the model to disk\r\nopen('best_joint_quant.tflite', \"wb\").write(model_quant_tflite)\r\n```\r\n\r\n### 3. Failure after conversion\r\nPart 1 (conversion successful but produces the following warning)\r\n```\r\nWARNING:absl:Found untraced functions such as residual_block_0_layer_call_and_return_conditional_losses, residual_block_0_layer_call_fn, residual_block_1_layer_call_and_return_conditional_losses, residual_block_1_layer_call_fn, residual_block_2_layer_call_and_return_conditional_losses while saving (showing 5 of 325). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as residual_block_0_layer_call_and_return_conditional_losses, residual_block_0_layer_call_fn, residual_block_1_layer_call_and_return_conditional_losses, residual_block_1_layer_call_fn, residual_block_2_layer_call_and_return_conditional_losses while saving (showing 5 of 325). These functions will not be directly callable after loading.\r\n```\r\nPart 2 (quantization fails)\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)\r\n     57       self._calibrator = (\r\n---> 58           _calibration_wrapper.CalibrationWrapper(model_content))\r\n     59     except Exception as e:\r\n\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-f34a9c965790> in <module>\r\n      7 # Provide a representative dataset to ensure we quantize correctly.\r\n      8 converter.representative_dataset = representative_dataset\r\n----> 9 model_quant_tflite = converter.convert()\r\n     10 # Save the model to disk\r\n     11 open('best_joint_quant.tflite', \"wb\").write(model_quant_tflite)\r\n\r\n~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    871           graph=frozen_func.graph)\r\n    872 \r\n--> 873     return super(TFLiteKerasModelConverterV2,\r\n    874                  self).convert(graph_def, input_tensors, output_tensors)\r\n    875 \r\n\r\n~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in convert(self, graph_def, input_tensors, output_tensors)\r\n    630     calibrate_and_quantize, flags = quant_mode.quantizer_flags()\r\n    631     if calibrate_and_quantize:\r\n--> 632       result = self._calibrate_quantize_model(result, **flags)\r\n    633 \r\n    634     flags_modify_model_io_type = quant_mode.flags_modify_model_io_type(\r\n\r\n~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/lite.py in _calibrate_quantize_model(self, result, inference_input_type, inference_output_type, activations_type, allow_float)\r\n    447     # Add intermediate tensors to the model if needed.\r\n    448     result = _calibrator.add_intermediate_tensors(result)\r\n--> 449     calibrate_quantize = _calibrator.Calibrator(result)\r\n    450     if self._experimental_calibrate_only or self._experimental_new_quantizer:\r\n    451       calibrated = calibrate_quantize.calibrate(\r\n\r\n~/anaconda3/envs/tflite/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py in __init__(self, model_content)\r\n     58           _calibration_wrapper.CalibrationWrapper(model_content))\r\n     59     except Exception as e:\r\n---> 60       raise ValueError(\"Failed to parse the model: %s.\" % e)\r\n     61     if not self._calibrator:\r\n     62       raise ValueError(\"Failed to parse the model.\")\r\n\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n\r\n```\r\n\r\n### 4. The Master Model Architecture\r\nInput: 6 channels of 200-sample floating point values\r\nOutput: 2 scalars (floating point)\r\n\r\n![Screenshot from 2021-03-01 17-57-39](https://user-images.githubusercontent.com/20337475/109585138-c7863f80-7ab7-11eb-9268-cfc13a7a2fa2.png)\r\n\r\n### 5. Link to original HDF5 model:\r\nhttps://drive.google.com/file/d/1GFRgMUkIVatSsUWgnzee_jjeF6D3l-A-/view?usp=sharing ", "comments": ["You can ignore the above warnings if the conversion was succeeded.\r\n\r\nCould you triage this issue, @ethkim ?", "@swapnilsayansaha,\r\nRegarding the quantization part, I was able to reproduce the issue with [TF v2.4.1](https://colab.research.google.com/gist/amahendrakar/737651fd677b87319bef3b969d395235/47488.ipynb#scrollTo=iHumRloTSBwh). \r\n\r\nHowever, with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/bcc12993a4b5dc07aa8186f61a32a6d9/47488-tf-nightly.ipynb#scrollTo=iHumRloTSBwh), I am facing a different error stating `NameError: name 'trainX' is not defined`. \r\n\r\nPlease check the linked gist for reference. Thanks!", "> @swapnilsayansaha,\r\n> Regarding the quantization part, I was able to reproduce the issue with [TF v2.4.1](https://colab.research.google.com/gist/amahendrakar/737651fd677b87319bef3b969d395235/47488.ipynb#scrollTo=iHumRloTSBwh).\r\n> \r\n> However, with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/bcc12993a4b5dc07aa8186f61a32a6d9/47488-tf-nightly.ipynb#scrollTo=iHumRloTSBwh), I am facing a different error stating `NameError: name 'trainX' is not defined`.\r\n> \r\n> Please check the linked gist for reference. Thanks!\r\n\r\ntrainX is any nX200X6 matrix as I stated above.. You can use numpy to generate it, for example:\r\n```\r\ntrainX = numpy.random.rand(500,200,6)\r\n```", "@swapnilsayansaha,\r\nThank you for the update. \r\n\r\n@rmothukuru,\r\nI was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/737651fd677b87319bef3b969d395235/47488.ipynb#scrollTo=iHumRloTSBwh). \r\n\r\nWhereas with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/2ad3f6ccf670658c6719035e1e14ca55/47488-tf-nightly.ipynb#scrollTo=iHumRloTSBwh), the error is `RuntimeError: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (3 != 2)Node number 0 (PAD) failed to prepare.` \r\n\r\nPlease check the linked gist for reference. Thanks!", "There was a bug in the `representative_dataset` method.\r\n\r\nTry the below one:\r\n\r\n```\r\nimport numpy\r\ndef representative_dataset():\r\n    for i in range(500):\r\n        yield ([numpy.random.rand(1,200,6).astype(np.float32)])\r\n```\r\n\r\nAfter applying the above one, the conversion for quantization will be succeeded.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47488\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47488\">No</a>\n", "It's not working on my end, same error persists. I copied and pasted the exact solution.\r\n", "Please use the tf-nightly version. I confirmed that the issue is gone with the above tf-nightly's gist. https://colab.research.google.com/gist/amahendrakar/2ad3f6ccf670658c6719035e1e14ca55/47488-tf-nightly.ipynb#scrollTo=iHumRloTSBwh", "Ya it works with tf-nightly. Thanks."]}, {"number": 47487, "title": "Can't save/load a Keras model's optimizer weights when using SavedModel format", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.10\r\n\r\n**Describe the current behavior**\r\nWhen saving a keras model in TensorFlow SavedModel format using `model.save` optimizer weights are not saved, even when specifying `include_optimizer=True` in the `model.save` call  and `compiled=True` in the `tf.keras.models.load_model` call. This is crucial for continuing training from a checkpoint with adaptive optimizers.\r\nLooks like the optimizer object gets saved and loaded along with its parameters. Only the weights are missing.\r\n\r\nWeights **are** saved as expected when saving in `h5` format.\r\n\r\n**Describe the expected behavior**\r\nOptimizer weights should be saved when specifying `include_optimizer=True`.\r\n\r\n**Standalone code to reproduce the issue**\r\nReproduced in [this Colab](https://colab.research.google.com/drive/1bEZif1c6xzHdcGO4SQQMB9q0DMDo8jbI?usp=sharing)\r\n", "comments": ["@edend10,\r\nLooking at similar issue [#33424](https://github.com/tensorflow/tensorflow/issues/33424#issuecomment-544933871), I was able to save and load the model's optimizer weights.\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/307f21bd82efefb6a97daf321c946919/test_load_optimizer_weights.ipynb). Thanks!", "Thank you @amahendrakar !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47487\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47487\">No</a>\n"]}, {"number": 47486, "title": "Building tensorflow_cc target (cpu-only) from scratch with xla failing (TF1.15.5)", "body": "### System information\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n-   **TensorFlow installed from (source or binary)**: source\r\n-   **TensorFlow version (use command below)**: 1.15\r\n-   **Python version**: 3.6\r\n-   **Bazel version (if compiling from source)**: 0.26.1\r\n-   **GCC/Compiler version (if compiling from source)**: 7.5.0 (set using --action env GCC_HOST_COMPILER_PATH)\r\n\r\n### Source code / logs\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ntensorflow/compiler/xla/service/generic_transfer_manager.cc: In member function 'virtual tensorflow::Status xla::GenericTransferManager::WriteSingleTupleIndexTable(stream_executor::Stream*, absl::Span<const stream_executor::DeviceMemoryBase>, const xla::Shape&, stream_executor::DeviceMemoryBase*)':\r\ntensorflow/compiler/xla/service/generic_transfer_manager.cc:56:47: error: expected ',' before '{' token\r\n   stream->ThenDoHostCallback([element_pointers{std::move(element_pointers)}]() {\r\n                                               ^\r\ntensorflow/compiler/xla/service/generic_transfer_manager.cc:56:47: error: expected identifier before '{' token\r\nTarget //tensorflow:tensorflow_cc failed to build\r\nINFO: Elapsed time: 1827.953s, Critical Path: 236.32s\r\nINFO: 5317 processes: 5317 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully", "comments": ["@ksangeet9ap \r\nWe see that you are using a old version of tenor flow and there is no support for tf 1.x, can you please upgrade to 2,x and verify you meet the [requirements](https://www.tensorflow.org/install/source).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47486\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47486\">No</a>\n"]}, {"number": 47485, "title": "flatbuffers download:", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Windows 10:\r\n- TensorFlow installed from (source or binary):Binary\r\n- TensorFlow version: 3 (Lite)\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: cloned it. from github\r\n\r\n**Describe the problem**\r\n flatbuffers download is failing.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n$ make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test\r\nSYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\r\nsyswgetrc = C:\\Program Files (x86)\\GnuWin32/etc/wgetrc\r\n--2021-03-01 22:41:11--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\nResolving mirror.tensorflow.org... 172.217.6.48\r\nConnecting to mirror.tensorflow.org|172.217.6.48|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1760478 (1.7M) [application/zip]\r\nSaving to: `/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip'\r\n\r\n     0K .......... .......... .......... .......... ..........  2%  526K 3s\r\n    50K .......... .......... .......... .......... ..........  5%  855K 2s\r\n   100K .......... .......... .......... .......... ..........  8% 1.33M 2s\r\n   150K .......... .......... .......... .......... .......... 11% 2.38M 2s\r\n   200K .......... .......... .......... .......... .......... 14%  612K 2s\r\n   250K .......... .......... .......... .......... .......... 17% 63.8M 1s\r\n   300K .......... .......... .......... .......... .......... 20%  444K 2s\r\n   350K .......... .......... .......... .......... .......... 23% 16.7M 1s\r\n   400K .......... .......... .......... .......... .......... 26% 1.03M 1s\r\n   450K .......... .......... .......... .......... .......... 29% 1.62M 1s\r\n   500K .......... .......... .......... .......... .......... 31%  340K 1s\r\n   550K .......... .......... .......... .......... .......... 34% 89.0M 1s\r\n   600K .......... .......... .......... .......... .......... 37%  106M 1s\r\n   650K .......... .......... .......... .......... .......... 40% 1.37M 1s\r\n   700K .......... .......... .......... .......... .......... 43% 4.26M 1s\r\n   750K .......... .......... .......... .......... .......... 46%  697K 1s\r\n   800K .......... .......... .......... .......... .......... 49% 1.48M 1s\r\n   850K .......... .......... .......... .......... .......... 52% 1.04M 1s\r\n   900K .......... .......... .......... .......... .......... 55% 1.81M 1s\r\n   950K .......... .......... .......... .......... .......... 58% 1.79M 1s\r\n  1000K .......... .......... .......... .......... .......... 61% 1.73M 1s\r\n  1050K .......... .......... .......... .......... .......... 63% 1.44M 1s\r\n  1100K .......... .......... .......... .......... .......... 66% 1.31M 0s\r\n  1150K .......... .......... .......... .......... .......... 69% 1.58M 0s\r\n  1200K .......... .......... .......... .......... .......... 72% 1.40M 0s\r\n  1250K .......... .......... .......... .......... .......... 75% 1.70M 0s\r\n  1300K .......... .......... .......... .......... .......... 78%  525K 0s\r\n  1350K .......... .......... .......... .......... .......... 81% 71.7M 0s\r\n  1400K .......... .......... .......... .......... .......... 84% 2.75M 0s\r\n  1450K .......... .......... .......... .......... .......... 87% 1.14M 0s\r\n  1500K .......... .......... .......... .......... .......... 90%  888K 0s\r\n  1550K .......... .......... .......... .......... .......... 93% 6.11M 0s\r\n  1600K .......... .......... .......... .......... .......... 95% 1.66M 0s\r\n  1650K .......... .......... .......... .......... .......... 98%  892K 0s\r\n  1700K .......... .........                                  100% 2.13M=1.4s\r\n\r\n2021-03-01 22:41:13 (1.20 MB/s) - `/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip' saved [1760478/1760478]\r\n\r\ntensorflow/lite/micro/tools/make/bash_helpers.sh: line 29: /tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: No such file or directory\r\ntensorflow/lite/micro/tools/make/Makefile:525: *** Something went wrong with the flatbuffers download: Bad checksum. Expected: aa9adc93eb9b33fa1a2a90969e48baee, Got: .  Stop.\r\n\r\nThe file dca12522a9f9e37f126ab925fd385c807ab4f84e.zip does exist under c:/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\n", "comments": ["@mushtaqsyed,\r\nPlease go through these comments from similar issues [#46916](https://github.com/tensorflow/tensorflow/issues/46916#issuecomment-774973928) and [#46936](https://github.com/tensorflow/tensorflow/issues/46936#issuecomment-776585339) and check if it helps. Thanks!", "Hi Abhilash:\nThese links,  #46916\n<https://github.com/tensorflow/tensorflow/issues/46916#issuecomment-774973928>\n and #46936\n<https://github.com/tensorflow/tensorflow/issues/46936#issuecomment-776585339>,\ndid not help. The impression that I get from these is that TFLite is not\nsupported under Windows 10. Is this correct?\nI am following the book TiniML, and that does not say so. Could you please\nelaborate\nThanks a lot!\nCheers,\nMushtaq\n\nOn Tue, Mar 2, 2021 at 1:28 AM Abhilash Mahendrakar <\nnotifications@github.com> wrote:\n\n> @mushtaqsyed <https://github.com/mushtaqsyed>,\n> Please go through these comments from similar issues #46916\n> <https://github.com/tensorflow/tensorflow/issues/46916#issuecomment-774973928>\n> and #46936\n> <https://github.com/tensorflow/tensorflow/issues/46936#issuecomment-776585339>\n> and check if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47485#issuecomment-788762833>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ATBN5BO2YKTSUQHVERAVXUTTBSVVXANCNFSM4YNTBNRQ>\n> .\n>\n", "Hi Abhilash:\nAny feedback?\nI forgot to mention in my first post that I was running the make command in\na cygwin terminal as an administrator. I have also tried the same in a\nMINGW64 as administrator and got the same error message.\nThanks a lot!\nCheers,\nMushtaq\n\nOn Tue, Mar 2, 2021 at 1:28 AM Abhilash Mahendrakar <\nnotifications@github.com> wrote:\n\n> @mushtaqsyed <https://github.com/mushtaqsyed>,\n> Please go through these comments from similar issues #46916\n> <https://github.com/tensorflow/tensorflow/issues/46916#issuecomment-774973928>\n> and #46936\n> <https://github.com/tensorflow/tensorflow/issues/46936#issuecomment-776585339>\n> and check if it helps. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47485#issuecomment-788762833>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ATBN5BO2YKTSUQHVERAVXUTTBSVVXANCNFSM4YNTBNRQ>\n> .\n>\n", "@mushtaqsyed,\r\n\r\nCan you refer the issue #45033 and this [link](https://forums.tinyml.org/t/make-doesnt-work-on-hello-world-test-from-tinyml-book/344/5) and let us know if it helps to resolve your issue? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47485\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47485\">No</a>\n"]}, {"number": 47484, "title": "Update wrappers.py", "body": "Name of the keyword argument corrected.", "comments": []}, {"number": 47483, "title": "Convert Pack to Reshape when there is only one operand to be packed.", "body": "This can improve optimization as there are more patterns about Reshape. Take `MultiHeadAttention` for example,\r\n\r\n```python\r\nlayer = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=5)\r\ntarget = tf.keras.Input(shape=[8, 16], batch_size=1)\r\nsource = tf.keras.Input(shape=[4, 16], batch_size=1)\r\noutput_tensor = layer(target, source, return_attention_scores=False)\r\nmodel = tf.keras.Model([target, source], output_tensor)\r\n```\r\n\r\nConverting Pack to Reshape can help the bias of the last EinsumDense fuse into FullyConnected.", "comments": []}, {"number": 47482, "title": "micro: LOG_SOFTMAX PR2", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nPR step 2 for issue #47291", "comments": ["@ddavis-2015 This PR is in draft, any update on this? Please. Thanks!", "@ddavis-2015 This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 47481, "title": "micro: LOG_SOFTMAX PR1", "body": "Extract the parsing out of a switch statement case to create a\r\nstandalone function which can be called by the micro op resolver.\r\n\r\nPR step 1 for issue  #47291", "comments": []}, {"number": 47480, "title": "[FIX] cudaMallocAsync:  make it works for distributed strategy", "body": "Follow up from https://github.com/tensorflow/tensorflow/pull/46551\r\n\r\nIt would be great that this get included in TF 2.5. It will allow more cases to use it.\r\n\r\nCurrently it crash with not related errors.\r\n\r\n@sanjoy @imintz \r\n", "comments": ["> @sanjoy @imintz\r\n\r\nNote: Idan is no longer at Google.", "Note, I found some problems on some computers for cudaMallocAsync with Distributed Strategy. You can merge this PR now or wait for a more complet fix hopefully in a few days.", "I updated this PR. It is ready for review.\r\nOne limitation force a more complex fixes.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}, {"number": 47479, "title": "Saving model in TF 2.4", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda 11.1 cudnn 8.0.5\r\n- GPU model and memory: A100, 40GB\r\n\r\n**Describe the current behavior**\r\n\r\nSaving a model in TF 2.4 omits sub-layers and generates warning messages about \"untraced function.\"\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warning and the sub-layers will be directly callable after loading.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nEXAMPLE:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION) ### v2.4.0-49-g85c8b2a817f 2.4.1\r\n\r\n'''\r\nsimple autoencoder example\r\n'''\r\n\r\nclass Encoder(tf.keras.layers.Layer):\r\n    def __init__(self, name=\"encoder\", **kwargs):\r\n        super(Encoder, self).__init__(name=name, **kwargs)\r\n        self.enc = tf.keras.layers.Dense(units=20, activation=\"relu\", name='encoder_layer')\r\n\r\n    def call(self, inputs):\r\n        x = inputs\r\n        x = self.enc(x)\r\n        return x\r\n\r\nclass Decoder(tf.keras.layers.Layer):\r\n    def __init__(self, name=\"decoder\", **kwargs):\r\n        super(Decoder, self).__init__(name=name, **kwargs)\r\n        self.dec = tf.keras.layers.Dense(units=1000, activation=\"sigmoid\", name='decoder_layer')\r\n\r\n    def call(self, inputs):\r\n        x = inputs\r\n        x = self.dec(x)\r\n        return x\r\n    \r\nclass AutoEncoder(tf.keras.Model):\r\n    def __init__(self, name=\"autoencoder\", **kwargs):\r\n        super(AutoEncoder, self).__init__(name=name, **kwargs)\r\n        self.encoder = Encoder()\r\n        self.decoder = Decoder()\r\n    \r\n    def call(self, inputs):\r\n        x = self.encoder(inputs)\r\n        x = self.decoder(x)\r\n        return x\r\n\r\n    def train_step(self, data):\r\n        x, y = data\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(y - y_pred), axis=-1), axis=-1) # mean squared error\r\n        gradients = tape.gradient(mse_loss, self.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\r\n        return {\"loss\": mse_loss}\r\n    \r\n'''\r\nexample run\r\n'''\r\ninp = np.random.rand(10000,1000)\r\nae = AutoEncoder()\r\nae.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3))\r\nae.fit(inp, inp)\r\n\r\n'''\r\nsaving throws warning message \"Found untraced functions ... These functions will not be directly callable after loading.\"\r\nThis is new in TF 2.4.\r\nTF 2.3 did not have this issue.\r\n'''\r\nae.save('tmp')\r\n```\r\nOUTPUT:\r\n```\r\nv2.4.0-49-g85c8b2a817f 2.4.1\r\n313/313 [==============================] - 1s 1ms/step - loss: 83.3821\r\nWARNING:absl:Found untraced functions such as encoder_layer_layer_call_and_return_conditional_losses, encoder_layer_layer_call_fn, decoder_layer_layer_call_and_return_conditional_losses, decoder_layer_layer_call_fn, encoder_layer_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as encoder_layer_layer_call_and_return_conditional_losses, encoder_layer_layer_call_fn, decoder_layer_layer_call_and_return_conditional_losses, decoder_layer_layer_call_fn, encoder_layer_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: tmp/assets\r\nINFO:tensorflow:Assets written to: tmp/assets\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nSpawn from issue #44541", "comments": ["@hoondy,\r\nI was able to reproduce the issue with [TF v2.4.1](https://colab.research.google.com/gist/amahendrakar/57712f2e5ad9bca4a6cc7018e2b365f8/47479.ipynb) and TF-nightly. \r\n\r\nWhereas with [TF v2.3.2](https://colab.research.google.com/gist/amahendrakar/47503d1d9f3f40b93c66861f922906cb/47479-2-3.ipynb), I am facing a different warning stating `WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\n.` \r\n\r\nPlease check the linked gist for reference. Thanks!", "Additionally, I am seeing these new warning messages after training a slightly more complex model with larger data:\r\n\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9a0b138940> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: Unable to locate the source code of <function canonicalize_signatures.<locals>.signature_wrapper at 0x2b9a0b138940>. Note that functions defined in certain environments, like the interactive Python shell do n\r\not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: co\r\nuld not get source code\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```", "I am getting the same error with tf 2.4.1\r\nWanted to know is this just an issue with warning message or if I load the model from the saved checkpoint then will it cause issues with predictions", "Same warning with a simple one layer LSTM model with a lambda function. Please update the save function.", "saving models in H5 format seems to work for me.  \r\n\r\n`model.save(filepath, save_format=\"h5\")`", "@hoondy This is intended behavior and the warning is to encourage users to use `model.save` instead of tf.saved_model.save when saving keras models.\r\n\r\nAs mentioned in another similar issue, i trained with `TF2.4` and saved and then loaded with `tf-nightly`. I was able to train the `loaded_model` without any issue. The warning was only to encourage saving Keras model with `model.save` that saves layer configs / trainable status / name / other python attributes along with graph_def. \r\n\r\nPlease let us know If you notice any error due to this warning. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47479\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47479\">No</a>\n", "Thanks for looking into this", "@jvishnuvardhan as mentioned in https://github.com/tensorflow/tensorflow/issues/47554, these warnings occur when using model.save as well - not just tf.saved_model.save - and it seems to lead to other problems for some people. Maybe you could have a look?", "> @jvishnuvardhan as mentioned in #47554, these warnings occur when using model.save as well - not just tf.saved_model.save - and it seems to lead to other problems for some people. Maybe you could have a look?\r\n\r\n\r\nI agree with this. The warning occurs with `model.save` as well, and then after loading the function if I train and save it again, I cannot load it. I can see this in **tensorlflow==2.6.0**", "Hey guys, so I was able to resolve the issue. \r\nI was using custom functions for loss and metric in my code and due to that was facing the issue. So included the get_config(self) function in those custom metrics code\r\n\r\nAnd while loading from saved file added  below how it was loaded\r\n\r\nsaved_model = keras.models.load_model(\r\n     f\"{dsm.checkpoint_dir}/epoch_{EPOCHS}/model\",\r\n     custom_objects={\r\n         \"SparseTopKCategoricalAccuracyMetric\": SparseTopKCategoricalAccuracyMetric,\r\n         \"custom_loss\": custom_loss,\r\n     },\r\n )\r\n \r\n Thanks everyone for all the help."]}, {"number": 47478, "title": "Tensorflow Lite Custom Object detection Model Error in Android app", "body": "Could you please help to solve this error?\r\n\r\nI am testing a custom Object Detection model using TensorFlow Lite in Android App according to the documentation, but I have an error when the library tries to recognize an image.\r\n\r\nI am using the Tensorflow lite sample app: `https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android`\r\n\r\n**Using Task Library:** `https://www.tensorflow.org/lite/inference_with_metadata/task_library/object_detector`\r\n\r\n**dependency version implementation:** `'org.tensorflow:tensorflow-lite-task-vision:0.1.0'`\r\n\r\nWhen this method is executed, this error is obtained::\r\n\r\n**method**\r\n` List<Detection> results = objectDetector.detect(TensorImage.fromBitmap(bitmap));`\r\n\r\nerror\r\n```\r\nAbort message: 'JNI DETECTED ERROR IN APPLICATION: JNI NewStringUTF called with pending exception java.lang.NoSuchMethodError: no static method Lorg/tensorflow/lite/support/label/Category;.create(Ljava/lang/String;Ljava/lang/String;F)Lorg/tensorflow/lite/support/label/Category;\"\r\n        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(long, java.nio.ByteBuffer, int, int, int) (ObjectDetector.java:-2)\r\n        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(org.tensorflow.lite.support.image.TensorImage, org.tensorflow.lite.task.core.vision.ImageProcessingOptions) (ObjectDetector.java:312)\r\n        at java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(org.tensorflow.lite.support.image.TensorImage) (ObjectDetector.java:292)\r\n        at java.util.List org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(android.graphics.Bitmap) (TFLiteObjectDetectionAPIModel.java:87)\r\n        at void org.tensorflow.lite.examples.detection.DetectorActivity$2.run() (DetectorActivity.java:187)\r\n        at void android.os.Handler.handleCallback(android.os.Message) (Handler.java:938)\r\n        at void android.os.Handler.dispatchMessage(android.os.Message) (Handler.java:99)\r\n        at void android.os.Looper.loop() (Looper.java:223)\r\n        at void android.os.HandlerThread.run() (HandlerThread.java:67)\r\n    \r\n        in call to NewStringUTF\r\n        from java.util.List org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(long, java.nio.ByteBuffer, int, int, int)'\r\n\r\n```\r\n", "comments": ["resolved"]}, {"number": 47477, "title": "micro: LOG_SOFTMAX PR3-5", "body": "PR steps 3 through 5 for the LOG_SOFTMAX operator as per Issue #47291 ", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Hi @ddavis-2015 Thanks for contributions!\r\n\r\nI would suggest splitting this request into two, one for refactoring in internals/reference_ops + parser change and one for micro kernel additions since the former one can be easily reviewed.", "> Hi @ddavis-2015 Thanks for contributions!\r\n> \r\n> I would suggest splitting this request into two, one for refactoring in internals/reference_ops + parser change and one for micro kernel additions since the former one can be easily reviewed.\r\n\r\n@abattery \r\nOh, whoops, forgot to submit PR1 and PR2...\r\n", "@ddavis-2015 Sure. I would not make a conflict comment so I am okay with proceeding the reviews as the current combined one. Thanks", "@ddavis-2015 This PR is in draft, any update on this? Please. Thanks!"]}, {"number": 47476, "title": "[Intel MKL] update public CI with gcc7", "body": "currently public CI https://tensorflow-ci.intel.com/ running with gcc5, upgrade to use gcc7", "comments": []}, {"number": 47475, "title": "Saving and loading a model using tf.keras and CategoricalCrossentropy with from_logits=True restores an incorrect model.", "body": "Only incorrect when using CategoricalCrossentropy with from_logits=True and sigmoid outputs. Using from_logits=False and softmax outputs is fine, as is BinaryCrossenropy with from_logits=True and sigmoid outputs.\r\n\r\nBig problem since CategoricalCrossentropy is most popular loss, and from_logits=True is recommended usage due to numerical stability.\r\n\r\nModel is saved using ModelCheckpoint, val_accuracy, max. Trained using Adam.\r\n\r\nSaved model loads successfully but predictions do not match the validation accuracy reported by tf when the model was saved, always much worse but does indicate partial training rather than random weights. \r\n\r\nLoaded model object reports correct from_logits value.\r\n\r\nThe first saved model usually gives correct result, the second save is often is wrong, third and beyond are always wrong.\r\n\r\nEven a simple models displays this behavior.\r\n\r\nTested on stable and nightly tf, same result. Using tf.keras, not standalone keras.", "comments": ["@dcbarton \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "I'm filing this as a courtesy and gave detailed info. I don't know what template you are referring to. My tf version is:\r\n\r\nv1.12.1-51802-gb334cd537ac 2.5.0-dev20210228\r\n\r\nI'm on ubuntu 20.04, cuda 11.1, cuDnn 8.1\r\n\r\nrtx 3090", "@dcbarton \r\nPlease hare simple stand alone code as requested for us to replicate the issue and analyse or if possible share a colab gist with the error reported.\r\n\r\nAlso could you try to downgrade to cuda 11.0 and let us know if you still face the issue as 11.0 is the tested version.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47473, "title": "tf-nightly-gpu builds from February are crashing on 'tf.keras model.fit()' for Nvidia RTX 3090; January builds performance is slower than 2080ti on : 2Layer Bidirectional LSTM + 3 Dense Layers", "body": "**System information**\r\nOS Windows 10 Pro : 1909 18363.1379\r\nCUDA TOOLKIT : 11.1\r\nCUDNN: 8.1.0\r\nLatest GPU Driver 461.72\r\n\r\nGPU : MSI RTX 3090 Gaming X Trio\r\nCPU i7 3930k 32GB ram\r\n\r\nusing Anaconda\r\npython 3.7.9\r\ntf-nightly-gpu 2.5.0.dev20210114\r\ntf-nightly 2.5.0.dev20210114\r\n\r\n**Describe the problem**\r\n\r\n=> The above config allows me to run my code LSTM code in tensor flow (extract below):\r\nHowever if i create an environment in Anaconda by cloning the above and pip --upgrade to a tf-nightly build more recent, including todays build\r\nof 2.5.0.dev20210228 ... when the code reaches tf.model.fit .. it crashes out with no error reporting at all.\r\n\r\nThe reason why i tried upgrading the tf-nightly build : currently the model training time is about the same as on my RTX 2080ti and sometimes even slower which i thought was down to issues with the tf-nightly january Build and Ampere.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe code can be run from CMD conda command prompt or from a terminal in Visual Studio Code and it crashes as soon as the model.fit line is reached.\r\n\r\n\r\n**Any other info / logs**\r\ncode block:\r\n\r\nmodel = tf.keras.models.Sequential([\r\n#input shape equals term points in curve\r\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,return_sequences=True),input_shape=(n_days,n_cols)),\r\ntf.keras.layers.Dropout(0.2),\r\n\r\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\ntf.keras.layers.Dropout(0.2),\r\n\r\ntf.keras.layers.Dense(64, activation='relu',kernel_initializer=initializer),\r\ntf.keras.layers.BatchNormalization(),\r\ntf.keras.layers.Dropout(0.2),\r\n    \r\ntf.keras.layers.Dense(64, activation='relu',kernel_initializer=initializer, bias_initializer=output_bias),\r\ntf.keras.layers.BatchNormalization(),\r\ntf.keras.layers.Dropout(0.2),\r\n\r\ntf.keras.layers.Dense(t_cols, activation='sigmoid', bias_initializer=output_bias)])\r\n#list of keras metrics to compute in model\r\nMETRICS = [\r\nkeras.metrics.BinaryAccuracy(name='binary_accuracy'),\r\nkeras.metrics.Precision(name='precision'),\r\nkeras.metrics.Recall(name='recall'),\r\nkeras.metrics.AUC(name='auc'),\r\n]\r\n\r\nmodel.compile(optimizer=Adam(learning_rate=lng_rate),\r\nloss='binary_crossentropy',\r\nmetrics=METRICS)\r\n\r\nhistory=model.fit(\r\nX_train,\r\nY_train,\r\nepochs=training_epochs,\r\nbatch_size=training_batch_size,\r\nverbose=1,\r\nclass_weight=label_weights[0],\r\nvalidation_data=(X_validation,Y_validation))", "comments": ["In addition to what i wrote above :  i have tested the issue with different sample codes and can replicate the error.\r\nAttached is the jupyter notebook from Laurence Mooneys Tensorflow for Developer Course (from Coursera).\r\nIt runs absolutely fine on Google colab, on my tf-nighlty build from January (as written above) but crashes on any tf-nightly-gpu build from feb onwards as soon as the code reaches the model.fit section  \r\n\r\n[S+P_Week_3_Lesson_4_LSTM.zip](https://github.com/tensorflow/tensorflow/files/6062232/S%2BP_Week_3_Lesson_4_LSTM.zip)\r\n\r\n", "Lastly to check that it wasnt my python environment in Anaconda that was corrupt. \r\nI have created a fresh new virtual env, python3.7, with tf-nightly-gpu 2.5.0.dev20210228, tf-nightly, tf-nightly-gpu 2.5.0.dev20210228, conda install pandas, numpy, scikit-learn and matplotlib : and still get the crash as described above when running the code on my 3090. I have a seperate pc with an RTX 2080ti and it has no issues. ", "I think the issue is with LSTMs in the current TF-nightly builds : they crash on a RTX 3090. A basic sequential model seems to run fine", "@ALIANS-PRODUCTIONS \r\nCan you please use cuda 11.0 and verify, then the stable version of tf 2.4 and let us know if you face the same issue.\r\n\r\nMeanwhile please have a look at existing issue and let us know:\r\n#46093, #44959, #46851", "Hi, i downgraded to cuda 11.0 : kept everything else the same : same crash on the 3090 as before.\r\nNext i kept cuda 11.0 and downgraded from tf-nightly-gpu and tf-nightly TO tf 2.4.1 : and it runs fine!\r\nI then UPGRADED cuda back to 11.1 : keeping tf 2.4.1 : also runs fine.\r\n\r\nSo there is an issue with the current tf-nightly builds for 3090s.\r\n\r\nI am happy to use tf 2.4.1 but will the fix for issue : multi-label class_weight be included in tf 2.4.1 ? \r\n>>Generalising class weights for all label ranks #46189\r\n\r\n", "@ALIANS-PRODUCTIONS \r\nPlease create a new issue for this query, as this issue is related to build and install and cuda  11.1 is not a tested we suggest to use 11.0. As this is resolved, we request you to move this to closed status and create anew issue for the query.", "> @ALIANS-PRODUCTIONS\r\n> Please create a new issue for this query, as this issue is related to build and install and cuda 11.1 is not a tested we suggest to use 11.0. As this is resolved, we request you to move this to closed status and create anew issue for the query.\r\n\r\n\r\nits not resolved. tf-nightly build from Feb crashes on ALL versions of cuda for 3090. \r\nApologies if i am misunderstanding you here. Are you saying that for now, for 3090 users: they cannot use any of the recent tf-nightly builds ? and have to use tf 2.4.1?", "Having the same frustrating issues!  \r\n\r\nUsing Cuda 11.0 with v2.4.1 is not the solution as the RTX 3090 has specific optimisations in cuda 11.2 which requires us to compile our own version of TF in order to benefit from the improved GPU speed.  It is not ideal having to pair this with v2.4.1, when we'd prefer tf-nightly.\r\n\r\n@ALIANS-PRODUCTIONS - out of interest where do you look to see that the nightly job is failing (do you have a link?), just so I can keep checking on here to see if it eventually gets fixed?", "Hi there , ,\r\n\r\n1) I have found that current performance on TF 2.4.1 with CUDA 11.0 and 3090 : is about same and sometimes even slower than my 2080ti which was quite demoralising considering the price/potential. So completely agree with you about the future potential when everything is fully working. \r\n\r\nI dont have the familiarity with UNIX/ going the docker route which others are doing to use NVIDIAS TensorFlow builds (where they ARE seeing significant performance gains on the 3090 over previous gen, puget system reviews - lambda labs etc)  \r\n\r\nAlso using MIXED precision makes the code run almost twice as slow (both 2080ti and 3090) but Tensorflow docs do mention that the speed up is on special use case for very very large networks and you should take care in implementation. So i have turned it off. \r\n\r\n2) on my second comment you will find a zip file containing a notebook from the Tensorflow developer certification course. This shows a simple LSTM solution for a Timeseries generated in the notebook. The sheet works fine on colab, TF 2.4.1, but not TF nightly for the 3090 when it reaches model.fit. I cannot figure out where deep in the tensorflow code it crashes (I happily raise my hand up to declare my ignorance to the inner nuts and bolts) \r\n3) I have also noticed that a simple DENSE layer model will actually work fine.  so maybe its a KERAS specific issue but am blindly guessing here.\r\n\r\n\r\n\r\n", "@ALIANS-PRODUCTIONS,\r\n\r\nCan you try building the latest stable version of tensorflow i.e `2.6.0` and use this [guide](https://www.tensorflow.org/install/source_windows) for reference. Please update us if the issue still persists. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47473\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47473\">No</a>\n"]}, {"number": 47472, "title": "Addition of TanhExp activation function - issue #45929", "body": "Addition of TanhExp activation function based on issue #45929 that leads changes to tensorflow/python/keras/activations.py and tensorflow/python/ops/nn_impl.py", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47472) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "The API owners are moving to a rotation based system instead of the meetings and assigning several owners to follow along and review.  Adding @fchollet, @manivaradarajan ", "This should be opened against master, not a release branch", "> This should be opened against master, not a release branch\r\n\r\nA new pull request has been created for the master branch - https://github.com/tensorflow/tensorflow/pull/47566"]}, {"number": 47471, "title": "TFLM: Reduce memory usage for some types", "body": "Reducing memory usage for non-int<8> types in conv and depthwise_conv\r\nkernels.\r\n\r\nThis is a fix for: https://github.com/tensorflow/tensorflow/issues/42883", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "ok, read the github issue and looked at the code again.\r\n\r\nOne more question:\r\n\r\nFor floating point models, this function is not called:\r\nhttps://github.com/tensorflow/tensorflow/blob/c3c035244ed464d2d3e2dddf203033971f737dd2/tensorflow/lite/micro/kernels/conv_common.cc#L105-L117\r\n\r\nSo the benefit here is reduced memory for TFLM for uint8 quatization, right? If that is the case, I would prefer removing uint8 support -- it has been on my TODO list and I was waiting for the conv refactoring work to be complete (https://github.com/tensorflow/tensorflow/pull/47112).\r\n\r\n@mansnils, once you confirm, I can go ahead and create a PR that removes uint8 support for conv from TFLM and close #42883", "Hi @advaitjain ! Is uint8 support being removed entirely from TFLu? What about TFL? I thought current uint8 supported kernels will remain, but future development will focus on int8? Thx", "> Hi @advaitjain ! Is uint8 support being removed entirely from TFLu? What about TFL? I thought current uint8 supported kernels will remain, but future development will focus on int8? Thx\r\n\r\nI'd like to remove completely for TFLM. TFLM is much newer compared to Tflite and so we do not need to support the old uint8 quantization.\r\n\r\nFurther, having partial uint8 support can send users down an unsupported path.\r\n\r\nKeeping that around also means some penalty in terms of both code size and complexity (as this change is illustrating as well).\r\n\r\nLinking to the tflite quantization spec for completeness:\r\nhttps://www.tensorflow.org/lite/performance/quantization_spec#signed_integer_vs_unsigned_integer\r\n\r\nIf you have a strong reason not to do this, then I would certainly like to hear that.\r\n\r\nNo such plan for Tflite. That will continue to have uint8 support.", "> > Hi @advaitjain ! Is uint8 support being removed entirely from TFLu? What about TFL? I thought current uint8 supported kernels will remain, but future development will focus on int8? Thx\r\n> \r\n> I'd like to remove completely for TFLM. TFLM is much newer compared to Tflite and so we do not need to support the old uint8 quantization.\r\n> \r\n> Further, having partial uint8 support can send users down an unsupported path.\r\n> \r\n> Keeping that around also means some penalty in terms of both code size and complexity (as this change is illustrating as well).\r\n> \r\n> Linking to the tflite quantization spec for completeness:\r\n> https://www.tensorflow.org/lite/performance/quantization_spec#signed_integer_vs_unsigned_integer\r\n> \r\n> If you have a strong reason not to do this, then I would certainly like to hear that.\r\n> \r\n> No such plan for Tflite. That will continue to have uint8 support.\r\n\r\nOk, thanks for the information @advaitjain! I will investigate that on my end. I fully understand the motivation.", "@advaitjain All right, let's go ahead and remove uint8 support and close https://github.com/tensorflow/tensorflow/issues/42883", "The issue will be fixed in another PR so closing this one.", "Create https://github.com/tensorflow/tensorflow/pull/47830 to remove uint8 support from conv and depthwise_conv"]}, {"number": 47470, "title": "Large overhead when py_function returns tuple of lists", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8 (verified in 3.6.9 as well)\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nGiven the code in the gist https://gist.github.com/kretes/02dc479ab7a63a8b5a559c5e7df89598 - I see that the version of the `py_function` that return a tuple of lists with data is introducing some large overhead on top of the actual operation. It takes ~10 seconds to finish the iteration in this code, while the other two versions (returning just tuple, or returning tuple of ndarrays) is done within ~2 secs.\r\n\r\n**Describe the expected behavior**\r\nThere should be no difference between returning a tuple with data, or wrapping them up with one more nesting level.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/kretes/02dc479ab7a63a8b5a559c5e7df89598 - code and logs from run on my side. I verified this in multiple installations, the recent one in jupyter/tensorflow docker image", "comments": ["This might be a duplicate of https://github.com/tensorflow/tensorflow/issues/44555 ", "Hi,\r\nI'm not sure if this is correct, but hear me out. As stated [here](https://www.tensorflow.org/guide/function#tracing), and [here](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)\r\n\r\n> Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options: ...\r\n\r\nThus, the map function traces the graphs if the input/output has a python object. numpy arrays work similar to Tensors and result in similar performance when a graph is traced.\r\n But, as mentioned in your last call,\r\n\r\n> In [10]: %%time\r\n    ...: for _ in make_ds(return_list):\r\n    ...:     pass\r\n    ...: \r\nCPU times: user 11.4 s, sys: 345 ms, total: 11.7 s\r\nWall time: 11.7 s\r\n\r\nReturning the list causes the function to retrace the graph every time and thus the time drastically increases.\r\nI think the solution to issue 44555 is on similar lines.  Prefer to continue discussion there.\r\nHope this helps.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47469, "title": "Tensorflow WARNING: tensorflow:Gradients do not exist for variables", "body": "I am trying to implement the VQ-VAE using TensorFlow 2.X. \r\nI borrow the code of the VQ-VAE training from [this github][1] which is written in TF 2x. and I'm working on merging the Keras code of PixelCNN training & sampling from [this tutorial][2] to the VQ-VAE TF 2.X code.\r\n\r\n\r\nFor the training of VQ-VAE, I think it was fine. But when I want to get the quantization vectors from the trained VQ-VAE and use the quantization vectors to further train the PixelCNN. I always get a warning:\r\n\r\n    \r\n\r\n> WARNING:tensorflow:Gradients do not exist for variables\r\n> ['conv2d_block/batch_normalization/gamma:0',\r\n> 'conv2d_block/batch_normalization/beta:0',\r\n> 'conv2d_block/conv2d/kernel:0', 'conv2d_block/conv2d/bias:0',\r\n> 'conv2d_block_1/batch_normalization_1/gamma:0',\r\n> 'conv2d_block_1/batch_normalization_1/beta:0',\r\n> 'conv2d_block_1/conv2d_1/kernel:0', 'conv2d_block_1/conv2d_1/bias:0',\r\n> 'conv2d_block_2/batch_normalization_2/gamma:0',\r\n> 'conv2d_block_2/batch_normalization_2/beta:0',\r\n> 'conv2d_block_2/conv2d_2/kernel:0', 'conv2d_block_2/conv2d_2/bias:0',\r\n> 'conv2d_block_3/batch_normalization_3/gamma:0',\r\n> 'conv2d_block_3/batch_normalization_3/beta:0',\r\n> 'conv2d_block_3/conv2d_3/kernel:0', 'conv2d_block_3/conv2d_3/bias:0',\r\n> 'conv2d_block_4/batch_normalization_4/gamma:0',\r\n> 'conv2d_block_4/batch_normalization_4/beta:0',\r\n> 'conv2d_block_4/conv2d_4/kernel:0', 'conv2d_block_4/conv2d_4/bias:0',\r\n> 'conv2d_block_5/batch_normalization_5/gamma:0',\r\n> 'conv2d_block_5/batch_normalization_5/beta:0',\r\n> 'conv2d_block_5/conv2d_5/kernel:0', 'conv2d_block_5/conv2d_5/bias:0',\r\n> 'conv2d_block_6/batch_normalization_6/gamma:0',\r\n> 'conv2d_block_6/batch_normalization_6/beta:0',\r\n> 'conv2d_block_6/conv2d_6/kernel:0', 'conv2d_block_6/conv2d_6/bias:0',\r\n> 'conv2d_block_7/batch_normalization_7/gamma:0',\r\n> 'conv2d_block_7/batch_normalization_7/beta:0',\r\n> 'conv2d_block_7/conv2d_7/kernel:0', 'conv2d_block_7/conv2d_7/bias:0',\r\n> 'conv2d_block_8/batch_normalization_8/gamma:0',\r\n> 'conv2d_block_8/batch_normalization_8/beta:0',\r\n> 'conv2d_block_8/conv2d_8/kernel:0', 'conv2d_block_8/conv2d_8/bias:0',\r\n> 'conv2d_block_9/batch_normalization_9/gamma:0',\r\n> 'conv2d_block_9/batch_normalization_9/beta:0',\r\n> 'conv2d_block_9/conv2d_9/kernel:0', 'conv2d_block_9/conv2d_9/bias:0',\r\n> 'conv2d_block_10/instance_normalization/scale:0',\r\n> 'conv2d_block_10/instance_normalization/offset:0',\r\n> 'conv2d_block_10/conv2d_10/kernel:0',\r\n> 'conv2d_block_10/conv2d_10/bias:0',\r\n> 'conv2d_block_11/instance_normalization_1/scale:0',\r\n> 'conv2d_block_11/instance_normalization_1/offset:0',\r\n> 'conv2d_block_11/conv2d_11/kernel:0',\r\n> 'conv2d_block_11/conv2d_11/bias:0',\r\n> 'conv2d_block_12/instance_normalization_2/scale:0',\r\n> 'conv2d_block_12/instance_normalization_2/offset:0',\r\n> 'conv2d_block_12/conv2d_12/kernel:0',\r\n> 'conv2d_block_12/conv2d_12/bias:0',\r\n> 'conv2d_block_13/instance_normalization_3/scale:0',\r\n> 'conv2d_block_13/instance_normalization_3/offset:0',\r\n> 'conv2d_block_13/conv2d_13/kernel:0',\r\n> 'conv2d_block_13/conv2d_13/bias:0',\r\n> 'conv2d_block_14/instance_normalization_4/scale:0',\r\n> 'conv2d_block_14/instance_normalization_4/offset:0',\r\n> 'conv2d_block_14/conv2d_14/kernel:0',\r\n> 'conv2d_block_14/conv2d_14/bias:0',\r\n> 'conv2d_block_15/instance_normalization_5/scale:0',\r\n> 'conv2d_block_15/instance_normalization_5/offset:0',\r\n> 'conv2d_block_15/conv2d_15/kernel:0',\r\n> 'conv2d_block_15/conv2d_15/bias:0',\r\n> 'conv2d_block_16/instance_normalization_6/scale:0',\r\n> 'conv2d_block_16/instance_normalization_6/offset:0',\r\n> 'conv2d_block_16/conv2d_16/kernel:0',\r\n> 'conv2d_block_16/conv2d_16/bias:0',\r\n> 'conv2d_block_17/instance_normalization_7/scale:0',\r\n> 'conv2d_block_17/instance_normalization_7/offset:0',\r\n> 'conv2d_block_17/conv2d_17/kernel:0',\r\n> 'conv2d_block_17/conv2d_17/bias:0',\r\n> 'conv2d_block_18/instance_normalization_8/scale:0',\r\n> 'conv2d_block_18/instance_normalization_8/offset:0',\r\n> 'conv2d_block_18/conv2d_18/kernel:0',\r\n> 'conv2d_block_18/conv2d_18/bias:0',\r\n> 'conv2d_block_19/conv2d_19/kernel:0',\r\n> 'conv2d_block_19/conv2d_19/bias:0', 'conv2d_20/kernel:0',\r\n> 'conv2d_20/bias:0', 'v_masked_conv_1/W_v:0', 'h_masked_conv_1/W_h:0',\r\n> 'v_masked_conv_2/W_v:0', 'h_masked_conv_2/W_h:0',\r\n> 'v_masked_conv_3/W_v:0', 'h_masked_conv_3/W_h:0',\r\n> 'v_masked_conv_4/W_v:0', 'h_masked_conv_4/W_h:0',\r\n> 'v_masked_conv_5/W_v:0', 'h_masked_conv_5/W_h:0',\r\n> 'v_masked_conv_6/W_v:0', 'h_masked_conv_6/W_h:0',\r\n> 'v_masked_conv_7/W_v:0', 'h_masked_conv_7/W_h:0',\r\n> 'v_masked_conv_8/W_v:0', 'h_masked_conv_8/W_h:0',\r\n> 'v_masked_conv_9/W_v:0', 'h_masked_conv_9/W_h:0',\r\n> 'v_masked_conv_10/W_v:0', 'h_masked_conv_10/W_h:0',\r\n> 'v_masked_conv_11/W_v:0', 'h_masked_conv_11/W_h:0',\r\n> 'v_masked_conv_12/W_v:0', 'h_masked_conv_12/W_h:0'] when minimizing\r\n> the loss.\r\n\r\n  The following is what I have for my codes.\r\n\r\n        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()  \r\n        x_train = (x_train[:200]/ 255.).astype(\"float32\")\r\n        x_test = (x_test[:200] / 255.).astype(\"float32\")\r\n        \r\n        #training vqvae \r\n        model = Autoencoder(config)\r\n        model.fit(x_train, config)\r\n        _, vq_return = model.call(x_train, training=False)\r\n        quantize = vq_return['quantize']\r\n\r\nwhere the `fit` function is I newly defined for the Autoencoder model (actually it should be the VQ-VAE mode), the rests are the same as what it has in [the git][3]. \r\n\r\nFor the PixelCNN part, I modified the above Keras tutorial to have Lambda layer as classes:\r\n\r\n    class SamplingLayer(tf.keras.layers.Layer):\r\n        def __init__(self, model, **kwargs):  \r\n            super(SamplingLayer, self).__init__()\r\n            self.model = model\r\n        def call(self, encoding_indices, training=False):\r\n            vq = self.model.vq\r\n            return vq.quantize(encoding_indices)\r\n    \r\n    class CodesSampler(tf.keras.Model):\r\n        def __init__(self, **kwargs):\r\n            super(CodesSampler, self).__init__()\r\n    \r\n        def call(self, model, size, training=False):\r\n            sampling_layer = SamplingLayer(model)\r\n            indices = tf.keras.layers.Input(shape=(size, size), name='codes_sampler_inputs', dtype='int32')\r\n            z_q = sampling_layer(indices)\r\n            codes_sampler = tf.keras.Model(inputs=indices, outputs=z_q, name=\"codes_sampler\")\r\n            return codes_sampler\r\nand the rest of Pixel CNN model is basically the same as the tutorial \r\n\r\n    '''Learning a prior over the latent space'''\r\n    class GateLayer(tf.keras.layers.Layer):\r\n        def __init__(self, **kwargs):\r\n            super(GateLayer, self).__init__()\r\n            # self.name = name\r\n        def call(self, inputs):\r\n            \"\"\"Gated activations\"\"\"\r\n            x, y = tf.split(inputs, 2, axis=-1)\r\n            return Kb.tanh(x) * Kb.sigmoid(y)\r\n    \r\n    \r\n    class MaskedConv2D(tf.keras.layers.Layer):\r\n        \"\"\"Masked convolution\"\"\"\r\n        def __init__(self, kernel_size, out_dim, direction, mode, **kwargs):\r\n            self.direction = direction     # Horizontal or vertical\r\n            self.mode = mode               # Mask type \"a\" or \"b\"\r\n            self.kernel_size = kernel_size\r\n            self.out_dim = out_dim\r\n            super(MaskedConv2D, self).__init__(**kwargs)\r\n        \r\n        def build(self, input_shape):   \r\n            filter_mid_y = self.kernel_size[0] // 2\r\n            filter_mid_x = self.kernel_size[1] // 2        \r\n            in_dim = int(input_shape[-1])\r\n            w_shape = [self.kernel_size[0], self.kernel_size[1], in_dim, self.out_dim]\r\n            mask_filter = np.ones(w_shape, dtype=np.float32)\r\n            # Build the mask\r\n            if self.direction == \"h\":\r\n                mask_filter[filter_mid_y + 1:, :, :, :] = 0.\r\n                mask_filter[filter_mid_y, filter_mid_x + 1:, :, :] = 0.\r\n            elif self.direction == \"v\":\r\n                if self.mode == 'a':\r\n                    mask_filter[filter_mid_y:, :, :, :] = 0.\r\n                elif self.mode == 'b':\r\n                    mask_filter[filter_mid_y+1:, :, :, :] = 0.0\r\n            if self.mode == 'a':\r\n                mask_filter[filter_mid_y, filter_mid_x, :, :] = 0.0\r\n            # Create convolution layer parameters with masked kernel\r\n            self.W = mask_filter * self.add_weight(\"W_{}\".format(self.direction), w_shape, trainable=True)\r\n            self.b = self.add_weight(\"v_b\", [self.out_dim,], trainable=True)\r\n        \r\n        def call(self, inputs):\r\n            return tf.keras.backend.conv2d(inputs, self.W, strides=(1, 1)) + self.b\r\n    \r\n        \r\n    def gated_masked_conv2d(v_stack_in, h_stack_in, out_dim, kernel, mask='b', residual=True, i=0):\r\n        \"\"\"Basic Gated-PixelCNN block. \r\n           This is an improvement over PixelRNN to avoid \"blind spots\", i.e. pixels missingt from the\r\n           field of view. It works by having two parallel stacks, for the vertical and horizontal direction, \r\n           each being masked  to only see the appropriate context pixels.\r\n        \"\"\"\r\n        kernel_size = (kernel // 2 + 1, kernel)\r\n        padding = (kernel // 2, kernel // 2)\r\n        v_gate = GateLayer(name=\"v_gate_{}\".format(i))\r\n        v_stack = tf.keras.layers.ZeroPadding2D(padding=padding, name=\"v_pad_{}\".format(i))(v_stack_in)\r\n        v_stack = MaskedConv2D(kernel_size, out_dim * 2, \"v\", mask, name=\"v_masked_conv_{}\".format(i))(v_stack)\r\n        v_stack = v_stack[:, :int(v_stack_in.get_shape()[-3]), :, :]\r\n        v_stack_out = v_gate(v_stack)\r\n        \r\n        kernel_size = (1, kernel // 2 + 1)\r\n        padding = (0, kernel // 2)\r\n        h_gate = GateLayer(name=\"h_gate_{}\".format(i))\r\n        h_stack = tf.keras.layers.ZeroPadding2D(padding=padding, name=\"h_pad_{}\".format(i))(h_stack_in)\r\n        h_stack = MaskedConv2D(kernel_size, out_dim * 2, \"h\", mask, name=\"h_masked_conv_{}\".format(i))(h_stack)\r\n        h_stack = h_stack[:, :, :int(h_stack_in.get_shape()[-2]), :]\r\n        h_stack_1 = tf.keras.layers.Conv2D(filters=out_dim * 2, kernel_size=1, strides=(1, 1), name=\"v_to_h_{}\".format(i))(v_stack)\r\n        h_stack_out = h_gate(h_stack + h_stack_1)\r\n        \r\n        h_stack_out =  tf.keras.layers.Conv2D(filters=out_dim, kernel_size=1, strides=(1, 1), name=\"res_conv_{}\".format(i))(h_stack_out)\r\n        if residual:\r\n            h_stack_out += h_stack_in\r\n        return v_stack_out, h_stack_out\r\n    \r\n    \r\n    def build_pixelcnn(codes_sampler, k, size, num_layers, num_feature_maps=32):\r\n        pixelcnn_prior_inputs = tf.keras.layers.Input(shape=(size, size), name='pixelcnn_prior_inputs', dtype=tf.int32)\r\n        z_q = codes_sampler(pixelcnn_prior_inputs, size) # maps indices (z_train in the implementation) to the actual codebook\r\n        \r\n        v_stack_in, h_stack_in = z_q, z_q\r\n        for i in range(num_layers):\r\n            mask = 'b' if i > 0 else 'a'\r\n            kernel_size = 3 if i > 0 else 7\r\n            residual = True if i > 0 else False\r\n            v_stack_in, h_stack_in = gated_masked_conv2d(v_stack_in, h_stack_in, num_feature_maps,\r\n                                                         kernel=kernel_size, residual=residual, i=i + 1)\r\n    \r\n        fc1 = tf.keras.layers.Conv2D(filters=num_feature_maps, kernel_size=1, name=\"fc1\")(h_stack_in)\r\n        fc2 = tf.keras.layers.Conv2D(filters=k, kernel_size=1, name=\"fc2\")(fc1) \r\n        # outputs logits for probabilities of codebook indices for each cell\r\n    \r\n        pixelcnn_prior = tf.keras.Model(inputs=pixelcnn_prior_inputs, outputs=fc2, name='pixelcnn-prior')\r\n    \r\n        # Distribution to sample from the pixelcnn\r\n        dist = tfp.distributions.Categorical(logits=fc2)\r\n        sampled = dist.sample()\r\n        prior_sampler = tf.keras.Model(inputs=pixelcnn_prior_inputs, outputs=sampled, name='pixelcnn-prior-sampler')\r\n        return pixelcnn_prior, prior_sampler\r\n    \r\n    \r\n    ##%%time\r\n    # Train the PixelCNN and monitor prediction accuracy\r\n    def accuracy(y_true, y_pred):\r\n        size = int(y_pred.get_shape()[-2])\r\n        k = int(y_pred.get_shape()[-1])\r\n        y_true = tf.reshape(y_true, (-1, size * size))\r\n        y_pred = tf.reshape(y_pred, (-1, size * size, k))\r\n        return Kb.cast(Kb.equal(y_true, Kb.cast(Kb.argmax(y_pred, axis=-1), Kb.floatx())), Kb.floatx())\r\n\r\nThe last is training PixelCNN using the quantized vectors:\r\n\r\n    z_train = model.call(x_train, training=False)[1]['encoding_indices']\r\n    pixelcnn_prior, prior_sampler = build_pixelcnn(codes_sampler, NUM_LATENT_K, SIZE, \r\n                                                   PIXELCNN_NUM_BLOCKS, PIXELCNN_NUM_FEATURE_MAPS)\r\n    pixelcnn_prior.summary()\r\n    pixelcnn_prior.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[accuracy],\r\n                           optimizer=tf.keras.optimizers.Adam(PIXELCNN_LEARNING_RATE))\r\n    prior_history = pixelcnn_prior.fit(z_train, z_train, epochs=PIXELCNN_NUM_EPOCHS, \r\n                                       batch_size=PIXELCNN_BATCH_SIZE, verbose=1)\r\n\r\n\r\n\r\nIt's also weird that when I plot the summary tables of the Keras tutorial and my modified TF2.X codes, there are different in: `codes_sampler` and `Non-trainable params: 8,064`.\r\n\r\nThe top is the summary of the Keras tutorial with most of the common parts omitted:\r\n\r\n    Model: \"pixelcnn-prior\"\r\n    __________________________________________________________________________________________________\r\n    Layer (type)                    Output Shape         Param #     Connected to                     \r\n    ==================================================================================================\r\n    pixelcnn_prior_inputs (InputLay [(None, 8, 8)]       0                                            \r\n    __________________________________________________________________________________________________\r\n    codes_sampler (Model)           (None, 8, 8, 256)    0           pixelcnn_prior_inputs[0][0]      \r\n    __________________________________________________________________________________________________\r\n    v_pad_1 (ZeroPadding2D)         (None, 14, 14, 256)  0           codes_sampler[1][0]              \r\n    __________________________________________________________________________________________________\r\n    h_pad_1 (ZeroPadding2D)         (None, 8, 14, 256)   0           codes_sampler[1][0]              \r\n    __________________________________________________________________________________________________\r\n    v_masked_conv_1 (MaskedConv2D)  (None, 11, 8, 64)    458816      v_pad_1[0][0]                    \r\n    __________________________________________________________________________________________________\r\n    h_masked_conv_1 (MaskedConv2D)  (None, 8, 11, 64)    65600       h_pad_1[0][0]                    \r\n    __________________________________________________________________________________________________\r\n    tf_op_layer_strided_slice_4 (Te [(None, 8, 8, 64)]   0           v_masked_conv_1[0][0]            \r\n    __________________________________________________________________________________________________\r\n    tf_op_layer_strided_slice_5 (Te [(None, 8, 8, 64)]   0           h_masked_conv_1[0][0]            \r\n    __________________________________________________________________________________________________\r\n    v_to_h_1 (Conv2D)               (None, 8, 8, 64)     4160        tf_op_layer_strided_slice_4[0][0]\r\n    __________________________________________________________________________________________________\r\n    tf_op_layer_add (TensorFlowOpLa [(None, 8, 8, 64)]   0           tf_op_layer_strided_slice_5[0][0]\r\n                                                                     v_to_h_1[0][0]                   \r\n    __________________________________________________________________________________________________\r\n    h_gate_1 (Lambda)               (None, 8, 8, 32)     0           tf_op_layer_add[0][0]            \r\n    __________________________________________________________________________________________________\r\n    v_gate_1 (Lambda)               (None, 8, 8, 32)     0           tf_op_layer_strided_slice_4[0][0]\r\n\r\n\r\nand \r\n\r\n    ==================================================================================================\r\n    Total params: 786,592\r\n    Trainable params: 786,592\r\n    Non-trainable params: 0\r\n\r\nFor my modified TF2.X code, \r\n\r\n    Model: \"pixelcnn-prior\"\r\n    __________________________________________________________________________________________________\r\n    Layer (type)                    Output Shape         Param #     Connected to                     \r\n    ==================================================================================================\r\n    pixelcnn_prior_inputs (InputLay [(None, 4, 4)]       0                                            \r\n    __________________________________________________________________________________________________\r\n    codes_sampler (Functional)      (None, 4, 4, 256)    21824387    pixelcnn_prior_inputs[0][0]      \r\n    __________________________________________________________________________________________________\r\n    v_pad_1 (ZeroPadding2D)         (None, 10, 10, 256)  0           codes_sampler[0][0]              \r\n    __________________________________________________________________________________________________\r\n    h_pad_1 (ZeroPadding2D)         (None, 4, 10, 256)   0           codes_sampler[0][0]              \r\n    __________________________________________________________________________________________________\r\n    v_masked_conv_1 (MaskedConv2D)  (None, 7, 4, 64)     458816      v_pad_1[0][0]                    \r\n    __________________________________________________________________________________________________\r\n    h_masked_conv_1 (MaskedConv2D)  (None, 4, 7, 64)     65600       h_pad_1[0][0]                    \r\n    __________________________________________________________________________________________________\r\n    tf.__operators__.getitem (Slici (None, 4, 4, 64)     0           v_masked_conv_1[0][0]            \r\n    __________________________________________________________________________________________________\r\n    tf.__operators__.getitem_1 (Sli (None, 4, 4, 64)     0           h_masked_conv_1[0][0]            \r\n    __________________________________________________________________________________________________\r\n    v_to_h_1 (Conv2D)               (None, 4, 4, 64)     4160        tf.__operators__.getitem[0][0]   \r\n    __________________________________________________________________________________________________\r\n    tf.__operators__.add (TFOpLambd (None, 4, 4, 64)     0           tf.__operators__.getitem_1[0][0] \r\n                                                                     v_to_h_1[0][0]                   \r\n    __________________________________________________________________________________________________\r\n    gate_layer_1 (GateLayer)        (None, 4, 4, 32)     0           tf.__operators__.add[0][0]       \r\n    __________________________________________________________________________________________________\r\n    gate_layer (GateLayer)          (None, 4, 4, 32)     0           tf.__operators__.getitem[0][0]   \r\n\r\nand \r\n\r\n    ==================================================================================================\r\n    Total params: 22,610,979\r\n    Trainable params: 22,602,915\r\n    Non-trainable params: 8,064\r\n\r\n\r\nCould you please let me know where I did wrong? I've been tried with different things but they cannot sort things out... \r\n\r\n  [1]: https://github.com/iomanker/VQVAE-TF2\r\n  [2]: https://www.kaggle.com/ameroyer/keras-vq-vae-for-image-generation/comments\r\n  [3]: https://github.com/iomanker/VQVAE-TF2/blob/master/vqvae.py\r\n\r\n", "comments": ["@JCL823,\r\nOn running the code, I am facing an error stating `NameError: name 'Autoencoder' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/99d470e30d820d97de94c728c9bb2dc2/47469.ipynb).\r\n\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Alternatively, you can also run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!", "@amahendrakar Thank you very much for the response. Please check [the Colab](https://colab.research.google.com/drive/1J3MN-nTLrs-LcS5DGwq-cWU_UWL6mUCI?usp=sharing) for the reproduction. Even though it is not exactly the same as my original output but I still have warnings saying missing losses:\r\n\r\n> WARNING:tensorflow:Gradients do not exist for variables ['to_vq/kernel:0', 'to_vq/bias:0', 'v_masked_conv_1/W_v:0', 'h_masked_conv_1/W_h:0', 'v_masked_conv_2/W_v:0', 'h_masked_conv_2/W_h:0', 'v_masked_conv_3/W_v:0', 'h_masked_conv_3/W_h:0', 'v_masked_conv_4/W_v:0', 'h_masked_conv_4/W_h:0', 'v_masked_conv_5/W_v:0', 'h_masked_conv_5/W_h:0', 'v_masked_conv_6/W_v:0', 'h_masked_conv_6/W_h:0', 'v_masked_conv_7/W_v:0', 'h_masked_conv_7/W_h:0', 'v_masked_conv_8/W_v:0', 'h_masked_conv_8/W_h:0', 'v_masked_conv_9/W_v:0', 'h_masked_conv_9/W_h:0', 'v_masked_conv_10/W_v:0', 'h_masked_conv_10/W_h:0', 'v_masked_conv_11/W_v:0', 'h_masked_conv_11/W_h:0', 'v_masked_conv_12/W_v:0', 'h_masked_conv_12/W_h:0'] when minimizing the loss.\r\n> WARNING:tensorflow:Gradients do not exist for variables ['to_vq/kernel:0', 'to_vq/bias:0', 'v_masked_conv_1/W_v:0', 'h_masked_conv_1/W_h:0', 'v_masked_conv_2/W_v:0', 'h_masked_conv_2/W_h:0', 'v_masked_conv_3/W_v:0', 'h_masked_conv_3/W_h:0', 'v_masked_conv_4/W_v:0', 'h_masked_conv_4/W_h:0', 'v_masked_conv_5/W_v:0', 'h_masked_conv_5/W_h:0', 'v_masked_conv_6/W_v:0', 'h_masked_conv_6/W_h:0', 'v_masked_conv_7/W_v:0', 'h_masked_conv_7/W_h:0', 'v_masked_conv_8/W_v:0', 'h_masked_conv_8/W_h:0', 'v_masked_conv_9/W_v:0', 'h_masked_conv_9/W_h:0', 'v_masked_conv_10/W_v:0', 'h_masked_conv_10/W_h:0', 'v_masked_conv_11/W_v:0', 'h_masked_conv_11/W_h:0', 'v_masked_conv_12/W_v:0', 'h_masked_conv_12/W_h:0'] when minimizing the loss.\r\n\r\n ", "@JCL823,\r\nI do not have access to the Colab notebook you have provided. Could you please provide the required permissions to view the file? Thanks!", "Please check [this](https://colab.research.google.com/drive/1J3MN-nTLrs-LcS5DGwq-cWU_UWL6mUCI?usp=sharing)", "@JCL823,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/42542#issuecomment-719675550) from a member of the TensorFlow team from similar issue [#42542](https://github.com/tensorflow/tensorflow/issues/42542). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47469\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47469\">No</a>\n"]}, {"number": 47468, "title": "how to Training .TextGrid file by tensorflow", "body": "now, i have get some audio label file , that  read .wav file and use Annotate -TextGrid(sentence) button operator by Praat tool;\r\nopen file  :\r\n\r\n`File type = \"ooTextFile\"\r\nObject class = \"TextGrid\"\r\n\r\nxmin = 0 \r\nxmax = 17.275351473922903 \r\ntiers? <exists> \r\nsize = 1 \r\nitem []: \r\n    item [1]:\r\n        class = \"IntervalTier\" \r\n        name = \"silences\" \r\n        xmin = 0 \r\n        xmax = 17.275351473922903 \r\n        intervals: size = 19 \r\n        intervals [1]:\r\n            xmin = 0 \r\n            xmax = 1.9536757369614515 \r\n            text = \"silent\" \r\n        intervals [2]:\r\n            xmin = 1.9536757369614515 \r\n            xmax = 3.4176757369614514 \r\n            text = \"sounding\" \r\n        intervals [3]:\r\n            xmin = 3.4176757369614514 \r\n            xmax = 4.281675736961452 \r\n            text = \"silent\" \r\n        intervals [4]:\r\n            xmin = 4.281675736961452 \r\n            xmax = 5.081675736961452 \r\n            text = \"sounding\" `\r\n\r\n---------\r\nbut  i do not how to use the file with tensorflow , \r\ngive me a dir,   3Q;", "comments": ["@UEBoy2019 \r\n\r\nThis is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 47467, "title": "Flatbuffers Checksum Issue Building With Make", "body": "**System information**\r\n- No custom code\r\n- Windows 10 \r\n- TensorFlow installed from source\r\n- TensorFlow current version\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUsing make to build Tensorflow Lite example projects fails using make for several projects. On Windows, setting command prompt to the tensorflow repository directory, and following the README for tensorflow\\lite\\micro\\examples\\magic_wand:\r\n\r\n`make -f tensorflow/lite/micro/tools/make/Makefile test_magic_wand_test`\r\n\r\nThat produces the following results:\r\n\r\n\r\n```\r\nC:\\Users\\jtork\\CMake\\tensorflow_src>make -f tensorflow/lite/micro/tools/make/Makefile test_magic_wand_test\r\nFIND: Parameter format not correct\r\nFIND: Parameter format not correct\r\n--2021-02-28 19:42:41--  http://mirror.tensorflow.org/github.com/google/flatbuffers/archive/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip\r\nResolving mirror.tensorflow.org (mirror.tensorflow.org)... 142.250.113.128, 2607:f8b0:4023:1000::80\r\nConnecting to mirror.tensorflow.org (mirror.tensorflow.org)|142.250.113.128|:80... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1760478 (1.7M) [application/zip]\r\nSaving to: '/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip'\r\n\r\n/tmp/dca12522a9f9e37f126ab925 100%[=================================================>]   1.68M  8.87MB/s    in 0.2s\r\n\r\n2021-02-28 19:42:41 (8.87 MB/s) - '/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip' saved [1760478/1760478]\r\n\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 1: $'PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 2: nL8Q5: command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 3: nL8Q?: command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 4: $'\\bnL8Qw\\030B\\005\\352\\002\\304\\005V': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 4: $'\\367\\257XY': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\\002T': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'v\\355\\236\\215\\265M4\\036w]\\027\\262\\036p\\250t5\\256\\367\\245f': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\\225L\\347\\313l\\036\\020\\350\\341\\320\\265\\254\\3218\\002~\\264B\\323\\302\\371\\016XC\\2408\\313': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'@\\244\\315\\217\\v\\2105': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'\\301\\2173H2\\037.\\342,\\311N\\223\\233d}\\231^\\257\\341': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'^\\255\\342\\345:\\231g\\220\\256': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: $'m\\237\\220Q7\\037\\324\\242\\304R\\234R\\372T\\235\\317k\\347T\\347\\316\\334\\235\\353F\\352\\217g\\317q': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 5: 6\u25a1m@0\u25a1cs/d\u25a1\u25a1 \u25a1j\u263c\u25a14\u25a1E)8\u25a1&\u25a1\u25a1\u25a1z@-i#hPo\u25a1q\u25a1\u2192\u263bX\u25a16\u25a1\u25a1\r\n\u25a1l\u25a1\u25a1\u25bc{\u25a1\u25a1A\u2642y+\u25a1\u25a1\u25a1\u2192\u25a1\u25a1\u25a1Ff\u25a1\u203c%\u25a1\u25a13\u23022\u25a1\u25a1\u0707\u25a15\u25a1s\u25a1\u25a1D%\u2192k<\u25a1\u0202\u00a7J\u25a1\u25a1\u25a1\u2194W\u2663\u25a19\u25a1\u25ba\u25a10\u25a1ub\u2666OO\u25a1\u25a1\u25a1_\u25a1\u0255\u25a1@\u25a1\u25a1Y\u2192\u25a10\u25a1!\u25a1\u2193\u25a1m\u25a1\u25a1b\u25a1\u046ah\u25a1R\u25a1(K\u25a1H\u25a12@L\u25a190j\u25a1\u25a163J\u25a1^%,\u25a1G\u25a1\u25a1j-\\\u25a1\u25a1\u2193\u2666\u263aliW\u25a1\u25a1\u25a1A\u2640\u25a127/\u25a11\u25a1\u25a1\u25a1\u221fU\u25a1e\u263a'\u25a1j\u2640Yk\u00a7\u25a1u9\u25a1\u25a1\u25a1#\u25a13\u25a1(l\u0da4\u25a1d7\u25a1\u25a1\u25a1Z4K\u25ac\u25a1Ez\u25a1\u25a1y}\u25a1]O\u25a1\u25a1,\u25a1m\u2302'>!\u25a1(\u25a1}\u25a1\u25a1\u25a1\u25a1\u25ba\u25a1W\u25a1t\u00a7\u25a1\u25a1\u2302|^\u25a1\u25a1\u25a18\u25a1\u25a1Zu\u25a1\u2195\u25a1\u221fv{\u266b\u25a1\u2660\u25a1Jt+{\u07d0%^\u25a1\u25a1k<~.\u25a1\u0640\u25a1Z46\u2666\u25a1\u25a1v9\u25a1\u25a1   \u2195\u25a1\u25a1v\u25a1\u25a1Z\u25a1\u050a\u25a1\u074f\u0732{7\u25a1o\u023d\u2666\u25a1\u2302)\u2640\u203cH\\\u25a1/     \u25a1e\u25bc\u25a1C\u25a1w\u25a1\u25a1@\u25a1\u25a1\u25a1\u25a1\u25a1\u263c\u25a1\u25a1B2\u25a1#-{\u2665\u25a1\u25a10\u25a1\u25a1\u25a1\u25a1\u25a1l>9\ufffd\u25a1t)\u25a1\u25a1s\u2193\u2191_\u25a1\u25a1P2\u25a1     PK\u2665\u2666\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 11: $'OO\\337\\367\\312\\262\\204\\0365\\3341P]\\034\\327\\025ZL4\\237t\\364]r\\363\\024c\\020\\202\\216\\214?h\\210\\254N\\326\\b\\a\\275\\221z]P\\022\\301c\\177\\346\\2608\\274\\350\\234\\3230tl4y\\006\\244\\272h.y\\371\\214i\\201,': command not found\r\nnL8Q>   flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.bazelci/UT\u2663\u263a\u25a1\u25a1l_PK\u2665\u2666\r\nnL8Q=\u25a1!X\u25a1K      flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.bazelci/presubmit.ymlUT\u2663\u263a\u25a1\u25a1l_\u25a1\u25a1\u25a1\u25a1J*\u25a1\u25a1I\u25a1L\u25a1L-\u25a1R\u25a1I,I-.\u25a1*\u25a1i\u25a1E\u25a1\u25a1V\r\n\u25a1I\u25a1y%\u25a1\u25a1f\u2660& \u25a1\u25a1\u263bX}|IbQzjI1DHWAIOOO        \u25a1\u2660\u2193\u25a1K\u25acj\u25a1\u2663\u00a7\u25a1\u25a1ML\u25a1/\u25a1\u25a1\u00b6PK\u2665\u2666\r\nnL8Q\u25a14%_\u25a10\u263aB    flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.clang-formatUT\u2663\u263a\u25a1\u25a1l_m\u25a1Mn\u25a1@\u2640\u25a1\u25a1>\u2663\u21a8)[v\u25ba\u25a1\u25a1*RP\u25a1\u2663\\p\u2660+\u25a1y\u25a1n\u25a1Q\u2302\u25a1E\u25a1: No such file or directory\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 11: $'\\034Z\\303\\211\\346\\236\\223\\377\\b\\277\\211\\215l\\270\\247\\326\\b\\257ot[\\005\\235:\\232\\004\\235\\243\\246\\277\\231\\304m\\242\\3711\\277\\356\\361\\235$\\235\\264\\031Y\\363\\351\\2365\\363\\334\\356\\004\\343\\232-_\\346_\\332+\\355[\\264\\371\\021\\324J\\234\\256\\377B\\252\\252\\002\\370\\004PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 12: $'\\bnL8Q\\33088\\322\\236\\337B': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 13: $'\\302@\\020D\\373': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\\021\\301\\336\\316\\322?\\020': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: r\u25a1\u25a1\u25a1\u25a1\u25b2T\u2191\u25a1bLl\u25a1XN\u25a1\u25a1\u25a1QtK*\u25c44: No such file or directory\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\\202\\v%Q\\256\\253\\333\\341^\\211ulh': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 17: $'f\\345eey\\206\\310?S\\336\\253wZ\\270pF\\223\\304\\2026\\306\\3236\\375{\\351\\003PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\\bnL8Q\\324\\240P3\\211\\375A': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: command substitution: line 19: syntax error near unexpected token `)'\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: command substitution: line 19: `\u263b1\u2640D\u25a1\u25a1\u25a1\u25a1\u25ac\u25ac\u25a1\u25a1\u25a1\u25a1\u2302\u25a1\u25b2d\u25a1mJ\u25a1\u25a1.\u25a1\u25a1[\u25a1^\u011d3o\u2195=\u263b&\u26669bxB \u25a10j\u25a1<\u25a1\u25a1\u25a1\u25a1k$\u25a1\u2666\u21a8x)(b\"1 \u25a1\u25a1P\u25a1x\u25a1\u25a1l'\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 16: $'\\305\\202\\205\\270\\312\\0261\\315\\274\\304RT\\373t\\363\\206\\036t\\371\\261\\204d\\340V\\327\\257v\\230\\036\\236\\332\\300U0\\330_\\246\\345\\f\\343L\\336\\027\\027mg\\252\\333\\256\\230\\235g\\376\\201I\\275\\317\\037PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 17: $'\\bnL8Q\\026\\345\\365\\275OUC': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\\200': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 18: $'\\365\\370\\243\\371PK\\003\\004': command not found\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 19: flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.github/UT\u2663\u263a\u25a1\u25a1l_PK\u2665\u2666: No such file or directory\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 20: syntax error near unexpected token `('\r\n/tmp/dca12522a9f9e37f126ab925fd385c807ab4f84e.zip: line 20: nL8Q}\u25a1\u25a1\u25a1B\u263bN flatbuffers-dca12522a9f9e37f126ab925fd385c807ab4f84e/.github/ISSUE_TEMPLATE.mdUT\u2663\u263a\u25a1\u25a1l_U\u25a1AO\u25a10\u2640\u25a1\u25a1\u25a1\u00a7o\u25a1\u25a1(\u25a1\u25a1v@\u221fABp\u263b  \u266bH\u25a1\u266b^\u25a1\u25a1\u25a1\u04a4\u25a1]`\u25a1\u25b2\u25a1C\u25a1I\u25a1!\u25a1\u25a1\u25a1=\u25a1\u25a1\u25a1\u25a1\u25a1!\u25a1hr\u25a1\u25a1\u25a1>\u25a1\u25a1\u0502\u2195\u25a1\u25a1\u25a1\u2642\u25a1^#\u25a10z\u06b3U\u00b6\u25a1\u25a1C\u25a1q\u25a1\u2193\u25a11\u2195\u25a1,\u25a1\u25a1|\u25a1\u25a1a\u25a1\\#RjGj\u25a1B.+\u25a1\u25a1\u263c!r\u25a1'\u21a8     9\u034fy\u25a1B\u25a1\u25a1\u221fD\u25a1?KR\u25a1\u25a1\u2191I\u25a1GkX\u25a1$g\u25a1\u203cB9\u263bB\u25a1F^9\u25a1\u25a1\u2663A\u25a1q\u25ba\u25a1\u25a1\u25a1u\u25a1\u25a1\u25a1\u00b6\u25a11\u25a1\u25a1\u25a1bj\u25a1\u21931\u25a1=HO\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1,\u25a1\u25a1\u25a1!\u25b2-\u25a19\u25a1G\u25a1s\u25a1\u2192\u263c\u25a1\u25a1\u25a1W\u25a1      \u25a1=\u25a1L\u25a1\u25a1\u25a1\u25a1<\\]Uh\u25a1\u25a1zu[\u25a1\u25a1\u25bc\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1\u25a1S\u25a17\u25a1dvT@\u2191J\u05bceY\u25a1      \u221f\u25a1      q)Z\u25a1\u25a1r\u25a1\u25a1\u25a1\u2666\u25a1;\u25a1\u25a1\uda07\udc61 \u2191\u25a1\u2660,\u25a1\u0660nV\u25a1\u25a1s\u03ff\u25a1\u25a16\u25a1\u25a1\u25a1\u25a1{4=;\u25a1I\u266b%\u25a1\u25b2\u25a1f\u25a1\u25a1\u25a1\u25a1\u263aPK\u2665\u2666'\r\ntensorflow/lite/micro/tools/make/Makefile:525: *** Something went wrong with the flatbuffers download: Bad checksum. Expected: aa9adc93eb9b33fa1a2a90969e48baee, Got: .  Stop.\r\n```\r\n\r\nI believe this same issue occurs for all make options I have tried.\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nAll dependencies should be resolved and the project should build. \r\n\r\n", "comments": ["@embeddetech,\r\nPlease go through these comments from similar issues [#46916](https://github.com/tensorflow/tensorflow/issues/46916#issuecomment-774973928) and [#46936](https://github.com/tensorflow/tensorflow/issues/46936#issuecomment-776585339) and check if it helps. Thanks!", "Thank you @amahendrakar , I see that there appears to not be a Windows build solution for a TFLite for Microcontrollers project. Will it be possible to create/repair a functioning Micro project build so as to have at least a single reference?\r\n\r\nNote that I am currently able to CMake TFLite in Windows as of today. However I am not exactly clear the distinction between TensorFlow Lite and Tensorflow Lite for Microcontrollers. A minimal build in Windows for Tensorflow Lite for Microcontrollers would be enormously helpful! Coming in from Windows, having to go through a Docker learning curve and work in Linux would be somewhat tedious...", "Could you please try again with the latest Tensorflow version, the above micro related files have been moved to separate repository `tensorflow/tflite-micro`. \r\nYou can find the details on the `magic_wand` examples [here]( https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/magic_wand).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47467\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47467\">No</a>\n"]}, {"number": 47466, "title": "RuntimeError: Mixing different tf.distribute.Strategy objects while using tf.distribute.MirroredStrategy()", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04)\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below): 1.15\r\n- Python version:3.6.9\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: TitanXP\r\n\r\nTF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`: v1.15.4-39-g3db52be 1.15.\r\n\r\n**Describe the current behavior**\r\nReferring to : https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py, I was trying to create a code and also converting it to multi-GPU with tf.distribute.MirroredStrategy(). The code works with out tf.distribute.MirroredStrategy() in single Titan XP, but once I use the below code its shows error as shown below. \r\n**Describe the expected behavior**\r\nUsing the tf.distribute.MirroredStrategy()\r\n\r\n`strategy=tf.distribute.MirroredStrategy()\r\n    with strategy.scope():\r\n           model....\r\n           model.compile`\r\n\r\nThe above structure should run with multi-gpu\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nReferring to https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py#L107, replaced with the above code. \r\n\r\nBut error `RuntimeError: Mixing different tf.distribute.Strategy objects: <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f36a44f72e8> is not <tensorflow.python.distribute.mirrored_strategy.MirroredStrategyV1 object at 0x7f36247d0128>`", "comments": ["@rrklearn2020,\r\nTensorFlow 1.x is not actively supported. Could you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same issue. Thanks!", "Code was written in TF1.13 and currently works with TF1.15 (Stable version). We tried to convert to TF2.x, starting with the 'Automatic conversion script' and trying make changes for each errors. But not successful. Request for hints to solve the issue in TF1.15 version for using distribute.MirroredStrategy(). ", "@rrklearn2020,\r\nCould you please share the Python script / notebook you are running, so that we can reproduce the issue on our end. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47466\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47466\">No</a>\n"]}, {"number": 47465, "title": "Correct range of input values for MobileNet", "body": "Hello,\r\n\r\nI want to use the implementation of MobileNetV3 (either the MobileNet_large or MobileNet_small version) in my project. I'm using the TensorFlow v2.4.1.\r\n\r\n I get confused when I read the documentation about the range of input values for MobileNet-based models. According to this [link](https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5)  the expected range of values for the model is [0,1]. However, according to this other [link](https://www.tensorflow.org/tutorials/images/transfer_learning), the correct range of values is [-1, 1]. A similar issue was already reported at this [link](https://github.com/tensorflow/hub/issues/637).\r\n\r\nOn the other hand, to my surprise, the function preprocess_input of the module [MobileNetV3](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L556-L558) does not apply any change over the input; i.e. the function is returning directly the input.\r\n\r\n```\r\n@keras_export('keras.applications.mobilenet_v3.preprocess_input')\r\ndef preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\r\n  return x\r\n\r\n```\r\n\r\nSo, what should be the correct range of input values for MobileNetV3?\r\n", "comments": ["Hi @ogreyesp. The following options might be worth trying while someone from the team replies to your query - \r\n\r\n* Load up the MobileNetV3 model and print the summary of the model. Inspect the first few layers and if you see something like Normalization or Rescaling to be present there then you can be sure that you can pass an input image as is. This is in fact the case for all the EfficientNet models that are present in `tf.keras.applications`. \r\n* If the above option is not applicable then maybe try with both [0, 1] and [-1, 1] range scaling and see which one works. I know this sounds terrible, so apologies in advance for this suggestion. ", "Hi @sayakpaul. Thank you for your suggestions, they were very helpful.\r\n\r\nIndeed, MobileNet v3 has a rescaling layer, so we don't need to rescale the images before passing them to the model.\r\n\r\nThanks!", "Hi @ymodak \r\n\r\nI'm not sure... according to the implementation of MobileNetV3 at this [link](https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/keras/applications/mobilenet_v3.py#L561-L563), more specifically line 265, the input is rescaled to [0,1] scale.\r\n\r\n```\r\n  x = img_input\r\n  x = layers.Rescaling(1. / 255.)(x) \r\n```", "The docs are updated for MobileNetV3 https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input\r\n"]}, {"number": 47464, "title": "Suspected duplicate code in resize_bilinear_op.cc", "body": "I suspect these two code blocks are doing very similar tasks and can potentially be merged: \r\n\r\n- [Code block 1](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/image/resize_bilinear_op.cc;l=254;drc=9e274c0b2ff75f64a97c9aec57aa59b030c5a01b;bpv=1;bpt=0)\r\n- [Code block 2](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/kernels/image/resize_bilinear_op.cc;l=265-283;drc=9e274c0b2ff75f64a97c9aec57aa59b030c5a01b;bpv=1;bpt=0)\r\n\r\nThe only difference which I can notice is that the code block 1 **might** benefit from sequential cache read. But I think the compiler should assist to achieve the same efficiency for the code block 2. If so, we should merge these two blocks to eliminate duplicate code. ", "comments": ["I can help to fix if my judgement is correct. That would be my first code contribution to TF :) ", "@ymodak I sent in a pull request which must solve this.", "I expected that I could become the assignee and fixed it. Why not give a newbie an opportunity? So sad. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47464\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47464\">No</a>\n", "@CyangXu Thanks for your issue. Assignee's generally facilitate the PR merging process, you may want to raise PR as an author to fix issue/add new feature. I am sorry you missed this opportunity however feel free to raise PR to address any other issue of your choice and we can help merge it. Contributions from the community are highly encouraged and welcomed. Thanks!", "@ymodak Thank you for explanation!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 47463, "title": "Addition of TanhExp Activation function", "body": "Changes to tensorflow\\python\\keras\\activations.py and \\tensorflow\\python\\ops\\nn_impl.py files by adding TanhExp function for the issue #45929", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47463) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47463) for more info**.\r\n\r\n\r\n\r\n> @googlebot I signed it!\r\n\r\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47463) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 47462, "title": "Problem with TensorFlow model example ", "body": "Hi,\r\n\r\nI would like to practice with the TensorFlow model example (https://www.tensorflow.org/lite/guide/ops_custom#create_a_tensorflow_model). In particular, I'm interested to repeat the error \r\n\"Error: Some of the operators in the model are not supported by the standard TensorFlow Lite runtime...... Here is\r\na list of operators for which you will need custom implementations: Sin.\" \r\n\r\nHow can I do this using, for example, tf.lite.TFLiteConverter.from_concrete_functions()?\r\nThanks in advance.\r\n", "comments": ["@Lucy20211 you need to add the `converter.allow_custom_ops = True` line to enable custom ops.", "Thanks @abattery for you reply, but before using the line you suggested I would just like to repeat the above error. Since the model in the example does not consist of layers, how can I tell the converter to try to convert in a TFLite model the model that uses the Sin operator?", "I haven't used the converter yet, I only copy and paste this part of the example code:\r\n\r\nimport tensorflow as tf\r\n \r\n\r\nx = [-8, 0.5, 2, 2.2, 201]\r\ny = [-0.6569866 ,  0.99749499,  0.14112001, -0.05837414,  0.80641841]\r\noffset = tf.Variable(0.0)\r\n\r\n\r\n@tf.function\r\ndef sin(x):\r\n  return tf.sin(x + offset, name=\"Sin\")\r\n\r\n \r\noptimizer = tf.optimizers.Adam(0.01)\r\ndef train(x, y):\r\n    with tf.GradientTape() as t:\r\n      predicted_y = sin(x)\r\n      loss = tf.reduce_sum(tf.square(predicted_y - y))\r\n    grads = t.gradient(loss, [offset])\r\n    optimizer.apply_gradients(zip(grads, [offset]))\r\n\r\nfor i in range(1000):\r\n    train(x, y)\r\n\r\nprint(\"The actual offset is: 1.0\")\r\nprint(\"The predicted offset is:\", offset.numpy())\r\n\r\n\r\nThen I would like the above error to appear, but I don't know how I should use the converter. I'm new to TensorFlow, so I'd like to repeat the example step by step.", "Yeah, I confirmed the custom op example is outdated. Sorry for that. tf.Sin op is now a part of the TFLite builtin op set. You can use tf.RandomUniform operator to produce a custom op.\r\n\r\nFor example,\r\n\r\n```\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\r\n  def random_uniform(x):\r\n    return tf.random.uniform([], 0, 10, dtype=tf.float32)\r\n\r\n  converter = tf.lite.TFLiteConverter.from_concrete_functions([random_uniform.get_concrete_function()])\r\n  converter.allow_custom_ops = True\r\n  tflite_model = converter.convert()\r\n```", "Ok, therefore can I convert an entire trained model in a TFLite model with:\r\n\r\n`tf.lite.TFLiteConverter.from_concrete_functions([my_custom_operator.get_concrete_function()])?` \r\n\r\nI mean, shouldn't I try to convert before the trained model somehow, which contains the custom operator that cannot be detected, then use these lines (since the error should appear):\r\n\r\n`converter = tf.lite.TFLiteConverter.from_concrete_functions([my_custom_operator.get_concrete_function()])\r\nconverter.allow_custom_ops = True ` ?\r\n\r\n By doing so, I thought I could obtain this error at the end:\r\n\r\n`Error:\r\nDidn't find custom operator for name 'my custom operator'\r\nRegistration failed.`\r\n\r\n\r\nSorry, I'm a little confused on this point.", "After `Error: Didn't find custom operator for name 'my custom operator' Registration failed.` I'd create and register the custom operator.\r\n", "Custom operators are only used when you really need it. For example, \r\n1) Performance improvements through fusing several ops into one. See https://www.tensorflow.org/lite/convert/operation_fusion This is an advanced technique that needs to understand how MLIR converter works.\r\n2) Custom operator is needed when TFLite builtin operator coverage is not sufficient for the given user's TF model. \r\nFor example, the above random uniform operator is the case. Implementing a custom operator is an option to avoid missing operator errors. However, in such cases, we recommend using the Select TF operator option to fallback TF ops since it does not need any extra operator implementation works. See https://www.tensorflow.org/lite/guide/ops_select", "If the converter does not throw any errors related to custom operators, it succeded on the conversion with the TFLite builtin operator set, which is a good sign.", "Okay, I'll try to follow your suggestions, thank you very much @abattery.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47461, "title": "minimize total loss without step loss", "body": "I want to minimize total loss without step loss (each loss no back-propagation), how to implement it?    \r\ntotal_loss = 0\r\nidx = 0\r\nfor data in train_datas:\r\n       pre = model(data)\r\n       loss = metric(pre, gt)\r\n       total_loss = total_loss + loss\r\n       idx = idx + 1\r\n       if idx %10 == 0:\r\n              train_ops = optimizer(total_loss)\r\n              sess.run(train_ops)", "comments": ["@Saduf2019 \r\n\r\nI want to minimize total loss without step loss (each loss no back-propagation), how to implement it?\r\ntotal_loss = 0\r\nidx = 0\r\nfor data in train_datas:\r\n  pre = model(data)\r\n  loss = metric(pre, gt)\r\n  total_loss = total_loss + loss\r\n  idx = idx + 1\r\n  if idx %10 == 0:\r\n    train_ops = optimizer(total_loss)\r\n    sess.run(train_ops)", "@leiup \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced], the description is not sufficient to analyse.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47460, "title": "unable to train model by using TPU(free TPU)", "body": "I have prepared pipe line to train x-rays image classification model, train dataset is not balanced so did balance it by using resampling methods see the below link for more information:\r\nhttps://www.tensorflow.org/guide/data#resampling\r\n\r\nplease see the error msg I got when fit the model:\r\n_\"\"UnavailableError: 2 root error(s) found.\r\n  (0) Unavailable: Socket closed\r\n  (1) Unavailable: Unable to find a context_id matching the specified one (12465293436922014039). Perhaps the worker was restarted, or the context was GC'd?\r\n0 successful operations.\r\n0 derived errors ignored._\r\n\r\nPlease advise?", "comments": ["@halhwadi,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47460\">No</a>\n"]}]