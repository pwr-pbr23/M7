[{"number": 37246, "title": "Adding support to TensorFlow Lite for more ops", "body": "Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, LOGISTIC, RESHAPE. Here is a list of operators for which you will need custom implementations: TFLite_Detection_PostProcess.", "comments": ["@sharmaninaad,\r\nCan you please share a standalone code to reproduce the issue. Did you try adding converter.allow_custom_ops = True to enable custom ops in the tf.lite? Thanks!", "@sharmaninaad,Let us know if this is still issue. Thanks", "@sharmaninaad, Closing this issue since its resolved. Please feel free to reopen if issue still persists. Thanks! "]}, {"number": 37244, "title": "Added reference papers in docstring", "body": "Added reference papers in docstring for models in `keras.appliactions`. Some models had reference papers, while others did not. Edited the docstrings such that the descriptions would appear consistent throughout all models.", "comments": []}, {"number": 37243, "title": "Problem with segmentation my dataset  in DeepLab", "body": "I adapted the \"build_convert\" of the PASCAL Dataset to convert my dataset to execute the DeepLab, but all my segmentation tests generate a black image prediction. \r\n\r\nMy dataset has images with 140x140 px. (see an example above:)\r\n\r\nOriginal Image\r\n![211789](https://user-images.githubusercontent.com/18166522/75722022-256f2400-5cb8-11ea-9308-0c7637d37cb5.png)\r\n\r\nOriginal Mask\r\n![211789_OriginalMask](https://user-images.githubusercontent.com/18166522/75721512-3d927380-5cb7-11ea-866d-1e16364d073b.png)\r\n\r\nConverted Mask\r\n![211789_ConvertedMask](https://user-images.githubusercontent.com/18166522/75721539-4b47f900-5cb7-11ea-9657-8644aa8eb335.png)\r\n\r\nPrediction Image (complete black, all prediction pixels are background)\r\n![211789_Prediction](https://user-images.githubusercontent.com/18166522/75721593-69155e00-5cb7-11ea-8580-61f163bf611c.png)\r\n\r\nWhat am I doing wrong?\r\nCould anybody help me with this problem?\r\n\r\nThanks\r\n", "comments": ["@rafaelmarconiramos, Can you provide the complete standalone code to replicate the reported issue. Thanks", "My step-by-step:\r\n\r\n1) Convert the segmentation masks\r\n\r\npython ./remove_gt_colormap.py \\\r\n  --original_gt_folder=\"${SEG_FOLDER}\" \\\r\n  --output_dir=\"${SEMANTIC_SEG_FOLDER}\"\r\nSEG_FOLDER -> the original mask images\r\n\r\n2) Create the tfrecord files\r\n(I reused the  PASCAL example)\r\n\r\npython ./build_voc2012_data.py \\\r\n  --image_folder=\"${IMAGE_FOLDER}\" \\\r\n  --semantic_segmentation_folder=\"${SEMANTIC_SEG_FOLDER}\" \\\r\n  --list_folder=\"${LIST_FOLDER}\" \\\r\n  --output_dir=\"${OUTPUT_DIR}\"\r\n\r\n3) To train\r\n\r\nNUM_ITERATIONS=100\r\npython \"${WORK_DIR}\"/train.py \\\r\n  --logtostderr \\\r\n  --train_split=\"val\" \\\r\n  --model_variant=\"${modelo}\" \\\r\n  --atrous_rates=12 --atrous_rates=24 --atrous_rates=36 --output_stride=8 \\\r\n  --decoder_output_stride=4 \\\r\n  --train_crop_size=\"513,513\" \\\r\n  --train_batch_size=4 \\\r\n  --training_number_of_steps=\"${NUM_ITERATIONS}\" \\\r\n  --train_logdir=\"${TRAIN_LOGDIR}\" \\\r\n  --dataset_dir=\"${PASCAL_DATASET}\"\r\n\r\n4) to eval\r\npython \"${WORK_DIR}\"/vis.py \\\r\n  --logtostderr \\\r\n  --vis_split=\"val\" \\\r\n  --model_variant=\"${modelo}\" \\\r\n  --atrous_rates=12 --atrous_rates=24 --atrous_rates=36 --output_stride=8 --decoder_output_stride=4 \\\r\n  --vis_crop_size=\"513,513\" \\\r\n  --checkpoint_dir=\"${TRAIN_LOGDIR}\" \\\r\n  --vis_logdir=\"${VIS_LOGDIR}\" \\\r\n  --dataset_dir=\"${PASCAL_DATASET}\" \\\r\n  --max_number_of_iterations=1\r\n\r\nThanks\r\n\r\n\r\n\r\n", "@rafaelmarconiramos I am closing this issue as this is not related to bug/performance, build/install, feature request or doc related issues. Please post this issue in stackoverflow where there is a wider community to respond. Thanks!"]}, {"number": 37242, "title": "TFLite model not running in spite of using SELECT_TF_OPS for Mean operation", "body": "**System information**\r\n- Android 9/TOCO ran on MAC OS (Tensorflow 2.1.0)\r\n\r\n-  implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n\r\nToco command (with allow custom ops and to use select tf ops), converted with **tensorflow 2.1.0**:\r\n\r\n`toco --input_file=/Users/debasish/Downloads/tfdir/agegendermultitaskspcnn.pb --output_file=/Users/debasish/Downloads/tfdir/agegendermultitaskspcnn.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=input --output_arrays=ageprob,genderprob --input_shapes=1,227,227,3 --mean_values=117 --std_values=1 --allow_custom_ops --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS`\r\n\r\nThe model builds OK with this command. But while running on Android at the below statement, I get following error while creating a interpreter.\r\n\r\n`        tflite = new Interpreter(tfliteModel, tfliteOptions);`\r\n\r\nError Message:\r\n\r\n```\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Encountered unresolved custom op: Mean.\r\n    Node number 4 (Mean) failed to prepare\r\n```\r\n\r\nGraph\r\n![agegendermultitaskspcnn](https://user-images.githubusercontent.com/7953422/75726912-9db20580-5d09-11ea-94e0-c043e5e7673f.png)\r\n\r\n", "comments": ["@ymodak  Any help?", "@haozha111 Any help?", "Can you try using the tf-nightly  (it will use the new TF Lite converter).", "Update: I was able to run the TOCO and obtain new converted tflite files. I am closing this issue.\r\n\r\n@haozha111 I installed latest tf-nightly and tried converting my model. It is giving me error.\r\n\r\n**TOCO Command**\r\n`toco --saved_model_dir=/Users/debasish/Downloads/tfdir/ --graph_def_file=/Users/debasish/Downloads/tfdir/agegendermultitaskdpcnn_2_1.pb --output_format=TFLITE --inference_type=FLOAT --inference_input_type=FLOAT --output_file=/Users/debasish/Downloads/tfdir/agegendermultitaskdpcnn_2_1.tflite --output_format=TFLITE --input_arrays=input --output_arrays=ageprob,genderprob --input_shapes=1,227,227,3 --mean_values=117 --std_dev_values=1 --allow_custom_ops --target_ops=TFLITE_BUILTINS,SELECT_TF_OPS --experimental_new_converter`\r\n\r\n**Error**\r\n```\r\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\r\nI0307 01:01:41.570394 4577048000 saver.py:1512] Saver not created because there are no variables in the graph to restore\r\n2020-03-07 01:01:41.596349: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe0ce6f6e30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-07 01:01:41.596378: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/toco\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 638, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/debasish/Library/Python/3.7/lib/python/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/debasish/Library/Python/3.7/lib/python/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 621, in run_main\r\n    _convert_tf2_model(tflite_flags)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 237, in _convert_tf2_model\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 483, in convert\r\n    raise ValueError(\"This converter can only convert a single \"\r\nValueError: This converter can only convert a single ConcreteFunction. Converting multiple functions is under development.\r\n```\r\n"]}, {"number": 37241, "title": "Using different version of tensorRT?", "body": "I wasn't sure what type of issue this was, so I'm trying others.\r\n\r\nHi. I'm trying to run tensorflow on a gpu server.\r\n\r\nI don't have access rights to installing(changing) packages on the server, and the server has TensorRT-5.1.5 version installed.\r\nBut I'm assuming when I import tensorflow 2.0.0, it's failing to find TensorRT v6, and just using cpu instead. Is there a way for the tensorflow to use a different version of TensorRT?\r\n\r\nThese are the errors I'm getting;\r\n\r\n2020-03-02 10:22:52.294562: W tensorflow/stream_executor/platform/default/dso_loader.cc:55<http://dso_loader.cc:55>] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /N/soft/rhel7/deeplearning/Python-3.7.6/lib:/N/soft/rhel7/deeplearning/Python-3.7.6/cuda/lib64:/N/soft/rhel7/deeplearning/TensorRT-5.1.5.0/lib:/N/soft/rhel7/deeplearning/gcc-7.4.0/lib:/N/soft/rhel7/deeplearning/gcc-7.4.0/lib64:/N/soft/rhel7/deeplearning/cuda/lib64:/N/soft/rhel7/deeplearning/Python-3.6.8/lib:/N/soft/rhel7/deeplearning/Python-3.6.8/pytorch/torch/lib64:/N/soft/rhel7/deeplearning/Python-3.6.8/pytorch/torch/lib:/N/soft/rhel7/cuda/10.0/lib64:/N/soft/rhel7/perl/gnu/5.24.1/lib:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/compiler/lib/intel64:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/ipp/lib/intel64:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/compiler/lib/intel64_lin:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/mkl/lib/intel64_lin:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/tbb/lib/intel64/gcc4.7:/N/soft/rhel7/intel/18.0.2/debugger_2018/iga/lib:/N/soft/rhel7/intel/18.0.2/debugger_2018/libipt/intel64/lib:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/daal/lib/intel64_lin\r\n   2020-03-02 10:22:52.295671: W tensorflow/stream_executor/platform/default/dso_loader.cc:55<http://dso_loader.cc:55>] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /N/soft/rhel7/deeplearning/Python-3.7.6/lib:/N/soft/rhel7/deeplearning/Python-3.7.6/cuda/lib64:/N/soft/rhel7/deeplearning/TensorRT-5.1.5.0/lib:/N/soft/rhel7/deeplearning/gcc-7.4.0/lib:/N/soft/rhel7/deeplearning/gcc-7.4.0/lib64:/N/soft/rhel7/deeplearning/cuda/lib64:/N/soft/rhel7/deeplearning/Python-3.6.8/lib:/N/soft/rhel7/deeplearning/Python-3.6.8/pytorch/torch/lib64:/N/soft/rhel7/deeplearning/Python-3.6.8/pytorch/torch/lib:/N/soft/rhel7/cuda/10.0/lib64:/N/soft/rhel7/perl/gnu/5.24.1/lib:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/compiler/lib/intel64:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/ipp/lib/intel64:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/compiler/lib/intel64_lin:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/mkl/lib/intel64_lin:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/tbb/lib/intel64/gcc4.7:/N/soft/rhel7/intel/18.0.2/debugger_2018/iga/lib:/N/soft/rhel7/intel/18.0.2/debugger_2018/libipt/intel64/lib:/N/soft/rhel7/intel/18.0.2/compilers_and_libraries_2018.2.199/linux/daal/lib/intel64_lin\r\n   2020-03-02 10:22:52.296073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30<http://py_utils.cc:30>] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.", "comments": ["I just realized tensorflow 2.0.0 only support TensorRT v6. I'll just have to inform them to install it. Thank you though!", "> I just realized tensorflow 2.0.0 only support TensorRT v6.\r\n\r\n@pjsjongsung , Where did you find such information?"]}, {"number": 37240, "title": "How to deserialize from a dict with tf.keras.losses.get", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses/get\r\n\r\n## Description of issue (what needs changing):\r\nCurrently there is no documentation at all.\r\nIts fairly straightforward to use by inputting a string denoting default class name:\r\nex:\r\n```\r\nidentifier = \"categorical_crossentropy\"\r\ntf.keras.losses.get(identifier)\r\n```\r\n\r\nHowever, I am having issues with dictionary objects:\r\nex:\r\n```\r\nidentifier = {\"class_name\":\"categorical_crossentropy\",\"config\":{\"from_logits\":True}}\r\ntf.keras.losses.get(identifier)\r\n```\r\nReturns:\r\n```\r\nTraceback (most recent call last):\r\n  File \".\\main.py\", line 85, in <module>\r\n    loss = tf.keras.losses.get(jsn)\r\n  File \"C:\\Users\\jopatterson\\Documents\\autoprime-ml\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\", line 1186, in get\r\n    return deserialize(identifier)\r\n  File \"C:\\Users\\jopatterson\\Documents\\autoprime-ml\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\", line 1175, in deserialize\r\n    printable_module_name='loss function')\r\n  File \"C:\\Users\\jopatterson\\Documents\\autoprime-ml\\env\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\generic_utils.py\", line 315, in deserialize_keras_object\r\n    return cls(**cls_config)\r\nTypeError: categorical_crossentropy() missing 2 required positional arguments: 'y_true' and 'y_pred'\r\n```\r\nI believe it is failing because cls is the already initialized loss function, and it is passing cls_config as its input, rather than using them as parameters during initialization.\r\n\r\n### Clear description\r\n\r\nThis is a very useful method for abstract implementations of loss objects.\r\n\r\n### Correct links\r\n\r\nThis is where the issue is occuring, within the `deserialize_keras_object` function:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py#L382\r\n\r\n### Parameters defined\r\n\r\nThere currently is no documentation for this, as `identifier` can be a string, dictionary or callable.\r\n\r\n### Returns defined\r\n\r\nReturns are not defined, but its fairly obvious it returns a loss function.\r\n\r\n### Raises listed and defined\r\n\r\nNo.\r\n\r\n### Usage example\r\n\r\nNo.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo.\r\n\r\n### Submit a pull request?\r\n\r\nI would do this if I had enough knowledge to do so. Unfortunately I only know how it works with `identifier` as a String.", "comments": ["@jpatts, Documentation is available for tf-nightly version. Please take a look at doc [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/get?version=nightly) for more about `identifier` and `returns`. Thanks!", "@gadagashwini, I saw the nightly documentation after this post, however there is still no explanation for what a \u201closs configuration dictionary\u201d is. That\u2019s the larger problem for this. The way I tried to do it is the intuitive way, but it doesn\u2019t work for the reasons I showed above, so I would like to know how it\u2019s supposed to be used.", "@jpatts would you mind to elaborate what exactly are you trying to do?\r\n\r\nthere are some examples how to serialize custom objects and functions \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils_test.py#L146", "@lc0 I gave an example at the top of this issue showing what I\u2019m trying to do. To use a configuration dictionary, I need two mandatory JSON parameters, \u201cclass_name\u201d and \u201cconfig\u201d. This can be seen in [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/generic_utils.py#L308) code section:\r\n```\r\ndef class_and_config_for_serialized_keras_object(\r\n    config,\r\n    module_objects=None,\r\n    custom_objects=None,\r\n    printable_module_name='object'):\r\n  \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\r\n  if (not isinstance(config, dict) or 'class_name' not in config or\r\n      'config' not in config):\r\n    raise ValueError('Improper config format: ' + str(config))\r\n```\r\nHowever, when I do this, the \u201cclass_name\u201d is used for the initialization and \u201cconfig\u201d is used as function inputs. The \u201cconfig\u201d should be setting \u201cclass_name\u201d parameter options during function initialization.", "This is the block of code where this is happening.\r\n```\r\nif isinstance(identifier, dict):\r\n    # In this case we are dealing with a Keras config dictionary.\r\n    config = identifier\r\n    (cls, cls_config) = class_and_config_for_serialized_keras_object(\r\n        config, module_objects, custom_objects, printable_module_name)\r\n\r\n    if hasattr(cls, 'from_config'):\r\n      arg_spec = tf_inspect.getfullargspec(cls.from_config)\r\n      custom_objects = custom_objects or {}\r\n\r\n      if 'custom_objects' in arg_spec.args:\r\n        return cls.from_config(\r\n            cls_config,\r\n            custom_objects=dict(\r\n                list(_GLOBAL_CUSTOM_OBJECTS.items()) +\r\n                list(custom_objects.items())))\r\n      with CustomObjectScope(custom_objects):\r\n        return cls.from_config(cls_config)\r\n    else:\r\n      # Then `cls` may be a function returning a class.\r\n      # in this case by convention `config` holds\r\n      # the kwargs of the function.\r\n      custom_objects = custom_objects or {}\r\n      with CustomObjectScope(custom_objects):\r\n        return cls(**cls_config)\r\n```\r\nThe way I send the dictionary, I am being put down the \"config holds the kwargs of the function\" path. I think I want to be going down the \"from_config\" path, but don't know how to do so.", "[This](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/losses.py#L148) is the \"from_config\" method for the loss class.\r\n```\r\n@classmethod\r\n  def from_config(cls, config):\r\n    \"\"\"Instantiates a `Loss` from its config (output of `get_config()`).\r\n    Args:\r\n        config: Output of `get_config()`.\r\n    Returns:\r\n        A `Loss` instance.\r\n    \"\"\"\r\n    return cls(**config)\r\n\r\n  def get_config(self):\r\n    \"\"\"Returns the config dictionary for a `Loss` instance.\"\"\"\r\n    return {'reduction': self.reduction, 'name': self.name}\r\n```\r\nThis is confusing, as `cls(**cls_config)` and `cls.from_config(cls_config)` end up being the same thing...", "Bump. I\u2019m pretty sure that the issues I\u2019m having are from the loss functions not actually having settable parameters. IE you can\u2019t set \u201cfrom_logits\u201d during initialization, it needs to be set every time it is executed. So in this case, \u201cconfig\u201d is literally just calling the function with it\u2019s supplied parameters. This makes me question why this is a feature... ", "I can explain why this doesn't work -- when you pass a dict, the class name should be the actual name of the class.  `categorical_crossentropy` is a function, and not a class. I think the documentation should be improved here. \r\n\r\n@pavithrasv  Should we modify `losses.get` to return partial functions, if the class name is the name of a function?", "Thank you very much for the clarification @k-w-w.\r\nSo this returns a function:\r\n```\r\nloss = tf.keras.losses.get(\"categorical_crossentropy\")\r\nprint(loss)\r\n\r\n<function categorical_crossentropy at 0x000002886AFC7318>\r\n```\r\nAnd this returns the class.\r\n```\r\njsn = {\"class_name\":\"CategoricalCrossentropy\", \r\n       \"config\": {\r\n            \"from_logits\": True\r\n        }\r\n}\r\nloss = tf.keras.losses.get(jsn)\r\nprint(loss)\r\n\r\n<tensorflow.python.keras.losses.CategoricalCrossentropy object at 0x0000018D9E219CC8>\r\n```\r\nThis solves my problem completely. I agree that the documentation should be updated, as this was very confusing. ", "@jpatts and @k-w-w , please take a look at PR #37538 and suggest more changes if needed. I have added descriptions and examples which will clarify all doubts regarding `tf.keras.losses.get`", "Thank you for the response @k-w-w and the quick PR @ashutosh1919 .\r\n\r\n@k-w-w Yes, we can look into returning partial functions if class name is the name of a function. From the top of my head i think `deserialize_keras_object` may need to be modified for that."]}, {"number": 42781, "title": "Community feedback for translated landing pages", "body": "Hello, I was wondering if the TensorFlow tutorial [landing page](https://www.tensorflow.org/tutorials?hl=ko) should be translated by the community. Though I cannot be certain, the page seems to be built from [this source](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/_index.yaml), which is in yaml format. After checking out the TensorFlow website under different language options, namely Korean, Chinese, and Japanese, I found out that none of the languages had a translated landing page. Should this be included in the to-do list of things, or is it somehow handled differently or internally? Thank you.\r\n\r\n--------------------\r\n\r\nOn a similar note, [`_toc.yaml`](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/_toc.yaml) is currently not being translated under any language. It seems like the website is internally/automatically translating it, but some of the translated words remain inconsistent with those used in the actual translated documents. ", "comments": ["Hi @jaketae \r\n\r\nSee the do not translate section here: https://github.com/tensorflow/docs-l10n#do-not-translate\r\nFor ko, ja, and zh-cn, we use a different system to translate landing pages and leftnav files. These files should not be translated directly by the community.\r\n\r\nTo resolve inconsistencies, I could send along a list of fixes to use as a reference\u2014similiar to this section: https://github.com/tensorflow/docs-l10n/tree/master/site/ko#korean-translation-guide\r\nThat said, getting these changes in sync is challenging since they use their own systems.", "@lamberta Thank you for pointing me to the links. I now realize that these files are not to be translated by the community and that the website is built using its own system. I was just wondering, however, if it would be possible to improve the translation of the website in general. For instance, the [overview page](https://www.tensorflow.org/overview?hl=ko) of the Korean website translates \"Tutorials\" and \"Guide\" both as \"\uac00\uc774\ub4dc,\" which can be very confusing for site navigation purposes (\"See guide\" and \"See tutorials\" both appear as \"\uac00\uc774\ub4dc \ubcf4\uae30\"). \r\n\r\nBut if the fix to this is complicated, maybe we should wait until better channels are established? I'd still be happy to contribute if you have plans for addressing this agenda, whether it be building a reference list or any other approach for that matter.\r\n\r\nThis isn't a dire problem though, so keep us informed if anything pops up :) ", "Yeah, it's definitely something we can work on.\r\nTo start, we could create a public spreadsheet that is maintained by the community\u2014then can point the internal folks to that for new requests.\r\n\r\nMaybe this is something you can suggest on the docs-ko@tensorflow.org list?\r\n\r\ncc: @rickiepark ", "Sounds great! I'd be more than happy to help, but that seems more like a communal initiave that @rickiepark would be fit to lead. At the very least, I'm thinking I can wait for his input on this to organize the logistics. We will get something started on docs-ko@tensorflow.org and keep you in the loop!", "Thanks @jaketae and @lamberta \r\nThere are a spreadsheet for this(https://docs.google.com/spreadsheets/d/1cuvihE7Xm3HC2v3kvWemmMpps606gLZ2PwaPwsIPlCU/).\r\nBut we need more LGTM in that sheet. :(", "@rickiepark I wasn't aware of this spreadsheet! Thank you for sharing. \r\n\r\n> To start, we could create a public spreadsheet that is maintained by the community\u2014then can point the internal folks to that for new requests.\r\n\r\n@lamberta Seems like we already had a public spreadsheet in the making, although it looks like some updating would be necessary. In this case, what would the workflow look like?", "Another thing to keep in mind is since we're now in this new translation-focused GitHub repo, we can use things like [GitHub Projects](https://github.com/features/project-management/), if we want.\r\n\r\nFor my purposes, I just need a URL to attach the job. Even still, they don't do the best job \ud83d\ude12 One reason why the community asked to translate themselves (!)\r\n\r\n", "Hi all, sorry for the delay but lots of developments :)\r\n\r\nBeen working with GitLocalize and will be directing most community translators to use the UI there: https://gitlocalize.com/tensorflow/docs-l10n (it's very nice and now has notebook support)\r\n\r\nWe're also working with some vendors to improve coverage---but it's still very important for the community to provide guidance since the community is more familiar with technical terms for ML developer docs. As part of this work with GitLocalize, we now have [a translation glossary](https://docs.google.com/spreadsheets/d/1Q1_jOgzWzVNpd4m_uz37Gbk1Sx7hAdYbhku285-k7Ks/preview) that we can collaborate on. I now need to create a process for the community to submit updates/fixes to this glossary. (This glossary is built into the GitLocalize UI.)\r\n\r\nI'll also use this glossary as a reference for the vendors who translate the landing pages and navigation using the internal process.", "Sounds awesome, thank you so much for the update! I'm not familiar with GitLocalize, but I'd be curious to know what the community contribution portion of the workflow is going to look like given the revised setup. Is there some standard protocol that community translators should adhere to when suggesting an update to the glossary (which seems to be a Google doc, potentially generated through GitLocalize)? Also, would it be possible to integrate the KO-glossary on GitLocalize with the [docs](https://docs.google.com/spreadsheets/d/1cuvihE7Xm3HC2v3kvWemmMpps606gLZ2PwaPwsIPlCU/) the community has been working on so far?", "Hi Can I contribute by translate in korean? If there's any task, I think I can do that! Please answer back.", "Hi @hibamtol1 , thanks for volunteering.\r\nCommunity contributions can be made to this repo: https://github.com/tensorflow/docs-l10n\r\nPlease use the GitLocalize project to make sure your contribution is reviewed: https://gitlocalize.com/tensorflow/docs-l10n\r\nThanks"]}, {"number": 37239, "title": "tf.keras.experimental.WideDeepModel example has wrong input for constructor", "body": "The tutorial example for tf.keras.experimental.WideDeepModel instantiated the class with first input augment: dnn_model, and second augment: linear_model as shown in :\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/experimental/WideDeepModel#example_4\r\n\r\nHowever, the class should be instantiated by linear_model then dnn_model as shown in:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/premade/wide_deep.py#L72\r\n\r\nPlease update the tutorial example.\r\n", "comments": ["@tobillsung-doc Do you want to raise a PR to update the docs? Thanks!", "@jvishnuvardhan Yes, please. Thank you."]}, {"number": 37238, "title": "Docs say that sigmoid is mapped to TfLite but the TfLite schema doesn't mention sigmoid", "body": "It lists ```tf.sigmoid``` as mappable to TfLite here: https://www.tensorflow.org/lite/guide/ops_compatibility\r\n\r\nBut the TfLite schema doesn't mention sigmoid: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\r\n", "comments": ["It is referenced as Logistic in the schema", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37238\">No</a>\n"]}, {"number": 37237, "title": "steps_per_epoch not propagating to Keras callbacks in tf-nightly", "body": "This behavior was tested in `tf-nightly==2.2.0.dev20200302` on macOS.\r\n\r\nSome of our Keras callbacks rely on accessing `self.params['steps']` in order to take actions based on how far along the process is into the current epoch (e.g., learning rate schedule).  However, it appears that this param is no longer being set properly, even when `mode.fit(steps_per_epoch=...)` is called.\r\n\r\nI did some digging, and it looks like the issue can be traced to the way `CallbackList(steps=...)` is being set [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L753).  The `DataAdapter` object is an instance of `DatasetAdapter`, which is initialized with the `steps_per_epoch` param and assigns it to `self._user_steps`, but the `get_size()` method always returns `None` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/data_adapter.py#L704).\r\n\r\nIs there a workaround for this?  Ideally, we'd like for callbacks to have access to the `steps_per_epoch` information without having to pass it in to each callback manually.  \r\n\r\nYou can see an example of how we use this in Horovod in the LearningRateWarmupCallback from the [tensorflow2_keras_mnist.py](https://github.com/horovod/horovod/blob/master/examples/tensorflow2_keras_mnist.py#L77) example.  As you can see in the implementation of that callback [here](https://github.com/horovod/horovod/blob/cff4de331edcd6cabbd807b7010e7432b03fe2b7/horovod/_keras/callbacks.py#L108), we are attempting to access `self.params.get('step')`, which used to get set correctly prior to v2.2.\r\n\r\ncc @reedwm @pkanwar23 ", "comments": ["Thanks for the issue! Working on a fix now", "Hey @omalleyt12, any update on this issue?", "@tgaddair Yes this should be fixed in the latest nightly\r\n\r\nAlso have a cherrypick out for the 2.2 release branch: https://github.com/tensorflow/tensorflow/pull/37314", "Thanks!  I'll try it out today and get back to you.", "Sounds good, thanks!", "Looks like this issue is resolved.  Thanks for the fix @omalleyt12!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37237\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37237\">No</a>\n", "Great, no prob, thanks for verifying!"]}, {"number": 37236, "title": "Android", "body": "@tomergafner @nathansilberman @AlonDaks", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37236) for more info**.\n\n<!-- need_author_consent -->", "Please reopen a new PR , looks like your fork is not up to date."]}, {"number": 37235, "title": "Impossible to install tensorflow on linux from PyPI with pip versions prior to 19", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (but affects any linux)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary): PyPI\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.6.10\r\n- Installed using virtualenv? pip? conda?: pip version less than 19\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTensorflow 1.15.2 only provides a `manylinux2010` build on [pypi](https://pypi.org/project/tensorflow/1.15.2/#files) and so can't be built with pip versions older than 19. This means it can't be installed it under certain circumstances where pip can't be upgraded; for example using a managed service such as GCP Composer.\r\n\r\nThis version of manylinux was made usable by pip in [version 19.0](https://pip.pypa.io/en/stable/news/#id209). As far as I'm aware though, pip still supports `manylinux1`, so I would assume it's reasonable to support this for older installs.\r\n\r\nWould it be possible to provide a `manylinux1` build on PyPI to circumvent this issue please? I would rather not have to install a version below 1.15.2 as they have a [high severity security issue](https://github.com/advisories/GHSA-977j-xj7q-2jr9)\r\n\r\nMy assumption is that it's easy to provide this build given the previous minor version used `manylinux1`. Apologies if this is incorrect and there are good reasons not to support this; I couldn't find any relevant issues which addressed this problem so figured it worth raising.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nTrivially reproducible on any linux within a virtualenv. With pip 18.1:\r\n\r\n```\r\n$ pip --version\r\npip 18.1 from /tmp/tensorflow/pip_18_1/lib/python3.6/site-packages/pip (python 3.6)\r\n$ python --version\r\nPython 3.6.10\r\n$ pip install tensorflow==1.15.2\r\nCollecting tensorflow==1.15.2\r\n  Could not find a version that satisfies the requirement tensorflow==1.15.2 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==1.15.2\r\nYou are using pip version 18.1, however version 20.0.2 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```\r\n\r\nWith version 19.0 (i.e. what I would expect):\r\n\r\n```\r\n$ pip --version\r\npip 19.0 from /tmp/tensorflow/pip_19_0/lib/python3.6/site-packages/pip (python 3.6)\r\n$ python --version\r\nPython 3.6.10\r\n$ pip install tensorflow==1.15.2\r\nCollecting tensorflow==1.15.2\r\n  Using cached https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl\r\nCollecting numpy<2.0,>=1.16.0 (from tensorflow==1.15.2)\r\n  Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.2MB 3.2MB/s \r\nCollecting six>=1.10.0 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.6.1 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl\r\nCollecting gast==0.2.2 (from tensorflow==1.15.2)\r\nCollecting absl-py>=0.7.0 (from tensorflow==1.15.2)\r\nCollecting termcolor>=1.1.0 (from tensorflow==1.15.2)\r\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in ./pip_19_0/lib/python3.6/site-packages (from tensorflow==1.15.2) (0.34.2)\r\nCollecting astor>=0.6.0 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\r\nCollecting tensorboard<1.16.0,>=1.15.0 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\r\nCollecting opt-einsum>=2.3.2 (from tensorflow==1.15.2)\r\nCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\r\nCollecting wrapt>=1.11.1 (from tensorflow==1.15.2)\r\nCollecting grpcio>=1.8.6 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/28/df/1f8a284a5e5819ae07d50bd76996d6f7208afef7533e4896fa1c6445574f/grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl\r\nCollecting tensorflow-estimator==1.15.1 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\r\nCollecting google-pasta>=0.1.6 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/c3/fd/1e86bc4837cc9a3a5faf3db9b1854aa04ad35b5f381f9648fbe81a6f94e4/google_pasta-0.1.8-py3-none-any.whl\r\nCollecting keras-applications>=1.0.8 (from tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\r\nRequirement already satisfied: setuptools in ./pip_19_0/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (45.2.0)\r\nCollecting werkzeug>=0.11.15 (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/ba/a5/d6f8a6e71f15364d35678a4ec8a0186f980b3bd2545f40ad51dd26a87fb1/Werkzeug-1.0.0-py2.py3-none-any.whl\r\nCollecting markdown>=2.6.8 (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2)\r\n  Downloading https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl (88kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 7.9MB/s \r\nCollecting h5py (from keras-applications>=1.0.8->tensorflow==1.15.2)\r\n  Using cached https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl\r\nInstalling collected packages: numpy, six, protobuf, gast, absl-py, termcolor, astor, grpcio, werkzeug, markdown, tensorboard, opt-einsum, keras-preprocessing, wrapt, tensorflow-estimator, google-pasta, h5py, keras-applications, tensorflow\r\nSuccessfully installed absl-py-0.9.0 astor-0.8.1 gast-0.2.2 google-pasta-0.1.8 grpcio-1.27.2 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.2.1 numpy-1.18.1 opt-einsum-3.1.0 protobuf-3.11.3 six-1.14.0 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1 termcolor-1.1.0 werkzeug-1.0.0 wrapt-1.12.0\r\nYou are using pip version 19.0, however version 20.0.2 is available.\r\nYou should consider upgrading via the 'pip install --upgrade pip' command.\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@seddy \r\nPlease upgrade pip to resolve the issue:\r\npython -m pip install --upgrade pip setuptools\r\n\r\nyou may check these links for reference [Link1](https://forum.rasa.com/t/could-not-find-a-version-that-satisfies-the-requirement-tensorflow-1-15-0/21509) and [Link2](https://github.com/tensorflow/tensorflow/issues/34302)", "Hi @Saduf2019, thanks for your response.\r\n\r\nAs per the description, I am aware of the fact this works with pip >= 19.0. The problem is there are environments where upgrading pip is not possible, so I'm requesting a package of tensorflow on PyPI built with `manylinux1` which would resolve this issue.", "Hi @seddy unfortunately this is not realistically supportable. `manylinux1` packages require being built on a system or toolchain on [CentOS 5](https://www.python.org/dev/peps/pep-0513/#id40).\r\n\r\nWe did a lot of work to make packages `manylinux2010` compliant and that packaging protocol is based off an OS that is almost 10 years old (CentOS 6). We are in talks with Python and the externally community on eventually adopting `manylinux2014`. \r\n\r\nAdditionally we want to abide by pypi's packaging standards so we do not want to improperly tag a package just to make it available via an older pip version.\r\n\r\nSorry for the trouble. \r\n\r\nAs an unofficial alternative I'm curious if downloading the whl and direct installing it or modifying the name might work.\r\n\r\nUPDATE: Clarifying my comment on the age of manylinux2010.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37235\">No</a>\n", "Thanks @av8ramit, that makes total sense. I hadn't realised quite how out of date `manylinux1` was that it relied on CentOS 5 :scream_cat:. Not supporting that seems reasonable, as does avoiding improper tagging.\r\n\r\nThanks for the suggestion about trying direct installing; I'll bear that in mind when upgrading tensorflow becomes a priority for us, as I think we can get away without upgrading for a little while longer.", "No problem @seddy I'm glad I could help. Good luck!", "@seddy @av8ramit are there any known gotchas for doing the renaming hack? What kind of tests can we do to make everything works?", "There are no known gotchas to me, but I personally have not tried it myself.", "@av8ramit note that the name \"manylinux2010\" does not imply it is a 10 year old specification -- it has only been in pip since [version 19.0](https://pip.pypa.io/en/stable/news/#id203) (2019-01-22).  the name \"2010\" is indicating the target machine on which it will be installed, essentially \"this should work on any linux distribution released since 2010\"\r\n\r\nmeaning that any pip older than ~a year ago will be unable to install this package", "@thegautam if you only change the tag part of the name there are no caveats. I have done it many times and it worked.\r\n\r\nIf you change the rest of the name then you also need to change internal contents (at least the name in severla places but this also causes some hashes to change)", "@asottile the age of the specification was not what I was trying to say, I meant to say that it is based off of an OS that was already 10 years old, which is why it was difficult to comply with it. I've updated my comments if that helps. "]}, {"number": 37234, "title": "Cloning models loaded from disk (SavedModel) fails", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen loading a simple Keras Sequential model with two Conv2D layers stored in SavedModel format from disk and applying the `tensorflow.keras.models.clone_model()` function, a `TypeError: ('Keyword argument not understood:', 'filters')` is thrown.\r\n\r\n**Describe the expected behavior**\r\nI expect the `tensorflow.keras.models.clone_model()` to return a clone of the provided model and not fail with an exception.\r\n\r\n**Standalone code to reproduce the issue** \r\nhttps://colab.research.google.com/gist/soyers/23ec2b19cffd1e2170cbbf2db72aad6d/cloning-models-loaded-from-disk.ipynb\r\n", "comments": ["@soyers,\r\nI was able to reproduce the issue with [TF 2.0](https://colab.sandbox.google.com/gist/amahendrakar/0db9995e445fc5de96f71f4703d0d403/37234.ipynb) and TF 2.1. However, the issue seems to be fixed in [TF-nightly](https://colab.sandbox.google.com/gist/amahendrakar/e3fdab8e1499358e2acc50445a8cef78/37234-2-1.ipynb). Please check the attached gist. Thanks!", "@amahendrakar Thank you very much for investigating this issue! I am gald that the nightly version has already introduced a fix.\r\nI just checked if TF 2.0.1 also contains the fix but it doesn't seem to. Will there be a patch version for TF 2.0 and TF 2.1 some time containig the fix?", "@soyers,\r\nIf the issue is fixed in the TF-nightly version, we can expect it in the next available stable release. Thanks!", "Thanks for the information and for looking into this!\r\nClosing this issue since the bug will be fixed in the next release.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37234\">No</a>\n"]}, {"number": 37233, "title": "Blas GEMM launch failed", "body": "- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source and pip\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.1 / 7.6.4\r\n- GPU model and memory: GTX 1080\r\n\r\nOn my setup any installation using CUDA 10.1 results in the given error:\r\n\r\n    tensorflow.python.framework.errors_impl.InternalError:  Blas GEMM launch failed\r\n\r\nI have the same installation on my own computer with a 2080ti, which runs just fine. Also using tensorflow-gpu==2.0.0 works fine. As some mentioned in stackoverflow  (https://stackoverflow.com/questions/58428466/tensorflow-blas-gemm-launch-failed-a-shape-2-128-b-shape-128-44-m-2-n) I just compiled from source with 10.0 and everything runs fine, too. Any ideas on how to compile with 10.1 and TF2.1?\r\n\r\nMight be related to #36781\r\n\r\nReproducible with:\r\n\r\n    import tensorflow as tf\r\n    print(tf.matmul([[1., 2.],[3., 4.]], [[1., 2.],[3., 4.]]))\r\n\r\nThanks in advance!\r\n\r\n", "comments": ["Check `apt-cache policy libcublas10`. Don't use 10.2.2.89-1 - it is not compatible!  So fix could be:\r\n```\r\napt-get purge libcublas10 libcublas-dev\r\napt-get install libcublas10=10.2.1.243-1 libcublas-dev=10.2.1.243-1 cuda-libraries-10-1 cuda-libraries-dev-10-1\r\n```\r\nIf you did not have installed the cuda-libraries-[dev-]10-1 meta packages, omit them.", "Thanks @jelmd, this resolves the issue!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37233\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37233\">No</a>\n", "> Check `apt-cache policy libcublas10`. Don't use 10.2.2.89-1 - it is not compatible! So fix could be:\r\n> \r\n> ```\r\n> apt-get purge libcublas10 libcublas-dev\r\n> apt-get install libcublas10=10.2.1.243-1 libcublas-dev=10.2.1.243-1 cuda-libraries-10-1 cuda-libraries-dev-10-1\r\n> ```\r\n> \r\n> If you did not have installed the cuda-libraries-[dev-]10-1 meta packages, omit them.\r\n\r\nThanks a lot, It works!!!\ud83d\ude04"]}, {"number": 37232, "title": "What does BuiltinOperator Resolver does ? what is custom ops in resolver.", "body": "I was trying to run Inference on Tflite Models. I want to know what BuiltinOperator Resolver does ? what are the custom ops in resolver operator", "comments": ["@pranathibl \r\n\r\nCan you please go through[ link1](https://www.tensorflow.org/lite/api_docs/cc/class/tflite/op-resolver), [link2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/all_ops_resolver.cc) and see if it helps you.This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n\r\n\r\n![opresolver](https://user-images.githubusercontent.com/51902062/75747876-16ce4e80-5d44-11ea-9026-927ff62bfee5.png).\r\n\r\n\r\n", "Hi ravikyram, \r\n\r\nThanks for the information provided.\r\n\r\nCould you please share the book/pdf where you took the screenshot. I would like to have better understanding of classes/methods used in tensorFlow lite. That would be very helpful.\r\n\r\nThanks,", "@pranathibl \r\nPlease, find the pdf link [here](https://books.google.co.in/books?id=tH3EDwAAQBAJ&pg=PT470&dq=what+is+custom+ops+in+resolver&hl=en&sa=X&ved=0ahUKEwjpyL3Tzf3nAhVI7HMBHSpNAIQQ6AEIKjAA#v=onepage&q=what%20is%20custom%20ops%20in%20resolver&f=true).Please, close this thread if it solves your question. Thanks!", "Hi ravikyram,\r\n\r\nThanks for sharing the pdf. \r\n\r\nAs per my understanding, opResolver fetches the operations implemtations needed to run Inference on model without fetching all function implementations. Let me know I am wrong ?\r\n\r\nI didn't understand what does custom op resolver actually mean ? Will the user do any modifications to Model ? \r\nPlease provide details on what custom resolver does ?\r\n\r\nOnce I got clarity on this, I would close this thread ?", "@pranathibl \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Thanks for the info"]}, {"number": 37231, "title": "Build TF 1.8 from source on Windows not working", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1 64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 1.8\r\n- TensorFlow version: 1.8\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip & conda\r\n- Bazel version (if compiling from source): 2.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0 7.1\r\n- GPU model and memory: NVIDIA GeForce GTX 760 compting capabilities 3.0 that's the reason why...\r\n\r\n\r\n\r\n**Describe the problem** Hello, I've followed the instructions on this page: https://www.tensorflow.org/install/source_windows#gpu\r\nAnd when I enter bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nthe result is: \r\nPS P:\\anaconda3\\tensorflow>\r\nPS P:\\anaconda3\\tensorflow> bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one o\r\nf the standard rc files:\r\np:\\anaconda3\\tensorflow/tools/bazel.rc\r\nWARNING: Ignoring JAVA_HOME, because it must point to a JDK, not a JRE.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=120\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=p:/Anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from p:\\anaconda3\\tensorflow\\bazel.rc:\r\n  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_proto\r\ns=true --define=grpc_no_ares=true --spawn_strategy=standalone --genrule_strategy=standalone -c opt\r\nINFO: Reading rc options for 'build' from p:\\anaconda3\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=p:/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=p:/Anaconda3/lib/si\r\nte-packages --force_python=py3 --host_force_python=py3 --python_path=p:/Anaconda3/python.exe --define with_xla_support=t\r\nrue --define with_gdr_support=true --define with_verbs_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_N\r\nEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1 --action_env TF_CUDA_V\r\nERSION=9.1 --action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1 --action_env TF_CUDNN\r\n_VERSION=7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --action_env TF_CUDA_CLANG=0 --action_env CUDA_PATH=C:/Program\r\nFiles/NVIDIA GPU Computing Toolkit/CUDA/v9.1 --action_env CUDA_COMPUTE_CAPABILITIE=None --action_env NO_WHOLE_ARCHIVE_OP\r\nTION=1 --config=win-cuda --define grpc_no_ares=true --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK --host_copt=-DGEMMLOWP_\r\nALLOW_SLOW_SCALAR_FALLBACK --config monolithic --copt=-w --host_copt=-w --verbose_failures\r\nINFO: Found applicable config definition build:win-cuda in file p:\\anaconda3\\tensorflow\\bazel.rc: --define=using_cuda=tr\r\nue --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:monolithic in file p:\\anaconda3\\tensorflow\\bazel.rc: --define framework_s\r\nhared_object=false\r\nERROR: Config value v1 is not defined in any .rc file\r\nPS P:\\anaconda3\\tensorflow>\r\n\r\n\r\n\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem** bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nThanks for your help!\r\nBest regards,\r\nFran\u00e7ois\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@amahendrakar Hello, I've not read this doc enough, my bazel version was wrong. Now, I have the 0.10.0rc. Things are different but not better ...\r\nThanks for your help!\r\nPS P:\\anaconda3\\tensorflow> bazel build --config=v1 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: Config values are not defined in any .rc file: v1\r\nLoading:\r\nLoading: 0 packages loaded\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package'\r\n: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Trac\r\neback (most recent call last):\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 747, in _get_cuda_config\r\n                _cuda_toolkit_path(repository_ctx)\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 282, in _cuda_toolkit_path\r\n                repository_ctx.path(cuda_toolkit_path).exists\r\nIllegal char <:> at index 90: C:/users/francois/appdata/local/temp/_bazel_francois/wkog3ss7/external/local_config_cuda/C\r\n:Program FilesNVIDIA GPU Computing ToolkitCUDAv9.0\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_\r\ndefs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1142\r\n                _create_local_cuda_repository(repository_ctx)\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 995, in _create_local_cuda_repository\r\n                _get_cuda_config(repository_ctx)\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 747, in _get_cuda_config\r\n                _cuda_toolkit_path(repository_ctx)\r\n        File \"P:/anaconda3/tensorflow/third_party/gpus/cuda_configure.bzl\", line 282, in _cuda_toolkit_path\r\n                repository_ctx.path(cuda_toolkit_path).exists\r\nIllegal char <:> at index 90: C:/users/francois/appdata/local/temp/_bazel_francois/wkog3ss7/external/local_config_cuda/C\r\n:Program FilesNVIDIA GPU Computing ToolkitCUDAv9.0\r\nINFO: Elapsed time: 0,476s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nPS P:\\anaconda3\\tensorflow> bazel version\r\nBuild label: 0.10.0rc1\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Sat Jun 5 11:32:18 +50010 (1516009433538)\r\nBuild timestamp: 1516009433538\r\nBuild timestamp as int: 1516009433538\r\nPS P:\\anaconda3\\tensorflow>", "@FB43 \r\nAs per the configurations, please refer to [this link](https://www.tensorflow.org/install/source_windows#gpu) which shows you would have to use cmake instead of bazel, is here any specific reason to use 1.8, could you use a later version of tensorflow.\r\n\r\nFor your second error:\r\nCould you please refer to [this link](https://stackoverflow.com/questions/45425199/bazel-build-error-with-tensorflow) and let us know if it helps.\r\n", "@Saduf2019 hello, sorry, when I was working I often missed those things as many people. Now, I'm retired but nothing changes!\r\nSo, I have Python 3.6, Bazel 0.10.0, MSVC 2015 update 3 (2014), Cmake v3.6.3, GCC 4.8, MSYS2, Chocolatey, Anaconda3 (where I want to build TF). And other things, of course.\r\nThe reason why I want to install TF 1.8 is that my GPU has 3.0 computing capabilities.\r\nI need it to run some functions of the Visions of Chaos software (free and interesting).\r\nIf an other version could run (or be recompiled) for 3.0, I'll take it, I just want the better version for my GPU (not the moment for changing it...).\r\nNow, I have the tools, but I'm lost.\r\nWhat to do now?\r\nIf you can help me a little bit more, it will be much appreciated.\r\nThanks for all.\r\nBest regards,\r\nFran\u00e7ois\r\n", "@Saduf2019 Hello, I opened a new issue to get some advices for changing my GPU. I think the results would be better.\r\nThanks for all.\r\nFran\u00e7ois", "@FB43 \r\nPlease confirm if we may move this issue to resolved, as the GPU change is monitored and this would be a duplicate issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37231\">No</a>\n"]}, {"number": 37230, "title": "tf.function second derivative error", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): macOS Mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):binary  v2.1.0-rc2-17\r\n\r\n- Python version:  - Bazel \r\nversion (if compiling from source): Python 3.7.1\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI have a custom function need to use slicing in a for loop. The first-order derivative is working properly but the second-order derivative gives the error. The error only occurs when the function is decorated with tf.function. Below is a simplified code to reproduce the error.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nx =tf.random.uniform([9],minval = -50,maxval = -30,seed = 1,dtype = tf.float32)\r\n\r\n@tf.function\r\ndef A(x):\r\n    return x[1]\r\n\r\n@tf.function\r\ndef g(x):\r\n\r\n    Z_sum = tf.constant([0.],dtype = tf.float32)\r\n\r\n    for i in tf.range(x.shape[0]):\r\n        Z_sum = tf.add(Z_sum, A(x))\r\n\r\n    return Z_sum\r\n\r\nwith tf.GradientTape() as t:\r\n    t.watch(x)\r\n    with tf.GradientTape() as tt:\r\n        tt.watch(x)\r\n        loss = g(x)\r\n    jac = tt.gradient(loss,x)\r\nhess = t.gradient(jac,x)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\r\n   2325       with c_api_util.tf_buffer() as buf:\r\n-> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2327         data = c_api.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\r\n   2329       # Convert to ValueError for backwards compatibility.\r\n-> 2330       raise ValueError(str(e))\r\n   2331     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\r\n   2325       with c_api_util.tf_buffer() as buf:\r\n-> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\r\n   2327         data = c_api.TF_GetBuffer(buf)\r\n\r\nInvalidArgumentError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    330     try:\r\n--> 331       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    332       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\r\n   2329       # Convert to ValueError for backwards compatibility.\r\n-> 2330       raise ValueError(str(e))\r\n   2331     x = attr_value_pb2.AttrValue()\r\n\r\nValueError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    467               as_ref=input_arg.is_ref,\r\n--> 468               preferred_dtype=default_dtype)\r\n    469         except TypeError as err:\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1313     if ret is None:\r\n-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1315 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    316   _ = as_ref\r\n--> 317   return constant(v, dtype=dtype, name=name)\r\n    318 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 258                         allow_broadcast=True)\r\n    259 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 296           allow_broadcast=allow_broadcast))\r\n    297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    438     if values is None:\r\n--> 439       raise ValueError(\"None values not supported.\")\r\n    440     # if dtype is provided, forces numpy array to be the type\r\n\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    481             observed = ops.convert_to_tensor(\r\n--> 482                 values, as_ref=input_arg.is_ref).dtype.name\r\n    483           except ValueError as err:\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n   1313     if ret is None:\r\n-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1315 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    316   _ = as_ref\r\n--> 317   return constant(v, dtype=dtype, name=name)\r\n    318 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)\r\n    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 258                         allow_broadcast=True)\r\n    259 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,\r\n--> 296           allow_broadcast=allow_broadcast))\r\n    297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)\r\n    438     if values is None:\r\n--> 439       raise ValueError(\"None values not supported.\")\r\n    440     # if dtype is provided, forces numpy array to be the type\r\n\r\nValueError: None values not supported.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-31-882dac5b0d47> in <module>\r\n     21         tt.watch(x)\r\n     22         loss = g(x)\r\n---> 23     jac = tt.gradient(loss,x)\r\n     24 hess = t.gradient(jac,x)\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1027         output_gradients=output_gradients,\r\n   1028         sources_raw=flat_sources_raw,\r\n-> 1029         unconnected_gradients=unconnected_gradients)\r\n   1030 \r\n   1031     if not self._persistent:\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     75       output_gradients,\r\n     76       sources_raw,\r\n---> 77       compat.as_str(unconnected_gradients.value))\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)\r\n   1254           break\r\n   1255       return backward._call_flat(  # pylint: disable=protected-access\r\n-> 1256           processed_args, remapped_captures)\r\n   1257 \r\n   1258     return _backward_function_wrapper, recorded_outputs\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1695         possible_gradient_type,\r\n   1696         executing_eagerly)\r\n-> 1697     forward_function, args_with_tangents = forward_backward.forward()\r\n   1698     if executing_eagerly:\r\n   1699       flat_outputs = forward_function.call(\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self)\r\n   1421     \"\"\"Builds or retrieves a forward function for this call.\"\"\"\r\n   1422     forward_function = self._functions.forward(\r\n-> 1423         self._inference_args, self._input_tangents)\r\n   1424     return forward_function, self._inference_args + self._input_tangents\r\n   1425 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self, inference_args, input_tangents)\r\n   1183       (self._forward, self._forward_graph, self._backward,\r\n   1184        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (\r\n-> 1185            self._forward_and_backward_functions(inference_args, input_tangents))\r\n   1186     return self._forward\r\n   1187 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)\r\n   1329     outputs = self._func_graph.outputs[:self._num_inference_outputs]\r\n   1330     return self._build_functions_for_outputs(\r\n-> 1331         outputs, inference_args, input_tangents)\r\n   1332 \r\n   1333 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)\r\n    888             self._func_graph.inputs,\r\n    889             grad_ys=gradients_wrt_outputs,\r\n--> 890             src_graph=self._func_graph)\r\n    891 \r\n    892       captures_from_forward = [\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in <lambda>()\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _WhileGrad(op, *grads)\r\n    357   body_grad_graph, args = _create_grad_func(\r\n    358       ys, xs, non_none_grads, cond_graph, body_graph,\r\n--> 359       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)\r\n    360 \r\n    361   if body_grad_graph.while_op_needs_rewrite:\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _create_grad_func(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)\r\n    599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\r\n    600                                          maximum_iterations, while_op,\r\n--> 601                                          body_graph_inputs, body_graph_outputs))\r\n    602 \r\n    603   # Update the list of outputs with tensors corresponding to the captured\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in <lambda>(*args)\r\n    595   grad_func_graph = func_graph_module.func_graph_from_py_func(\r\n    596       name,\r\n--> 597       lambda *args: _grad_fn(ys, xs, args, body_graph),\r\n    598       args, {},\r\n    599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _grad_fn(ys, xs, args, func_graph)\r\n    655   grad_outs = gradients_util._GradientsHelper(\r\n    656       ys, xs, grad_ys=grad_ys, src_graph=func_graph,\r\n--> 657       unconnected_gradients=\"zero\")\r\n    658 \r\n    659   # TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    334       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    335     except ValueError:\r\n--> 336       return grad_fn()  # Exit early\r\n    337 \r\n    338   if not xla_compile:\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in <lambda>()\r\n    667                 # functions.\r\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\r\n--> 669                                          lambda: grad_fn(op, *out_grads))\r\n    670               else:\r\n    671                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/list_ops.py in _PopBackGrad(op, dlist, delement)\r\n    187         element_shape=gen_list_ops.tensor_list_element_shape(\r\n    188             op.outputs[0], shape_type=dtypes.int32))\r\n--> 189   return gen_list_ops.tensor_list_push_back(dlist, delement), None\r\n    190 \r\n    191 \r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_list_ops.py in tensor_list_push_back(input_handle, tensor, name)\r\n    761   _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    762         \"TensorListPushBack\", input_handle=input_handle, tensor=tensor,\r\n--> 763                               name=name)\r\n    764   _result = _outputs[:]\r\n    765   if _execute.must_record_gradient():\r\n\r\n~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    484             raise ValueError(\r\n    485                 \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\r\n--> 486                 (input_name, err))\r\n    487           prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\r\n    488                     (input_name, op_type_name, observed))\r\n\r\nValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.\r\n", "comments": ["I think this is related to issue #15219, is there any solution for it now?", "Could able to reproduce the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/28674efbe2941bb129d5622c13892277/untitled416.ipynb). Thanks!", "@GeorgeLiang3 I don't run into any error when I am using tf-nightly. Please find my gist [here](https://colab.sandbox.google.com/gist/gowthamkpr/f7bcc6f630e4864f0c269256f9d3df96/untitled37.ipynb). Thanks!", "@gowthamkpr  Thanks for your reply! But if I add one more 'if' or 'for' statement in the function, the both give the same error. Please find my gist [here](https://colab.research.google.com/drive/1p0bb2ePkc6ect0Q26RVB_BByOUFtppeq)", "@gowthamkpr Any solution to this? Thanks in advance", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230\">No</a>\n"]}, {"number": 37229, "title": "inconsistent default parameters for adagrad optimizer", "body": "**System information** \r\nTensorFlow 2.1.0 (tested on anaconda package and docker image)\r\n\r\n**Describe the current behavior**\r\n\r\nThe default value for the `initial_accumulator_value` parameter of the Adagrad optimizer is different depending on whether it is passed as a string or as an instance of the optimizer class. This may lead to drastic differences in learning behavior, which is not apparent from the code.\r\n\r\n**Describe the expected behavior**\r\n\r\nBoth variants should use the same default parameters. \r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Model()\r\nmodel.compile(optimizer='adagrad')\r\nmodel.optimizer.get_config()\r\n# {'name': 'Adagrad', 'learning_rate': 0.001, 'decay': 0.0, 'initial_accumulator_value': 0.0, 'epsilon': 1e-07}\r\n\r\ntf.keras.optimizers.Adagrad().get_config()\r\n# {'name': 'Adagrad', 'learning_rate': 0.001, 'decay': 0.0, 'initial_accumulator_value': 0.1, 'epsilon': 1e-07}\r\n\r\n```\r\n", "comments": ["I have tried on colab with TF version 2.1.0, 2.2.0-dev20200302  and was able to reproduce the issue.Please,find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9a5b708a520ef1b30feeabcd9fc6023a/untitled698.ipynb). Thanks!", "This is fixed [here](https://github.com/tensorflow/tensorflow/commit/a90a1a863e250f607e1eb7118a99218b22e627cd)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37229\">No</a>\n"]}, {"number": 37228, "title": "tf.debugging.assert_shapes", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes\r\n\r\n## Description of issue (what needs changing):\r\nThe documentation for the `shapes` argument states that is expects a dict, but in fact in many circumstances it is not possible to assemble such a dict because eager tensors are unhashable.\r\nSo while it is possible to submit a dict here in GraphMode, Eager expects a list of (key, value) pairs.\r\n\r\nIt should also state whether `tf.Variable`s can be used as keys.\r\n\r\n### Usage example\r\nThe usage example seems to confuse the two concepts, and provides a mixup of both in invalid python syntax:\r\n```\r\ntf.assert_shapes([\r\n  (x: ('N', 'Q')),\r\n  (y: ('N', 'D')),\r\n  (param: ('Q',)),\r\n  (scalar: ()),\r\n])\r\n```\r\n*This seems to be fixed already in master*\r\n\r\n\r\nFurther, `tf.assert_shapes` should probably be changed to `tf.debugging.assert_shapes`\r\n\r\n### Submit a pull request?\r\nYes", "comments": ["Looking at the code in master, this seems to have been partially fixed (the usage example). However, now there are `assert_shapes` and `assert_shapes_v2` functions which slightly different documentation, even though the latter does nothing but call the former.", "@ngc92,\r\nCould you please check the nightly version of the docs from this [link](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes?version=nightly). \r\nI was able to reproduce the error with the example given on the stable version. However, the example given in nightly docs works without any issues. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/b63bf1fd7194fa28fa1a198f17c5baab/37228.ipynb). Thanks!", "yes, the example from the nightly docs works. The point about the `shapes` argument documented as being a dict still stands, though. ", "Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 37227, "title": "tf.expand_dims unable to read proper rank from input tensors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Linux Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): docker tensorflow/tensorflow:2.1.0-gpu-py3\r\n- Python version: - Bazel\r\nversion (if compiling from source): Python 3.6.9\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: CUDA V10.1.243 - Nvidia Titan RTX\r\n\r\n\r\n**Describe the current behavior**\r\nI have a custom loss function that maps `tf.expand_dims` across a set of minibatches. When this loss is calculated as part of the `model.fit` function, the rank of the input tensor is not correctly read in the `tf.expand_dims` function call and I get the following exception:\r\n```ValueError: dim 2 not in the interval [-2, 1]. for 'ExpandDims' (op: 'ExpandDims') with input shapes: [?], [] and with computed input tensors: input[1] = <2>.```\r\nIf I do not call `tf.expand_dims` and simply print the shape/rank of the input tensor, everything appears consistent. Furthermore, if the input dataset is simply iterated over and the batches are passed directly into the loss function, the exception is not raised.\r\n\r\n**Describe the expected behavior**\r\nAn error is not raised\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.losses import Loss\r\n\r\nMINI_BATCH = 3\r\nSPLITS = 2\r\nBATCH = SPLITS * MINI_BATCH\r\n\r\n\r\nclass CustomLoss(Loss):\r\n    def call(self, labels, outputs):\r\n\r\n        @tf.function\r\n        def fxn(inp):\r\n            return tf.expand_dims(inp, 2)\r\n        mapped = tf.map_fn(fxn, labels, dtype=tf.float32)\r\n        return tf.reduce_sum(mapped)\r\n\r\ndef gen_batches():\r\n    for _ in range(1000):\r\n        outp = np.random.random((BATCH, 64))\r\n        mask = np.random.random((SPLITS, MINI_BATCH, MINI_BATCH)) > .5\r\n        mask = tf.cast(mask, tf.float32)\r\n        yield outp, mask\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    # Toggle to run as training (throws exception) vs merely iterating and calculating loss\r\n    AS_MODEL = True\r\n\r\n    dataset = tf.data.Dataset.from_generator(gen_batches, (tf.float32, tf.float32), output_shapes=([BATCH, 64], [SPLITS, MINI_BATCH, MINI_BATCH]))\r\n    loss = CustomLoss()\r\n\r\n    if AS_MODEL:\r\n        inputs = tf.keras.Input(shape=(64,))\r\n        out = inputs + 1\r\n        model = tf.keras.Model(inputs=inputs, outputs=out)\r\n        opt = tf.keras.optimizers.Nadam(.0005)\r\n        model.compile(optimizer=opt, loss=loss)\r\n\r\n        # This throws exception\r\n        model.fit(dataset, epochs=1)\r\n\r\n    else:\r\n        # This runs fine\r\n        for data in dataset.__iter__():\r\n            tf.print(loss(data[1], data[0])) \r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"deepsee/networks/av/attention/test.py\", line 40, in <module>\r\n    model.compile(optimizer=opt, loss=loss)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 446, in compile\r\n    self._compile_weights_loss_and_weighted_metrics()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 1592, in _compile_weights_loss_and_weighted_metrics\r\n    self.total_loss = self._prepare_total_loss(masks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 1652, in _prepare_total_loss\r\n    per_sample_losses = loss_fn.call(y_true, y_pred)\r\n  File \"deepsee/networks/av/attention/test.py\", line 17, in call\r\n    mapped = tf.map_fn(fxn, labels, dtype=tf.float32)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/map_fn.py\", line 268, in map_fn\r\n    maximum_iterations=n)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2675, in while_loop\r\n    back_prop=back_prop)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/while_v2.py\", line 194, in while_loop\r\n    add_control_dependencies=add_control_dependencies)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/while_v2.py\", line 172, in wrapped_body\r\n    outputs = body(*_pack_sequence_as(orig_loop_vars, args))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/map_fn.py\", line 257, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    deepsee/networks/av/attention/test.py:16 fxn  *\r\n        return tf.expand_dims(inp, 2)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:399 expand_dims_v2\r\n        return gen_array_ops.expand_dims(input, axis, name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py:2202 expand_dims\r\n        \"ExpandDims\", input=input, dim=axis, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py:595 _create_op_internal\r\n        compute_device)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:3322 _create_op_internal\r\n        op_def=op_def)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1786 __init__\r\n        control_input_ops)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1622 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: dim 2 not in the interval [-2, 1]. for 'ExpandDims' (op: 'ExpandDims') with input shapes: [?], [] and with computed input tensors: input[1] = <2>.\r\n```", "comments": ["I could replicate the issue with Tensorflow 2.1 on colab.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/216cc11e3d5417acb0ceeb3af8c50ead/untitled414.ipynb). Thanks!", "Any updates regarding this issue?", "Apologies for the delay in response. This is fixed with TensorFlow version 2.2\r\nPlease give it a try. Thanks!", "Thanks, no issues now with 2.2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37227\">No</a>\n"]}, {"number": 37226, "title": "RaggedTensor are converted to Tensors inside exported signature. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 18.04`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`\r\n- TensorFlow installed from (source or binary): `no`\r\n- TensorFlow version (use command below): `2.0.1`\r\n- Python version: `3.7.3`\r\n- Bazel version (if compiling from source): `-`\r\n- GCC/Compiler version (if compiling from source): `-`\r\n- CUDA/cuDNN version: `-`\r\n- GPU model and memory: `-`\r\n\r\n**Describe the current behavior**\r\nAs [properly documented](https://www.tensorflow.org/api_docs/python/tf/function#args_2), it is currently possible to pass a possibly nested sequence of [`tf.TensorSpec`](https://www.tensorflow.org/api_docs/python/tf/TensorSpec) objects as a `tf.function` `input_signature` argument.\r\n\r\nHowever, if one attempts to pass a `tf.RaggedTensorSpec` inside an `input_signature` sequence, everything still works, but the `RaggedTensorSpec` gets converted in its \"dense\" `values` + `row_splits` form.\r\n \r\n**Describe the expected behavior**\r\nI would expect that if one specifies a `tf.RaggedTensorSpec` element inside the `input_signature` sequence it should either be exported as a `tf.RaggedTensorSpec`, or raise an error.\r\n\r\nOf course the firs option sounds more desiderable, since it's pretty common to have ragged input sequences. In this case `tf.RaggedTensorSpec` would be also lacking of a `name` argument, but this is easier to solve.\r\n\r\n**Standalone code to reproduce the issue** \r\n[Colab Notebook](https://colab.research.google.com/drive/1mEXNfmIiEU9XNL7niKCC_LRccfW5ZDob)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass SomeModule(tf.Module):\r\n\r\n  @tf.function(input_signature=[\r\n    tf.RaggedTensorSpec(shape=[None, None], dtype=tf.string)\r\n  ])\r\n  def return_ragged_inputs(self, inputs: tf.RaggedTensor):\r\n    return inputs\r\n\r\nragged_inputs = tf.ragged.constant([[\"foo\"], [\"bar\"], [\"foo\", \"bar\"]], dtype=tf.string)\r\n\r\nsome_module = SomeModule()\r\nreturn_ragged_inputs_result = some_module.return_ragged_inputs(ragged_inputs)\r\nprint(f\"ragged_inputs => {ragged_inputs}\\n\")\r\nprint(\"We can use the method with ragged inputs:\")\r\nprint(f\"return_ragged_inputs => {return_ragged_inputs_result}\\n\")\r\nprint(\"Saving some_module...\")\r\ntf.saved_model.save(some_module, export_dir=\"foobar\")\r\nsome_module = tf.saved_model.load(\"foobar\")\r\nprint(f\"\\nWe can pass and return ragged inputs even by saving and reloading the module, if we use it 'eagerly':\")\r\nprint(some_module.return_ragged_inputs(ragged_inputs))\r\nprint(\"\\nBut the graph signature tell us that our exported signature expects tf.Tensor values instead:\")\r\nexported_model_graph_inputs = some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs\r\nexported_model_graph_outputs = some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs\r\nprint(f\"Expected inputs  : {exported_model_graph_inputs}\")\r\nprint(f\"Expected outputs : {exported_model_graph_outputs}\")\r\nprint(\"\\nThis means we couldn't use the method as we would expect in (e.g.) TensorFlow Serving.\")\r\n```\r\n\r\n**Other info / logs**\r\nThis issue follows up #37049 , as suggested by @gowthamkpr .\r\n", "comments": ["@stefanondisponibile I was able to feed in ragged input into the loaded model and it works but if I don't feed in the ragged tensor, it fails. Here is the [gist](https://colab.sandbox.google.com/gist/gowthamkpr/f8f851ab2ec82a39cfc3e4216d063acc/untitled32.ipynb)\r\n", "I believe this is due to `RaggedTensor` not being a first-class type inside GraphDef - internally, it is represented as a [CompositeTensor](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/composite_tensor.py#L31), which in turn is lowered to a list of `Tensor`, by `tf.nest.flatten(expand_composites=True)` when the function is compiled.\r\n\r\nWhen loading a saved model, that lowering becomes visible.\r\n\r\nAs a workaround, you should be able to build inputs compatible with the loaded model by flattening the ragged tensor: `tf.nest.flatten(ragged_inputs, expand_composites=True)`.\r\n\r\nI think we can eventually handle them correctly inside SavedModel, by placing the necessary type spec into the graph metadata and reconstructing it upon import, but I'll let @k-w-w and @edloper give more expert insights.\r\n\r\nNit: in the colab, did you mean the last two lines to be \"exported\", not \"expected\"?", "So, @stefanondisponibile is expecting it to be a ragged tensor but its not the case as seen in the gist.", "@gowthamkpr yes, for what I've seen so far, and as explained by @mdanatg this difference between Ragged and normal Tensors pops up when the function gets converted to a Graph. I can't do it in Colab, but if you tried to load that servable with tf serving you would see you would have to pass two different dense Tensors in place of the Ragged Tensor.\n\n@mdanatg thanks for the explanation! I am currently working around this by actually passing two dense Tensors at inference time, one with the values and the other with the splits, and building them up inside the function itself via tf.RaggedTensor.from_row_splits. So those last two lines were as \"expected\", since I was expecting to pass RaggedTensors to the served model, but I end up passing dense ones, building the RaggedTensor inside the function.", "Loaded model `signatures` currently flatten any structured input into a flat list of tensors.  This is true for RaggedTensor, but also for nested structure, such as a dictionary of tensors.  E.g., if we define a module whose function takes and returns a dict of tensors, then the model signature inputs & outputs are currently flat lists of tensors:\r\n\r\n```\r\nclass SomeModule(tf.Module):\r\n\r\n  @tf.function(input_signature=[{'foo': tf.TensorSpec(None, tf.int32),\r\n                                 'bar': tf.TensorSpec(None, tf.string)}\r\n  ])\r\n  def return_dict(self, inputs):\r\n    return inputs\r\n\r\nsome_module = SomeModule()\r\ntf.saved_model.save(some_module, export_dir=\"foobar2\")\r\nsome_module = tf.saved_model.load(\"foobar2\")\r\nprint(some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs)\r\n# prints: [<tf.Tensor 'inputs:0' shape=<unknown> dtype=string>, <tf.Tensor 'inputs_1:0' shape=<unknown> dtype=int32>]\r\nprint( some_module.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].outputs)\r\n# prints: [<tf.Tensor 'Identity:0' shape=<unknown> dtype=string>, <tf.Tensor 'Identity_1:0' shape=<unknown> dtype=int32>]\r\n```\r\n\r\nHopefully we can improve on this, but the problem is a general issue with any structured inputs & outputs, and is not specific to ragged tensors.", "@stefanondisponibile\r\nPlease let us know if this is still an issue.", "Hey @Saduf2019 , sorry for not having replied yet! I would say that _yes_, this still is an issue, but as @edloper kindly explained is not specific to ragged tensors, moreover there are workarounds/better-ways for tackling this, so I'll close this specific issue. Thanks for you help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37226\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37226\">No</a>\n", "I'm having a similar issue, my model input is a RaggedTensor (1000, None, None) connected to an LSTM layer. \r\nThe model is working as expected and when calling predict I get the predictions.\r\n```\r\ninput = tf.ragged.constant([[\r\n                            [-0.9984272718429565, -0.9422321319580078, -0.27657580375671387, -3.185823678970337, -0.6360141634941101, -1.6579184532165527, -1.9000954627990723, -0.49169546365737915, -0.6758883595466614, -0.6677696704864502, -0.532067060470581], \r\n                            [-0.9984272718429565, -0.9421600103378296, 2.2048349380493164, -1.273996114730835, -0.6360141634941101, -1.5917999744415283, 0.6147914528846741, -0.49169546365737915, -0.6673409938812256, -0.6583622694015503, -0.5273991227149963], \r\n                            [-0.9984272718429565, -0.942145586013794, 2.48842453956604, -1.6836735010147095, -0.6360141634941101, -1.5785763263702393, -1.900200605392456, -0.49169546365737915, -0.6656315326690674, -0.6583622694015503, -0.5273991227149963]\r\n]])\r\nmodel.predict(input)\r\n\r\narray([[0.5138151 , 0.3277698 , 0.26122513]], dtype=float32)\r\n```\r\nWhen saving the model the input is split into `args_0`and `args_0_1`.\r\n\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['args_0'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 11)\r\n        name: serving_default_args_0:0\r\n    inputs['args_0_1'] tensor_info:\r\n        dtype: DT_INT64\r\n        shape: (-1)\r\n        name: serving_default_args_0_1:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['dense_2'] tensor_info:\r\n        dtype: DT_FLOAT\r\n        shape: (-1, 3)\r\n        name: StatefulPartitionedCall:0\r\n  Method name is: tensorflow/serving/predict\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0124 13:38:49.266864 139946102503296 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n\r\nDefined Functions:\r\n  Function Name: '__call__'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          DType: RaggedTensorSpec\r\n          Value: RaggedTensorSpec(TensorShape([None, None, 11]), tf.float32, 1, tf.int64)\r\n        Argument #2\r\n          DType: bool\r\n          Value: True\r\n        Argument #3\r\n          DType: NoneType\r\n          Value: None\r\n    Option #2\r\n      Callable with:\r\n        Argument #1\r\n          DType: RaggedTensorSpec\r\n          Value: RaggedTensorSpec(TensorShape([None, None, 11]), tf.float32, 1, tf.int64)\r\n        Argument #2\r\n          DType: bool\r\n          Value: False\r\n        Argument #3\r\n          DType: NoneType\r\n          Value: None\r\n\r\n  Function Name: '_default_save_signature'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          DType: RaggedTensorSpec\r\n          Value: RaggedTensorSpec(TensorShape([None, None, 11]), tf.float32, 1, tf.int64)\r\n\r\n  Function Name: 'call_and_return_all_conditional_losses'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          DType: RaggedTensorSpec\r\n          Value: RaggedTensorSpec(TensorShape([None, None, 11]), tf.float32, 1, tf.int64)\r\n        Argument #2\r\n          DType: bool\r\n          Value: True\r\n        Argument #3\r\n          DType: NoneType\r\n          Value: None\r\n    Option #2\r\n      Callable with:\r\n        Argument #1\r\n          DType: RaggedTensorSpec\r\n          Value: RaggedTensorSpec(TensorShape([None, None, 11]), tf.float32, 1, tf.int64)\r\n        Argument #2\r\n          DType: bool\r\n          Value: False\r\n        Argument #3\r\n          DType: NoneType\r\n          Value: None\r\n```\r\n I tried to use `tf.nest.flatten` as follows:\r\n\r\n\r\n```\r\ninput = tf.ragged.constant([[\r\n                            [-0.9984272718429565, -0.9422321319580078, -0.27657580375671387, -3.185823678970337, -0.6360141634941101, -1.6579184532165527, -1.9000954627990723, -0.49169546365737915, -0.6758883595466614, -0.6677696704864502, -0.532067060470581], \r\n                            [-0.9984272718429565, -0.9421600103378296, 2.2048349380493164, -1.273996114730835, -0.6360141634941101, -1.5917999744415283, 0.6147914528846741, -0.49169546365737915, -0.6673409938812256, -0.6583622694015503, -0.5273991227149963], \r\n                            [-0.9984272718429565, -0.942145586013794, 2.48842453956604, -1.6836735010147095, -0.6360141634941101, -1.5785763263702393, -1.900200605392456, -0.49169546365737915, -0.6656315326690674, -0.6583622694015503, -0.5273991227149963]\r\n]])\r\n\r\ninputs = tf.nest.flatten(input, expand_composites=True)\r\n```\r\n\r\n\r\nand got a list of 3 tensors:\r\n\r\n```\r\n[<tf.Tensor: shape=(33,), dtype=float32, numpy=\r\n array([-0.9984273 , -0.94223213, -0.2765758 , -3.1858237 , -0.63601416,\r\n        -1.6579185 , -1.9000955 , -0.49169546, -0.67588836, -0.6677697 ,\r\n        -0.53206706, -0.9984273 , -0.94216   ,  2.204835  , -1.2739961 ,\r\n        -0.63601416, -1.5918    ,  0.61479145, -0.49169546, -0.667341  ,\r\n        -0.65836227, -0.5273991 , -0.9984273 , -0.9421456 ,  2.4884245 ,\r\n        -1.6836735 , -0.63601416, -1.5785763 , -1.9002006 , -0.49169546,\r\n        -0.66563153, -0.65836227, -0.5273991 ], dtype=float32)>,\r\n <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 3])>,\r\n <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 0, 11, 22, 33])>]\r\n```\r\n\r\nI tried calling tensorflow serving like so:\r\n```\r\nimport json\r\nimport requests\r\nheaders = {\"content-type\": \"application/json\"}\r\ndata = json.dumps({\"instances\":[\r\n                                  {\r\n                                      \"args_0\":np.array(inputs[0]).tolist(),\r\n                                      \"args_0_1\":np.array(inputs[2]).tolist()\r\n                                  }\r\n                               ]\r\n                   })      \r\n\r\njson_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)\r\npredictions = json.loads(json_response.text)\r\n```\r\n\r\nBut got the following error:\r\n`{'error': 'Specified a list with shape [?,11] from a tensor with shape [0,33]\\n\\t [[{{node sequential_1/first_lstm/TensorArrayUnstack/TensorListFromTensor}}]]'}\r\n`\r\n\r\nWhat am I missing?\r\n"]}, {"number": 37225, "title": "Update XNNPACK for support Windows", "body": "This change add support Windows of XNNPACK delegate.\r\n\r\nhttps://github.com/google/XNNPACK/pull/383\r\n\r\nXNNPACK did not support Windows until yesterday. I sent pull-request to XNNPACK and pthreadpool and the pull-requests were merged. Now XNNPACK works on Windows. So this change update commit-id to enable Windows support of XNNPACK delegate.", "comments": ["What does this change bring?", "Sorry my too short description. XNNPACK did not support Windows until yesterday. I sent pull-request to XNNPACK and pthreadpool and the pull-requests were merged. Now XNNPACK works on Windows. So this change update commit-id to enable Windows support of XNNPACK delegate.", "Thank you", "Can you fix builds please?\r\n\r\n```\r\nERROR: /tmpfs/tmp/bazel/external/XNNPACK/BUILD.bazel:1854:1: @XNNPACK//:XNNPACK depends on @pthreadpool//:pthreadpool in repository @pthreadpool which failed to fetch. no such package '@pthreadpool//': java.io.IOException: Error extracting\r\n```\r\n\r\n(one of the errors from the sanity build)", "Probably, mirror server is not ready?\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/37225/files#diff-a9d9ac7fcfb09fe0b85976cdcad9ccbeR11", "How to fix this?\r\n\r\n![image](https://user-images.githubusercontent.com/10111/75865812-71040800-5e47-11ea-8448-ea24d318acdb.png)\r\n", "Mirror should be ready now"]}, {"number": 37224, "title": "Add comment to unused argument: sample_weight_mode", "body": "Argument: `sample_weight_mode` of Keras Model:compile is no longer needed but for backward compatibility, we need to keep it untouched.\r\n\r\nThis PR adds a comment for it.\r\n\r\nrelated to #37125", "comments": ["@omalleyt12 I updated the PR, please take a look. Thank you!", "@howl-anderson Can you please resolve conflicts? Thanks!", "@gbaned Sure, I submit a new commit for this later.", "@gbaned conflicts resolved.", "@omalleyt12 Please take a look. Thank you!", "@howl-anderson Can you please resolve conflicts? Thanks!", "Hi @gbaned and @omalleyt12, recently, a commit [Remove sample_weight_mode arg from compile and cleanup docs](https://github.com/tensorflow/tensorflow/commit/c7aa3e8d1711f49add2c6dc76fc8c000ce1edee8) merged into the master branch by @pavithrasv, that commit fixes the issue this PR attend to solve. So this PR will be closed since it already finishes its purpose. Thank you all for efforts!"]}, {"number": 37223, "title": "Use `tf.compat.v1.summary.FileWriter` under TF v1", "body": "@mihaimaruseac Here is the new Pull Request. \r\n\r\nIssue: #37113 ", "comments": ["Ok, now what's left is to write a test that would fail before this change and passes afterwards.", "What should i do next ? \r\nAll the other parts of the code is compatible with tensorflow 2.x", "So this PR is not needed on master (where 2.x behavior and API is used)?", "\"module_test.py\" failed initially. Converting  \"tf.summary.FileWriter\" to  \"tf.compat.v1.summary.FileWriter\" , solves the problem.\r\nHence, i think this should be on the master.\r\n\r\nThis is the Screenshot, before i made the changes.\r\n![Screenshot from 2020-03-02 22-58-37](https://user-images.githubusercontent.com/41910134/75701216-63307480-5cd9-11ea-8043-69ed9bf78579.png)\r\n\r\n\r\nHere is the Screenshot to the Final Result after the changes\r\n![Screenshot from 2020-03-02 22-56-34](https://user-images.githubusercontent.com/41910134/75701069-1ba9e880-5cd9-11ea-959e-8c275719e037.png)\r\n", "@mihaimaruseac Can you please take a look on the above comment from @ayushmankumar7. Thank you!", "@mihaimaruseac Thanks for the approval. :+1: "]}, {"number": 37222, "title": "ModuleNotFoundError: No module named 'utility'", "body": "![0](https://user-images.githubusercontent.com/61679488/75651231-4d9c5a00-5c9b-11ea-8054-bc2c8a22a1d1.PNG)\r\n\r\nhi\r\ni wanna use '**from utility import lazy_property**' code but i can't\r\n i think that code for TF1. can you let me know what is the Code that serves the same role in tf2?\r\n\r\nIf not, please let me know the appropriate code thanks", "comments": ["is utility a part of Tensorflow? Can you provide the directory you are in?  ", "> is utility a part of Tensorflow? Can you provide the directory you are in?\r\n\r\nI don't think the utility folder exists I'm just copying a book", "Yes. Absolutely. ", "> Yes. Absolutely.\r\n\r\nshould i downgrade the tensorflow same as  book ?", "Downgrading Tensorflow may not help, i guess. \r\nUtility maybe a different library.", "Hi, sorry but this is not a tensorflow issue. In your example, `utility` is some kind of third-party package, or home-made library, not a part of tensorflow, as is clear from your import syntax.", "> Hi, sorry but this is not a tensorflow issue. In your example, `utility` is some kind of third-party package, or home-made library, not a part of tensorflow, as is clear from your import syntax.\r\n\r\nTHANKs for help", "@tonyamrtin \r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nHence moving this issue to resolved status."]}, {"number": 37221, "title": "The DLL error in importing tensorflow.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): tensorflow pypi\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.8 with updated pip==20.0.2\r\n- Installed using: pip in virtualenv\r\n- CUDA/cuDNN version: CUDA V10.0.130, cuDNN 7.4.1\r\n- GPU model and memory: GeForce GTX 1660\r\n\r\n**Describe the problem**\r\nI got this error when importing TensorFlow.\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nNVIDIA driver version is 432.00. I install CUDA 10 and configured cudnn 7.4.1. Also, installed Visual Studio 2017 buildtools. I tried to install from tf-1.12 to tf-2.0, all versions were failed. I checked previous issues about this error, but some guys resolved using conda. If so, couldn't I use pip installer with tensorflow?\r\n", "comments": ["Hello,\r\n\r\nDo you have any solution?", "@Venus24kk \r\nJust to verify did you follow the instructions to install from pip using [Tensorflow website](https://www.tensorflow.org/install/pip).Thanks", "Thank you for your message. I found my CPU doesn't support AVX instruction. When I install tensorflow-gpu, does it matter if CPU supports AVX instruction?\r\n\r\nRegards,", "@Venus24kk \r\nYes while you do pip install from windows,it does matter.", "Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\n#36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37221\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37221\">No</a>\n"]}, {"number": 37220, "title": "Cannot get model to work", "body": "It currently fails when trying to fit the model. The 'train_ds' seems to cause it to fail because it is a Batchdataset and not a numpty array. When I convert this to an np.array it fails because it is expecting a \"dense_1_input to have 2 dimensions, but got array with shape ()\". but I have no idea what this means.\r\n\r\n\r\n`from __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\n\r\nfrom keras import layers\r\nfrom keras import regularizers\r\n\r\n#maybe not needed\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.layers import LSTM\r\n\r\nimport tensorflow_docs as tfdocs\r\nimport tensorflow_docs.modeling\r\nimport tensorflow_docs.plots\r\n\r\nfrom  IPython import display\r\nfrom matplotlib import pyplot as plt\r\n\r\nimport numpy as np\r\n\r\nimport pathlib\r\nimport shutil\r\nimport tempfile\r\n\r\nlogdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\r\nshutil.rmtree(logdir, ignore_errors=True)\r\n\r\ngz = tf.keras.utils.get_file('HIGGS2.csv.gz', 'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')\r\n\r\nFEATURES = 28\r\n\r\nds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type=\"GZIP\")\r\n\r\n\r\ndef pack_row(*row):\r\n  label = row[0]\r\n  features = tf.stack(row[1:],1)\r\n  return features, label\r\n\r\npacked_ds = ds.batch(10000).map(pack_row).unbatch() #think this does (none, none) but not sure what this means\r\n\r\nfor features,label in packed_ds.batch(1000).take(1):\r\n  print(features[0])\r\n  plt.hist(features.numpy().flatten(), bins = 101)\r\nprint(features[0].shape)\r\n\r\nN_VALIDATION = int(1e3)\r\nN_TRAIN = int(1e4)\r\nBUFFER_SIZE = int(1e4)\r\nBATCH_SIZE = 500\r\nSTEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\r\n\r\nvalidate_ds = packed_ds.take(N_VALIDATION).cache()\r\ntrain_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()\r\n\r\nprint(train_ds)\r\n\r\nvalidate_ds = validate_ds.batch(BATCH_SIZE)\r\n#validate_ds = np.array(validate_ds) #model.fit expects x and y to be numpy array. Seems like you pass a list, it tried to get shape of input by reading ndim attribute of numpy array and failed\r\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)\r\n#train_ds = np.array(train_ds) #model.fit expects x and y to be numpy array. Seems like you pass a list, it tried to get shape of input by reading ndim attribute of numpy array and failed\r\n\r\nprint(train_ds)\r\n\r\n\r\n\r\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\r\n  0.001,\r\n  decay_steps=STEPS_PER_EPOCH*1000,\r\n  decay_rate=1,\r\n  staircase=False)\r\n\r\ndef get_optimizer():\r\n  return tf.keras.optimizers.Adam(lr_schedule)\r\n\r\nstep = np.linspace(0,100000)\r\nlr = lr_schedule(step)\r\nplt.figure(figsize = (8,6))\r\nplt.plot(step/STEPS_PER_EPOCH, lr)\r\nplt.ylim([0,max(plt.ylim())])\r\nplt.xlabel('Epoch')\r\n_ = plt.ylabel('Learning Rate')\r\n#plt.show(block=True)\r\n\r\ndef get_callbacks(name):\r\n  return [\r\n    tfdocs.modeling.EpochDots(),\r\n    keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\r\n    keras.callbacks.TensorBoard(logdir/name),\r\n  ]\r\ndef compile_and_fit(model, name, optimizer=None, max_epochs=10000):\r\n  if optimizer is None:\r\n    optimizer = get_optimizer()\r\n  model.compile(optimizer=optimizer,\r\n                loss=keras.losses.BinaryCrossentropy(from_logits=True),\r\n                metrics=[\r\n                  keras.losses.BinaryCrossentropy(\r\n                      from_logits=True, name='binary_crossentropy'),\r\n                  'accuracy'])\r\n\r\n  model.summary()\r\n  \r\n  history = model.fit(\r\n    np.array(train_ds), #model.fit expects x and y to be numpy array. Seems like you pass a list, it tried to get shape of input by reading ndim attribute of numpy array and failed\r\n    steps_per_epoch = STEPS_PER_EPOCH,\r\n    epochs=max_epochs,\r\n    validation_data=validate_ds,\r\n    callbacks=get_callbacks(name),\r\n    verbose=0)\r\n  return history\r\n\r\nsize_histories = {}\r\n\r\n\r\nplotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\r\n#plotter.plot(size_histories)\r\n#plt.ylim([0.5, 0.7])\r\n\r\nsmall_model = Sequential([\r\n    # `input_shape` is only required here so that `.summary` works.\r\n    layers.Dense(16, activation='elu', input_shape=(28,)),\r\n    layers.Dense(16, activation='elu'),\r\n    layers.Dense(1)\r\n])\r\n\r\nsize_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')`\r\n\r\n\r\nraceback (most recent call last):\r\n  File \"/Users/jasonrae/.vscode/extensions/ms-python.python-2020.3.64983-dev/pythonFiles/ptvsd_launcher.py\", line 48, in <module>\r\n    main(ptvsdArgs)\r\n  File \"/Users/jasonrae/.vscode/extensions/ms-python.python-2020.3.64983-dev/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py\", line 432, in main\r\n    run()\r\n  File \"/Users/jasonrae/.vscode/extensions/ms-python.python-2020.3.64983-dev/pythonFiles/lib/python/old_ptvsd/ptvsd/__main__.py\", line 316, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/Users/jasonrae/Documents/Python Files/Higgs1.py\", line 145, in <module>\r\n    size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')\r\n  File \"/Users/jasonrae/Documents/Python Files/Higgs1.py\", line 120, in compile_and_fit\r\n    verbose=0)\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py\", line 1154, in fit\r\n    batch_size=batch_size)\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training.py\", line 579, in _standardize_user_data\r\n    exception_prefix='input')\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/keras/engine/training_utils.py\", line 135, in standardize_input_data\r\n    'with shape ' + str(data_shape))\r\nValueError: Error when checking input: expected dense_1_input to have 2 dimensions, but got array with shape ()\r\nException ignored in: <function _MemoryCacheDeleter.__del__ at 0x13e9554d0>\r\nTraceback (most recent call last):\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2944, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\nException ignored in: <function _MemoryCacheDeleter.__del__ at 0x13e9554d0>\r\nTraceback (most recent call last):\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2944, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'\r\nException ignored in: <function _RandomSeedGeneratorDeleter.__del__ at 0x13e9557a0>\r\nTraceback (most recent call last):\r\n  File \"/Users/jasonrae/opt/miniconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3009, in __del__\r\nAttributeError: 'NoneType' object has no attribute 'device'", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\nPlease, share colab link or simple standalone code with proper indentation to reproduce the issue in our environment. It helps us in localizing the issue faster.\r\nPlease, fill  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n\r\n", "Hi Ravikyram. No worries. Is this what you need?\r\n\r\n== check python ===================================================\r\npython version: 3.7.4\r\npython branch: \r\npython build version: ('default', 'Aug 13 2019 15:17:50')\r\npython compiler version: Clang 4.0.1 (tags/RELEASE_401/final)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64\r\nos release version: 19.3.0\r\nos platform: Darwin-19.3.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.15.3', ('', '', ''), 'x86_64')\r\nuname: uname_result(system='Darwin', node='Jasons-MBP', release='19.3.0', version='Darwin Kernel Version 19.3.0: Thu Jan  9 20:58:23 PST 2020; root:xnu-6153.81.5~1/RELEASE_X86_64', machine='x86_64', processor='i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple clang version 11.0.0 (clang-1100.0.33.17)\r\nTarget: x86_64-apple-darwin19.3.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== check pips ===================================================\r\nnumpy                  1.18.1    \r\ntensorflow-docs        0.0.0     \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n-bash: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 4, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n\r\n", "Here is the code in txt:\r\n[Higgs1.txt](https://github.com/tensorflow/tensorflow/files/4274220/Higgs1.txt)", "@JSunRae \r\n\r\nI have tried in colab with TF 2.1.0 and i am seeing `IndexError ` .Only change is I have replaced with tensorflow.keras in place of keras while importing.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/8a5eb39a0b42ab141b99df254b098702/untitled699.ipynb). Please, help me with reproducing the issue .Thanks!", "@JSunRae \r\n\r\nAny update on this issue please. Thanks!", "still having so many issues with getting it to run sorry. Its now saying\r\n\r\nAttributeError: 'DatasetV1Adapter' object has no attribute 'unbatch'.", "@JSunRae \r\n\r\nWill it be possible to share the colab link or simple standalone code to reproduce the issue in our environment. It helps in localizing the issue for faster resolution.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37219, "title": "Official tutorial (Text classification with an RNN )code running error !!!", "body": "In the official text tutorial of tensorflow: Text classification with an RNN (https://tensorflow.google.cn/tutorials/text/text_classification_rnn),\r\n```\r\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE)\r\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE)\r\n```\r\nWhen I run the code there is an error:\r\n```\r\n    train_dataset = train_dataset.padded_batch(BATCH_SIZE)\r\nTypeError: padded_batch() missing 1 required positional argument: 'padded_shapes'\r\n```\r\n\r\nMy tensorflow version: 2.1.0rc2\r\nMy python version: 3.7\r\n\r\nWhat should be the correct answer\uff1f\r\n\r\n\r\n", "comments": ["@AITutorials,\r\nI was able to run the code from the above tutorial without any issues. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/1e109000a7657b71ed170b9862f381f6/37219.ipynb). Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "You need tensorflow version 2.2.0 to make the code work. \r\nSince 2.2.0, the padded_shapes argument is not mandatory.\r\n\r\nYou can see a start marked note here https://tensorflow.google.cn/tutorials/text/text_classification_rnn", "I have the same problem today."]}, {"number": 37218, "title": "Fix flaky host_tracer_test with data race", "body": "In test `CollectsTraceMeEventsAsXSpace` added in 883b5becaced22f7dd9e3c23d9d259f55e087cb5, `thread_id` is not properly synchronized between threads, causing the following test flakiness:\r\n\r\n```\r\ntensorflow/core/profiler/internal/cpu/host_tracer_test.cc:150: Failure\r\nExpected equality of these values:\r\n  line.id()\r\n    Which is: 3350296320\r\n  thread_id\r\n    Which is: -944670976\r\n```", "comments": ["Ping @jbaiocchi to review.", "@byronyi Can you please check jbaiocchi's comments and keep us posted? Thanks!", "This is not a synchronization issue as the values are read after joining the auxiliary thread. It is a sign issue. Will fix internally and reject this PR."]}, {"number": 37217, "title": "Implement support for AV1 / AVIF for image files", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1\r\n- Are you willing to contribute it (Yes/No): Yes; but I have very limited time & have little experience with implementing codecs, thus I could only assist in testing.\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIn image processing there are scenarios, especially when you deal with higher resolution images, where compression artifacts actually deteriorates your model training sessions. This specifically applies to old image formats like GIF or JPEG. Storing data in, for example, BMP format however drastically increases disk space usage.\r\n\r\nI propose the implementation of **AV1 / AVIF for image files**, i.e. adding AVIF format to `tf.io.decode_image`. **AVIF** is a **free, open source format**.\r\n\r\nAV1 (see [AV1 wikipedia page](https://en.wikipedia.org/wiki/AV1)) is a next generation video (and image) codec. It is broadly supported by Google, Amazon, Apple, ARM, Cisco, Facebook, IBM, Intel Corporation, Microsoft, Mozilla, Netflix, Nvidia, Samsung Electronics, and others since AV1 is developed and supported by [AOMedia](https://en.wikipedia.org/wiki/Alliance_for_Open_Media), where these and other corporations are contributing to.\r\n\r\nAOMedia's main goal is developing royalty-free video and image formats. Hence, long-term support can be expected. \r\n\r\nAVIF (see [AVIF wikipedia entry](https://en.wikipedia.org/wiki/AV1#AV1_Image_File_Format_(AVIF))) is the image file format of the AV1 codec. It has major benefits compared to older formats like JPEG or PNG -> lower file size while still maintaining a significantly better quality.\r\n\r\nThis [blog post by netflix](https://netflixtechblog.com/avif-for-next-generation-image-coding-b1d75675fe4) demonstrates that AVIF covers little to zero compression artifacts **and** having a lower file size compared to JPEG.\r\n\r\nSee AVIF defintion: https://aomediacodec.github.io/av1-avif/\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo, just an addition.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone interested in image processing. Lower disk usage **and** little to zero compression artifacts benefit everyone in image processing, especially users with an interest for higher resolutions.\r\n", "comments": ["I recommend to use https://github.com/AOMediaCodec/libavif\r\nI use libavif in my software to open/save images in AVIF format.", "@novomesk yes, I also already implemented some solutions with AOMedia's libavif repo.\r\n\r\nHowever, TensorFlow should integrate this directly. If you bypass tf.dataset by loading the data yourself, you loose performance (and, in case you do not \"re-invent the wheel\" and implement buffering / pre-loading yourself, you will be limited to datasets that completely fit into your memory).", "@Daniel451 The AV1/AVIF image format falls into the scope of [tensorflow/io](https://github.com/tensorflow/io) which consists of images formats beyond default png/jpeg/gif. So AV1/AVIF is more suited in [tensorflow/io](https://github.com/tensorflow/io) package.\r\n\r\nI have created an issue https://github.com/tensorflow/io/issues/882 there to track the progress.", "@yongtang thanks! :)"]}]