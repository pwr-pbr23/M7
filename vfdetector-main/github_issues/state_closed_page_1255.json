[{"number": 15496, "title": "Add ws2_32.lib to gcs_dns_cache_test's linkopts", "body": "Needed for Windows", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 15495, "title": "Update mnist.py", "body": "Comment modification", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Thanks @15751064254! Could you fix cla? ", "Pardon? [0,NUM_CLASSES] has NUM_CLASSES+1 numbers!", "ah thanks @snnn! Totally missed that. @15751064254 I will close this PR for now.", "Thanks @tensorflow-jenkins I CLA has been submitted."]}, {"number": 15494, "title": "Enables 0-D indexing for Gather() in TFLite", "body": "Hi,\r\n\r\nThis patch enables use of scalar (0-D) index for Gather() in TFLite.\r\n\r\nSince `Dims<>` still works correctly (which becomes {1, 1, 1, 1}) when calculating a scalar tensor,\r\nno actual internal change is needed. Still, it only supports indexing the first dimension.\r\n\r\nAt [here](https://github.com/tensorflow/tensorflow/compare/master...scottcjt:gather_0d?expand=1#diff-3aa3a18bc0cba3f7fe969252dbd8ed09L51), `int32` is changed to `int32_t` because my clang on OSX 10.12 complains that `int32` is undefined.", "comments": ["Can one of the admins verify this patch?", "Ping. Any update on this?\r\n\r\nI have several other changes willing to PR back to official. Any suggestion or official roadmap is appreciated so that I can adjust my commits if needed.", "Jenkins, test this please. ", "Thanks. Are those CI tasks automatic or they just got stuck?", "Hello is anybody still there?", "Hi @yifeif, I'm not familiar with the merging flow. \r\nSince you assign me as the reviewer, could you help to merge this or assign someone? Thanks. "]}, {"number": 15493, "title": "Add S3 logging to TensorFlow's logging system", "body": "This fix is an attempt to help the issue raised in #15159 where there is no logging in S3 file system and it is not easy to debug to diagnose.\r\n\r\nThis fix adds S3 logging to TensFlow's logging with\r\n```\r\nLogLevel::Info -> INFO\r\nLogLevel::Warn -> WARNING\r\nLogLevel::Error -> ERROR\r\nLogLevel::Fatal -> FATAL\r\n```\r\n\r\nThis fix is related to #15159.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@drpngx Thanks for the review. The PR has been updated. Please take a look.", "I see, sorry I didn't spot that, but it looks like `s3_logging` is more like `aws_logging` (including classes and tag). Could you make that change? Move `s3_logging` to `aws_logging` and change everything from `s3` to `aws` in that file and change the tag name, Thanks.", "Thanks @drpngx. The PR has been updated with naming changed. Please take a look.", "Thank you!\r\n\r\nJenkins, test this please.", "Sigh. This seems to trigger an extra log message that foils the debug.\r\n\r\n```\r\n\r\nERROR: offline_analyzer output didn't match expectation: 2017-12-27 23:08:24.429427: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library\r\nERROR: dump_dir flag is empty.\r\nExpected output: ERROR: dump_dir flag is empty.\r\n================================================================================\r\n```\r\n", "Looking at the test, I think we should suppress that line from the input (any log info) before we compare with the goldens. Could you do that?", "Jenkins, test this please.", "/CC @gunan out of disk. I'm wondering if we need to add more quota.\r\n\r\n```\r\ndevmapper: Thin Pool has 968455 free data blocks which is less than minimum required 983040 free data blocks. Create more free space in thin pool or use dm.min_free_space option to change behavior\r\n```", "Jenkins, test this please.", "Looks like it still has an issue, could you check locally?\r\n\r\n```\r\nTesting offline_analyzer\r\n\r\nERROR: offline_analyzer output didn't match expectation: 2017-12-29 00:07:58.941632: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library\r\nERROR: dump_dir flag is empty.\r\nExpected output: ERROR: dump_dir flag is empty.\r\n```", "Thanks @drpngx. The PR has been updated. The previous failures should have been fixed now.\r\n\r\nThere is a new failure of `GPU Python3 \u2014 Internal CI build failed` without any detail. Maybe it is from infrastructure failure and is unrelated?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "Woohoo!"]}, {"number": 15492, "title": "build issue: invalid paths", "body": "### System information\r\nLinux ubuntu 16.04\r\nBazel 0.9.0\r\nCUDA 9.1\r\ncuDNN 7\r\nTF Branch r1.4\r\n\r\nI am getting the following errors / warnings using the set-up above, whilst trying to build the python packages.\r\n\r\n```\r\nfrancesco@franny:~/Repositories/tensorflow$ git checkout r1.4\r\nBranch 'r1.4' set up to track remote branch 'r1.4' from 'origin'.\r\nSwitched to a new branch 'r1.4'\r\nfrancesco@franny:~/Repositories/tensorflow$ ./configure \r\nExtracting Bazel installation...\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/francesco/.cache/bazel/_bazel_francesco/install/754ae0b065b3dfe883541ff567ae8b5e/_embedded_binaries/A-server.jar) to field java.nio.Buffer.address\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nYou have bazel 0.9.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]: n\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]5.2\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\nfrancesco@franny:~/Repositories/tensorflow$\r\n\r\nfrancesco@franny:~/Repositories/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n.........\r\nERROR: /home/francesco/.cache/bazel/_bazel_francesco/4ce2bc3731d0d87739dc505f1772132b/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/francesco/.cache/bazel/_bazel_francesco/4ce2bc3731d0d87739dc505f1772132b/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/francesco/.cache/bazel/_bazel_francesco/4ce2bc3731d0d87739dc505f1772132b/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/home/francesco/.cache/bazel/_bazel_francesco/4ce2bc3731d0d87739dc505f1772132b/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/home/francesco/.cache/bazel/_bazel_francesco/4ce2bc3731d0d87739dc505f1772132b/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /home/francesco/.cache/bazel/_bazel_francesco/4ce2bc3731d0d87739dc505f1772132b/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'\r\nERROR: /home/francesco/Repositories/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 13.322s\r\nFAILED: Build did NOT complete successfully (93 packages loaded)\r\n    currently loading: tensorflow/core/kernels ... (2 packages)\r\n\r\n```\r\n\r\nAny help?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hello, I experienced the same issue as OP and I believe this is a conflict with Bazel version 0.9.\r\n\r\nMy setup: \r\n  * Have I written custom code:  No\r\n  * OS Platform and Distribution: Linux Ubuntu 16.04\r\n  * TensorFlow installed from: Source\r\n  * TensorFlow version 1.4.1\r\n  * Bazel version: 0.9.0\r\n  * CUDA/cuDNN version 9.0 / 7.0.3\r\n  * GPU model and memory: Titan X (12 GB)\r\n  * Exact command to reproduce: `bazel build -c opt --copt=-mfpmath=both --copt=-msse4.2 --config=cuda -k //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI noticed that I recently upgraded Bazel to 0.9 and that OP was also using Bazel 0.9. Problem resolved after downgrading Bazel to 0.8.1.\r\n\r\n", "I've tried to build both under Max OSX 10.13.2 and under Ubuntu 16.04 LTS and are seeing similar errors under both. Here are the errors from Ubuntu:\r\n\r\njonathan@ubuntu:~/applications/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/home/jonathan/.cache/bazel/_bazel_jonathan/1d3b816549b2ceeb51a746e232f12c69/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /home/jonathan/applications/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@com_googlesource_code_re2//:LICENSE' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: /home/jonathan/applications/tensorflow/tensorflow/tools/pip_package/BUILD:101:1: Target '@local_config_sycl//sycl:LICENSE.text' contains an error and its package is in error and referenced by '//tensorflow/tools/pip_package:licenses'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 0.932s\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/contrib/layers ... (10 packages)\r\n", "I have had the same error. I have troubleshooted my cuda and cudnn to make sure they work, but get essentially the same error. I rolled bazel back, as mentioned above, and moved on to the next build error.\r\n\r\nHave I written custom code: No \r\nOS Platform and Distribution: Ubuntu 16.04 (on AWS)\r\nTensorFlow installed from: Github\r\nTensorFlow version: v1.4.0\r\nBazel version: 0.9\r\nCUDA/cuDNN version: 9.1/7.0.5\r\nGPU model and memory: K80 12GB\r\nExact command to reproduce: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`\r\n~~~~\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'\r\nERROR: /home/ubuntu/tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 4.766s\r\nFAILED: Build did NOT complete successfully (96 packages loaded)\r\n    currently loading: tensorflow/core/kernels ... (2 packages)\r\n~~~~", "In the fix posted above, I deleted my bazel cache.  I didn't mention this because I didn't think it was a necessary step. But it appears your errors are caused by files in the cache. I would check that these files have existed since before you rolled  back Bazel and if so, I would delete the whole cache. ", "I purged all bazel files (cache included) and reinstalled 0.9 twice before rolling back. ", "I was having a similar problem on Ubuntu 16.04, Bazel 0.9.0. Ended up fixing it by adding \r\n\"`--incompatible_load_argument_is_label=false`\" \r\noption to the build command as suggested in the error message. So on my system the entire build command on was,\r\n\r\n`bazel build --config=opt --config=cuda --config=mkl --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nThe build completed successfully after that. You may want to remove the \"`--config=mkl`\" option (or check out [this](https://github.com/01org/mkl-dnn) repository if you are interested in building with Math Kernal Library as well)", "I just ran into this yesterday, so yes, it's still an issue.", "Same here...can someone please add an \"awaiting tensorflower\" tag?", "Hit the same problem yesterday (building r1.4), after adding `--incompatible_load_argument_is_label=false` the compilation finished successfully.", "I ran into this issue on Ubuntu 16.04 LTS bazel 0.9.0 and 0.8.1 without Cuda. The fix suggested above worked.", "same issue on jan 23\r\nubuntu 16.04 cuda9.1 cudnn7.04 bazel 0.9\r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ngit checkout r1.4\r\n./configure\r\n bazel build -c opt --copt=-march=\"haswell\" --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1\r\n vi pip_package_build2.log   \r\n                          clearly failed to build..usual message about bazel caches\r\n bazel build -c opt --copt=-march=\"haswell\" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1\r\n vi pip_package_build2.log \r\nworked\r\n", "I can confirm that I am seeing the same issue. Tensorflow v.1.4.1, and Bazel 0.9.0.\r\n\r\nAdding `--incompatible_load_argument_is_label=false` lets the build process go past this issue, but that's of course not the right fix.", "I also saw same issue with bazel 0.9.0 building tensorflow 1.4.1 on Ubuntu 17.10, used `--incompatible_load_argument_is_label=false` and got past it.", "Not sure what qualifies as an issue on here. The `--incompatible_load_argument_is_label=false` option seems to be getting everyone past the problem, and that option is mentioned in the error message already provided by bazel. Would be nice if the reason for the error was made more clear and the solution was displayed more prominently in the error message though", "I would expect that doing a default build of TF by following the official guide  from TF webpage should go smoothly without any errors and needs for weird workarounds.", "I built current top of tree (1.8RC1) with bazel 0.11.0 without any\nadditional flags\n\n\nOn Wed, May 2, 2018 at 11:54 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 29 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-386081980>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIUuT5GHm6aa-AqYuPnjo6zpjGomp08_ks5tugDegaJpZM4RHdW7>\n> .\n>\n", "build without additional flags on R1.8 (git checkout) and bazel 0.13 worked\nfine\nd\n\n\nOn Thu, May 17, 2018 at 11:58 AM, Alfred Sorten Wolf <\nnotifications@github.com> wrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-389971552>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIUuTzSgxW0Fkyx6QCg_NTMbhTmh69sSks5tzchtgaJpZM4RHdW7>\n> .\n>\n", "hello, I have the same problem,\r\nHave I written custom code: No\r\nOS: Ubuntu 16.04\r\nTensorFlow installed from: Source\r\nTensorFlow version: r1.4\r\nBazel version: 0.9.0 (version 0.13.0 fails to check that it is superior to version 0.4.5 -.-)\r\nCUDA: 8.0\r\ncuDNN: 5\r\nGPU model and memory: gtx 1050 TI (notebook) 2001MB\r\nDriver: 384.130 (other drivers cause conflict -.-)\r\nExact command to reproduce:\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 tensorflow:libtensorflow_cc.so\r\n\r\nusing --incompatible_load_argument_is_label=false \"solves\" the problem", "go to a newer version of TF..the issue is driven by your version of Bazel\nno problems with TF R1.8 and Bazel 0.13 as of about a week ago\nd\n\n\nOn Fri, May 18, 2018 at 3:28 PM, JosephIWB <notifications@github.com> wrote:\n\n> hello, I have the same problem,\n> Have I written custom code: No\n> OS: Ubuntu 16.04\n> TensorFlow installed from: Source\n> TensorFlow version: r1.4\n> Bazel version: 0.9.0 (version 0.13.0 fails to check that it is superior to\n> version 0.4.5 -.-)\n> CUDA: 8.0\n> cuDNN: 5\n> GPU model and memory: gtx 1050 TI (notebook) 2001MB\n> Driver: 384.130 (other drivers cause conflict -.-)\n> Exact command to reproduce:\n> bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma\n> --copt=-mfpmath=both --copt=-msse4.2 tensorflow:libtensorflow_cc.so\n>\n> using --incompatible_load_argument_is_label=false \"solves\" the problem\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-390348450>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIUuT94C4HgKLdbUVXid4EDKUUbsgbz9ks5tz0skgaJpZM4RHdW7>\n> .\n>\n", "I can't, I don't have cuda 9 in my computer for work reasons, and I have to\nuse cuda 8 for compatibility with other codes I use at work, so it must be\ntf 1.4 (as I understand, later versions need cuda 9)\n\nOn Fri, May 18, 2018 at 7:14 PM, David Levinthal Ph.D. <\nnotifications@github.com> wrote:\n\n> go to a newer version of TF..the issue is driven by your version of Bazel\n> no problems with TF R1.8 and Bazel 0.13 as of about a week ago\n> d\n>\n>\n> On Fri, May 18, 2018 at 3:28 PM, JosephIWB <notifications@github.com>\n> wrote:\n>\n> > hello, I have the same problem,\n> > Have I written custom code: No\n> > OS: Ubuntu 16.04\n> > TensorFlow installed from: Source\n> > TensorFlow version: r1.4\n> > Bazel version: 0.9.0 (version 0.13.0 fails to check that it is superior\n> to\n> > version 0.4.5 -.-)\n> > CUDA: 8.0\n> > cuDNN: 5\n> > GPU model and memory: gtx 1050 TI (notebook) 2001MB\n> > Driver: 384.130 (other drivers cause conflict -.-)\n> > Exact command to reproduce:\n> > bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma\n> > --copt=-mfpmath=both --copt=-msse4.2 tensorflow:libtensorflow_cc.so\n> >\n> > using --incompatible_load_argument_is_label=false \"solves\" the problem\n> >\n> > \u2014\n> > You are receiving this because you commented.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 15492#issuecomment-390348450>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> AIUuT94C4HgKLdbUVXid4EDKUUbsgbz9ks5tz0skgaJpZM4RHdW7>\n> > .\n> >\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-390355264>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AkbU2bPa9jEB-TmWESO8mY4AirE9gyVAks5tz1XUgaJpZM4RHdW7>\n> .\n>\n\n\n\n-- \nAtte. Jos\u00e9 Ignacio Wielandt\nLicenciado en F\u00edsica\nPontificia Universidad Cat\u00f3lica de Chile\n", "I find r1.8 and r1.9 rc0 both install with bazel 13 without the need for\nextra flags\nI'll speak up if nobody else will\nprobably fine to close..\n:-)\nd\n\nOn Mon, Jul 2, 2018 at 6:00 PM, Alfred Sorten Wolf <notifications@github.com\n> wrote:\n\n> It has been 44 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-401980656>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIUuT68wanMBPZ4C4R8viwkSVjm6Y2qYks5uCsI9gaJpZM4RHdW7>\n> .\n>\n", "```\r\nERROR: Unrecognized option: --incompatible_load_argument_is_label=false\r\n```", "@formigone same issue here.", "hit `ERROR: Unrecognized option: --incompatible_load_argument_is_label=false` on bazel `0.16.1`. downgrading to `0.16.0` also hit this. \r\n\r\nAccording to https://blog.bazel.build/2018/04/11/bazel-0.12.html\r\nthis flag was removed in `0.12`\r\n\r\nThen downgrading to `0.11.1` solved this issue.", "not with recent drops of TF..build works without additional flags with\nrecent Bazel updates\nfeel free to close this\n\n\nOn Fri, Sep 7, 2018 at 11:58 AM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 15 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15492#issuecomment-419532549>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AIUuTxo0DJnVwb6uskzxTZn0M_1r4Gjpks5uYsHmgaJpZM4RHdW7>\n> .\n>\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Can anyone respond if this issue still exist ?"]}, {"number": 15491, "title": "Branch 179578952", "body": "Push", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Thanks @ekelsen!"]}, {"number": 15490, "title": "Cannot reshape with shape=[tensorshapes...]", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Debian 9.3\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\n- **Python version**: \r\n2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n6.0.21\r\n- **GPU model and memory**:\r\nGTX titan X (pascal) 12GB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI have several Conv layers and after the last max pooling step, I want to flatten the last pooled layer shape=(Batchsize, height, width, channels) to (Batchsize, -1). It returned error saying some dims are None.  \r\nI tried to print the BS and other_dims, it can be printed to int numbers, none of them are None\r\nIf I entered explicit int numbers tf.reshape(x, [2,10]) it will work.\r\n# top MLP\r\n        shape = tf.shape(pooled_h[-1])\r\n        BS = shape[0]\r\n        other_dims = shape[1] * shape[2] * shape[3]\r\n        last_pooled_flat = tf.reshape(pooled_h[-1], [BS, other_dims])  # (BS, n_flat_feats)\r\n        mlp_h = [last_pooled_flat]  # to store mlp hidden layers\r\n# MLP alyers\r\n        for j in range(self.n_mlp_layers):\r\n            h = tf.layers.dense(\r\n                inputs=mlp_h[j],\r\n                units=self.hidden_sizes[i],\r\n                activation=tf.nn.relu,\r\n                kernel_initializer=tf.contrib.layers.xavier_initializer(\r\n                            uniform=True, seed=None, dtype=tf.float32),\r\n                bias_initializer=tf.zeros_initializer(),\r\n                kernel_regularizer=L2_regularizer,\r\n                bias_regularizer=L2_regularizer,\r\n                name=\"mlp_{}\".format(j),\r\n                reuse=False)\r\n            mlp_h.append(h)\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nTraceback (most recent call last):\r\n  File \"basemodel_builder.py\", line 143, in <module>\r\n    out = model.apply_rel(x_q, x_d, 1.0)\r\n  File \"basemodel_builder.py\", line 117, in apply_rel\r\n    reuse=False)\r\n  File \"/u/nieyifan/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 215, in dense\r\n    return layer.apply(inputs)\r\n  File \"/u/nieyifan/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/u/nieyifan/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 443, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/u/nieyifan/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/core.py\", line 109, in build\r\n    raise ValueError('The last dimension of the inputs to `Dense` '\r\n\r\n", "comments": ["How about using  tf.TensorShape([BS, other_dims])? ", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 146 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15489, "title": "Hotfix/fix android example for focus mode continuous picture - #15487", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "ping @andrewharp "]}, {"number": 15488, "title": "Bad access error when deserializing a fully connected TF Lite model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n`('v1.3.0-rc1-5910-ge2174cc943', '1.4.0')`\r\n\r\n### Describe the problem\r\nI trained a very simple feed forward network (model structure is y = W*x + b so basically linear regression) using raw tensorflow code (matrix multiply, bias add and relu) so not using any Estimators or higher order APIs. My main purpose was to see if I could serialize the model, convert it to .lite format and then deserialize it correctly in C++.\r\n\r\nI did freeze_graph on this graph.pbtxt and model checkpoints using tensorflow's provided freeze_graph method, and converted to .lite format using the bazel command line instruction (in the lite documentation) Lite conversion command used is\r\n\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=frozen_model.pb  \\               \r\n --output_file=frozen_model.lite \\               \r\n --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=**1,1** \\ \r\n --input_array=**x** \\          \r\n --output_array=**output**\r\n``` \r\nAs x is the name of the input 1-Dim placeholder tensor, output is the name of the result of W*x+b. I did not include y as part of the input_array as running with --input_array=x,y, --input_shape=1,1:1,1 as this produced a .lite model with 0 bytes (no output)\r\n\r\nUsing code very similar to what is in tflite_driver.cc and tflite_driver_test.cc, I was able to get the mobile_net example working fine but this simple model I described above fails with a EXC_BAD_ACCESS error. Upon debugging I saw that the error happens at fully_conncted.cc\r\n\r\n```\r\ntemplate <KernelType kernel_type>\r\nTfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\r\n....\r\n\u2002\u2002switch (input->type) {\u2002\u2002// Already know in/out types are same.\r\n\u2002\u2002\u2002\u2002case kTfLiteFloat32:\r\n\u2002\u2002\u2002\u2002\u2002\u2002return EvalFloat<**kernel_type**>(context, node, params, data, input, filter,\r\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002bias, output);\r\n....\r\n\u2002\u2002}\r\n\u2002\u2002return kTfLiteOk;\r\n}\r\n\r\n```\r\n\r\nkernel_type is **kPie** at this point. Looking further into fully_connected.cc, I notice two things.\r\n\r\n```\r\nenum KernelType {\r\n  kReference,\r\n  kGenericOptimized,  // Neon-free\r\n  kNeonOptimized,\r\n  kPie,  **// Used by the PIE team**\r\n};\r\n\r\n```\r\nAnd also\r\n\r\n```\r\nTfLiteRegistration* Register_FULLY_CONNECTED() {\r\n\u2002\u2002**// TODO(ahentz): We don't have a dedicated quantized version of the PIE\r\n\u2002\u2002// kernel. For now, the quantized version just defer to the corresponding\r\n\u2002\u2002// optimized MINI kernel. At some point we will allow different libraries to\r\n\u2002\u2002// be built with different kernels, but for now we have to pick one here.\r\n\u2002\u2002return Register_FULLY_CONNECTED_PIE();**\r\n#ifdef USE_NEON\r\n\u2002\u2002return Register_FULLY_CONNECTED_NEON_OPT();\r\n#else\r\n\u2002\u2002return Register_FULLY_CONNECTED_GENERIC_OPT();\r\n#endif\r\n}\r\n\r\n```\r\nSpecifically, the EXC_BAD_ACCESS error happens at\r\n\r\n```\r\nTfLiteStatus EvalPie(TfLiteContext* context, TfLiteNode* node,\r\n                     ...\r\n  // Output = bias if bias tensor exists.\r\n  if (bias) {\r\n    tensor_utils::VectorBatchVectorAssign(bias->data.f, num_units, batch_size,\r\n                                          output->data.f);\r\n  } \r\n```\r\n\r\n![upload](https://user-images.githubusercontent.com/3603839/34171209-63703e96-e4a2-11e7-89fd-446b1c1c44cb.png)\r\n\r\nAs we can see the filter and bias terms have bad memory addresses, the input and output tensors have valid memory addresses though\r\n\r\nMy questions are \r\n1) Why is kPie kernel_type being picked for my simple model?\r\n2) Based on the comments it looks like kPie is not supported for external use, so could this explain the bad access error?\r\n3) Is this an issue with how I converted to lite format? Here is my command again\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=frozen_model.pb  \\               \r\n --output_file=frozen_model.lite \\               \r\n --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --inference_type=FLOAT \\\r\n  --input_shape=**1,1** \\ \r\n --input_array=**x** \\          \r\n --output_array=**output**\r\n``` \r\n4) Is there any example of a very simple model composed out of matmul, add and relu which can be converted to .lite in such a way that it can be correctly deserialized in C++, just like the mobile_net example? I can build up from such an example\r\n", "comments": ["Hey guys, wanted to add an update to this as I tried to get something much simpler to work. Please let me know if there is something not quite right in what I attempted.\r\n\r\nI installed TF 1.5 from source.\r\n\r\nThis is the simplest possible DNN model - no hidden layer, dropout, or activation function, bias and weight are 1 dimensional.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n\r\nmodel_dir = \"simple_lr_ckpt/\"\r\nx = tf.placeholder(tf.float32, shape=[None,1], name=\"x\")\r\ny = tf.placeholder(tf.float32, shape=[None,1], name=\"y\")\r\nW = tf.Variable(tf.truncated_normal(shape=[1, 1], mean=0, stddev=0.001, dtype=tf.float32), name=\"W\")\r\nb = tf.Variable(tf.truncated_normal(shape=[1], mean=0, stddev=0.001, dtype=tf.float32), name=\"b\")\r\noutput_name = \"logit\"\r\nlogit = tf.add(tf.matmul(x, W), b, name=output_name)\r\nloss = tf.reduce_mean(tf.square(logit - y))\r\ntf.summary.scalar('loss', loss)\r\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\r\nmerged = tf.summary.merge_all()\r\nN = 1000000\r\nbatch_size = 100\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    train_writer = tf.summary.FileWriter(model_dir, sess.graph)\r\n    for start in xrange(0, N, batch_size):\r\n        x_ = np.reshape(np.random.rand(batch_size), (-1,1))\r\n        y_ = 2*x_ + 1\r\n        _, mse, summary = sess.run([train_op, loss, merged], feed_dict={x: x_, y: y_})\r\n        if start % (batch_size*100) == 0:\r\n            print mse\r\n        # if start % batch_size * 100 == 0:\r\n            saver.save(sess, model_dir + \"model.ckpt-old\", global_step=start)\r\n        train_writer.add_summary(summary, start)\r\n\r\n    tf.train.write_graph(sess.graph_def, model_dir, 'graph.pbtxt')\r\n    frozen_graph_def = convert_variables_to_constants(sess, sess.graph_def, [output_name])\r\n\r\n    with tf.gfile.GFile(model_dir+ \"graph.pb\", \"w\") as f:\r\n        f.write(frozen_graph_def.SerializeToString())\r\n```\r\n\r\nTo freeze the graph this is the code I used\r\n\r\n```\r\nimport os\r\nfrom tensorflow.python.tools import freeze_graph\r\nimport tensorflow as tf\r\n\r\ndir(tf.contrib)\r\noutput_path = model_dir\r\ninput_graph_path = os.path.join(output_path, \"graph.pbtxt\")\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = None\r\noutput_graph_path = os.path.join(output_path, \"frozen_graph.pb\")\r\nclear_devices = False\r\nfreeze_graph.freeze_graph(\r\n    input_graph_path, input_saver_def_path, input_binary, tf.train.latest_checkpoint(model_dir),\r\n    output_name, restore_op_name, filename_tensor_name,\r\n    output_graph_path, clear_devices, \"\", \"\")\r\n```\r\n\r\nAfter this, I convert the .pb file to .lite using the following\r\n\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\                                                                        \r\n  --input_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.pb \\\r\n  --output_format=TFLITE \\\r\n  --output_file=/Users/ushnishde/tf_lite_example_new/simple_lr_ckpt/frozen_graph.lite \\\r\n  --inference_type=FLOAT \\\r\n  --inference_input_type=FLOAT \\\r\n  --input_arrays=x \\\r\n  --output_arrays=logit \\\r\n  --input_shapes=1,1\r\n`\r\n\r\nI kept the names of the tensors consistent from when I created the model. Is the y tensor considered as one of the input_arrays?\r\n\r\n\r\nWhen I try to deserialize the .lite model in C++, I followed the code statements which ultimately lead to the EXC_BAD_ACCESS error\r\n\r\ntflite::Interpreter::Invoke() interpreter.cc:414\r\n<img width=\"522\" alt=\"screen shot 2018-01-03 at 12 32 01 am\" src=\"https://user-images.githubusercontent.com/3603839/34513536-8fc5d04c-f01d-11e7-9ffa-df64ccf53545.png\">\r\n\r\nTfLiteStatus tflite::ops::builtin::fully_connected::Eval<(tflite::ops::builtin::fully_connected::KernelType)3>(TfLiteContext*, TfLiteNode*) fully_connected.cc:250\r\n<img width=\"664\" alt=\"screen shot 2018-01-03 at 12 32 37 am\" src=\"https://user-images.githubusercontent.com/3603839/34513554-a37ad13c-f01d-11e7-8a05-b3768ac7612e.png\">\r\n\r\nTfLiteStatus tflite::ops::builtin::fully_connected::EvalFloat<(tflite::ops::builtin::fully_connected::KernelType)3>(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:226\r\n<img width=\"652\" alt=\"screen shot 2018-01-03 at 12 33 15 am\" src=\"https://user-images.githubusercontent.com/3603839/34513573-b61ae930-f01d-11e7-9fa5-c58c6ba74d14.png\">\r\n\r\ntflite::ops::builtin::fully_connected::EvalPie(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) fully_connected.cc:148\r\n<img width=\"647\" alt=\"screen shot 2018-01-03 at 12 33 45 am\" src=\"https://user-images.githubusercontent.com/3603839/34513588-c9a6ea08-f01d-11e7-8b07-16b40857b3fe.png\">\r\n\r\ntflite::tensor_utils::VectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.h:147\r\n<img width=\"513\" alt=\"screen shot 2018-01-03 at 12 34 16 am\" src=\"https://user-images.githubusercontent.com/3603839/34513599-dca703d6-f01d-11e7-8bed-b5a83ab9fa74.png\">\r\n\r\ntflite::tensor_utils::PortableVectorBatchVectorAssign(float const*, int, int, float*) portable_tensor_utils.cc:104\r\n<img width=\"524\" alt=\"screen shot 2018-01-03 at 12 34 54 am\" src=\"https://user-images.githubusercontent.com/3603839/34513615-f2a41b24-f01d-11e7-87c4-ef2be1638d84.png\">\r\n\r\n```\r\nvoid PortableVectorBatchVectorAssign(const float* vector, int v_size,\r\n                                     int n_batch, float* batch_vector) {\r\n  for (int b = 0; b < n_batch; b++) {\r\n    memcpy(batch_vector + b * v_size, vector, v_size * sizeof(float));\r\n  }\r\n}\r\n\r\n```\r\n<img width=\"554\" alt=\"screen shot 2018-01-03 at 12 35 48 am\" src=\"https://user-images.githubusercontent.com/3603839/34513633-10fa2c3a-f01e-11e7-93de-7bcdf1a23f87.png\">\r\n\r\nAny ideas what could be going on? I'm eager to try any suggestions at this point.\r\n  ", "@aselle any ideas on how we can proceed forward? Thank you. ", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ushnish We have been unable to reproduce this. It seems to work on Android1, Nexus5 and Nexus-6p phones. Is there anything special about your setup?", "Hello Andre, thanks for responding. I am working on a MacBook Pro, High Sierra. I use CLion as my IDE. ", "> I kept the names of the tensors consistent from when I created the model. Is the y tensor considered as one of the input_arrays?\r\n\r\nI don't think so. You'll have to write --input_arrays=x,y  and define their shapes too.\r\n\r\nBTW, this should be unrelated to the kPie kernel. You can replace it with the kReference for testing, but I think the issue is with the way you are populating input arrays.  Maybe you can provide us with the .pb file you are using?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ushnish, were you able to resolve your issue?", "Unfortunately I was not able to resolve this issue. Has anyone tried to run the end-to-end example I provided? I\u2019ll be happy to revisit this issue if the example I provided works now  ", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 77 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 92 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I just went over your example, and it should work without issues. \r\n\r\nMy guess is that your inference code is not holding on to the flatbuffer bytes (or of the FlatBufferModel) for the whole duration of inference. \r\n\r\nHere's what I've done:\r\n\r\n- Conversion:\r\n```\r\nbazel run tensorflow/contrib/lite/toco:toco -- --input_format=TENSORFLOW_GRAPHDEF \\\r\n   --input_file=frozen_graph.pb --inference_type=FLOAT --inference_input_type=FLOAT \\\r\n   --input_arrays=x --output_arrays=logit --input_shapes=1,1 \\\r\n   --output_file=frozen_graph.tflite\r\n```\r\n- Accuracy (vs TF):\r\n```\r\nbazel run tensorflow/contrib/lite/testing:tflite_diff --  --tensorflow_model=frozen_graph.pb \\\r\n   --tflite_model=frozen_graph.tflite --input_layer=x --input_layer_type=float \\\r\n   --input_layer_shape=1,1 --output_layer=logit\r\n```\r\n- Latency\r\n```\r\nbazel run tensorflow/contrib/lite/tools/benchmark:benchmark_model -- \\\r\n    --num_threads=4 --num_runs=1000  --graph=frozen_graph.tflite\r\n```"]}, {"number": 15487, "title": "Android Example breaks for old cameras not having support for FOCUS_MODE_CONTINUOUS_PICTURE", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nNo fixed a bug.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nUbuntu 16.04, Tried on Android 21,22,23,24,25\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nAndroid App\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\nLatest in Android App\r\n\r\n- **Python version**: \r\n\r\n3.2\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\nNot Applicable\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n\r\nNot Applicable\r\n\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\nNot Applicable\r\n\r\n\r\n- **GPU model and memory**:\r\n\r\nNot Applicable\r\n\r\n- **Exact command to reproduce**:\r\n\r\nCompile the Android Example as it is and execute on any Android device with old camera not supporting FOCUS_MODE_CONTINUOUS_PICTURE\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nAndroid Example breaks for old cameras not having support for FOCUS_MODE_CONTINUOUS_PICTURE\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nCreating a pull request for the fix.\r\n", "comments": ["Pull request sounds good."]}, {"number": 15486, "title": "fix _Pooling1D data format bug", "body": "When `data_format` is `channels_last`, input is `NWC`, so we have to `expand_dim(1)` to make it become `NHWC`. Then we apply pooling on `W` which is the 3rd dimention.\r\n\r\nWhen `data_format` is `channels_first`, input is `NCW`, so we have to `expand_dim(2)` to make it become `NCHW`. Then we apply pooling on `W`, which is the 4th dimention.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for your PR @stegben. We no longer accept changes to released branches, so I will go ahead and close this PR. Did you mean to send the PR to master? If yes, could you start another one against master branch?", "Oh, my bad, I'll PR on the master"]}, {"number": 15485, "title": "Possible Bug with GPU: matmul_op.cc ", "body": "Hello,\r\n\r\nI am trying to run a matrix multiplication on GPU. This is the code i am running on python. \r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n  c = tf.matmul(a, b)\r\n\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nprint(sess.run(c))\r\n```\r\n\r\nIn the matmul_op.cc i have added an ```std::cout << \"Device: \" << ctx->device()->name << endl;``` in function ```Compute``` that is on line 456. The weird thing is that this ```cout``` prints: ```Device:  /job:localhost/replica:0/task:0/device:CPU:0 ``` while the log_device_placement from the session reports: \r\n```\r\n2017-12-19 18:50:35.432196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GT 720 major: 3 minor: 5 memoryClockRate(GHz): 0.797\r\npciBusID: 0000:82:00.0\r\ntotalMemory: 1.95GiB freeMemory: 1.95GiB\r\n2017-12-19 18:50:35.432273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5\r\n2017-12-19 18:50:35.455437: I tensorflow/core/common_runtime/direct_session.cc:300] Device mapping:\r\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GT 720, pci bus id: 0000:82:00.0, compute capability: 3.5\r\n\r\nTensor(\"MatMul:0\", shape=(2, 2), dtype=float32, device=/device:GPU:0)\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-12-19 18:50:35.456469: I tensorflow/core/common_runtime/placer.cc:874] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\r\nb: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-12-19 18:50:35.456525: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\na: (Const): /job:localhost/replica:0/task:0/device:GPU:0\r\n2017-12-19 18:50:35.456555: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\nI have also checked the ```bool USE_CUBLAS``` and it has value 0, in function ```Compute```. Is it a possible bug or i am doing something wrong?", "comments": ["Very likely this is because TF optimizes the graph and pre-compute the results, since all the inputs are constant. It's not a bug.", "So, how i can force a matmul to execute on GPU? ", "Thank you for helping our friend @ppwwyyxx. If someone can find a serious bug in matrix multiplication, then maybe we should have a parade. The closest we've come recently is this heroic optimization by @rmlarsen in https://github.com/tensorflow/tensorflow/commit/49f147388676d77532598afb881b8a3bbf97bd41."]}, {"number": 15484, "title": "[Building Error] clang: error: no such file or directory: 'x86_64'", "body": "Hi all,\r\n\r\nI met this problem when I was trying to build the Tensorflow Lite for iOS following [this instruction](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/ios.md).\r\n\r\n\r\n\r\nEvery thing went well until I tried the command:\r\n`tensorflow/contrib/lite/build_ios_universal_lib.sh`\r\n\r\nThe error occured:\r\n`clang: error: no such file or directory: 'x86_64'`\r\n\r\nWhat could be the problem? Thank you very much!\r\nMy environment: Mac Sierra 10.12.6, XCode 9.2", "comments": ["To fix this, open in XCode > Preferences > Locations and look Command Line Tools. Mine was empty. Selecting a value fixed in that dropdown fixed this issue for me.\r\nCitation : - mikeknapp \r\n[Link](https://github.com/tensorflow/tensorflow/issues/15258)", "@printdhruv Great help! That solved the problem. However, it brings another new error when I retry the same command. It is:\r\n`./tensorflow/contrib/lite/kernels/internal/optimized/optimized_ops.h:28:10: fatal error: 'third_party/eigen3/Eigen/Core' file not found\r\n#include \"third_party/eigen3/Eigen/Core\"`\r\n\r\nI searched it on google but I did not get any solutions", "Can you make sure you run tensorflow/contrib/lite/download_dependencies.sh from the tensorflow repo root directory?", "Thank you @aselle ! That is exactly the problem!\r\nI did not run that script from the tensorflow repo root directory, but the tensorflow/tensorflow directory. After I change to the right location, everything goes just fine.\r\n\r\nProblem solved! Thank you again!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "To fix this, open in XCode > Preferences > Locations and look Command Line Tools. Mine was empty. Selecting a value fixed in that dropdown fixed this issue for me."]}, {"number": 15483, "title": "What is the best practice for running training and evaluation on the same machine?", "body": "I ask the question at [stackoverflow](https://stackoverflow.com/questions/47883570/what-is-the-best-practice-for-running-training-and-evaluation-on-the-same-machin). No answer so far, maybe I can find people have similar needs.\r\n\r\nHere is the question:\r\n\r\n**What I want to do?**\r\n\r\n1. I only have 1 machine. \r\n\r\n2. I want to evaluate the mode periodically. \r\n\r\n**What I have now?** \r\n\r\n\r\n1.  use a placeholder. Say I run 1000 step of training by feeding the training data. then I feed in validation dataset for evaluation. put it in a loop.\r\n\r\n    But as google suggested, placeholder is not a good way for long run training.\r\n\r\n\r\n2. So, I use slim dataset to feed in data. Now, the model is bonded with training dataset like this:\r\n    >      net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID',\r\n    >                                 scope='conv1')\r\n\r\n I have to construct another model(in another graph) which is bonded with validation dataset. \r\n\r\n**Is there a better way of doing that?**\r\n\r\nI know that google is focusing on distribution training on large scale, but I think as tensorflow  is a low-level and flexible framwork. There must be a way can do what I want. ", "comments": ["Answered [on Stack Overflow](https://stackoverflow.com/q/47883570/3574081)."]}, {"number": 15482, "title": "Fix a compile error in file_block_cache_test.cc", "body": "tensorflow/core/platform/cloud/file_block_cache_test.cc(461): error C3493: 'block_size' cannot be implicitly captured because no default capture mode has been specified\r\n\r\nCompiler: Visual Studio 2017 v15.4.4", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15481, "title": "[BUG] \"no viable conversion \" ERROR raised in \"tensorflow/core/kernels/eigen_pooling.h\" when build v1.4.1 for opencl", "body": "### System information\r\n- **OS Platform and Distribution**:Linux Ubuntu 17.10\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**:1.4.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:0.8.1\r\n- **GCC/Compiler version (if compiling from source)**:7.2\r\n\r\n### Describe the problem\r\n\"no viable conversion \" ERROR raised when build v1.4.1 with opencl (computecpp CE 0.5.0)\r\n\r\n### Source code / logs\r\nERROR: .../tensorflow/tensorflow/core/kernels/BUILD:3169:1: C++ compilation of rule '//tensorflow/core/kernels:pooling_ops' failed (Exit 1)\r\nIn file included from tensorflow/core/kernels/pooling_ops_3d.cc:29:\r\n./tensorflow/core/kernels/eigen_pooling.h:338:12: error: no viable conversion from '__m128' (vector of 4 'float' values) to 'cl::sycl::vec<float, 4>'\r\n    Packet skip_mask =\r\n           ^\r\n./tensorflow/core/kernels/eigen_pooling.h:333:5: note: in instantiation of function template specialization 'Eigen::internal::AvgPoolMeanReducer<float>::reducePacketWithType<cl::sycl::vec<float, 4> >' requested here\r\n    reducePacketWithType(static_cast<T>(0), p, accum);\r\n    ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:186:15: note: in instantiation of function template specialization 'Eigen::internal::AvgPoolMeanReducer<float>::reducePacket<cl::sycl::vec<float, 4> >' requested here\r\n      reducer.reducePacket(self.m_impl.template packet<Unaligned>(firstIndex + j), &p);\r\n              ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:279:56: note: in instantiation of member function 'Eigen::internal::InnerMostDimReducer<Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>, Eigen::internal::AvgPoolMeanReducer<float>, true>::reduce' requested here\r\n          InnerMostDimReducer<Self, Op, Vectorizable>::reduce(self, 0, num_coeffs, reducer);\r\n                                                       ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:523:48: note: in instantiation of member function 'Eigen::internal::FullReducer<Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>, Eigen::internal::AvgPoolMeanReducer<float>, Eigen::ThreadPoolDevice, true>::run' requested here\r\n      internal::FullReducer<Self, Op, Device>::run(*this, reducer, m_device, data);\r\n                                               ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:133:19: note: in instantiation of member function 'Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer>, Eigen::ThreadPoolDevice>::evalSubExprsIfNeeded' requested here\r\n    return m_impl.evalSubExprsIfNeeded(data);\r\n                  ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:132:24: note: (skipping 2 contexts in backtrace; use -ftemplate-backtrace-limit=0 to see all)\r\n    return m_rightImpl.evalSubExprsIfNeeded(m_leftImpl.data());\r\n                       ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:59: note: in instantiation of member function 'Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, MakePointer>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 5>, const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer> > >, Eigen::ThreadPoolDevice, true>::run' requested here\r\n      internal::TensorExecutor<const Assign, DeviceType>::run(assign, m_device);\r\n                                                          ^\r\ntensorflow/core/kernels/pooling_ops_3d.cc:108:71: note: in instantiation of function template specialization 'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 5, 1, long>, 16, MakePointer>, Eigen::ThreadPoolDevice>::operator=<Eigen::TensorReshapingOp<const Eigen::DSizes<long, 5>, const Eigen::TensorReductionOp<Eigen::internal::AvgPoolMeanReducer<float>, const Eigen::IndexList<Eigen::type2index<1>>, const Eigen::TensorReshapingOp<const Eigen::DSizes<long, 3>, const Eigen::TensorVolumePatchOp<-1, -1, -1, const Eigen::TensorMap<Eigen::Tensor<const float, 5, 1, long>, 16, MakePointer> > >, MakePointer> > >' requested here\r\n    output->tensor<T, 5>().device(context->eigen_device<CPUDevice>()) =\r\n                                                                      ^\r\ntensorflow/core/kernels/pooling_ops_3d.cc:194:39: note: in instantiation of member function 'tensorflow::LaunchPoolingOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::launch' requested here\r\n    LaunchPoolingOp<Device, T, Type>::launch(context, tensor_in, window, stride,\r\n                                      ^\r\ntensorflow/core/kernels/pooling_ops_3d.cc:133:12: note: in instantiation of member function 'tensorflow::Pooling3DOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::Compute' requested here\r\n  explicit Pooling3DOp(OpKernelConstruction* context) : UnaryOp<T>(context) {\r\n           ^\r\ntensorflow/core/kernels/pooling_ops_3d.cc:738:15: note: in instantiation of member function 'tensorflow::Pooling3DOp<Eigen::ThreadPoolDevice, float, tensorflow::PoolingType::AVG>::Pooling3DOp' requested here\r\nTF_CALL_float(REGISTER_CPU_KERNELS);\r\n              ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/vec.h:9461:3: note: candidate constructor not viable: no known conversion from '__m128' (vector of 4 'float' values) to 'const vec<float, 4> &' for 1st argument\r\n  vec(const vec<dataT, kElems> &rhs) {\r\n  ^\r\nexternal/local_config_sycl/crosstool/../sycl/include/SYCL/vec.h:9437:3: note: candidate template ignored: could not match 'swizzled_vec<float, kElemsRhs, kIndexRhsN...>' against '__attribute__((__vector_size__(4 * sizeof(float)))) float' (vector of 4 'float' values)\r\n  vec(const swizzled_vec<dataT, kElemsRhs, kIndexRhsN...> &rhs) {\r\n  ^\r\n1 error generated.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: no\r\nCUDA/cuDNN version: no\r\nGPU model and memory: Intel(R) HD Graphics 615\r\nExact command to reproduce: bazel build -c opt \u2013config=sycl --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-mfma //tensorflow/tools/pip_package:build_pip_package", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly."]}, {"number": 15480, "title": " sparse_multiclass_hinge_loss() Error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows/MacOS\r\n- **TensorFlow installed from (source or binary)**:N\r\n- **TensorFlow version (use command below)**:1.4\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:N\r\n- **GCC/Compiler version (if compiling from source)**:N\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:See below\r\n\r\n### Describe the problem\r\nThere seems to be a bug in sparse_multiclass_hinge_loss(), as per the example below.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.random.uniform(0, 1, size = (100, 5))\r\ny = np.random.choice(3, 100) \r\ny = y.reshape(100, 1)\r\n\r\nX = tf.placeholder(\"float32\", [None, 5])\r\nY = tf.placeholder(\"int32\", [None, 1])\r\n\r\nweights = {'w': tf.Variable(tf.random_uniform([5, 3]))}\r\nbiases = {'b': tf.Variable(tf.zeros([3]))}\r\n\r\nlogits = tf.add(tf.matmul(X, weights['w']), biases['b'])\r\n\r\nloss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    res = sess.run(loss, feed_dict={X: x, Y: y}) \r\n\r\n\r\nres\r\n```\r\n\r\n make_tensor_proto(values, dtype, shape, verify_shape)\r\n    369   else:\r\n    370     if values is None:\r\n--> 371       raise ValueError(\"None values not supported.\")\r\n    372     # if dtype is provided, forces numpy array to be the type\r\n    373     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n\r\n", "comments": ["According to [docs](https://www.tensorflow.org/api_docs/python/tf/contrib/kernel_methods/sparse_multiclass_hinge_loss)\r\n\r\n```\r\nRaises:\r\nValueError: If logits, labels or weights have invalid or inconsistent shapes.\r\nValueError: If labels tensor has invalid dtype.\r\n```", "This is not the case here.\r\n\r\nThe issue seems on passing tensors to the logits argument in the loss function. For instance if I pass a constant as below, the issue goes away.\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import constant_op\r\nimport numpy as np\r\n\r\nx = np.random.uniform(0, 1, size = (3, 3))\r\ny = np.random.choice(3, 3) \r\ny = y.reshape(3, 1)\r\n\r\nX = tf.placeholder(\"float32\", [None, 3])\r\nY = tf.placeholder(\"int32\", [None, 1])\r\n\r\nweights = {'w': tf.Variable(tf.random_uniform([3, 3]))}\r\nbiases = {'b': tf.Variable(tf.zeros([3]))}\r\n\r\n#logits = tf.add(tf.matmul(X, weights['w']), biases['b'])\r\n\r\nlogits = constant_op.constant([[1.6, -0.4, 0.8], [1.5, 0.8, -1.0],\r\n                                     [0.2, -1.8, 4.0]])\r\nloss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    res = sess.run(loss, feed_dict={X: x, Y: y}) \r\n\r\n\r\nres\r\n```\r\n", "Could you paste the whole exception stack? ", "---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-b3ef74c071b2> in <module>()\r\n     14 logits = tf.add(tf.matmul(X, weights['w']), biases['b'])\r\n     15 \r\n---> 16 loss = tf.reduce_mean(tf.contrib.kernel_methods.sparse_multiclass_hinge_loss(logits=logits, labels=Y))\r\n     17 \r\n     18 init = tf.global_variables_initializer()\r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/contrib/kernel_methods/python/losses.pyc in sparse_multiclass_hinge_loss(labels, logits, weights, scope, loss_collection, reduction)\r\n    114     # Compute the logits tensor corresponding to the correct class per instance.\r\n    115     example_indices = array_ops.reshape(\r\n--> 116         math_ops.range(batch_size), shape=[batch_size, 1])\r\n    117     indices = array_ops.concat(\r\n    118         [\r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in range(start, limit, delta, dtype, name)\r\n   1215   with ops.name_scope(name, \"Range\", [start, limit, delta]) as name:\r\n   1216     start = ops.convert_to_tensor(start, dtype=dtype, name=\"start\")\r\n-> 1217     limit = ops.convert_to_tensor(limit, dtype=dtype, name=\"limit\")\r\n   1218     delta = ops.convert_to_tensor(delta, dtype=dtype, name=\"delta\")\r\n   1219 \r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    834       name=name,\r\n    835       preferred_dtype=preferred_dtype,\r\n--> 836       as_ref=False)\r\n    837 \r\n    838 \r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n    924 \r\n    925     if ret is None:\r\n--> 926       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    927 \r\n    928     if ret is NotImplemented:\r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    227                                          as_ref=False):\r\n    228   _ = as_ref\r\n--> 229   return constant(v, dtype=dtype, name=name)\r\n    230 \r\n    231 \r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name, verify_shape)\r\n    206   tensor_value.tensor.CopyFrom(\r\n    207       tensor_util.make_tensor_proto(\r\n--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n    210   const_tensor = g.create_op(\r\n\r\n/users/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    369   else:\r\n    370     if values is None:\r\n--> 371       raise ValueError(\"None values not supported.\")\r\n    372     # if dtype is provided, forces numpy array to be the type\r\n    373     # provided if possible.\r\n\r\nValueError: None values not supported.\r\n", "Thanks for reporting this. I will take a look today\r\n\r\nPetros", "The exception is triggered when batch size is unknown, I added #15544 to fix it.", "the issue has to do with dynamic vs static shape. The [change](https://github.com/tensorflow/tensorflow/pull/15544) by facaiy fixes it."]}, {"number": 15479, "title": "Fix issue building memory_stats with opencl", "body": "This PR fixes #15477 \r\nThis bug exists in at least 1.4, 1.5 and master branch, should I send PR for every branch?", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15478, "title": "[XLA/tfcompile] Add Env::CreateUniqueFileName and use it in SaveGraph", "body": "Split part of `tensorflow::Env::LocalTempFilename` into `tensorflow::Env::CreateTempFilename` so that it can be used in `SaveGraph`.\r\n\r\nThis PR is to replace #15335.\r\n\r\nI am terrible in creating descriptive function name, better name suggestion for `CreateTempFilename` is welcomed.\r\n\r\n/cc @jlebar.\r\n\r\n #15213", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\n(Am I doing this right?  I don't review too many OSS patches.)", "@jlebar I think you need to mention @tensorflow-jenkins \r\n\r\nSee https://github.com/tensorflow/tensorflow/pull/15491#issuecomment-352861743", "Hm, maybe?  I see some people doing it without, e.g. https://github.com/tensorflow/tensorflow/pull/12503#issuecomment-330725991\r\n\r\nAnyway, can't hurt.\r\n\r\n@tensorflow-jenkins, test this please.", "@yifeif Can you help us to trigger CI tests?", "@tensorflow-jenkins test this please"]}, {"number": 15477, "title": "[BUG] Undeclared error in: \"tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc\"", "body": "### System information\r\n- **Linux Ubuntu 17.10**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version 1.4.1**:\r\n- **Python version 3.6**: \r\n- **Bazel version (0.8.1)**:\r\n- **GCC/Compiler version (7.2)**:\r\n\r\n### Describe the problem\r\nundeclared error raised in \"tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc\" when build v1.4.1 with jemalloc, OpenCL\r\n\r\n### Source code / logs\r\nERROR: .../tensorflow/tensorflow/contrib/memory_stats/BUILD:17:1: C++ compilation of rule '//tensorflow/contrib/memory_stats:python/ops/_memory_stats_ops.so' failed (Exit 1)\r\ntensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc:64:5: error: unknown type name '**MaxBytesInUseOp**'; did you mean 'BytesInUseOp'?\r\n    MaxBytesInUseOp);\r\n    ^~~~~~~~~~~~~~~\r\n    BytesInUseOp\r\n./tensorflow/core/framework/op_kernel.h:1209:68: note: expanded from macro 'REGISTER_KERNEL_BUILDER'\r\n  REGISTER_KERNEL_BUILDER_UNIQ_HELPER(__COUNTER__, kernel_builder, __VA_ARGS__)\r\n                                                                   ^\r\n./tensorflow/core/framework/op_kernel.h:1212:53: note: expanded from macro 'REGISTER_KERNEL_BUILDER_UNIQ_HELPER'\r\n  REGISTER_KERNEL_BUILDER_UNIQ(ctr, kernel_builder, __VA_ARGS__)\r\n                                                    ^\r\n./tensorflow/core/framework/op_kernel.h:1225:24: note: expanded from macro 'REGISTER_KERNEL_BUILDER_UNIQ'\r\n            return new __VA_ARGS__(context);                          \\\r\n                       ^\r\ntensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc:44:7: note: 'BytesInUseOp' declared here\r\nclass BytesInUseOp : public MemoryStatsOp {\r\n      ^\r\n1 error generated.\r\n", "comments": ["When using `REGISTER_KERNEL_BUILDER`, he name `\"BytesInUse\"` does not match the op `MaxBytesInUseOp`. This may be a typo.\r\nhttps://github.com/tensorflow/tensorflow/blob/e210cb140a60a74d5e9ce3bf9ebedb21b4910f1c/tensorflow/contrib/memory_stats/kernels/memory_stats_ops.cc#L60-L63\r\n\r\nI will make a PR to fix this."]}, {"number": 15476, "title": "fix typos", "body": "fix typos", "comments": ["Can one of the admins verify this patch?", "well done!"]}, {"number": 15475, "title": "[WIP]  Add bazel runfiles manifest support for Windows", "body": "To implement the ideas in:\r\nhttps://groups.google.com/forum/#!msg/bazel-discuss/Po8xN8dhWkI/sWPUYV9YBAAJ\r\n\r\nNow //tensorflow/core:example_example_parser_configuration_test is disabled on Windows, because it cannot find the data files.\r\n\r\ntesting::RunFileRelocator::GetInstance().Relocate() is the replacement of testing::TensorFlowSrcRoot(). We should mark TensorFlowSrcRoot as deprecated or completely remove it.\r\n\r\nPlaces need to change:\r\nc/c_api_test.cc\r\ncc/saved_model/loader_test.cc\r\ncompiler/aot/codegen_test.cc\r\ncompiler/xla/service/gpu/llvm_gpu_backend/utils_test.cc\r\ncompiler/xla/tests/sample_file_test.cc\r\ncontrib/ffmpeg/default/ffmpeg_lib_test.cc\r\ncontrib/lite/models/test_utils.h\r\ncontrib/lite/testing/generated_examples_zip_test.cc\r\ncontrib/session_bundle/bundle_shim_test.cc\r\ncontrib/session_bundle/test_util.cc\r\ncore/distributed_runtime/rpc/grpc_testlib.cc\r\ncore/grappler/costs/graph_properties_test.cc\r\ncore/grappler/utils/scc_test.cc\r\ncore/kernels/hexagon/graph_transferer_test.cc\r\ncore/kernels/spectrogram_test.cc\r\ncore/platform/cloud/google_auth_provider_test.cc\r\ncore/profiler/internal/tfprof_show_test.cc\r\ncore/profiler/internal/tfprof_stats_test.cc\r\ncore/profiler/internal/tfprof_tensor_test.cc\r\ncore/profiler/internal/tfprof_timeline_test.cc\r\ncore/example/example_parser_configuration_test.cc\r\ncore/platform/cloud/oauth_client_test.cc\r\n\r\nRef: https://github.com/bazelbuild/bazel/issues/4215\r\n\r\nTODO:\r\n1. deal with folders\r\n2. do not translate abs path.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@laszlocsomor  \r\nShould we switch to use \"RUNFILES_DIR\" env on Linux? Or continue to use TEST_SRCDIR/TEST_WORKSPACE.", "@snnn : \r\n\r\n> @laszlocsomor\r\n> Should we switch to use \"RUNFILES_DIR\" env on Linux? Or continue to use TEST_SRCDIR/TEST_WORKSPACE.\r\n\r\n~~Yes, I recommend using `$RUNFILES_DIR`.~~\r\n\r\nWait, actually, let me double check that... :) sorry", "Hi @laszlocsomor \r\nI have two more questions:\r\n\r\n1.  Paths contain spaces \r\nI saw there is a TODO at\r\nhttps://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/windows/runfiles/WindowsRunfiles.java#L54\r\nWhat's the current status of this? \r\n\r\n2.  The first column in manifest file starts with \"org_tensorflow/tensorflow/\" . Now I hardcode this string in tensorflow's c++ code. Is it ok? How can I get this string from ... somewhere?", "I don't think it matters which one you use.\r\n\r\nTests conventionally use `TEST_SRCDIR`. Its value is either [the same as `RUNFILES_DIR`](https://github.com/bazelbuild/bazel/blob/7b423ccd9506c6fb500b5c4998e1f26aebf28912/src/main/java/com/google/devtools/build/lib/exec/StandaloneTestStrategy.java#L69), or is set to a language-specific runfiles directory value [such as `JAVA_RUNFILES`](https://github.com/bazelbuild/bazel/blob/7b423ccd9506c6fb500b5c4998e1f26aebf28912/src/main/java/com/google/devtools/build/lib/bazel/rules/java/java_stub_template.txt#L163), which ironically is again [the same as `RUNFILES_DIR`](https://github.com/bazelbuild/bazel/blob/7b423ccd9506c6fb500b5c4998e1f26aebf28912/src/main/java/com/google/devtools/build/lib/exec/StandaloneTestStrategy.java#L71).", "> 1. Paths contain spaces\r\n> I saw there is a TODO at\r\n> https://github.com/bazelbuild/bazel/blob/master/src/main/java/com/google/devtools/build/lib/windows/runfiles/WindowsRunfiles.java#L54\r\n> What's the current status of this?\r\n\r\nAFAIK nobody fixed it yet. Thanks for raising my awareness to it! Filed https://github.com/bazelbuild/bazel/issues/4327, I'll fix it in Q1-2018.\r\n\r\n> 2. The first column in manifest file starts with \"org_tensorflow/tensorflow/\" . Now I hardcode this string in tensorflow's c++ code. Is it ok? How can I get this string from ... somewhere?\r\n\r\nThe first segment \"org_tensorflow\" is the name of the repository. If it's the main workspace, its name is in `$TEST_WORKSPACE`. If it's an external repo, then its name is the name you gave the repository rule in the `WORKSPACE` file.\r\n\r\nThe second segment is already part of the path within the workspace or repository.", "Hi TF team,\r\n\r\nIn order to make these cc_tests work on Windows, I need to touch a lot of source files.  Should I submit them one by one, or , combine all these changes in one PR?\r\n\r\nNow this PR contains one change to the test framework, and two unitest fixes for \r\n```\r\ncore/example/example_parser_configuration_test.cc\r\ncore/platform/cloud/oauth_client_test.cc\r\n```\r\n", "Hi @laszlocsomor \r\n\r\nCurrent implementation has a limit: it only has file to file mappings, no folder to folder mappings. Say, If we have a function which takes a folder as input, e.g.\r\n\r\nmodel.h\r\n```c++\r\nvoid LoadModel(const string& model_dir);\r\n```\r\nAnd we want to test this function.\r\n\r\nmodel_test.cc\r\n```c++\r\nTEST(Model, LoadModelFromLocalDir) {\r\n   string dirname = \"data/checkpoint1\"\r\n   dirname = TranslateDirNameByUsingManifestInfo(dirname); //This step is not do-able\r\n   LoadModel(dirname); \r\n}\r\n```\r\n\r\nIt's not do-able. Because the path, \"data/checkpoint1\", doesn't exist in the MANIFEST file, which only has things like:\r\n```\r\ndata/checkpoint1/aaa/bbb.txt        C:/xxx/aaa/bbb.txt\r\ndata/checkpoint1/aaa/ccc/ddd.txt        C:/xxx/aaa/ccc/ddd.txt\r\n```\r\nThere is no entry for \"data/checkpoint1\"\r\n\r\n\r\n", "@snnn : Sorry I don't follow, what has a limitation? Do you mean that the runfiles manifest doesn't allow enumerating runfiles directories, or something else?", "@laszlocsomor: Yes.  I don't know how to relocate a dir. ", "@snnn : I see. I filed https://github.com/bazelbuild/bazel/issues/4334. In the meantime, can you use `find` on Linux, and `grep` on Windows?", "Yes, I can. Then?", "Then until https://github.com/bazelbuild/bazel/issues/4334 is fixed, use `find`/`grep` to work around the missing feature of listing directories, like so:\r\n\r\n- on Linux:\r\n\r\n    ```\r\n    find \"$RUNFILES_DIR/repository_name/path/to/dir\" -not -type d\r\n    ```\r\n\r\n- on Windows:\r\n\r\n    ```\r\n    grep '^repository_name/path/to/dir' \"$RUNFILES_MANIFEST_FILE\" | cut -d ' ' -f 2-\r\n    ```", "Have no time to work on this right now. Close it temporarily."]}, {"number": 15474, "title": "Revert changes in TrainingHelper that cause \"no gradient\" error", "body": "Change occured in https://github.com/tensorflow/tensorflow/commit/69c324591ba4dfeafb403ee59de56ffe063c1e94\r\n\r\nDiscussed in https://github.com/tensorflow/tensorflow/issues/15278", "comments": ["Can one of the admins verify this patch?", "cc: @joel-shor ", "@tensorflow-jenkins test this please", "Was it intended to revert only one of the two instances?", "I have not tested the GreedyEmbeddingHelper so I did not apply that fix there.", "Does the other function also depend on gradient?", "If you use the sample_ids from GreedyEmbeddingHelper for any kind of loss calculation then yes, this would probably also throw an error right now.\r\nI can open up a pull request for that one as well. But I will not have tested it in that case."]}, {"number": 15473, "title": "tf.contrib.ffmpeg.decode_audio() prints the ffmpeg stdout", "body": "tf.contrib.ffmpeg.decode_audio() prints the ffmpeg stdout for each input and there seems to be not functionality available to suppress that. \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15472, "title": "tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported \t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04  and 14.04and windows\r\n- **TensorFlow installed from (source or binary):Anaconda\r\n- **TensorFlow version (use command below):1.30\r\n- **Python version:2.7 \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:CUDA 8.0\r\n- **GPU model and memory**:GeForce GTX TITAN \r\n- **Exact command to reproduce**:\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nW tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.669140: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:18.803520: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.803612: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:18.804101: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.804146: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:18.804578: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.804622: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:18.921353: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.921412: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:18.921736: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.921769: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:18.922064: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:18.922096: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.056649: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.056716: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.057046: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.057079: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.057369: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.057399: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.192128: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.192198: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.192519: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.192551: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.192829: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.192858: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.581540: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.581599: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.581928: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.581961: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.582241: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.582271: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.712218: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.712290: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.712602: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.712634: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:19.712909: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:19.712937: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:20.109602: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.109676: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n2017-12-18 14:27:20.110017: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.110051: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n2017-12-18 14:27:20.110337: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.110368: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n2017-12-18 14:27:20.258401: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.258473: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n2017-12-18 14:27:20.258812: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.258845: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n2017-12-18 14:27:20.259145: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.259175: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast/x)]]\r\n2017-12-18 14:27:20.409659: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.409726: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:20.410061: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.410096: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n2017-12-18 14:27:20.410375: W tensorflow/core/framework/op_kernel.cc:1182] Unimplemented: Cast float to string is not supported\r\n2017-12-18 14:27:20.410406: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Unimplemented: Cast float to string is not supported\r\n\t [[Node: Cast_6 = Cast[DstT=DT_STRING, SrcT=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Cast_6/x)]]\r\n\r\n### Source code / logs\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\nimport random\r\nimg_weight = 224\r\nimg_hight = 224\r\nVOC_NUM_CLASS =15\r\nimg_channels=3\r\n\r\ndef getfile(datapath,labelpath):\r\n    class_train = []\r\n    label_train = []  \r\n    for root ,dirs,files in os.walk(datapath):\r\n        for pic in files:\r\n            class_train.append(os.path.join(root,pic))\r\n    with open(labelpath) as filee:\r\n        i = 0\r\n        for l in filee.readlines():\r\n            if i != 0:\r\n                t = []\r\n               for j in l.split(','):\r\n                    t.append(j.strip('\\n'))\r\n                label_train.append(t[0:15])\r\n            else:\r\n                i += 1\r\n    temp=[]\r\n    for i in label_train:\r\n        array1=np.array(i,dtype=int).tostring()\r\n        temp.append(array1)\r\n    label_train=temp\r\n    return  class_train,label_train\r\n\r\ndef get_batch(image, label, image_W, image_H, batch_size, capacity):\r\n    '''\r\n    Args:\r\n        image: list type\r\n        label: list type\r\n        image_W: image width\r\n        image_H: image height\r\n        batch_size: batch size\r\n        capacity: the maximum elements in queue\r\n    Returns:\r\n        image_batch: 4D tensor [batch_size, width, height, 3], dtype=tf.float32\r\n        label_batch: 1D tensor [batch_size], dtype=tf.int32\r\n    '''\r\n\r\n    image = tf.cast(image, tf.string)\r\n    label = tf.cast(label, tf.string)\r\n\r\n    # # make an input queue \r\n    input_queue = tf.train.slice_input_producer([image, label],shuffle=True)\r\n\r\n    label = input_queue[1]\r\n    image_contents = tf.read_file(input_queue[0])\r\n    image = tf.image.decode_png(image_contents, channels=3)\r\n    image = tf.image.flip_left_right(image)#\u5bf9\u56fe\u7247\u8fdb\u884c\u6c34\u5e73\u7ffb\u8f6c\r\n    image.set_shape([image_H, image_W,3])\r\n    image = tf.image.per_image_standardization(image)#(\u51cf\u53bb\u5747\u503c\u9664\u4ee5\u65b9\u5dee\u5bf9\u56fe\u50cf\u8fdb\u884c\u6807\u51c6\u5316)\r\n    label = tf.decode_raw(label, tf.int64)#\u5b57\u7b26\u4e32\u7c7b\u578b\u8f6c\u6362\u4e3afloat32\u7684\u5411\u91cf\r\n   \r\n    label = tf.cast(label, tf.float32)\r\n    label = tf.reshape(label, [\r\n        VOC_NUM_CLASS,\r\n    ])\r\n    image_batch, label_batch = tf.train.batch(\r\n        [image, label],\r\n        batch_size=batch_size,\r\n        num_threads=64,\r\n        capacity=capacity)\r\n    image_batch = tf.cast(image_batch, tf.float32)\r\n    return image_batch, label_batch\r\ndef shulffedata(image,label):\r\n    a = [int(i) for i in range(len(image))]\r\n    random.shuffle(a)\r\n    temp_image = []\r\n    temp_label = []\r\n    for i in a:\r\n        temp_image.append(image[i])\r\n        temp_label.append(label[i])\r\n    image=temp_image\r\n    label=temp_label\r\n    return image,label\r\n\r\ndef dataprovider():\r\n    train_dir = '/media/thomas/\u529e\u516c/images/'\r\n    train_dircsv = '/media/thomas/\u529e\u516c/cxr8/Binarylabels.csv'\r\n    save_dir='/home/thomas/\u6587\u6863/Densenet/vision_networks-master/chexray/'\r\n    BATCH_SIZE = 1\r\n    name_test = 'datatest'\r\n    images,labels = getfile(train_dir,train_dircsv)\r\n\r\n\r\n    images,labels=shulffedata(images,labels)\r\n\r\n  \r\n    train_images=images[:(int(len(images)*0.8))]\r\n\r\n    train_label=labels[:(int(len(images)*0.8))]\r\n    test_images=images[int(len(images)*0.8):]\r\n    test_label =labels[int(len(images)*0.8):]\r\n   train_image_batch,train_label_batch=get_batch(train_images,train_label,img_weight,img_hight,BATCH_SIZE,2000)\r\n    test_image_batch,test_label_batch=get_batch(test_images,test_label,img_weight,img_hight,BATCH_SIZE,2000)\r\n\r\n    with tf.Session() as sess:\r\n        i = 0\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)  #when exeute  this sentence ,it report error.\r\n        try:\r\n            while not coord.should_stop() and i < 1:\r\n                image, label = sess.run([test_image_batch, test_label_batch])\r\n                # image, label = sess.run([train_image_batch, train_label_batch])\r\n                # image,label = sess.run([image_batch,label_batch])\r\n                for j in np.arange(BATCH_SIZE):\r\n                    print(\"label:\")\r\n                    print(label[j])\r\n                    plt.imshow(image[j, :, :, :])\r\n                    plt.show()\r\n                i += 1\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"done!\")\r\n        finally:\r\n\r\n            coord.request_stop()\r\n\r\n    coord.join(threads)", "comments": ["As the exception message said, I think `tf.cast` cannot convert float to string, use [` tf.as_string`](https://www.tensorflow.org/api_docs/python/tf/as_string) instead.", "my input data is a string types  ,so i just want to it convert tensor ", "Sorry, perhaps I misunderstand you. In your code here,\r\n```python\r\nimage = tf.cast(image, tf.string)\r\nlabel = tf.cast(label, tf.string)\r\n```\r\nat the first glance I think that you want to convert an image to string, however it seems not true, right?\r\n\r\nAs show in your log, the exception is raised because `tf.cast` cannot convert float to string. However, the log is not enough to locate where the problem is. Hence I suggest you to check all your codes which contains `tf.cast` at first.\r\n\r\nBy the way, perhaps this question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is a larger community that reads questions there.", "Agreed, this is probably better discussed on stackoverflow."]}, {"number": 15471, "title": "BUG: fix name scope collision in `tf.layers`", "body": "Fix  #13429.\r\n\r\nSince  #14390 has been merged into master branch, we can easily solve the problem with `auxiliary_name_scope=False`.\r\n\r\n### How to test\r\n\r\n+ [x] add test case.\r\n+ [x] pass all tests.\r\n", "comments": ["Can one of the admins verify this patch?", "This change looks very promising, but let's test it carefully of course.", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "Thank you, @lukaszkaiser @yifeif .", "Thanks for your contribution @facaiy!"]}, {"number": 15470, "title": "Release notes and version string update.", "body": "", "comments": ["Should be all addressed now. I messed up the version string update, but it should be good now."]}, {"number": 15469, "title": "Compute test accuracy in batches to avoid OOM on GPUs", "body": "Reported here: https://github.com/tensorflow/tensorflow/issues/136\r\nAlternative to this for mnist_deep.py: https://github.com/tensorflow/tensorflow/pull/157\r\n\r\nNote that some reports in https://github.com/tensorflow/tensorflow/issues/136 claim that BFC solves the problem and that it would be included in the next binary release, but as this writing is two years after those comments, it stands to reason that the example still doesn't work out of the box for \"low\" memory GPUs.\r\n\r\nAlso noting for completeness that I am running on a GeForce GTX 670 (2GB) and that avg(avg(x_i)) = avg(x_i) when |x_i| for all i is equal. ", "comments": ["Can one of the admins verify this patch?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@MarkDaoust Not sure we even want to keep this example at all? Is this going to be rewritten?", "Nagging Reviewer @MarkDaoust: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 38 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 52 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @MarkDaoust: It has been 67 days with no activity and the `awaiting review` label was assigned. Can you please take a look?"]}, {"number": 15468, "title": "Compute test accuracy in batches to avoid OOM on GPUs.", "body": "Reported here: https://github.com/tensorflow/tensorflow/issues/136\r\nAlternative to this for mnist_deep.py: https://github.com/tensorflow/tensorflow/pull/157\r\n\r\nNote that some reports in https://github.com/tensorflow/tensorflow/issues/136 claim that BFC solves the problem and that it would be included in the next binary release, but as this writing is two years after those comments, it stands to reason that the example still doesn't work out of the box for \"low\" memory GPUs.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Noting for completeness that I am running on a GeForce GTX 670 (2GB) and that avg(avg(x_i)) = avg(x_i) when |x_i| for all i is equal. Also, new to python and realizing that for _ in range(...) is more pythonic, so happy to make that change if requested."]}, {"number": 15467, "title": "some proposal about  tf.metrics", "body": "Have I written custom code  :  YES\r\nOS Platform and Distribution  -Win10\r\nTensorFlow installed from - pip3\r\nTensorFlow version-1.4\r\nBazel version\r\nCUDA/cuDNN version-8.0\r\nGPU model and memory-2GB\r\nExact command to reproduce \r\n## Describe the problem\r\n    \r\n\r\nI want to know true-positive,ture-nagetive,false-positive,false-nagetive,recall,precision for each class.\r\n Although I can implement it at this version(1.40), it need lots of code. I mean the functions don't\r\n support the multi-class well, it just for the 2 class condition. I think if I can implement them with few\r\n code, it looks more elegant. \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "sorry, I think the information is for BUG. So I didn't write them down.  And I have updated them. \r\n", "Yup, the robot sometime is a little annoying. Sorry for that.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "@martinwicke Excuse me, do you know anyone who can answer the issue?", "I think this is a feature request for multi-class metrics similar to the binary ones? That's a fair feature request.", "Closing this issue, as `TruePositives`, `TrueNegatives`, `FalsePositives`, and `FalseNegatives` have been implemented for [`tf.metrics`](https://www.tensorflow.org/api_docs/python/tf/metrics). Thanks! \ud83d\ude0a "]}]