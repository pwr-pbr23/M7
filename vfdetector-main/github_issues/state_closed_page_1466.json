[{"number": 8968, "title": "Intel Mkl kernels for Concat, LRN and FusedBatchNorm", "body": "This commit implements following changes:\r\n\r\n1) MKL kernels for LRN, Concat, ConcatV2 and FusedBatchNorm and applicable gradient ops\r\n2) Graph layout pass change to support new ops\r\n3) Removes Mkl graph node merge pass as a separate pass\r\n4) Changes MKL-TF tensor ordering from interleaved to contiguous", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "@nhasabni could you pull rebase and push again? googlebot seems confused.\r\n\r\n@zhangyaobit how do the new changes look?", "CLAs look good, thanks!\n\n<!-- ok -->", "@zhangyaobit @drpngx We have updated the code based on the review feedback. I had to reset the checkins to fix the CLA issue. Please can you continue reviewing and testing?", "Yes, @zhangyaobit I think it's ready for another pass.\r\n\r\nJenkins, test this please.", "Tests are green :)", "@zhangyaobit Actually the example at line 248 explains \"within context matching depth\". To put an example for \"outside context matching depth\" we would need to add at least 10 lines of example. We think that may not be desirable. We have explained what the context and the depth means, so we think that should be enough. Also, there are unit tests for context match.\r\n\r\nFor renaming, Conv2DWithBiasBackpropBias to Conv2DWithBiasGrad, we used naming consistent with TensorFlow's naming convention. We found Conv2DBackpropFilter and Conv2DBackpropInput. Since we merge Conv2D and Bias together, we have Conv2DWithBiasBackpropBias. We don't think Conv2DWithBiasGrad is intuitive here.", "@zhangyaobit I've addressed your comments so far. I will commit my changes if you are done with your review. Otherwise, I will wait. ", "@nhasabni @zhangyaobit could we address the remaining style and minor corrections now and maybe defer major refactoring for better code re-use to a followup?", "@nhasabni @rmlarsen, sounds good to me. Please add TODOs if you prefer to address  later.", "@rmlarsen Yes, sounds good. We have addressed comments for graph passes. We will ensure to add TODOs to kernels and address style/minor correction issues now.", "Jenkins, test this please.", "All set?"]}, {"number": 8967, "title": "using gabor filter in tensorflow", "body": "I want to use gabor filter in my first layer of  network in convolution layer. \r\nand idea how can I use it?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8966, "title": "Can't Link Shared Libraries Located in /usr/local/include Folder", "body": "Folks,\r\nI can't add any libraries in the /usr/local/include (or /lib) to the BUILD file. Bazel complains it cannot find the headers, nor the *.so files.\r\nAre you planning on adding support for that?\r\n\r\nThanks,", "comments": ["This would be a bazel question, not a TF one.\r\nPlease file an issue, or reach out to bazel user forums on how to include libraries in your local folders in your BUILD files.", "Aight.\r\nI'll ask at Bazel's (yet, my program is a tensorflow program, though)\r\nThanks."]}, {"number": 8965, "title": "Branch 152198296", "body": "Push of internal changes.", "comments": []}, {"number": 8964, "title": "Is there a bug in tf.layers.batch_normalization?", "body": "Since TF 1.0 API came out, I have been trying to use `tf.layers.batch_normalization` instead of the version in tf.contrib.layers. However, I found this layer works abnormally in my case.\r\n\r\nHere is my simple code that uses `tf.layers.batch_normalization`.\r\n\r\n```\r\noutput = tf.nn.bias_add(tf.matmul(input_tensor, self._weight), self._bias)\r\n\r\n  if self._use_batch_norm:\r\n       output = tf.layers.batch_normalization(\r\n             output,\r\n             momentum=0.9,\r\n             training=training,\r\n             name=self._name + \"_bn\",\r\n             reuse=reuse\r\n        )\r\n\r\n output = tf.nn.relu(output)\r\n```\r\n\r\nHowever, when I enabled the batch_normalization on this layer. I found that my model maps all raw data to a single point in the hidden representation space. When I disabled batch_normalization, this cannot happen at all.\r\n\r\nHere is a final view of my data in the latent space:\r\n\r\narray([[ 0.46338093,  0.53661913],\r\n       [ 0.46339276,  0.53660733],\r\n       [ 0.46329296,  0.53670704],\r\n       ...,\r\n       [ 0.4633435 ,  0.53665644],\r\n       [ 0.46335611,  0.53664398],\r\n       [ 0.4633027 ,  0.53669727]], dtype=float32)\r\n\r\nThe input data are generated from a multivariate gaussian distribution so that their hidden representations should be different as well. I searched the usage of this function on stackoverflow but the example is so trivial and it doesn't help. I think there may be a bug in this layer object or I may misuse it somehow.\r\n\r\nIn the contrib.layers's version, the batch_norm layer should be linked with update_ops collection. But I read the source code of this implementation and it seems that it is not necessary. Is there anyone has some thoughts on this?\r\n", "comments": ["The data in this issue might be biased."]}, {"number": 8963, "title": "mputecpp", "body": "", "comments": []}, {"number": 8962, "title": "Graph Collections in the C API", "body": "Hi,\r\n\r\nIn Python we have graph collections (e.g., TRAINABLE_VARIABLES) which are stored in the Graph class. I assume that these are somehow serialized in the graph protobuf so that when a GraphDef is imported, the relevant collections are imported too. However, I do not see any option in the C API for defining such collections? Is there something that I am missing? If not, why not include that functionality in the C API?\r\n\r\nThank you,\r\nAnthony", "comments": ["You are correct in that graph collections in the Graph class in Python (which make it to the `MetaGraphDef` protocol buffer, which in turn is exported in the saved model format (`TF_LoadSessionFromSavedModel`)) are not part of the C API - users will have to write their own collection management on top of the existing primitives.\r\n\r\nContributions are welcome (though perhaps want to discuss design a bit before pursuing it)", "I see. I'm currently developing a Scala API and that's what I was thinking of doing (i.e., independent management of collections). However, I was wondering what would be the right way to serialize the graph so that it can also be imported in the Python API and contain the same information. You are implying I should be using the MetaGraphDef protocol buffer and the saved model format used in the Python API, right?\n\nIn terms of design, wouldn't it make more sense to make collections part of the GraphDef and add functions for manipulating them in the C API?\n\nOn Apr 4, 2017, 8:50 PM -0400, Asim Shankar <notifications@github.com>, wrote:\n>\n> You are correct in that graph collections in the Graph class in Python (which make it to the MetaGraphDef protocol buffer, which in turn is exported in the saved model format (TF_LoadSessionFromSavedModel)) are not part of the C API - users will have to write their own collection management on top of the existing primitives.\n>\n>\n> Contributions are welcome (though perhaps want to discuss design a bit before pursuing it)\n>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub (https://github.com/tensorflow/tensorflow/issues/8962#issuecomment-291693274), or mute the thread (https://github.com/notifications/unsubscribe-auth/ABPCXCeQ4FYq9icePDBm-6Ls4iJiYXdOks5rsuVfgaJpZM4MzagI).\n>\n>\n>\n\n", "I'm guessing you mean making functions part of the `TF_Graph` and add functions for manipulating them? Yes, that sounds reasonable, just not something we've gotten around to yet and hence contributions welcome :)\r\n\r\n(BTW, just want to make sure you're aware of the [TensorFlow Java API](https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary) which may be useful for Scala as well)", "Ok for now I'll use a temporary Scala-only solution, but will also look into what would be required for supporting that in the C API.\r\n\r\nI am aware of Java API. I actually took the JNI bindings from that project, extended them to support more features of the C API (e.g., control inputs, obtaining all ops in the graph, among others), and built an entirely new Scala library on top of it. It is loosely based on the Java library but supports more features (such as a createWith function for specifying name scopes, control dependencies, device contexts, among others). My goal is to make it support much of what the Python API supports so I am currently going through the Python code and adding features. I plan to open it to the public once it is complete enough to be usable in wider contexts than the Java API."]}, {"number": 8961, "title": "ValueError(\"No variables provided.\") for apply_gradients", "body": "In [optimizer.p](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/optimizer.py)y, the first part of code segment is\r\n\r\n```\r\ndef apply_gradients(self, grads_and_vars, global_step=None, name=None):\r\n\r\n   grads_and_vars = tuple(grads_and_vars)  # Make sure repeat iteration works.\r\n   if not grads_and_vars:\r\n      raise ValueError(\"No variables provided.\")\r\n```\r\nRunning my program, I got the error message caused by this specific error. I then printed out `tuple(grads_and_vars)`, part of which is. I don't know why it can cause the error of no variables provided.\r\n\r\n> ((<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_0:0' shape=(3, 3, 3, 64) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2afc746b5c50>), (<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_1:0' shape=(64,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2affd48189b0>), (<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_2:0' shape=(3, 3, 64, 64) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2affd486d940>), (<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_3:0' shape=(64,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2affd488cf98>), (<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_4:0' shape=(3, 3, 64, 128) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2afc746b5d68>), (<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_5:0' shape=(128,) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2affd48f4278>), (<tf.Tensor 'Optimizer/training/clip_by_global_norm/Optimizer/training/clip_by_global_norm/_6:0' shape=(3, 3, 128, 128) dtype=float32>, <tensorflow.python.ops.variables.Variable object at 0x2affd4915e10>), \r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!"]}, {"number": 8960, "title": "Fix unbound <table> tag.", "body": "Change: 152029426", "comments": ["Jenkins, test this please."]}, {"number": 8959, "title": "Gradients of Gradients throws 'NAN' outputs for LSTM's", "body": "\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nCuda 8.0, tensorflow 1.0\r\n\r\nIf you try to take the gradient of a gradient such as needed in Improved Techniques of Wasserstien GAN's (https://arxiv.org/abs/1704.00028) for LSTMs in the generator, throws a NAN error. If you use convolutions, then there is no error. \r\n\r\nAuthor's source code for language model:\r\n\r\nhttps://github.com/igul222/improved_wgan_training/blob/master/gan_language.py\r\n", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\n(Also, if possible, a more focused, smaller snippet that can reproduce the problem without importing a lot of external code will go a long way in being able to help)"]}, {"number": 8958, "title": "Branch 152141388", "body": "", "comments": []}, {"number": 8957, "title": "Tensorflow multi-GPU training and variable scope", "body": "### Context\r\n\r\nI'm working on a detector model on multiple GPUs using Tensorflow 1.0. As suggested [here](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py), Gradients are computed on multiple GPUs individually and are averaged on CPU. To share the trainable variables (e.g. weights and biases) across the GPU towers, the `reuse` flag is turned on using `tf.get_variable_scope().reuse_variables()`, as in the cifar10 example. The difference is that I am using an `AdamOptimizer` instead of `GradientDescentOptimizer`.\r\n\r\n\r\n### Problem\r\n\r\nWhen I run the training job, it prints out a long stacktrace and raise the following error at `opt.apply_gradients()`:\r\n\r\n```\r\nValueError: Variable conv1_1/kernel/Adam/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n```\r\n\r\nLooking into the source code I found that the `AdamOptimizer` is creating a number of zero-initialized slots within the `_create_slots()` method, wherein it calls the `_zeros_slot()`. This calls a separate module called the [`slot_creator`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L62) (source code linked).\r\n\r\n**In `line 62` of the `slot_creator`, it uses `variable_scope.get_variable()`. This used to be `tf.Variable()` in 0.12.**\r\n\r\nMy understanding of variable scopes is that `variable_scope.get_variable()` would fail to create a variable **if `reuse` flag is on`. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/variable_scope.py#L669) for source code.**\r\n\r\nBut the cifar10 example by Tensorflow creators seems to suggest enabling reuse to share variables across the GPU towers using `tf.get_variable_scope().reuse_variables()`. This happens **before** we average and apply the gradients. It looks like Tensorflow 1.0 refuses to create variables for the `AdamOptimizer`.\r\n\r\nThis happens for all optimizers that directly or indirectly call the `slot_creator` module.\r\n\r\n### Question\r\nAs a quick fix, I added a custom function into the `VariableScope` class to disable the `_reuse` flag right before calling `opt.apply_gradients`. However, I am sure there is a merit to forcing the `reuse` flag to be only set to `True`. I am not sure what the better workaround would be. Any suggestions?\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI created question [here](http://stackoverflow.com/questions/43212725/tensorflow-multi-gpu-training-and-variable-scope)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5.1 respectively\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nThe commit hash (`git rev-parse HEAD`): 1cb96893a64f59b7265f9def9968f7bed1e57662#diff-a25cc0fc9d475568e0069e5dc0b67d85 (see `slot_creator`)", "comments": ["@lukaszkaiser @martinwicke : Is `tf.get_variable_scope().reuse_variables()` the recommended way to share variables in 1.0+? And is there a way to disable reuse so that variables created by the `AdamOptimizer` can be created after all the towers?", "The way to share I'd recommend is to always open a scope and specify reuse parameter.\r\nIt should work fine with everything, there is no need to un-reuse at all.\r\n\r\n```\r\nwith tf.variable_scope(\"my_model\"):\r\n   outputs = tf.layers....(inputs)\r\n# some other code\r\nwith tf.variable_scope(\"my_model\", reuse=True):\r\n     outputs = tf.layers....(inputs)\r\n```", "In a multi-gpu loop you can say:\r\n\r\n```\r\nfor gpu in xrange(num_gpus):\r\n  with tf.variable_scope(\"my_model\", reuse=(gpu > 0)):\r\n    outputs = tf.layers....(inputs)\r\n```", "Hey I know this is closed but I came to the same conclusions as above and found explicitly creating a variable_scope with reuse=False/True around a tower fixed the issue. Would be great if this was in the official documentation or in the cifar example as its currently misleading. ", "I met the same problem. Could you please give me some cases to understand the problem? I don't understand your answer. Thank you!", "If I understand correctly, @xiao-dong, this issue comes up when you try to replicate your model across multiple GPUs.  Suppose you create a model on CPU with `net = tf.layers.dense(net, units=128)`.  Behind the scenes `tf.layers` is going to use `get_variable` to create a variable which may be called, say, `dense/kernel`.\r\n\r\nNow if you try to do this on multiple GPUs you need to put the same variables on all the GPUs.  The naive way to do this might be to use a loop that looks like this:\r\n\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.device('/gpu:{}'.format(i)):\r\n    tf.layers.dense(net, units=128)\r\n```\r\n\r\nBut this is going to fail, because on the second iteration of the loop, `get_variable` is going to try to reuse the same `dense/kernel` variable as before, but won't be able to because it's not shared.  If you were to simply wrap this with\r\n\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.variable_scope('my_scope', reuse=True), tf.device('/gpu:{}'.format(i)):\r\n    tf.layers.dense(net, units=128)\r\n```\r\n\r\nthis also will not work because `get_variable` will try to reuse a variable that does not exist on the first iteration of the loop.  @lukaszkaiser's solution is to not reuse the variable on the first iteration of the loop, but then reuse it on all others:\r\n\r\n```\r\nfor i in range(num_gpus):\r\n  with tf.variable_scope('my_scope', reuse=(i > 0)), tf.device('/gpu:{}'.format(i)):\r\n    tf.layers.dense(net, units=128)\r\n```", "@joe-antognini Thank you"]}, {"number": 8956, "title": "Linking errors when using Tensorflow/Bazel to call Python from C++", "body": "I am writing a C++ application (based on Tensorflow Serving) in which I need to call Python and Numpy functions (from `Python.h` and `numpy.h`).\r\n\r\nThis application is built with Bazel.\r\n\r\nSo I include the header: \r\n\r\n`#include \"tensorflow/python/lib/core/numpy.h\"`\r\n\r\nwhich in turn includes `Python.h`. \r\n\r\nThis file seems to wrap numpy to fix an issue described in [this](http://stackoverflow.com/questions/31971185/segfault-when-import-array-not-in-same-translation-unit) Stack Overflow post.\r\n\r\nAny other C++ code within the Tensorflow project which needs to make calls to `Python.h` also does not directly include it, but only includes `tensorflow/python/lib/core/numpy.h`. On the Bazel side, it seems to suffice to simply add `//util/python:python_headers` to the `deps` of the build.\r\n\r\nMy `BUILD` file looks like this:\r\n\r\n```\r\n\r\n...\r\n\r\ncc_library(\r\n\tname = \"my_library\",\r\n\tsrcs = [\"my_library.cc\"],\r\n\thdrs = [\"my_library.h\"],\r\n\tdeps = [\r\n\t\t@org_tensorflow//util/python:python_headers\",\r\n\t\t@org_tensorflow//third_party/py/numpy:headers\",\r\n\t\t\t] + SOME_OTHER_DEPS + TENSORFLOW_DEPS + SUPPORTED_TENSORFLOW_OPS,\r\n)\r\n\r\ncc_binary(\r\n\tname = \"my_main\",\r\n\tsrcs = [\"my_main.cc\"],\r\n\tdeps = [\":my_library\"],\r\n)\r\n\r\n...\r\n\r\n```\r\n\r\nBuilding the library with `bazel build :my_library` works fine.\r\nBuilding the binary the same way does not work, I get the following errors:\r\n\r\n```\r\nbazel-out/local-fastbuild/bin/servable/_objs/my_library/servable/my:library.pic.o:my_library.cc:function tensorflow::serving::NumpyInitializer::init(): error: undefined reference to 'Py_Initialize'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::MakeArgTuple(tensorflow::(anonymous namespace)::PyCall*, _object**): error: undefined reference to 'PyList_New'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::MakeArgTuple(tensorflow::(anonymous namespace)::PyCall*, _object**): error: undefined reference to 'PyList_SetItem'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::MakeArgTuple(tensorflow::(anonymous namespace)::PyCall*, _object**): error: undefined reference to 'Py_BuildValue'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::IsSingleNone(_object*): error: undefined reference to 'PyType_IsSubtype'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::IsSingleNone(_object*): error: undefined reference to '_Py_NoneStruct'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::DoCallPyFunc(tensorflow::(anonymous namespace)::PyCall*): error: undefined reference to 'PyEval_CallObjectWithKeywords'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::DoCallPyFunc(tensorflow::(anonymous namespace)::PyCall*): error: undefined reference to 'PyErr_Occurred'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::DoCallPyFunc(tensorflow::(anonymous namespace)::PyCall*): error: undefined reference to 'PyErr_Print'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::DoCallPyFunc(tensorflow::(anonymous namespace)::PyCall*): error: undefined reference to 'PyList_Size'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::DoCallPyFunc(tensorflow::(anonymous namespace)::PyCall*): error: undefined reference to 'PyList_GetItem'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::(anonymous namespace)::DoCallPyFunc(tensorflow::(anonymous namespace)::PyCall*): error: undefined reference to 'PyType_IsSubtype'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::ConvertNdarrayToTensor(_object*, tensorflow::Tensor*): error: undefined reference to 'PyString_AsStringAndSize'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::ConvertTensorToNdarray(tensorflow::Tensor const&, _object**): error: undefined reference to 'PyString_FromStringAndSize'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::PyFuncOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'PyGILState_Ensure'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/py_func_lib/external/org_tensorflow/tensorflow/python/lib/core/py_func.pic.o:py_func.cc:function tensorflow::PyFuncOp::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'PyGILState_Release'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/numpy_lib/external/org_tensorflow/tensorflow/python/lib/core/numpy.pic.o:numpy.cc:function _import_array: error: undefined reference to 'PyImport_ImportModule'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/numpy_lib/external/org_tensorflow/tensorflow/python/lib/core/numpy.pic.o:numpy.cc:function _import_array: error: undefined reference to 'PyExc_ImportError'\r\nbazel-out/local-fastbuild/bin/external/org_tensorflow/tensorflow/python/_objs/numpy_lib/external/org_tensorflow/tensorflow/python/lib/core/numpy.pic.o:numpy.cc:function _import_array: error: undefined reference to 'PyErr_SetString'\r\n\r\n\r\n```\r\n\r\nSo there is obviously linking errors against `Python.h`. However, compiling any Tensorflow-internal goals works fine. I find it weird that it now can not link to `Python.h` even inside Tensorflow files.\r\n\r\nAfter spending a few days looking into Tensorflow and their `BUILD` files I am out of ideas how to make this work.\r\n\r\n## So now I ask here: where and how exactly is the correct inclusion of Python defined in Tensorflow (and its Bazel files?).\r\n\r\nSome clues seem to be in the definitions in `util/python/BUILD` and `tensorflow/tensorflow.bzl`.\r\n\r\nThere seems to be quite a bit of Bazel magic going on there.\r\n\r\n", "comments": ["This seems more about linking and bazel than TensorFlow.\r\n\r\nThat said, looking at the error messages, what I suspect is happening is that you need to make sure that the binary dynamically links with the python shared libraries (so you need to pass the flags suggested in https://docs.python.org/2/extending/embedding.html#compiling-and-linking-under-unix-like-systems or https://docs.python.org/3/extending/embedding.html#compiling-and-linking-under-unix-like-systems to be provided when your binary is being linked).\r\n\r\nHope that helps. Since this is not an issue with TensorFlow itself, I'm going to close this out (so we keep the issues here focused on bugs/feature-requests for TensorFlow).  Thanks!"]}, {"number": 8955, "title": "Fix gradient implementation documentation", "body": "The docs for implementing gradients for a custom op are inconsistent. It seems that a gradient op for an op that takes multiple *inputs* should return a list of tensors with the gradients with respect to the individual inputs. The next paragraph matches this interpretation.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I think the documentation is correct:\r\n\r\n- An op that produces multiple outputs may receive gradients from each of them.  Hence, in the gradient function for an op, 'grads' will be the gradients it receives for each output, and so 'grads' will contain as many Tensors as there are outputs. \r\n\r\n- The gradient function must *produce* / return Tensors (or None) for each of its inputs, since the ops that produce the input would also expect to receive a Tensor as their gradients. \r\n\r\nIs that clear?  Feel free to comment if you think further clarification is needed.."]}, {"number": 8954, "title": "Add TensorFlow equivalent to np.repeat", "body": "This is requested in #8246 \r\n\r\nI'll add the gradient op soon.\r\n\r\nThis is my first TensorFlow PR. Please let me know if I made any mistake. Thanks!", "comments": ["Can one of the admins verify this patch?", "ping @aselle ", "@mrry, I'm not sure we should add this to the API. If we do, it seems like it should follow your stackoverflow answer and be a python only op. Thoughts?", "@mrry 's method seems only work when the repeats is a scalar. One can do np.repeat([1,2],[1,2]) and get [1,2,2]. @aselle ", "@aselle Yeah, I think there's more to a general reimplementation of `np.repeat()` than what I posted in that answer (assuming I'm thinking of the same one as you...). If there were a neat way to do it in Python, I'd be all in favor of doing it that way, but none springs to mind.", "As I noted over in https://github.com/tensorflow/tensorflow/issues/8246, this can be done in Python with `while_loop` and `TensorArray`. I'm not sure how efficient that solution is.", "@shoyer I wrote a similar python implement like yours. It runs the tests (without the gradient tests) in 51.048s. The current c++ cpu implement runs the same tests in 0.539s. I can write a benchmark if need.", "@zycdragonball Yeah, I'm not too surprised the performance of the while_loop solution is abysmal :)", "@aselle @mrry -- what do we do here?", "If it has feature parity with the equivalent NumPy function, I'd be all in favor of adding it.", "@zycdragonball any updates on this? Do you think we can get numpy parity on this easily?", "It now has the same interface as numpy. For example, repeats can either be a scalar or a vector. There are two cases in numpy I didn't cover: input is a scalar and the default axis (which make the tensor flat). Both of them only need a few lines of codes. I can add them in if need.", "Is there any reason not to be feature complete with numpy, if it's easy to do?", "I would skip the default behavior of flattening the input. Numpy has been\ntrying to move away from this sort of default behavior, which usually\ndoesn't work very well for multi-dimensional inputs. Safer default behavior\nwould be to raise an error if no axis is provided and the input is not 1D.\nOn Tue, May 2, 2017 at 10:42 AM Vijay Vasudevan <notifications@github.com>\nwrote:\n\n> Is there any reason not to be feature complete with numpy, if it's easy to\n> do?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8954#issuecomment-298707936>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABKS1sgStojYGRhKK_OqvmByPReF4x4xks5r12sWgaJpZM4My-DL>\n> .\n>\n", "It would be great to have it support negative axis in the numpy style.\r\n", "It now supports scalar input and negative axis. The only missing feature I think is the default axis. Providing no axis is currently raising a TypeError.", "Thanks @zycdragonball!  Will await @aselle or @mrry's review  ", "Since this is being proposed as a few function in `tf.contrib` (as opposed to `tf`), removing the API Review label", "It's also fine to put this into core, as long as the signature for `tf.repeat` matches `np.repeat` (so perhaps the input `input` should be called `a`?)", "> so perhaps the input input should be called a?\r\n\r\nI would skip this part. The numpy argument name is so non-descriptive that I don't think anyone ever uses it as a keyword argument.", "I'd defer to @aselle  - the resident expert on our goals with numpy parity. ", "@aselle what do you think?", "Can one of the admins verify this patch?", "@aselle does this look OK to you?", "@aselle friendly ping", "@tensorflow-jenkins test this please", "@zycdragonball could you please fix the build file errors here:\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4383/consoleFull", "@rmlarsen I don't understand the build error as the suggested changes by the buildifier seems void. I'm not sure whether the reason is deps in py_test was not in lexicographic order. I've made it ordered. Not sure if it will work.", "@aselle can you check whether this is all good now?", "@zycdragonball could you resolve conflicts and push again?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@drpngx It seems that I did something wrong when solving the conflict. The file changed now is huge. Should I close this PR and open a new one?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@zycdragonball Maybe you can try the following command?\r\n```shell\r\ngit reset --hard <SOME-COMMIT>\r\n# redo the merge and then\r\ngit push origin add-repeat --force\r\n```", "@zycdragonball could you rebase and push again?", "@drpngx I have rebased the PR. I was trying to resolve the conflict. But I can't do it on web editor, as it is \"too complex\". I also can't see the conflict on my local branch. What is the git command to see the conflict? Thanks! ", "Could you rebase this commit against master so it can be merged.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I've signed the CLA. Not sure why one of the commits doesn't have my name on it.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@aselle There is no conflict anymore.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@zycdragonball Can you resolve the CLA issue? Thanks!", "Jenkins, test this please", "@frankchn The CLA should be good now.", "Jenkins, test this please", "Jenkins, test this please.", "Mind addressing the test failures?", "@jhseu Let's see how it goes.", "@tensorflow-jenkins test this please", "Jenkins, test this please", "Any rough estimate as to when this feature will be available ?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Looks to be some legit failures.  @zycdragonball do you have the ability to test locally before sending out?", "@vrv Sorry for the mess. Maybe I messed up some version when solving the CLA issue. The tests should run now.", "@tensorflow-jenkins test this please", "@zycdragonball it looks like the tests are still failing.", "@zycdragonball please rebase and fix failing tests.", "@rmlarsen  It seems that contrib/metrics failed, which I didn't touch.", "Jenkins, test this please", "Jenkins, test this please", "Mind looking at the test failures? It looks like you're getting:\r\n`ImportError: cannot import name 'repeat'`", "@zycdragonball ping?", "@jhseu @martinwicke I can't reproduce the failed test. It passed on my local machine just now.", "The python3 failure should be easy to fix:\r\n\r\n```\r\npackages/tensorflow/contrib/repeat/python/ops/repeat_op.py\", line 50, in _RepeatGrad\r\n    for i in xrange(axis):\r\nNameError: name 'xrange' is not defined\r\n```\r\n\r\nYou need to use `six.moves.xrange`.", "For CMAKE, you have to add `add_python_module(\"tensorflow/contrib/repeat\")` to `tensorflow/contrib/cmake/tf_python.cmake`", "@zycdragonball please fix and push again.", "@zycdragonball ping", "@sb2nov @drpngx Done.", "Jenkins, test this please.\n\nOn Sep 26, 2017 11:11 PM, \"zycdragonball\" <notifications@github.com> wrote:\n\n> @sb2nov <https://github.com/sb2nov> @drpngx <https://github.com/drpngx>\n> Done.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8954#issuecomment-332418758>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbY0OrXW3kwSFjZ09uSb0OREPd1onks5smecNgaJpZM4My-DL>\n> .\n>\n", "@tensorflow-jenkins test this please", "Jenkins, test this please.", "@zycdragonball there are failures in the cmake build, that all look like this:\r\n\r\n```\r\n11:09:16 ImportError: No module named 'tensorflow.contrib.repeat.python'\r\n```\r\n\r\nThis usually happens when you don't add the repeat module in the tf_python cmake build file.", "@martinwicke An aside, but I notice that I've been pulled into quite a few PR threads (#9376 is another example) where we're adding a new `tf.contrib` submodule for a single op. Should we try to consolidate these (to improve loading time, navigability, etc.)?", "@mrry, yes, we should have contrib.ops maybe, similar to how we have contrib.framework, contrib.layers, etc.?", "@zycdragonball while fixing the error, can you move to a different contrib place, contrib.ops?", "@zycdragonball any update? I'll mark stalled for now.", "@zycdragonball we haven't heard from you since Nov 9. It looks like this was promising. Feel free to reopen if you get cycles to work on this.", "why don't we have this yet?", "@imsrgadich we're still looking for a contributor to complete this", "I'd love to contribute a c++ version but don't have the time at the moment (certainly not in the general dimensional case). Meanwhile a simple but sub-optimal solution at least in the 1d case is to use a combination of sequence_mask and boolean_mask, together with tile:\r\n\r\n```\r\ndef tf_repeat1d(values, repeats):\r\n    max_len = tf.reduce_max(repeats)\r\n    tiled = tf.reshape(tf.tile(tf.reshape(values, [-1, 1]), [1, max_len]), [-1])\r\n    seq_mask = tf.reshape(tf.sequence_mask(repeats, max_len), [-1])\r\n    return tf.boolean_mask(tiled, seq_mask)\r\n```"]}, {"number": 8953, "title": "Enable full message with tf.Print", "body": "**Operating System:** Debian 4.8.15-2\r\n**Installed version of CUDA and cuDNN:** CUDA 8, cuDNN 5\r\n**python3 -c \"import tensorflow; print(tensorflow.__version__)\":** 1.0.1\r\n\r\n**Minimal reproducible example**\r\n```\r\nimport tensorflow as tf\r\nph = tf.placeholder(tf.float32, [3,4,5,6])\r\nts = tf.shape(ph)\r\ntp = tf.Print(ts, [ts])\r\ntm = tf.reduce_mean(tp)\r\nsess = tf.Session()\r\nres = sess.run(tm)\r\n```\r\n\r\n**Output**\r\n`I tensorflow/core/kernels/logging_ops.cc:79] [3 4 5...]`\r\n\r\nMaybe there is some obvious way around this, but I couldn't find any on the [docs](https://www.tensorflow.org/api_docs/python/tf/Print). To state the obvious : I'm trying to see the full shape of the tensor, and not only the first 3 positions. In this example its size is 4, but I would like to be able to see whatever I want with whatever size it has.\r\n\r\nFurthermore, debugging into TensorFlow did became a problem when I got to line [62 of logging_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/logging_ops.py#L62), where `gen_logging_ops` is used but it doesn't seem to be declared anywhere \u2014 and older versions already had this, I'm probably missing some link, because `grep` and `find` couldn't find anything useful:\r\n\r\n**Output of `grep`**\r\n```\r\ntensorflow$ grep -R \"gen_logging_ops\" .\r\n./tensorflow/python/framework/function_test.py:from tensorflow.python.ops import gen_logging_ops\r\n./tensorflow/python/framework/function_test.py:      check = gen_logging_ops._assert(math_ops.greater(x, 0), [x])\r\n./tensorflow/python/ops/control_flow_ops.py:from tensorflow.python.ops import gen_logging_ops\r\n./tensorflow/python/ops/control_flow_ops.py:      return gen_logging_ops._assert(\r\n./tensorflow/python/ops/control_flow_ops.py:        return gen_logging_ops._assert(\r\n./tensorflow/python/ops/summary_ops.py:from tensorflow.python.ops import gen_logging_ops\r\n./tensorflow/python/ops/summary_ops.py:from tensorflow.python.ops.gen_logging_ops import *\r\n./tensorflow/python/ops/summary_ops.py:    val = gen_logging_ops._tensor_summary(\r\n./tensorflow/python/ops/logging_ops.py:from tensorflow.python.ops import gen_logging_ops\r\n./tensorflow/python/ops/logging_ops.py:from tensorflow.python.ops.gen_logging_ops import *\r\n./tensorflow/python/ops/logging_ops.py:  return gen_logging_ops._print(input_, data, message, first_n, summarize, name)\r\n./tensorflow/python/ops/logging_ops.py:    val = gen_logging_ops._histogram_summary(\r\n./tensorflow/python/ops/logging_ops.py:    val = gen_logging_ops._image_summary(\r\n./tensorflow/python/ops/logging_ops.py:    val = gen_logging_ops._audio_summary_v2(tag=tag,\r\n./tensorflow/python/ops/logging_ops.py:    val = gen_logging_ops._merge_summary(inputs=inputs, name=name)\r\n./tensorflow/python/ops/logging_ops.py:    val = gen_logging_ops._scalar_summary(tags=tags, values=values, name=scope)\r\n./tensorflow/python/kernel_tests/control_flow_ops_py_test.py:from tensorflow.python.ops import gen_logging_ops\r\n./tensorflow/python/kernel_tests/control_flow_ops_py_test.py:        unguarded_assert = gen_logging_ops._assert(\r\n./tensorflow/python/summary/summary.py:from tensorflow.python.ops import gen_logging_ops as _gen_logging_ops\r\n./tensorflow/python/summary/summary.py:    val = _gen_logging_ops._scalar_summary(\r\n./tensorflow/python/summary/summary.py:    val = _gen_logging_ops._image_summary(\r\n./tensorflow/python/summary/summary.py:    val = _gen_logging_ops._histogram_summary(\r\n./tensorflow/python/summary/summary.py:    val = _gen_logging_ops._audio_summary_v2(\r\n./tensorflow/python/summary/summary.py:    val = _gen_logging_ops._merge_summary(inputs=inputs, name=name)\r\n```\r\n\r\n**Output of `find`**\r\n```\r\ntensorflow$ find . -iname \"*logging_ops*\"\r\n./tensorflow/core/ops/logging_ops.cc\r\n./tensorflow/core/kernels/logging_ops_test.cc\r\n./tensorflow/core/kernels/logging_ops.cc\r\n./tensorflow/python/ops/logging_ops.py\r\n./tensorflow/python/kernel_tests/logging_ops_test.py\r\n```\r\n\r\nBesides evaluating what I want with `sess.run` and manually printing it with python, is there any tensorflow-friendly solution to this? Debugging should be easier. :)", "comments": ["Closing: parameter `summarize` in `tf.Print`. Still no clue about the `gen_logging_ops`", "For the benefit of anyone else who reaches this issue while searching for the origin of `gen_logging_ops`: The file `gen_logging_ops.py` is generated by the bazel build. The rule that causes this file to be generated is located in the file `tensorflow/core/BUILD` (search for \"tf_gen_op_libs\"). The build rule takes as input the contents of `tensorflow/core/ops/logging_ops.cc` and generates a Python function for each instance of the `REGISTER_OP` macro in that C++ source file. By convention, the name of the generated Python function is a lowercased version of the corresponding operator's name, with an underscore placed before every character that was in uppercase in the original name (i.e. \"Print\" ==> \"_print\"). The corresponding kernel for the generated Python function `_print()` is located in `tensorflow/core/kernels/logging_ops.cc`.", "Correction: the `tf_gen_op_libs` rule is just the first step in the codegen process. That rule generates a compiled C++ library containing the operator. Later in the build, the rule at line 1146 of `python/BUILD` invokes the rule `tf_gen_op_wrapper_private_py` from `python/build_defs.bzl`. This rule in turn invokes the rule `tf_gen_op_wrapper_py` from `tensorflow.bzl`, which creates a static binary containing the library file for the logging ops and the compiled object file for `python_op_gen_main.cc`; then invokes the binary, passing it the contents of the text file `python/ops/hidden_ops.txt`.\r\n\r\nNext, the main function in `python_op_gen_main.cc` creates an operators registry. Since the executable was only linked with the logging operators, this registry only contains the logging operators. The main function passes this registry, along with the list of operators in `hidden_ops.txt`, to the function `PrintPythonOps()`, which is defined in `python_op_gen.cc`. This function generates the contents of `gen_logging_ops.py` and prints those contents to STDOUT. Back in `tensorflow.bzl`, the `tf_gen_op_wrapper_py` rule directs STDOUT of the C++ program to `gen_logging_ops.py`.", "I found a simple way to show the full message of tensor.shape.\r\n\r\n`def tensor_check(tensor):\r\n    return tf.Print(tensor, [str(tensor.shape), tensor.shape,tf.shape(tensor), tensor.get_shape()], message='||| ')`\r\n\r\nand it will return the follow message:\r\n`||| [(2, 16, 16, 64)][2 16 16...][2 16 16...][2 16 16...]`\r\n\r\njust use str()    \r\n\r\n(\u309c-\u309c)\u3064\u30ed "]}, {"number": 8952, "title": "AttributeError: module 'tensorflow.python.ops.array_ops' has no attribute 'pack' ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Can you please fill in the issue template. It will help with solving your issue. Most importantly, which version of Tensorflow are you running as I believe that the tf.python group was deprecated in 1.0. Please post a code snippet too so we can understand what you were trying to achieve and what function you were trying to find as the error from your title suggests that you're calling something similar to tf.python.ops.array_ops.pack instead of .pack() perhaps?\r\n\r\nEDIT: Part of me also thinks that pack became stack but I may be mistaken."]}, {"number": 8951, "title": "Using output of `tf.argmax` as index raises TypeError", "body": "### Minimal example\r\n```\r\na = tf.constant([1, 2, 3], dtype=tf.float32)\r\nb = tf.argmax(a)\r\ntf.Session().run(a[b])\r\n```\r\nThis raises a `TypeError`\r\n```\r\nTypeError: Input 'strides' of 'StridedSlice' Op has type int32 that does not match type int64 of argument 'begin'.\r\n```\r\nThe solution is to `tf.cast` `b` into a `tf.int32` before using it as an index. The output of `tf.argmax` should be usable as an index immediately. I feel that the current state is not consistent, is there some hidden reason for this?", "comments": ["Marking this as contributions welcome since it seems that @yongtang has a solution in review."]}, {"number": 8950, "title": "Execution Stuck after few steps in sess.run()", "body": "\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nExecution is stuck in between steps. For few steps, it seems to run fine but after that the execution just halts without throwing any exception or the error.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\nGPU: NVIDIA TITAN X (Pascal)\r\nGPU Memory: 12GB\r\n\r\nInstalled version of CUDA and cuDNN: \r\n-rw-r--r-- 1 root root   556000 Mar 29 05:10 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Mar 29 05:10 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Mar 29 05:10 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.61\r\n-rwxr-xr-x 1 root root   415432 Mar 29 05:10 /usr/local/cuda/lib64/libcudart.so.8.0.61\r\n-rw-r--r-- 1 root root   775162 Mar 29 05:10 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       18 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.10\r\n-rwxr-xr-x 1 root root 84163560 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\nlrwxrwxrwx 1 root root       18 Apr  4 12:55 /usr/local/cuda/lib64/libcudnn.so.6 -> libcudnn.so.6.0.20\r\n-rwxrwxrwx 1 root root 84163560 Apr  4 13:17 /usr/local/cuda/lib64/libcudnn.so.6.0.20\r\n-rwxrwxrwx 1 root root 70364814 Apr  4 13:18 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI ran the mnist_deep.py script provided in tutorials.\r\n\r\n### What other attempted solutions have you tried?\r\nPreviously, the display used to get hang showing error CUDA_LAUNCH_ERROR_TIMEOUT. On nvidia forum, someone suggested to switch off the X-server. I switched it off, but the problem still persisted.\r\nI noticed that the script was hogging full memory of GPU, I tried to limit the allocation by using \"allow_growth\" flag. But the problem still persists. \r\n\r\n### Logs or other output that would be helpful\r\nstep 47, training accuracy 0.64\r\nstep 48, training accuracy 0.72\r\nstep 49, training accuracy 0.7\r\nstep 50, training accuracy 0.68\r\nstep 51, training accuracy 0.76\r\nstep 52, training accuracy 0.66\r\nstep 53, training accuracy 0.82\r\nstep 54, training accuracy 0.82\r\nstep 55, training accuracy 0.64\r\nstep 56, training accuracy 0.64\r\nstep 57, training accuracy 0.74\r\nstep 58, training accuracy 0.76\r\nstep 59, training accuracy 0.8\r\nstep 60, training accuracy 0.68\r\nstep 61, training accuracy 0.88\r\nstep 62, training accuracy 0.62\r\nstep 63, training accuracy 0.84\r\nstep 64, training accuracy 0.72\r\nstep 65, training accuracy 0.76\r\nstep 66, training accuracy 0.74\r\nstep 67, training accuracy 0.86\r\nstep 68, training accuracy 0.76\r\nstep 69, training accuracy 0.9\r\nstep 70, training accuracy 0.84\r\nstep 71, training accuracy 0.82\r\nstep 72, training accuracy 0.7\r\n\r\nAttached is the screenshot of \"nvidia-smi\"\r\n![nvidia-smi](https://cloud.githubusercontent.com/assets/1628210/24651492/a5b54ada-194b-11e7-8ba2-9c3ff6dc2901.PNG)\r\n\r\n", "comments": ["@nealwu did we check recently that `mnist_deep` works?", "/CC: @jmchen-g since it's related to models.", "That's odd, I just ran it and it worked fine for me. @ajeetksingh can you make sure that you aren't running anything else on the GPU at the same time? And it's normal for it to use the full memory.\r\n\r\nMy output below:\r\n\r\n```\r\nstep 0, training accuracy 0.14\r\nstep 100, training accuracy 0.76\r\nstep 200, training accuracy 0.9\r\nstep 300, training accuracy 0.82\r\n...\r\nstep 19700, training accuracy 1\r\nstep 19800, training accuracy 1\r\nstep 19900, training accuracy 1\r\ntest accuracy 0.9932\r\n```", "@ajeetksingh Can you check what your % CPU utilization is while TF hangs? What is the model of your motherboard?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8949, "title": "ResourceExhausted", "body": "I get the following error when running in GPU on AWS p2-xlarge:\r\n```\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1205162d00 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x120517b100 of size 73984\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1215a5ba00 of size 96000\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1215a8a800 of size 96000\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x1215affb00 of size 263424\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 24 Chunks of size 256 totalling 6.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 13 Chunks of size 512 totalling 6.5KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 12 Chunks of size 1280 totalling 15.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 1792 totalling 8.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2560 totalling 2.5KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 3584 totalling 17.5KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 6144 totalling 6.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 8 Chunks of size 25600 totalling 200.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 29184 totalling 28.5KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1386 Chunks of size 96000 totalling 126.89MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 131328 totalling 128.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 233 Chunks of size 192000 totalling 42.66MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 7 Chunks of size 204800 totalling 1.37MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 232192 totalling 226.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 257792 totalling 251.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2640128 totalling 2.52MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 5 Chunks of size 19200000 totalling 91.55MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 265.87MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \r\nLimit:                   279314432\r\nInUse:                   278784768\r\nMaxInUse:                278976768\r\nNumAllocs:                    4097\r\nMaxAllocSize:             19200000\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ****************************************************************************************************\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 375.0KiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[300,320]\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3720 get requests, put_count=1106 evicted_count=1000 eviction_rate=0.904159 and unsatisfied allocation rate=0.998387\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\n```\r\nI'm using TensorFlow R0.12.1 and Session code is:\r\n```\r\n# Launch the graph\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\nconfig.gpu_options.allocator_type = 'BFC'\r\nsess = tf.Session(config = config)\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\ntimestart = datetime.datetime.now() # Start time in seconds\r\ntimeprev  = datetime.datetime.now() # Start time in seconds\r\ncountprev = 0\r\nratelist = []\r\nboxcarcount = 10\r\nX_train = X_train.astype(np.float32)  # Cast to float32 from float64\r\n\r\nprint(\"Starting training...\")\r\n# Perform Training steps with \"batch_size\" iterations at each loop\r\nstep = 1\r\nwhile step * batch_size <= training_iters:\r\n    # Note: type(X_train) = float32\r\n    # Note: type(y_train) = int32\r\n    # Note: type(step)       = int\r\n    # Note: type(batch_size) = int\r\n    batch_xs =         extract_batch_size(X_train, step, batch_size)\r\n    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size),LabelMax)\r\n\r\n    # Fit training using batch data\r\n    output, loss, acc = sess.run(\r\n        [optimizer, cost, accuracy],\r\n        feed_dict={\r\n            Xin   : batch_xs, \r\n            Ytrue : batch_ys,\r\n            keep_prob: DO_keep_prob\r\n        }\r\n    )\r\n    train_losses.append(loss)\r\n    train_accuracies.append(acc)\r\n```\r\nMy question: is there a way to allocate more memory in the GPU?\r\nThis seems like a really small data set to be using?!?!?\r\nThanks.", "comments": ["The GPU on the p2-xlarge AMI has 12 GiB of memory, so I really don't think that I am exceeding a physical constraint on the GPU.  I'm wondering if there is a TF limitation that I need to tweak to increase the available memory for the graph.", "The log suggests that you have a limit of ~270MB (from the line `Limit:                   279314432`).\r\nIs it possible that you've limited the memory that can be used by TensorFlow using an environment variable like `TF_CUDA_HOST_MEM_LIMIT_IN_MB`?", "I don't know of any such variable, so I know that I didn't set it.  Is it possible it is a default value?\r\nLooking through my environment variables, I see no variable with any CUDA settings (other than CUDA_HOME)\r\nRegardless, can I just force this environment variable to, say, 10GiB (or 10000 since it's in MB). For example:\r\n```\r\nTF_CUDA_HOST_MEM_LIMIT_IN_MB = 10000\r\nexport TF_CUDA_HOST_MEM_LIMIT_IN_MB\r\n```\r\nAlternatively, can this be set from in Tensorflow or Python?", "This question might be more appropriate for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 8948, "title": "Feature Request: Erosion2d and Dilation2d implemented on GPU", "body": "Hi everybody!\r\nErosion2d and Dilation2d can be used for various reason. e.g. [Ronneberg et al.](https://arxiv.org/abs/1505.04597) compute a weight map using morphological operations for their cost function. This can be done using morphological Erosions and Dilations but those operations are quite slow on CPU. It seems Tensorflow do not use the GPU for those operation. Would it be possible to have those two operations implemented on GPU?", "comments": ["@vanpact : It seems that TensorFlow does have GPU implementations of these kernels - since the time they were added in 2016 (commit https://github.com/tensorflow/tensorflow/commit/283782a2dc3301f4d9e8dee65654eb97c698d635). See [`dilation_ops.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/dilation_ops.cc#L471) for the kernel implementation.\r\n\r\nAm I missing something?", "Ok, I found why I got an error stating \"Cannot assign a device to node 'erosion2d': Could not satisfy explicit device specification '/device:GPU:0' because ;no supported kernel for GPU devices is available\". The kernel is available for float32 but not for int32. \r\n\r\nSorry!\r\n\r\nMaybe a more explicit error indicating the kernels available for a specific pair of device/function would be nice?", "Will look into that. Closing this out now since the original issue seems to be resolved."]}, {"number": 8947, "title": "XLA: Could not open input file: Is a directory", "body": "XLA failed with Could not open input file: Is a directory\r\n\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```bash\r\n.opt/anaconda/lib64/libcudadevrt.a\r\n.opt/anaconda/lib64/libcudart.so\r\n.opt/anaconda/lib64/libcudart.so.8.0\r\n.opt/anaconda/lib64/libcudart.so.8.0.61\r\n.opt/anaconda/lib64/libcudart_static.a\r\n.opt/anaconda/lib64/libcudnn.so\r\n.opt/anaconda/lib64/libcudnn.so.6\r\n.opt/anaconda/lib64/libcudnn.so.6.0.20\r\n.opt/anaconda/lib64/libcudnn_static.a\r\n```\r\n\r\n### code setup\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. http://q-phantom.com/conda/linux-64/tensorflow-1.1.0rc0-py36_3.tar.bz2\r\n2. 1.1.0-rc0\r\n\r\n### code init\r\n\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\ntf.reset_default_graph()\r\ntl.layers.set_name_reuse(True)\r\nplacehold_mapping, networks = c_network(None, label_indices=label_index, feature_indices=feature_index)\r\nnetwork = networks[0]\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\nsess = tf.Session(config=config)\r\ntl.layers.initialize_global_variables(sess)\r\n```\r\n\r\n### Log\r\n\r\n```bash\r\n2017-04-04 16:26:48.275644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0\r\n2017-04-04 16:26:48.275648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y\r\n2017-04-04 16:26:48.275653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:0a:00.0)\r\n2017-04-04 16:26:48.479102: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-04-04 16:26:48.479122: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices\r\n2017-04-04 16:26:48.481008: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x5dd4360 executing computations on platform Host. Devices:\r\n2017-04-04 16:26:48.481021: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-04-04 16:26:48.481138: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-04-04 16:26:48.481146: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 16 visible devices\r\n2017-04-04 16:26:48.482239: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x5f0f950 executing computations on platform CUDA. Devices:\r\n2017-04-04 16:26:48.482248: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 1080, Compute Capability 6.1\r\nGEN DATASET: 0.00 seconds elapsed\r\nROUND:  0\r\n2017-04-04 16:26:57.149563: F tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/utils.cc:31] -1:-1: Could not open input file: Is a directory\r\n```\r\n", "comments": ["This is a package we do not maintain.\r\nI am not sure when it was built, and which commit it was synced to.\r\nI would recommend reaching out to the contributors of the package you installed.\r\n\r\nEither they can escalate to us whith information about how they built the package, or you can then share the information you got from the package maintainers with us.", "the package is built using bazel.\r\n\r\n```bash\r\nTF_ROOT_DIR=$HOME/git/tensorflow\r\n\r\nmkdir -p $HOME/git\r\n\r\nif [ -d $TF_ROOT_DIR ]; then\r\n  cd $TF_ROOT_DIR\r\n  git pull\r\nelse\r\n  cd $HOME/git\r\n  git clone https://github.com/tensorflow/tensorflow\r\n  cd $TF_ROOT_DIR\r\nfi\r\n\r\ngit checkout r1.1\r\n\r\necho  $PREFIX\r\n\r\nbazel clean\r\necho $PYTHON_BIN_PATH\r\nPYTHON_BIN_PATH=$(which python) \\\r\nPYTHON_LIB_PATH=$PREFIX/lib/python3.6/site-packages \\\r\nTF_NEED_MKL=1 \\\r\nMKL_INSTALL_PATH=$PREFIX \\\r\nCC_OPT_FLAGS=\"-march=native\" \\\r\nTF_NEED_JEMALLOC=1 \\\r\nTF_NEED_GCP=0 \\\r\nTF_NEED_HDFS=0 \\\r\nTF_ENABLE_XLA=1 \\\r\nTF_NEED_OPENCL=0 \\\r\nTF_NEED_CUDA=1 \\\r\nGCC_HOST_COMPILER_PATH=$(which gcc) \\\r\nTF_CUDA_VERSION=\"8.0\" \\\r\nCUDA_TOOLKIT_PATH=$PREFIX \\\r\nTF_CUDNN_VERSION=6 \\\r\nCUDNN_INSTALL_PATH=$PREFIX \\\r\nTF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n./configure\r\n\r\nbazel build -c opt --copt=-march=native --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda -k  //tensorflow/tools/pip_package:build_pip_package\r\nrm -rf /tmp/tensorflow_pkg\r\nmkdir -p /tmp/tensorflow_pkg\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install $(ls /tmp/tensorflow_pkg/tensorflow*)\r\n```", "Thanks for the information.\r\n@tatatodd @hawkinsp is this an issue we have seen before?\r\n\r\n@av8ramit Looks like this is an issue in the release branch. We may think about a cherrypick based on the investigation.", "No, (at least) I have never seen this issue before.\r\n\r\nNote that the code example is setting CUDA_VISIBLE_DEVICES=0, and then enabling session-level JIT.  Enabling session-level JIT only supports GPU, as explained here (in the starred blue box):\r\nhttps://www.tensorflow.org/performance/xla/jit#turning_on_jit_compilation\r\n\r\nIt would be nice to not return a cryptic error, but at a high-level, setting CUDA_VISIBLE_DEVICES=0 and then enabling session-level JIT is at best not going to turn XLA on anyways.  I'll advise against doing this.", "Oops, sorry, brain freeze.  I just realized CUDA_VISIBLE_DEVICES=0 is selecting the 0th device, and the logs show it is being detected.\r\n\r\nSo my response is back to - \"no, I've never seen this, we should probably debug\".", "I suspect it's because TensorFlow cannot find the CUDA libraries, though I'm not sure since I'm not sure what `$PREFIX` is in your snippet above. To confirm, try running the program after setting the `TF_CPP_MIN_VLOG_LEVEL` environment variable to 1 before starting Python.\r\n\r\nIn particular, I'm interested in the log messages from [`gpu_backend_lib.cc`](https://github.com/tensorflow/tensorflow/blob/05d7f793ec5f04cd6b362abfef620a78fefdb35f/tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc#L320), that might help figure out which file it's trying to load (and failing on)", "Closing due to inactivity. If you're still running into this, please feel free to file an updated issue (including any output from suggestions above). Thanks!"]}, {"number": 8946, "title": "Tensorflow results in Segmenation fault ", "body": "Hello\r\n\r\nI have installed tensorflow gpu version with python2.7. It results in following error once i define Session. I am unable to train my model. Please assist\r\n\r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library lib                                                                                                         cublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library lib                                                                                                         cudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library lib                                                                                                         cufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library lib                                                                                                         cuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library lib                                                                                                         curand.so locally\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node rea                                                                                                         d from SysFS had negative value (-1), but there must be at least one NUMA node, so r                                                                                                         eturning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties                                                                                                         :\r\nname: Tesla K40c\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.10GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is c                                                                                                         urrently active; existing: 0x248a6e0\r\nSegmentation fault (core dumped)\r\n\r\n```", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "**System details** : ubuntu 14.04 x86_64 \r\n**Tensorflow version:** 0.10.0\r\n\r\nI installed it as binary \r\ni.e i followed this - > \r\n\r\n```\r\n# Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\r\n# Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\r\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\r\n\r\n sudo pip install --upgrade $TF_BINARY_URL\r\n\r\n```\r\nI just found out its due to **Session()** as i tried basic program i.e \r\n\r\n`>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, TensorFlow!')\r\n>>> sess = tf.Session()`\r\n\r\n \r\nI am using tensorflow cpu version as of now since gpu dint work. I am able to train my model with the same. But I would really appreciate if you could please help me with gpu version \r\n\r\n\r\nThanks!\r\n", "Seems like you're using a very old version of TensorFlow.\r\n\r\nCould you try with the latest version (1.0)?", "Tried with Tensoflow Version 1.0 \r\n\r\nResults in following error as i execute commands in python shell : \r\n\r\nWill be waiting for reply. \r\n\r\nThanks \r\n```\r\nPython 2.7.6 (default, Oct 26 2016, 20:30:19)\r\n[GCC 4.8.4] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally\r\n>>> hello = tf.constant('hi,tensorflow')\r\n>>> sess = tf.Session()\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node                                                                                                              read from SysFS had negative value (-1), but there must be at least one NUMA no                                                                                                             de, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with prop                                                                                                             erties:\r\nname: Tesla K40c\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.10GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one                                                                                                              is currently active; existing: 0x2e0fe90\r\nE tensorflow/core/common_runtime/direct_session.cc:137] Internal: failed initial                                                                                                             izing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevic                                                                                                             ePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.                                                                                                             py\", line 1187, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.                                                                                                             py\", line 552, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/error                                                                                                             s_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\n```\r\n", "This error looks suspicious:\r\n```\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one  is currently active; existing: 0x2e0fe90\r\nE tensorflow/core/common_runtime/direct_session.cc:137] Internal: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE\r\n```\r\nIs there any chance you are running two TF programs at the same time?\r\n\r\nYou can find more similar issues here:\r\nhttps://github.com/NVIDIA/nvidia-docker/issues/262\r\nhttps://github.com/tensorflow/tensorflow/issues/152", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I am also getting the exact same issue. This is with tensorflow branches r1.5 and r1.8. I tried the following:\r\n\r\n- buidling tf locally and installing\r\n- installing tf distribution using pip\r\n- installing tf using anaconda\r\n\r\nNone of it has worked so far. I have posted my issue on https://stackoverflow.com/questions/50347871/segmentation-fault-core-dumped-on-tf-session\r\n"]}, {"number": 8945, "title": "Bazel test failure in TF (Assertion errors) on Ubuntu 16.04 on pcc64le architecture", "body": "###Environment info:\r\nOperating System: Ubuntu 16.04 (ppc64le)\r\n\r\n###Installed version of CUDA and cuDNN: cuda & OpenCL disabled and all other features enabled\r\n\r\n###Build and test INFO\r\nBuilt TF (version 1.0.1) successfully on Ubuntu 16.04 and RHEL 7.3. \r\nNow I am trying to run the bazel tests -  12 tests are failing out of 1044. Please find the attachment for log details.\r\n\r\nThanks!\r\n\r\n[C-C++Test Failure  analysis for TF.xlsx](https://github.com/tensorflow/tensorflow/files/892266/C-C.Test.Failure.analysis.for.TF.xlsx)\r\n", "comments": ["The architecture being used is not one that we currently support. For example, some of the tests are failing with `py-cpuinfo currently only works on X86 and some ARM CPUs`. Community contributions in keeping TensorFlow chugging along with ppc64le are welcome!", "the tests that mention tolerance are failing because the test tolerance bounds are too tight.\r\nThey may need the bounds to be loosened a little and they will pass.\r\n\r\npy-cpuinfo exactly as @asimshankar described.\r\n\r\nThe above is as much information as I can provide on the failures based on very little information you have on the spreadsheet. If you read the full logs, you should be able to adjust the bounds as needed based on the numbers there.\r\n", "Hi @gunan,  as you said \"need tolerance bounds to be loosened a little and test will pass\" \r\n\r\nFor me 4 tests are passing after adjusting the tolerance and the details are as follows :\r\n\r\n1.  `//tensorflow/compiler/tests:pooling_ops_test_cpu `\r\n    Need to change tolerance from `rtol=1e-05` to `rtol=1` \r\n\r\n2. ` //tensorflow/python/kernel_tests:summary_image_op_test   `    \r\n    Need to change tolerance from `rtol=1e-06` to `rtol=1` \r\n\r\n3.  `//tensorflow/python/kernel_tests:linalg_grad_test   `   \r\n    To pass this test need to add `0.01` in `rtol` i.e `rtol=tol+0.01` (initially `rtol=0.16` and now `0.17`)\r\n   \r\n4.  `//tensorflow/contrib/image:image_ops_test`           \r\n  Here we need to change comparison function for `ppc64le` i.e. from `assertAllEqual ` to` assertAllClose` \r\n  (as we can not pass tolerance argument to `assertAllEqual` function) \r\n  Once changed the comparison function pass` rtol = 1e-1 `and test will execute successfully\r\n\r\nAbove tests are passed successfully on `X86` without changing the tolerance . But to pass these tests on `ppc64le` we need to adjust the tolerance (like we mentioned above).\r\n\r\nSo as per my point of view - to implement above changes, we should add the power specific condition for these tests : \r\ne.g. For test ` //tensorflow/python/kernel_tests:linalg_grad_test` need following action : \r\nhttps://github.com/tensorflow/tensorflow/blob/v1.0.1/tensorflow/python/kernel_tests/linalg_grad_test.py#L129\r\n\r\n`self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)`\r\n\r\nIt could be changed to :\r\n\r\n```\r\nif platform.machine() == \"ppc64le\":\r\n     self.assertAllClose(theoretical, numerical, atol=tol+0.01, rtol=tol)\r\nelse \r\n    self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\n```\r\n\r\nPlease provide your comments/suggestions on this.Thanks!", "I am worried about 1e-2 or 1e-1 tolerance, which seems too high to me.\r\nIt is possible there is a bug with the code on ppc64?\r\n\r\nCC @Nayana-ibm who also is working on getting TF to build and run on ppc64 machines. (Big endian in their case, but maybe there is some information that could be shared.)", "We had not observed above test failure on Ubuntu distribution. \r\nOn RHEL distribution, we had observed `//tensorflow/python/kernel_tests:summary_image_op_test `failure however test was passing  on re-running test on high memory vm.\r\nPlease note we had executed tests for Tensorflow v0.10.0", "As we do not have access to ppc64le machines, we will not be able to officially support the architecture ourselves.\r\nLooking at the test results you had, it looks like there is a problem in the code that cause the issues.\r\nWe welcome any contributions to fix the issues. I will mark this as contributions welcome, and leave it open.", "I reran the above test cases on high end vm, but still same test failures were observed (Looks like failures related to TF v1.0.1 and ppc64le platform).\r\n\r\n@gunan, as you said \"It is possible there is a bug with the code on ppc64?\"\r\nI have done some investigation around these test failures :  \r\n\r\n1. //tensorflow/compiler/tests:pooling_ops_test_cpu\r\n     The test expects **avg_pool** o/p as [ 7. , 8. , 9. , 11.5, 12.5, 13.5] but gets [ 7. , 8. , 9. , 11.5, 9. , 13.5] (4th entry is different)\r\n\r\n     The error looks like :\r\n```\r\n  F.\r\n  ======================================================================\r\n  FAIL: testAvgPoolSamePadding (__main__.PoolingTest)\r\n  ----------------------------------------------------------------------\r\n  Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/pooling_ops_test.py\", line 288, in testAvgPoolSamePadding\r\n    expected=expected_output)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/pooling_ops_test.py\", line 125, in _VerifyValues\r\n    data_format, expected)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/pooling_ops_test.py\", line 108, in _VerifyOneTest\r\n    self.assertAllClose(expected, actual.flatten(), rtol=1e-5, atol=1e-6)\r\n  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/tests/pooling_ops_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 485, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n    raise AssertionError(msg)\r\n\r\n    AssertionError:\r\n    Not equal to tolerance rtol=1e-05, atol=1e-06\r\n    (mismatch 16.6666666667%)\r\n    x: array([ 7. , 8. , 9. , 11.5, 12.5, 13.5])\r\n    y: array([ 7. , 8. , 9. , 11.5, 9. , 13.5], dtype=float32)\r\n    ----------------------------------------------------------------------\r\n   Ran 2 tests in 0.368s\r\n\r\n   FAILED (failures=1)\r\n   not close where =  (array([4]),)\r\n   not close lhs =  [ 12.5]\r\n   not close rhs =  [ 9.]\r\n   not close dif =  [ 3.5]\r\n   not close tol =  [  9.10000017e-05]\r\n   dtype = float64, shape = (6,)\r\n\r\n```\r\n\r\n2. //tensorflow/python/kernel_tests:summary_image_op_test\r\n\r\n     This test is failing due to **summary.image** function returning incorrect value (0 vs expected 1) \r\n\r\n     The error looks like :\r\n  \r\n```\r\n     F..\r\n     ======================================================================\r\n     FAIL: testImageSummary (__main__.SummaryImageOpTest)\r\n     ----------------------------------------------------------------------\r\n     Traceback (most recent call last):\r\n       File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/summary_image_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/summary_image_op_test.py\", line 83, in testImageSummary\r\n         self.assertAllClose(image, adjusted[0])\r\n       File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/summary_image_op_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 485, in assertAllClose\r\n         np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n       File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n         verbose=verbose, header=header, equal_nan=equal_nan)\r\n       File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n         raise AssertionError(msg)\r\n\r\n         AssertionError:\r\n         Not equal to tolerance rtol=1e-06, atol=1e-06\r\n\r\n        (mismatch 2.85714285714%)\r\n\r\n     Expected                            Actual\r\n\r\n       ([[[[ ....                                  ([[[[ ....\r\n\r\n        [[ 136.],                                [[ 136.], \r\n         [ 106.],                                [ 106.],\r\n         [ 240.],                                [ 240.],\r\n         [ 125.],                                [ 125.],                              \r\n         [  47.],                                [  47.],\r\n         [ 105.],                                [ 105.],\r\n         [   1.]],                               [   0.]],\r\n\r\n        .....]]]                                      .....]]]\r\n         ----------------------------------------------------------------------\r\n         Ran 3 tests in 0.113s\r\n\r\n        FAILED (failures=1)\r\n        not close where =  (array([3]), array([6]), array([0]))\r\n        not close lhs =  [0]\r\n        not close rhs =  [ 1.]\r\n        not close dif =  [ 1.]\r\n        not close tol =  [  1.99999999e-06]\r\n        dtype = uint8, shape = (5, 7, 1)\r\n\r\n```\r\n\r\n3. //tensorflow/python/kernel_tests:linalg_grad_test\r\n  \r\n\tThis test is failing due to  **compute_gradient**  function returning incorrect value (-0.27654076 vs expected -0.09766237) \r\n\r\n\r\n\t The error looks like :\r\n```\r\n         ..F.........\r\n\t  ======================================================================\r\n\t  FAIL: test_MatrixSolveLsGradient_float32_10_10_1e-06 (__main__.MatrixBinaryFunctorGradientTest)\r\n\t  ----------------------------------------------------------------------\r\n\t  Traceback (most recent call last):\r\n\t  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/linalg_grad_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/linalg_grad_test.py\", line 129, in Test\r\n    \t    self.assertAllClose(theoretical, numerical, atol=tol, rtol=tol)\r\n\t  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/linalg_grad_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 485, in assertAllClose\r\n\t    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n\t  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 1411, in assert_allclose\r\n    \t    verbose=verbose, header=header, equal_nan=equal_nan)\r\n\t  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n\t    raise AssertionError(msg)\r\n  \r\n          AssertionError:\r\n\t  Not equal to tolerance rtol=0.16, atol=0.16\r\n\r\n\t  (mismatch 0.01%)\r\n\t   x: array([[-0.27454 ,  0.232044,  0.238228, ..., -0.034112, -0.373461, 0.033982],\r\n                         [-0.781968,  3.879236, -0.101676, ..., -2.040482, -3.555872,...\r\n \t   y: array([[-0.27545 ,  0.234431,  0.2381  , ..., -0.040039, -0.382814, 0.036853],\r\n          \t         [-0.782201,  3.879493, -0.102113, ..., -2.039877, -3.555654,...\r\n            ----------------------------------------------------------------------\r\n           Ran 12 tests in 1.460s\r\n\r\n          FAILED (failures=1)\r\n          not close where =  (array([40]), array([33]))\r\n          not close lhs =  [-0.27654076]\r\n          not close rhs =  [-0.09766237]\r\n          not close dif =  [ 0.1788784]\r\n          not close tol =  [ 0.17562598]\r\n          dtype = float32, shape = (100, 100)\r\n```\r\n\r\n4. //tensorflow/contrib/image:image_ops_test\r\n\r\n\tThis test is failing due to **rotate** function returning incorrect value (32 vs expected 33)\r\n\tError looks like :\r\n```\r\n        ======================================================================\r\n\tFAIL: test_rotate_even (__main__.ImageOpsTestGpu)\r\n\t----------------------------------------------------------------------\r\n\tTraceback (most recent call last):\r\n\t  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/contrib/image/python/kernel_tests/image_ops_test.py\", line 75, in test_rotate_even\r\n\t    [1, 7, 13, 19, 25, 31], [0, 6, 12, 18, 24, 30]]])\r\n\t  File \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/image/image_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 535, in assertAllEqual\r\n\t    np.testing.assert_array_equal(a, b)\r\n\t  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 871, in assert_array_equal\r\n\t    verbose=verbose, header='Arrays are not equal')\r\n\t  File \"/usr/lib64/python2.7/site-packages/numpy/testing/utils.py\", line 796, in assert_array_compare\r\n\t    raise AssertionError(msg)\r\n\r\n\tAssertionError:\r\n\tArrays are not equal\r\n\t(mismatch 1.85185185185%)\r\n\r\n\r\n\tExpected =                                                                  Actual = \r\n\r\n\t[[[0,  1,  2,  3,  4,  5],                                             [[[ 0  1  2  3  4  5]  \r\n\t  [6,  7,  8,  9,  10, 11],                                             [ 6  7  8  9 10 11]\r\n\t  [12, 13, 14, 15, 16, 17],                                             [12 13 14 15 16 17]\r\n\t  [18, 19, 20, 21, 22, 23],                                             [18 19 20 21 22 23]\r\n\t  [24, 25, 26, 27, 28, 29],                                             [24 25 26 27 28 29] \r\n\t  [30, 31, 32, 33, 34, 35]],                                            [30 31 32 33 34 35]]\r\n\t [[0,  3,  4,  11, 17, 0],                                              [[ 0  3  4 11 17  0]\r\n\t  [2,  3,  9,  16, 23, 23],                                             [ 2  3  9 16 23 23]\r\n\t  [1,  8,  15, 21, 22, 29],                                             [ 1  8 15 21 22 29]\r\n\t  [6,  13, 20, 21, 27, 34],                                             [  6 13 20 20 27 34]   \r\n\t [12, 18, 19, 26, \"33\", 33],                                            [12 18 19 26 \"32\" 33]\r\n\t  [0,  18, 24, 31, 32, 0]],                                             [ 0 18 24 31 32  0]]         \r\n\t [[5,  11, 17, 23, 29, 35],                                             [[ 5 11 17 23 29 35] \r\n\t  [4,  10, 16, 22, 28, 34],                                             [ 4 10 16 22 28 34]\r\n\t  [3,  9,  15, 21, 27, 33],                                             [ 3  9 15 21 27 33]\r\n\t  [2,  8,  14, 20, 26, 32],                                             [ 2  8 14 20 26 32]\r\n\t  [1,  7,  13, 19, 25, 31],                                             [ 1  7 13 19 25 31]\r\n\t  [0,  6,  12, 18, 24, 30]]])                                           [ 0  6 12 18 24 30]]]\r\n          ----------------------------------------------------------------------\r\n         Ran 8 tests in 31.264s\r\n\r\n        FAILED (failures=2)\r\n       not equal where =  (array([1, 1]), array([3, 4]), array([3, 4]))\r\n       not equal lhs =  [20 32]\r\n       not equal rhs =  [21 33]\r\n       not equal where =  (array([1, 1]), array([3, 4]), array([3, 4]))\r\n       not equal lhs =  [20 32]\r\n       not equal rhs =  [21 33]\r\n\r\n```\r\n\r\nIn above tests checking 2 arrays are equal or not (expected and actual) each array has 100+ entries, out of which only one entry mismatching and test fails.\r\nHere very difficult to pinpoint the issue on power platform as all entries matching except one. For now I think better solution would be just change the tolerance as required by adding power specific condition (like I mentioned in previous comment). Please provide your view on this.Thanks!\r\n\r\n", "My concern is, we have very numerical differences.\r\nIt is possible TF is just not working on ppc64le, and it needs to be fixed.\r\nIf we suppress these warnings, sure you might have all tests passing, but people using TF may run into worse issues getting wrong outputs, models not converging, etc.\r\n\r\nIMO these numerical differences are too large to ignore, and will cause issues using TF on ppc platform. Therefore, I am against changing tolerances fro `1e-6` to `10`. You can just create a fork of TensorFlow and make those changes there. But I think you will eventually need to debug and get to the bottom of these differences, because they look like real bugs with TF on ppc64le to me.", "Thanks @gunan , for your suggestions and pointers. \r\n\r\nNow I am debugging more into these failures and trying to find the root cause.  ", "This GitHub issue was created for 12 test failures , now all got resolved for me. Please find the details below (currently I am working on TF1.3.1 ):\r\n\r\n**1     //tensorflow/compiler/tests:binary_ops_test_cpu** \r\n\r\nThis test was failing with the error : `nan vs expected inf.`\r\nI ran sample code using inputs equivalent to ones used in the test case and using tensorflow \"realdiv\" API :\r\n\t\r\n```\r\n\t>>> import tensorflow as tf\r\n\t>>> import numpy as np\r\n\t>>> sess = tf.Session()\r\n\t>>> dtype = np.float32\r\n\t>>> a =  np.array([3, 3, -1.5, -8, 44], dtype=dtype)\r\n\t>>> b = np.array([2, -2, 7, -4, 0], dtype=dtype)\r\n\t>>> pb = tf.placeholder(b.dtype, b.shape, name=\"b\")\r\n\t>>> pa = tf.placeholder(a.dtype, a.shape, name=\"a\")\r\n\t>>> output = tf.realdiv(pa, pb)\r\n\t>>> result = sess.run(output, {pa: a, pb: b})\r\n\t>>> print(result)\r\n\t[ 1.5        -1.5        -0.21428572    2.     inf]\r\n\t\r\n```\r\n\r\nThis is printing results as expected by the test case.\r\n\r\n**2   //tensorflow/contrib/image:image_ops_test**\r\n\r\nThis test was failing on RHEL7.3 vm. Now this machine upgraded to RHEL7.4.\r\nI reran \"image_ops_test\"  test on new OS and test passed successfully without any changes.\r\n\r\n**3\t//tensorflow/tools/test:cast_op_benchmark\r\n4\t//tensorflow/tools/test:rnn_op_benchmark**\r\n\r\nSubmitted code changes to fix these 2 tests.\r\nRelevant commit : https://github.com/tensorflow/tensorflow/commit/69ba4d3d49bd5775131ae7f00830a41f478dbbf5\r\n\r\n( Note - Changes are available in TF1.3.1 )\r\n\r\n**5      //tensorflow/python/kernel_tests:sparse_matmul_op_test\r\n6\t//tensorflow/core/kernels:sparse_matmul_op_test_gpu**\r\n\r\nThese 2 tests fixed in PR https://github.com/tensorflow/tensorflow/pull/12138\r\n\r\n\r\n( Note - Changes are not available in TF1.3.1 )\r\n\r\n**7\t//tensorflow/compiler/tests:pooling_ops_test_cpu\r\n8\t//tensorflow/python/kernel_tests:linalg_grad_test\r\n9\t//tensorflow/compiler/tests:ternary_ops_test_cpu**\r\n\r\nThese 3 tests are passing in TF1.3.1 (without any changes).\r\n\r\n**10 //tensorflow/python/kernel_tests:summary_image_op_test test fixed in PR** https://github.com/tensorflow/tensorflow/pull/15147\r\n\r\n( Note - Changes are not available in TF1.3.1 )\r\n\r\n**11\t//tensorflow/core:platform_profile_utils_cpu_utils_test\r\n12\t//tensorflow/python/kernel_tests:cast_op_test**\r\n\r\nThese 2 tests fixed in PR  https://github.com/tensorflow/tensorflow/pull/10522\r\n( Note - Changes are available in TF1.3.1 )\r\n\r\nThanks !!!"]}, {"number": 8944, "title": "Merge pull request #1 from tensorflow/master", "body": "Update", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "No, did it backwards, sorry."]}, {"number": 8943, "title": "Ibverbs-based RDMA path", "body": "This patch introduces an ibverbs-based RDMA path between servers for tensor transfer (weights, gradients, etc). The existing gRPC path remains and is responsible for \"administrative\" tasks, such as setting up the Rdma path, exchanging computation graphs, etc. Design details can be found in the README file below:\r\nhttps://github.com/yahoo/tensorflow/blob/verbs_rdma/tensorflow/contrib/verbs/README.md\r\n", "comments": ["Can one of the admins verify this patch?", "Via another channel I am in communication wtih @junshi15 about this PR.  It needs to be updated slightly to the most recent version of the main repository, then he will submit again.", "rebased to master branch, conflicts resolved.", "Jenkins, test this please.", "Jenkins, test this please", "@junshi15 : the buildifier sanity check is failing, and that appears to be blocking all further testing.\r\n\r\nIt looks like you can run \r\nrun buildifier tensorflow/contrib/verbs/BUILD\r\nor fix the errors by hand.  It looks like some build rule deps are not ordered properly.\r\n", "working on  tensorflow/contrib/verbs/BUILD, buildifier (https://github.com/bazelbuild/buildtools) does not build for me, so I will edit it manually.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "What's the problem with two BUILD files that failed sanity checks? indentation? ", "Not sure, trying to figure that out.\n\nOn Fri, Apr 7, 2017 at 1:36 PM, Jun Shi <notifications@github.com> wrote:\n\n> What's the problem with two BUILD files that failed sanity checks?\n> indentation?\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8943#issuecomment-292644909>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818e4BFtUkrUAwSSpqvX03yORDYEPLks5rtp47gaJpZM4MyR40>\n> .\n>\n", "Jenkins, test this please", "Those failed are due to unavailability of infiniband/verbs.h. But I do see the following line, meaning verbs is not turned on. I am looking into it.\r\n\r\n\"No VERBS support will be enabled for TensorFlow\"", "Yeah, note that we disable new configuration options by default. Please try building with verbs disabled.", "How do I disable verbs? On my local box, during ./configure, if I answer \"n\" to the verbs question, verbs option is disabled, and I see a message confirms that. How is this done with Jenkins? I saw this line \"No VERBS support will be enabled for TensorFlow\" in Jenkins build log. Does this mean verbs was already turned off? Do I have to do something explicitly?", "Yeah, don't worry about Jenkins. It'll disable verbs for you.\r\n\r\nI just meant you should try disabling verbs on your local machine and verifying that it builds. According to the build log, it seems to include the verbs dependency despite being disabled.", "When I disable verbs on my local box, verbs related code is not included. Not sure why it is not so on Jenkins. ", "I did another experiment on my local box by moving /usr/include/infiniband/verbs.h to a different location. Then I disabled verbs, the build went through. If I enable verbs, it complained about unable to find verbs.h. So the code works if options are enabled/disabled with ./configure, at least on my local box.\r\n\r\nDoes Jenkins run configure as well? For some options, such as jemalloc and verbs, I see the printout from configure, but I see nothing about other options, such as HDFS.", "The underlying issue is that `bazel test //tensorflow/...` will try to build all rules, including the new ones introduced in //tensorflow/contrib/verbs.\r\n\r\nThe right fix is to wrap code that requires external verb headers in a #define (look around the code for #ifdef GOOGLE_CUDA for an example) and set that define if the rdma config is set.\r\n\r\nA simple way to test if your code works is to run: `bazel test //tensorflow/contrib/verbs/...`", "See `//tensorflow/contrib/nccl` for an example.", "Adding a #define means we will need a \"--config=verbs\" in the bazel build command line, right? Our early internal version had that. I hated it because I had to enable verbs in two places, one is in ./configure, the other is \"bazel build --config=verbs\". If this is what it takes, I will resurrect that code. I will take a look at the example you mentioned.", "No, you only need to conditionally set the #define in the BUILD rule. See the jemalloc example here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L194", "I added a flag \"TENSORFLOW_USE_VERBS\" to exclude all the verbs related code. Could you please ask Jenkins to start the build? \r\n\r\nBTW, this line won't compile, will it? I suppose it's a typo for \"#ifdef\"  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/nccl/kernels/nccl_ops.cc#L16", "Jenkins, test this please", "Aha, buildifier again, :-(", "Buildifier is not available in my local box. So I manually fixed the file. Hopefully it will make buildifier happy. Could any of you start the build? Sorry for the inconvenience. I really appreciate it.", "Jenkins, test this please.", "Do we have an example where link option can be included/excluded with a flag?\r\n\"#define\" only takes care of header files. I need to disable a link option below:\r\nhttps://github.com/yahoo/tensorflow/blob/5776b4ffc851cbcb742952c6721739f45f0dbf6f/tensorflow/contrib/verbs/BUILD#L138-L140\r\n", "I found one here, will give it a try.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/BUILD#L1297-L1300", "Added a switch to turn on/off link option \"-libverbs\". Please initiate a build. Thank you.", "Jenkins test this please", "Can one of the admins verify this patch?", "Added \"#ifdef TENSORFLOW_USE_VERBS\" to two more files. Please initiate the build. Thanks.", "Jenkins, test this please", "@poxvoculi are we good with this then?", "Jenkins, test this please.", "Typos in README.md:\r\n`fro` in line 17\r\n`transimssion` in line 39", "Right. @wydwww could you please send a follow-up PR?", "@wydwww Thanks for catching the typos.", "#9926 \r\n When I training inception model, I got the following error\r\n2017-05-18 09:50:50.534662: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:2\r\n2017-05-18 09:51:02.357013: I tensorflow/contrib/verbs/rdma.cc:518] channel already connected\r\n2017-05-18 09:51:02.357102: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:worker/replica:0/task:0\r\n2017-05-18 09:51:02.358775: I tensorflow/contrib/verbs/rdma_mgr.cc:56] connecting to remote node /job:ps/replica:0/task:0\r\n2017-05-18 09:51:07.867035: I tensorflow/contrib/verbs/rdma.cc:518] channel already connected\r\n2017-05-18 09:51:08.095058: I tensorflow/contrib/verbs/rdma.cc:518] channel already connected\r\nINFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=3; total_num_replicas=3\r\nINFO:tensorflow:2017-05-18 09:52:16.455855 Supervisor\r\n2017-05-18 09:52:19.322077: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 126a016806cf6873 with config: \r\nallow_soft_placement: true\r\n\r\n2017-05-18 09:52:27.325014: F tensorflow/contrib/verbs/rdma.cc:129] Check failed: wc_[i].status == IBV_WC_SUCCESS Failed status \r\ntransport retry counter exceeded 12 40500432 129\r\nAborted (core dumped)\r\nthis is my command line\r\nCUDA_VISIBLE_DEVICES=1 bazel-bin/inception/imagenet_distributed_train \\\r\n> --batch_size=32 \\\r\n> --data_dir=/data0/imagenet-data \\\r\n> --job_name='worker' \\\r\n> --task_id=1 \\\r\n> --ps_hosts='localhost:2222' \\\r\n> --worker_hosts='localhost:2223,localhost:2224,localhost:2225' --protocol='grpc+verbs'\r\n\r\n@junshi15 @llhe   Can you help me to solve this problem?", "@fanlu Please use this link https://github.com/tensorflow/tensorflow/issues/9926", "verbs build failed on current master branch, last [commit](https://github.com/tensorflow/tensorflow/commit/82456f9fee7c4b5e9beb100e59ba8dc5eb688b28).\r\n```\r\nERROR: /home/dongziming/tensorflow/tensorflow/contrib/verbs/BUILD:158:1: C++ compilation of rule '//tensorflow/contrib/verbs:verbs_server_lib' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 164 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/contrib/verbs/verbs_server_lib.cc: In constructor 'tensorflow::{anonymous}::VerbsServerRegistrar::VerbsServerRegistrar()':\r\ntensorflow/contrib/verbs/verbs_server_lib.cc:153:5: error: 'gpr_allocation_functions' was not declared in this scope\r\n     gpr_allocation_functions alloc_fns;\r\n     ^\r\ntensorflow/contrib/verbs/verbs_server_lib.cc:154:5: error: 'alloc_fns' was not declared in this scope\r\n     alloc_fns.malloc_fn = port::Malloc;\r\n     ^\r\ntensorflow/contrib/verbs/verbs_server_lib.cc:157:43: error: 'gpr_set_allocation_functions' was not declared in this scope\r\n     gpr_set_allocation_functions(alloc_fns);\r\n                                           ^\r\n```", "@junshi15 : we are not able to use NIC bonding feature with this PR. is this PR supports NIC bonding? If not is it possible to add NIC bonding feature with verbs. May I know how to handle in code? do you have any plans to add it in near future?", "@vdevaram I do not have experience with NIC bonding and do not have a plan to add this feature in the future. Your contributions are welcome."]}, {"number": 8942, "title": "Remove empty line in dummy file", "body": "that causes issue during pull.", "comments": ["@tensorflow-jenkins test this please."]}, {"number": 8941, "title": "Log Selected Convolution Algorithm", "body": "Reposting from https://github.com/tensorflow/tensorflow/issues/8928#issuecomment-291330814:\r\n\r\n>It might make sense for the selected algorithm to be logged either to the logging system or maybe in the RunMetadata protocol buffer. \r\n\r\n/CC @asimshankar ", "comments": ["FYI @zheng-xq ", "It is logged with TF_CPP_MIN_VLOG_LEVEL=1. [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops_gpu.h#L221)", "In that case I will close."]}, {"number": 8940, "title": "Updating update version script", "body": "Updating the install source files to be rc1 compliant. Updating the update_version script as well to handle these changes.", "comments": []}, {"number": 8939, "title": "Update the issue template to be more compact.", "body": "", "comments": []}]