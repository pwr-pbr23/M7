[{"number": 9152, "title": "can not run mnistcnn.py, servers waiting for eachother ", "body": "Hello everyone, I tried to run this script on three AWS machines. One parameter server and two workers.\r\nHowever, the ps is always in \"Initializing session...\" and workers always waiting for the ps.\r\n\r\nI believe the challenge is from here: \r\nps definitions:\r\nflags.DEFINE_integer(\"worker_index\", 0,\r\n                     \"Worker task index, should be >= 0. worker_index=0 is \"\r\n                     \"the master worker task the performs the variable \"\r\n                     \"initialization \")\r\n\r\nflags.DEFINE_string(\"workers\", \"34.205.143.107:2222,54.83.142.82:2222\",\r\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\r\n\r\nflags.DEFINE_string(\"parameter_servers\", \"http://54.86.223.142:2222\",\r\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\r\n\r\nflags.DEFINE_string(\"worker_grpc_url\", \"grpc://54.86.223.142:2222\",\r\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\r\n                    \"grpc://tf-worker0:2222)\")\r\n\r\nworker definitions:\r\nflags.DEFINE_integer(\"worker_index\", 1,\r\n                     \"Worker task index, should be >= 0. worker_index=0 is \"\r\n                     \"the master worker task the performs the variable \"\r\n                     \"initialization \")\r\n\r\nflags.DEFINE_string(\"workers\", \"34.205.143.107:2222,54.83.142.82:2222\",\r\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\r\n\r\nflags.DEFINE_string(\"parameter_servers\", \"http://54.86.223.142:2222\",\r\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\r\n\r\nflags.DEFINE_string(\"worker_grpc_url\", \"grpc://34.205.143.107:2222\",\r\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\r\n                    \"grpc://tf-worker0:2222)\")\r\nsecond worker\r\nflags.DEFINE_integer(\"worker_index\", 2,\r\n                     \"Worker task index, should be >= 0. worker_index=0 is \"\r\n                     \"the master worker task the performs the variable \"\r\n                     \"initialization \")\r\n\r\nflags.DEFINE_string(\"workers\", \"34.205.143.107:2222,54.83.142.82:2222\",\r\n                    \"The worker url list, separated by comma (e.g. tf-worker1:2222,1.2.3.4:2222)\")\r\n\r\nflags.DEFINE_string(\"parameter_servers\", \"http://54.86.223.142:2222\",\r\n                    \"The ps url list, separated by comma (e.g. tf-ps2:2222,1.2.3.5:2222)\")\r\n\r\nflags.DEFINE_string(\"worker_grpc_url\", \"grpc://83.142.82:2222\",\r\n                    \"Worker GRPC URL (e.g., grpc://1.2.3.4:2222, or \"\r\n                    \"grpc://tf-worker0:2222)\")\r\n[mnistcnn.txt](https://github.com/tensorflow/tensorflow/files/915085/mnistcnn.txt)\r\n\r\n\r\nWe appreciate any assistance .\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9151, "title": "Remove boston_input_fn_with_queue", "body": "since it is not used.", "comments": ["Can one of the admins verify this patch?", "@fwiffo what do you think?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Test failure seems unrelated.", "Right. The test failure in //tensorflow/contrib/xla_tf_graph:xla_tf_graph_util_test is unrelated. (But we need to look into that separately.) Merging PR."]}, {"number": 9149, "title": "Optimization flags are erroneously ignored in compilation of Tensorflow 1.1.0-rc1", "body": "I am getting a large number of optimization flag related warnings in compilation of Tensorflow 1.1.0-rc1 from source\r\n\r\n```\r\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\r\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\r\n    ^\r\n```\r\nSystem Information:  \r\nOS:CentOS 7\r\nGCC: gcc 4.8.5\r\nPython: python3.5\r\nTensorflow version:  TensorFlow 1.1.0-rc1\r\nBazel:  version 0.4.5\r\nCUDA: CUDA 8.0 with GTX 1080 GPU\r\n\r\nin the prompt of ./configure:\r\n\r\n```\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] Y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\n```\r\nexplicitly typing `-march=native -O2` or `-O2` in the line `Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: `\"doesn't help. \r\n\r\nto compile I used:\r\n\r\n`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nI tried as early as tensorflow version 1.0.0 and I always get the \"`warning _FORTIFY_SOURCE requires compiling with optimization (-O)`\". Tensorflow version 0.12.1 doesn't seem to have this problem for me. \r\n\r\nRelated issue:\r\n#2153 \r\n", "comments": ["Could you pass `-s` to bazel build so we can see what the flags are that are being passed to g++ when this error is raised? I just tried compiling a little bit at HEAD and it seems to be passing `-O2` when I say `--config=opt`.", "@jart \r\nThanks so much for your help. Here is my ./configure prompt\r\n```\r\n./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] \r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] \r\nNo Hadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] \r\nNo XLA support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/lib/python3.5/site-packages\r\n  /usr/lib64/python3.5/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3.5/site-packages]\r\n\r\nUsing python library path: /usr/lib/python3.5/site-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] \r\nnvcc will be used as CUDA compiler\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 6.1\r\n```\r\n\r\nAfter executing` bazel build -s --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nthe output looks like the following, I am not sure if I am copying the right snippet with the information you need. Please let me know.\r\n\r\n`external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_stream.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_stream.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local_linux-py3-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local_linux-py3-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-py3-opt/genfiles/external/snappy -iquote external/local_config_cuda -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda -isystem external/jemalloc/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_stream.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_stream.pic.o)\r\n`\r\n\r\n", "It helps if you can identify the specific invocation that triggered the warning, because -O2 is present in that one.\r\n\r\nAlso for future reference, we have an on-call rotation for triaging issues. It's not necessary to tag individuals when filing tickets. Let us take care of that for you.", "cc: @wichtounet @vladfi1 who also reported this in https://github.com/tensorflow/tensorflow/issues/2153.", "Thanks jart. I have removed all individual tags. Sorry about that.\r\n\r\nHere is the snippet of the bazel output for one file that has this `_FORTIFY_SOURCE requires compiling with optimization (-O)`\r\n\r\n```\r\n>>>>> # //tensorflow/core/kernels:segment_reduction_ops [action 'Compiling tensorflow/core/kernels/segment_reduction_ops.cc']\r\n(cd /home/user/.cache/bazel/_bazel_user/f272c01b20e6327aedaf5e56025f70d2/execroot/tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/usr/lib:/home/cad/MATLAB/R2016b/sys/os/glnxa64:/usr/lib64:/usr/local/lib:/home/cad/gurobi/linux64/lib:/usr/local/cuda/lib64 \\\r\n    PATH=/usr/local/cuda/bin:/home/cad/gurobi/linux64/bin:/usr/local/cuda/bin:/home/cad/intel/compilers_and_libraries_2017.1.132/linux/bin/intel64:/home/cad/intel/compilers_and_libraries_2017.1.132/linux/mpi/intel64/bin:/home/cad/intel/debugger_2017/gdb/intel64_mic/bin:/home/cad/gurobi/linux64/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/bin:/sbin:/home/user/.local/bin:/home/user/bin \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_VERSION='' \\\r\n    TF_CUDNN_VERSION='' \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/segment_reduction_ops/tensorflow/core/kernels/segment_reduction_ops.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/segment_reduction_ops/tensorflow/core/kernels/segment_reduction_ops.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_sycl -iquote external/jemalloc -iquote bazel-out/local_linux-py3-opt/genfiles/external/jemalloc -iquote external/protobuf -iquote bazel-out/local_linux-py3-opt/genfiles/external/protobuf -iquote external/gif_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -iquote external/snappy -iquote bazel-out/local_linux-py3-opt/genfiles/external/snappy -iquote external/local_config_cuda -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/jemalloc/include -isystem external/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/protobuf/src -isystem external/gif_archive/lib -isystem bazel-out/local_linux-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda/include -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/kernels/segment_reduction_ops.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/core/kernels/_objs/segment_reduction_ops/tensorflow/core/kernels/segment_reduction_ops.pic.o)\r\nINFO: From Compiling tensorflow/core/kernels/depthtospace_op_gpu.cu.cc:\r\nIn file included from external/local_config_cuda/cuda/include/host_config.h:173:0,\r\n                 from external/local_config_cuda/cuda/include/cuda_runtime.h:78,\r\n                 from <command-line>:0:\r\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\r\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\r\n    ^\r\nIn file included from external/local_config_cuda/cuda/include/host_config.h:173:0,\r\n                 from external/local_config_cuda/cuda/include/cuda_runtime.h:78,\r\n                 from <command-line>:0:\r\n/usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]\r\n #  warning _FORTIFY_SOURCE requires compiling with optimization (-O)\r\n\r\n```\r\n\r\nPlease let me know if there is any other information that I can provide. Thanks!", "That's interesting. It still says that, even though `-O2` is being passed.\r\n\r\nCentOS7 is definitely in our support matrix. However this is a problem Bazel might need to solve. There seems to be a bug tracking this. https://github.com/bazelbuild/bazel/issues/1762 It might help if you tell them your OS and GCC version combos.\r\n\r\nSince this is only a warning, no workaround should be necessary.\r\n\r\ncc: @wichtounet @vladfi1\r\nSee also: https://github.com/tensorflow/serving/issues/137", "jart I would say it is more than just a warning. It was mentioned in #2153 that that a tensorflow binary compiled with this warning is 10% slower than the binary provided by Google. Can you further advise please?", "10%, while nontrivial, is still a tenth of a order of a magnitude. As you mentioned, the binary provided by Google is still a possibility. I agree that this is an issue that needs to be solved. However it only appears to affect certain platforms. It doesn't appear to be an issue in TensorFlow itself. Please let me know if my assessment is inaccurate.", "jart I agree with your assessment. Meanwhile, can you please tag someone who may have more information regarding this issue?", "If you're Mao Mao then I'm Cao Cao when it comes to build systems. You have my assurance that I've offered you first class support today.", "Thank you jart.", "Any updates on this?", "The problem continues with v1.1.0 release. ", "Since this is only a build warning for CentOS, and multiple users are now reporting this, I'm going to reopen and mark it contributions welcome.", "I have the same issue with openSUSE, python 2.7, GTX 980 Ti, gcc 4.8.3. and the latest version of bazel.", "same error on Fedora 25, anaconda 3, GTX 750 TI, gcc54, bazel 5", "I think the problem is, we configure using SIMD instruction sets when running with `--config=opt`. You should be able to solve this by adding `--copt=-O2` to our bazelrc when running configure.\r\nAs I cannot test this, I will leave this as contributions welcome.", "Closing as this is resolved"]}, {"number": 9148, "title": "Feature Request - conv1d_transpose", "body": "Would make things easier", "comments": ["Didn't realize already discussed"]}, {"number": 9147, "title": "`model_checkpoint_dir` in `ProjectorConfig` is not implemented", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n`model_checkpoint_dir` is defined in the [projector_config.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/plugins/projector/projector_config.proto#L45) definition but currently, it is not functional. I believe there is missing logic in [tensorboard/plugins/projector/projector_plugin.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/plugins/projector/projector_plugin.py): in fact, a quick search reveals that `model_checkpoint_dir` does not appear in source. There is good reason to support this (seemingly planned) feature as checkpoint files and summary event files are often saved under different directories. \r\n\r\nI would love to contribute a solution.\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["@dsmilkov is the best person to comment on the timeline of opening up that functionality.", "@itsmeolivia i think this should be migrated to the tensorboard repo?"]}, {"number": 9146, "title": "Cannot move checkpoint files saved with an absolute path", "body": "When the checkpoint save path is an absolute path, moving the checkpoint results in a failure. \r\n\r\nTensorFlow was installed from source branch v1.0.1\r\n\r\nPython script to reproduce:\r\n\r\n```python\r\n#!/usr/bin/env python\r\nfrom __future__ import print_function\r\n\r\nimport sys\r\n\r\nimport os\r\nimport tensorflow as tf\r\n\r\n\r\ndef load_and_save(saver, session, model_dir):\r\n    print(\"Looking for models in : \" + model_dir)\r\n    incumbent = tf.train.latest_checkpoint(model_dir)\r\n    if incumbent is not None:\r\n        print(\"Loading incumbent: \" + incumbent)\r\n        saver.restore(session, incumbent)\r\n    else:\r\n        print(\"No incumbent found...\")\r\n        tf.global_variables_initializer().run()\r\n\r\n    out_path = os.path.join(model_dir, 'model')\r\n    print(\"Saving to: \" + out_path)\r\n    saver.save(session, out_path)\r\n\r\n\r\ndef test(first_dir, second_dir):\r\n    # clean up for idempotence\r\n    os.system('rm -rf {}'.format(second_dir))\r\n\r\n    if not os.path.exists(first_dir):\r\n        os.makedirs(first_dir)\r\n\r\n    with tf.Graph().as_default(), tf.Session() as session:\r\n        test_var = tf.get_variable('test', [50, 50], dtype=tf.float32, initializer=tf.random_normal_initializer())\r\n        saver = tf.train.Saver([test_var])\r\n\r\n        # save to the first abs path dir\r\n        load_and_save(saver, session, first_dir)\r\n\r\n        # move to to the second dir\r\n        mv_command = 'mv {} {}'.format(first_dir, second_dir)\r\n        print(mv_command)\r\n        os.system(mv_command)\r\n\r\n        # attempt to load from the second dir\r\n        load_and_save(saver, session, second_dir)\r\n\r\n\r\ndef main():\r\n    # WORKS\r\n    test('./FIRST_DIR', './SECOND_DIR')\r\n\r\n    # BUG?\r\n    test(os.path.abspath('./FIRST_DIR'), './SECOND_DIR')\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n\r\n```\r\n\r\nLog output\r\n\r\n```\r\nLooking for models in : ./FIRST_DIR\r\nNo incumbent found...\r\nSaving to: ./FIRST_DIR/model\r\nmv ./FIRST_DIR ./SECOND_DIR\r\nLooking for models in : ./SECOND_DIR\r\nLoading incumbent: ./SECOND_DIR/model\r\nSaving to: ./SECOND_DIR/model\r\nLooking for models in : /home/rklopfer/REPORT/FIRST_DIR\r\nNo incumbent found...\r\nSaving to: /home/rklopfer/REPORT/FIRST_DIR/model\r\nmv /home/rklopfer/REPORT/FIRST_DIR ./SECOND_DIR\r\nLooking for models in : ./SECOND_DIR\r\nTraceback (most recent call last):\r\n  File \"./bug_report.py\", line 57, in <module>\r\n    sys.exit(main())\r\n  File \"./bug_report.py\", line 53, in main\r\n    test(os.path.abspath('./FIRST_DIR'), './SECOND_DIR')\r\n  File \"./bug_report.py\", line 45, in test\r\n    load_and_save(saver, session, second_dir)\r\n  File \"./bug_report.py\", line 12, in load_and_save\r\n    incumbent = tf.train.latest_checkpoint(model_dir)\r\n  File \"/home/rklopfer/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1482, in latest_checkpoint\r\n    if file_io.get_matching_files(v2_path) or file_io.get_matching_files(\r\n  File \"/home/rklopfer/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 269, in get_matching_files\r\n    compat.as_bytes(filename), status)]\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/rklopfer/.virtualenvs/tf/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/rklopfer/REPORT/FIRST_DIR\r\n\r\n\r\n```\r\n", "comments": ["An option to the `Saver` to use relative paths was added recently in https://github.com/tensorflow/tensorflow/commit/2843a7867d51c2cf065b85899ea0b9564e4d9db9 (which is part of the 1.1 release, not 1.0).\r\n\r\nFrom that, it seems that this behavior of saving the absolute path in the checkpoint is intentional (when an absolute path has been provided to the `save` function).\r\n\r\nCC @sherrym @martinwicke @ispirmustafa in case they have any comments they want to add.", "We have found nobody with a valid use case for absolute paths. However, changing this behavior would be a behavior change and we would rather add the option than break someone, hence the option. We have changed `Estimator` to always save relative paths.", "Checkpoints saved with an absolute path will throw exceptions on load if the checkpoint is moved. I am having trouble coming up with instances where this behavior is desirable. It sounds like there is a work around or a fix in v1.1, but I'll just make sure I use relative paths from now on. ", "I hit this issue and the scenario is this: on a windows machine, while all my source tree is in one partition, I am running out of space and have to save the data into another partition. I can work around it now using symbolic link but I believe more ppl will hit this as Tensorflow gets more popular "]}, {"number": 9145, "title": "Fix loading metadata in tensorboard embedding projector.", "body": "Fixes loading metadata (eg. vocab files, sprites) for display in\r\ntensorboard's embedding projector plugin.\r\n\r\nSince [1], `project_plugin.pbtxt` is looked for under the directory\r\n`$LOGDIR/plugins/org_tensorflow_tensorboard_projector`, different to the\r\ndocs and where `projector.visualise_embeddings()` stores the file.\r\nRevert to the previous location.\r\n\r\n[1] See #9059, commit 52dcb259", "comments": ["Can one of the admins verify this patch?", "OK, checking with author of that commit.", "Hi @darrengarvey,\r\n\r\nThe change we made is about an oncoming higher-level API that should be backwards compatible. Did it stop working for you? Can you share with us the directory structure of your `LOGDIR` (no contents, just file/dir names will suffice)  so we can reproduce the error?\r\n\r\nThanks!", "Thanks @dsmilkov. I can get the exact structure later, but `LOGDIR` is the directory given to a `SummaryWriter` and the same passed to `tensorboard --logdir $LOGDIR`. It contains `events-*`, `checkpoint`, the run meta stuff as normal.\r\n\r\nI've added a `metadata.tsv` in the correct format and a `projector_config.pbtxt` that looks like:\r\n\r\n``\r\nembeddings {\r\n  tensor_name: 'word_embedding'\r\n  metadata_path: '/full/path/to/LOGDIR/metadata.tsv'\r\n}\r\n``\r\n\r\n(as per. https://www.tensorflow.org/get_started/embedding_viz)\r\n(also as per. [`projector.visualize_embeddings()`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorboard/plugins/projector/__init__.py#L39))\r\n\r\nThis doesn't work for me (ie. the metadata isn't returned from the API call that tensorboard frontend makes). As in, the metadata isn't shown, even though the embedding projector otherwise does work.\r\n\r\nIf I move the `projector_config.pbtxt` to `$LOGDIR/plugins/org_tensorflow_tensorboard_projector` then the metadata shows up, but that's a very strange path and not what the docs suggest.\r\n\r\nIf I move [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/plugins/projector/projector_plugin.py#L364) - which was added in the referenced commit - to the bottom of the function then it also works, but presumably breaks on the next time `_latest_checkpoints_changed()` is called. That seemed like a hack, hence the suggested \"fix\".", "Thanks for helping identify the problem. We can't accept your fix unfortunately since it won't work for the oncoming new API, but we have a fix internally that should surface on Github as a commit within couple of days. "]}, {"number": 9144, "title": "regarding error message of tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform", "body": "I have created a Python development environment support Python 2.7,\r\n\r\n```\r\n(py2.7) [abcd@cluster1830 ~]$ python --version\r\nPython 2.7.9\r\n```\r\nThen I was trying to install Tensorflow following [this guide ](https://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package)\r\n\r\n> (py2.7) [abcd@cluster1830 ~]$ pip install --upgrade /data/pythonlibs/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl\r\n> tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\n> (py2.7) [abcd@cluster1830 ~]$ pip install /data/pythonlibs/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl\r\n> tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\n> (py2.7) [abcd@cluster1830 ~]$ pip2 install /data/pythonlibs/tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl\r\n> tensorflow_gpu-1.0.1-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.\r\n> \r\n\r\nNone of the above options works, what can be the reason underlying this failure?\r\n\r\n\r\n", "comments": ["As per [the guide](https://www.tensorflow.org/install/install_linux#python_27), please use the URL for Python 2.7 and not 3.4: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\r\n\r\nAlternatively, just use `pip install tensorflow`"]}, {"number": 9143, "title": "Branch 152843957", "body": "Conflict in tensorflow/python/summary/writer/event_file_writer.py: keep both sides (flush + suffix change).", "comments": []}, {"number": 9141, "title": "Fused batch renormalization", "body": "Recently introduced batch renormalization could be very useful, but without support for a fused batch norm, it has limited use (XLA JIT still has too many problems to be used instead of a fused version)", "comments": ["@fchollet Our friend @ygoncharov has a brief request relating to batch normalization, which I noticed you worked on in the past.", "We will support fused BN in the future (ideally this should be done via XLA rather than via explicit TF Python code, but we will see). If you need fused BN for the time being, you can simply use the BN layer in `contrib.layers` (although it is now deprecated).", "@fchollet Is there value in keeping this issue open and assigning it to you? That way when fused BN come along, you can put `Fixed #9141` in the CL description to notify subscribers.", "Please note that my request was about batch REnormalization (https://arxiv.org/pdf/1702.03275.pdf) that recently appeared as an option for batch_norm in contrib.layers, not about regular batch norm.", "From my understanding, Fused Batch Norm is mostly faster because of CuDNN implementation.\r\n\r\nCuDNN implementation return the variance (while it internally compute the inverse of stddev to normalize), but Batch Renormalization apply the moving average on the the stddev, so you would have to apply again rsqrt to find the inverse of stddev and the stddev, which is I guess the slowest part of the batch norm.\r\n\r\nSo unless Nvidia decide to do a CuDNN implementation of batch renormalization, XLA should be the way to go.", "What @Caenorst said sounds like a compelling reason to hold off on implementing this feature for the time being. If my cursory assessment is inaccurate, please let me know and I'll re-open this issue.", "@Caenorst are you sure that fused batch norm is implemented on the CuDNN level, not on the tf ops level? It seems that a new tensorflow op would be enough to implement batch renormalization with fusion, no new CuDNN functions are needed.\r\n\r\n@jart it seems that it is a reasonable feature request, unless there is a decision to do all such optimizations via XLA JIT\r\n", "Fused Batch Norm **is** CuDNN batch normalization, on the original/non-fused batch normalization code I did not found any CuDNN call.\r\n\r\nCuDNN is not open sourced so I don't think there is a point to rebuild it from scratch while one of the main objective of XLA JIT is to \"fuse\" op to get speedup.", "@Caenorst @jart \r\n\r\nThat's incorrect. Non-fused batch normalization is dramatically slower than the fused version when using NCHW \u2013 in fact, when using non-fused BN, NCHW is _slower_ than NHWC, despite all the convolutions going faster: https://github.com/tensorflow/tensorflow/issues/7551#issuecomment-280421351\r\n\r\nNon-fused batch norm currently slows things down so much in NCHW that it's unusable in that mode, and this is actually a pretty significant performance landmine.", "The minor performance hit from not using cuDNN is \"whatever\" to me, but the much larger performance hit of having to drop to NHWC to not suffer that massive performance penalty detailed in my link above is a pretty big deal.\r\n\r\nThe **6x** slowdown with non-fused batch norm in NCHW means that, in practice, this is a pretty big performance issue for most users, to the point where I think it is in fact quite compelling.", "It seems that the options are:\r\n1. Implement a fused version of batch renorm as a custom op without using CuDNN (it is a bigger task than an original fused batch norm that, as @Caenorst correctly noted, relies on CuDNN)\r\n2. Make a strategic decision to rely on XLA JIT for all such cases (even though it is still in alpha)\r\n3. Wait for CuDNN 7+ that hopefully implements batch renorm functions and then implement a corresponding op", "@taion Thank you for you correction. So looking at issue #7551 doesn't it mean that the issue is at a lower level than Batch Normalization ? Or should we build a fused kernel every time we have to do a broadcast add or scale on non-last axis (or use bias_add given your last comment) ?", "It's probable that I should split that into two issues. I don't think the broadcasted arithmetic is the only issue with non-fused BN when using NCHW. And anyway, as far as I'm aware, there's no multiplication equivalent to `bias_add`, so there's not a good way to implement the scale operation.\r\n\r\nCertainly there is some problem here at a lower level than non-fused BN, but in practice my models ran fine as soon as I switched to the fused BN, so perhaps those effects are less important.", "@broune I want to bring to your attention a discussion relating to XLA JIT. How would you recommend this issue be triaged?", "We are aware that batch normalization can be improved on GPU (and presumably on CPU). We are currently working on enabling fusion nodes with multiple outputs (also for other reasons), and this may help here, since some batchnorm division ops are not getting fused because they are needed by several other ops, but those ops, including the divide, could be combined into a single fusion node if fusion nodes could have multiple outputs. That would prevent the intermediate data from getting written to memory. The division nodes in particular are not getting fused because the XLA heuristic is that these are expensive on GPU and so they are not duplicated into the consuming fusion nodes.\r\n\r\nThere also seems to be a problem where a broadcast op is not fused into a fusion node that does a reduce, so the broadcast ends up being written to memory and then read back, which doesn't seem good, especially since the source data for the broadcast is small. I'll pass that problem on.\r\n\r\nWe may in future consider adding a dedicated batchnorm op to XLA, since batchnorm is too large of a subgraph to reasonably pattern match, though not clear at this point if that will end up being useful.", "Thank you for the information @broune. I'll assign so you can determine how this issue should be handled from here.", "Also, even for the [fused batch norm implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L248) they actually transform the data_format from NCHW to NHWC before applying the CuDNN batch normalization.\r\n\r\nWhile bias_add can natively support NCHW and NHWC (there are both \"BiasNHWCKernel\" and \"BiasNCHWKernel\"), I have no idea if there is a speed difference between both.\r\n\r\nI guess a kind of \"broadcast_mul\" in the same manner than \"bias_add\" would be useful, for many case (batch renorm being one of them).", "@Caenorst\r\n\r\nThat's going the other way around, no? See also https://github.com/tensorflow/tensorflow/issues/8286. For those of us using cuDNN, I don't think there's any reason why we wouldn't want to always use NCHW anyway, modulo the performance bugs we're discussing here.", "Will this ever be supported? - Using ultra large images constrain batch sizes to 1, so renormalization is a must for myself. I use `NCHW` format, hence the interest.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I assume that you're using this on GPU. For BatchNorm, we do have an op in XLA, and lately Justin Lebar tried it with the GPU backend of XLA using cuDNN, but concluded that raw XLA is actually faster than cuDNN for this operation, IIRC. So that may also be true for batch RE-normalization at this point. Assigning over to Justin to comment on the GPU angle on this.", "> For BatchNorm, we do have an op in XLA, and lately Justin Lebar tried it with the GPU backend of XLA using cuDNN, but concluded that raw XLA is actually faster than cuDNN for this operation, IIRC\r\n\r\nJust to be clear, what I saw was that on our models, using cudnn batchnorm was roughly the same speed as our XLA implementation.  However, our lowering of these particular XLA ops to PTX is not very good at the moment -- I think it could be made significantly faster, at which point we would beat cudnn.\r\n\r\nI don't see why batch re-normalization would be different, although someone would have to benchmark.\r\n\r\nNote that the comparison I was making was between everything-compiled-with-xla+cudnn-batchnorm vs everything-compiled-with-xla+pure-xla-batchnorm.  If you tried instead compiling just the one TF batchnorm (or batch-renorm) op with XLA, then XLA wouldn't be able to fuse the op's inputs or outputs, and you might indeed find that cudnn is faster than XLA.\r\n\r\nI'm going to unassign myself since it doesn't sound like there's work that we need to do here that's separate from the regular XLA performance work we're already doing.  (This is a good thing -- part of the promise of XLA is that we don't have to open a bug every time we have a new op that we want to optimize.)\r\n  ", "With @jlebar's comment, I am closing this issue. If there is any further discussion on this feel free to reopen the issue.."]}, {"number": 9140, "title": "Update issue template (thanks jart@) and add env collection script.", "body": "", "comments": ["It generates an error if a variable is undefined. That's good for spotting\ntypos. You can set -u in the script.\n\nThe style guide mandates -eu, but -e which causes the script to end on any\nnon success command is too harsh in that case\n\nOn Apr 11, 2017 11:11 PM, \"Andrew Selle\" <notifications@github.com> wrote:\n\n> *@aselle* commented on this pull request.\n> ------------------------------\n>\n> In tools/tf_env_collect.sh\n> <https://github.com/tensorflow/tensorflow/pull/9140#discussion_r111070932>\n> :\n>\n> > @@ -0,0 +1,83 @@\n> +#!/usr/bin/env bash\n>\n> That does not seem to work. What is it supposed to do?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9140#discussion_r111070932>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbWNxRoaF9ErpVSnhyTeYIMQ3OB_Pks5rvGsmgaJpZM4M6fDc>\n> .\n>\n", "It seems I can do\r\n```\r\n#!/bin/bash -u\r\n```\r\nbut I don't think I can use env (which gives robustness to bash not installed in /bin)\r\n```\r\n#!/usr/bin/env bash -u\r\n```\r\ndoesn't work because the -u doesn't get sent to bash.\r\n\r\n", "OK. From the bash script you can just do set -u first thing.\n\nOn Wed, Apr 12, 2017 at 11:16 AM, Andrew Selle <notifications@github.com>\nwrote:\n\n> It seems I can do\n>\n> #!/bin/bash -u\n>\n> but I don't think I can use env (which gives robustness to bash not\n> installed in /bin)\n>\n> #!/usr/bin/env bash -u\n>\n> doesn't work because the -u doesn't get sent to bash.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9140#issuecomment-293663003>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_Sbb8gY_x9aoc6O8a1ZDWnlTCWvtqvks5rvRTtgaJpZM4M6fDc>\n> .\n>\n", "@drpngx, your changes are now addressed I believe.\r\n", "Jenkins, test this please."]}, {"number": 9139, "title": "Incorrect results when graph is split across several GPUs.", "body": "Background info:\r\n- Custom code\r\n- Tensorflow r1.0 installed from binaries on Windows\r\n- CUDA 8.0, cuDNN 5.1.5\r\n- 2 GPUs: GTX Titan X and Titan X Pascal\r\n\r\nProblem:\r\n\r\nI have a model that is small enough to be trained on a single GPU with 12GB memory. Training works fine and converges.\r\n\r\nHowever, when I evaluate the model with a validation set that is too large to fit on one of my GPUs, TensorFlow seems to use both of my GPUs (one GTX Titan X and one Titan X Pascal). When this happens, **a large fraction of the results returned by TensorFlow are incorrect**. The returned values are not completely missing, i.e. not all zero or something like that, but so inaccurate that the validation performance is terrible. \r\n\r\nMore specifically, my model consists of a shared initial stage, followed by a list of ~50 sub-networks that all receive input from the shared stage, but are otherwise independent. Data is split using `tf.dynamic_partition()`. From the results that I get, it appears that TensorFlow moves some of the 50 sub-networks to the second GPU if the memory on the first one isn't sufficient. The moved sub-models then return incorrect results (the others are unchanged).\r\n\r\nIf I instead force evaluation to be performed on the CPU, all results are as expected. All I need to do is add `with tf.device('/cpu:0')` to the very top of my script. The results also look good if I reduce the size of the validation set so that it fits onto one GPU.\r\n\r\nI am sorry for not providing a working example. I will try to create one, but it might take a while since, by nature of the problem, it needs to be a fairly large/complex model.\r\n\r\n\r\n", "comments": ["Speaking as someone who's just triaging issues, this seems like it might be more appropriate for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). If you can narrow down the scope of this problem, help us understand why it's inconsistent with what the documentation promises, then please feel free to file another issue."]}, {"number": 9138, "title": "Issues found by PVS-Studio", "body": "Hello,\r\nDevelopers of PVS-Studio C/C++/C# static analyzer present their check report of the source code of 'TensorFlow' in the article, containing the review of the most suspicious code fragments they discovered.\r\n\r\nYou can read article at the official site:\r\nhttps://www.viva64.com/en/b/0497/\r\n\r\nI have provided links to GitHub for each code fragment to make viewing more comfortable. The article doesn't cover all the issues that were found by the analyzer, so perhaps it would be interesting for you to review them yourself. In case you have questions, feel free to ask them.\r\n\r\nBest regards,\r\nSergey Vasiliev", "comments": ["The article contains links to source files but not to the defined code lines. Here are the correct links:\r\n\r\n- PVS-Studio warning V595. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/c7b80d51da4fb6d51ea54a0bdf2601afa379d60c/tensorflow/core/common_runtime/function.cc#L1044)\r\n- PVS-Studio warning V547. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/c7b80d51da4fb6d51ea54a0bdf2601afa379d60c/tensorflow/core/distributed_runtime/master_session.cc#L1114)\r\n- PVS-Studio warning V581. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/contrib/rnn/kernels/lstm_ops.h#L284)\r\n- PVS-Studio warning V760. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/core/kernels/encode_jpeg_op.cc#L56)\r\n- PVS-Studio warning V629. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/contrib/tensor_forest/hybrid/core/ops/unpack_path_op.cc#L55)\r\n- PVS-Studio warning V592. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/core/lib/strings/strcat.cc#L43)\r\n- PVS-Studio warning V519. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/bbe056e5a0ab81b67fcb6053400812b3d5805fc7/tensorflow/core/kernels/resize_area_op.cc#L174)\r\n- PVS-Studio warning V614. [A link to the source code.](https://github.com/tensorflow/tensorflow/blob/eae70b8ee31d87bdfd339aecf05100f0a1633961/tensorflow/contrib/tensor_forest/kernels/sample_inputs_op.cc#L351)", "As far as I can tell, these are either style issues (e.g. not using curly braces around single statement if clause), and some mixing of signed and unsigned in cases were it is safe (from the context), such as\r\n\r\nunsigned = 1 << unsigned -1 \r\n\r\nwhere unsigned is known to be >= 0. Do you actually see anything alarming? Maybe I missed it.", "Closing issue, feel free to reopen if you spot some actual bugs.", "Looking more carefully, some of these findings do merit attention/cleanup. Thanks.", "> Do you actually see anything alarming?\r\n\r\nit may get undefined behavior if the value returned by `c->Value(tree_depth)` is more than 31.\r\n```cpp\r\n#include <iostream>\r\nusing namespace std;\r\n\r\nint main() {\r\n  for (int64_t v = 29; v < 35; ++v) {\r\n    int64_t a = (1L << v) - 1;\r\n    int64_t b = (1  << v) - 1;\r\n    cout << \"value=\" << v << \", \";\r\n    cout << \"a=\" << a << \", \";\r\n    cout << \"b=\" << b << \" - \" << (a==b ? \"OK \" : \"FAIL!\") << endl;\r\n  }\r\n  return 0;\r\n}\r\n```\r\nThat produces the following output:\r\n```\r\nvalue=29, a=536870911, b=536870911 - OK \r\nvalue=30, a=1073741823, b=1073741823 - OK \r\nvalue=31, a=2147483647, b=2147483647 - OK \r\nvalue=32, a=4294967295, b=0 - FAIL!\r\nvalue=33, a=8589934591, b=1 - FAIL!\r\nvalue=34, a=17179869183, b=3 - FAIL!\r\n```\r\nExplanation from the article:\r\n\r\n> PVS-Studio warning: V629 Consider inspecting the '1 << c->Value(tree_depth)' expression. Bit shifting of the 32-bit value with a subsequent expansion to the 64-bit type. unpack_path_op.cc 55\r\n> \r\n> The strangeness of this code is in the fact that the 32 and 64 bit values are mixed in the shift and assignment operations. The literal 1 is a 32-bit value, for which a left-side shift is performed. The result of the shift still has a 32-bit type, but is written to the 64-bit variable. It is suspicious, because we may get undefined behavior if the value returned by the Value method is more than 32.\r\n> \r\n> Here is a quote from the standard: The value of E1 << E2 is E1 left-shifted E2 bit positions; vacated bits are zero-filled. If E1 has an unsigned type, the value of the result is E1 * 2^E2, reduced modulo one more than the maximum value representable in the result type. Otherwise, if E1 has a signed type and non-negative value, and E1*2^E2 is representable in the result type, then that is the resulting value; otherwise, the behavior is undefined.\r\n> \r\n> This code can be fixed by writing 1 as a 64-bit literal or doing the type extension via casting. More details on the shift operations can be found in the article \"Wade not in unknown waters. Part three\".", "Thanks @maks-a.\r\n\r\n@rmlarsen , I would also like to note that I haven\u2019t described in the article all the fragments that the analyzer considered to be suspicious. There were other warnings that I haven\u2019t noted down. For example, V730 warning, that I mentioned in the article. I didn\u2019t include such warnings in the article, because I am not very familiar with the code and it\u2019s hard to say if the code contains an error, or if was written intentionally.\r\n\r\nIn any case it might be interesting for you to check the code yourself and have a look at the log. You can contact us by e-mail (support@viva64.com) to get a temporary license key.", "Will take a look this weekend", "Submitted CL internally. It should close automatically at the next push. Thanks for the write-up and reporting back to us!"]}, {"number": 9137, "title": "\"undefined symbol\" when compiling the example Op", "body": "- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: No\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: 1.0.1\r\n- *Bazel version (if compiling from source)*: NA\r\n- *CUDA/cuDNN version*: NA\r\n- *GPU Model and Memory*: NA\r\n- *Exact command to reproduce*: \r\n\r\n### Describe the problem clearly\r\nTrying to build and compile the example in \"adding a new op\"\r\nhttps://www.tensorflow.org/extend/adding_an_op\r\nThis was moved to\r\ntensorflow/examples/adding_an_op\r\nand was not updated in the tutorial page.\r\nAfter compiling the zero_out_op_kernel_1.cc example, I'm trying to run the zero_out_1_test.py. Yet this returns the following error\r\n\r\n  File \"zero_out_1_test.py\", line 25, in <module>\r\n    import zero_out_op_1\r\n  File \"/home/me/tf_compile/tensorflow/tensorflow/examples/adding_an_op/zero_out_op_1.py\", line 26, in <module>\r\n    'zero_out_op_kernel_1.so'))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/me/tf_compile/tensorflow/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev\r\n\r\nAny ideas what could that symbol be?", "comments": ["I tried the example with 1.0.1 and it worked for me. You might be using an unsupported operating system which has a weird version of GCC. We support Ubuntu Linux 14 LTE to 16. Mac OS X El Capitan. CentOS 7 and above.", "I'm using Ubuntu 16 with GCC 5.\r\nCould it be that one of the headers was not compiled correctly?", "Interesting. So you installed the TensorFlow binary distro, which was probably built using GCC 4.x. Then compiled a custom op using GCC 5.x. Out of curiosity, could you try apt-get installing the GCC 4.x series and see if it works? If so, then I might want to bring to the attention of some other people that a binary compatibility problem exists.", "Sure, thanks for the quick response. I'll give it a try either tonight or tomorrow at some point and will let you know", "I've the exact same problem : ' undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv' .\r\n\r\nDoes '_ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringEv' have a different error meaning with respect to '_ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev' ?\r\n\r\n We've defined our own custom op and our code works successfully with tensorflow 0.12.1 and it crashes with the above error with tensorflow 1.0.0. I'm using Ubuntu 16 with gcc version 5.4.\r\n@idofr  Did your problem get solved with gcc 4 ?\r\n\r\n**Edit:**\r\nFollowing link solved my problem :\r\nhttps://github.com/deepsense-io/roi-pooling/issues/1", "@shashankneocfc Thanks for reporting back with the solution.\r\n\r\nIt turns out we have the magic incantation `-D_GLIBCXX_USE_CXX11_ABI=0` in our [pip installation](https://www.tensorflow.org/install/install_sources#install_the_pip_package) and [adding an op](https://www.tensorflow.org/extend/adding_an_op) documentation. Those error messages are really worrisome, but I'm not sure what we can do to make this easier for users.", "I am facing this problem while adding a custom ops. \r\n\r\nlib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/afzal786/PycharmProjects/multi/venv/lib/python3.6/site-packages/multidim_image_augmentation/python/ops/_augmentation_ops.so: undefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev\r\n\r\nundefined symbol: _ZN10tensorflow8internal21CheckOpMessageBuilder9NewStringB5cxx11Ev\r\ngcc and ubuntu version:\r\ngcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n\r\nTensorFlow version 2.2.0\r\n"]}, {"number": 9136, "title": "Issues when using Queues + tf.train.Server", "body": "NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: Yes\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: 1.0.0 CPU / 1.0.1 (CPU and GPU enabled) / 1.1.0rc1 (CPU)\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*: N/A\r\n- *GPU Model and Memory*: N/A\r\n- *Exact command to reproduce*: cf below.\r\n\r\nThis problem has been reproduced on both Linux and various Mac OS machines.\r\n\r\n### Describe the problem clearly\r\n\r\nWe seem to experience issues when using both queues + `tf.train.Server`. When executed in a simple python 3.5.3 console, the following script hangs:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n\r\ncluster = tf.train.ClusterSpec({\"cpu1\" : ['localhost:2222']})\r\nserver = tf.train.Server(cluster, job_name=\"cpu1\", task_index=0)\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    # Queue\r\n    input_queue = tf.train.input_producer(tf.constant([0.], dtype=tf.float32))\r\n\r\n    # Useless variable\r\n    variable = tf.Variable(1., dtype=tf.float32, trainable=False, name=\"variable\")\r\n\r\n    # Session and queue runners\r\n    session = tf.Session(target=server.target)\r\n    session.run(tf.global_variables_initializer())\r\n    tf.train.start_queue_runners(session)\r\n\r\nprint(session.run(variable))  # this works\r\nprint(session.run(tf.assign(variable, 2)))  # this also works, but only if called directly\r\n\r\n# any pause between creating and running the session breaks it\r\ntime.sleep(1)\r\n\r\nprint(session.run(variable))  # retrieving a variable still works, but...\r\nprint(session.run(tf.assign(variable, 3)))  # ... assigning a variable will make the program hang.\r\n```\r\n\r\nIt outputs:\r\n\r\n```\r\n1\r\n2\r\n2\r\n```\r\n\r\nand then hangs forever. The problem vanishes when either commenting the `input_queue=...` line, or when writing `session = tf.Session()` instead of passing the `server.target`.\r\n\r\nThe problems seems to happen not only with variable assignments, but also saving the model using `tf.train.Saver().save(session, 'my_model')` for instance (and possibly other operations). Note that reading a variable works fine.\r\n\r\nIn the example script, the `time.sleep`command simulates a pause between creating the session and running it to set a variable. The same effect is achieved, for example, when splitting session creation and running code across two Jupyter notebook cells. When executing the whole code in one cell, it works fine.\r\n\r\n\r\n### Source Code / Logs\r\nThe source code to reproduce the problem is displayed above. I have attached a traceback using gdb, which shows that the program is hanging while trying to acquire a lock.\r\n\r\n[tf-issue-gdb-bt.txt](https://github.com/tensorflow/tensorflow/files/913097/tf-issue-gdb-bt.txt)\r\n[tf-issue-gdb-stack-threads.txt](https://github.com/tensorflow/tensorflow/files/913102/tf-issue-gdb-stack-threads.txt)\r\n\r\n", "comments": ["Thanks for the detailed report and stacktraces, this helps a lot and is much appreciated.\r\n\r\n@mrry pointed out that we might have a bug when graphs are extended in a distributed session while some operations (in this case the enqueue operation) are in progress (See [`master_session.cc:1038`](https://github.com/tensorflow/tensorflow/blob/87cdfafd44ff5e332fd820608783432fea83a4c9/tensorflow/core/distributed_runtime/master_session.cc#L1038) - that code predates the queue runners).\r\n\r\n@suharshs : Would you have the bandwidth to look into that TODO?\r\n\r\n@jdonier : In the mean time, a workaround for you would be to ensure that the graph isn't modified after the queue runners are started. For example, your snippet above could be rewritten as:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\ncluster = tf.train.ClusterSpec({\"cpu1\" : ['localhost:2222']})\r\nserver = tf.train.Server(cluster, job_name=\"cpu1\", task_index=0)\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    # Queue\r\n    input_queue = tf.train.input_producer(tf.constant([0.], dtype=tf.float32))\r\n\r\n    # Useless variable\r\n    variable = tf.Variable(1., dtype=tf.float32, trainable=False, name=\"variable\")\r\n\r\n    # Session and queue runners\r\n    session = tf.Session(target=server.target)\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    # CHANGE FROM PREVIOUS SNIPPET: Assign operations\r\n    assign2 = tf.assign(variable, 2)\r\n    assign3 = tf.assign(variable, 3)\r\n\r\n    tf.train.start_queue_runners(session)\r\n\r\nprint(session.run(variable))\r\nprint(session.run(assign2))\r\n\r\n# Freely sleep\r\ntime.sleep(1)\r\n\r\nprint(session.run(variable))\r\nprint(session.run(assign3))\r\n```\r\n\r\nFYI @jhseu @saeta who might like to know about this too.", "@asimshankar the reason why I want to modify the graph after the queue runners are started is to change some parameters during / after training (e.g. the training rate during training, or dropout rates between training and inference) so this needs to be done after the queue runners have been started. I guess I could define them as placeholders but it's a bit weird to have to feed these values for every computation...\r\n\r\nAbout the problem with model saving: I was creating a `tf.train.Saver()` at saving time, which was causing the problem, consistent with your explanation. It all works fine if I define it when I create the graph -- so thanks a lot!", "I have a change coming soon that should fix this. (Thanks @mrry and @asimshankar for flagging!)"]}, {"number": 9135, "title": "YoloDetector example needs java_test() rule", "body": "Hi:\r\n   when I use android studio to compile TensorflowYoloDetector, the following error occurs:\r\n\r\nE/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate()\r\n\r\n (tried Java_org_tensorflow_contrib_android_RunStats_allocate and  Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n\r\nTensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\nTensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n\r\nTensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/graph-tiny-yolo-voc.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef\r\n\r\ntensorflow: TensorFlowYoloDetector: TF init status: 1.\r\n\r\n\u3000\u3000 Look forward to your reply. \r\n\r\n", "comments": ["`TensorFlowInferenceInterface: Failed to load model from 'file:///android_asset/graph-tiny-yolo-voc.pb': java.io.IOException: Not a valid TensorFlow Graph serialization: Invalid GraphDef`\r\n\r\nIt doesn't sound like you've put the required model into the assets/ directory, or it's otherwise corrupt. The yolo model isn't provided with TF, so you'll need to create it yourself and add it to the demo. There are instructions on using darkflow to create one in TensorFlowDetectorActivity.java.", "This broke when the RunStats interface was added in 6005d639932dd586733bfb3f50a9925f195c3910. The author must have forgotten to implement `allocate` in the class definition. Root cause IMHO is there not being a `java_test` rule for this example. We have `@junit` defined in workspace.bzl. Shouldn't be too much trouble to add a BUILD file to that directory with a test rule.", "@jart Actually, it just tries to call allocate() to see if the native methods are loaded. The following log messages indicate that they were, so the previous error can be ignored (I'm unaware of any means to suppress it or otherwise test for native method existence).\r\n\r\nRegarding testing: even though @junit may be available for Java tests I don't believe Bazel supports android_test rules yet (we actually have a few Android tests internally that are not exposed yet due to this).", "@zgq91 \r\nTo clarify, this error has nothing at all to do with RunStats, that's just a red herring.\r\n\r\nI'd need more info to debug the actual YOLO problem (like what's the output of `unzip -v tensorflow_demo.apk`), but if you're doing anything outside the basic tiny-yolo-voc sample conversion command shown in DetectorActivity then it's outside the scope of TF support.", "@andrewharp It seems I misread the yolo class, for some reason thinking it implemented RunStats. I'll let you take matters from here. I'll also send you an email with some thoughts on how we can improve the Android testing situation.", "@jart hi have you implementation in yolov2.pb or yolov2-tiny.pb not the tiny-yolo-voc.pb ,should i change the label in source code,it seems like ,but there is something wrong in my project?can anyone help me,thanks a lot  "]}, {"number": 9134, "title": "Broken link in Python API documentation on RNN and Cells (contrib)", "body": "\r\nThere's a broken link in the Python API documentation on RNN and Cells (contrib).\r\n\r\nThe link appears at the very bottom of this page: www.tensorflow.org/api_guides/python/contrib.rnn\r\nTitle of link: tf.contrib.rnn.stack_bidirectional_dynamic_rnn\r\nURL of link: https://www.tensorflow.org/api_guides/python/BROKEN_LINK\r\n", "comments": ["Thanks for pointing this out. \r\n\r\nThere seems to an issue with the website push that accidentally pushed the guide for version 1.1 instead of for the current stable release 1.0. `tf.contrib.rnn.stack_bidirectional_dynamic_rnn` is not available in 1.0.\r\n(FYI @wolffg )\r\n\r\nHowever, those symbols do (and will) exist in release 1.1. So, in the mean time you're welcome to try out the 1.1 release candidate version (or wait for it's final release) and the documentation can be found inhttps://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/stack_bidirectional_rnn\r\n\r\nOnce the 1.1 release is made, this page will be fixed.\r\n\r\nClosing it out since I believe the problem has been fixed in code and will update itself on the next website/release push."]}, {"number": 9133, "title": "Update quantization.md", "body": "A simple typo in the quantization documentation.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Thanks!\r\n"]}, {"number": 9132, "title": "Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) only in the test mode", "body": "When I fine tuned the resnet50 with keras 2.0.3 and tensorflow 1.1.0. \r\nCUDNN: 5.1\r\nCUDA: 8.0\r\nEvery time when I directly try to test it, I just got the error:\r\n017-04-11 17:35:03.446611: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-04-11 17:35:03.446639: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-04-11 17:35:03.446647: F tensorflow/core/kernels/conv_ops.cc:665] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\nBut, if I test it after the training, there is no problem. ", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  **Make sure you also include the exact command if possible to produce  the output included in your test case**.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nIf this involves other libraries, it's probably better to post on StackOverflow.", "It seems that the problem comes from the conflicts with cv2 package. I used the deep learning models from https://github.com/fchollet/deep-learning-models. \r\n~~~~\r\ndef preprocess_input(x, dim_ordering='default'):\r\n     if dim_ordering == 'default':\r\n         dim_ordering = K.image_dim_ordering()\r\n     assert dim_ordering in {'tf', 'th'}\r\n \r\n     if dim_ordering == 'th':\r\n         x[0, :, :] -= 103.939\r\n         x[1, :, :] -= 116.779\r\n         x[2, :, :] -= 123.68\r\n         # 'RGB'->'BGR'\r\n         x = x[::-1, :, :]\r\n     else:\r\n         x[:, :, 0] -= 103.939\r\n         x[:, :, 1] -= 116.779\r\n         x[:, :, 2] -= 123.68\r\n         # 'RGB'->'BGR'\r\n         x = x[:, :, ::-1]\r\n     return x\r\n\r\ndef preprocess_image(im):\r\n     im = cv2.resize(im, (224, 224)).astype(np.float32)\r\n     im = preprocess_input(im)\r\n \r\n     dim_ordering = K.image_dim_ordering()\r\n     print dim_ordering\r\n     assert dim_ordering in {'tf', 'th'}\r\n \r\n     if dim_ordering == 'th':\r\n         im = im.transpose((2,0,1))\r\n     return im\r\n\r\ndef eval(self):\r\n         img_path = 'elephant.jpg'\r\n         img = image.load_img(img_path, target_size=(224, 224))\r\n         x = image.img_to_array(img)\r\n         #x = preprocess_input(x)\r\n         #x = np.expand_dims(x, axis=0)\r\n         x = preprocess_image(x)\r\n         x = np.expand_dims(x, axis=0)\r\n         print('Input image shape:', x.shape)\r\n         intention = np.expand_dims(config.transform_intention(config.FORWARD),      axis=0)\r\n \r\n         print x.shape, intention.shape\r\n         pred = self.model.predict([x, intention])\r\n         print 'pred', pred\r\n\r\n~~~~\r\nWhen I call preprocess_image() in the eval() function, I got the error:\r\n2017-04-12 11:29:55.861593: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-04-12 11:29:55.861630: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-04-12 11:29:55.861639: F tensorflow/core/kernels/conv_ops.cc:665] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\nBut If I call preprocess_input(), there is no error. It's very weired.\r\nThanks a lot.", "As mentioned in [the previous comment](https://github.com/tensorflow/tensorflow/issues/9132#issuecomment-293322827), please do fill in all the details requested and in particular provide a means of reproducing the problem. Without that it is hard to provide useful assistance. The snippet you provided above seems incomplete, relying on external libraries and data files which were not provided.\r\n", "Tensorflow: 1.1.0, no matter install from binary or source\r\nKeras: 2.0.3\r\n~~~~\r\n# -*- coding: utf-8 -*-\r\n'''ResNet50 model for Keras.\r\n\r\n# Reference:\r\n\r\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\r\n\r\nAdapted from code contributed by BigMoyan.\r\n'''\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport warnings\r\n\r\nfrom keras.layers import Input\r\nfrom keras import layers\r\nfrom keras.layers import Dense\r\nfrom keras.layers import Activation\r\nfrom keras.layers import Flatten\r\nfrom keras.layers import Conv2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import GlobalMaxPooling2D\r\nfrom keras.layers import ZeroPadding2D\r\nfrom keras.layers import AveragePooling2D\r\nfrom keras.layers import GlobalAveragePooling2D\r\nfrom keras.layers import BatchNormalization\r\nfrom keras.models import Model\r\nfrom keras.preprocessing import image\r\nimport keras.backend as K\r\nfrom keras.utils import layer_utils\r\nfrom keras.utils.data_utils import get_file\r\nfrom keras.applications.imagenet_utils import _obtain_input_shape\r\nfrom keras.applications.imagenet_utils import decode_predictions\r\nfrom keras.applications.imagenet_utils import preprocess_input\r\nfrom keras.engine.topology import get_source_inputs\r\n\r\n\r\nWEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\r\nWEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\n\r\n\r\ndef identity_block(input_tensor, kernel_size, filters, stage, block, name=None):\r\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\r\n\r\n    # Arguments\r\n        input_tensor: input tensor\r\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\r\n        filters: list of integers, the filterss of 3 conv layer at main path\r\n        stage: integer, current stage label, used for generating layer names\r\n        block: 'a','b'..., current block label, used for generating layer names\r\n\r\n    # Returns\r\n        Output tensor for the block.\r\n    \"\"\"\r\n    filters1, filters2, filters3 = filters\r\n    if K.image_data_format() == 'channels_last':\r\n        bn_axis = 3\r\n    else:\r\n        bn_axis = 1\r\n    conv_name_base = 'res' + str(stage) + block + '_branch'\r\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n\r\n    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(filters2, kernel_size,\r\n               padding='same', name=conv_name_base + '2b')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\r\n\r\n    x = layers.add([x, input_tensor])\r\n    if name:\r\n        x = Activation('relu', name=name)(x)\r\n    else:\r\n        x = Activation('relu')(x)\r\n    return x\r\n\r\n\r\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\r\n    \"\"\"conv_block is the block that has a conv layer at shortcut\r\n\r\n    # Arguments\r\n        input_tensor: input tensor\r\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\r\n        filters: list of integers, the filterss of 3 conv layer at main path\r\n        stage: integer, current stage label, used for generating layer names\r\n        block: 'a','b'..., current block label, used for generating layer names\r\n\r\n    # Returns\r\n        Output tensor for the block.\r\n\r\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\r\n    And the shortcut should have strides=(2,2) as well\r\n    \"\"\"\r\n    filters1, filters2, filters3 = filters\r\n    if K.image_data_format() == 'channels_last':\r\n        bn_axis = 3\r\n    else:\r\n        bn_axis = 1\r\n    conv_name_base = 'res' + str(stage) + block + '_branch'\r\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\r\n\r\n    x = Conv2D(filters1, (1, 1), strides=strides,\r\n               name=conv_name_base + '2a')(input_tensor)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(filters2, kernel_size, padding='same',\r\n               name=conv_name_base + '2b')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\r\n    x = Activation('relu')(x)\r\n\r\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\r\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\r\n\r\n    shortcut = Conv2D(filters3, (1, 1), strides=strides,\r\n                      name=conv_name_base + '1')(input_tensor)\r\n    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\r\n\r\n    x = layers.add([x, shortcut])\r\n    x = Activation('relu')(x)\r\n    return x\r\n\r\n\r\ndef ResNet50(include_top=True, weights='imagenet',\r\n             input_tensor=None, input_shape=None,\r\n             pooling=None,\r\n             classes=1000):\r\n    \"\"\"Instantiates the ResNet50 architecture.\r\n\r\n    Optionally loads weights pre-trained\r\n    on ImageNet. Note that when using TensorFlow,\r\n    for best performance you should set\r\n    `image_data_format=\"channels_last\"` in your Keras config\r\n    at ~/.keras/keras.json.\r\n\r\n    The model and the weights are compatible with both\r\n    TensorFlow and Theano. The data format\r\n    convention used by the model is the one\r\n    specified in your Keras config file.\r\n\r\n    # Arguments\r\n        include_top: whether to include the fully-connected\r\n            layer at the top of the network.\r\n        weights: one of `None` (random initialization)\r\n            or \"imagenet\" (pre-training on ImageNet).\r\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\r\n            to use as image input for the model.\r\n        input_shape: optional shape tuple, only to be specified\r\n            if `include_top` is False (otherwise the input shape\r\n            has to be `(224, 224, 3)` (with `channels_last` data format)\r\n            or `(3, 224, 244)` (with `channels_first` data format).\r\n            It should have exactly 3 inputs channels,\r\n            and width and height should be no smaller than 197.\r\n            E.g. `(200, 200, 3)` would be one valid value.\r\n        pooling: Optional pooling mode for feature extraction\r\n            when `include_top` is `False`.\r\n            - `None` means that the output of the model will be\r\n                the 4D tensor output of the\r\n                last convolutional layer.\r\n            - `avg` means that global average pooling\r\n                will be applied to the output of the\r\n                last convolutional layer, and thus\r\n                the output of the model will be a 2D tensor.\r\n            - `max` means that global max pooling will\r\n                be applied.\r\n        classes: optional number of classes to classify images\r\n            into, only to be specified if `include_top` is True, and\r\n            if no `weights` argument is specified.\r\n\r\n    # Returns\r\n        A Keras model instance.\r\n\r\n    # Raises\r\n        ValueError: in case of invalid argument for `weights`,\r\n            or invalid input shape.\r\n    \"\"\"\r\n    if weights not in {'imagenet', None}:\r\n        raise ValueError('The `weights` argument should be either '\r\n                         '`None` (random initialization) or `imagenet` '\r\n                         '(pre-training on ImageNet).')\r\n\r\n    if weights == 'imagenet' and include_top and classes != 1000:\r\n        raise ValueError('If using `weights` as imagenet with `include_top`'\r\n                         ' as true, `classes` should be 1000')\r\n\r\n    # Determine proper input shape\r\n    input_shape = _obtain_input_shape(input_shape,\r\n                                      default_size=224,\r\n                                      min_size=197,\r\n                                      data_format=K.image_data_format(),\r\n                                      include_top=include_top)\r\n\r\n    if input_tensor is None:\r\n        img_input = Input(shape=input_shape)\r\n    else:\r\n        if not K.is_keras_tensor(input_tensor):\r\n            img_input = Input(tensor=input_tensor, shape=input_shape)\r\n        else:\r\n            img_input = input_tensor\r\n    if K.image_data_format() == 'channels_last':\r\n        bn_axis = 3\r\n    else:\r\n        bn_axis = 1\r\n\r\n    x = ZeroPadding2D((3, 3))(img_input)\r\n    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\r\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\r\n    x = Activation('relu')(x)\r\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\r\n\r\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\r\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\r\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\r\n\r\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\r\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d', name='feat1')\r\n\r\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\r\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f', name='feat2')\r\n\r\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\r\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\r\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c', name='feat_layer')\r\n\r\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\r\n\r\n    if include_top:\r\n        x = Flatten()(x)\r\n        x = Dense(classes, activation='softmax', name='fc1000')(x)\r\n    else:\r\n        if pooling == 'avg':\r\n            x = GlobalAveragePooling2D()(x)\r\n        elif pooling == 'max':\r\n            x = GlobalMaxPooling2D()(x)\r\n\r\n    # Ensure that the model takes into account\r\n    # any potential predecessors of `input_tensor`.\r\n    if input_tensor is not None:\r\n        inputs = get_source_inputs(input_tensor)\r\n    else:\r\n        inputs = img_input\r\n    # Create model.\r\n    model = Model(inputs, x, name='resnet50')\r\n\r\n    # load weights\r\n    if weights == 'imagenet':\r\n        if include_top:\r\n            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\r\n                                    WEIGHTS_PATH,\r\n                                    cache_subdir='models',\r\n                                    md5_hash='a7b3fe01876f51b976af0dea6bc144eb')\r\n        else:\r\n            weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\r\n                                    WEIGHTS_PATH_NO_TOP,\r\n                                    cache_subdir='models',\r\n                                    md5_hash='a268eb855778b3df3c7506639542a6af')\r\n        model.load_weights(weights_path)\r\n        if K.backend() == 'theano':\r\n            layer_utils.convert_all_kernels_in_model(model)\r\n\r\n        if K.image_data_format() == 'channels_first':\r\n            if include_top:\r\n                maxpool = model.get_layer(name='avg_pool')\r\n                shape = maxpool.output_shape[1:]\r\n                dense = model.get_layer(name='fc1000')\r\n                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\r\n\r\n            if K.backend() == 'tensorflow':\r\n                warnings.warn('You are using the TensorFlow backend, yet you '\r\n                              'are using the Theano '\r\n                              'image data format convention '\r\n                              '(`image_data_format=\"channels_first\"`). '\r\n                              'For best performance, set '\r\n                              '`image_data_format=\"channels_last\"` in '\r\n                              'your Keras config '\r\n                              'at ~/.keras/keras.json.')\r\n    return model\r\n\r\ndef preprocess_input(x, dim_ordering='default'):\r\n     if dim_ordering == 'default':\r\n         dim_ordering = K.image_dim_ordering()\r\n     assert dim_ordering in {'tf', 'th'}\r\n\r\n     if dim_ordering == 'th':\r\n         x[0, :, :] -= 103.939\r\n         x[1, :, :] -= 116.779\r\n         x[2, :, :] -= 123.68\r\n         # 'RGB'->'BGR'\r\n         x = x[::-1, :, :]\r\n     else:\r\n         x[:, :, 0] -= 103.939\r\n         x[:, :, 1] -= 116.779\r\n         x[:, :, 2] -= 123.68\r\n         # 'RGB'->'BGR'\r\n         x = x[:, :, ::-1]\r\n     return x\r\n\r\ndef preprocess_image(im):\r\n     import cv2\r\n     im = cv2.resize(im, (224, 224)).astype(np.float32)\r\n     im = preprocess_input(im)\r\n\r\n     dim_ordering = K.image_dim_ordering()\r\n     assert dim_ordering in {'tf', 'th'}\r\n\r\n     if dim_ordering == 'th':\r\n         im = im.transpose((2,0,1))\r\n     return im\r\n\r\ndef test():\r\n    model = ResNet50(include_top=True, weights='imagenet')\r\n    img_path = 'elephant.jpg'\r\n    img = image.load_img(img_path, target_size=(224, 224))\r\n    x = image.img_to_array(img)\r\n    #x = preprocess_input(x)\r\n    #x = np.expand_dims(x, axis=0)\r\n    x = preprocess_image(x)\r\n    x = np.expand_dims(x, axis=0)\r\n\r\n    pred = model.predict(x)\r\n\r\nif __name__ == '__main__':\r\n    test()\r\n~~~~\r\nWhen I call preprocess_image() in the test() function, I got the error:\r\n2017-04-12 11:29:55.861593: E tensorflow/stream_executor/cuda/cuda_dnn.cc:359] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n2017-04-12 11:29:55.861630: E tensorflow/stream_executor/cuda/cuda_dnn.cc:326] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\n2017-04-12 11:29:55.861639: F tensorflow/core/kernels/conv_ops.cc:665] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\r\nAborted (core dumped)\r\nBut If I call preprocess_input(), there is no error. It's very weired.\r\nThanks a lot.", "This seems like a graphics card or CUDNN configuration error. Could you check if any of the suggestions in #8879 help?", "I'm running into this same issue but didn't see this open issue until now. I added my post to \"the original\" #8879.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I had the same problem \"Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\" and resolved it by downgrading CuDNN from version 9.1-7 to version 9.0-7\r\n\r\nOS: Windows 10\r\nTF version: 1.6\r\nCUDA version: 9.0\r\nCuDNN version: 9.0-7"]}, {"number": 9131, "title": "NotFoundError while restoring inception_v3 model", "body": "Hi,\r\n\r\nI am trying to make use of `inception_v3` model from `https://github.com/tensorflow/models/tree/master/slim`\r\n\r\nAfter downloading the model as described in the webpage, I wanted to restore the model using \r\n\r\n    saver = tf.train.Saver()\r\n    saver.restore(sess, 'model/inception_v3.ckpt')\r\n\r\nAnd I get this error: \r\n\r\n    NotFoundError (see above for traceback): Tensor name \"InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/biases\" not found in checkpoint files model/inception_v3.ckpt\r\n\r\nUpon inspection of `tf.trainable_variables()`, i find the variable with name -    \r\n\r\n    \"InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/biases:0\"\r\n\r\nIs the suffix `:0` causing this error ? If not how can i load this model ? \r\n\r\nI raised this issue @tensorflow/models, but unfortunately, I did not get any response. \r\nThanks in advance!", "comments": ["I think while you are trying to download the model you might have downloaded it partially or somewhere else so you got that error. ", "@VigneshSrinivasan10 We monitor both tensorflow/tensorflow and tensorflow/models when working on issues, so crossposting is not necessary. https://github.com/tensorflow/models/issues/1315.", "Also take into consideration that opening an issue here causes thousands of people to receive notifications. We want them to see an individual communicating an interesting problem, rather than being redirected to Stack Overflow. The TensorFlow team aims to focus on work that benefits the entire community, e.g. fixing bugs and writing features. Support only helps individuals."]}, {"number": 9130, "title": "r.0.11 avro missing files ", "body": "Hi\r\n\r\nI am trying to install tf r0.11 in a Centos 7 without GPU support from sources. I have a similar problem like the https://github.com/tensorflow/tensorflow/issues/4312. I followed the recommended actions, yet I get the same error. BESIDES, I get the following (not described in the issue 4312) error:\r\n\r\n`Error downloading from http://www-us.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz` \r\n\r\nI tryed to find this file using the web browser and I coudn't find it. The online repository doesn't have it anymore. Just the avro-1.8.1 and avro-1.7.7 are available.\r\n\r\nThanks\r\nRegards\r\nA\r\n\r\n", "comments": ["0.11 is a very old release (prior to stability guarantees of 1.0). Is it possible for you to use 1.0?\r\n\r\nThat said, **please provide all information asked for in the new issue template**. Without that it is hard to be able to help (in particular, do provide the exact sequence of commands to reproduce the problem).\r\n\r\nThanks!", "I run my applications in serveral clusters. One of them, has only tf.11 installed, and since I don't have admin rights, I would like to keep the other servers with the tf.11. \r\n\r\nThe secuence of commands was:\r\n\r\n```\r\n./configure\r\n~/tensorflow ~/tensorflow\r\nPlease specify the location of python. [Default is /home/AVV/rocm/anaconda2/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /home/AVV/rocm/anaconda2/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/AVV/rocm/anaconda2/lib/python2.7/site-packages]\r\n\r\n/home/AVV/rocm/anaconda2/lib/python2.7/site-packages\r\nDo you wish to build TensorFlow with GPU support? [y/N] N\r\nNo GPU support will be enabled for TensorFlow\r\nConfiguration finished\r\n...................\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n..........\r\nWARNING: /home/AVV/rocm/.cache/bazel/_bazel_rocm/5fee00a38fb04f0267978a3cccec9960/external/protobuf/protobuf.bzl:90:19: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nWARNING: /home/AVV/rocm/.cache/bazel/_bazel_rocm/5fee00a38fb04f0267978a3cccec9960/external/protobuf/protobuf.bzl:96:28: Variables HOST_CFG and DATA_CFG are deprecated in favor of strings \"host\" and \"data\" correspondingly.\r\nERROR: /home/AVV/rocm/tensorflow/tensorflow/contrib/avro/BUILD:13:1: no such package '@avro_archive//': Error downloading from http://www-us.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz to /home/AVV/rocm/.cache/bazel/_bazel_rocm/5fee00a38fb04f0267978a3cccec9960/external/avro_archive: Error downloading http://www-us.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz to /home/AVV/rocm/.cache/bazel/_bazel_rocm/5fee00a38fb04f0267978a3cccec9960/external/avro_archive/avro-cpp-1.8.0.tar.gz: 404 Not Found: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\r\n<html><head>\r\n<title>404 Not Found</title>\r\n</head><body>\r\n<h1>Not Found</h1>\r\n<p>The requested URL /dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz was not found on this server.</p>\r\n</body></html>\r\n and referenced by '//tensorflow/contrib/avro:example_h'.\r\nERROR: /home/AVV/rocm/tensorflow/tensorflow/contrib/avro/BUILD:13:1: no such package '@avro_archive//': Error downloading from http://www-us.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz to /home/AVV/rocm/.cache/bazel/_bazel_rocm/5fee00a38fb04f0267978a3cccec9960/external/avro_archive: Error downloading http://www-us.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz to /home/AVV/rocm/.cache/bazel/_bazel_rocm/5fee00a38fb04f0267978a3cccec9960/external/avro_archive/avro-cpp-1.8.0.tar.gz: 404 Not Found: <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\r\n<html><head>\r\n<title>404 Not Found</title>\r\n</head><body>\r\n<h1>Not Found</h1>\r\n<p>The requested URL /dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz was not found on this server.</p>\r\n</body></html>\r\n and referenced by '//tensorflow/contrib/avro:example_h'.\r\nERROR: Evaluation of query \"deps((//tensorflow/... union @bazel_tools//tools/jdk:toolchain))\" failed: errors were encountered while computing transitive closure.\r\n```\r\n", "I see, it seems that `avro-1.8.0` is no longer downloadable. Your best bet would be to [update `workspace.bzl`](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/workspace.bzl#L204) with the compatible AVRO version - perhaps to http://www-us.apache.org/dist/avro/avro-1.8.1/cpp/avro-cpp-1.8.1.tar.gz (and update the checksum accordingly).\r\n\r\nPlease feel free to fix that up and send a pull request to the r0.11 branch if that works.\r\n\r\nThanks!", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I'm trying to install from source TensorFlow r.011 (since I need to use a code developed using this version). \r\n\r\nTo fix the error you should do the following change:\r\n\r\n*Original snippet from `workspace.bzl`*:\r\n\r\n`native.new_http_archive(\r\n    name = \"avro_archive\",\r\n    url = \"http://www-us.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz\",\r\n    sha256 = \"ec6e2ec957e95ca07f70cc25f02f5c416f47cb27bd987a6ec770dcbe72527368\",\r\n    strip_prefix = \"avro-cpp-1.8.0\",\r\n    build_file = str(Label(\"//:avro.BUILD\")),\r\n  )\r\n`\r\n\r\n*Modified one*:\r\n\r\n`native.new_http_archive(\r\n    name = \"avro_archive\",\r\n    url = \"https://archive.apache.org/dist/avro/avro-1.8.0/cpp/avro-cpp-1.8.0.tar.gz\",\r\n    sha256 = \"ec6e2ec957e95ca07f70cc25f02f5c416f47cb27bd987a6ec770dcbe72527368\",\r\n    strip_prefix = \"avro-cpp-1.8.0\",\r\n    build_file = str(Label(\"//:avro.BUILD\")),\r\n  )`\r\n\r\nBasically, the url changes.\r\n\r\nHope this help.\r\n"]}, {"number": 9129, "title": "tensorflow1.1 rnn lstm:ValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights....", "body": "Environment info\r\n\r\nOperating System: Ubuntu 14.04.5 LTS\r\n\r\nInstalled version of CUDA and cuDNN:\r\nNo CUDA, I use CPU-only.\r\n\r\nPip version: pip 1.5.4\r\nPython version: 2.7.6\r\nOperating System: Ubuntu 14.04.5 LTS\r\nTensorflow version: tensorflow-1.1.0rc0-cp27-none-linux_x86_64 , CPU-only\r\nDescription:\r\n\r\nI was testing the tutorial example of LSTM .\r\nmy main function  train_rnn_classify.py:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport datetime\r\nfrom rnn_model import RNN_Model\r\nimport data_helper\r\n\r\n\r\nflags =tf.app.flags\r\nFLAGS = flags.FLAGS\r\n\r\n\r\nflags.DEFINE_integer('batch_size',64,'the batch_size of the training procedure')\r\nflags.DEFINE_float('lr',0.1,'the learning rate')\r\nflags.DEFINE_float('lr_decay',0.6,'the learning rate decay')\r\nflags.DEFINE_integer('vocabulary_size',20000,'vocabulary_size')\r\nflags.DEFINE_integer('emdedding_dim',128,'embedding dim')\r\nflags.DEFINE_integer('hidden_neural_size',128,'LSTM hidden neural size')\r\nflags.DEFINE_integer('hidden_layer_num',1,'LSTM hidden layer num')\r\nflags.DEFINE_string('dataset_path','/home/hadoop/lstm/subj0.pkl','dataset path')\r\nflags.DEFINE_integer('max_len',40,'max_len of training sentence')\r\nflags.DEFINE_integer('valid_num',100,'epoch num of validation')\r\nflags.DEFINE_integer('checkpoint_num',1000,'epoch num of checkpoint')\r\nflags.DEFINE_float('init_scale',0.1,'init scale')\r\nflags.DEFINE_integer('class_num',2,'class num')\r\nflags.DEFINE_float('keep_prob',0.5,'dropout rate')\r\nflags.DEFINE_integer('num_epoch',60,'num epoch')\r\nflags.DEFINE_integer('max_decay_epoch',30,'num epoch')\r\nflags.DEFINE_integer('max_grad_norm',5,'max_grad_norm')\r\nflags.DEFINE_string('out_dir',os.path.abspath(os.path.join(os.path.curdir,\"runs\")),'output directory')\r\nflags.DEFINE_integer('check_point_every',10,'checkpoint every num epoch ')\r\n\r\nclass Config(object):\r\n\r\n    hidden_neural_size=FLAGS.hidden_neural_size\r\n    vocabulary_size=FLAGS.vocabulary_size\r\n    embed_dim=FLAGS.emdedding_dim\r\n    hidden_layer_num=FLAGS.hidden_layer_num\r\n    class_num=FLAGS.class_num\r\n    keep_prob=FLAGS.keep_prob\r\n    lr = FLAGS.lr\r\n    lr_decay = FLAGS.lr_decay\r\n    batch_size=FLAGS.batch_size\r\n    num_step = FLAGS.max_len\r\n    max_grad_norm=FLAGS.max_grad_norm\r\n    num_epoch = FLAGS.num_epoch\r\n    max_decay_epoch = FLAGS.max_decay_epoch\r\n    valid_num=FLAGS.valid_num\r\n    out_dir=FLAGS.out_dir\r\n    checkpoint_every = FLAGS.check_point_every\r\n\r\n\r\ndef evaluate(model,session,data,global_steps=None,summary_writer=None):\r\n\r\n\r\n    correct_num=0\r\n    total_num=len(data[0])\r\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\r\n\r\n         fetches = model.correct_num\r\n         feed_dict={}\r\n         feed_dict[model.input_data]=x\r\n         feed_dict[model.target]=y\r\n         feed_dict[model.mask_x]=mask_x\r\n         model.assign_new_batch_size(session,len(x))\r\n         state = session.run(model._initial_state)\r\n         for i , (c,h) in enumerate(model._initial_state):\r\n            feed_dict[c]=state[i].c\r\n            feed_dict[h]=state[i].h\r\n         count=session.run(fetches,feed_dict)\r\n         correct_num+=count\r\n\r\n    accuracy=float(correct_num)/total_num\r\n    dev_summary = tf.scalar_summary('dev_accuracy',accuracy)\r\n    dev_summary = session.run(dev_summary)\r\n    if summary_writer:\r\n        summary_writer.add_summary(dev_summary,global_steps)\r\n        summary_writer.flush()\r\n    return accuracy\r\n\r\ndef run_epoch(model,session,data,global_steps,valid_model,valid_data,train_summary_writer,valid_summary_writer=None):\r\n    for step, (x,y,mask_x) in enumerate(data_helper.batch_iter(data,batch_size=FLAGS.batch_size)):\r\n\r\n        feed_dict={}\r\n        feed_dict[model.input_data]=x\r\n        feed_dict[model.target]=y\r\n        feed_dict[model.mask_x]=mask_x\r\n        model.assign_new_batch_size(session,len(x))\r\n        fetches = [model.cost,model.accuracy,model.train_op,model.summary]\r\n        state = session.run(model._initial_state)\r\n        for i , (c,h) in enumerate(model._initial_state):\r\n            feed_dict[c]=state[i].c\r\n            feed_dict[h]=state[i].h\r\n        cost,accuracy,_,summary = session.run(fetches,feed_dict)\r\n        train_summary_writer.add_summary(summary,global_steps)\r\n        train_summary_writer.flush()\r\n        valid_accuracy=evaluate(valid_model,session,valid_data,global_steps,valid_summary_writer)\r\n        if(global_steps%100==0):\r\n            print(\"the %i step, train cost is: %f and the train accuracy is %f and the valid accuracy is %f\"%(global_steps,cost,accuracy,valid_accuracy))\r\n        global_steps+=1\r\n\r\n    return global_steps\r\n\r\n\r\n\r\n\r\n\r\ndef train_step():\r\n\r\n    print(\"loading the dataset...\")\r\n    config = Config()\r\n    eval_config=Config()\r\n    eval_config.keep_prob=1.0\r\n\r\n    train_data,valid_data,test_data=data_helper.load_data(FLAGS.max_len,batch_size=config.batch_size)\r\n\r\n    print(\"begin training\")\r\n\r\n    # gpu_config=tf.ConfigProto()\r\n    # gpu_config.gpu_options.allow_growth=True\r\n    with tf.Graph().as_default(), tf.Session() as session:\r\n        initializer = tf.random_uniform_initializer(-1*FLAGS.init_scale,1*FLAGS.init_scale)\r\n        with tf.variable_scope(\"model\",reuse=None,initializer=initializer):\r\n            model = RNN_Model(config=config,is_training=True)\r\n\r\n        with tf.variable_scope(\"model\",reuse=True,initializer=initializer):\r\n            valid_model = RNN_Model(config=eval_config,is_training=False)\r\n            test_model = RNN_Model(config=eval_config,is_training=False)\r\n\r\n        #add summary\r\n        # train_summary_op = tf.merge_summary([model.loss_summary,model.accuracy])\r\n        train_summary_dir = os.path.join(config.out_dir,\"summaries\",\"train\")\r\n        train_summary_writer =  tf.train.SummaryWriter(train_summary_dir,session.graph)\r\n\r\n        # dev_summary_op = tf.merge_summary([valid_model.loss_summary,valid_model.accuracy])\r\n        dev_summary_dir = os.path.join(eval_config.out_dir,\"summaries\",\"dev\")\r\n        dev_summary_writer =  tf.train.SummaryWriter(dev_summary_dir,session.graph)\r\n\r\n        #add checkpoint\r\n        checkpoint_dir = os.path.abspath(os.path.join(config.out_dir, \"checkpoints\"))\r\n        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\r\n        if not os.path.exists(checkpoint_dir):\r\n            os.makedirs(checkpoint_dir)\r\n        saver = tf.train.Saver(tf.all_variables())\r\n\r\n\r\n        tf.initialize_all_variables().run()\r\n        global_steps=1\r\n        begin_time=int(time.time())\r\n\r\n        for i in range(config.num_epoch):\r\n            print(\"the %d epoch training...\"%(i+1))\r\n            lr_decay = config.lr_decay ** max(i-config.max_decay_epoch,0.0)\r\n            model.assign_new_lr(session,config.lr*lr_decay)\r\n            global_steps=run_epoch(model,session,train_data,global_steps,valid_model,valid_data,train_summary_writer,dev_summary_writer)\r\n\r\n            if i% config.checkpoint_every==0:\r\n                path = saver.save(session,checkpoint_prefix,global_steps)\r\n                print(\"Saved model chechpoint to{}\\n\".format(path))\r\n\r\n        print(\"the train is finished\")\r\n        end_time=int(time.time())\r\n        print(\"training takes %d seconds already\\n\"%(end_time-begin_time))\r\n        test_accuracy=evaluate(test_model,session,test_data)\r\n        print(\"the test data accuracy is %f\"%test_accuracy)\r\n        print(\"program end!\")\r\n\r\n\r\n\r\ndef main(_):\r\n    train_step()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n```\r\nmodel code rnn_model.py   :\r\n```\r\n`\r\nimport tensorflow as tf\r\n\r\nimport numpy as np\r\n\r\n\r\n\r\nclass RNN_Model(object):\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    def __init__(self,config,is_training=True):\r\n\r\n\r\n\r\n        self.keep_prob=config.keep_prob\r\n\r\n        self.batch_size=tf.Variable(0,dtype=tf.int32,trainable=False)\r\n\r\n\r\n\r\n        num_step=config.num_step\r\n\r\n        self.input_data=tf.placeholder(tf.int32,[None,num_step])\r\n\r\n        self.target = tf.placeholder(tf.int64,[None])\r\n\r\n        self.mask_x = tf.placeholder(tf.float32,[num_step,None])\r\n\r\n\r\n\r\n        class_num=config.class_num\r\n\r\n        hidden_neural_size=config.hidden_neural_size\r\n\r\n        vocabulary_size=config.vocabulary_size\r\n\r\n        embed_dim=config.embed_dim\r\n\r\n        hidden_layer_num=config.hidden_layer_num\r\n\r\n        self.new_batch_size = tf.placeholder(tf.int32,shape=[],name=\"new_batch_size\")\r\n\r\n        self._batch_size_update = tf.assign(self.batch_size,self.new_batch_size)\r\n\r\n\r\n\r\n        #build LSTM network\r\n\r\n\r\n\r\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_neural_size,forget_bias=0.0,state_is_tuple=True)\r\n\r\n        if self.keep_prob<1:\r\n\r\n            lstm_cell =  tf.contrib.rnn.DropoutWrapper(\r\n\r\n                lstm_cell,output_keep_prob=self.keep_prob\r\n\r\n            )\r\n\r\n\r\n\r\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell]*hidden_layer_num,state_is_tuple=True)\r\n\r\n\r\n\r\n        self._initial_state = cell.zero_state(self.batch_size,dtype=tf.float32)\r\n\r\n\r\n\r\n        #embedding layer\r\n\r\n        with tf.device(\"/cpu:0\"),tf.name_scope(\"embedding_layer\"):\r\n\r\n            embedding = tf.get_variable(\"embedding\",[vocabulary_size,embed_dim],dtype=tf.float32)\r\n\r\n            inputs=tf.nn.embedding_lookup(embedding,self.input_data)\r\n\r\n\r\n\r\n        if self.keep_prob<1:\r\n\r\n            inputs = tf.nn.dropout(inputs,self.keep_prob)\r\n\r\n\r\n\r\n        out_put=[]\r\n\r\n        state=self._initial_state\r\n\r\n        with tf.variable_scope(\"LSTM_layer\"):\r\n\r\n            for time_step in range(num_step):\r\n\r\n                if time_step>0: tf.get_variable_scope().reuse_variables()\r\n\r\n                (cell_output,state)=cell(inputs[:,time_step,:],state)\r\n\r\n                out_put.append(cell_output)\r\n\r\n\r\n\r\n        out_put=out_put*self.mask_x[:,:,None]\r\n\r\n\r\n\r\n        with tf.name_scope(\"mean_pooling_layer\"):\r\n\r\n\r\n\r\n            out_put=tf.reduce_sum(out_put,0)/(tf.reduce_sum(self.mask_x,0)[:,None])\r\n\r\n\r\n\r\n        with tf.name_scope(\"Softmax_layer_and_output\"):\r\n\r\n            softmax_w = tf.get_variable(\"softmax_w\",[hidden_neural_size,class_num],dtype=tf.float32)\r\n\r\n            softmax_b = tf.get_variable(\"softmax_b\",[class_num],dtype=tf.float32)\r\n\r\n            self.logits = tf.matmul(out_put,softmax_w)+softmax_b\r\n\r\n\r\n\r\n        with tf.name_scope(\"loss\"):\r\n\r\n            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.target,logits=self.logits+1e-10,)\r\n\r\n            self.cost = tf.reduce_mean(self.loss)\r\n\r\n\r\n\r\n        with tf.name_scope(\"accuracy\"):\r\n\r\n            self.prediction = tf.argmax(self.logits,1)\r\n\r\n            correct_prediction = tf.equal(self.prediction,self.target)\r\n\r\n            self.correct_num=tf.reduce_sum(tf.cast(correct_prediction,tf.float32))\r\n\r\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32),name=\"accuracy\")\r\n\r\n\r\n\r\n        #add summary\r\n\r\n        loss_summary = tf.contrib.deprecated.scalar_summary(\"loss\",self.cost)\r\n\r\n        #add summary\r\n\r\n        accuracy_summary=tf.contrib.deprecated.scalar_summary(\"accuracy_summary\",self.accuracy)\r\n\r\n\r\n\r\n        if not is_training:\r\n\r\n            return\r\n\r\n\r\n\r\n        self.globle_step = tf.Variable(0,name=\"globle_step\",trainable=False)\r\n\r\n        self.lr = tf.Variable(0.0,trainable=False)\r\n\r\n\r\n\r\n        tvars = tf.trainable_variables()\r\n\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\r\n\r\n                                      config.max_grad_norm)\r\n\r\n\r\n\r\n\r\n\r\n        # Keep track of gradient values and sparsity (optional)\r\n\r\n        grad_summaries = []\r\n\r\n        for g, v in zip(grads, tvars):\r\n\r\n            if g is not None:\r\n\r\n                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\r\n\r\n                sparsity_summary = tf.contrib.deprecated.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\r\n\r\n                grad_summaries.append(grad_hist_summary)\r\n\r\n                grad_summaries.append(sparsity_summary)\r\n\r\n        self.grad_summaries_merged = tf.summary.merge(grad_summaries)\r\n\r\n\r\n\r\n        self.summary =tf.summary.merge([loss_summary,accuracy_summary,self.grad_summaries_merged])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n        optimizer = tf.train.GradientDescentOptimizer(self.lr)\r\n\r\n        optimizer.apply_gradients(zip(grads, tvars))\r\n\r\n        self.train_op=optimizer.apply_gradients(zip(grads, tvars))\r\n\r\n\r\n\r\n        self.new_lr = tf.placeholder(tf.float32,shape=[],name=\"new_learning_rate\")\r\n\r\n        self._lr_update = tf.assign(self.lr,self.new_lr)\r\n\r\n\r\n\r\n    def assign_new_lr(self,session,lr_value):\r\n\r\n        session.run(self._lr_update,feed_dict={self.new_lr:lr_value})\r\n\r\n    def assign_new_batch_size(self,session,batch_size_value):\r\n\r\n        session.run(self._batch_size_update,feed_dict={self.new_batch_size:batch_size_value})`\r\n```\r\n  data handle code  data_helper.py:\r\n\r\n```\r\n\r\nimport numpy as np\r\n\r\nimport cPickle as pkl\r\n\r\n\r\n\r\n#file path\r\n\r\ndataset_path='/home/hadoop/lstm/subj0.pkl'\r\n\r\n\r\n\r\ndef set_dataset_path(path):\r\n\r\n    dataset_path=path\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef load_data(max_len,batch_size,n_words=20000,valid_portion=0.1,sort_by_len=True):\r\n\r\n    f=open(dataset_path,'rb')\r\n\r\n    print ('load data from %s',dataset_path)\r\n\r\n    train_set = np.array(pkl.load(f))\r\n\r\n    test_set = np.array(pkl.load(f))\r\n\r\n    f.close()\r\n\r\n\r\n\r\n    train_set_x,train_set_y = train_set\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    #train_set length\r\n\r\n    n_samples= len(train_set_x)\r\n\r\n    #shuffle and generate train and valid dataset\r\n\r\n    sidx = np.random.permutation(n_samples)\r\n\r\n    n_train = int(np.round(n_samples * (1. - valid_portion)))\r\n\r\n    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\r\n\r\n    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\r\n\r\n    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\r\n\r\n    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\r\n\r\n\r\n\r\n\r\n\r\n    train_set = (train_set_x, train_set_y)\r\n\r\n    valid_set = (valid_set_x, valid_set_y)\r\n\r\n\r\n\r\n\r\n\r\n    #remove unknow words\r\n\r\n    def remove_unk(x):\r\n\r\n        return [[1 if w >= n_words else w for w in sen] for sen in x]\r\n\r\n\r\n\r\n    test_set_x, test_set_y = test_set\r\n\r\n    valid_set_x, valid_set_y = valid_set\r\n\r\n    train_set_x, train_set_y = train_set\r\n\r\n\r\n\r\n    train_set_x = remove_unk(train_set_x)\r\n\r\n    valid_set_x = remove_unk(valid_set_x)\r\n\r\n    test_set_x = remove_unk(test_set_x)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    def len_argsort(seq):\r\n\r\n        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\r\n\r\n\r\n\r\n    if sort_by_len:\r\n\r\n        sorted_index = len_argsort(test_set_x)\r\n\r\n        test_set_x = [test_set_x[i] for i in sorted_index]\r\n\r\n        test_set_y = [test_set_y[i] for i in sorted_index]\r\n\r\n\r\n\r\n        sorted_index = len_argsort(valid_set_x)\r\n\r\n        valid_set_x = [valid_set_x[i] for i in sorted_index]\r\n\r\n        valid_set_y = [valid_set_y[i] for i in sorted_index]\r\n\r\n\r\n\r\n\r\n\r\n        sorted_index = len_argsort(train_set_x)\r\n\r\n        train_set_x = [train_set_x[i] for i in sorted_index]\r\n\r\n        train_set_y = [train_set_y[i] for i in sorted_index]\r\n\r\n\r\n\r\n    train_set=(train_set_x,train_set_y)\r\n\r\n    valid_set=(valid_set_x,valid_set_y)\r\n\r\n    test_set=(test_set_x,test_set_y)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    new_train_set_x=np.zeros([len(train_set[0]),max_len])\r\n\r\n    new_train_set_y=np.zeros(len(train_set[0]))\r\n\r\n\r\n\r\n    new_valid_set_x=np.zeros([len(valid_set[0]),max_len])\r\n\r\n    new_valid_set_y=np.zeros(len(valid_set[0]))\r\n\r\n\r\n\r\n    new_test_set_x=np.zeros([len(test_set[0]),max_len])\r\n\r\n    new_test_set_y=np.zeros(len(test_set[0]))\r\n\r\n\r\n\r\n    mask_train_x=np.zeros([max_len,len(train_set[0])])\r\n\r\n    mask_test_x=np.zeros([max_len,len(test_set[0])])\r\n\r\n    mask_valid_x=np.zeros([max_len,len(valid_set[0])])\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    def padding_and_generate_mask(x,y,new_x,new_y,new_mask_x):\r\n\r\n\r\n\r\n        for i,(x,y) in enumerate(zip(x,y)):\r\n\r\n            #whether to remove sentences with length larger than maxlen\r\n\r\n            if len(x)<=max_len:\r\n\r\n                new_x[i,0:len(x)]=x\r\n\r\n                new_mask_x[0:len(x),i]=1\r\n\r\n                new_y[i]=y\r\n\r\n            else:\r\n\r\n                new_x[i]=(x[0:max_len])\r\n\r\n                new_mask_x[:,i]=1\r\n\r\n                new_y[i]=y\r\n\r\n        new_set =(new_x,new_y,new_mask_x)\r\n\r\n        del new_x,new_y\r\n\r\n        return new_set\r\n\r\n\r\n\r\n    train_set=padding_and_generate_mask(train_set[0],train_set[1],new_train_set_x,new_train_set_y,mask_train_x)\r\n\r\n    test_set=padding_and_generate_mask(test_set[0],test_set[1],new_test_set_x,new_test_set_y,mask_test_x)\r\n\r\n    valid_set=padding_and_generate_mask(valid_set[0],valid_set[1],new_valid_set_x,new_valid_set_y,mask_valid_x)\r\n\r\n\r\n\r\n    return train_set,valid_set,test_set\r\n\r\n\r\n\r\n\r\n\r\n#return batch dataset\r\n\r\ndef batch_iter(data,batch_size):\r\n\r\n\r\n\r\n    #get dataset and label\r\n\r\n    x,y,mask_x=data\r\n\r\n    x=np.array(x)\r\n\r\n    y=np.array(y)\r\n\r\n    data_size=len(x)\r\n\r\n    num_batches_per_epoch=int((data_size-1)/batch_size)\r\n\r\n    for batch_index in range(num_batches_per_epoch):\r\n\r\n        start_index=batch_index*batch_size\r\n\r\n        end_index=min((batch_index+1)*batch_size,data_size)\r\n\r\n        return_x = x[start_index:end_index]\r\n\r\n        return_y = y[start_index:end_index]\r\n\r\n        return_mask_x = mask_x[:,start_index:end_index]\r\n\r\n        # if(len(return_x)<batch_size):\r\n\r\n        #     print(len(return_x))\r\n\r\n        #     print return_x\r\n\r\n        #     print return_y\r\n\r\n        #     print return_mask_x\r\n\r\n        #     import sys\r\n\r\n        #     sys.exit(0)\r\n\r\n        yield (return_x,return_y,return_mask_x)\r\n```\r\n\r\n\r\nWhen I open a terminal and run\r\n`  python train_rnn_classify.py`\r\nthen  has error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train_rnn_classify.py\", line 176, in <module>\r\n    tf.app.run()\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"train_rnn_classify.py\", line 172, in main\r\n    train_step()\r\n  File \"train_rnn_classify.py\", line 128, in train_step\r\n    valid_model = RNN_Model(config=eval_config,is_training=False)\r\n  File \"/home/hadoop/lstm/rnn_model.py\", line 51, in __init__\r\n    (cell_output,state)=cell(inputs[:,time_step,:],state)\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 953, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 235, in __call__\r\n    with _checked_scope(self, scope or \"basic_lstm_cell\", reuse=self._reuse):\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/home/hadoop/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\", line 93, in _checked_scope\r\n    \"the argument reuse=True.\" % (scope_name, type(cell).__name__))\r\nValueError: Attempt to have a second RNNCell use the weights of a variable scope that already has weights: 'model/LSTM_layer/multi_rnn_cell/cell_0/basic_lstm_cell'; and the cell was not constructed as BasicLSTMCell(..., reuse=True).  To share the weights of an RNNCell, simply reuse it in your second calculation, or create a new one with the argument reuse=True.\r\n```\r\n\r\nWhy can't I run this example?How to solve this  problem?\r\nThank you all for your kind help!!!", "comments": ["This question sounds like it would be more appropriate for [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow).", "You can have a look at [https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py](url).\r\nRow 112-116\r\nThis is something diffierent between tf1.0 and tf1.1"]}, {"number": 9128, "title": "Problem with label_image.py ", "body": "\r\nI used docker toolbox on windows 10 64-bit and used this command \r\ndocker run -it gcr.io/tensorflow/tensorflow:latest-devel  \r\nto install tensorflow image. \r\nAfter retraining of model is successfully done and when I run label_image.py it gives following message\r\n![capture](https://cloud.githubusercontent.com/assets/20141573/24899312/27d5b654-1ea0-11e7-8df6-517c4df18bfb.PNG)\r\n\r\nI used this line \r\ncurl -L https://goo.gl/tx3dqg > $HOME/tf_files/label_image.py\r\nand then\r\ndocker run -it -v $HOME/tf_files:/tf_files  gcr.io/tensorflow/tensorflow:latest-devel \r\n\r\n\r\n\r\n\r\n\r\n\r\nNOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.\r\n\r\n### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*:\r\n- *TensorFlow installed from (source or binary)?*:\r\n- *TensorFlow version*:\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*:\r\n- *GPU Model and Memory*:\r\n- *Exact command to reproduce*:\r\n\r\n### Describe the problem clearly\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem\r\n", "comments": ["From the error message, it seems that somehow your `label_image.py` file is corrupt (and contains `<HTML>` in it). Please confirm the contents of `label_image.py` are correct.", "Thanks for your reply. Yes I found out that somehow the curl wasn't able to put code in the file. I copied and pasted from the site  and now its fine. "]}, {"number": 9127, "title": "How to reinitialize state prior to inference with RNN", "body": "I have an RNN graph which has been trained in TF with good accuracy, but when it runs in Android (using libtensorflow_inference.so) it runs poorly.  I have a hypothesis that the problem is due to the statefulness of the RNN.  It is my understanding that during training the RNN state is reinitialized to a fresh state with each minibatch.  However, when running in reallife, it is running continuously without the state being refreshed.  Thus causing anomalous results.\r\nIs there a way to force the state to be refreshed when running in an Android environment?  I note that the java interface (tensorflowinferenceinterface.java) has no such method.\r\nI have also documented this inquiry in SO nine days ago, but had no response: [link](http://stackoverflow.com/questions/43159921/tensorflow-initializing-state-for-rnn-between-inferences-in-android).", "comments": ["Kevin, you did the right thing by originally going to StackOverflow with this question. This issue tracker is not an escalation path.\r\n\r\nWe have a *bugs and feature requests only policy* on this issue tracker. The reason is because TensorFlow developers respond to issues (usually within a day.) Our goal is to focus on work that benefits everyone in the community, e.g. fixing bugs and writing features. Support only helps individuals. Filing an issue also causes thousands of people receive notifications. We want those people to see individuals communicating the most interesting problems possible.\r\n\r\nThe reason why I'm explaining the policy is because I'm noticing this isn't the first issue. I believe policy works best when it's transparent."]}, {"number": 9126, "title": "How to run TF learn (skflow) Kmeans clustering in multi-machine multi-gpu environment?", "body": "Is there any example / guide which shows how to implement skflow(tf.contrib.learn) in distributed GPUs? For example :- I want to implement KMeansClustring using skflow(tf.contrib.learn) in a distributed GPU environment. How should I proceed?\r\n\r\nStackoverflow questions :- [here](http://stackoverflow.com/questions/43339285/how-to-run-tf-learn-skflow-kmeans-clustering-in-multi-machine-multi-gpu-enviro)  and [here](http://stackoverflow.com/questions/42516334/implement-skflowtf-contrib-learn-in-a-distributed-gpu-environment)\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9125, "title": "KeyError in tf.contrib.graph_editor.graph_replace", "body": "When applying `graph_replace` to graphs containing ops with the `_original_op` attribute, it can fail with a `KeyError`. The error occurs in `Transformer._copy_ops` when trying to copy an op whose `_original_op` has not yet been copied. The ordering of ops that are copied is not deterministic so this error pops up somewhat randomly.\r\n\r\nThe `_original_op` attributes appear to be created by `tf.gradients` to point back to the op from the forward pass.\r\n\r\nExample code snippet (note: you may need to run this multiple times to get a failure):\r\n```python\r\nimport tensorflow as tf\r\ngraph_replace = tf.contrib.graph_editor.graph_replace\r\nw = tf.Variable(0.0, name=\"w\")\r\ny = tf.multiply(tf.multiply(w, w, name=\"mul1\"), w, name=\"mul2\")\r\ng = tf.gradients(y, w)[0]\r\ng_new = graph_replace(g, {w.value(): g})\r\n```\r\n\r\nError:\r\n```\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/graph_editor/transform.py in transform_op_if_inside_handler(info, op, keep_if_possible)\r\n    122   \"\"\"\r\n    123   if op in info.sgv.ops:\r\n--> 124     return info.transformed_ops[op]\r\n    125   else:\r\n    126     if keep_if_possible and info.graph is info.graph_:\r\n\r\nKeyError: <tf.Operation 'mul1' type=Mul>\r\n```\r\n\r\nI see three possible fixes:\r\n1. Remove `_original_op` attributes in the copied graph (I don't see anywhere in the TF codebase where it is used)\r\n2. Move the creation of the `_original_op` attribute from the `copy_op_handler` function to the end of `Transformer._copy_ops` after all ops have been copied.\r\n3. Topologically sort the ops being copied so that ops that are `_original_op` attributes are created before their children.\r\n\r\nMy [implementation](https://github.com/poolio/tensorflow/pull/1/files) of option 2 seems to fix this problem, but I might be missing something about the usage of `_original_op`.", "comments": ["@purpledog Here's an interesting graph editor issue for you. If you need toposort I wrote a Tarjan implementation in Python I can give you. Also get_walks_intersection_ops looks quadratic.", "The similar issue still persist when variable is matix:-\r\n\r\nimport tensorflow as tf\r\ngraph_replace = tf.contrib.graph_editor.graph_replace\r\nw = tf.Variable(tf.zeros([784, 784]),name=\"w\")\r\ny = tf. matmul(tf.matmul(w, w, name=\"mul1\"), w, name=\"mul2\")\r\ng = tf.gradients(y, w)[0]\r\ng_new = graph_replace(g, {w.value(): g})\r\n\r\n\r\nTraceback (most recent call last):\r\n File \"/scratch0/username/Users/Jaiusername/Documents/Study/research/pytorchExamples/GAN/keras-adversarial/examples/testKeyErrorFailure.py\", line 9, in <module>\r\n    g_new = graph_replace(g, {w.value(): g})\r\n  File \"/scratch0/username/installs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 655, in graph_replace\r\n    ops, replacement_ts, None, dst_scope, src_scope, reuse_dst_scope)\r\n  File \"/scratch0/username/installs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 617, in copy_with_input_replacements\r\n    sgv, dst_graph, dst_scope, src_scope, reuse_dst_scope=reuse_dst_scope)\r\n  File \"/scratch0/username/installs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 434, in __call__\r\n    self._copy_ops(info)\r\n  File \"/scratch0/username/installs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 447, in _copy_ops\r\n    op_, op_outputs_ = self.transform_op_handler(info, op)\r\n  File \"/scratch0/username/installs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 171, in copy_op_handler\r\n    original_op = info.transform_original_op_handler(info, op._original_op)\r\n  File \"/scratch0/username/installs/tensorflow/lib/python2.7/site-packages/tensorflow/contrib/graph_editor/transform.py\", line 124, in transform_op_if_inside_handler\r\n    return info.transformed_ops[op]\r\nKeyError: <tf.Operation 'mul1' type=MatMul>\r\n\r\nProcess finished with exit code 1\r\n\r\n\r\n", "I'm happy to submit a PR using my [implementation](https://github.com/poolio/tensorflow/pull/1/files) of the proposed second option. Please let me know whether that would be helpful as `graph_replace` was the only way of copying and replacing subgraphs. It would be great to have this working again, especially given that other PRs for similar features have been rejected (#5802).", "Hi,\r\nWhat's the current status of this bug as of r1.4?\r\nThanks!", "@gokul-uf it still exists. This is the same issue as https://github.com/tensorflow/tensorflow/issues/9978 . Current work-around is to modify graph_editor source locally although PR would be welcome", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "Closing out this issue, as `tf.contrib` is no longer supported in TensorFlow 2.x. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9125\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/9125\">No</a>\n"]}, {"number": 9124, "title": "[CMake] Optionally support building TF as a shared lib", "body": "Currently, TF is already built as a shared library that is included in the Python package. However, that library:\r\n\r\n1. Is implicitly linked against Python libs, and thus expects Python to be present on the machine wherever it is used. It is undesirable for scenarios not requiring Python (e.g. native application that need TensorFlow).\r\n2.  Does not include the TF C++ API.\r\n\r\nThis PR allows optionally building TF as a stand-alone DLL that does not have the above issues. I am also working on allowing CMake to link all C++ tests against that DLL, and will submit such changes in a separate PR.\r\n\r\nAs a bonus, this PR also fixes a build break for tf_tools.cmake when GPU is enabled.", "comments": ["Can one of the admins verify this patch?", "It seems like you save my life ;) Thank you!", "@vit-stepanovs I should run cmake with `-DBUILD_SHARED_LIB=ON` but then what build target do I run?", "@tomjaguarpw, the actual option name is tensorflow_BUILD_SHARED_LIB, not BUILD_SHARED_LIB. If you use that, cmake generates a project tensorflow.vcxproj (not tensorflow.sln!) which you can use to build the DLL.", "Nice!\r\n\r\nLGTM, @mrry what do you think?", "Jenkins, test this please.", "Thanks!", "It works great on both Linux and Windows for me. Why is this PR removed from release 1.1.0? (it was there in 1.1.0-rc2)", "It's probably because it didn't make it before the cutoff point when we forked the branch.\r\n\r\n/CC: @av8ramit ", "I don't believe this was in 1.1.0-rc2, but yes we decided to push it back to 1.2 since it was merged too late for 1.1 to test properly.", "Is there some way to build tensorflow as static lib?", "I tried this in conjunction with https://github.com/tensorflow/tensorflow/pull/9666 to try to build a debug DLL,, but the generated .def file had over 120K symbols, so it exceeded the 65535 limit.  Any way to filter more aggressively?", "@girving another reason to restrict symbols #9525\n\nOn May 12, 2017 1:24 PM, \"Andy Dennie\" <notifications@github.com> wrote:\n\nI tried this in conjunction with #9666\n<https://github.com/tensorflow/tensorflow/pull/9666> to try to build a\ndebug DLL,, but the generated .def file had over 120K symbols, so it\nexceeded the 65535 limit. Any way to filter more aggressively?\n\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\n<https://github.com/tensorflow/tensorflow/pull/9124#issuecomment-301176827>,\nor mute the thread\n<https://github.com/notifications/unsubscribe-auth/AT_SbaXsy-KifiQ0902rs8mKlt-4mHgSks5r5MAKgaJpZM4M5rj->\n.\n", "Nice.  Is there a thread I should notify once we make progress here?  This one seems merged."]}, {"number": 9123, "title": "no such package '@gmock_archive//': Error downloading in Tensorflow (on ubuntu_X86 14.04) ", "body": "I'm getting the following error when trying to run the configure command (on ubuntu_X86:14.04):\r\n\r\nERROR: /root/tf/tensorflow/tensorflow/workspace.bzl:221:3: no such package '@gmock_archive//': Error downloading\r\n\r\nPlease find the attached log for more details.\r\n[logfile.txt](https://github.com/tensorflow/tensorflow/files/912026/logfile.txt)\r\n\r\n\r\nNote- On ubuntu-ppc64le vm , configure command ran successfully.\r\n\r\nAny suggestion/solution ?", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also **include your TensorFlow version**. Also, did you compile from source or install a binary?  **Make sure you also include the exact command** if possible to produce  the output included in your test case.   We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nThough, looking at the log file you provided and the errors like:\r\n\r\n```\r\nERROR: /root/tf/tensorflow/tensorflow/workspace.bzl:221:3: no such package '@gmock_archive//': Error downloading [http://bazel-mirror.storage.googleapis.com/pkgs.fedoraproject.org/repo/pkgs/gmock/gmock-1.7.0.zip/073b984d8798ea1594f5e44d85b20d66/gmock-1.7.0.zip, http://pkgs.fedoraproject.org/repo/pkgs/gmock/gmock-1.7.0.zip/073b984d8798ea1594f5e44d85b20d66/gmock-1.7.0.zip] to /root/.cache/bazel/_bazel_root/8e11c3f2ede966708196f520db8b1e69/external/gmock_archive/gmock-1.7.0.zip: All mirrors are down: [Checksum was 49a998729226839bc06d6d4b8384af0c83e05e842b979193f43e1c3440e9cd16 but wanted 26fcbb5925b74ad5fc8c26b0495dfc96353f4d553492eb97e85a8a6d2f43095b, Checksum was dc73d26ce544ae4ba7f1ad6d305de0dee72bd953e4bfd9ba7170b2d5326946de but wanted 26fcbb5925b74ad5fc8c26b0495dfc96353f4d553492eb97e85a8a6d2f43095b] and referenced by '//external:gtest'\r\n```\r\n\r\nthere seems to be some network error on the machine that might be truncating or corrupting the files."]}, {"number": 9122, "title": "Mofify seq2seq.py", "body": "Modify the tf.contrib.legacy_seq2seq.embedding_attention_seq2seq so that it allows user to explicitly provide a second cell for the encoding step, and if it's None then to fallback on the deepcopy.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 9121, "title": "Copying the whl file for python 3.6 as well.", "body": "Change: 150691934", "comments": []}]