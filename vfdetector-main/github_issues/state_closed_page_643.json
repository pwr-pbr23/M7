[{"number": 34331, "title": "Enable GPUNMS v3 and v4", "body": "This PR enables GPU implementation for NonMaxSuppresion ops v3 and v4 which was disabled due to high memory utilization at extremely large input counts (~8GB for 256000 boxes). Op has modest memory requirements for most models having about O(10k) inputs.", "comments": ["This should be against master and then cherry-picked on r2.1.", "@mihaimaruseac I thought you didn't want this in master because of your internal models.", "It should always go to master and then cherry-picked. We don't want `r..` branches to diverge from master", "@pkanwar23 do you know what would be the best way to solve this issue?"]}, {"number": 34330, "title": "TF World '19 Talk: Facemesh with TF.js in the browser", "body": "Just saw the wonderful Unlocking the power of ML for your JavaScript applications with TensorFlow.js talk, link: https://youtu.be/kKp7HLnPDxc.\r\n\r\nI'd also like to try the code of using Facemesh in the browser. Following the example code in the video at 25:48, I went to https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh. \r\n\r\nHowever, I get the message \"Failed to resolve the requested file.\" \r\n\r\nWill you please help me find the Facemesh model?", "comments": ["This issue is more suitable for TensorFlow TFjs repo. Please post it on TFjs repo from [here.](https://github.com/tensorflow/tfjs/issues/new). Thanks!", "@kavikode \r\nPlease, let us know if you have posted the issue in TensorFlow TFjs repo ?. Thanks!", "I am closing the issue as this issue is related to TFjs repo. Thanks!"]}, {"number": 34329, "title": "Tensorflow TensorRT: Could not load dynamic library 'libnvinfer.so.5'", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): Tensorflow 2.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n_(Note - also posted to NVIDIA developer forums)_\r\n\r\nI'm following the Tensorflow 2.0 [instructions](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#usage-example) for optimizing a SavedModel file with TensorRT. I have my Tensorflow model saved to models/mymodel. When I run the following:\r\n\r\n```\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir='models/mymodel')\r\nconverter.convert()\r\nconverter.save('models/mymodel_tensorrt')\r\n```\r\n\r\nI get the error (from second line):\r\n\r\n> 2019-11-14 17:29:07.427738: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.5'; dlerror: libnvinfer.so.5: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:/home/ubuntu/src/cntk/bindings/python/cntk/libs:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:\r\n2019-11-14 17:29:07.427783: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.\r\nAborted (core dumped)\r\n\r\nLooking at the thread [here](https://devtalk.nvidia.com/default/topic/1036527/tensorrt/importerror-libnvinfer-so-4-cannot-open-shared-object-file-no-such-file-or-directory/), it seems like it may be an issue with needing to set LD_LIBRARY_PATH.\r\n\r\nHowever, when I google setting the LD_LIBRARY_PATH, it seems only necessary when manually installing TensorRT from tar. I have not built/installed TensorRT separately and am just using what's bundled in with Tensorflow 2.0. \r\n\r\n**Describe the expected behavior**\r\nThe converter should run and save optimized model to 'models/mymodel_tensorrt'", "comments": ["I solved the problem. While it seems like some portion of tensorrt is bundled in with tensorflow-gpu, there are still installation steps required (see [installation under Ubuntu 18.04 (CUDA 10)](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_10)\r\n\r\nI already had CUDA drivers installed (10.1). I ran the following code, without modifying version numbers, and this allowed me to successfully convert the model. I'm not sure if I needed to the \"install NVIDIA driver\", as I'm assuming that was already installed, as I had already been running tensorflow-gpu without issue.\r\n\r\n```\r\n# Add NVIDIA package repositories\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\r\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\r\nsudo apt-get update\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n\r\n# Install NVIDIA driver\r\nsudo apt-get install --no-install-recommends nvidia-driver-418\r\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\r\n\r\n# Install TensorRT. Requires that libcudnn7 is installed above.\r\nsudo apt-get install -y --no-install-recommends libnvinfer5=5.1.5-1+cuda10.0 \\\r\n    libnvinfer-dev=5.1.5-1+cuda10.0\r\n```", "It seems tensorrt library import was failing. Install tensorrt \r\n<https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html>\r\n\r\n -> Dowload and install tensorrt compatible with your cuda version. \r\n   <https://developer.nvidia.com/nvidia-tensorrt-6x-download>\r\n\r\n >sudo apt-get update\r\n >sudo apt-get install tensorrt\r\n> sudo apt-get install python3-libnvinfer-dev (for python3)\r\n>sudo apt-get install uff-converter-tf\r\n>dpkg -l | grep TensorRT", "If you are getting this error while running in jetson nano, \r\n\r\nHi,\r\n\r\nThe error indicates that TensorFlow want to link TensorRT 5.0 library but your device already upgrades to 6.0.\r\nSuppose you are using JetPack 4.3, so you need to update the TensorFlow link into v43.\r\n\r\nWe provide two TensorFlow version for JetPack 4.3 user now.\r\n\r\n**TensorFlow 2.0**\r\n`$ sudo pip3 install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow-gpu\r\n`\r\n**TensorFlow 1.15**\r\n`pip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v43 tensorflow-gpu==1.15.0+nv19.12`\r\n\r\n [Source Discussion link](https://forums.developer.nvidia.com/t/tensorflow-python-framework-errors-impl-notfounderror-libnvinfer-so-5-cannot-open-shared-object-file-no-such-file-or-directory/110107/4)\r\n", "installing tensorflow-cpu solved the issue for me", "For my case, I believe it means that you have the wrong version of library. libnvinfer.so.6 is located at 'TensorRT-*/lib' and the number 6 means tensorFlow is looking for the libvinfer of TensorRT6. So if it's \"could not load dynamic library libnvinfer.so.5\", it means that you need TensorRT 5 to run the code.\r\n\r\nSame as above, if it is showing Could not load dynamic library 'libcudart.so.10.0', you need the library in cuda 10.0 to run the code.\r\n\r\nSo updating your tensorrt/Cuda/Cudnn to match your tensorflow version would help. Note that your tensorrt/cuda/cudnn version should also match each other.", "Tensorflow has handled this in their [installation guide](https://www.tensorflow.org/install/gpu)"]}, {"number": 34328, "title": "prepare_attention API of tensorflow 1.0.0 equivalent in tensorflow 1.13.1", "body": "What is the equivalent API in tensorflow 1.13.1 for prepare_attention(tf.contrib.seq2seq.prepare_attention) of tensorflow 1.0.0?\r\n\r\nThanks!!!", "comments": ["@lpcoutinho ,\r\nHi,Please note that `tf.contrib.seq2seq.DynamicAttentionWrapper `will work. Also for 2.0 `contrib` has been removed and you can find the equivalent from the this repo [ Addon-Seq2Seq](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/README.md).Thanks!", "@lpcoutinho ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34327, "title": "[r2.1:Cherrypick]Update the TF CUDA version to 10.1", "body": "PiperOrigin-RevId: 280237880\nChange-Id: Idb9ea64c73746e18b372f3c3a0e56b8b64d53b06", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34327) for more info**.\n\n<!-- need_sender_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F34327) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 34326, "title": "tf.signal.mfccs_from_log_mel_spectrograms caused internal error in converted TensorFlow Lite model", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: OnePlus A3010\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 2.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.26.1\r\n- **GCC/Compiler version (if compiling from source)**:  Apple clang version 11.0.0 (clang-1100.0.33.12)\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nHello everyone,\r\n\r\nI have tried to extract MFCC features from audio signal using tf.signal.mfccs_from_log_mel_spectrograms. My python code runs pretty well, but when I converted it to a TensorFlow Lite model, and used it on android phone. I got the following error:\r\n\r\n\"\r\n...\r\n...\r\nCaused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/strided_slice.cc ellipsis_mask is not implemented yet.\r\n    Node number 20 (STRIDED_SLICE) failed to prepare.\r\n...\r\n...\r\n\"\r\n\r\nI am pretty sure the error was caused by tf.signal.mfccs_from_log_mel_spectrograms, since I did not directly use \"strided_slice\" directly in my code, and in the visualized graph of the html file obtained by [visualization tool](https://www.tensorflow.org/lite/guide/faq) , I can only find the context of strided_slice like: \r\n\r\n\"\r\n...\r\n161 | StatefulPartitionedCall/model_1/lambda/mfccs_from_log_mel_spectrograms_2/dct/strided_slice | COMPLEX64 | [] | 59 | {u'quantized_dimension': 0, u'details_type': u'NONE'}\r\n-- | -- | -- | -- | -- | --\r\n162 | StatefulPartitionedCall/model_1/lambda/mfccs_from_log_mel_spectrograms_2/dct/strided_slice/stack | INT32 | [2] | 136 | {u'quantized_dimension': 0, u'details_type': u'NONE'}\r\n163 | StatefulPartitionedCall/model_1/lambda/mfccs_from_log_mel_spectrograms_2/dct/strided_slice/stack_1 | INT32 | [2] | 177 | {u'quantized_dimension': 0, u'details_type': u'NONE'}\r\n164 | StatefulPartitionedCall/model_1/lambda/mfccs_from_log_mel_spectrograms_2/dct/strided_slice/stack_2 | INT32 | [2] | 63 | {u'quantized_dimension': 0, u'details_type': u'NONE'}\r\n...\r\n\"\r\n\r\nIn the same graph file, I also found\r\n\"\r\n22 | [157, 162, 163, 164] | [161] | {u'begin_mask': 2, u'ellipsis_mask': 1, u'end_mask': 0, u'new_axis_mask': 0, u'shrink_axis_mask': 0}\r\n-- | -- | -- | --\r\n\"\r\n\r\nSo I think tf.signal.mfccs_from_log_mel_spectrograms trigged a dct, following a strided_slice, whose \"ellipsis_mask\" was set to 1, which is not implemented in TensorFlow Lite. \r\n\r\nBut I truly don't know how to avoid that error, actually, I got the same error, when using tf.signal.stft, also tf.signal.frame. They all called \"tf.strided_slice\" implicitly and set \"ellipsis_mask\" to 1.\r\n\r\nBy the way, the dimension of input tensor(log of mel spectrograms) to tf.signal.mfccs_from_log_mel_spectrograms was (None, 64, 20). Please kindly help me!\r\n\r\nRegards,\r\nSimon\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["For me it worked, when I saved model in the following matter:\r\n```\r\nimport tensorflow as tf\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n\r\n\r\nAnd then added tensorflow-lite-select-tf-ops nightly build to the dependencies in build.gradle:\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n}\r\n\r\n```\r\n\r\nCheck this guide for further details: https://www.tensorflow.org/lite/guide/ops_select", "@tpeet, thank you for your reply! I was using the same convert scripts, but built a local Tensorflow lite aar by Bazel from source. So did you succeed in using tf.signal.mfccs_from_log_mel_spectrograms, or any mfcc function like audio_ops.mfcc in a Tensorflow lite on android? If so, could you kindly provide more details, like your Tensorflow version, lines of codes using mfcc? Thank you so much!\r\n\r\nRegards,\r\nSimon", "Hi, we're actively working to add native support for the ellipsis_mask, targeting the next TF 2.2 release in Q1.", "@jdduke is ellipsis_mask supported in TF2.2 now? Seems like it's still not working.", "> @jdduke is ellipsis_mask supported in TF2.2 now? Seems like it's still not working.\r\n\r\nCan you try with the nightly build? I don't think our ellipsis_mask fix made it into the 2.2 release. Note that you'll have to re-converter your model.\r\n\r\n", "Right, you need re-export your model.\r\n\r\nWe handle the ellipsis_mask in the converter.\r\n\r\nthanks!", "thanks. i end up converting all my usage like `[..., 0:1]` into something more explicit like `[:, :, :, 0:1]`. but it's good to know that `ellipsis_mask` will be supported in next release.", "Great.\r\nOur converter basically does that \"[..., 0:1] => [:, :, :, 0:1]\" for you right now. ", "@simonchen375,\r\n\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to `2.6` which is latest stable version of TF and let us know if the issue still persists in newer versions. we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34326\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34326\">No</a>\n"]}, {"number": 34325, "title": "Fix the performance regression for model.predict for non-tpu strategy.", "body": "PiperOrigin-RevId: 280718986\r\nChange-Id: I8eed8854a58ec3adfaccd7d96067d248ee2696c4", "comments": []}, {"number": 34324, "title": "Complex dtype support tflite converter", "body": "Are complex dtypes planned to be supported in the experimental converter? using nightly 1108 build and it is throwing an error. This is super useful for MFCC calculations.\r\n\r\n```python\r\ndef generate_mfcc_features(audio_array):\r\n    sample_rate = 16000.0\r\n\r\n    # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n    stfts = tf.signal.stft(audio_array, frame_length=1024, frame_step=256,fft_length=1024)\r\n    spectrograms = tf.abs(stfts)\r\n#     spectrograms = tf.cast(stfts + 1, tf.float32)\r\n\r\n    # Warp the linear scale spectrograms into the mel-scale.\r\n    num_spectrogram_bins = stfts.shape[-1]\r\n    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n    linear_to_mel_weight_matrix = tf.cast(tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz), tf.float32)\r\n    mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\r\n    mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\r\n\r\n    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n    # Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n    return tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :13]\r\n\r\ncnn = tf.keras.Sequential([\r\n    tf.keras.layers.Lambda(generate_mfcc_features, input_shape=(40000,)),\r\n    tf.keras.layers.Conv1D(filters=32, kernel_size=10, strides=2, padding='same', activation='relu')\r\n])\r\n```\r\n\r\nError:\r\n```\r\n---------------------------------------------------------------------------\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-295-26886e1134e8> in <module>()\r\n      8 converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_LATENCY]\r\n      9 converter.target_spec.supported_ops = set([tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS])\r\n---> 10 tflite_model_enc = converter.convert()\r\n     11 \r\n     12 print('Converting decoder')\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py in convert(self)\r\n    462         input_tensors=input_tensors,\r\n    463         output_tensors=output_tensors,\r\n--> 464         **converter_kwargs)\r\n    465 \r\n    466     if self._is_calibration_quantize():\r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\r\n    455       input_data.SerializeToString(),\r\n    456       debug_info_str=debug_info_str,\r\n--> 457       enable_mlir_converter=enable_mlir_converter)\r\n    458   return data\r\n    459 \r\n\r\n~/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    201       stdout = _try_convert_to_unicode(stdout)\r\n    202       stderr = _try_convert_to_unicode(stderr)\r\n--> 203       raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\n    204   finally:\r\n    205     # Must manually cleanup files.\r\n\r\nConverterError: See console for info.\r\n2019-11-15 15:12:34.503239: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2019-11-15 15:12:34.503275: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2019-11-15 15:12:35.451940: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2019-11-15 15:12:35.452051: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2019-11-15 15:12:35.452072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2019-11-15 15:12:36.763021: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:89] Ignored output_format.\r\n2019-11-15 15:12:36.763078: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:95] Ignored drop_control_dependency.\r\n2019-11-15 15:12:36.923578: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-15 15:12:36.930986: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294685000 Hz\r\n2019-11-15 15:12:36.931794: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55eb1a6ac800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-15 15:12:36.931821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-15 15:12:36.933506: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2019-11-15 15:12:36.933531: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-11-15 15:12:36.933572: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (VXML-Ubuntu): /proc/driver/nvidia/version does not exist\r\nloc(callsite(\"sequential_5/lambda_4/mfccs_from_log_mel_spectrograms/dct/strided_slice\"(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/ops/signal/dct_ops.py\":130:0) at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/ops/signal/mfcc_ops.py\":107:0 at callsite(\"<ipython-input-228-d9ade39d80ce>\":20:0 at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py\":827:0 at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\":778:0 at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\":891:0 at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\":717:0 at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py\":267:0 at callsite(\"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\":778:0 at \"<ipython-input-290-205be255c740>\":3:0)))))))))): error: 'tfl.strided_slice' op operand #0 must be tensor of 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or 1-bit integer values, but got 'tensor<1x153x81xcomplex<f32>>'\r\nTraceback (most recent call last):\r\n  File \"/home/mattc/anaconda3/envs/main/bin/toco_from_protos\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: /home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/ops/signal/dct_ops.py:130:15: error: 'tfl.strided_slice' op operand #0 must be tensor of 32-bit float or 32-bit integer or 64-bit integer or 8-bit integer or QI8 type or QUI8 type or 1-bit integer values, but got 'tensor<1x153x81xcomplex<f32>>'\r\n              input, fft_length=[2 * axis_dim])[..., :axis_dim] * scale)\r\n              ^\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/ops/signal/mfcc_ops.py:107:5: note: called from\r\n    dct2 = dct_ops.dct(log_mel_spectrograms, type=2)\r\n    ^\r\n<ipython-input-228-d9ade39d80ce>: note: called from\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py:827:7: note: called from\r\n      return self.function(inputs, **arguments)\r\n      ^\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:778:19: note: called from\r\n                  outputs = call_fn(cast_inputs, *args, **kwargs)\r\n                  ^\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py:891:11: note: called from\r\n          output_tensors = layer(computed_tensors, **kwargs)\r\n          ^\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py:717:9: note: called from\r\n        convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n        ^\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py:267:7: note: called from\r\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\r\n      ^\r\n/home/mattc/anaconda3/envs/main/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py:778:19: note: called from\r\n                  outputs = call_fn(cast_inputs, *args, **kwargs)\r\n                  ^\r\n<ipython-input-290-205be255c740>: note: called from\r\n```\r\n\r\nIs there a plan to support signal operations like this in a TFLite model?", "comments": ["hi, does this problem still exist?", "@mattc-eostar \r\nIt looks like you are using an older Version of Tensorflow 2.0. Many bugs have been fixed in the latest version. Could you please try to execute using Latest Version of TF 2.6.0 and let us know if the issue still persists? Thanks!", "The project related to this code has since been abandoned. I do not have the means currently to check if this is still an issue."]}, {"number": 34323, "title": "Remove clone with new values for saving distributed variables", "body": "This PR removes the unused `_clone_with_new_values` method of distributed variables and allows us to simplify `AutoCastDistributedVariable`.\r\n\r\n**Related Changes**\r\n- 6801a4b2fbc3cbe6bbfde2a82c7ed17d8997ed9f introduced `_clone_with_new_values` which calls `type(var)(var._distribute_strategy, var._device_map, ...)` in order to handle saving of distributed variables\r\n- Therefor c3300a21cc3c136741e3c020169ff904c58ca4bf needed to overwrite `AutoCastDistributedVariable.__init__` in order to handle this case\r\n- Since a3fd013fe9df291c6a461001a4570f5ab15102d1 distribute variables go through the resource variable code path for saving which and doesn't rely on `_clone_with_new_values` anymore\r\n- Thus `_clone_with_new_values` is not used anywhere else in the code base and is a private method so it can be safely removed\r\n- As far as I can tell the `type(var)(var._distribute_strategy, var._device_map, ...)` pattern is not used anywhere else in the code which allows us to cleanup `AutoCastDistributedVariable` if I am not missing something. This would allow us to further simplify `AutoCastVariable` in the future or allow us to get rid of the `create_autocast_variable` helper.\r\n\r\n\r\n/cc @guptapriya @reedwm @rchao ", "comments": []}, {"number": 34321, "title": "Colab, TPU training using keras  InternalError: Failed to serialize message", "body": "Hi,\r\n\r\nI am trying to test TPU on colab in order to see how that works on keras.\r\nI try to train 3 models. And differents bugs/abnormal behaviors occured : \r\n1) when I trained a simple model ( see \"create_model\" function in colab) it works well with an image size of (100,100,3) but it does not work with (200,200,3). The batch size is about 80 so I doubt it is about an OOM error. \r\n\r\n\r\n\r\n2)  same as 1) with vgg19/inceptionresnetv2 + issue if I use the parameters imagenet=\"weights\"\r\n\r\n\r\n3) using tensorflow 2.0 as backend does not seem to work even with image size equal to (100, 100, 3)\r\n\r\nThese issue seems abnormal and the message does not help to understand how to solve them. Anyone know how to solve them or why they occured ?\r\n\r\nLink to the code : https://colab.research.google.com/drive/1z40RZOqBexmniel8m9KdgSa7tSFaUUcG\r\n\r\n\r\nMessage error : \r\n1)\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n13 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n-> 1350                                       target_list, run_metadata)\r\n   1351 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1442                                             fetch_list, target_list,\r\n-> 1443                                             run_metadata)\r\n   1444 \r\n\r\nInternalError: Failed to serialize message\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-16-f1e92e5b4b78> in <module>()\r\n      2   X_train.astype(np.float32), Y_train.astype(np.float32),\r\n      3   batch_size = 10*8,\r\n----> 4   epochs=5)#, validation_data=(x_test, y_test))\r\n      5 \r\n      6   #validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    725         max_queue_size=max_queue_size,\r\n    726         workers=workers,\r\n--> 727         use_multiprocessing=use_multiprocessing)\r\n    728 \r\n    729   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    617         validation_split=validation_split,\r\n    618         shuffle=shuffle,\r\n--> 619         epochs=epochs)\r\n    620     if not dist_utils.is_distributing_by_cloning(model):\r\n    621       with model._distribution_strategy.scope():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2286 \r\n   2287         ds = strategy.extended.experimental_make_numpy_dataset(in_tuple,\r\n-> 2288                                                                session=session)\r\n   2289         if shuffle:\r\n   2290           # We want a buffer size that is larger than the batch size provided by\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py in experimental_make_numpy_dataset(self, numpy_input, session)\r\n   1684     \"\"\"\r\n   1685     _require_cross_replica_or_default_context_extended(self)\r\n-> 1686     return self._experimental_make_numpy_dataset(numpy_input, session=session)\r\n   1687 \r\n   1688   def _experimental_make_numpy_dataset(self, numpy_input, session):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py in _experimental_make_numpy_dataset(self, numpy_input, session)\r\n    247     return numpy_dataset.one_host_numpy_dataset(\r\n    248         numpy_input, numpy_dataset.SingleDevice(self._host_device),\r\n--> 249         session)\r\n    250 \r\n    251   def _experimental_distribute_dataset(self, dataset):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/numpy_dataset.py in one_host_numpy_dataset(numpy_input, colocate_with, session)\r\n     86                       for i in numpy_flat)\r\n     87   for v, i in zip(vars_flat, numpy_flat):\r\n---> 88     init_var_from_numpy(v, i, session)\r\n     89   vars_nested = nest.pack_sequence_as(numpy_input, vars_flat)\r\n     90   return dataset_ops.Dataset.from_tensor_slices(vars_nested)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/numpy_dataset.py in init_var_from_numpy(input_var, numpy_input, session)\r\n     70           start_placeholder: start,\r\n     71           end_placeholder: end,\r\n---> 72           slice_placeholder: numpy_input[start:end]})\r\n     73       start = end\r\n     74 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    954     try:\r\n    955       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 956                          run_metadata_ptr)\r\n    957       if run_metadata:\r\n    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1179       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1180                              feed_dict_tensor, options, run_metadata)\r\n   1181     else:\r\n   1182       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1357     if handle is None:\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n   1361       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nInternalError: Failed to serialize message\r\n```\r\n\r\n\r\n2)\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1364     try:\r\n-> 1365       return fn(*args)\r\n   1366     except errors.OpError as e:\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n-> 1350                                       target_list, run_metadata)\r\n   1351 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1442                                             fetch_list, target_list,\r\n-> 1443                                             run_metadata)\r\n   1444 \r\n\r\nNotFoundError: From /job:worker/replica:0/task:0:\r\n2 root error(s) found.\r\n  (0) Not found: Resource worker/block1_conv1/bias/replica_7/N10tensorflow3VarE does not exist.\r\n\t [[{{node TPUReplicateMetadata}}]]\r\n  (1) Not found: Resource worker/block1_conv1/bias/replica_3/N10tensorflow3VarE does not exist.\r\n\t [[{{node TPUReplicateMetadata}}]]\r\n0 successful operations.\r\n7 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-10-f1e92e5b4b78> in <module>()\r\n      2   X_train.astype(np.float32), Y_train.astype(np.float32),\r\n      3   batch_size = 10*8,\r\n----> 4   epochs=5)#, validation_data=(x_test, y_test))\r\n      5 \r\n      6   #validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    725         max_queue_size=max_queue_size,\r\n    726         workers=workers,\r\n--> 727         use_multiprocessing=use_multiprocessing)\r\n    728 \r\n    729   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    668             steps_per_epoch=steps_per_epoch,\r\n    669             validation_steps=validation_steps,\r\n--> 670             validation_freq=validation_freq)\r\n    671 \r\n    672     return training_arrays.fit_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py in experimental_tpu_fit_loop(model, dataset, epochs, verbose, callbacks, initial_epoch, steps_per_epoch, val_dataset, validation_steps, validation_freq)\r\n    241         prev_step_count = step_count\r\n    242       try:\r\n--> 243         _, outputs = K.batch_get_value([train_op, output_tensors])\r\n    244       except errors.OutOfRangeError:\r\n    245         logging.warning('Your dataset iterator ran out of data; '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py in batch_get_value(tensors)\r\n   3183     raise RuntimeError('Cannot get value inside Tensorflow graph function.')\r\n   3184   if tensors:\r\n-> 3185     return get_session(tensors).run(tensors)\r\n   3186   else:\r\n   3187     return []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    954     try:\r\n    955       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 956                          run_metadata_ptr)\r\n    957       if run_metadata:\r\n    958         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1178     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1179       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1180                              feed_dict_tensor, options, run_metadata)\r\n   1181     else:\r\n   1182       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1357     if handle is None:\r\n   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1359                            run_metadata)\r\n   1360     else:\r\n   1361       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py in _do_call(self, fn, *args)\r\n   1382                     '\\nsession_config.graph_options.rewrite_options.'\r\n   1383                     'disable_meta_optimizer = True')\r\n-> 1384       raise type(e)(node_def, op, message)\r\n   1385 \r\n   1386   def _extend_graph(self):\r\n\r\nNotFoundError: From /job:worker/replica:0/task:0:\r\n2 root error(s) found.\r\n  (0) Not found: Resource worker/block1_conv1/bias/replica_7/N10tensorflow3VarE does not exist.\r\n\t [[node TPUReplicateMetadata (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n  (1) Not found: Resource worker/block1_conv1/bias/replica_3/N10tensorflow3VarE does not exist.\r\n\t [[node TPUReplicateMetadata (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\r\n0 successful operations.\r\n7 derived errors ignored.\r\n\r\nOriginal stack trace for 'TPUReplicateMetadata':\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\r\n    app.start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 832, in start\r\n    self._run_callback(self._callbacks.popleft())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\r\n    ret = callback()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\r\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\r\n    self._handle_recv()\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-10-f1e92e5b4b78>\", line 4, in <module>\r\n    epochs=5)#, validation_data=(x_test, y_test))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 727, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 670, in fit\r\n    validation_freq=validation_freq)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 196, in experimental_tpu_fit_loop\r\n    initial_loop_values=initial_loop_values)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1756, in experimental_run_steps_on_iterator\r\n    initial_loop_values)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py\", line 334, in _experimental_run_steps_on_iterator\r\n    replicate_outputs = rewrite_fn()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py\", line 315, in rewrite_fn\r\n    run_fn, replicate_inputs, device_assignment=self._device_assignment)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu.py\", line 639, in replicate\r\n    maximum_shapes=maximum_shapes)[1]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tpu/tpu.py\", line 927, in split_compile_and_replicate\r\n    num_replicas=num_replicas, use_tpu=use_tpu, **metadata_kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_tpu_ops.py\", line 6105, in tpu_replicate_metadata\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\r\n    attrs, op_def, compute_device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n\r\n\r\n3)\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-10-f1e92e5b4b78> in <module>()\r\n      2   X_train.astype(np.float32), Y_train.astype(np.float32),\r\n      3   batch_size = 10*8,\r\n----> 4   epochs=5)#, validation_data=(x_test, y_test))\r\n      5 \r\n      6   #validation_data=(x_test.astype(np.float32), y_test.astype(np.float32)))\r\n\r\n13 frames\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    726         max_queue_size=max_queue_size,\r\n    727         workers=workers,\r\n--> 728         use_multiprocessing=use_multiprocessing)\r\n    729 \r\n    730   def evaluate(self,\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    683         validation_steps=validation_steps,\r\n    684         validation_freq=validation_freq,\r\n--> 685         steps_name='steps_per_epoch')\r\n    686 \r\n    687   def evaluate(self,\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    142       steps_per_epoch = training_utils.infer_steps_for_dataset(\r\n    143           inputs, steps_per_epoch, epochs=epochs, steps_name=steps_name)\r\n--> 144     input_iterator = _get_iterator(inputs, model._distribution_strategy)\r\n    145 \r\n    146   # Enter tf.distribute.Strategy scope.\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_arrays.py in _get_iterator(inputs, distribution_strategy)\r\n    547   if distribution_strategy:\r\n    548     return distributed_training_utils.get_iterator(\r\n--> 549         inputs, distribution_strategy)\r\n    550   return training_utils.get_iterator(inputs)\r\n    551 \r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/distribute/distributed_training_utils.py in get_iterator(dataset, distribution_strategy)\r\n    585 def get_iterator(dataset, distribution_strategy):\r\n    586   with distribution_strategy.scope():\r\n--> 587     iterator = distribution_strategy.make_dataset_iterator(dataset)\r\n    588   initialize_iterator(iterator, distribution_strategy)\r\n    589   return iterator\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/distribute_lib.py in make_dataset_iterator(self, dataset)\r\n    559   def make_dataset_iterator(self, dataset):\r\n    560     \"\"\"DEPRECATED TF 1.x ONLY.\"\"\"\r\n--> 561     return self._extended._make_dataset_iterator(dataset)  # pylint: disable=protected-access\r\n    562 \r\n    563   @doc_controls.do_not_generate_docs  # DEPRECATED: TF 1.x only\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/tpu_strategy.py in _make_dataset_iterator(self, dataset)\r\n    225         self._input_workers,\r\n    226         self._container_strategy(),\r\n--> 227         split_batch_by=self._num_replicas_in_sync)\r\n    228 \r\n    229   def _make_input_fn_iterator(\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, input_workers, strategy, split_batch_by, input_context)\r\n    760         strategy,\r\n    761         split_batch_by=split_batch_by,\r\n--> 762         input_context=input_context)\r\n    763     worker_iterators = _create_iterators_per_worker(\r\n    764         dist_dataset._cloned_datasets, input_workers)  # pylint: disable=protected-access\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, input_workers, strategy, split_batch_by, input_context)\r\n    556         strategy,\r\n    557         split_batch_by=split_batch_by,\r\n--> 558         input_context=input_context)\r\n    559 \r\n    560   def make_one_shot_iterator(self):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_lib.py in __init__(self, dataset, input_workers, strategy, split_batch_by, input_context)\r\n    518           # TODO(b/129506833): Figure out between graph cases\r\n    519           cloned_dataset = input_ops.auto_shard_dataset(  # pylint: disable=protected-access\r\n--> 520               cloned_dataset, len(input_workers.worker_devices), i)\r\n    521           self._cloned_datasets.append(cloned_dataset)\r\n    522 \r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/distribute/input_ops.py in auto_shard_dataset(dataset, num_shards, index)\r\n     47       return distribute._AutoShardDatasetV1(dataset, num_shards, index)\r\n     48     else:\r\n---> 49       return distribute._AutoShardDataset(dataset, num_shards, index)\r\n     50   else:\r\n     51     return dataset\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/data/experimental/ops/distribute.py in __init__(self, input_dataset, num_workers, index)\r\n     54         num_workers=num_workers,\r\n     55         index=index,\r\n---> 56         **self._flat_structure)\r\n     57     super(_AutoShardDataset, self).__init__(input_dataset, variant_tensor)\r\n     58 \r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/ops/gen_experimental_dataset_ops.py in auto_shard_dataset(input_dataset, num_workers, index, output_types, output_shapes, name)\r\n    169       else:\r\n    170         message = e.message\r\n--> 171       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n    172   # Add nodes to the TensorFlow graph.\r\n    173   if not isinstance(output_types, (list, tuple)):\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:worker/replica:0/task:0/device:CPU:0 in order to run AutoShardDataset: Unable to parse tensor proto\r\nAdditional GRPC error information:\r\n{\"created\":\"@1573841168.053370969\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Unable to parse tensor proto\",\"grpc_status\":3} [Op:AutoShardDataset]\r\n```", "comments": ["HI, JUST CHECK THIS\r\nhttps://github.com/tensorflow/tensorflow/issues/29896", "HOPE IT HELPS", "Hi @developer22-university , \r\nthank for your reply. In factm that helps to answer my second question about the pretrained weights from imagenet.", "I'm getting the same InternalError, were you able to solve your problem @ShiroKL ? ", "get same error  'Failed to serialize message'\r\n\r\nuse small dataset works well, use large dataset get error.\r\n\r\n", "@algoromeo  Sorry for the late response.\r\nI have still the issue, as @ofpppppppdbfjs  said, we need to reduce the amount of data to feed your model. I have no idea why this issue occurs though. ", "The issue exist with TF 2.2 version. [GitHub gist](https://colab.research.google.com/gist/ymodak/4c500019b72b992568a5eb7ff36f111a/copy-of-tensorflow-2-0-handbook-tpu-example.ipynb)\r\nPotential duplicate #35785", "Reassigning to current TensorFlow TPU on-call @lzr-google ", "One thing about TF2 TPU training is TPU initialization has to happen first before any dataset/model creation https://www.tensorflow.org/guide/tpu#tpu_initialization, I noticed in this [GitHub example](https://colab.sandbox.google.com/gist/ymodak/4c500019b72b992568a5eb7ff36f111a/copy-of-tensorflow-2-0-handbook-tpu-example.ipynb#scrollTo=bjps3-L_55qt) the TPU initialization happen much later, can you move it to the beginning of your code?", "@Shiro-LK \r\nIs this still an issue, Can you please update as per above comment. ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi, I have a new issue :\r\n\r\n```\r\nInternalError: Assigned device '/job:worker/replica:0/task:0/device:TPU:0' does not have registered OpKernel support for _Arg\r\n\t [[{{node iterator_2}}]] [Op:__inference_train_function_3873]\r\n```\r\n\r\nI do not know what it means. Is there something wrong with the code ?", "Shiro, can you share your colab? Did you follow the instructions in https://www.tensorflow.org/guide/tpu#tpu_initialization to run TPU initialization at the beginning?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34321\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34321\">No</a>\n"]}, {"number": 34320, "title": "[ROCm] Update ROCm CI builds to use ROCm 2.8", "body": "This PR/commit updates the Dockerfile.rocm file to use ROCm version 2.8 (from the current 2.6).\r\n\r\nSwitching to ROCm version 2.8, also adds to the requirement of specifying a couple of extra option to the `docker run` command.\r\nThat change is also a part of this PR/commit.\r\n\r\n-----------------------\r\n\r\n/cc @whchung @sunway513 @chsigg ", "comments": ["@chsigg  gentle ping"]}, {"number": 34319, "title": "Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa9a442b440> could not be transformed and will be executed ", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution: Ubuntu 18.06\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.4\r\n-using spyder3 ide in anaconda\r\n- CUDA/cuDNN version: None\r\n\r\n\r\n**Describe the current behavior**\r\ngetting this error and the training accuracy doesnt improve\r\n\r\n\r\nWARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa9a442b440> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\nWARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7fa9a442b440> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \r\n**Describe the expected behavior**\r\nwell, increasing ccuracy and no error basically\r\n**Code to reproduce the issue**\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\ny_train = np.random.rand(1)\r\nepochs = 1\r\nx_train = np.random.rand(1,20,16)\r\n\r\nmodel = Sequential()\r\nmodel.add(layers.LSTM(activation='sigmoid',units = 3,batch_input_shape = (1,20,16)))\r\nmodel.add(layers.Dense(2, activation='softmax'))\r\n\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\nmodel.summary()\r\nhistory = model.fit(x_train, y_train, batch_size=1, epochs=epochs)\r\n\r\n\r\n\r\nthis reproduces the error in my usecase\r\n\r\n\r\nedit: using the solution proposed in\r\nhttps://github.com/tensorflow/tensorflow/issues/32377\r\n\r\ndid not work for me ( Requirement already satisfied: gast==0.2.2 )\r\n\r\n", "comments": ["@PedroDKE \r\n\r\nI tried reproducing the issue with TF 2.0 using colab. However i am not seeing any error message. Please, help me in reproducing the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d03583e7c0604f9413ab84cd7ccf64f7/untitled378.ipynb). Thanks!", "wasn't able to reproduce it there either, i tried to reinstall tensorflow but it didtn help either.\r\nhere is the error with verbosity set to 10:\r\n\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7f05a833dcb0>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x7f05a833dcb0>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7f062b289dd0>\r\n    args: (<tf.Tensor 'args_0:0' shape=(1,) dtype=int64>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7f062b289dd0>: DoNotConvert rule for tensorflow\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7f050ac61200>\r\n    args: (<tf.Tensor 'args_0:0' shape=(1,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(1, 20, 16) dtype=float64>, <tf.Tensor 'args_2:0' shape=(1, 1) dtype=float64>))\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Whitelisted: <function TensorLikeDataAdapter.__init__.<locals>.grab_batch at 0x7f050ac61200>: DoNotConvert rule for tensorflow\r\n\r\nthe error occurs when i try to fit the model", "@PedroDKE I tried my laptop (local) and colab and could not reproduce the issue. Can you please share a gist which is throwing the error. Thanks!\r\n\r\nPlease close the issue if this was already resolved. Thanks!", "I think this issue was resolved as I could not reproduce the issue in my local as well as colab. I am closing this issue but please feel free to reopen if the issue persists again. Thanks!", "Hi, I have the same problem.", "Hi, I have the same problem.\r\n\r\n"]}, {"number": 34318, "title": "ModuleNotFoundError: No module named 'tensorflow'", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow-gpu installed from pip :\r\n- TensorFlow-gpu version: 2.0.0\r\n\r\n- Python version:3.6.8 \r\n- Installed using pip\r\n- CUDA/cuDNN version:10\r\n- GPU model and memory: 1050 mobile\r\n\r\nI followed the installation tutorial for tensorflow GPU, specified in\r\nhttps://www.tensorflow.org/install/gpu\r\n\r\nThe command \"pip3 show tensorflow-gpu \" has the following output : \r\n ```bash\r\nName: tensorflow-gpu\r\nVersion: 2.0.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /home/niccle27/.local/lib/python3.6/site-packages\r\nRequires: tensorflow-estimator, keras-preprocessing, astor, tensorboard, keras-applications, absl-py, six, protobuf, numpy, termcolor, gast, google-pasta, opt-einsum, grpcio, wheel, wrapt\r\nRequired-by: \r\n```\r\n\r\ni made sure that cuda was working properly and it does.\r\n\r\nwhen importing the tensorflow package, i'm getting the following error :\r\n \r\n ```python\r\nimport tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n", "comments": ["Please post the **full** output of (eventually in a virtualenv)\r\n\r\n```\r\npython -m pip install tensorflow && python -c \"import tensorflow\"\r\n```", "Well that command would work. i have no problem installing and running the CPU version of tensorflow. My problem only concerns the GPU version. \r\n\r\nHere is your command output in my system : \r\n```bash\r\npython -m pip install tensorflow-gpu && python -c \"import tensorflow\"\r\nRequirement already satisfied: tensorflow-gpu in ./.local/lib/python3.6/site-packages (2.0.0)\r\nRequirement already satisfied: numpy<2.0,>=1.16.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.17.1)\r\nRequirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (3.1.0)\r\nRequirement already satisfied: six>=1.10.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.12.0)\r\nRequirement already satisfied: keras-preprocessing>=1.0.5 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement already satisfied: google-pasta>=0.1.6 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (0.1.8)\r\nRequirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (2.0.1)\r\nRequirement already satisfied: protobuf>=3.6.1 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (3.10.0)\r\nRequirement already satisfied: keras-applications>=1.0.8 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.0.8)\r\nRequirement already satisfied: grpcio>=1.8.6 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.24.3)\r\nRequirement already satisfied: gast==0.2.2 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (0.2.2)\r\nRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow-gpu) (0.30.0)\r\nRequirement already satisfied: absl-py>=0.7.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (0.8.1)\r\nRequirement already satisfied: astor>=0.6.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (0.8.0)\r\nRequirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.1.0)\r\nRequirement already satisfied: tensorboard<2.1.0,>=2.0.0 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (2.0.1)\r\nRequirement already satisfied: wrapt>=1.11.1 in ./.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.11.2)\r\nRequirement already satisfied: setuptools in ./.local/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.2.0)\r\nRequirement already satisfied: h5py in ./.local/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./.local/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in ./.local/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.6.3)\r\nRequirement already satisfied: werkzeug>=0.11.15 in ./.local/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\r\nRequirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.2.0)\r\nRequirement already satisfied: cachetools>=2.0.0 in ./.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\r\nRequirement already satisfied: rsa>=3.1.4 in ./.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.2.7)\r\nRequirement already satisfied: requests>=2.0.0 in ./.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.22.0)\r\nRequirement already satisfied: oauthlib>=3.0.0 in ./.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.0)\r\nRequirement already satisfied: pyasn1>=0.1.3 in ./.local/lib/python3.6/site-packages (from rsa>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.7)\r\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in ./.local/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.25.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2019.6.16)\r\nRequirement already satisfied: idna<2.9,>=2.5 in ./.local/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.8)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n", "Can you try\r\n\r\n```\r\npython -m pip uninstall tensorflow tensorflow-gpu && python -m pip install tensorflow-gpu && python -c \"import tensorflow\"\r\n```\r\n\r\nIt's possible the install have left bad files on your disk, so it's better to start fresh.", "Well, i tried at least 3 times to remove everything and restart from a clean environnement. Though it never worked. This time your command worked with a little trick (add --user to install otherwise i don't have the right to launch the command properly) : \r\n\r\n```bash\r\npython -m pip uninstall tensorflow tensorflow-gpu && python -m pip install --user tensorflow-gpu && python -c \"import tensorflow\"\r\n```\r\nThank you a lot !!", "This is interesting. Probably because there is not enough space left on superroot disk. Or probably because some files need normal permissions not root.\r\n\r\nIn any case, the following trick should always work:\r\n\r\n```\r\n$ python -m virtualenv .\r\n$ source bin/activate\r\n$ python -m pip install ...\r\n```\r\n\r\nAs everything would get installed relative to current directory.\r\n\r\nSince this is working now, I'm going ahead and closing this issue."]}, {"number": 34317, "title": "Parallelize input pipeline with TensorFlow2.0", "body": "We have already implemented distributed training with `MultiWorkerMirroredStrategy` and `ParameterServerStrategy` (multi-worker on multiple machines). The problem is that every script from every worker extracts the same data set which is then sharded among the workers but we would like to directly pass to each worker only the part of data to be processed. Is it possible? \r\nMaybe we should execute different data queries from every machine or one of the workers should have this task and then pass the proper subsets to other workers?\r\n", "comments": ["Apologies for the delay in response.\r\nPerhaps ```tf.data.Dataset.interleave``` from ```tf.data``` API can be used in this case to perform data extraction in parallel.\r\nSee https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave", "Thanks for the reply! At first it seemed to me that `interleave` served to parallelize data processing so each `worker` would still have to query the data from the remote storage but I will definitely deepen the documentation.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Will close this issue for now. Feel free to reopen if have further questions. Thanks!"]}, {"number": 34316, "title": "tf.function issue after replacing the optimiser", "body": "**System information**\r\n- Have I written custom code: no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro linux testing\r\n- TensorFlow installed from (source or binary): pypi binary\r\n- TensorFlow version (use command below): v1.12.1-16854-g6778662 2.1.0-dev20191028\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.1.243 / 7.6.4.38\r\n- GPU model and memory: GeForce GTX 1080, 8GB\r\n\r\n**Describe the current behavior**\r\nI am training a model using multiple optimisers and am getting an error when the train function is decorated with @tf.function.\r\n\r\n**Describe the expected behavior**\r\nI expected the converted function to be invariant of the optimiser instance.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n\r\n        self.x = self.add_weight(shape=[1,], dtype=tf.float32)\r\n\r\n    def call(self, inputs):\r\n        return inputs * self.x\r\n\r\nmodel = MyModel()\r\noptimiser = tf.keras.optimizers.Adam(0.1)\r\n\r\n@tf.function\r\ndef train():\r\n    with tf.GradientTape() as tape:\r\n        output = model([1.0])\r\n        loss = output\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimiser.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n\r\ntrain()\r\nprint('First call ok')\r\noptimiser = tf.keras.optimizers.Adam(0.2)\r\ntrain()\r\nprint('Second call ok')\r\n```\r\n\r\n**Other info / logs**\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError:  Error while reading resource variable Adam/beta_2_33 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/Adam/beta_2_33/N10tensorflow3VarE does not exist.\r\n\t [[node Adam/Cast_3/ReadVariableOp]] [Op:__inference_train_135]\r\n```\r\n", "comments": ["Could reproduce the error with TF Nightly Version. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/786d873d44f8e135ac00802e2f16ff69/34316.ipynb). Thanks!", "Hi, unfortunately, when @tf.function is traced (the first call), the first optimizer is captured and will still try to use that in the subsequent calls.  Another way to get around is defining tf.function for each.\r\n\r\n```python\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n\r\n        self.x = self.add_weight(shape=[1,], dtype=tf.float32)\r\n\r\n    def call(self, inputs):\r\n        return inputs * self.x\r\n\r\nmodel = MyModel()\r\noptimizer = tf.keras.optimizers.Adam(0.1)\r\n\r\ndef train():\r\n    with tf.GradientTape() as tape:\r\n        output = model([1.0])\r\n        loss = output\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n\r\ntf.function(train)()\r\nprint('First call ok')\r\noptimizer = tf.keras.optimizers.Adam(0.2)\r\ntf.function(train)()\r\nprint('Second call ok')\r\n```", "I understand, thanks.\r\n\r\nIs there a better way of manually changing the learning rate of an optimiser (and optionally resetting the moments) than creating a new instance ? I see a `_set_hyper` method, but looks protected.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34316\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34316\">No</a>\n", "Yes you can also dynamically set it. Here is the code https://github.com/tensorflow/tensorflow/blob/bf9c196f37b9cbb3109b2891aaf9da85bf5f712a/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L556\r\n\r\n```python\r\noptimizer = tf.keras.optimizers.Adam(0.2)\r\noptimizer.lr = 0.5\r\noptimizer.get_config()\r\n#{'name': 'Adam',\r\n# 'learning_rate': 0.5,\r\n# 'decay': 0.0,\r\n# 'beta_1': 0.9,\r\n# 'beta_2': 0.999,\r\n# 'epsilon': 1e-07,\r\n# 'amsgrad': False}\r\n```"]}, {"number": 34315, "title": "Unable to use new TensorFlow Lite Android Support Library for object detection", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 3XL\r\n- TensorFlow installed from (source or binary): \r\n- TensorFlow version (use command below):\r\n      From build.gradle\r\n      api 'org.tensorflow:tensorflow-lite:2.0.0'\r\n      api 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\n      api 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n\r\n**Describe the current behavior**\r\nI'm attempting to modify the current TFLiteObjectDetectionAPIModel.java from [the android object detection example](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java) to use the experimental [TensorFlow Lite Android Support Library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java)\r\n\r\nI seem to be loading the input and output tensor buffers and running `tflite.runForMultipleInputsOutputs` just fine. Getting a Map of labeled probabilities crashes with the error:` java.lang.IllegalArgumentException: Label number 97 mismatch the shape on axis 1`. I am using a custom quantized model and custom label file with 97 classes. I am certain that this model works, as it runs perfectly fine with the original example (with modifications to sizes of course).\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect to get an array of labeled probabilities, but the app crashes instead.\r\n\r\n**Code to reproduce the issue**\r\nHere's how I set up input/output tensors:\r\n\r\n        // Setup output tensors\r\n        int outputTensorIndex = 0;\r\n        int[] outputLocationShape =\r\n                tflite.getOutputTensor(outputTensorIndex).shape();\r\n        DataType outputLocationType = tflite.getOutputTensor(outputTensorIndex).dataType();\r\n        outputTensorIndex++;\r\n        int[] outputClassShape =\r\n                tflite.getOutputTensor(outputTensorIndex).shape();\r\n        DataType outputClassType = tflite.getOutputTensor(outputTensorIndex).dataType();\r\n        outputTensorIndex++;\r\n        int[] outputScoreShape =\r\n                tflite.getOutputTensor(outputTensorIndex).shape();\r\n        DataType outputScoreType = tflite.getOutputTensor(outputTensorIndex).dataType();\r\n        outputTensorIndex++;\r\n        int[] numDetectionShape =\r\n                tflite.getOutputTensor(outputTensorIndex).shape();\r\n        DataType numDetectionType= tflite.getOutputTensor(outputTensorIndex).dataType();\r\n        \r\n       // Setup input tensor\r\n        int imageTensorIndex = 0;\r\n        int[] imageShape = tflite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}\r\n        imageSizeY = imageShape[1];\r\n        imageSizeX = imageShape[2];\r\n        DataType imageDataType = tflite.getInputTensor(imageTensorIndex).dataType();\r\n\r\n        // Creates the input tensor.\r\n        inputImageBuffer = new TensorImage(imageDataType);\r\n\r\n        // Creates the output tensors and its processors.\r\n        outputLocationBuffer = TensorBuffer.createFixedSize(outputLocationShape, outputLocationType);\r\n        outputClassBuffer = TensorBuffer.createFixedSize(outputClassShape, outputClassType);\r\n        outputScoreBuffer = TensorBuffer.createFixedSize(outputScoreShape, outputScoreType);\r\n        numDetectionBuffer = TensorBuffer.createFixedSize(numDetectionShape, numDetectionType);\r\n\r\n        // Creates the post processor for the output probability.\r\n        outputLocationProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();\r\n        outputClassProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();\r\n        outputScoreProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();\r\n        numDetectionProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();\r\n\r\n        LOGGER.d(\"Created a Tensorflow Lite Classifier.\");\r\n    }\r\n\r\nAnd here's how I'm loading images and attempting to get a labeled probability map\r\n\r\n        Trace.beginSection(\"loadImage\");\r\n        long startTimeForLoadImage = SystemClock.uptimeMillis();\r\n        inputImageBuffer = loadImage(bitmap, sensorOrientation);\r\n        long endTimeForLoadImage = SystemClock.uptimeMillis();\r\n        Trace.endSection();\r\n        LOGGER.v(\"Timecost to load the image: \" + (endTimeForLoadImage - startTimeForLoadImage));\r\n\r\n        // Runs the inference call.\r\n        Trace.beginSection(\"runInference\");\r\n        long startTimeForReference = SystemClock.uptimeMillis();\r\n\r\n        Object[] inputArray = {inputImageBuffer.getBuffer().rewind()};\r\n        Map<Integer, Object> outputMap = new HashMap<>();\r\n        outputMap.put(0, outputLocationBuffer.getBuffer().rewind());\r\n        outputMap.put(1, outputClassBuffer.getBuffer().rewind());\r\n        outputMap.put(2, outputScoreBuffer.getBuffer().rewind());\r\n        outputMap.put(3, numDetectionBuffer.getBuffer().rewind());\r\n        tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n        long endTimeForReference = SystemClock.uptimeMillis();\r\n        Trace.endSection();\r\n        LOGGER.v(\"Timecost to run model inference: \" + (endTimeForReference - startTimeForReference));\r\n\r\n        Map<String, Float> labeledProbability =\r\n            new TensorLabel(labels, outputClassProcessor.process(outputClassBuffer))\r\n                .getMapWithFloatValue();\r\n        Trace.endSection();\r\n        // Gets top-k results.\r\n        return getTopKProbability(labeledProbability);\r\n\r\n\r\n**Other info / logs**\r\n`java.lang.RuntimeException: Unable to start activity ComponentInfo{com.example}: java.lang.IllegalArgumentException: Label number 97 mismatch the shape on axis 1`\r\n\r\nfor the line \r\n\r\n`Map<String, Float> labeledProbability =\r\n            new TensorLabel(labels, outputClassProcessor.process(outputClassBuffer))\r\n                .getMapWithFloatValue();`\r\n", "comments": ["Also tested with a clean version of the android example object detection app and ran into the same issue using the stock detect.tflite and label file.\r\n\r\n`2019-11-15 12:12:19.265 22400-22458/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 22400\r\n    java.lang.IllegalArgumentException: Label number 0 mismatch the shape on axis 1\r\n        at org.tensorflow.lite.support.common.SupportPrecondtions.checkArgument(SupportPrecondtions.java:107)\r\n        at org.tensorflow.lite.support.label.TensorLabel.<init>(TensorLabel.java:86)\r\n        at org.tensorflow.lite.support.label.TensorLabel.<init>(TensorLabel.java:104)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImageNewAPI(TFLiteObjectDetectionAPIModel.java:394)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:191)\r\n        at android.os.Handler.handleCallback(Handler.java:883)\r\n        at android.os.Handler.dispatchMessage(Handler.java:100)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)`\r\n\r\n\r\nWhere recognizeImageNewAPI is: \r\n\r\n\r\n    long startTimeForLoadImage = SystemClock.uptimeMillis();\r\n    inputImageBuffer = loadImage(bitmap, sensorOrientation);\r\n    long endTimeForLoadImage = SystemClock.uptimeMillis();\r\n    Trace.endSection();\r\n    LOGGER.v(\"Timecost to load the image: \" + (endTimeForLoadImage - startTimeForLoadImage));\r\n\r\n    // Runs the inference call.\r\n    Trace.beginSection(\"runInference\");\r\n    long startTimeForReference = SystemClock.uptimeMillis();\r\n\r\n    Object[] inputArray = {inputImageBuffer.getBuffer().rewind()};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, outputLocationBuffer.getBuffer().rewind());\r\n    outputMap.put(1, outputClassBuffer.getBuffer().rewind());\r\n    outputMap.put(2, outputScoreBuffer.getBuffer().rewind());\r\n    outputMap.put(3, numDetectionBuffer.getBuffer().rewind());\r\n    tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    long endTimeForReference = SystemClock.uptimeMillis();\r\n    Trace.endSection();\r\n    LOGGER.v(\"Timecost to run model inference: \" + (endTimeForReference - startTimeForReference));\r\n\r\n    Map<String, Float> labeledProbability =\r\n            new TensorLabel(labels, outputClassProcessor.process(outputClassBuffer))\r\n                    .getMapWithFloatValue();\r\n    Trace.endSection();\r\n    // Gets top-k results.\r\n    return getTopKProbability(labeledProbability);\r\n    }\r\n", "Hi Michelle,\r\n\r\nThanks for experimenting TFLite Support Library! \r\n\r\nUnfortunately, TensorLabel does not work in this specific use case for Object Detection.\r\n\r\nTensorLabel is used to attach axis labels to a tensor. For example, in TFLite's image classification reference app, the output is a tensor with shape [1, 1001], where each element in the tensor is a probability of the corresponding category. Therefore, by using TensorLabel, the original TensorBuffer is annotated by the label file.\r\n\r\nHowever, in the object detection reference app, the output tensor, outputClass, contains the indexes of detected categories, meaning each element is the index pointing to the labels. We need to convert the index into string, and it is unsupported yet (but will be coming soon.).\r\n\r\nWe are extending the coverage of TFLite Support Library. More reference app examples using TFLite Support are coming soon, including the Object Detection one.\r\n\r\nI'll close the issue for now. Feel free to reopen it if you have further questions.\r\n\r\nThanks,\r\nLu", "how to solve this\r\n", "@lu-wang-g @xunkai55 Any updates on that?   \r\n\r\nI am using Teachable Machine to export a Tensorflow Lite model and after building and running the app on my Android I am getting the same error. \r\n\r\nThe example I am building on is the github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\n\r\nI strictly followed the guide in Teachable Machine and changed nothing else. \r\n\r\nPlease give some feedback on how to resolve this. Thank you for your time!\r\n\r\n![Screenshot_2020-01-31-18-25-58-010_com miui bugreport](https://user-images.githubusercontent.com/24626324/73556220-a9cf4c80-4457-11ea-88e0-197ba9f2c9d9.jpg)\r\n", "Has anyone found a work around for this ?", "> @lu-wang-g @xunkai55 Any updates on that?\r\n> \r\n> I am using Teachable Machine to export a Tensorflow Lite model and after building and running the app on my Android I am getting the same error.\r\n> \r\n> The example I am building on is the github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android\r\n> \r\n> I strictly followed the guide in Teachable Machine and changed nothing else.\r\n> \r\n> Please give some feedback on how to resolve this. Thank you for your time!\r\n> \r\n> ![Screenshot_2020-01-31-18-25-58-010_com miui bugreport](https://user-images.githubusercontent.com/24626324/73556220-a9cf4c80-4457-11ea-88e0-197ba9f2c9d9.jpg)\r\n\r\nIt looks like that the number of labels in your label file provided is 2, which doesn't match what the model outputs. Could you please double check that the model's output shape matches the number of your labels?", "> Has anyone found a work around for this ?\r\n\r\nCould you please tell us more about your usage? Thanks.", "I had the wrong file in my classification class like 56 thats why i was getting that error when i changed it, it began to work\n\nSent from my Samsung Galaxy S8 - powered by Three\nGet Outlook for Android<https://aka.ms/ghei36>\n\n________________________________\nFrom: Zhang Xunkai <notifications@github.com>\nSent: Monday, February 10, 2020 10:32:50 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Chris Kavanagh <c16492454@mydit.ie>; Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Unable to use new TensorFlow Lite Android Support Library for object detection (#34315)\n\n\nHas anyone found a work around for this ?\n\nCould you please tell us more about your usage? Thanks.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/34315?email_source=notifications&email_token=AKMFQQR4QATFAHBZX5G6NY3RCEUNFA5CNFSM4JN5MPG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELIACQA#issuecomment-584057152>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKMFQQR6TFKV3VRFEC2227LRCEUNFANCNFSM4JN5MPGQ>.\n", "> I had the wrong file in my classification class like 56 thats why i was getting that error when i changed it, it began to work Sent from my Samsung Galaxy S8 - powered by Three Get Outlook for Android<https://aka.ms/ghei36>\r\n\r\nCould you please give more detail about the solution?\r\n\r\n", "> I had the wrong file in my classification class like 56 thats why i was getting that error when i changed it, it began to work Sent from my Samsung Galaxy S8 - powered by Three Get Outlook for Android<https://aka.ms/ghei36>\r\n> [\u2026](#)\r\n> ________________________________ From: Zhang Xunkai <notifications@github.com> Sent: Monday, February 10, 2020 10:32:50 AM To: tensorflow/tensorflow <tensorflow@noreply.github.com> Cc: Chris Kavanagh <c16492454@mydit.ie>; Comment <comment@noreply.github.com> Subject: Re: [tensorflow/tensorflow] Unable to use new TensorFlow Lite Android Support Library for object detection (#34315) Has anyone found a work around for this ? Could you please tell us more about your usage? Thanks. \u2014 You are receiving this because you commented. Reply to this email directly, view it on GitHub<#34315?email_source=notifications&email_token=AKMFQQR4QATFAHBZX5G6NY3RCEUNFA5CNFSM4JN5MPG2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOELIACQA#issuecomment-584057152>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AKMFQQR6TFKV3VRFEC2227LRCEUNFANCNFSM4JN5MPGQ>.\r\n\r\nCould you please give more info?\r\n"]}, {"number": 34314, "title": "Error when importing tensorflow", "body": "\r\nhello github\r\ni have been having issues importing tensorflow\r\nfirst i installed the tensorflow==1.31 and i got an error attached below while trying to import tensorflow.\r\nsomeone suggested i use conda install tensorflow which i did and when trying to import tensorflow my python stops working and my kernel dies.\r\ni was also instructed to upgrade numpy using pip install numpy --upgrade. which i also did. but on trying to import tensorflow again my computer sends a notification of python stops working and kernel dies and restarts again.\r\nmy computer is 4g Ram\r\nplease your suggestion will be needed. i am stuck", "comments": ["Seems like a pipy distribution problem. I cannot get 2.0.0 either.", "Also seemed to only affect me running python 3.8. I reverted to 3.7 and it found it fine.", "How do I revert my python to 2.7 and I don't understand your first comment please can you explain more", "@chisom812 Please following the template [here](https://github.com/tensorflow/tensorflow/issues/new/choose)  as the current information is too vague for us to help you. \r\n\r\nAlso I found out that when you have multiple notebooks with Tensorflow running, this problem occurs. Also, simply closing the unused notebook windows wont work, they're still running in the background, you'll have to **'Shutdown'** the notebooks\r\n\r\nHere are the steps to shutdown a notebooks:\r\n\r\n     Go to Home (of Jupyter notebook) \r\n     Select 'Running' tab \r\n     Select the unused notebooks \r\n     Click 'Shutdown' button\r\n\r\nYou will notice in the Jupyter Home that the active notebook icon is green while inactive ones are gray Thanks!", "@chisom812 Can you please respond to the question. Did shutting down kernels work? Thanks!", "Closing this issue as it has been inactive for more than 3 weeks. Please add additional  comments and we can open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34314\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34314\">No</a>\n"]}, {"number": 34313, "title": "A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x70 in tid 28007", "body": "There was a problem when calling converted custom model (LSTM.tflite) in Android Studio.\r\nwith\r\n```Python\r\nconverter.experimental_new_converter = True\r\n```\r\n\r\n**System information**\r\nmodel training machine\r\n- Ubuntu 16.04 (LTS)\r\n- GTX1080\r\n- tensorflow 1.13.1\r\n- Keras 2.2.4\r\n- Python 3.6.8\r\n\r\nmodel converting & loading machine\r\n\r\n- MacOS Mojave 10.14.5\r\n- Python 3.7.1\r\n- tensorflow 2.0.0, tf-nightly 2.1.0.dev20191113\r\n- Android Studio 3.4.1\r\n- targetSdkVersion 28\r\n\r\nSmartphone\r\n\r\n- Android 8.0.0\r\n- SONY SO-04J (docomo xperia)\r\n\r\nI can convert custom LSTM model from .h5 to .tflite with following code:\r\n\r\n```Python\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.load_model(\"LSTM.h5\")\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\nopen(\"LSTM.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\nNext I want to load this model (LSTM.tflite) into my Android APK.\r\nI already have class that can perform inference using 1D-CNN custom model (1DCNN.tflite).\r\n\r\n```Java\r\npackage iis.kmjlab.kazuimotn.sartips.others;\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport org.tensorflow.lite.Interpreter;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\n\r\npublic class TensorFlowLiteClassifier {\r\n\r\n    private Interpreter interpreter;\r\n    public static final String MODEL_FILE = \"1DCNN.tflite\";\r\n    public static final int LABEL_NUM = 2;\r\n\r\n    /**\r\n     * Registers interpreter\r\n     */\r\n    private TensorFlowLiteClassifier(Interpreter interpreter) {\r\n        this.interpreter = interpreter;\r\n    }\r\n\r\n    /**\r\n     * Loads model into interpreter\r\n     */\r\n    public static TensorFlowLiteClassifier classifier(AssetManager assetManager, String modelPath) throws IOException {\r\n        ByteBuffer byteBuffer = loadModelFile(assetManager, modelPath);\r\n        Interpreter interpreter = new Interpreter(byteBuffer);\r\n        return new TensorFlowLiteClassifier(interpreter);\r\n    }\r\n\r\n    /**\r\n     * Loads model\r\n     */\r\n    private static MappedByteBuffer loadModelFile(AssetManager assets, String path) throws IOException {\r\n        AssetFileDescriptor file = assets.openFd(path);\r\n        FileInputStream stream = new FileInputStream(file.getFileDescriptor());\r\n        FileChannel channel = stream.getChannel();\r\n        long startOffset = file.getStartOffset();\r\n        long declaredLength = file.getDeclaredLength();\r\n        return channel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n    }\r\n\r\n    /**\r\n     * Function that actually performs inference\r\n     */\r\n    public float[][] predictProbabilities(float[][] input) {\r\n        float[][] output = new float[1][LABEL_NUM];\r\n        interpreter.run(input, output);\r\n        return output;\r\n    }\r\n}\r\n```\r\n\r\nWhen I use 1DCNN.tflite, this class works completely. However, when I change the model into LSTM.tflite, getting ERROR at this line\r\n\r\n```Java\r\nInterpreter interpreter = new Interpreter(byteBuffer);\r\n```\r\n\r\nAnd ERROR logcat is following\r\n**A/libc: Fatal signal 11 (SIGSEGV), code 1, fault addr 0x70 in tid 28007**\r\n\r\nHow can I deal with this unknown error?\r\nAfter my some investigation, this error seems to occur when the prepared array is exceeded for some reason.\r\n", "comments": ["I'm wondering if it's an issue with the Android APK environment. Does the same lstm flatbuffer model run on desktop?", "I can infer probabilities using same LSTM.tflite model with this Python code:\r\n\r\n```Python\r\nimport tensorflow as tf\r\ninterpreter = tf.compat.v2.lite.Interpreter(\"LSTM.tflite\")\r\ninterpreter.allocate_tensors()\r\ninput  = interpreter.tensor(interpreter.get_input_details()[0][\"index\"])\r\noutput = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\r\nfor i in range(10):\r\n  input().fill(1)\r\n  interpreter.invoke()\r\n  print(\"inference %s\" % output())\r\n```\r\n\r\nOutput is following:\r\n\r\n```\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\ninference [[9.5466515e-09 1.0000000e+00]]\r\n```\r\n\r\nMy model is for binary classification, it means this output is what I want.\r\nIn other words, does it mean that the Android APK environment is incorrect?\r\n", "It might be due to some native methods [here](https://github.com/lizhangqu/TensorflowLite/blob/master/library/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java):\r\nTensorflowLite/library/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java\r\n\r\n```private static native long createErrorReporter(int size);```\r\n```private static native long createModelWithBuffer(MappedByteBuffer modelBuffer, long errorHandle);```\r\n```private static native long createInterpreter(long modelHandle);```\r\n\r\n```Java\r\n/**\r\n   * Initializes a {@code NativeInterpreterWrapper} with a {@code MappedByteBuffer}. The\r\n   * MappedByteBuffer should not be modified after the construction of a {@code\r\n   * NativeInterpreterWrapper}.\r\n   */\r\n  NativeInterpreterWrapper(MappedByteBuffer mappedByteBuffer) {\r\n    modelByteBuffer = mappedByteBuffer;\r\n    errorHandle = createErrorReporter(ERROR_BUFFER_SIZE);\r\n    modelHandle = createModelWithBuffer(modelByteBuffer, errorHandle);\r\n    interpreterHandle = createInterpreter(modelHandle);\r\n  }\r\n```", "Hi!, could any of you solve this issue? \r\nI'm having the same error with an LSTM clasifier.\r\nIn fact, not exactly the same... actually I'm getting this:\r\n\r\n`java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model`", "I solved a similar issue following [this guide](https://www.tensorflow.org/lite/guide/ops_select#android_aar).\r\n\r\nParticularly adding this into **build.gradle**:\r\n\r\n`\r\nandroid {\r\n    defaultConfig {\r\n        ndk {\r\n            abiFilters 'armeabi-v7a', 'arm64-v8a'\r\n        }\r\n    }\r\n}\r\n`\r\n\r\nto filter unnecessary ABI dependencies.\r\n", "@kazuimotn \r\nPlease update as per above comment.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34313\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34313\">No</a>\n", "I am seeing the same crash:  Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR)\r\n\r\nThis occurs while running the [standard object detection routine ](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/android/lib_interpreter/src/main/java/org/tensorflow/lite/examples/detection/tflite/TFLiteObjectDetectionAPIModel.java) using the mobileNet SSD .tflite model that can be found in tf model zoo:\r\n\r\nThe issue was replicated on Samsung Galaxy S20+ as well as on Motorola phones and it seems like this crash is phone independent. On Motorola phones the crash occurs within 2 hours while running the detection while on Samsung Galaxy the issue sometimes occurs after 8h of running the same code on the same input data in a repetitive loop. Filtering the abi's did not affect this case.\r\nDid anyone resolved this crash in a more general fashion?\r\n\r\nIt seems like it occurs on the  RenderThread during the DeferredLayerUpdate. \r\nHere is my crash report:\r\n--------- beginning of crash\r\n2022-02-13 15:33:42.474 18219-18249/? A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x2820 in tid 18249 (RenderThread), pid 18219 (dia.rec)\r\n2022-02-13 15:33:42.626 18717-18717/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n2022-02-13 15:33:42.626 18717-18717/? A/DEBUG: Build fingerprint: 'motorola/kyoto_retaile/kyoto:11/RRKS31.Q3-19-97-3/bc988:user/release-keys'\r\n2022-02-13 15:33:42.626 18717-18717/? A/DEBUG: Revision: 'PVT1'\r\n2022-02-13 15:33:42.626 18717-18717/? A/DEBUG: ABI: 'arm64'\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG: Timestamp: 2022-02-13 15:33:42-0500\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG: pid: 18219, tid: 18249, name: RenderThread  >>> dia.rec <<<\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG: uid: 10324\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x2820\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x0  b4000076636106d0  x1  b4000076636dbed0  x2  0000007576fea8c0  x3  00000000000000ff\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x4  0000000000000003  x5  b4000076c35c8980  x6  b4000076c35c8980  x7  0000000000000000\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x8  0000000000000000  x9  0000000000000001  x10 0000000000000000  x11 0000000063610768\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x12 00000000e1b5b851  x13 2365000000084108  x14 001c222856e7e2a0  x15 000021d5f589e147\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x16 00000078764327f8  x17 00000078739dbb70  x18 0000007576d54000  x19 b4000076636106d0\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x20 b4000076636dbed0  x21 b4000076636dbef0  x22 0000000000002800  x23 b4000075f35b75b8\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x24 b4000076235d4c50  x25 0000007576feacc0  x26 0000007576feaff8  x27 00000000000fe000\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     x28 00000000000fc000  x29 0000007576fea870\r\n2022-02-13 15:33:42.628 18717-18717/? A/DEBUG:     lr  0000007874205500  sp  0000007576fea870  pc  000000787420ae00  pst 0000000020001000\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG: backtrace:\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #00 pc 0000000000239e00  /system/lib64/libhwui.so (android::uirenderer::Layer::Layer(android::uirenderer::RenderState&, sk_sp<SkColorFilter>, int, SkBlendMode)+304) (BuildId: 0e3f217960712f3e10c4b8b3b618903d)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #01 pc 00000000002344fc  /system/lib64/libhwui.so (android::uirenderer::DeferredLayerUpdater::apply()+428) (BuildId: 0e3f217960712f3e10c4b8b3b618903d)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #02 pc 000000000021fbd8  /system/lib64/libhwui.so (_ZNSt3__110__function6__funcIZN7android10uirenderer12renderthread13DrawFrameTask11postAndWaitEvE3$_0NS_9allocatorIS6_EEFvvEEclEv$c303f2d2360db58ed70a2d0ac7ed911b+684) (BuildId: 0e3f217960712f3e10c4b8b3b618903d)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #03 pc 000000000020e504  /system/lib64/libhwui.so (android::uirenderer::WorkQueue::process()+220) (BuildId: 0e3f217960712f3e10c4b8b3b618903d)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #04 pc 000000000022f41c  /system/lib64/libhwui.so (android::uirenderer::renderthread::RenderThread::threadLoop()+88) (BuildId: 0e3f217960712f3e10c4b8b3b618903d)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #05 pc 0000000000015414  /system/lib64/libutils.so (android::Thread::_threadLoop(void*)+260) (BuildId: 82f928b900a93dc07b75aefd76a59775)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #06 pc 0000000000014cd8  /system/lib64/libutils.so (thread_data_t::trampoline(thread_data_t const*)+412) (BuildId: 82f928b900a93dc07b75aefd76a59775)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #07 pc 00000000000af86c  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) (BuildId: ced3426ba03689dee1bf74b8de9c436a)\r\n2022-02-13 15:33:42.669 18717-18717/? A/DEBUG:       #08 pc 0000000000050110  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (BuildId: ced3426ba03689dee1bf74b8de9c436a)\r\n\r\n"]}, {"number": 34312, "title": "Error when loading SavedModel with models.load_model() and compile=false ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0.130/7.6.3.30\r\n- GPU model and memory: Tesla K80 with 11441MiB; GeForce GTX 1080Ti with 11441MiB\r\n\r\n**Describe the current behavior**\r\nWhen loading a model saved using `model.save('./model', save_format='tf')` without compiling it and executing an inference, using `model.predict()` the following exception is raised:\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n\r\n<ipython-input-3-d52b49db9273> in <module>()\r\n      1 backend.clear_session()\r\n      2 model = models.load_model('./model', compile=False)\r\n----> 3 model.predict(x_data, batch_size=10)\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in predict(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\r\n    907         max_queue_size=max_queue_size,\r\n    908         workers=workers,\r\n--> 909         use_multiprocessing=use_multiprocessing)\r\n    910 \r\n    911   def reset_metrics(self):\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_arrays.py in predict(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\r\n    713     batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)\r\n    714     x, _, _ = model._standardize_user_data(\r\n--> 715         x, check_steps=True, steps_name='steps', steps=steps)\r\n    716     return predict_loop(\r\n    717         model,\r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2431     is_compile_called = False\r\n   2432     if not self._is_compiled and self.optimizer:\r\n-> 2433       self._compile_from_inputs(all_inputs, y_input, x, y)\r\n   2434       is_compile_called = True\r\n   2435 \r\n\r\n/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py in _compile_from_inputs(self, all_inputs, target, orig_inputs, orig_target)\r\n   2659     self.compile(\r\n   2660         optimizer=self.optimizer,\r\n-> 2661         loss=self.loss,\r\n   2662         metrics=self._compile_metrics,\r\n   2663         weighted_metrics=self._compile_weighted_metrics,\r\n\r\nAttributeError: 'Model' object has no attribute 'loss'\r\n```\r\n\r\nThe problem is due to the load function setting the optimizer of the model with an instance of `tensorflow.python.keras.optimizer_v2.optimizer_v2.RestoredOptimizer` and, therefore, the model is tried to be compiled in:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4ff2916c8ca78839cebcfa39e2ac939fa50eab80/tensorflow/python/keras/engine/training.py#L2320-L2322\r\n\r\nRaising the error as there is no loss/metrics loaded previously.\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour is that the model performs the inference without any need to be compiled and returns the result. In fact, this behaviour is meet when the model is saved with the Keras format ('model.h5') and loaded. \r\n\r\n**Code to reproduce the issue**\r\nA executable notebook can be found [here](https://colab.research.google.com/drive/10ljGMdWthbZ3waalt-4vKguEyxbZzUbt)\r\n\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras import backend, layers, models\r\n\r\nx = layers.Input(shape=(4,))\r\ny = layers.Dense(1, activation='softmax')(x)\r\nmodel = models.Model(inputs=x, outputs=y)\r\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\r\n\r\nx_data = np.random.random((10, 4))\r\ny_data = np.random.random((10,))\r\nmodel.fit(x_data, y_data, epochs=5, batch_size=10)\r\nmodel.save('./model', save_format='tf')  # Loading this will raise an error\r\nmodel.save('./model.h5', save_format='h5')  # Loading this will work\r\n\r\n# This block will load the SavedModel without compiling and will \r\n# perform the inference raising an error\r\nbackend.clear_session()\r\nmodel2 = models.load_model('./model', compile=False)\r\nmodel2.predict(x_data, batch_size=10)\r\n\r\n# This block will load the Keras saved model without compiling and will \r\n# perform the inference working as expected\r\nbackend.clear_session()\r\nmodel3 = models.load_model('./model.h5', compile=False)\r\nmodel3.predict(x_data, batch_size=10)\r\n```", "comments": ["Issue replicating with `TF-2.0` and `tf-nightly`.", "Hi @k-w-w , Can I take on this issue? ", "@lsgrep Feel free to pick this up if you're interested!", "@adrigrillo Looks like this was resolved in recent `tf-nightly`. I was not able to reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/3bd6555e2cbc31bfba51a7f04dc3b199/load-model-bug-tf2-0.ipynb).\r\n\r\nPlease close the issue if it was resolved for you. Thanks!", "I can confirm that the latest nightly solves the problem. Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34312\">No</a>\n"]}, {"number": 34311, "title": "Batch pre-padding with Tensorflow datasets", "body": "I have a dataset of variable-length sequences to feed an LSTM network and I want to do pre-padding in the batches, but current padded_batch function only pads at the sequences end. How can I do it?\r\nI asked this question on stackoverflow but nobody answered me.\r\n", "comments": ["i know it is little tricky but what helped me is this article check it out. and please update me.\r\nhttps://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction\r\n\r\n", "happy to help", "@developer22-university \r\nThanks for reply. What it says in the article is about padding a sequence by itself, but what I want is an integrated batch&pad capability for a tensorflow dataset. This article has nothing to do with datasets.", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. \r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@rmothukuru \r\nI am referring to the padded_batch function as described in https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch\r\nI just want to know how to do this thing (padded_batch) in pre-padding format. The current function only implements post-padding.\r\nI am using Tenorflow 1.12 on Python 3.6/Windows platform, but my question is general.", "You can implement your custom batching logic using existing tf.data transformation thanks to the following API: https://github.com/tensorflow/community/blob/master/rfcs/20180726-tf-data-windowing-reducers.md", "@jsimsa \r\nThanks for link!", "@shahriar49 \r\n\r\nPlease close this thread if it solves your question. Thanks!"]}, {"number": 34310, "title": "tf-lite version info", "body": "I have preinstalled TensorFlow - lite on my system, is it possible to get version information of it programmatically? If so - how?\r\nThanks. ", "comments": ["TF Lite comes with TensorFlow prebuilt binary. So it will be the same version as your installed TF.\r\nFrom python interpreter you may try;\r\n```python\r\nimport tensorflow as tf\r\ntf.__version__\r\n```", "Thanks. How about in C++ case?\n\nDne p\u00e1 15. 11. 2019 19:25 u\u017eivatel Yasir Modak <notifications@github.com>\nnapsal:\n\n> TF Lite comes with TensorFlow prebuilt binary. So it will be the same\n> version as your installed TF.\n> From python interpreter you may try;\n>\n> import tensorflow as tf\n> tf.__version__\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34310?email_source=notifications&email_token=ALGLVQR5XJM7YW4ED26JFILQT3SRXA5CNFSM4JN3XDV2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEEGJB2Y#issuecomment-554471659>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALGLVQRXZLV33RJZACFJCSDQT3SRXANCNFSM4JN3XDVQ>\n> .\n>\n", "The version will be same and you may use the above snippet to get tf version. Thanks!\r\nClosing this issue now, feel free to reopen if have any questions. Thanks!"]}, {"number": 34308, "title": "Feature Request: Pass 'elapsed_secs' into LoggingHook string formatter.", "body": "Can the tf.train.LoggingTensorHook be changed to pass the time elapsed into the string formatter?\r\n\r\nIt would be quite easy to add it to the dictionary of values already being passed to the formatter function. Currently you have to choose between your own custom string formatter, and seeing the time elapsed - I think it would be quite easy to have both with a small change to the code.\r\n\r\nThanks.\r\n", "comments": ["@kiunthmo \r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "@kiunthmo \r\n\r\nAny update on this issue please?. Thanks!", "My use case is during evaluation to monitor the performance. For short, scalar tensors you don't really need to do much formatting, but if i'm comparing two larger tensors it can be useful to align them using formatting. The problem is that by using the formatter you sacrifice the elapsed time.\r\n\r\nCurrently a logging entry will look like this without a formatter:\r\n\r\n> A: [1,2,3,4,5,6,7,8,9], B: [1,2,3,4,6,6,7,8,9], 1.2 seconds\r\n\r\nWhen it could look like this with a formatter and amending the LoggingHook:\r\n\r\n> A: [1,2,3,4,5,6,7,8,9], \r\n> B: [1,2,3,4,6,6,7,8,9], \r\n> 1.2 seconds\r\n\r\nThis kind of example looks especially bad when the length of A and B together are wider than the console.\r\n\r\nIn LoggingHook source you can see that when there is a formatter only the tensor values are passed to said formatter:\r\n\r\n```\r\n  def _log_tensors(self, tensor_values):\r\n    original = np.get_printoptions()\r\n    np.set_printoptions(suppress=True)\r\n    elapsed_secs, _ = self._timer.update_last_triggered_step(self._iter_count)\r\n    if self._formatter:\r\n      logging.info(self._formatter(tensor_values))\r\n    else:\r\n      stats = []\r\n      for tag in self._tag_order:\r\n        stats.append(\"%s = %s\" % (tag, tensor_values[tag]))\r\n      if elapsed_secs is not None:\r\n        logging.info(\"%s (%.3f sec)\", \", \".join(stats), elapsed_secs)\r\n      else:\r\n        logging.info(\"%s\", \", \".join(stats))\r\n    np.set_printoptions(**original)\r\n```\r\n\r\nThis could be easily changed to append to elapsed seconds to the values, or pass it to the formatter as a second argument.", "Thanks @kiunthmo for the request. I am wary of complicating the code for the hook, and we are unlikely to have cycles for this, but we could consider a PR if you send one. ", "I would like to work on this, hopefully I'll have a PR that solves it soon.  ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 34307, "title": "error \" 'tflite::FlatbufferModel' has not been declared \"when use Tensorflow lite C++ API", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (or github SHA if from source):r1.14\r\n\r\ni run `bazel build //tensorflow/lite:libtensorflowlite` and `bazel build //tensorflow/lite:framework`,so i get libtensorflowlite.so and libframework.so. \r\nThen i use the library in my own code,but i got an error:\r\n\r\n```\r\nmain.cpp: In function \u2018int main()\u2019:\r\nmain.cpp:9:59: error: \u2018tflite::FlatbufferModel\u2019 has not been declared\r\n  std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatbufferModel::BuildFromFile(\"test1.tflite\");\r\n                                                           ^\r\nmain.cpp:17:10: error: \u2018tflite::ops\u2019 has not been declared\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n          ^\r\nmain.cpp:17:42: error: expected \u2018;\u2019 before \u2018resolver\u2019\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n                                          ^\r\nmain.cpp:19:43: error: \u2018resolver\u2019 was not declared in this scope\r\n  tflite::InterpreterBuilder(*model.get(), resolver)(&interpreter);\r\n                                           ^\r\n```\r\n\r\nthe command i used:\r\n`g++  -std=c++11 -I/home/wang/work/ARM-BAZEL/tensorflow-r1.14 -I/home/wang/work/ARM-BAZEL/tensorflow-r1.14/tensorflow/lite/tools/make/downloads/flatbuffers/include main.cpp -o test -L/home/wang/work/ARM-BAZEL/tensorflow-r1.14/bazel-bin/tensorflow/lite -ltensorflowlite -lframework`\r\n\r\ni want know where is the problem chould be ? I'm relly not familiar with how to use tensorflow lite.", "comments": ["what happened.. how did you solved?"]}, {"number": 34306, "title": "tf2.0.0 using  tf.saved_model.save save model,  java SavedModelBundle load can not find ouput opt", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): centos7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\npython code\r\n\r\n    X = np.array([[0,0], [0,1], [1,0], [1,1]], 'float32')\r\n    Y = np.array([[0], [1], [1], [0]], 'int64')\r\n\r\n    input_x = tf.keras.Input(shape=(2), name=\"text\")\r\n    dense1 = tf.keras.layers.Dense(64, activation='relu', name=\"ds1\")\r\n    dense2 = tf.keras.layers.Dense(2, name=\"label\")\r\n\r\n    x = dense1(input_x)\r\n    x = dense2(x)\r\n    pred = tf.reduce_mean(x, 1, name=\"prediction\")\r\n\r\n    model = tf.keras.Model(inputs=input_x, outputs=pred, name= \"test\")\r\n\r\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['binary_accuracy'])\r\n    model.fit(X, Y, batch_size=1, epochs=10, verbose=0)\r\n\r\n    print('inputs: ', [input.op.name for input in model.inputs])\r\n    print('outputs: ', [output.op.name for output in model.outputs])\r\n\r\n    tf.saved_model.save(model, \"./model_00\")\r\n\r\n\r\n[java]\r\n           String modelpath=\"./model_00\"\r\n            SavedModelBundle bundle = SavedModelBundle.load(modelpath,\"serve\");\r\n            Graph graph = bundle.graph();\r\n            \r\n            Iterator<Operation> iter = graph.operations();\r\n            while (iter.hasNext()) {\r\n                Operation opt = iter.next();\r\n                System.out.println(opt.name());\r\n            }\r\n\r\n\r\ncan not find  output opt ??\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n"]}, {"number": 34305, "title": "Behaviour of the loss function with sample_weights (keras)", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n\r\n## Description of issue (what needs changing):\r\nI think we need a description of how the loss is computed when (temporal) sample_weights are applied, i.e. how is the loss aggregated across samples/time-steps. As addressed in #25178, the behaviour has even changed over time in the external version of Keras from ignoring zero-values to counting them making the situation confusing. \r\n\r\nEven if I agree the current implementation is the right one, I think it is far from intuitive, especially when using zero-weighted zero-padding which decreases the loss value and the effective learning rate.\r\n\r\nI am not very familiar with the structure to be followed in the build-in TF.Keras documentation, but this could be also specified elsewhere than in the `fit` method doc which is already pretty dense.\r\n\r\nSorry if this is specified elsewhere in the doc but I can't find it so if it exists, it should maybe be referenced in the `fit` doc.\r\nThanks a lot,\r\nEmilien", "comments": ["Hey!!\r\nI confirmed the #25178 and still, it works inconsistently. But as the solution mentioned there is to use tf-nightly. If that too doesn't work please ping again.", "Hi,\r\nThanks for having a look :)\r\nI think the main issue now is about documenting the issue so that we are aware of whatever loss aggregation function is chosen.", "Sounds like a good suggestion!\r\nCan you add it to the docstring here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L596\r\nWe now support DocTest snippets in the API reference so that might be a good place to start: https://www.tensorflow.org/community/contribute/docs_ref", "@lamberta Thanks for the references, I will make a doc edition proposal :)", "@Emilien-P \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34303, "title": "two words not mached at the same time ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 34302, "title": "Tensorflow 1.15 doesn't exists within pip install", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/install/pip\r\n\r\n## Description of issue (what needs changing):\r\nIt says tensorflow 1.15 is final version for 1xx versions yet pip install 1.15 returns this\r\n\r\n\"$ pip3 install tensorflow==1.15\r\nCollecting tensorflow==1.15\r\n  Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nNo matching distribution found for tensorflow==1.15\r\n\"\r\n\r\n\r\nIt should be corrected I think.\r\n", "comments": ["Hi, I just came across the problem myself, and I fixed it by upgrading pip to 19.0.", "@ugurkanates, PIP install TF 1.15 is available.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/21d328ab6113dce454199f711badb4cf/untitled263.ipynb). Thanks! ", "Please upgrade pip.\r\n\r\n```\r\npython -m pip install --upgrade pip setuptools\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34302\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34302\">No</a>\n", "Getting this issue even with updated pip and setuptools, both on Windows and Arch Linux:\r\n\r\n```\r\nC:\\windows\\system32>python -m pip install tensorflow==1.15\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.15\r\n\r\nC:\\windows\\system32>python -m pip --version\r\npip 19.3.1 from C:\\Program Files (x86)\\Python37-32\\lib\\site-packages\\pip (python 3.7)\r\n\r\nC:\\windows\\system32>python --version\r\nPython 3.7.4\r\n```\r\n\r\n```\r\ncol@col-siris ~/AIDungeon> python -m pip install tensorflow==1.15.0\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==1.15.0\r\ncol@col-siris ~/AIDungeon> python -m pip --version\r\npip 19.3.1 from /usr/lib/python3.8/site-packages/pip (python 3.8)\r\ncol@col-siris ~/AIDungeon> python --version\r\nPython 3.8.1\r\n```\r\n\r\nEDIT: Installed Python3.7-x64 on windows and it appears to install. Do you guys have packages for 3.8?", "https://pypi.org/project/tensorflow/#files\r\n\r\nAlso, please see https://github.com/tensorflow/tensorflow/issues/33374#issuecomment-571074915", "Even with the latest pip there is no 1.15 package available", "@MasterGroosha : 32 bits python? Python 2? Python 3.8?", "@mihaimaruseac Yeah, my bad, I didn't understand your messages. With Python 3.8, I cannot install TF-1.15, but I did install it with 3.7.6", "Also encountered this issue but pip showed a different message for me\r\n\r\n```zsh\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)\r\nERROR: No matching distribution found for tensorflow==1.15\r\n```", "@raphtlw that is because you are using python3.8 which is not supported in TF before 2.2", "@mihaimaruseac So what are the correct versions for the issue that @raphtlw is having? Should I downgrade from python3.8 to something else?", "Yes, [any version of Python above 3.5 and below 3.8](https://www.tensorflow.org/install/pip#system-requirements)"]}, {"number": 34301, "title": "tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Did you try using [TFLITE_BUILTINS, SELECT_TF_OPS ](https://www.tensorflow.org/lite/guide/ops_select)to convert your model? Thanks!", "Check that `~/.local/bin` in PATH.", "@fsy8515-dev \r\n\r\nAny update on this issue please? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 34300, "title": "Feature_columns  causes model training to be slow", "body": "I use tf2.0 functional API to build the model, and use feature columns to process the data. I find that each step of my training takes a long time, and the CPU utilization is very low. My device is a 16 core CPU. When I don't use feature columns, I can run the model with multiple cores. I'm confused. Can someone answer this question?\r\n![image](https://user-images.githubusercontent.com/26998046/68914950-39d7f100-079b-11ea-9f54-7d1053a5f623.png)\r\nIn a certain training, each step takes more than 100 ms, which is very slow\r\n![image](https://user-images.githubusercontent.com/26998046/68915018-6b50bc80-079b-11ea-811c-37cb22bf237c.png)\r\nThe utilization rate of cou in the training process is very low, and it does not play the multi-core performance at all.\r\n\r\nThe result is a test when using the official feature _columns tutorial.https://www.tensorflow.org/tutorials/structured_data/feature_columns?hl=zh-CN\r\nWhen I use the MNIST tutorial to run in the same environment, CPU utilization is completely multi-core performance.\r\nHas anyone encountered this problem? Thank you very much.\r\n", "comments": ["I want to know why feature_columns cause model training not to use multi-core CPU", "@ztz818 ,\r\nWhen tried running the tutorial for TF-2.0, I was able to run it without any issues. Please find the [gist](https://colab.sandbox.google.com/gist/oanush/014444e1e04149353045f14d15297af6/34300.ipynb) of colab. Kindly let us know if your are using any different code? provide us with the same.", "@ztz818 ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "The feature_columns in TF 2.x is really slow in eager mode when initialization, and very very slow when forwarding. I am annoyed by TF 2.x. Some docs in feature_columns of TF 2.x are only copied from TF 1.x. For example, `input_layer` is not available in TF 2.x, but still on docs of TF 2.x. \r\n\r\n@ztz818 \r\n\r\nHave you figured out how to solve this?\r\n\r\n ", "2021\uff0c tf2.4, same problem, "]}, {"number": 34299, "title": "How to use Per-channel quantize in TFLite\uff1f", "body": "Hello\r\nfrom the tflite conv.cc , I can see that now tflite supports Per-channel quantize , is there any guide or doc how to use per-channel quantize when convert tf model to tflite?", "comments": ["hi,\r\n\r\nhttps://github.com/KhronosGroup/NNEF-Tools/issues/40\r\n\r\nhttps://www.tensorflow.org/lite/performance/post_training_quantization\r\n\r\nCHECK THESE LINKS UPDATE ME\r\n", "When you use post-training integer quantization, per-channel quantization is enabled by default. Please follow instructions here:\r\nhttps://medium.com/tensorflow/tensorflow-model-optimization-toolkit-post-training-integer-quantization-b4964a1ea9ba", "@liyunlu0618 Do you know how to specify for using per-layer quantization instead in tflite?  I searched in many tutorials and documents, no luck however.", "Currently if you use post-training integer quantization it's per-channel by default. There's no way to configure that to use per-layer quant. The legacy quantization-aware training (QAT) used per-layer quantization but we're deprecating that. The upcoming QAT will also use per-channel.", "Hey @liyunlu0618  , sorry to bring up this issue again (since been closed for a while), but I\"m just looking for some answer and hopefully you can provide me some. To my understanding, right now TF QAT for Conv2D is still supporting per-axis quantization, I'm wondering if we can pass in some custom configuration for quantizing to make it per-tensor quantization instead? If it's possible, I'm wondering if you can provide me some examples/ resources to do so.", "Hello guys,\r\n\r\nI'm facing a similar issue of not-having per-layer quantization available - is it somehow enforce this feature (per-layer-quantization)?\r\nAs @liyunlu0618 mentioned this feature ((per-layer-quantization)) will be removed but is it somehow possible to have this feature - if so, how?\r\n\r\nThank you.\r\n\r\nPeter"]}]