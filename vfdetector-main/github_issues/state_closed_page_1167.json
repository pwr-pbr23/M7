[{"number": 18188, "title": "contrib.distribute is not Python3 compatible", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. Porting internal ResNet distributed back into models/official\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04 on GCP\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n- **TensorFlow version (use command below)**:\r\ntf-nightly-gpu==1.8.0.dev20180330\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0, CUDNN 7.0\r\n- **GPU model and memory**:\r\n4xP100\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\n```\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:batch_all_reduce invoked for batches size = 153 with algorithm = nccl and num_packs = 1\r\nINFO:tensorflow:Error reported to Coordinator: 'dict_keys' object does not support indexing\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 667, in _distributed_apply\r\n    reduced_grads = distribution.batch_reduce(\"sum\", grads_and_vars)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 796, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 295, in _batch_reduce\r\n    value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 175, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 462, in _batch_reduce\r\n    [v[0] for v in value_destination_pairs])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 517, in _batch_all_reduce\r\n    method_string)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 276, in _ungroup_and_make_mirrored\r\n    index[i][destinations[d]] = v\r\nTypeError: 'dict_keys' object does not support indexing\r\nTraceback (most recent call last):\r\n  File \"imagenet_main.py\", line 318, in <module>\r\n    main(argv=sys.argv)\r\n  File \"imagenet_main.py\", line 313, in main\r\n    shape=[_DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, _NUM_CHANNELS])\r\n  File \"/home/taylorrobie/TensorFlow/models/official/resnet/resnet_run_loop.py\", line 444, in resnet_main\r\n    max_steps=flags.max_train_steps)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 363, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 841, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py\", line 884, in _train_model_distributed\r\n    self.config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 751, in call_for_each_tower\r\n    return self._call_for_each_tower(fn, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 254, in _call_for_each_tower\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 248, in _call_for_each_tower\r\n    self, *merge_args, **merge_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py\", line 667, in _distributed_apply\r\n    reduced_grads = distribution.batch_reduce(\"sum\", grads_and_vars)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py\", line 796, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py\", line 295, in _batch_reduce\r\n    value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 175, in batch_reduce\r\n    return self._batch_reduce(method_string, value_destination_pairs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 462, in _batch_reduce\r\n    [v[0] for v in value_destination_pairs])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 517, in _batch_all_reduce\r\n    method_string)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/cross_tower_ops.py\", line 276, in _ungroup_and_make_mirrored\r\n    index[i][destinations[d]] = v\r\nTypeError: 'dict_keys' object does not support indexing\r\n```\r\n", "comments": ["I think this issue is the same as #18205. The PR #18212 will fix the issue I think.", "Lovely. Thank you."]}, {"number": 18187, "title": "Adam optimizer with decaying learning rate", "body": "I tried to implement the Adam optimizer with different beta1 and beta2 to observe the decaying learning rate changes using:\r\n```\r\noptimizer_obj = tf.train.optimizer(learning_rate=0.001, beta1=0.3, beta2=0.7)\r\n```\r\n\r\nTo track the changes in learning rate, I printed the ```_lr_t``` variable of the object in the session:\r\n```\r\nprint(sess.run(optimizer_obj._lr_t))\r\n```\r\n\r\nBut for all iterations, I get the same value (the initialized 0.001 value).\r\n\r\nI couldn't find how the parameters are updated in this class.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: \r\n```\r\noptimizer_obj = tf.train.optimizer(learning_rate=0.001, beta1=0.3, beta2=0.7)\r\nwith tf.Session() as sess:\r\n    print(sess.run(optimizer_obj._lr_t))\r\n```", "comments": ["Hi, I think the question is better to be asked on stackoverflow. You should use tf.minimize with your optimizer to create a train_op, and print lr_t after session.run(train_op) in each iteration. ", "I already checked the stackoverflow and found [this](https://stackoverflow.com/questions/37842913/tensorflow-confusion-regarding-the-adam-optimizer?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa) post explaining the concept. I am familiar with the concept. But I think there is an issue that the TensorFlow is not using the decaying learning rate. That is why I submitted this issue.\r\n\r\nFull code to reproduce the results:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n\r\ndef fc_layer(x, num_units, name, use_relu=True):\r\n    in_dim = x.get_shape()[1]\r\n    W = tf.get_variable('W_' + name,\r\n                    dtype=tf.float32,\r\n                    shape=[in_dim, num_units],\r\n                    initializer=tf.truncated_normal_initializer(stddev=0.01))\r\n    b = tf.get_variable('b_' + name,\r\n                           dtype=tf.float32,\r\n                           initializer=tf.constant(0., shape=[num_units], dtype=tf.float32))\r\n    layer = tf.matmul(x, W)\r\n    layer += b\r\n    if use_relu:\r\n        layer = tf.nn.relu(layer)\r\n    return layer\r\n\r\n# Data Dimensions\r\nimg_size_flat = 784\r\nn_classes = 10\r\n\r\n\r\n# Hyper-parameters\r\nlr = 10  # The optimization initial learning rate\r\nbatch_size = 100  # Training batch size\r\ndisplay_freq = 100  # Frequency of displaying the training results\r\nepochs = 10\r\n\r\nh1 = 128  # Number of neurons in hidden layer layer.\r\n\r\n\r\n# Create the network graph\r\n# Placeholders for inputs (x), outputs(y)\r\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\r\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\r\n\r\nfc1 = fc_layer(x, h1, 'FC1', use_relu=True)\r\noutput_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\r\n\r\n# Define the loss function, optimizer, and accuracy\r\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\r\n\r\noptimizer_obj = tf.train.AdamOptimizer(learning_rate=lr,beta1=0.7, beta2=.3, name='Adam-op')\r\noptimizer = optimizer_obj.minimize(loss)\r\n\r\n# Initialize the variables\r\ninit = tf.global_variables_initializer()\r\n\r\n\r\n# Launch the graph (session)\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n\r\n    # Number of training iterations in each epoch\r\n    num_tr_iter = int(mnist.train.num_examples / batch_size)\r\n    step = 0\r\n    for epoch in range(epochs):\r\n        for iteration in range(num_tr_iter):\r\n            start = iteration * batch_size\r\n            end = (iteration + 1) * batch_size\r\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\r\n\r\n            # Run optimization op (backprop)\r\n            feed_dict_batch = {x: x_batch, y: y_batch}\r\n            sess.run(optimizer, feed_dict=feed_dict_batch)\r\n            print('step:{} \\t learning rate: {}'.format(step, sess.run(optimizer_obj._lr_t, feed_dict=feed_dict_batch)))\r\n            step = step + 1\r\n```\r\n\r\nAnd this is the output that I get for each step:\r\n```\r\nstep:0 \t learning rate: 10\r\nstep:1 \t learning rate: 10\r\nstep:2 \t learning rate: 10\r\nstep:3 \t learning rate: 10\r\nstep:4 \t learning rate: 10\r\nstep:5 \t learning rate: 10\r\nstep:6 \t learning rate: 10\r\nstep:7 \t learning rate: 10\r\nstep:8 \t learning rate: 10\r\nstep:9 \t learning rate: 10\r\nstep:10 \t learning rate: 10\r\nstep:11 \t learning rate: 10\r\nstep:12 \t learning rate: 10\r\nstep:13 \t learning rate: 10\r\nstep:14 \t learning rate: 10\r\nstep:15 \t learning rate: 10\r\nstep:16 \t learning rate: 10\r\nstep:17 \t learning rate: 10\r\nstep:18 \t learning rate: 10\r\nstep:19 \t learning rate: 10\r\nstep:20 \t learning rate: 10\r\nstep:21 \t learning rate: 10\r\nstep:22 \t learning rate: 10\r\nstep:23 \t learning rate: 10\r\nstep:24 \t learning rate: 10\r\nstep:25 \t learning rate: 10\r\nstep:26 \t learning rate: 10\r\nstep:27 \t learning rate: 10\r\nstep:28 \t learning rate: 10\r\nstep:29 \t learning rate: 10\r\nstep:30 \t learning rate: 10\r\nstep:31 \t learning rate: 10\r\nstep:32 \t learning rate: 10\r\nstep:33 \t learning rate: 10\r\nstep:34 \t learning rate: 10\r\nstep:35 \t learning rate: 10\r\nstep:36 \t learning rate: 10\r\nstep:37 \t learning rate: 10\r\nstep:38 \t learning rate: 10\r\nstep:39 \t learning rate: 10\r\nstep:40 \t learning rate: 10\r\nstep:41 \t learning rate: 10\r\nstep:42 \t learning rate: 10\r\nstep:43 \t learning rate: 10\r\nstep:44 \t learning rate: 10\r\nstep:45 \t learning rate: 10\r\nstep:46 \t learning rate: 10\r\nstep:47 \t learning rate: 10\r\nstep:48 \t learning rate: 10\r\nstep:49 \t learning rate: 10\r\nstep:50 \t learning rate: 10\r\n```\r\n\r\nI just output the first 50 steps. But the learning rate is fixed all the time.", "@jjahanip  You're right. `lr_t_` is constant, which is `learning_rate` in the [API document](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/AdamOptimizer).   \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f9c5e71104cb30583127fdc918591cc7604f17ca/tensorflow/python/training/adam.py#L135\r\n\r\nAnd the real decaying `lr_t` is computed as an intermediate result inside the computing function. You seems to have to compute it by yourself.\r\nhttps://github.com/tensorflow/tensorflow/blob/f9c5e71104cb30583127fdc918591cc7604f17ca/tensorflow/python/training/adam.py#L176", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Assigning to @alextp, who probably has a better idea about this than I do.", "@facaiy 's answer is correct, so I'm closing this issue. Please reopen with additional information if something is not clear.", "> Hi, I think the question is better to be asked on stackoverflow. You should use tf.minimize with your optimizer to create a train_op, and print lr_t after session.run(train_op) in each iteration.\r\n\r\nUnfortunately, this doesn't work. Neither 'lr_t', '_lr_t' nor '_lr' is an attribute of 'Operation' object which returned from minimize().\r\n\r\nI think it should be easier to have these values ready from the optimizer itself than re-computing it by explicit equations.\r\n\r\nIf anybody has an easy solution or a working code, please share it. This would be appreciated."]}, {"number": 18186, "title": "Estimators can't restore from their saved configuration", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A: no verbatim example from TensorFlow on restoring from and `export_savedmodel()`.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Window 10x64; Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\n# Create and train some classifier ....\r\nc = tf.estimator.DNNClassifier(hidden_units = [10,10], feature_columns=my_feature_columns)\r\n# ... do some training\r\n# We'll assume here that I'm using Pandas, and just have floats\r\nspecDict = {}\r\nfor col in list(train_x.columns):\r\n    specDict[col] = tf.FixedLenFeature([], tf.float32)\r\npsirf = tf.estimator.export.build_parsing_serving_input_receiver_fn(specDict, 100)\r\npathOut = c.export_savedmodel(\"./trainedModels/\", psirf)\r\n# I'll write pathOut to a file ...\r\n```\r\n\r\nIn a new instance:\r\n\r\n```python\r\n# We recreate my cols, and pull pathOut from the file it was written to\r\nc = tf.estimator.DNNClassifier(hidden_units = [10,10], feature_columns=my_feature_columns, model_dir = pathOut)\r\n# Using the canned `eval_input_fn` from the iris_data.py example\r\n# dataDict is the source data\r\npredictions = c.predict(input_fn=eval_input_fn(dataDict, None))\r\nfor predDict in predictions:\r\n    # raises \"ValueError: Could not find trained model in model_dir: pathOut\"\r\n    print(predDict)\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nThe estimator [only looks for the latest checkpoint](https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/python/estimator/estimator.py#L476-L480), and ignores any exports that might have been created from its own `export_savedmodel` function\r\n\r\nBecause of https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L644-L652 , seems like [tf.saved_model.loader.load](https://www.tensorflow.org/api_docs/python/tf/saved_model/loader/load) should also be checked", "comments": ["Nagging Assignee @jart: It has been 150 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 165 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18185, "title": "Building Graphs documentation", "body": "System information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 1.4.1 gpu\r\nPython version: 3.5.4\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: Cuda 8.0/Cudnn 6.0\r\nGPU model and memory: Titan xp\r\nExact command to reproduce: no method for outer product\r\n\r\nDescribe the problem\r\nMany links in the documentation say 'see the guide' and point to `Building Graphs > ...` \r\nhttps://www.tensorflow.org/api_guides/python/framework#Core_graph_data_structures\r\nbut no prose guide is present. It would be useful to have some examples of using such methods as `tf.reset_default_graph` , etc.\r\n \r\n\r\nFor example, the `tf.Graph` page links to 'Building Graphs': https://www.tensorflow.org/api_docs/python/tf/Graph", "comments": ["Maybe something like this for `tf.reset_default_graph`?: \r\n```\r\n\r\ndef example():\r\n   c = tf.constant(4.0)\r\n   assert c.graph is tf.get_default_graph()\r\n   \r\n   tf.reset_default_graph()\r\n   assert c.graph is not tf.get_default_graph()\r\n\r\n````", "Thanks for the report.\r\n\r\nAnywhere you have concrete suggestions we'd welcome a PR.", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "There's a [graph guide](https://www.tensorflow.org/guide/graphs) ([in github](https://github.com/tensorflow/docs/blob/master/site/en/guide/graphs.md)) if there are details you'd like to add.\r\n\r\nGiven the [emphasis on eager execution for TF 2.0](https://groups.google.com/a/tensorflow.org/forum/#!topic/announce/qXfsxr2sF-0), I don't think we'll building out this section much more."]}, {"number": 18184, "title": "Variable Scope with Eager Execution", "body": "", "comments": []}, {"number": 18183, "title": "Fix an issue in api_compatibility_test", "body": "While trying to run on my machine (Ubuntu 16.04 Python 2.7) the api_compatibility_test:\r\n```\r\nbazel test -s --config=opt --cache_test_results=no //tensorflow/tools/api/tests:api_compatibility_test\r\n```\r\n\r\nThe following error was encountered:\r\n```\r\n  ......\r\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/tools/api/generator/create_python_api.runfiles/org_tensorflow/tensorflow/tools/api/generator/create_python_api.py\", line 125, in get_api_imports\r\n    if not module or 'tensorflow.' not in module.__name__:\r\n  File \"/usr/lib/python2.7/dist-packages/py/_apipkg.py\", line 171, in __getattribute__\r\n    return getattr(getmod(), name)\r\n  File \"/usr/lib/python2.7/dist-packages/py/_error.py\", line 43, in __getattr__\r\n    raise AttributeError(name)\r\nAttributeError: __name__\r\n```\r\n\r\nThe issue is that `<AliasModule 'py.error' for 'py._error.error'>` does not have a `__name__` attribute (See similiar issue in https://github.com/pytest-dev/py/issues/73).\r\n\r\nThis fix tries to address the issue by adding an `hasattr()` check so that AttributeError is not thrown.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18182, "title": "Tensorflow llite more support ", "body": "Hello,\r\nI am using tflite for my development and I used label_image example for inference. Can you please provide more documentation either on tflite and how to use it or provide support for reading PNG and JPEG files for the inference?\r\n\r\nIs there any python example available for tflite inference pipeline?\r\n\r\nAdditionally, tensorflow lite is slow on RaspberryPi. Slower than normal tensorflow graph. I can run MobileNet in 600ms and for same model converted in tflite takes 1300ms for an inference. Is there any solution for this?\r\n\r\nHave I written custom code - N/A\r\nOS Platform and Distribution - Any\r\nTensorFlow installed from - Source\r\nTensorFlow version - 1.6\r\nBazel version - 0.9\r\nCUDA/cuDNN version - 9/7\r\nGPU model and memory - 1080Ti 12GB\r\nExact command to reproduce - N/A\r\n\r\nThank you,", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "/CC @andrehentz", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes it is", "@andrehentz any solution for my issues? ", "python API for TF Lite is in the works. Stay tuned.\r\n\r\nRegarding label_image, it is mostly and external contribution, and we currently have no plans to support other formats.\r\n\r\nPerformance on RPI: maybe set the number of threads to 4 (or 8)? If using label_image, that's -t 4", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18181, "title": "Update the get_started_for_beginners.md file", "body": "Matched and added variable names to match the same ones used in the script iris_data.py that it references from models/samples/core/get_started/.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I am part of the IBM CLA, and it shows up under my corporate agreements.  I do not know why it did not pass that check...", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 75 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 90 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 105 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrewharp: It has been 120 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This doc no longer exists, so I will close this PR. But CC @MarkDaoust and @lamberta in case these changes are still relevant?", "These docs got moved to docs_src/guide\r\n\r\n`get_started_for_beginners` doesn't exist anymore. `guide/premade_estimators.md` is the closest thing. \r\nAny fixes would be welcome, especially if the fix is to convert them to a `.ipynb` notebook."]}, {"number": 18180, "title": "Eager: tf.size() does not respect `out_type`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nprint(tf.size([1, 2, 3]).dtype)\r\n```\r\n\r\n### Describe the problem\r\nAs per the documentation of [`tf.size`](https://www.tensorflow.org/api_docs/python/tf/size), the `dtype` of the returned tensor should default to `tf.int32` and it can be optionally overridden by providing an `out_type` argument.\r\n\r\nHowever, in the snippet above, `tf.size()` returns a `tf.int64` tensor, and in a related StackOverflow question: https://stackoverflow.com/questions/49604969/gradient-error-occurred-when-calculate-two-embeddings-on-eager-mode the `tf.size()` used by `_GatherGrad` is resulting in a `float64` tensor.\r\n\r\nLong story short, this is a buggy discrepancy between eager execution and graph construction.\r\n(Likely introduced in commit https://github.com/tensorflow/tensorflow/commit/47ea851d3faf029d5b23ee70cb3b96bad0128324)\r\n\r\nCC @alextp ", "comments": []}, {"number": 18179, "title": "tf.layers.Input missing on 1.7", "body": "Recently updated to 1.7 and a script is now crashing because tf.layers.Input is missing. I just check the docs and it appears until 1.6 but it wasn't deprecated. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@fchollet `tf.layers.Input` seem removed in 32ccc6a0. Is this a mistake?", "@cgarciae Could you replace it by `tf.keras.layers.Input`? I think it should work for you as well.", "Sorry for the inconvenience, it was not possible to keep it in the namespace after relocating the code. This is deliberate.\r\n\r\nYou can now import it as `tf.keras.Input`.", "@fchollet I think the change should have been explained in [1.7 release notes](https://github.com/tensorflow/tensorflow/releases/tag/v1.7.0)", "@facaiy works for me, thanks!", "@facaiy good point. I think the reason it wasn't included was:\r\n\r\n- There is no use case whatsoever for `Input` without `Network` (or `Model`)\r\n- `Network` was never part of the public API so it was fine to remove\r\n- `Input` got moved together with it (since they're part of the same setup)\r\n- Since `Network` wasn't part of the public API its removal didn't warrant a release note"]}, {"number": 18178, "title": "Pass dtype to constructor in LSTMCell", "body": "This fix tries to address the issue raised in #16228 where the dtype is not passed to LSTMCell and causes a TypeError when build() is called.\r\n\r\nThis fix pass the dtype to constructor in LSTMCell so that the initializer in build could be invoked. In case None is passed for dtype, float32 is used for initializer.\r\n\r\nThis fix fixes #16228.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@ebrevdo Thanks for the review. The PR has been updated.", "Good for API change"]}, {"number": 18177, "title": "pip install tensorflow in anaconda,can't use it in pycharm", "body": "\r\ni use anaconda environment with \"pip install tensorflow-gpu==1.0\" to install tensorflow.i can use it in console,while i wanted to create a project,i chose the interpreter \"home/anaconda2/envs/tensorflow/bin/python\",and it occur a error\"encodings.CodecRegistryError: incompatible codecs in module \"encodings.utf_8\" (/home/red/anaconda2/envs/tensorflow/lib/python2.7/encodings/utf_8.pyc)\"\r\n------------------------\r\n\r\n### System information\r\nyes,a very easy hello code- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nLinux 16.04- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nsource- **TensorFlow installed from (source or binary)**:\r\n1.0- **TensorFlow version (use command below)**:\r\n2.7.14- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9) - **GCC/Compiler version (if compiling from source)**:\r\n8.0/5.1- **CUDA/cuDNN version**:\r\ngtx1060/3gb- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nencodings.utf_8\" (/home/red/anaconda2/envs/tensorflow/lib/python2.7/encodings/utf_8.pyc)\r\n\r\n### Source code / logs\r\nhello = tf.constant(\"hello,world\")\r\nprint(sess.run(hello))\r\n", "comments": ["I just encountered the same problem and no solution was found, any updates?", "Hi @CUGfred ! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please try with[ latest version ](https://www.tensorflow.org/install)2.7 and let us know. Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 18176, "title": "_pywrap_tensorflow_internal.so gdb load very slow!", "body": "I build a _pywrap_tensorflow_internal.so have 2.7G big.  so that gdb load very slow \uff0cmybe takes one hour. please help me !\r\n\r\nI try many way , such as strip symbols from .so and so on.\r\n\r\nI want to ask is there way to use -g in the single tensorflow module by bazel  build ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I am also watching this problem, the debug version of tensorflow v1.13.1 comes to 4.7G. \r\nAny idea now?"]}, {"number": 18175, "title": "import data_provider from google3.third_party.tensorflow_models.gan.pix2pix import networks  .............. error importing these modules... ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 18174, "title": "'unsupported/Eigen/CXX11/Tensor' file not found on iOS/Xcode", "body": "when I ran this project( https://github.com/yjmade/ios_camera_object_detection )on ios. I met this warning.\r\n\r\nStack Overflow didn't give any help, case #4680 is for Raspberry Pi. It didn't help me.\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n#18174\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nmacOS10.13.4 Xcode9.2 iOS11.2\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.7\r\n- **Python version**: \r\npython3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\u201c\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Davids-New-MacBook-Pro.local 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Davids-New-MacBook-Pro.local 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.5.0)\r\ntensorflow (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\nSanity check: array([1], dtype=int32)\r\n/Users/David/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n/Users/David/tensorflow-1.7.0/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Davids-New-MacBook-Pro.local 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.13.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.1.0 (clang-902.0.39.1)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Davids-New-MacBook-Pro.local 17.5.0 Darwin Kernel Version 17.5.0: Mon Mar  5 22:24:32 PST 2018; root:xnu-4570.51.1~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.5.0)\r\ntensorflow (1.4.0)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514\r\nSanity check: array([1], dtype=int32)\r\n/Users/David/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\r\n  return f(*args, **kwds)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n/Users/David/tensorflow-1.7.0/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\u201d\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "comments": ["Can you be a little more explicit about what exactly you did and what was the failure?", "I follow the procedures of building the iOS demo in \"examples\" in Tensorflow. And it worked well. but when I ran this project( https://github.com/yjmade/ios_camera_object_detection )on ios. I met this warning, 'unsupported/Eigen/CXX11/Tensor'. I checked the unsupported/Eigen/CXX11/Tensor , and it is right in the folder.", "Nagging Assignee @rohan100jain: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "What exactly did you do with https://github.com/yjmade/ios_camera_object_detection ? And why is it a TensorFlow issue?", "Nagging Assignee @allenlavoie: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @allenlavoie: It has been 32 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 18173, "title": "how to debug lite jni so in android ", "body": "\r\nafter building the so of lite, some how i need to debug in the android mobile; how can i debug the native c++ code using android studio?  appriciated", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Here is the guideline to debug code in android studio: https://developer.android.com/studio/debug/.\r\n\r\nYou can use LLDB to debug your native code. If you want to debug the whole flow: java --> jni --> native, I would recommend you to use logs for debugging."]}, {"number": 18172, "title": "Do tensorflow have plan to support deep complex netowrk?", "body": "Do tensorflow have plan to support deep complex netowrk?\r\nsuch as \"Deep Complex Networks\" in https://arxiv.org/abs/1705.09792", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 18171, "title": "Remove warnings for tf.contrib.copy_graph.copy_op_to_graph.", "body": "When I run the test for tf.contrib.copy_graph, there were the warnings as shown below.\r\n\r\n```\r\nWARNING:tensorflow:Operation._node_def is private, use Operation.node_def instead. Operation._node_def will eventually be removed.\r\nWARNING:tensorflow:Operation._op_def is private, use Operation.op_def instead. Operation._op_def will eventually be removed.\r\nWARNING:tensorflow:Operation._node_def is private, use Operation.node_def instead. Operation._node_def will eventually be removed.\r\nWARNING:tensorflow:Operation._op_def is private, use Operation.op_def instead. Operation._op_def will eventually be removed.\r\nWARNING:tensorflow:Operation._node_def is private, use Operation.node_def instead. Operation._node_def will eventually be removed.\r\nWARNING:tensorflow:Operation._op_def is private, use Operation.op_def instead. Operation._op_def will eventually be removed.\r\nWARNING:tensorflow:Operation._node_def is private, use Operation.node_def instead. Operation._node_def will eventually be removed.\r\nWARNING:tensorflow:Operation._op_def is private, use Operation.op_def instead. Operation._op_def will eventually be removed.\r\n```\r\n\r\nI made a patch to follow the instruction in warning messages :)\r\n", "comments": []}, {"number": 18170, "title": "How about supporting delegation of OEM operations in TFL ?", "body": "Currently, the interpreter delegates only built-in ops to NN API. I'm writing here to see if any plan to support to delegate OEM operations(ANEURALNETWORKS_OEM_OPERATION) defined in\r\nhttps://android.googlesource.com/platform/frameworks/ml/+/master/nn/runtime/include/NeuralNetworksOEM.h\r\n\r\nMy understanding is it's just a minor change on TOCO and interpreter, right ?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Android\r\nTensorFlow installed from: N/A\r\nTensorFlow version: N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A", "Can you please share the usecase you are trying to get to work ? There is no plan to support delegation of OEM ops currently. Your usecase will help us understand if this is needed.", "Nagging Assignee @andrehentz: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18169, "title": "Why tensorflow.org is not reachable from Iran? ", "body": "When I try to access [tensorflow.org](tensorflow.org) from my computer in Iran I get the error \"403: Your client does not have permission to get URL / from this server. That\u2019s all we know.\"\r\n\r\nI didn't know any other (reachable) location to post it. Would appreciate it if someone would respond/act on this. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "/CC @martinwicke @MarkDaoust ", "tensorflow.org is hosted on GCP. You appear to be seeing this issue: https://stackoverflow.com/questions/35150576/any-suggestion-about-accessing-google-cloud-hosted-website-from-iran\r\n\r\nWe won't be able to do much about that, since we rely on GCP for hosting."]}, {"number": 18168, "title": "Fix adam optimizer related math equation rendering format", "body": "This PR is to fix mess-up math equation format in below Adam optimizer related docstrings.\r\n\r\nThis PR is to fix this math equation issue according to the [Math in markdown guideline](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/community/documentation.md#math-in-markdown).\r\n\r\nTake [LazyAdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/LazyAdamOptimizer#methods) as an example below:\r\nBefore:\r\n![image](https://user-images.githubusercontent.com/1680977/38183933-b681b03a-3676-11e8-8099-b53e2f55d0cc.png)\r\n\r\nAfter:\r\n![image](https://user-images.githubusercontent.com/1680977/38184016-476ff1e2-3677-11e8-8d48-b68d2f257f41.png)\r\n", "comments": ["Thanks @MarkDaoust , you're right and I've fixed this.", "@MarkDaoust can you take another look?", "Thanks a lot for your comments, @MarkDaoust . Could u pls take another look?", "Thanks. \r\n\r\nI replaced the \"\\leftarrows\" with \":=\" as renders less badly than \"<-\" but is still readable in the source file.", "This fails because the APIDef isn't valid protobuf any more. Most likely a string escape gone wrong.", "@martinwicke Fixed the invalid protobuf typo. Could u pls kindly help run the gate tests?", "The failure case is the timeout of `//tensorflow/core:common_runtime_ring_reducer_test`, which seems to be unrelated."]}, {"number": 18167, "title": "tensorboard take an TypeError after change tensorflow-cpu to tensorflow-gpu installed by pip ", "body": "error messages are as follows\uff1a\r\nTraceback (most recent call last):\r\nFile \"e:\\anaconda\\lib\\runpy.py\", line 184, in _run_module_as_main\r\n\"main\", mod_spec)\r\nFile \"e:\\anaconda\\lib\\runpy.py\", line 85, in run_code\r\nexec(code, run_globals)\r\nFile \"E:\\Anaconda\\Scripts\\tensorboard.exe_main.py\", line 5, in \r\nFile \"e:\\anaconda\\lib\\site-packages\\tensorboard\\main.py\", line 30, in \r\nfrom tensorboard import default\r\nFile \"e:\\anaconda\\lib\\site-packages\\tensorboard\\default.py\", line 35, in \r\nfrom tensorboard.plugins.audio import audio_plugin\r\nFile \"e:\\anaconda\\lib\\site-packages\\tensorboard\\plugins\\audio\\audio_plugin.py\", line 30, in \r\nfrom tensorboard.plugins.audio import metadata\r\nFile \"e:\\anaconda\\lib\\site-packages\\tensorboard\\plugins\\audio\\metadata.py\", line 22, in \r\nfrom tensorboard.plugins.audio import plugin_data_pb2\r\nFile \"e:\\anaconda\\lib\\site-packages\\tensorboard\\plugins\\audio\\plugin_data_pb2.py\", line 63, in \r\noptions=None, file=DESCRIPTOR),\r\nTypeError: init() got an unexpected keyword argument 'file'\r\n\r\nbefore i changed the version\uff0ctensorboard worked\uff0ci don't know whether the tensorboard arguments are different between cpu version and gpu version\r\nI will appreciate for your help\r\n\r\nOS Platform and Distribution \uff1a windows 10\r\nTensorFlow installed from\uff1a pip\r\nTensorFlow version\uff1a tensorflow-gpu 1.7\r\nCUDA/cuDNN version\uff1a cuda 9.0 cudnn9.0", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution \uff1a   windows 10\r\nTensorFlow installed from\uff1a pip \r\nTensorFlow version\uff1a tensorflow-gpu 1.7\r\nCUDA/cuDNN version\uff1a cuda 9.0   cudnn9.0\r\n"]}, {"number": 18166, "title": "Tensorboard has takew", "body": "", "comments": []}, {"number": 18165, "title": "eager execution not working with placeholders", "body": "------------\r\n### System information\r\n- **Have I written custom code: Yes**:\r\n- **OS Platform and Distribution (Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (binary)**:\r\n- **TensorFlow version (1.7)**:\r\n- **Python version (3.6)**: \r\n- **CUDA/cuDNN version (9.0/7.0.)**:\r\n- **GPU model and memory (1080 Ti)**:\r\n\r\n### Describe the problem\r\nI get the following error when I use eager execution on a simple graph with a couple of convolutional layers.\r\n```\r\ntf.placeholder() is not compatible with eager execution\r\n```\r\n\r\nDoes that mean that eager execution is not working with placeholders. That would be useful to have since if have older code constructed using graphs and sessions we won't be able to utilize eager execution for debugging purposes.\r\n", "comments": ["Placeholders don't quite make sense with eager execution since placeholders are meant to be \"fed\" in a call to `Session.run`, as part of the `feed_dict` argument. Since eager execution means immediate (and not deferred execution), there is no notion of a session or a placeholder.\r\n\r\nConsider the following program with graph execution:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef my_model(x):\r\n  return tf.square(x) # you'd likely have something more sophisticated\r\n\r\nx = tf.placeholder(tf.float32)\r\ny = my_model(x)\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(y, feed_dict={x: 3.0})\r\n```\r\n\r\nWith eager execution enabled, you can use `my_model` (and step through it, debug it etc.) with much less boilerplate and no placeholders / sessions:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef my_model(x):\r\n  return tf.square(x)  # you'd likely have something more sophisticated\r\n\r\nprint(my_model(3.0)) \r\n```\r\n\r\nHope that helps.\r\nMore information in the [Getting Started](https://www.tensorflow.org/get_started/eager) and [Programmer's Guide](https://www.tensorflow.org/programmers_guide/eager) sections on eager execution.\r\n\r\nI'm going to close this issue out since we'd like to keep GitHub issues focused on bugs or feature requests. If I have misunderstood what you're asking, then feel free to re-open with more details of what you're trying to do with placeholders and what precisely the bug/feature request is.\r\n\r\nThanks.", "@asimshankar , what you should know is that most of these issues come from the sandbox itself. Developers trying to adapt tensor-flow are faced with bugs right from the sample directory of tensorflow itself on first run without modifying any default code. This should not be the case as these kind of issues will keep coming up. If the git clone of tensorflow  runs normally with all th prerequisites perfectly installed on the host machine, then there might not be such issue as \"eager execution not working with placeholders\" or any other related bug issue.\r\n\r\nProbably the TensorFlow developers need to work better at a perfectly running code sample without issues cropping up each time a default sample application is tested.", "Issue was resolved after installing the nightly build, for me. \r\n` pip3 install --upgrade tf-nightly-gpu --user`", "> user\r\n\r\nDid you mean eager exec works with placeholders after installing nightly-gpu?\r\n", "I am facing the same issue. I am converting my code to TF 2.0. After getting everything to the coherent state running the code I am getting \r\n`tf.placeholder() is not compatible with eager execution`\r\nWhich sounds a little funky, since I was not trying any eager execution at all. I am backwards and still trying to run in TF with a session.\r\n\r\nSo, the problem is 2-fold. First, the error message does not indicate an error. It states some fact, that might or might not be relevant to the situation. It seems to implicate that someone is trying the eager execution.\r\n\r\nSecond of all, it seems to be relevant to the migration script. The script was glad to convert my TF1.12 code to TF2.0 and now TF2.0 is not happy with the result.", "@yselivonchyk : You may want to file a new issue about this, particularly since this seems more about an issue with what the upgrade script should do rather than the fact that `eager execution doesn't work with placeholders`. ", "@yselivonchyk  Tensorflow 2.0 by default uses Eager-Execution. Hence Placeholders are not getting executed. \r\n\r\nJust put this line to deactivate the eager execution :\r\n\r\n# tf.compat.v1.disable_eager_execution()", "Thank you Mainak. The solution worked for me. \r\ntf.compat.v1.disable_eager_execution()", "Finally found the answer. It worked for me also.  I understand why too. Thanks Mainak431\r\ntf.compat.v1.disable_eager_execution()", "You could see the reason why it has a problem https://www.tensorflow.org/guide/eager there: since tensorflow 2.0 has the eager execution feature. If you still want to use the graphical feature in Tensorflow, you could always disable the eager execution based on suggestions from Mainak431"]}, {"number": 18164, "title": "Fix warning in rnn_cell.py", "body": "This fix fixes the warning in rnn_cell.py caused by l2_normalize with dim:\r\n```\r\nrnn_cell.py:2894: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 18163, "title": "cannot import name bayesflow Error", "body": "Hi, \r\nI get the error I mentioned in the title. I did a search on Google and I usually found a solution to update dask. I updated Dask to version 0.17.2 but I still get the same error. I can not import BayesFlow. The Tensorflow version is 0.12.1. Thanks for the answers ...\r\n\r\nCode :  \r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"/tmp/data/\")\r\n\r\nOS : Ubuntu 16.04 LTS\r\nTensorflow version is 0.12.1\r\nCuda : 8.0\r\nCuDNN : 5.1\r\nGPU : 4  GB GTX 1050Ti\r\nDask : 0.17.2\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Oh sorry, I updated my post. Thanks...", "0.12.1 is a very old TensorFlow release, prior to the 1.x series where we provide API stability guarantees.\r\n\r\nThis doesn't directly address your question, but is it possible for you to use a more recent (or at least a 1.x version) of TensorFlow?\r\n\r\nThanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 18162, "title": "contrib/image: minor spelling tweaks", "body": "", "comments": []}, {"number": 18161, "title": "Added tensorboard link in Readme file", "body": "There was not Tensorboard link in Readme file since added a nice one to introduce.", "comments": ["Hi @andrewharp \r\nCan you please look into this pull request?", "@rmlarsen \r\nThanks for approving :)\r\nThere is no change in code base. Can we go directly for merge? :)"]}, {"number": 18160, "title": "Power operator function instead of double star - cleanup", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLA issues since closing this PR"]}, {"number": 18159, "title": "Power operator function instead of double star", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@rmlarsen I am facing CLA issues, everything looks good. Can you please help me?"]}]