[{"number": 41620, "title": "Definition of dynamic-shape variable error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Conda repo\r\n- TensorFlow version (use command below): 1.15\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Tesla V100-SMX3-32GB\r\n\r\n**Describe the current behavior**\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [] rhs shape= [1,1]\r\n         [[{{node Variable/Assign}}]]\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\r\n\r\nwith tf.Session() as sess:\r\n    v = tf.Variable(np.zeros(shape=[1,1]),shape=tf.TensorShape(None))\r\n    sess.run(tf.global_variables_initializer())\r\n```\r\n\r\nObseration:\r\nThe error did not appear when I use eager_execution_mode()\r\n\r\nCode:\r\n```\r\ntf.enable_eager_execution()\r\nv = tf.Variable(np.zeros([1,1]),shape=tf.TensorShape(None))\r\ntf.print(v)\r\nv.assign(np.ones([2,2]))\r\ntf.print(v)\r\n```\r\nOutput:\r\n```\r\n[[0]]\r\n[[1 1]\r\n [1 1]]\r\n```\r\n\r\n\r\n[tensorshape_bug.log](https://github.com/tensorflow/tensorflow/files/4958503/tensorshape_bug.log)", "comments": ["Was able to reproduce the issue, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/3fe8345db4092d520246205be4b97948/41620.ipynb). Thanks!", "Just have to enable resource variable as dynamic shape behavior is only available for this 'updated' Variable class.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\r\ntf.compat.v1.enable_resource_variables()\r\n\r\nwith tf.Session() as sess:\r\n    v = tf.Variable(np.zeros(shape=[1,1]),shape=tf.TensorShape(None))\r\n    sess.run(tf.global_variables_initializer())\r\n```\r\n"]}, {"number": 41619, "title": "Error in loading tflite model at runtime from native C++ library.", "body": "**System information**\r\n\r\nOS Platform and Distribution (windows 10):\r\nMobile device (oppo reno2) i am building and running on this device through android studio.\r\nTensorFlow installed from (source or binary): Ubuntu terminal through pip3\r\nTensorFlow version:\r\nPython version: python 3.8.2\r\nInstalled using pip\r\nBazel version (if compiling from source): I have created libtensorflowlite.so using bazle(2.0.0) . Now trying to directly use the lib.\r\nGCC/Compiler version (if compiling from source): gnu++11\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n\r\n**Describe the current behavior**\r\nTrying to load trained model in native source code but getting an error \"Could not open '/deeplab/deeplabv3_257_mv_gpu.tflite'.\" I have built libtensorflowlite.so through bazel for arm64.\r\n\r\nCode:-\r\nJava_com_example_myapplication_MainActivity_stringFromJNI(JNIEnv *env, jobject thiz) {\r\n    string model_file = \"/deeplab/deeplabv3_257_mv_gpu.tflite\";\r\n    StderrReporter error_reporter;\r\n    unique_ptr<tflite::FlatBufferModel> model;\r\n\r\n    model = FlatBufferModel::BuildFromFile(model_file.c_str(),\r\n                                           &error_reporter);\r\n    ops::builtin::BuiltinOpResolver resolver;\r\n    std::unique_ptr<tflite::Interpreter> interpreter;\r\n    if(model){\r\n        InterpreterBuilder(*model, resolver)(&interpreter);\r\n        interpreter->AllocateTensors();\r\n    }else{\r\n        return env->NewStringUTF(\"Model is Null\");\r\n    }\r\n    return env->NewStringUTF(\"Hi from JNI LIBS!\");\r\n}\r\n![Code_1](https://user-images.githubusercontent.com/68632656/88143714-5a532780-cc15-11ea-972b-59deda7e0f6b.PNG)\r\n\r\nTried to keep model at different locations like inside cpp folder where source file is present or inside assets folder parallel to cpp folder but still same error is coming.\r\nAlso tried to put model in mobile sdcard  and passing that path but still no luck.\r\n**Describe the expected behavior**\r\nModel should load.\r\n\r\n**Other info / logs** \r\n\r\n2020-07-22 12:13:07.773 13653-13653/com.example.myapplication D/Main: loaded\r\n2020-07-22 12:13:07.875 13653-13653/com.example.myapplication E/tflite: Could not open '/deeplab/deeplabv3_257_mv_gpu.tflite'.\r\n\r\n", "comments": ["@sunilkumar-op Apologies for the delay in response. Is this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41619\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41619\">No</a>\n"]}, {"number": 41618, "title": "Gcs filesystem test part 1", "body": "@mihaimaruseac \r\nThis PR ports some tests from `//tensorflow/core/platform/cloud:gcs_file_system_test`", "comments": ["Internal build (clang compiler) fails with \r\n\r\n```\r\ntensorflow/c/experimental/filesystem/plugins/gcs/gcs_filesystem.cc:456:7: error: field 'block_size' will be initialized after field 'block_cache_lock' [-Werror,-Wreorder-ctor]\r\n      block_size(block_size),\r\n      ^\r\n1 error generated.\r\n```\r\n\r\nThe initializers on a constructor must come in the same order as the class members are declared", "Thank you !"]}, {"number": 41617, "title": "Doe tensorflow has api  similar to pytorch's \u201cmasked_fill_\u201d", "body": "pytorch has squeeze , tf also has squeeze.\r\npytorch has unsqueeze, tf has expand_dims do the same thing with diff name.\r\npythorh has masked_fill , tf has ?\r\n\r\n<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):  2.2.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n   [masked_fill_ api doc in pytorch](https://pytorch.org/docs/stable/tensors.html?highlight=masked_fill_#torch.Tensor.masked_fill_)\r\n**Will this change the current api? How?**\r\n   Maybe not. It will add a new api. \r\n**Who will benefit with this feature?**\r\n    someone try to migrate from pytorch to tensorflow like me.\r\n**Any Other info.**\r\n   ", "comments": ["@jason-liew \r\n\r\nCan you please go through [tf.boolean_mask](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) and see this is the feature you are looking for?\r\nThanks!", "No,it's different.\r\n\r\ntensorflow:\r\n```\r\n t1 = tf.constant([1,2,3])\r\n t1_mask = numpy.array([1,0,1])\r\n tf.boolean_mask(t1,t1_mask) # [1, 3]\r\n```\r\npytorch:\r\n```\r\np1 = torch.tensor([1,2,3])\r\np1_mask = torch.tensor([1,0,1])\r\np1.masked_fill_(p1_mask,5) # [5, 2, 5]\r\n```\r\n", "@jason-liew \r\n\r\nGot it.Thanks!\r\nAre you interested to submit a PR?", "@gowthamkpr Hi, I would like to work in this. Can anyone help with how and from where I can start ?\r\n ", "@gowthamkp Should we have to add a new api or we can also override boolean_mask with one more parameter, which one will be better?\r\n", "Strawman proposal. Can this be implemented as\r\n\r\ndef masked_fill(t, mask, value):\r\n  return t * (1 - tf.cast(mask, tf.float32)) + value * tf.cast(mask, tf.float32)\r\n\r\nMight have to do some dtype handling but roughly should work? What are the performance requirements for this? ", "@ravikyram @gowthamkpr please review this PR #41975 ", "```python\r\ntf.where([True, False, True],5,[1,2,3])\r\n```", "We recommend the tf.where implementation."]}, {"number": 41616, "title": "training hangs using `tf.keras.model.fit` on `tf.data.Dataset` in GAN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA 10.1.243/cuDNN 7.6.5\r\n- GPU model and memory: Tesla K80\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nafter training for any number of steps equivalent to processing roughly 200 images, the training hangs and does not progress. reports CPU usage to be at over 100% even though the GPU is available and being used.\r\n\r\n**Describe the expected behavior**\r\n\r\nthe training continues as normal at a fairly consistent pace.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n    ...\r\n    31/Unknown - 12s 380ms/step - psnr: 21.3111 - sharpdiff: 14.1349 - disc_loss: 0.6919 - gen_loss: 363.1030 \r\n    32/Unknown - 12s 379ms/step - psnr: 21.3856 - sharpdiff: 14.1562 - disc_loss: 0.6919 - gen_loss: 355.9599  \r\n    33/Unknown - 12s 379ms/step - psnr: 21.4397 - sharpdiff: 14.1673 - disc_loss: 0.6919 - gen_loss: 348.9806  \r\n    34/Unknown - 13s 380ms/step - psnr: 21.5334 - sharpdiff: 14.1921 - disc_loss: 0.6919 - gen_loss: 341.5822\r\n    <hangs here>\r\n\r\nthe call to `model.fit`:\r\n\r\n    checkpoint = tf.keras.callbacks.ModelCheckpoint \\\r\n    (\r\n        filepath = os.path.join(util.get_dir(\"save/checkpoints\"), \"checkpt\"),\r\n        save_weights_only = True,\r\n        save_freq = 25\r\n    );\r\n\r\n    image_save = callbacks.ImageSave(util.get_dir(\"save/images\"));\r\n\r\n    model = models.GAN(num_scales = 4, hist_len = 4, pred_len = 1);\r\n    model.compile \\\r\n    (\r\n        disc_optim = tf.keras.optimizers.Adam(learning_rate = 1e-6, beta_1 = 0.89, beta_2 = 0.99),\r\n        gen_optim = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1 = 0.89, beta_2 = 0.99),\r\n        disc_loss = losses.Adversarial(),\r\n        gen_loss = losses.Combined(),\r\n        metrics = [ metrics.PSNR(), metrics.SharpDiff() ],\r\n        run_eagerly = True\r\n    );\r\n\r\n    model.fit \\\r\n    (\r\n        dataset,\r\n        epochs = epochs,\r\n        callbacks = [ checkpoint, image_save ]\r\n    );\r\n\r\nthe losses, metrics, `GAN` model, and the `ImageSave` callback are custom (subclassed in keras), if it matters.", "comments": ["@aegooby,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!", "i fixed the issue - i was using `PIL.Image.open` to load my image files from animated PNGs, which would not load the files correctly all the time. i switched to using `np.savez_compressed` to save images", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41616\">No</a>\n"]}, {"number": 41615, "title": "Enable Transactions API", "body": "This PR enables Transactional FileSystem API C++ layer, added in previous PRs (#41431, #41433, #41434, #41434, #41462, #41463 and #41516)  by uncommenting extra arguments and running clang-format on modified files. Follow-up PRs will expose the API to python layers.", "comments": ["Sorry for the delay. This is failing internally since we have a different implementation for filesystems. So I need to apply some patches internally.\r\n\r\nHowever, before I do that, there are two linter issues that need to be handled\r\n\r\n- [x] having functions marked as both `virtual` and `override`. This I can fix on the manual import, it's not something that requires much work on import\r\n- [ ] having default arguments on virtual/override functions. This is [disabled by the style guide](https://google.github.io/styleguide/cppguide.html#Default_Arguments) and likely will make the manual import fail to be approved. I guess in the end we'll need to duplicate methods in `Filesystem` class, to have some that take the `token` argument (with no default, inserted in the signature before the output parameters) and some that don't take it (and call on the ones that do). This is a significant change and unfortunately I am unable to do it do to other time constraints.", "Hi @mihaimaruseac I can do the changes you asked but will you accept a big PR doing that or do we need to go with split PRs again?", "Let's do a big one since it's only function signatures mostly", "Hi @mihaimaruseac,\r\n\r\nI opened #41793. I think I will be able to split this into multiple steps.", "Closing this in favor of #41793 to workaround the issues described above."]}, {"number": 41614, "title": "Training with Keras mixed precision policy crashes.", "body": "@DEKHTIARJonathan and @nluehr for visibility.\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo - Followed stock example to write a CTL\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nNAME=\"Ubuntu\"\r\nVERSION=\"18.04.3 LTS (Bionic Beaver)\"\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n\r\n- TensorFlow installed from (source or binary):\r\ncontainer built on tensorflow/tensorflow:nightly-gpu\r\n\r\n- TensorFlow version (use command below):\r\nv1.12.1-37099-g3873154276 2.4.0-dev20200721\r\n\r\n- Python version:\r\nPython 3.6.9\r\n\r\n- CUDA/cuDNN version:\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\n- GPU model and memory:\r\nV100 32G\r\n\r\n**Describe the current behavior**\r\nUsing mixed precision training with keras mixed precision policy, seems like nodes aren't being casted to FP16\r\n\r\n```Traceback (most recent call last):\r\n  File \"run_tf_squad.py\", line 615, in <module>\r\n    main()\r\n  File \"run_tf_squad.py\", line 343, in main\r\n    model = TFElectraForQuestionAnswering.from_pretrained(electra_model, config=config, cache_dir=args.cache_dir, args=args)\r\n  File \"/workspace/electra/modeling_utils.py\", line 406, in from_pretrained\r\n    model(model.dummy_inputs, training=False)  # build the network with dummy inputs\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 986, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/workspace/electra/modeling.py\", line 811, in call\r\n    input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, training=training\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 986, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/workspace/electra/modeling.py\", line 278, in call\r\n    hidden_states = self.embeddings([input_ids, position_ids, token_type_ids, inputs_embeds], training=training)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 986, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"/workspace/electra/modeling.py\", line 82, in call\r\n    return self._embedding(inputs, training=training)\r\n  File \"/workspace/electra/modeling.py\", line 107, in _embedding\r\n    embeddings = inputs_embeds + position_embeddings + token_type_embeddings\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1126, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\", line 1448, in _add_dispatch\r\n    return gen_math_ops.add_v2(x, y, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 487, in add_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 6886, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2]\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\nMixed-precision training should start without any issues.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n1) git clone https://github.com/sharathts/electra.git\r\n2) cd electra\r\n3) bash scripts/docker/build.sh\r\n4) bash scripts/docker/launch.sh\r\n5) python run_tf_squad.py --init_checkpoint=None --do_train --train_batch_size=16    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case --electra_model=google/electra-base-discriminator  --learning_rate=4e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 4  --joint_head True  --null_score_diff_threshold -5.6  --output_dir=results/   --amp  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback has been provided in the \"Expected behaviour\" section.", "comments": ["@sharathts \r\nCan you please share simple indented stand alone code to replicate the issue faced or a colab gist to analyse.\r\n\r\nI see some similar issues:\r\nplease refer to [link](https://stackoom.com/question/3tkBY/InvalidArgumentError-%E6%97%A0%E6%B3%95%E8%AE%A1%E7%AE%97Mul%E4%B8%BA%E8%BE%93%E5%85%A5-%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B-%E5%BA%94%E8%AF%A5%E6%98%AFint-%E5%BC%A0%E9%87%8F-%E4%BD%86%E6%98%AF%E6%B5%AE%E7%82%B9%E5%BC%A0%E9%87%8F-Op-Mul-%E5%90%8D%E7%A7%B0-mul) and [link1](https://stackoverflow.com/questions/54255431/invalidargumenterror-cannot-compute-matmul-as-input-0zero-based-was-expected)\r\n\r\nalso the tf version used is not the later version, support is available from 1.15 and 2.x", "Stand alone file to repro the issue above\r\n\r\n```\r\n\r\n## NOTE: If keras policy declaration is moved after model = TFElectraEmbeddings(), code doesn't crash, but seems to simply run in FP32\r\n\r\nimport tensorflow as tf\r\ndef get_initializer(initializer_range=0.02):\r\n    \"\"\"Creates a `tf.initializers.truncated_normal` with the given range.\r\n    Args:\r\n        initializer_range: float, initializer range for stddev.\r\n    Returns:\r\n        TruncatedNormal initializer with stddev = `initializer_range`.\r\n    \"\"\"\r\n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\r\ndef shape_list(x):\r\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\r\n    static = x.shape.as_list()\r\n    dynamic = tf.shape(x)\r\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\r\nclass TFElectraEmbeddings(tf.keras.layers.Layer):\r\n    \"\"\"Construct the embeddings from word, position and token_type embeddings.\r\n    \"\"\"\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.vocab_size = 30522\r\n        self.embedding_size = 768\r\n        self.initializer_range = 0.02\r\n        self.max_position_embeddings = 384\r\n        self.type_vocab_size = 2\r\n        self.position_embeddings = tf.keras.layers.Embedding(\r\n            self.max_position_embeddings,\r\n            self.embedding_size,\r\n            embeddings_initializer=get_initializer(self.initializer_range),\r\n            name=\"position_embeddings\",\r\n        )\r\n        self.token_type_embeddings = tf.keras.layers.Embedding(\r\n            self.type_vocab_size,\r\n            self.embedding_size,\r\n            embeddings_initializer=get_initializer(self.initializer_range),\r\n            name=\"token_type_embeddings\",\r\n        )\r\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\r\n        # any TensorFlow checkpoint file\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(0.1)\r\n    def build(self, input_shape):\r\n        \"\"\"Build shared word embedding layer \"\"\"\r\n        with tf.name_scope(\"word_embeddings\"):\r\n            # Create and initialize weights. The random normal initializer was chosen\r\n            # arbitrarily, and works well.\r\n            self.word_embeddings = self.add_weight(\r\n                \"weight\",\r\n                shape=[self.vocab_size, self.embedding_size],\r\n                initializer=get_initializer(self.initializer_range),\r\n            )\r\n        super().build(input_shape)\r\n    def call(self, inputs, mode=\"embedding\", training=True):\r\n        \"\"\"Get token embeddings of inputs.\r\n        Args:\r\n            inputs: list of three int64 tensors with shape [batch_size, length]: (input_ids, position_ids, token_type_ids)\r\n            mode: string, a valid value is one of \"embedding\" and \"linear\".\r\n        Returns:\r\n            outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\r\n                shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\r\n                linear tensor, float32 with shape [batch_size, length, vocab_size].\r\n        Raises:\r\n            ValueError: if mode is not valid.\r\n        Shared weights logic adapted from\r\n            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\r\n        \"\"\"\r\n        if mode == \"embedding\":\r\n            return self._embedding(inputs, training=training)\r\n    def _embedding(self, inputs, training=False):\r\n        \"\"\"Applies embedding based on inputs tensor.\"\"\"\r\n        input_ids, position_ids, token_type_ids, inputs_embeds = inputs\r\n        if input_ids is not None:\r\n            input_shape = shape_list(input_ids)\r\n        else:\r\n            input_shape = shape_list(inputs_embeds)[:-1]\r\n        seq_length = input_shape[1]\r\n        if position_ids is None:\r\n            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\r\n        if token_type_ids is None:\r\n            token_type_ids = tf.fill(input_shape, 0)\r\n        if inputs_embeds is None:\r\n            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\r\n        position_embeddings = self.position_embeddings(position_ids)\r\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\r\n        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\r\n        embeddings = self.LayerNorm(embeddings)\r\n        embeddings = self.dropout(embeddings, training=training)\r\n        return embeddings\r\n# TensorFlow configuration\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    for gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, True)\r\npolicy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\", loss_scale=\"dynamic\")\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\nmodel = TFElectraEmbeddings()\r\ninput_ids = tf.convert_to_tensor([[1,2,3,4,5], [1,2,3,4,5]])\r\nposition_ids = None\r\ntoken_type_ids = tf.convert_to_tensor([[0,0,1,1,1], [0,0,1,1,1]])\r\ninput_embeds = None\r\ninputs = [input_ids, position_ids, token_type_ids, input_embeds]\r\nembeddings = model(inputs)\r\nprint(embeddings)\r\n```\r\n\r\nRE: also the tf version used is not the later version, support is available from 1.15 and 2.x\r\nI build on top of master but the issue is visible in TF 2.2.", "@sharathts \r\nDid you happen to check the links shared.", "@Saduf2019 the links you shared seems to be a completely different issue.\r\n\r\nSee Google Collab: https://colab.research.google.com/drive/14bfKOShVFEb1i4m3RE3SiCaJ2uG6e4cm?usp=sharing\r\n\r\nIt works perfectly without the Keras Policy (as shown in the Collab). However it fails with the mixed_precision precision. This is not an expected behavior.\r\n\r\n@reedwm FYI", "Thank you for filing the issue! The issue is caused by the fact `tf.keras.layers.Embedding` outputs a float32 tensor when mixed precision is used. Normally mixed precision layers output float16 tensors, as they cast their floating-point inputs to float16, but Embedding does not take floating-point inputs. Normally this isn't an issue, as the layer that consumes Embedding's output will cast to float16. However in this case, you directly call the `+` operator on the embedding output instead of passing it to another layer:\r\n\r\n```\r\nembeddings = inputs_embeds + position_embeddings + token_type_embeddings\r\n```\r\n\r\nYou can fix this by casting the embedding outputs to float16:\r\n\r\n```\r\nembeddings = inputs_embeds + tf.cast(position_embeddings, tf.float16) + tf.cast(token_type_embeddings, tf.float16)\r\n```\r\n\r\nAlternatively, you can create an `Add` layer in the constructor or `build` and call it, which will automatically cast inputs to float16.\r\n\r\nI'm considering making mixed precision `Embedding` layers output float16 tensors, since this behavior is clearly confusing. On the other hand, I'm hesitant to make a special case for Embedding when the general rule is \"inputs and variables are casted\". I'll make a decision by the time of the design doc.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41614\">No</a>\n", "This works. Thank you!"]}, {"number": 41613, "title": "detect.tflite failure converting TF model to TFlite", "body": "I am using my mac, running Catalina 10.15.5, with Python 2.7.17, running Tensorflow 1.15.3 from source, installed using pip, and my bazel version is 0.26.1.\r\n- GCC/Compiler version (if compiling from source): Unsure\r\n- CUDA/cuDNN version: N/A\r\n\r\nI am trying to convert my custom model on ssd_mobilenet_v3_small_coco to TFlite, following this tutorial: [Step 3](https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi) but cannot convert my tflite_graph.pb to detect.tflite using this line:\r\n`bazel run --config=opt tensorflow/lite/toco:toco -- --input_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/tflite_graph.pb --output_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --inference_type=FLOAT --allow_custom_ops`\r\n\r\nI keep running into this error (I only included the last bit because it's quite lengthy):\r\n```\r\nWARNING: /Users/jp3spinelli/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:proto_text_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/jp3spinelli/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:scanner.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nWARNING: /Users/jp3spinelli/tensorflow/tensorflow/core/BUILD:2455:12: in srcs attribute of cc_library rule //tensorflow/core:lib_internal_impl: please do not import '//tensorflow/core/lib/strings:strcat.cc' directly. You should either move the file to this package or depend on an appropriate rule there\r\nINFO: Analyzed target //tensorflow/lite/toco:toco (0 packages loaded, 0 targets configured).\r\nINFO: Found 1 target...\r\nTarget //tensorflow/lite/toco:toco up-to-date:\r\n  bazel-bin/tensorflow/lite/toco/toco\r\nINFO: Elapsed time: 0.472s, Critical Path: 0.00s\r\nINFO: 0 processes.\r\nINFO: Build completed successfully, 1 total action\r\nINFO: Running command line: bazel-bin/tensorflow/lite/toco/toco '--input_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/tflite_graph.pb' '--output_file=/Users/jp3spinelli/Desktop/models/research/object_detection/TFLite_model/detect.tflite' '--input_shapes=1,300,300,3' '--input_arrays=normalized_input_image_tensor' '--output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_DetectioINFO: Build completed successfully, 1 total action\r\n2020-07-21 18:03:47.675656: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TFLite_Detection_PostProcess\r\n2020-07-21 18:03:47.691874: F tensorflow/lite/toco/tooling_util.cc:1669] Check failed: input_array_dims[i] == input_array_proto.shape().dims(i) (320 vs. 300)\r\nAbort trap: 6\r\n```\r\nIt is creating a file, but it has zero bytes so I know something is wrong.\r\nI am working out of my tensorflow directory, which is in my home directory. One weird thing I noticed is that the path to my \"models\" folder on my Desktop says it starts in \"iCloud Drive\" not \"Users.\"\r\n\r\nPlease let me know how to fix this, I am new with coding so I need some step-by-step instructions. Thanks!", "comments": ["@jp3spinelli from your log\r\n```\r\nCheck failed: input_array_dims[i] == input_array_proto.shape().dims(i) (320 vs. 300)\r\n```\r\n\r\nPlease try `input_shapes=1,320,320,3` instead of `input_shapes=1,300,300,3`\r\n", "@freedomtan This worked! Thank you very much!!"]}, {"number": 41612, "title": "Where is Tensorflow Lite Support Library ", "body": "Here is the guide to Tensorflow Lite Support Library:\r\nhttps://www.tensorflow.org/lite/guide/lite_support\r\n\r\nAn exert:\r\n\r\n> The [TensorFlow Lite Android Support Library](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java) is designed to help process the input and output of TensorFlow Lite models, and make the TensorFlow Lite interpreter easier to use.\r\n\r\nWhich links to:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/support/java\r\n\r\nwhich is 404 Page not found.\r\n ", "comments": ["Found it.\r\nhttps://github.com/tensorflow/tflite-support\r\n\r\nYou can close this ticket if you want, I will leave it open because the guide should be updated.", "Thanks for reporting the broken links! Team will fix the links soon."]}, {"number": 41611, "title": "Per-op tf32 plumbing for GPUs (Stage1: API change)", "body": "This PR is related to the global setting of TensorFloat32 (TF32) for GPUs. Though TF32 can provide significant performance benefits, TensorFlow uses matmul operations in some contexts where full fp32 precision needs to be maintained. For these, we need an easy way to control precision in a per op basis. This PR adds an attribute \"allow_fast_math\" to the matmul/batchedmatmul op to control TF32/fp32. Correspondingly, the python APIs are also changed.\r\n\r\nWith that, we have three options for \"allow_fast_math\":\r\n- -1(None for python API): use global setting.\r\n- 0  (False): use fp32.\r\n- 1  (True): use TF32.\r\n\r\nNote: For now, this PR shouldn't affect the unit tests, since TF32 is not enabled globally by default and in the PR this attribute is set to -1 (following the global setting). When it is enabled in the future, we have another PR to fix those affected unit tests (which might fail due to reduced precision issues) based on this PR.\r\n \r\n@nluehr \r\n", "comments": ["For @tensorflow/api-owners:\r\n\r\n@reedwm and/or @sanjoy could you please review the functionality in this PR? thank you", "I'll defer the detailed review to @reedwm, but my two cents is that we should not use -1/0/+1 as a tri-state.  Instead I'd prefer:\r\n\r\n - Making the `OpDef` / `OpKernel` attribute be a string that can be one of `\"always\"`, `\"never\"`, `\"global\"` (i.e. string as a \"poor man's enum\").\r\n - Using a normal C++ enum type in the stream executor APIs.\r\n\r\nI have no opinions on how this should be represented in the Python API.", "@kaixih Can you please resolve conflicts? Thanks!", "As discussed over email, let's put this PR on hold for now. We are still unsure of any concrete cases where tf32 needs to be selectively enabled, but once we find one we can reconsider this PR.\r\n\r\nClosing for now but will reopen if we find cases where we want to selectively enable tf32."]}, {"number": 41610, "title": "Seed for dropout in LSTM - Difference in model(X) and model.predict(X)", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nOutputs for LSTM layer in tensorflow when using model(X) and model.predict(X) differ when using dropout.\r\n\r\nLet's call the output of model(X) as Fwd Pass and model.predict(X) as Prediction\r\n\r\nFor a regular dropout layer, we can specify the seed but LSTM layer doesn't have such an argument. I'm guessing this is causing the difference between these Fwd Pass and Prediction.\r\n\r\nIn the following code sample, if dropout=0.4, these the outputs are different but when dropout=0.0 they match exactly. This makes me believe that every evaluation is using a different operation level seed but there is no way to set that.\r\n\r\nPS: I want to use dropout during inference, so that is by design.\r\n\r\n**Describe the expected behavior**\r\n**Fwd Pass** and **Prediction** should be exactly the same when using dropout.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.initializers import GlorotUniform\r\n\r\nSEED = 200\r\nHIDDEN_UNITS = 4\r\nN_OUTPUTS = 1\r\nN_INPUTS = 4\r\nBATCH_SIZE = 4\r\nN_SAMPLES = 4\r\n\r\nnp.random.seed(SEED)\r\ntf.random.set_seed(SEED)\r\n\r\n\r\n# Simple LSTM Model\r\ndef my_model():\r\n    inputs = x = keras.Input(shape=(N_INPUTS, 1))\r\n    initializer = GlorotUniform(seed=SEED)\r\n\r\n    x = layers.LSTM(HIDDEN_UNITS,\r\n                     kernel_initializer=initializer,\r\n                     recurrent_dropout=0.0,\r\n                     dropout=0.4,\r\n                     # return_sequences=True,\r\n                     use_bias=False)(x, training=True)\r\n\r\n    output = x\r\n    model = keras.Model(inputs=inputs, outputs=[output])\r\n    return model\r\n\r\n# Create Sample Data\r\n# Target Function\r\ndef f_x(x):\r\n    y = x[:, 0] + x[:, 1] ** 2 + np.sin(x[:, 2]) + np.sin(x[:, 3] ** 3)\r\n    y = y[:, np.newaxis]\r\n    return y\r\n\r\n# Generate random inputs\r\nd = np.linspace(0.1, 1, N_SAMPLES)\r\nX = np.transpose(np.vstack([d*0.25, d*0.5, d*0.75, d]))\r\nX = X[:, :, np.newaxis]\r\nY = f_x(X)\r\n\r\n# PRINT FWD PASS\r\nmodel = my_model()\r\nn_out = model(X).numpy()\r\nprint('FWD PASS:')\r\nprint(n_out, '\\n')\r\n\r\n# PRINT PREDICT OUTPUT\r\nprint('PREDICT:')\r\nout = model.predict(X)\r\nprint(out)\r\n```\r\n\r\n**Output (dropout=0.4) - do not match**\r\n\r\n```\r\nFWD PASS:\r\n[[ 0.          0.          0.          0.        ]\r\n [ 0.          0.          0.          0.        ]\r\n [ 0.0526864  -0.13284351  0.02326298 -0.30357683]\r\n [ 0.06297918 -0.14084947  0.02214929 -0.44425806]] \r\n\r\nPREDICT:\r\n[[ 0.00975818 -0.029404    0.00678372 -0.03232396]\r\n [ 0.0347842  -0.0974849   0.01938616 -0.15696262]\r\n [ 0.          0.          0.          0.        ]\r\n [ 0.06297918 -0.14084947  0.02214929 -0.44425806]]\r\n```\r\n\r\n**Output (dropout=0.0) - no dropout, outputs match**\r\n\r\n```\r\nFWD PASS:\r\n[[ 0.00593475 -0.01799661  0.00424165 -0.01876264]\r\n [ 0.02226446 -0.06519517  0.01399653 -0.08595844]\r\n [ 0.03620889 -0.10084937  0.01987283 -0.1663805 ]\r\n [ 0.0475584  -0.12453148  0.02269932 -0.2541136 ]] \r\n\r\nPREDICT:\r\n[[ 0.00593475 -0.01799661  0.00424165 -0.01876264]\r\n [ 0.02226446 -0.06519517  0.01399653 -0.08595844]\r\n [ 0.03620889 -0.10084937  0.01987283 -0.1663805 ]\r\n [ 0.0475584  -0.12453148  0.02269932 -0.2541136 ]]\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["for further clarification, I am calling the LSTM layer with the argument `training=True`, so that dropout is enabled when `model.predict()` is called", "i am able to replicate this issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d4afdd07e202c47b5b4533e66a6b61eb/untitled292.ipynb)", "Was able to replicate the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/25b97830f9b53c0a386d96eb9a1305f6/untitled114.ipynb#scrollTo=VzPWy9TmOmC0)..Thanks !", "@lakshaykc I tried to run your code on TF 2.7 and got different result, please find the gist [here](https://colab.research.google.com/gist/kumariko/4be9813a59e5a1845f5572cd192b3d25/untitled114.ipynb#scrollTo=pWgavkVhOn85) and confirm the same.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41610\">No</a>\n"]}, {"number": 41609, "title": "TF_OpKernelConstruction_GetName", "body": "Added method to access NodeDef name during kernel computation for debugging purposes. \r\n\r\nAdded string_view struct to pass strings across the API without copying or changing ownership of memory for the underlying string. \r\n\r\n@annarev @bmzhao ", "comments": []}, {"number": 41608, "title": "Converted TFLite-model produces wrong results, while PB-model produces correct result", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- TensorFlow installed from (source or binary):  `pip install tf-nightly`\r\n- TensorFlow version (or github SHA if from source): 2.4.0-dev20200721\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\nColab (GPU is disabled): https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb\r\n\r\n```\r\ntoco --graph_def_file model-f46da743.pb --output_file model-f46da743.tflite --output_format TFLITE --inference_type FLOAT --inference_input_type FLOAT --input_arrays 0 --output_arrays 1195 --enable_v1_converter --target_ops=SELECT_TF_OPS\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-07-21 19:15:30.657556: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-21 19:15:32.252326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-21 19:15:32.255365: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2020-07-21 19:15:32.255412: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (71fb61714d13): /proc/driver/nvidia/version does not exist\r\n2020-07-21 19:15:32.255749: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-21 19:15:32.261434: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\r\n2020-07-21 19:15:32.261660: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1aeef40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-21 19:15:32.261724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nI0721 19:15:35.806807 139688388913024 lite.py:1321] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\r\n2020-07-21 19:15:36.648004: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n2020-07-21 19:15:36.648067: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n\r\n* [model-f46da743.pb](https://github.com/AlexeyAB/tensorflow-tools/releases/download/tmp_version/model-f46da743.pb)\r\n\r\n* [model-f46da743.tflite](https://github.com/AlexeyAB/tensorflow-tools/releases/download/tmp_version/model-f46da743.tflite)\r\n\r\n\r\n**Failure details**\r\nThe conversion is successful, but the generated model is wrong,\r\n- Producing wrong results\r\n\r\n**Any other info / logs**\r\n\r\nI convert model from ONNX to PB and it works well with `PB-model + TensorFlow`, it shows desired **good result**:\r\n![good](https://user-images.githubusercontent.com/4096485/88097044-43262280-cba0-11ea-8dc9-5b8fff654485.png)\r\n\r\nBut when I convert PB to TFLITE, it works with `TFLITE-model + TensorFlow`, but it shows unexpected **bad result**:\r\n![bad](https://user-images.githubusercontent.com/4096485/88097079-4f11e480-cba0-11ea-9ee0-600b5899d687.png)\r\n\r\n\r\nReprocudable commands and code (GPU is disabled): https://colab.research.google.com/gist/AlexeyAB/07e7aa3c9ab49f1d733153f64d6fd270/onnx_to_tf_to_tflite.ipynb\r\n", "comments": ["@AlexeyAB \r\n\r\nRequest you to grant me the access for the colab link. Thanks!", "@AlexeyAB Can't access your colab. I googled a bit, found what the model is for, and created a [run_tflite](https://github.com/freedomtan/MiDaS/blob/tflite/tf/run_tflite.py) python script derived from you `run_pb.py`. It seems the tflite model works.\r\n\r\nBTW, any reason why you want `--target_ops=SELECT_TF_OPS`? TFLite supports all the ops of this MiDaS model. I tried my script with both model with  `--target_ops=SELECT_TF_OPS` and without. Both of them work.", "@ravikyram \r\nYes, there was my little bug, now it works fine: https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb\r\n\r\nTFLite result:\r\n![tflite_result](https://user-images.githubusercontent.com/4096485/88172347-82e51c80-cc29-11ea-9f8a-99f49f2125d1.png)\r\n\r\n\r\n----\r\n\r\nBut why does it work well if GPU is disabled in Colab, but it doesn't work if GPU is enabled?\r\n\r\nI use the same PB-file in both cases:  https://github.com/intel-isl/MiDaS/releases/download/v2/model-f46da743.pb\r\nEverything is the same, except whether there is a GPU or not:\r\n\r\n* **GPU disabled**: (PB->TFLite) works well, and inference using TFLITE works well https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb\r\n\r\n* **GPU enabled**: (PB->TFLite) works well, but inference  using TFLITE failed on line `interpreter.invoke()` (**without any error message**) https://colab.research.google.com/gist/AlexeyAB/c1da5acb2be4496455d9433cfe861ef5/onnx_to_tf_to_tflite.ipynb\r\n\r\n", "@freedomtan  Thank you for helping me find a typo in my code!", "@AlexeyAB no problem. I like your YOLOv4.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41608\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41608\">No</a>\n", "@ymodak Hi, is there any progress? It seems that even this way now doesn't work **if we re-run it**: \r\n\r\n> GPU disabled: (PB->TFLite) works well, and inference using TFLITE works well https://colab.research.google.com/gist/AlexeyAB/59e8a5e4116e03858bc1fd6c7f1a8d5f/onnx_to_tf_to_tflite.ipynb"]}, {"number": 41607, "title": "spelling error", "body": "spelling error in release versions", "comments": []}, {"number": 41606, "title": "ImportError: cannot import name 'tensorflow' from 'opt_einsum.backends' ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:2.1\r\n- Python version:3.7.7\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1 cuda\r\n- GPU model and memory: GTX 1650\r\n\r\n\r\n\r\n**Describe the problem**\r\nAnother environment with tensorflow cpu has setup with no issues. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\extras\\CUPTI\\lib64;%PATH%\r\nSET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\include;%PATH%\r\nSET PATH=C:\\tools\\cuda\\bin;%PATH%\r\nI have set these variables too\r\n\r\n**Any other info / logs**\r\n>>> import tensorflow\r\n2020-07-22 00:42:00.297809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 85, in <module>\r\n    from tensorflow.python.ops.standard_ops import *\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\ops\\standard_ops.py\", line 48, in <module>\r\n    from tensorflow.python.ops.special_math_ops import *\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\ops\\special_math_ops.py\", line 30, in <module>\r\n    import opt_einsum\r\n  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\opt_einsum\\__init__.py\", line 9, in <module>\r\n    from .contract import contract, contract_path, contract_expression\r\n  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\opt_einsum\\contract.py\", line 10, in <module>\r\n    from . import backends, blas, helpers, parser, paths, sharing\r\n  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\opt_einsum\\backends\\__init__.py\", line 7, in <module>\r\n    from .dispatch import (get_func, has_einsum, has_tensordot, build_expression, evaluate_constants, has_backend)\r\n  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\opt_einsum\\backends\\dispatch.py\", line 13, in <module>\r\n    from . import tensorflow as _tensorflow\r\nImportError: cannot import name 'tensorflow' from 'opt_einsum.backends' (C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\opt_einsum\\backends\\__init__.py)\r\n", "comments": ["@surabhijain123,\r\nInstallation issues within the Anaconda environment are tracked in the Anaconda repo.\r\n\r\nCould you please submit a new issue using [this link](https://github.com/ContinuumIO/anaconda-issues/issues/new) and fill in the template, so that the issue can be tracked there. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41606\">No</a>\n"]}, {"number": 41605, "title": "Additional portable_optimized kernels plus associated test-suite improvements", "body": "Adds portable_optimized implementations of fully_connected, conv, and depthwise_conv ops to tflite(u)\r\n\r\nAssociated tflite(u) fixes/improvements:\r\n\r\n-  Test suite updated to run correctly on (at least x86) 32-bit targets\r\n-  Minor coverage/robustness improvements in test suite.\r\n-  MinGW target support \"windows_x86_makefile.inc\"\r\n", "comments": ["@andrewstevens-infineon  Can you please resolve conflicts? Thanks!", "@andrewstevens-infineon,  Any update on this PR? Please. Thanks!", "Returned from Vacation.   Resolving confilicts w.r.t. current master.", "@andrewstevens-infineon Can you please resolve conflicts? Thanks!", "Actually, before we ask @andrewstevens-infineon to do more work on this PR, I think it might be worth a conversation on the higher level goals of this work.\r\n\r\nTagging @petewarden to get his thoughts.", "@petewarden Any update on this PR? Please. Thanks!", "Hi Pete,\r\n\r\nI'd assume the PR was on Ice due to the Freeze - thus hadn't follow-up on it.The overall objective was to provide better support for lowest-end HW lacking any acceleration features/intrinsics library.   In our case a family of small RISC-V uC. \r\n\r\nIn the meantime the code has become starting point for so-far uncontributed/local features (packed sub 8-bit quantized weights, the pre-interpreter to reduce RAM/ROM footprint, custom HW support) .  This is actually our primary focus right now. However, if you feel the PR is worth pursuing I'll see if I can peek to resolve the conflicts with current master and update with back-ported/stripped \"portable_optimized\" versions of some of the more recent stuff.\r\n\r\n-Andrew\r\n", "Let's close the current PR.\r\n\r\nI think its probably worth having a conversation around what you guys are working on and how that can be merged upstream (if that is of interest to you). Maybe start with a discussion at the next SIG-micro meeting and go from there?", "Ok.\r\n\r\nLet's  put an item on the Agenda for next SIG.   \r\n\r\n- portable_optimized kernels (better support for uC missing dedicated kernels, especially low-end)\r\n- Progress on offline pre-interpreter/static code generator."]}, {"number": 41604, "title": "How can I run Tensorflow on one single core, single thread CPP?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to restrict the number of threads that TensorFlow spawns. In python, I understand we need to use the following steps as pointed out [Here](https://stackoverflow.com/questions/38187808/how-can-i-run-tensorflow-on-one-single-core) I was trying to do the same in CPP, but it doesn't seem that straight forward. \r\nQuestions:\r\nHow to modify intra_op_parallelism_threads and inter_op_parallelism_threads correctly?\r\nHow to modify the device_count to control the core as well?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nSessionOptions options;\r\n    ConfigProto* config = &options.config;\r\n    string key = \"CPU\";\r\n    //not sure if this is the correct way to do it.\r\n    (*config->mutable_device_count())[key] = 1; \r\n    config->set_inter_op_parallelism_threads(1);\r\n    config->set_intra_op_parallelism_threads(1);\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@amitpandey2194 \r\nCan you please refer to existing issue for the same reported here and let us know.\r\n#35784 [link](https://stackoverflow.com/questions/38187808/how-can-i-run-tensorflow-on-one-single-core) [link1](https://ask.cyberinfrastructure.org/t/tensorflow-script-to-run-on-single-core/225) ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41604\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41604\">No</a>\n"]}, {"number": 41603, "title": "Fix issue in tf.ones where tf.quint8/quint16 does not work in graph mode", "body": "This PR tries to address the issue where tf.ones where tf.quint8/quint16 does not work in graph mode:\r\n```\r\n>>> @tf.function(autograph=False)\r\n... def f():\r\n...     return tf.ones([], tf.qint16)\r\n...\r\n>>> f()\r\n...\r\n...\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 456, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/Library/Python/3.7/site-packages/tensorflow/python/framework/tensor_util.py\", line 336, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected qint16, got 1 of type 'int' instead.\r\n```\r\n\r\nThe reason is similiar to the internal error encountered in #41421.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 41602, "title": "Unable to import tensorflow 2.2.0 in conda environment", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.2.0\r\n- Python version:3.7.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nError Details - \r\n\r\n    ***---------------------------------------------------------------------------\r\n    ImportError                               Traceback (most recent call last)\r\n    ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n         57 \r\n    ---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n         59 \r\n    \r\n    ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n         27             return _mod\r\n    ---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n         29     del swig_import_helper\r\n    \r\n    ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n         23             try:\r\n    ---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n         25             finally:\r\n    \r\n    ~\\.conda\\envs\\mldeep\\lib\\imp.py in load_module(name, file, filename, details)\r\n        241         else:\r\n    --> 242             return load_dynamic(name, filename, file)\r\n        243     elif type_ == PKG_DIRECTORY:\r\n    \r\n    ~\\.conda\\envs\\mldeep\\lib\\imp.py in load_dynamic(name, path, file)\r\n        341             name=name, loader=loader, origin=path)\r\n    --> 342         return _load(spec)\r\n        343 \r\n    \r\n    ImportError: DLL load failed: The specified module could not be found.\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    ImportError                               Traceback (most recent call last)\r\n    <ipython-input-1-d6579f534729> in <module>\r\n    ----> 1 import tensorflow\r\n    \r\n    ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\__init__.py in <module>\r\n         39 import sys as _sys\r\n         40 \r\n    ---> 41 from tensorflow.python.tools import module_util as _module_util\r\n         42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n         43 \r\n    \r\n    ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n         48 import numpy as np\r\n         49 \r\n    ---> 50 from tensorflow.python import pywrap_tensorflow\r\n         51 \r\n         52 # Protocol buffers\r\n    \r\n    ~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n         67 for some common reasons and solutions.  Include the entire stack trace\r\n         68 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n    ---> 69   raise ImportError(msg)\r\n         70 \r\n         71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n    \r\n    ImportError: Traceback (most recent call last):\r\n      File \"C:\\Users\\Vinayak\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n        from tensorflow.python.pywrap_tensorflow_internal import *\r\n      File \"C:\\Users\\Vinayak\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n        _pywrap_tensorflow_internal = swig_import_helper()\r\n      File \"C:\\Users\\Vinayak\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n        _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n      File \"C:\\Users\\Vinayak\\.conda\\envs\\mldeep\\lib\\imp.py\", line 242, in load_module\r\n        return load_dynamic(name, filename, file)\r\n      File \"C:\\Users\\Vinayak\\.conda\\envs\\mldeep\\lib\\imp.py\", line 342, in load_dynamic\r\n        return _load(spec)\r\n    ImportError: DLL load failed: The specified module could not be found.\r\n    \r\n    \r\n    Failed to load the native TensorFlow runtime.\r\n    \r\n    ***\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am getting below error on importing tensorflow. The steps I followed were -\r\n> Installed anaconda \r\n> Created conda env and installed using conda command prompt.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@vinayakvaid \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "@ravikyram My processor is Intel i5 8th gen.\n\nI followed the instructions given on the tensorflow website only. Actually i earlier had tensorflow's 1.x version which was working fine. Recently only I had updated it to 2.2.0 to use keras where the issue has come.", "@vinayakvaid \r\n\r\nDid you install the latest MSVC redistributable from https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads .\r\nTensorflow 2.2 is compiled using MSVC 2019, which appears to require an additional DLL.\r\nYour CPU/Python is on 32 bits?\r\n\r\nThanks!", "Hi  @ravikyram \uff0c  I have compiled the c++ version of  r2.2 branch  tflite successfully.   use this \r\n> bazel build -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\nbazel build -c opt //tensorflow/lite:tensorflowlite\r\n\r\nBut it is x64 lib, How I can the x86 or win32  library.\r\n\r\n", "@Bluewind001 \r\n\r\nIf your workstation is x64, bazel generates a x64 binary by default.\r\n\"--cpu=x86\" option is needed to build x86 binary.", " @ravikyram, I try this:\r\n> bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\n\r\nbut, it tell me , no cpu x86 toolchain \r\n> ERROR: C:/users/administrator/_bazel_administrator/rqkipcky/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86'\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\n\r\nI try clean bazel info, but still get this error. \r\n", "@Bluewind001 \r\n\r\nCan you please raise a new issue by filling issue template .We can have discussion there and we can track the issue.Thanks!", "this issue #41765. Thanks! ", "hi i had the same issue of importing tensorflow  from conda command prompt  or jupyter  on my CPU .i already  successfully installed Anaconda and later install tensor flow successfully.i got the above error.latter i downloaded and installed msv140 dll still didn't work .i then latter downloaded\r\nMicrosoft Visual C++ 2015-2019 Redistributable (x64) and installed\r\nVC_redist.x64 version.\r\n i had the problem resolved afterwards. hope this help someone out-there with same problem.\r\ncheers", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41602\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41602\">No</a>\n"]}, {"number": 41601, "title": "S3 multi part copy", "body": "@mihaimaruseac \r\nThis PR add S3 `MultipartCopy` and its callback", "comments": ["@mihaimaruseac \nCould you check `import/copybara` ? Thank you. \n\nEdit: It passed. Thank you"]}, {"number": 41600, "title": "Pylint sanity check using docker stays in a hung state.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary):  -\r\n- TensorFlow version (use command below): -\r\n- Python version: -\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nWhen the following command is executed to perform sanity checks using a docker container, the pylint step just hangs and doesn't complete.\r\n\r\n```\r\n$  tensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh\r\n\r\n== omitted logs ==\r\n\r\n=== Sanity check step 2 of 15: do_pylint (Python 3 pylint) ===\r\n\r\nERROR_ALLOWLIST=\"^tensorflow/python/framework/function_test\\.py.*\\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\\.py.*\\[E0102.*function\\salready\\sdefined ^tensorflow/python/feature_column/feature_column_test\\.py.*\\[E0110.*abstract-class-instantiated ^tensorflow/contrib/layers/python/layers/feature_column\\.py.*\\[E0110.*abstract-class-instantiated ^tensorflow/contrib/eager/python/evaluator\\.py.*\\[E0202.*method-hidden ^tensorflow/contrib/eager/python/metrics_impl\\.py.*\\[E0202.*method-hidden ^tensorflow/contrib/rate/rate\\.py.*\\[E0202.*method-hidden ^tensorflow/python/training/tracking/tracking\\.py.*\\[E0202.*method-hidden ^tensorflow/python/platform/gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/keras/callbacks\\.py.*\\[E1133.*not-an-iterable ^tensorflow/python/keras/engine/base_layer.py.*\\[E0203.*access-member-before-definition ^tensorflow/python/keras/layers/recurrent\\.py.*\\[E0203.*access-member-before-definition ^tensorflow/python/kernel_tests/constant_op_eager_test.py.*\\[E0303.*invalid-length-returned ^tensorflow/python/keras/utils/data_utils.py.*\\[E1102.*not-callable ^tensorflow/python/autograph/.*_py3_test\\.py.*\\[E0001.*syntax-error ^tensorflow/python/keras/preprocessing/image\\.py.*\\[E0240.*Inconsistent method resolution \"\r\nRunning pylint on 2805 files with 8 parallel jobs...\r\n\r\n```\r\nIt just hangs in this state inside the docker container\r\n\r\n**Describe the expected behavior**\r\nHowever, when I run the check directly on my system (MAC), the stage passes:\r\n\r\n```\r\n$  ./tensorflow/tools/ci_build/ci_sanity.sh\r\n\r\n== omitted logs ==\r\n\r\n=== Sanity check step 2 of 15: do_pylint (Python 3 pylint) ===\r\n\r\nERROR_ALLOWLIST=\"^tensorflow/python/framework/function_test\\.py.*\\[E1123.*noinline ^tensorflow/python/platform/default/_gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/platform/default/_googletest\\.py.*\\[E0102.*function\\salready\\sdefined ^tensorflow/python/feature_column/feature_column_test\\.py.*\\[E0110.*abstract-class-instantiated ^tensorflow/contrib/layers/python/layers/feature_column\\.py.*\\[E0110.*abstract-class-instantiated ^tensorflow/contrib/eager/python/evaluator\\.py.*\\[E0202.*method-hidden ^tensorflow/contrib/eager/python/metrics_impl\\.py.*\\[E0202.*method-hidden ^tensorflow/contrib/rate/rate\\.py.*\\[E0202.*method-hidden ^tensorflow/python/training/tracking/tracking\\.py.*\\[E0202.*method-hidden ^tensorflow/python/platform/gfile\\.py.*\\[E0301.*non-iterator ^tensorflow/python/keras/callbacks\\.py.*\\[E1133.*not-an-iterable ^tensorflow/python/keras/engine/base_layer.py.*\\[E0203.*access-member-before-definition ^tensorflow/python/keras/layers/recurrent\\.py.*\\[E0203.*access-member-before-definition ^tensorflow/python/kernel_tests/constant_op_eager_test.py.*\\[E0303.*invalid-length-returned ^tensorflow/python/keras/utils/data_utils.py.*\\[E1102.*not-callable ^tensorflow/python/autograph/.*_py3_test\\.py.*\\[E0001.*syntax-error ^tensorflow/python/keras/preprocessing/image\\.py.*\\[E0240.*Inconsistent method resolution \"\r\nRunning pylint on     2805 files with 16 parallel jobs...\r\n\r\n\r\npylint took 77 s\r\n\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nRunning the above-mentioned commands should reproduce the issue\r\n\r\n** Supporting logs in attachments **\r\n1. sanity_pass.log -> run on MAC.\r\n2. sanity_fail.log -> run inside docker as per the above-mentioned command where the process hangs.\r\n\r\n[sanity_pass.log](https://github.com/tensorflow/tensorflow/files/4955013/sanity_pass.log)\r\n[sanity_fail.log](https://github.com/tensorflow/tensorflow/files/4955015/sanity_fail.log)\r\n", "comments": ["Hi, any update on this issue? @ymodak ", "@angerson , mentioning for notification.", "The issue seems to be with the pylint version that was being used. \r\nWith pylint==1.6.4 the sanity check step2 hangs, but with the latest version of pylint==2.5.3, it completes successfully.\r\n\r\nWill raise a PR to fix this."]}, {"number": 41599, "title": "Fix markdown bullet list in keras.models.save_model docstring", "body": "Removes the four spaces of indent on a bullet list, which was causing it to render as a code block on [the documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).\r\n\r\n### Old output:\r\n\r\n> The saved model contains:\r\n>\r\n>     - the model's configuration (topology)\r\n>     - the model's weights\r\n>     - the model's optimizer's state (if any)\r\n\r\n### New output:\r\n\r\n> The saved model contains:\r\n>\r\n> - the model's configuration (topology)\r\n> - the model's weights\r\n> - the model's optimizer's state (if any)", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41599) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41599) for more info**.\n\n<!-- ok -->"]}, {"number": 41598, "title": "Failed to load the native TensorFlow runtime", "body": "Python 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\r\nType \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\nIPython 7.12.0 -- An enhanced Interactive Python.\r\n\r\nruncell(0, 'C:/Debasish/personal/DS_city/Practice with Dataset_Mine/NLP Code/untitled0.py')\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Debasish\\personal\\DS_city\\Practice with Dataset_Mine\\NLP Code\\untitled0.py\", line 8, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\edebago\\Anaconda3\\Anaconda_new3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@Debasish2003 \r\nPlease refer to [this link](https://github.com/tensorflow/tensorflow/issues/36683#issuecomment-585097726) and let us know.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41598\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41598\">No</a>\n"]}, {"number": 41597, "title": "Added tests for unsupported types in UniqueDataset", "body": "This PR extends the testing of the `tf.data.experimental.unique()` functionality by adding test cases which address the following:\r\n\r\n- [x] Validates whether proper errors are raised when unsupported data types are given as input.\r\n", "comments": ["Tagging @aaudiber  for notification.", "@aaudiber , committed the changes and updated the PR description as well. Please review. Thanks.", "@aaudiber thanks for pointing out the `[]` scenario. I ran the tests again and figured out a few things that I missed last time. I have documented the details and the reasons for populating the expected output so that it will be helpful for future reference. Please let me know.", "@aaudiber Refactoring the tests helped in cleaning up the clutter. Also, I added an additional test to check whether the proper errors are raised with unsupported dtypes as well. Please review and let me know. Thanks.", "@aaudiber  As per this stack trace, I guess for the int types, it is internally falling back to NumPy types. Other comments have been addressed. \r\n\r\nSTRING CASE:\r\n```======================================================================\r\nERROR: testStringTypeMismatch_test_mode_graph_tfapiversion_2 (__main__.UniqueTest)\r\ntestStringTypeMismatch_test_mode_graph_tfapiversion_2 (__main__.UniqueTest)\r\ntestStringTypeMismatch_test_mode_graph_tfapiversion_2(mode='graph', tf_api_version=2)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Unsupported object type int\r\n\t [[{{node PyFunc}}]]\r\n\t [[IteratorGetNext]]\r\n```\r\n\r\n\r\nINT CASE:\r\n```\r\n=====================================================================\r\nERROR: testIntTypeMismatch_test_mode_graph_tfapiversion_2_dtype_dtypeint64 (__main__.UniqueTest)\r\ntestIntTypeMismatch_test_mode_graph_tfapiversion_2_dtype_dtypeint64 (__main__.UniqueTest)\r\ntestIntTypeMismatch_test_mode_graph_tfapiversion_2_dtype_dtypeint64(mode='graph', tf_api_version=2, dtype=tf.int64)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1350, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1443, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int64, but the yielded element was foo.\r\nTraceback (most recent call last):\r\n\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/data/ops/dataset_ops.py\", line 843, in generator_py_func\r\n    ret, dtype=dtype.as_numpy_dtype))\r\n\r\n  File \"/private/var/tmp/_bazel_vignesh/05976bfb96706d6ac2ba2db31e669ffb/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/python/data/experimental/kernel_tests/unique_test.runfiles/org_tensorflow/tensorflow/python/ops/script_ops.py\", line 204, in _convert\r\n    result = np.asarray(value, dtype=dtype, order=\"C\")\r\n\r\n  File \"/Users/vignesh/.tf-tf-venv/lib/python3.7/site-packages/numpy/core/_asarray.py\", line 83, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n```", "Are you sure the tests are actually covering unique_dataset? It looks like the error occurs in the generator dataset, before reaching the unique dataset.", "@aaudiber , looking at it from that perspective, I think yes, seems like we are mostly ensuring the dataset creation aspect raises the error and not the unique transformation on top of it. Do you think I should move the tests someplace else and keep only the `testUnsupportedTypes` test here ?", "@aaudiber the errors were being raised even after removing the `unique` transformation on the dataset. So it didn't make sense to put those tests here. Also, I checked tests in `/tensorflow/python/data/kernel_tests/from_generator_test.py` and saw that they are already covering the `dtype` mismatch scenarios.  As a result, I removed the type mismatch tests from this file and retained only the unsupported type error check test.\r\n", "@aaudiber removed the import. Thanks for being patient throughout our long conversation. It was of great help. ", "@gbaned , can you please help with the import/copybara failure. Thanks"]}, {"number": 41596, "title": "Failed to load the native tensorflow runtime", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2.2\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: GTX 1650\r\n\r\n\r\n\r\n**issue while importing**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\ninstalled cuda 10.1\r\ncudnn\r\nfollowing command:\r\npip install --ignore-installed --upgrade tensorflow-gpu\r\n\r\n\r\n**Any other info / logs**\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\r\n   * For TF-GPU - See point 1\r\n   * For TF-CPU - See point 2\r\n-----------------------------------------------------------------------------------------------\r\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\r\n\r\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\r\n| TF  | CUDA |\r\n| :-------------: | :-------------: |\r\n| 2.1.0 - 2.2.0  | 10.1 |\r\n| 1.13.1 - 2.0  | 10.0  |\r\n| 1.5.0 - 1.12.0 | 9.0 |\r\n\r\n  * If you have above configuration and using _**Windows**_ platform -\r\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\r\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\r\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\r\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\r\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\r\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\r\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\r\n\r\n-----------------------------------------------------------------------------------------------\r\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\r\n\r\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\n   * Try Google Colab to use TensorFlow.\r\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\r\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n      * All you need is a good internet connection and you are all set.\r\n   * Try to build TF from sources by changing CPU optimization flags.\r\n\r\n*Please let us know if this helps.*\r\n", "@surabhijain123 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issue #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "Tensorflow-gpu is installed using conda but running a program gives following output as error:\r\n\r\nFigures now render in the Plots pane by default. To make them also appear inline in the Console, uncheck \"Mute Inline Plotting\" under the Plots pane options menu. \r\n\r\n\r\n \r\n2020-07-23 00:20:27.296592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.601086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-23 00:25:43.698117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-07-23 00:25:43.700705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.706002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-23 00:25:43.709851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-23 00:25:43.711804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-23 00:25:43.716109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-23 00:25:43.719046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-23 00:25:43.725904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:43.727008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-07-23 00:25:43.728448: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-07-23 00:25:43.731493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-07-23 00:25:43.732498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.733014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-23 00:25:43.733525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-23 00:25:43.734032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-23 00:25:43.734545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-23 00:25:43.735064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-23 00:25:43.735583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:43.736528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-07-23 00:25:44.352026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-23 00:25:44.352581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-07-23 00:25:44.352914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-07-23 00:25:44.353923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2917 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 128, 128, 3) 0                                            \r\n__________________________________________________________________________________________________\r\nlambda (Lambda)                 (None, 128, 128, 3)  0           input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv2d (Conv2D)                 (None, 128, 128, 16) 448         lambda[0][0]                     \r\n__________________________________________________________________________________________________\r\ndropout (Dropout)               (None, 128, 128, 16) 0           conv2d[0][0]                     \r\n__________________________________________________________________________________________________\r\nconv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        dropout[0][0]                    \r\n__________________________________________________________________________________________________\r\nmax_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           conv2d_1[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \r\n__________________________________________________________________________________________________\r\ndropout_1 (Dropout)             (None, 64, 64, 32)   0           conv2d_2[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        dropout_1[0][0]                  \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_3[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \r\n__________________________________________________________________________________________________\r\ndropout_2 (Dropout)             (None, 32, 32, 64)   0           conv2d_4[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       dropout_2[0][0]                  \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_5[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \r\n__________________________________________________________________________________________________\r\ndropout_3 (Dropout)             (None, 16, 16, 128)  0           conv2d_6[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      dropout_3[0][0]                  \r\n__________________________________________________________________________________________________\r\nmax_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_7[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \r\n__________________________________________________________________________________________________\r\ndropout_4 (Dropout)             (None, 8, 8, 256)    0           conv2d_8[0][0]                   \r\n__________________________________________________________________________________________________\r\nconv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      dropout_4[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_transpose (Conv2DTranspo (None, 16, 16, 128)  131200      conv2d_9[0][0]                   \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 16, 16, 256)  0           conv2d_7[0][0]                   \r\n                                                                 conv2d_transpose[0][0]           \r\n__________________________________________________________________________________________________\r\nconv2d_10 (Conv2D)              (None, 16, 16, 128)  295040      concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\ndropout_5 (Dropout)             (None, 16, 16, 128)  0           conv2d_10[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      dropout_5[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_transpose_1 (Conv2DTrans (None, 32, 32, 64)   32832       conv2d_11[0][0]                  \r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 32, 32, 128)  0           conv2d_5[0][0]                   \r\n                                                                 conv2d_transpose_1[0][0]         \r\n__________________________________________________________________________________________________\r\nconv2d_12 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_1[0][0]              \r\n__________________________________________________________________________________________________\r\ndropout_6 (Dropout)             (None, 32, 32, 64)   0           conv2d_12[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       dropout_6[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_transpose_2 (Conv2DTrans (None, 64, 64, 32)   8224        conv2d_13[0][0]                  \r\n__________________________________________________________________________________________________\r\nconcatenate_2 (Concatenate)     (None, 64, 64, 64)   0           conv2d_3[0][0]                   \r\n                                                                 conv2d_transpose_2[0][0]         \r\n__________________________________________________________________________________________________\r\nconv2d_14 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_2[0][0]              \r\n__________________________________________________________________________________________________\r\ndropout_7 (Dropout)             (None, 64, 64, 32)   0           conv2d_14[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        dropout_7[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_transpose_3 (Conv2DTrans (None, 128, 128, 16) 2064        conv2d_15[0][0]                  \r\n__________________________________________________________________________________________________\r\nconcatenate_3 (Concatenate)     (None, 128, 128, 32) 0           conv2d_1[0][0]                   \r\n                                                                 conv2d_transpose_3[0][0]         \r\n__________________________________________________________________________________________________\r\nconv2d_16 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_3[0][0]              \r\n__________________________________________________________________________________________________\r\ndropout_8 (Dropout)             (None, 128, 128, 16) 0           conv2d_16[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        dropout_8[0][0]                  \r\n__________________________________________________________________________________________________\r\nconv2d_18 (Conv2D)              (None, 128, 128, 1)  17          conv2d_17[0][0]                  \r\n==================================================================================================\r\nTotal params: 1,941,105\r\nTrainable params: 1,941,105\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\nTrain on 603 samples, validate on 67 samples\r\nEpoch 1/100\r\n\r\n2020-07-23 00:20:27.296592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.601086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-23 00:25:43.698117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-07-23 00:25:43.700705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.706002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-23 00:25:43.709851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-23 00:25:43.711804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-23 00:25:43.716109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-23 00:25:43.719046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-23 00:25:43.725904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:43.727008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-07-23 00:25:43.728448: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-07-23 00:25:43.731493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-07-23 00:25:43.732498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.733014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-23 00:25:43.733525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-23 00:25:43.734032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-23 00:25:43.734545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-23 00:25:43.735064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-23 00:25:43.735583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:43.736528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-07-23 00:25:44.352026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-23 00:25:44.352581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-07-23 00:25:44.352914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-07-23 00:25:44.353923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2917 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-07-23 00:25:46.339563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n 16/603 [..............................] - ETA: 3:23WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \r\nTraceback (most recent call last):\r\n\r\n  File \"F:\\Projects\\Semantic_seg\\Segmentation\\untitled0.py\", line 136, in <module>\r\n    result=model.fit(X_train, Y_train,validation_split=0.1, batch_size=16, epochs=100, callbacks=callbacks)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 632, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\Lenovo\\anaconda3\\envs\\tf-gpu-cuda10\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nUnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node model/conv2d/Conv2D (defined at F:\\Projects\\Semantic_seg\\Segmentation\\untitled0.py:136) ]] [Op:__inference_distributed_function_3807]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node model/conv2d/Conv2D:\r\n model/lambda/truediv (defined at F:\\Projects\\Semantic_seg\\Segmentation\\untitled0.py:61)\r\n\r\nFunction call stack:\r\ndistributed_function\r\n\r\n\r\n\r\n2020-07-23 00:20:27.296592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.601086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-23 00:25:43.698117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-07-23 00:25:43.700705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.706002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-23 00:25:43.709851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-23 00:25:43.711804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-23 00:25:43.716109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-23 00:25:43.719046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-23 00:25:43.725904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:43.727008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-07-23 00:25:43.728448: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-07-23 00:25:43.731493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5\r\ncoreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2020-07-23 00:25:43.732498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-23 00:25:43.733014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-07-23 00:25:43.733525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-23 00:25:43.734032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-23 00:25:43.734545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-23 00:25:43.735064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-07-23 00:25:43.735583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:43.736528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-07-23 00:25:44.352026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-23 00:25:44.352581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-07-23 00:25:44.352914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-07-23 00:25:44.353923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2917 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-07-23 00:25:46.339563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-07-23 00:25:50.484064: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-07-23 00:25:50.491286: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\r\n2020-07-23 00:25:50.498779: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node model/conv2d/Conv2D}}]]\r\n2020-07-23 00:25:50.639169: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\r\n2020-07-23 00:25:50.640311: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs\r\n2020-07-23 00:25:50.642950: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found\r\n2020-07-23 00:25:50.643768: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-07-23 00:25:50.644897: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-07-23 00:25:50.687514: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\r\n2020-07-23 00:25:50.688439: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n", "@surabhijain123 \r\n\r\nDid you try with tested build configuration from [here](https://www.tensorflow.org/install/source_windows#gpu) and see if you are facing the issue.\r\nDid you install the latest MSVC redistributable from https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads .\r\nTensorflow 2.2 is compiled using MSVC 2019, which appears to require an additional DLL.\r\nYour CPU/Python is on 32 bits?\r\n\r\nThis issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda.](https://github.com/ContinuumIO/anaconda-issues/issues)\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41596\">No</a>\n"]}, {"number": 41595, "title": "Fix binary crossentropy double epsilon", "body": "When I compute a binary crossentropy with Keras betweeen a target value == 1 and a prediction value == 0, the crossentropy formula contains a `log(0)` which does not exist and is avoided adding a small epsilon value. \r\n\r\nThe epsilon has always been `1e-7` so far, and so the numerical value of the binary crossentropy in the case described above is supposed to be equal to `-log(1e-7) == 16.11809539794922 `\r\n\r\nIn previous versions of Keras this was true. However, on (at least) tensorflow 2.2, it's equal to `15.424948470398375` which is the same as `2e-7`. This is because the binary crossentropy code:\r\n\r\n1. clips the predictions between `(epsilon, 1-epsilon)`\r\n2. adds an epsilon when computing the log\r\n\r\nThis results in a clipping between `(2*epsilon, 1)` which I believe is not the expected behaviour. \r\n\r\nThis pull request removes the second step.\r\n\r\nHere is a snippet to reproduce:\r\n\r\n```python\r\nfrom tensorflow.keras.losses import BinaryCrossentropy\r\nfrom tensorflow.keras import backend\r\nimport numpy as np\r\n\r\nloss = BinaryCrossentropy()\r\nprint(backend.eval(loss(np.array([1.]), np.array([0.]))))\r\n```\r\n\r\nIf this is indeed a bug, I can also open an issue for easier traceability.", "comments": ["Here are the internal errors, @fhennecker can you please verify ?\r\n\r\nTraceback (most recent call last):\r\n  File \"<embedded stdlib>/unittest/case.py\", line 59, in testPartExecutor\r\n    yield\r\n  File \"<embedded stdlib>/unittest/case.py\", line 605, in run\r\n    testMethod()\r\n  File \"/third_party/py/absl/testing/parameterized.py\", line 282, in bound_param_test\r\n    return test_method(self, **testcase_params)\r\n  File \"//tensorflow/python/framework/test_combinations.py\", line 315, in decorated\r\n    execute_test_method()\r\n  File \"//tensorflow/python/framework/test_combinations.py\", line 298, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"//tensorflow/python/keras/losses_test.py\", line 766, in test_sample_weighted\r\n    self.assertAlmostEqual(self.evaluate(loss), 4.6, 3)\r\n  File \"<embedded stdlib>/unittest/case.py\", line 878, in assertAlmostEqual\r\n    raise self.failureException(msg)\r\nAssertionError: 4.782716 != 4.6 within 3 places", "@fhennecker, Any update on this PR? Please. Thanks!", "@pavithrasv @gbaned sorry was away for a couple of weeks.\r\n\r\nI changed the expected values for all binary crossentropy tests that were affected and updated explanatory comments. They were pretty explicitly \"using\" double epsilons, I hope the behaviour I'm proposing is still the expected one.", "Thank you @fhennecker . The change looks correct to me, however since this could be a big breaking change let me run a few tests to see how much impact this has. ", "As suspected this will be a breaking change breaking a lot of other Google users. I will need to make all of these changes together. Will make sure to keep this PR updated.", "@pavithrasv  Any update on this PR? Please. Thanks!", "@gbaned Sorry about the delay, i have been busy working on other items. I will try to make this change sometime within the next couple of weeks.", "@pavithrasv Any update on this PR? Please. Thanks!", "@gbaned @pavithrasv Hello! Any news on this?", "It seems we're having trouble merging this PR due to test failures. Could you take a look?", "Here are the internal errors, @fhennecker can you please verify ?\r\n\r\nTraceback (most recent call last):\r\n  File \"/py/absl/testing/parameterized.py\", line 314, in bound_param_test\r\n    return test_method(self, **testcase_params)\r\n  File \"/tensorflow/python/framework/test_combinations.py\", line 367, in decorated\r\n    execute_test_method()\r\n  File \"/tensorflow/python/framework/test_combinations.py\", line 350, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"/tensorflow/python/keras/losses_test.py\", line 931, in test_ragged_tensors\r\n    self.assertAlmostEqual(self.evaluate(loss), 3.0666, 3)\r\nAssertionError: 3.1884775 != 3.0666 within 3 places", "@fhennecker Any update on this PR? Please. Thanks!", "Will try to look at it today, or next week.", "@gbaned @fchollet Should be fixed now -- I also rebased on master", "There are ongoing test failures -- please fix.", "I fixed what I could fix, but if the tests fail again I might need a bit of help. I couldn't access the details of the failures for some of the github checks, and I ran into quite a lot of trouble trying to run the tests locally, but at least the Ubuntu CPU check should be fixed. ", "Ok, so it looks like there are 3 last failing steps: `import/copybara`, and both Windows Bazel builds. I have tried looking into why they're failing but have no idea how to fix them (and I don't have a Windows machine at hand). Do you guys have any pointers for this?", "The PR says \"merged\", but in practice it isn't merged. What happened?"]}, {"number": 41593, "title": "label_map_util", "body": "It was opened by mistake. I haven't problem. ", "comments": ["@kadirnar \r\nCan you please fill the issue template with the issue faced  along with the tf version."]}, {"number": 41592, "title": "Refactor gcs part 2 random access file", "body": "@mihaimaruseac \r\nThis PR add `stat_cache` for `NewRandomAccessFile` and `LoadBufferFromGCS`. It also fixes some problems related to `TF_OUT_OF_RANGE` because the `core` implementation does not consider `TF_OUT_OF_RANGE` as an error.\r\n\r\nNow we have to wait for the next release of `google-cloud-cpp`. That release will contain the features needed for compose mode of `NewAppendableFile` and some gcs filesystem ops ( `Stat`, `GetChildren` )\r\n\r\nIn the mean time, I will send you some PRs to add tests ported from `core` implementation.", "comments": []}, {"number": 41591, "title": "TF 2.2.0 with cuda 11.0 Gpu test returns False", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.4 (3.7 too?)\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0/8.0\r\n- GPU model and memory: Geforce GTX 980 Ti\r\n\r\n\r\n\r\nI installed the latest Nvidia Driver 451.67, Visual Studio 2019 version 16.6.4, tensorflow-gpu 2.2.0rc2. Then I downloaded and installed cuda 11.0 and cuDNN8.0.1 RC2 for cuda11.0. I'm able to verify the installation and run the sample programs. The Problem arises when I try to test it in anaconda:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n2020-07-20 23:29:09.430847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n>>> tf.__version__\r\n'2.2.0'\r\n>>> print(tf.test.is_gpu_available())\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-07-20 23:29:13.406332: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-07-20 23:29:13.418916: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2706c6031b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-20 23:29:13.422382: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-20 23:29:13.425719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-20 23:29:13.456071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:03:00.0 name: GeForce GTX 980 Ti computeCapability: 5.2\r\ncoreClock: 1.228GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 313.37GiB/s\r\n2020-07-20 23:29:13.461916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-20 23:29:13.467883: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-07-20 23:29:13.474696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-20 23:29:13.479956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-20 23:29:13.490763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-20 23:29:13.493807: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-07-20 23:29:13.498571: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-07-20 23:29:13.501335: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-20 23:29:13.601860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-20 23:29:13.606065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n2020-07-20 23:29:13.609379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2020-07-20 23:29:13.615932: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2707213a850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-20 23:29:13.618721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 980 Ti, Compute Capability 5.2\r\nFalse\r\n```\r\nAnd:\r\n```\r\n>>> tf.config.list_physical_devices('GPU')\r\n2020-07-21 13:18:04.066692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-07-21 13:18:04.096830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:03:00.0 name: GeForce GTX 980 Ti computeCapability: 5.2\r\ncoreClock: 1.228GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 313.37GiB/s\r\n2020-07-21 13:18:04.101741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-07-21 13:18:07.858480: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-07-21 13:18:07.937031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-07-21 13:18:07.981968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-07-21 13:18:08.309451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-07-21 13:18:08.314100: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-07-21 13:18:08.320636: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-07-21 13:18:08.325392: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```\r\n                     \r\nI uninstalled tensorflow and tensorflow-gpu and installed it again in anaconda (base) but still get the same msg. Can someone tell me why it is looking for outdated dll? I've read that I could manually add dlls but doing so is not advised. Does anyone have a solution to this?\r\n\r\nI installed python 3.8.4 from the python website but anaconda gives me this: (base) PS C:\\Windows\\system32> python --version Python 3.7.6. Probably not related, just wanted to mention it. Thank you!", "comments": ["@DataPrincess123 \r\n\r\nCan you try with tested build configuration from [here](https://www.tensorflow.org/install/source_windows#gpu) and see if the error still persists.Thanks!", "> @DataPrincess123\r\n> \r\n> Can you try with tested build configuration from [here](https://www.tensorflow.org/install/source_windows#gpu) and see if the error still persists.Thanks!\r\n\r\nThank you, that worked!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41591\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41591\">No</a>\n", "even I am getting the same error and have checked the version of all of them"]}, {"number": 41590, "title": "Jpeg decoding (for example when loading TFRecords from files) causes error on TPU when trying to fit a model", "body": "**System information**\r\n- TensorFlow version (use command below): 2.2.0 (v2.2.0-0-g2b96f3662b)\r\n- Python version: 3.6.9\r\n- GPU model and memory: Google Colab TPU\r\n\r\nI'm not sure that this is a bug, but I've encountered this weird behaviour with my .tfrec dataset and made simple code to reproduce it. This problem only exists at TPU.\r\n\r\nFirstly I initialize TPU:\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.get_logger().propagate = False\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n```\r\n```\r\nINFO:tensorflow:Initializing the TPU system: grpc://10.26.115.226:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\n```\r\nThen I run the following code, which creates tf.data.Dataset of dummy images, encodes it to jpeg and back, then normalizes to float32 and makes batches.\r\n```\r\nwith strategy.scope():\r\n\r\n  def encode_jpg(image, class_idx):\r\n    return tf.io.encode_jpeg(image, quality = 95, optimize_size = True, chroma_downsampling = False), class_idx\r\n\r\n  def decode_jpg(image, class_idx):\r\n    return tf.image.decode_jpeg(image, channels = 3), class_idx\r\n\r\n  def normalize_img(image, class_idx):\r\n    return image / 255 - 0.5, class_idx\r\n\r\n  dataset = tf.data.Dataset.from_tensor_slices((\r\n    [tf.cast(np.zeros((256, 256, 3)), dtype = tf.uint8) for _ in range(300)],\r\n    [0 for _ in range(300)]\r\n  ))\r\n  dataset = dataset.map(encode_jpg)\r\n  dataset = dataset.map(decode_jpg)\r\n  dataset = dataset.map(normalize_img)\r\n  dataset = dataset.batch(8)\r\n\r\n  print('\\nhow does our dataset look like?')\r\n  for i, (image, label) in enumerate(dataset):\r\n    print(image.shape, label.shape)\r\n    if i == 2: break\r\n\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Flatten(input_shape = (256, 256, 3)),\r\n    tf.keras.layers.Dense(100, activation = 'relu'),\r\n    tf.keras.layers.Dense(10)\r\n  ])\r\n\r\n  print('\\nhow does our model model like?')\r\n  model.summary()\r\n\r\n  model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\r\n  model.fit(dataset, epochs = 1)\r\n```\r\nI receive the following output which ends with exception:\r\n```\r\nhow does our dataset look like?\r\n(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>\r\n(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>\r\n(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>\r\n\r\nhow does our model model like?\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nflatten (Flatten)            (None, 196608)            0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 100)               19660900  \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                1010      \r\n=================================================================\r\nTotal params: 19,661,910\r\nTrainable params: 19,661,910\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-2-9c1762b0cefe> in <module>()\r\n     36 \r\n     37   model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\r\n---> 38   model.fit(dataset, epochs = 1)\r\n\r\n10 frames\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nUnimplementedError: {{function_node __inference_train_function_5323}} Compilation failure: Asked to propagate a\r\ndynamic dimension from hlo dot.472@{}@0 to hlo %all-reduce.477 = f32[<=196608,100]{1,0}\r\nall-reduce(f32[<=196608,100]{1,0} %dot.472), replica_groups={{0,1,2,3,4,5,6,7}}, to_apply=%sum.473,\r\nmetadata={op_type=\"CrossReplicaSum\" op_name=\"CrossReplicaSum_2\"}, which is not implemented.\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4970277850434216321/_5}}]]\r\n```\r\nWhen I remove these two lines:\r\n```\r\ndataset = dataset.map(encode_jpg)\r\ndataset = dataset.map(decode_jpg)\r\n```\r\nThen it works:\r\n```\r\n38/38 [==============================] - 1s 16ms/step - loss: 16.8563\r\n```\r\nHowever shapes and types of dataset batches remain the same:\r\n```\r\nhow does our dataset look like?\r\n(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>\r\n(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>\r\n(8, 256, 256, 3) of <dtype: 'float32'> (8,) of <dtype: 'int32'>\r\n```\r\nTo fix this error I tried to case labels to tf.int64, but error still occurs. I tried to run this code on CPU version of Colab (removing `with strategy.scope():`), and then it works perfectly. So I guess the problem is in TPU and jpeg encoding-decoding.", "comments": ["I also tried to change label to `tf.one_hot(label, 10)` and change loss to `categorical_crossentropy`, but the error remains the same.", "Was able to reproduce the issue with TF v2.2. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b90177eba7517553edf96d0d50a50add/41590.ipynb#scrollTo=SFy8e1tob2Xm). Thanks!", "The error is not reproducible with MirroredStrategy so I think this is TPU specific.\r\n\r\nBased on the error it looks like you're passing in some dynamic shape. I wonder if the following has something to do with it? \r\nIf you look at the `DatasetSpec` of the dataset without the encoding/decoding the dimensions are known.  \r\n`DatasetSpec(<BatchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int32)>, TensorShape([]))`\r\n\r\nBut the encoded/decoded dataset has None for the height and width.\r\n`DatasetSpec(<BatchDataset shapes: ((None, None, None, 3), (None,)), types: (tf.float32, tf.int32)>, TensorShape([]))`", "Try explicitly setting the size after you decode, using tf.reshape. I think that should work. \r\n\r\n```\r\ndef decode_jpg(image, class_idx):\r\n  return tf.reshape(tf.image.decode_jpeg(image, channels = 3),[256,256, 3]), class_idx\r\n```", "@nikitamaia thanks! that helped", "Closing this issue since a solution was found. Explicit size is needed for TPUs.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41590\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41590\">No</a>\n"]}]