[{"number": 6298, "title": "Long sentence take too much memory", "body": "I use one layer LSTM to train my data.\r\nI use Dynamic_rnn, rnn_size=128, num_layers=1, seq_max_length=2500, batch_size=10, embedding_size=128, softmax_size=1600.\r\n\r\n\r\nMy code like this:\r\n\r\n> x_vec = tf.nn.embedding_lookup(embedding_matrix_variable, self.x)\r\n> lstm_fw_cell = rnn_cell.LSTMCell(num_units = hidden_unit, input_size = hidden_unit)\r\n> lstm_fw_cell = rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=self.dropout_keep_prob, input_keep_prob=self.dropout_keep_prob)\r\n> outputs, _ = rnn.dynamic_rnn(lstm_fw_cell, x, dtype=tf.float32, sequence_length=real_length, swap_memory=False)\r\n> \r\n\r\n\r\nI specify the GPU like this:\r\n<img width=\"281\" alt=\"screenshot\" src=\"https://cloud.githubusercontent.com/assets/4569055/21170698/3bbf5de8-c20a-11e6-9b66-647830bdd371.png\">\r\n\r\n\r\n\r\nCommand \"nvidia-smi\" shows as:\r\n<img width=\"511\" alt=\"screenshot\" src=\"https://cloud.githubusercontent.com/assets/4569055/21170316/7738d3b6-c207-11e6-860a-cdc5ea166f40.png\">\r\n\r\n\r\n\r\nAfter lunching the program, it always shows :\r\n\r\n> \"I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator:...............\":\r\n\r\ndetails show as below:\r\n\r\n> Step: 0       (epoch: 0.00000)       time:28.85937s\r\n> Minibatch loss:          7.43977     Minibatch accuracy:  0.00000\r\n> Test loss:         nan     Test accuracy: 0.00000\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=9012 evicted_count=9000 eviction_rate=0.998668 and unsatisfied allocation rate=0\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 9559 get requests, put_count=15572 evicted_count=6000 eviction_rate=0.385307 and unsatisfied allocation rate=0\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 25809 get requests, put_count=41821 evicted_count=16000 eviction_rate=0.382583 and unsatisfied allocation rate=3.87462e-05\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 42059 get requests, put_count=68071 evicted_count=26000 eviction_rate=0.381954 and unsatisfied allocation rate=2.37761e-05\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 58309 get requests, put_count=94321 evicted_count=36000 eviction_rate=0.381675 and unsatisfied allocation rate=1.715e-05\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=8014 evicted_count=8000 eviction_rate=0.998253 and unsatisfied allocation rate=0\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7817 get requests, put_count=25828 evicted_count=18000 eviction_rate=0.696918 and unsatisfied allocation rate=0.000383779\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 24067 get requests, put_count=52078 evicted_count=28000 eviction_rate=0.537655 and unsatisfied allocation rate=0.000124652\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 40317 get requests, put_count=78328 evicted_count=38000 eviction_rate=0.485139 and unsatisfied allocation rate=7.44103e-05\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 56567 get requests, put_count=104578 evicted_count=48000 eviction_rate=0.458988 and unsatisfied allocation rate=5.30345e-05\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=7016 evicted_count=7000 eviction_rate=0.997719 and unsatisfied allocation rate=0\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 6070 get requests, put_count=23084 evicted_count=17000 eviction_rate=0.736441 and unsatisfied allocation rate=0.000329489\r\n> I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 22320 get requests, put_count=49333 evicted_count=27000 eviction_rate=0.547301 and unsatisfied allocation rate=0.000134409\r\n> \r\n\r\n\r\n", "comments": ["Finally I find the answer, If I use \"config.gpu_options.allow_growth = True\", it will shows the real memory usage of GPU"]}, {"number": 6297, "title": "Added TF.Learn white paper links and citation", "body": "cc: @martinwicke @ilblackdragon ", "comments": ["@tensorflow-jenkins Test this please", "@martinwicke Thanks. Just for my curiosity - will the actual change on website come from automated internal commit? ", "Not automated, but the website is built (manually, but regularly) from github.", "Got it. Thanks.\n\nOn Thu, Dec 15, 2016 at 2:29 PM Martin Wicke <notifications@github.com>\nwrote:\n\n> Not automated, but the website is built (manually, but regularly) from\n> github.\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6297#issuecomment-267435426>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AEEnSiYrkbIxQuR8z7mGqADHh2fmqGw7ks5rIaNEgaJpZM4LMeoT>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n"]}, {"number": 6296, "title": "Branch 141962539", "body": "", "comments": ["@tensorflow-jenkins test this please", "//tensorflow/python/kernel_tests:variable_scope_test failure seems to be legit, reproducible failure. This failure is not seen on CL postsubmit tests. So I guess it is caused by a clash with a external PR. This one looks like a candidate: https://github.com/tensorflow/tensorflow/commit/0958bb705aa124368f7b603151a71386146df2f9", "I'm fixing it, slowly. I'll handle this, you should go sleep.", "There was a large change affecting the behavior if zeros_initializer by @itsmeolivia, which the external PR wasn't aware of. Soon...", "@mrry can you help with the cmake failure? I think it's new kernels in contrib, how should these be added to the cmake build?", "You need to add a line [here](https://github.com/tensorflow/tensorflow/blob/a2dac9bce595e3d85334c582f690583f5d869ae7/tensorflow/contrib/cmake/tf_python.cmake#L515):\r\n\r\n```\r\nGENERATE_PYTHON_OP_LIB(\"contrib_tensor_forest_ops\"\r\n  DESTINATION ${CMAKE_CURRENT_BINARY_DIR}/tf_python/tensorflow/contrib/tensor_forest/python/ops/gen_tensor_forest_ops.py)\r\n```\r\n\r\nIt looks like the kernels are already built (see [here](https://github.com/tensorflow/tensorflow/blob/a2dac9bce595e3d85334c582f690583f5d869ae7/tensorflow/contrib/cmake/tf_core_kernels.cmake#L45)), but the op registrations are missing, so we'll need another line [here](https://github.com/tensorflow/tensorflow/blob/a2dac9bce595e3d85334c582f690583f5d869ae7/tensorflow/contrib/cmake/tf_core_ops.cmake#L51) like:\r\n\r\n```\r\nGENERATE_CONTRIB_OP_LIBRARY(tensor_forest \"${tensorflow_source_dir}/tensorflow/contrib/tensor_forest/ops/tensor_forest_ops.cc\")\r\n```", "Thank you, that worked."]}, {"number": 6295, "title": "Redo: Find correct path of libcudnn.dylib on macOS.", "body": "I just found that commit a62c532 overwrote my change in #5944.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6294, "title": "feature request: MaxPool gradient of a gradient", "body": "MaxPool gradient of a gradient is not supported as determined in https://github.com/fchollet/keras/issues/4694, which means that examples work in keras with the theano backend but not with the TensorFlow backend. I believe the relevant place where the op is registered is at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_grad.cc#L153.\r\n\r\nFrom the keras issue:\r\n\r\nI successfully ran `python mnist_cnn.py` from https://github.com/fchollet/keras/commit/7e2e7a5e5a43443122df0b497f88dd77fd3bfc7c on my GTX1080 with a TensorFlow R0.12rc0 backend. I then ran python [mnist_swwae.py](https://github.com/fchollet/keras/blob/7e2e7a5e5a43443122df0b497f88dd77fd3bfc7c/examples/mnist_swwae.py)  and got the following failure:\r\n\r\n```bash\r\n~/src/keras/examples on master\r\n\u00b1 python mnist_swwae.py\r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\nX_train shape: (60000, 1, 28, 28)\r\n60000 train samples\r\n10000 test samples\r\nTraceback (most recent call last):\r\n  File \"mnist_swwae.py\", line 136, in <module>\r\n    y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool)\r\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.1.2-py2.7.egg/keras/engine/topology.py\", line 517, in __call__\r\n    self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.1.2-py2.7.egg/keras/engine/topology.py\", line 571, in add_inbound_node\r\n    Node.create_node(self, inbound_layers, node_indices, tensor_indices)\r\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.1.2-py2.7.egg/keras/engine/topology.py\", line 155, in create_node\r\n    output_tensors = to_list(outbound_layer.call(input_tensors[0], mask=input_masks[0]))\r\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.1.2-py2.7.egg/keras/layers/pooling.py\", line 158, in call\r\n    dim_ordering=self.dim_ordering)\r\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.1.2-py2.7.egg/keras/layers/pooling.py\", line 207, in _pooling_function\r\n    border_mode, dim_ordering, pool_mode='max')\r\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.1.2-py2.7.egg/keras/backend/tensorflow_backend.py\", line 1853, in pool2d\r\n    x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1617, in max_pool\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1598, in _max_pool\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2242, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1568, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 675, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Negative dimension size caused by subtracting 2 from 1 for 'MaxPool' (op: 'MaxPool') with input shapes: [?,1,32,8].\r\n-> [1]\r\n```", "comments": ["The basic issue (not actually reflected by the error message above) is that the example script is creating a loss that depends on the gradient of some operations. This causes the computation of a gradient of a gradient, which isn't supported for the MaxPool operation (and possibly other operations as well).\r\n\r\nError message:\r\n\r\n```\r\n  File \"/Library/Python/2.7/site-packages/Keras-1.1.2-py2.7.egg/keras/optimizers.py\", line 62, in get_gradients\r\n    grads = K.gradients(loss, params)\r\n  File \"/Library/Python/2.7/site-packages/Keras-1.1.2-py2.7.egg/keras/backend/tensorflow_backend.py\", line 1144, in gradients\r\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 459, in gradients\r\n    (op.name, op.type))\r\nLookupError: No gradient defined for operation 'gradients/MaxPool_grad/MaxPoolGrad' (op type: MaxPoolGrad)\r\n```", "This is a duplicate of #6143", "closed in favor of #6143"]}, {"number": 6293, "title": "Disable tutorials test in docker builds", "body": "", "comments": []}, {"number": 6292, "title": "Squeeze dimension in compute_weighted_loss()", "body": "Operating System: Debian Jessy. CUDA8, CuDNN v5.1. TF 0.12rc0\r\n\r\nI am training a CNN for semantic image labeling task and use an FCN-style architecture. In one of the prediction layers I compute weighted loss. The prediction tensor is of dimensions [1, Height, Width, num_predictions] and weight tensor is [1, Height, Width], batch size is always 1 for me. Training fails on this piece of code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/losses/python/losses/loss_ops.py#L178-L179\r\n\r\n```\r\n  File \"/BS/eldar/work/src/pose-tf/train_net.py\", line 30, in <module>\r\n    losses = pose_net_train(train_param, batch)\r\n  File \"/BS/eldar/work/src/pose-tf/PoseNet.py\", line 108, in pose_net_train\r\n    loss['locref_loss'] = loss_func(locref_pred, locref_targets, locref_weights)\r\n  File \"/BS/eldar/work/src/pose-tf/losses.py\", line 39, in huber_loss\r\n    return tf_losses.compute_weighted_loss(losses, weight)\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/python/util/deprecation.py\", line 191, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py\", line 176, in compute_weighted_loss\r\n    weights = array_ops.squeeze(weights, [-1])\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/array_ops.py\", line 2258, in squeeze\r\n    return gen_array_ops._squeeze(input, axis, name)\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 3313, in _squeeze\r\n    squeeze_dims=squeeze_dims, name=name)\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/BS/eldar/work/software/python3_tf_0.12/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Tried to explicitly squeeze dimension 2 but dimension was not 1: 24\r\n         [[Node: absolute_difference/Squeeze = Squeeze[T=DT_FLOAT, squeeze_dims=[-1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](fifo_queue_Dequeue/_1045)]]\r\n         [[Node: Momentum/update/_1080 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor\r\n_name=\"edge_4872_Momentum/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nMy assumption is that it squeezes the dummy dimension that corresponds to the batch size=1, while it clearly shouldn't. For now I just comment out these two lines in the code, but it'd be nicer if there was a fix upstream.\r\n\r\nCheers,\r\nEldar.", "comments": ["After submitting the ticket and thinking about it I realised that it was not the batch size that was causing it, but rather missing singleton dimension in the weights, so the weights tensor should have been [1, Height, Width, 1] and squeezing is applied on the channel dimension. Doing that fixed the issue. Sorry for the inconvenience! I will close the issue.", "@eldar Can you share your revision?"]}, {"number": 6291, "title": "Fix tutorials test", "body": "Temporarily disable the testing of models that have recently been moved to a\r\nseparate repository, e.g., cifar10_train.\r\n\r\nAdd two tutorials in tensorflow/examples/tutorials:\r\nword2vec_basic\r\nestimators/abalone", "comments": ["I think we should drop the continuous tests for tutorials and models once we guarantee API stability.\r\nI think at best, we will separate all their tests from core tensorflow. Tutorials will also move out of the core repo, and that is in part the reason for this.\r\n\r\nSo I think we can completely disable running tutorials instead in jenkins.", "The tutorial tests are already completely disabled in Jenkins right now. But as a test script, this PR makes sure it runs all right. For releases, it might be useful (we don't want to ship tutorial/model code that doesn't work).", "That's the thing, we will not be shipping them anymore."]}, {"number": 6290, "title": "Run tensorboard embedding visualization learning in backgroup (inactive tab of Chrome)", "body": "Hi, \r\n\r\nI came across a chrome problem when running embedding visualization using tensorboard. The visualization learning (t-SNE) takes hours to finish. If I switch browser to another tab, the tensorboard tab becomes inactive. Any javascript stops working on the inactive tab. This seems to be the root cause: https://blog.pivotal.io/labs/labs/chrome-and-firefox-throttle-settimeout-setinterval-in-inactive-tabs\r\n\r\nI tried making the tab a standalone window, it doesn't work.\r\n\r\nThat means I have to dedicate a laptop to run the learning. This is awful. Does anyone have a solution? Thanks\r\n", "comments": ["Please try asking this question on StackOverflow with the `tensorflow` tag."]}, {"number": 6289, "title": "Is there 3D ConvNets support in TF Slim?", "body": "Is there support for 3D Convnets in Tensorflow Slim? I know Tensorflow does provide support, just would be easier to use TF Slim. ", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6288, "title": "Revert \"Add synthetic datasets (#5315)\"", "body": "This reverts commit ca29fa3cc16663a8dddf5968b5600c60e2dc71a8.\r\n\r\nReason for rollback: Broke builds due to errors such as:\r\n\r\nFile\r\n\"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\",\r\nline 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets import synthetic\r\n    ImportError: cannot import name synthetic", "comments": ["CC @martinwicke @zafartahirov @gunan. Please roll forward after fixing. Thanks.", "Hey @caisq can you please share which builds fail? I cannot seem to replicate the error", "@zafartahirov zafartahirov \r\n\r\nTo give a few examples:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-python35-linux-cpu/228/console\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/323/console\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/323/consoleFull\r\n\r\n@comicchang has opened a PR to fix it. This this rollback PR got merged first so we didn't get a chance to test that fixing PR. https://github.com/tensorflow/tensorflow/pull/6283", "OK, thanks! If ca29fa3 + #6283 don't fix the issue, I will take a look at it tomorrow evening."]}, {"number": 6287, "title": "Windows cuptiActivityRegisterCallbacks not found", "body": "### Environment info\r\nOperating System:\r\nWindows 10 64bit\r\nPython 3.5.2\r\nnVidia GTX 760 (compute capability 3.0)\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 8.0\r\ncuDNN 5.1\r\n\r\nA link to the pip package you installed:\r\nLatest nightly build (at least latest I could find)\r\n\r\nhttp://ci.tensorflow.org/job/nightly-win/DEVICE=gpu,OS=windows/lastSuccessfulBuild/artifact/*zip*/archive.zip\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.12.head\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nThis is the complete code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport matplotlib.image as mpimg\r\nfrom tensorflow.python.client import timeline\r\n\r\nfilename = 'data/MarshOrchid.jpg'\r\nimage = mpimg.imread(filename)\r\n\r\nwith tf.device('/gpu:0'):\r\n    x = tf.Variable(image, name='x', dtype=tf.float32)\r\n    y = tf.transpose(x, perm=[1, 0, 2])\r\n    model = tf.global_variables_initializer()\r\n\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.RunMetadata()\r\n    sess.run(model)\r\n    result = sess.run(y, options=run_options, run_metadata=run_metadata)\r\n\r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    ctf = tl.generate_chrome_trace_format()\r\n    with open('timeline.json', 'w') as f:\r\n        f.write(ctf)\r\n\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\nWorks OK:\r\ntf.RunOptions.SOFTWARE_TRACE\r\n\r\nDon't work:\r\ntf.RunOptions.HARDWARE_TRACE\r\ntf.RunOptions.FULL_TRACE\r\n\r\n### Logs or other output that would be helpful\r\n\r\n```\r\n\"C:\\Program Files\\Python35\\python.exe\" E:/Documents/Workspace/machine_learning/test/test-1.py\r\nE c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:586] Could not identify NUMA node of /job:localhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been built with NUMA support.\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX 760, pci bus id: 0000:01:00.0\r\nx: (Variable): /job:localhost/replica:0/task:0/gpu:0\r\nx/read: (Identity): /job:localhost/replica:0/task:0/gpu:0\r\ntranspose: (Transpose): /job:localhost/replica:0/task:0/gpu:0\r\nx/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0\r\ninit: (NoOp): /job:localhost/replica:0/task:0/gpu:0\r\ntranspose/perm: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nx/initial_value: (Const): /job:localhost/replica:0/task:0/gpu:0\r\nF c:\\tf_jenkins\\home\\workspace\\nightly-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\default\\gpu\\cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: cuptiActivityRegisterCallbacks not found)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```\r\n\r\nI suppressed \"I\" messages with \"TF_CPP_MIN_LOG_LEVEL = 1\". I don't know if I'm missing something but it seems like it's trying to load libcupti.so from \"c:\\tf_jenkins\\...\" path witch doesn't exist. I do have:\r\n\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin\"\r\n\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\libnvvp\"\r\nin path environment variable. Program works OK. It just gives me an error when I try to trace execution.\r\n\r\nI don't know if I messed up something or is this a bug.", "comments": ["1. The `gpu_device.cc` NUMA error, apparently is a warning being logged as error that can be ignored in your case, which is a single GPU.\r\nSee [this StackOverflow answer](http://stackoverflow.com/questions/40891844/tensorflow-windows-native-gpu-support-could-not-identify-numa-node/40892205#40892205).\r\n \r\n2. Regarding `cupti_wrapper.cc` error, could you please try adding `<path-to-cuda-folder>\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\extras\\CUPTI\\libx64` to your `PATH` and see if it solves this problem?\r\nTwo CUDA 8 files, `cupti64_80.dll` and ` cupti.lib`, are located at `...CUDA\\v8.0\\extras\\CUPTI\\libx64` and moving those files into `...\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\bin` and `\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\\lib\\x64` respectively might yield the same results. Another alternative if you rather this way.\r\n \r\n", "I can reproduce a similar error by running the example file `examples/tutorials/mnist/mnist_with_summaries.py`, as follows:\r\n\r\n```\r\n(py3-tf) D:\\Developer\\frameworks\\Anaconda2\\envs\\py3-tf\\Lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist>python mnist_with_summaries.py\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties:\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.8095\r\npciBusID 0000:01:00.0\r\nTotal memory: 8.00GiB\r\nFree memory: 6.66GiB\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0\r\n000:01:00.0)\r\nWARNING:tensorflow:From mnist_with_summaries.py:141 in train.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\r\nWARNING:tensorflow:From mnist_with_summaries.py:142 in train.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\r\nAccuracy at step 0: 0.0724\r\nAccuracy at step 10: 0.6865\r\nAccuracy at step 20: 0.8022\r\nAccuracy at step 30: 0.8291\r\nAccuracy at step 40: 0.8745\r\nAccuracy at step 50: 0.8785\r\nAccuracy at step 60: 0.8857\r\nAccuracy at step 70: 0.8884\r\nAccuracy at step 80: 0.887\r\nAccuracy at step 90: 0.8902\r\nI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:119] Couldn't open CUDA library cupti64_80.dll\r\nF c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\platform\\default\\gpu\\cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbol\r\nFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: cuptiActivityRegisterCallbacks not found)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n```", "@JDRomano2 did you try suggested above?", "@Carmezim Thank you for the quick reply. I'm sorry I wasn't able to reply earlier.\r\n\r\n1. Yes. I'm aware of that. I just copy/pasted entire console output.\r\n\r\n2. Your suggestion solved the problem. It works now. Thank you very much.", "@mrry do you think this Cupti files issue is worth adding to \"common problems\"?\r\nAdd `extras\\CUPTI\\libx64` to `PATH`", "@Carmezim Yes, a PR with that change would be excellent!"]}, {"number": 6286, "title": "Problem about get_variable() and variable_scope()", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI retrained the inception-V3 model according to the image_retrain example with one extra conv and pool layers on the top of the original bottleneck layer. The two layers are initialized with get_variable() function\r\nin a variable_scope() and after training the graph and parameters are stored in a .pb file.\r\nBut when i read the just trained .pb file for test and wanna retrieve the variables in the two layers, with variable_scope() and get_variable(,reuse=true), exception are raised about \r\n'Variable weights/W does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?'\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\ncuda 8.0\r\ncudnn 5.1\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: rc0.12.0\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\n0.12.0-rc0\r\n\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI initialize the two extra layer as:\r\n\r\n```\r\n  with tf.variable_scope('CAM_unit'):\r\n      CAM_conv = new_conv_layer(bottleneck_input,   [3,3,BOTTLENECK_TENSOR_SIZE,BOTTLENECK_TENSOR_SIZE], 'CAM_CONV')\r\n      CAM_pool = tf.reduce_mean(tf.nn.relu(CAM_conv), [1,2], name='CAM_AVG_POOL')\r\n  with tf.variable_scope('weights'):\r\n      layer_weights = tf.get_variable('W', \r\n                                      shape = [BOTTLENECK_TENSOR_SIZE, class_count], \r\n                                      initializer = tf.random_normal_initializer(0., 0.01))\r\n```\r\n\r\nAnd access the weights/W variables as:\r\n\r\n```\r\n with tf.variable_scope('weights', reuse=True):\r\n        label_w = tf.gather(tf.transpose(tf.get_variable('W')), label)\r\n```\r\nerror is\r\n`Variable weights/W does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?`\r\n\r\n### What other attempted solutions have you tried?\r\nI tried to set reuse=None, but the error is \r\n`Shape of a new variable (weights/W) must be fully defined, but instead was <unknown>`\r\nI doubt about the .pb file in which the model converged with a very high accuracy, because when i retrain again with this output .pb file, the accuracy is very low like an untrained one. But i import the model, the new added layers are there as expected, just inaccessible with get_variable()\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nI import the .pb file with code:\r\n```\r\nops = [op.name for op in graph.get_operations() if 'weights/' in op.name]\r\n    pprint.pprint(ops)\r\n```\r\nand find the weights/W variables are available:\r\n`[u'weights/W', u'weights/W/read']`", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag."]}, {"number": 6285, "title": "in debug mode, got Assertion `cudaGetLastError() == cudaSuccess' failed", "body": "Environment: Ubuntu 14.04, CUDA 8, CuDNN 5.1, Tensorflow r0.12\r\nBuild command\r\n`bazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package`\r\n\r\npreviously, following the installation of tensorflow (not in debug mode), my code works well. But, after I rebuild with above command, it shows this error in the middle of running the session.\r\n\r\n> I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcupti.so.8.0 locally\r\n> python: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<const float, const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer> > > >; bool Vectorizable = true]: Assertion `cudaGetLastError() == cudaSuccess' failed.\r\n\r\nthe backtrace result from gdb when the error comes (seems from ReLU operator):\r\n> (gdb) bt\r\n> #0  0x00007ff450df6c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\r\n> #1  0x00007ff450dfa028 in __GI_abort () at abort.c:89\r\n> #2  0x00007ff450defbf6 in __assert_fail_base (\r\n>     fmt=0x7ff450f403b8 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", \r\n>     assertion=assertion@entry=0x7ff42dc57260 \"cudaGetLastError() == cudaSuccess\", \r\n>     file=file@entry=0x7ff42dc57210 \"external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=line@entry=262, \r\n>     function=function@entry=0x7ff42dc58f80 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<float const, float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::GpuDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<float const, float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION__> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap\"...) at assert.c:92\r\n> #3  0x00007ff450defca2 in __GI___assert_fail (\r\n>     assertion=0x7ff42dc57260 \"cudaGetLastError() == cudaSuccess\", \r\n>     file=0x7ff42dc57210 \"external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=262, \r\n>     function=0x7ff42dc58f80 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<float const, float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::GpuDevice, true>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<float const, float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION__> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap\"...) at assert.c:101\r\n> #4  0x00007ff42ad39672 in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<float const, float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const> const> const> const, Eigen::GpuDevice, true>::run (expr=..., device=...)\r\n>     at external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:260\r\n> #5  0x00007ff42ad37a6b in Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16, Eigen::MakePointer>, Eigen::GpuDevice>::operator=<Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_max_op<float const, float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16, Eigen::MakePointer> const> const> > (\r\n>     this=0x7ff35b7fcfe0, other=...)\r\n>     at external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35\r\n> #6  0x00007ff42ad36cb0 in tensorflow::functor::Relu<Eigen::GpuDevice, float>::operator() (\r\n>     this=0x7ff35b7fd04f, d=..., features=..., activations=...)\r\n>     at ./tensorflow/core/kernels/relu_op_functor.h:35\r\n> #7  0x00007ff42acf48c1 in tensorflow::ReluOp<Eigen::GpuDevice, float>::Operate (this=0x59fde00, \r\n>     context=0x7ff35b7fd8f0, input=..., output=0x7ff23ce68390)\r\n>     at ./tensorflow/core/kernels/relu_op.h:40\r\n> #8  0x00007ff42acee479 in tensorflow::UnaryElementWiseOp<float, tensorflow::ReluOp<Eigen::GpuDevice, float> >::Compute (this=0x59fde00, context=0x7ff35b7fd8f0)\r\n>     at ./tensorflow/core/framework/numeric_op.h:62\r\n> #9  0x00007ff42c18523c in tensorflow::BaseGPUDevice::Compute (this=0x5964560, op_kernel=0x59fde00, \r\n>     context=0x7ff35b7fd8f0) at tensorflow/core/common_runtime/gpu/gpu_device.cc:385\r\n> #10 0x00007ff42c402c39 in tensorflow::(anonymous namespace)::ExecutorState::Process (\r\n>     this=0x3d553c80, tagged_node=..., scheduled_usec=1481634999542636)\r\n>     at tensorflow/core/common_runtime/executor.cc:1364\r\n> #11 0x00007ff42c404881 in tensorflow::(anonymous namespace)::ExecutorState::__lambda4::operator() (\r\n>     __closure=0x3d557a70) at tensorflow/core/common_runtime/executor.cc:1746\r\n> #12 0x00007ff42c40ada4 in std::_Function_handler<void(), tensorflow::(anonymous namespace)::ExecutorState::ScheduleReady(const TaggedNodeSeq&, tensorflow::(anonymous namespace)::ExecutorState::TaggedNodeReadyQueue*)::__lambda4>::_M_invoke(const std::_Any_data &) (__functor=...)\r\n>     at /usr/include/c++/4.8/functional:2071\r\n> #13 0x00007ff428bcc7a8 in std::function<void ()>::operator()() const (this=0x3d557ab0)\r\n>     at /usr/include/c++/4.8/functional:2471\r\n> #14 0x00007ff42c6f0b80 in tensorflow::thread::EigenEnvironment::ExecuteTask (this=0x59921a8, t=...)\r\n>     at tensorflow/core/lib/core/threadpool.cc:82\r\n> #15 0x00007ff42c6f29fb in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop (this=0x59921a0, thread_id=7)\r\n>     at external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:167\r\n> #16 0x00007ff42c6f1508 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, tensorflow::thread::EigenEnvironment)::{lambda()#1}::operator()() const\r\n>     () at external/eigen_archive/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:58\r\n> #17 0x00007ff42c6f3927 in std::_Function_handler<void (), Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::NonBlockingThreadPoolTempl(int, tensorflow::thread::EigenEnvironment)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...) at /usr/include/c++/4.8/functional:2071\r\n> #18 0x00007ff428bcc7a8 in std::function<void ()>::operator()() const (this=0x59c3bf0)\r\n\r\nI haven't tried anything as I don't have any idea about this error. I tried with other architecture, it was working. The difference between this and other architecture is this architecture (the one with error) has depthwise layer and the other (the one without error) doesn't have depthwise layer. Seems not connected with the error from relu layer, so I don't have any idea. Both architectures were working in the non-debugging mode", "comments": ["Just adding some information (we don't currently have a solution or cycles to look at this). We have seen issues like this in the past where it is either an ODR violation or a bug in nvcc.", "@zheng-xq, do you have any other thoughts. If we can't reproduce this, I will need to close it as unreproducible. Sorry :(.", "@kalkaneus Did you resolve the problem? I have a similar issue here.", "@endernewton unfortunately not :( ", "Oh I actually got it working here. The reason is I am compiling on my centos server without sudo access. I need to use the customized gcc to compile blaze. I didn't set up blaze correctly. Once it was compiled with right gcc, the problem is gone.", "@kalkaneus A few suggestions:\r\n\r\nPerhaps you can try TensorFlow 1.0?  Or take @endernewton 's advice and try to ensure you have your build environment set up to use the right gcc?\r\n\r\nNote that the build command you listed has both `-c opt` and `-c dbg`.  That is an uncommon usage:\r\n`bazel build -c opt --config cuda -c dbg --strip=never //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nIf you just want symbols in your binary, you should drop the `-c dbg`:\r\n`bazel build -c opt --config cuda --strip=never //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nLet us know whether that helps!", "@tatatodd @endernewton thank you for your suggestions. But as for now, I am doing something else, so later when I have extra time, I'll try to do that in my machine. I'll let you know the result afterwards\r\n\r\nAnd I want to have dbg option as I want thoroughly debug it. Without the dbg flag, everything works well so far.", "In that case I'll close this issue out.  @kalkaneus feel free to file a new issue if you encounter new problems.  Thanks!"]}, {"number": 6284, "title": "Permission denied: '/usr/local/lib/python2.7/dist-packages/protobuf-3.1.0.post1-py2.7.egg/EGG-INFO/namespace_packages.txt'", "body": "Operating System: **Ubuntu 14.04**\r\nTensorflow version: **0.12.0rc1**\r\n\r\nhi all, I have successfully installed tensorflow in my computer, but I encountered the following problem when `import tensorflow` in python 2.7 environment: \r\n\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.7.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.4 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.7.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.7.0 locally\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/tensorflow/_python_build/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/tensorflow/_python_build/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/tensorflow/_python_build/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"/usr/local/lib/python2.7/dist-packages/protobuf-3.1.0.post1-py2.7.egg/google/__init__.py\", line 2, in <module>\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2760, in <module>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 738, in subscribe\r\n    callback(dist)\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2760, in <lambda>\r\n    add_activation_listener(lambda dist: dist.activate())\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2314, in activate\r\n    for pkg in self._get_metadata('namespace_packages.txt'):\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2305, in _get_metadata\r\n    for line in self.get_metadata_lines(name):\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 1369, in get_metadata_lines\r\n    return yield_lines(self.get_metadata(name))\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 1361, in get_metadata\r\n    return self._get(self._fn(self.egg_info,name))\r\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 1470, in _get\r\n    stream = open(path, 'rb')\r\nIOError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/dist-packages/protobuf-3.1.0.post1-py2.7.egg/EGG-INFO/namespace_packages.txt'\r\n```\r\n\r\nBy searching online, I can solve this error by \r\n```\r\nsudo chmod o+r /usr/local/lib/python2.7/dist-packages/protobuf-3.1.0.post1-py2.7.egg/EGG-INFO/namespace_packages.txt\r\n```\r\nHowever, does anyone has other solutions, since I **DO NOT** want use `sudo` frequently? \r\n\r\nBtw, can anyone solve another confusion of mine? I installed tensorflow from source according to the tensorflow's [tutorial.](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#installing-from-sources) But I don't quite understand the differences of the following commands used in my installation:\r\n```\r\nsudo pip install /tmp/tensorflow_pkg/tensorflow-0.12.0rc1-py2-none-any.whl\r\n```\r\nand\r\n```\r\npython setup.py develop\r\n```\r\n\r\nIt seems to me that these two commands all install tensorflow in my system and they all need sudo privilege to work. I also see from tensorflow installation guide that the second command is for development. Could anyone explain in more detail about the differences and purposes of them?\r\n\r\nThanks in advance!", "comments": ["You should only have to run the `sudo chmod...` command once.\r\n\r\nThe first of these commands installs tensorflow from a binary distribution, the second install symbolic links to a source version, so if you change the python during development this is immediately reflected in the installed version, which is very convenient."]}, {"number": 6283, "title": "fix 'ImportError: cannot import name synthetic'", "body": "fix 'ImportError: cannot import name synthetic' error introduced in #5315\r\n\r\nsee also #6281\r\n\r\n```\r\n>>> import tensorflow.examples.tutorials.mnist.input_data as input_data\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/examples/tutorials/mnist/__init__.py\", line 21, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/examples/tutorials/mnist/input_data.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/__init__.py\", line 29, in <module>\r\n    from tensorflow.contrib import factorization\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/factorization/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.factorization.python.ops.gmm import *\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py\", line 30, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/learn/__init__.py\", line 66, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.learn.python.learn import datasets\r\n  File \"/usr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets import synthetic\r\nImportError: cannot import name 'synthetic'\r\n```", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "@googlebot \r\nI signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "The original change was already reverted, can you add this commit onto a new version of that?", "hello martin,\n\nShould I revert the 'revert commit' and rebase my patch to it?\nMartin Wicke <notifications@github.com>\u4e8e2016\u5e7412\u670814\u65e5 \u5468\u4e09\u4e0a\u534812:56\u5199\u9053\uff1a\n\n> The original change was already reverted, can you add this commit onto a\n> new version of that?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/6283#issuecomment-266794639>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABMu0T9SjRIT_6fRl_v0FXICus7qq14sks5rHs4tgaJpZM4LLulE>\n> .\n>\n", "Yes, that is the easiest way. \r\n\r\nJenkins, test this please!", "Can you run buildifier (I suspect some tabs have crept in)?", "@martinwicke did you mean this ?\r\nhttps://github.com/tensorflow/tensorflow/pull/6283/commits/2409471010687383328b308f3236cb9443a44a5c", "Jenkins, test this please.", "Jenkins, test this once more please.", "Jenkins, test this please."]}, {"number": 6282, "title": "TensorFlow Serving Broken ?", "body": "I am running TensorFlow from branch r0.11 along with Bazel 0.3.2. I have managed to build TF and with the work arounds to several bugs get TensorBoard up and running. The last problem I am trying to overcome is TensorFlow Serving.\r\n\r\nCurrently when I try and run: bazel build tensorflow_serving/...\r\n\r\nI get the following crash :\r\n\r\nbazel build tensorflow_serving/...\r\n.........\r\nWARNING: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/inception_model/WORKSPACE:1: Workspace name in /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/inception_model/WORKSPACE (@inception) does not match the name given in the repository's definition (@inception_model); this will cause a build error in future versions.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/org_tensorflow/tensorflow/core/BUILD:1121:1: no such target '@org_tensorflow//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/org_tensorflow/tensorflow/tools/git/BUILD and referenced by '@org_tensorflow//tensorflow/core:version_info_gen'.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/org_tensorflow/tensorflow/core/BUILD:1121:1: no such target '@org_tensorflow//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/org_tensorflow/tensorflow/tools/git/BUILD and referenced by '@org_tensorflow//tensorflow/core:version_info_gen'.\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/org_tensorflow/tensorflow/core/BUILD:1121:1: no such target '@org_tensorflow//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/org_tensorflow/tensorflow/tools/git/BUILD and referenced by '@org_tensorflow//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow_serving/model_servers:server_core' failed; build aborted.\r\nINFO: Elapsed time: 2.908s\r\n\r\nIs this a known problem ?\r\n\r\nIf I run the following as I am using GPU processing (working on AWS) :\r\n\r\nbazel build -c opt --config=cuda tensorflow_serving/...\r\n\r\nI get this error:\r\n\r\nbazel build -c opt --config=cuda tensorflow_serving/...\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/local_config_cuda/crosstool/BUILD:4:1: Traceback (most recent call last):\r\n\tFile \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/local_config_cuda/crosstool/BUILD\", line 4\r\n\t\terror_gpu_disabled()\r\n\tFile \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/local_config_cuda/crosstool/error_gpu_disabled.bzl\", line 3, in error_gpu_disabled\r\n\t\tfail(\"ERROR: Building with --config=c...\")\r\nERROR: Building with --config=cuda but TensorFlow is not configured to build with GPU support. Please re-run ./configure and enter 'Y' at the prompt to build with GPU support.\r\nERROR: no such target '@local_config_cuda//crosstool:toolchain': target 'toolchain' not declared in package 'crosstool' defined by /home/ubuntu/.cache/bazel/_bazel_ubuntu/7317d353d890cf5e09cb18f5cfc053f7/external/local_config_cuda/crosstool/BUILD.\r\nINFO: Elapsed time: 0.275s\r\n\r\nBoth these issues seem to be related to the cache. The second message is odd as GPU processing is enabled and is being used when I run TensorFlow.  Is there a safe way of removing the cache and rebuilding just the bits needed for TensorFlow Serving ? Do you have to install TensorFlow Serving before you install TensorFlow ? I don't want to rebuild everything if I can help it ?", "comments": ["Guys you can close this one. The above occurs when you have TensorFlow installed and then you try and install TensorFlow Serving. I overcome the above by doing a clean build of TensorFlow Serving without having TensorFlow installed and allowing it to pull its own version of TensorFlow from the Git repo.\r\n\r\nWhen I have done subsequent builds, following that approach has resulted in no errors."]}, {"number": 6281, "title": "Cannot import name synthetic - when using add_arg_scope", "body": "### Environment info\r\nOperating System:\r\nUbuntu 16.04\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n````\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudadevrt.a\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart.so -> libcudart.so.8.0\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart.so.8.0.44\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudart_static.a\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so -> libcudnn.so.5\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so.5 -> libcudnn.so.5.1.5\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so.5.1.3\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn.so.5.1.5\r\n/path/to/opt/cuda/toolkit_tensorflow/cuda/lib/libcudnn_static.a\r\n````\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n9c41d5f64efedf44d30e5cc21f900ea07097395c\r\n\r\n2. The output of `bazel version`\r\n````\r\nBuild label: 0.3.2-2016-10-18 (@8fd25d7)\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Oct 18 13:58:42 2016 (1476799122)\r\nBuild timestamp: 1476799122\r\nBuild timestamp as int: 1476799122\r\n````\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n````python\r\nimport tensorflow as tf\r\n\r\n@tf.contrib.framework.add_arg_scope\r\ndef my_func(*args, **kwargs):\r\n    pass\r\n\r\n````\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n````\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/__init__.py\", line 29, in <module>\r\n    from tensorflow.contrib import factorization\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/factorization/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.factorization.python.ops.gmm import *\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/factorization/python/ops/gmm.py\", line 30, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/__init__.py\", line 66, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 23, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.learn.python.learn import datasets\r\n  File \"/home/user/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py\", line 29, in <module>\r\n    from tensorflow.contrib.learn.python.learn.datasets import synthetic\r\nImportError: cannot import name synthetic\r\n````\r\n", "comments": ["Closing as it looks like #6283 was committed."]}, {"number": 6280, "title": "made contrib flatten layer accept tensors with dynamic shapes", "body": "", "comments": ["Can one of the admins verify this patch?", "@ppries sorry we're taking a long time to review. Could you merge and re-upload?", "@drpngx Done!", "I've added a test for the case of unknown dimensions \u2013 the known dimensions case is already handled in testFlattenBatchSize, and it seems silly to add another identical test only for semantics. Let me know if you disagree.\r\n\r\nIt is currently failing testInvalidRank, which I can fix by checking the length of get_shape and reverting to the previous way of asserting rank > 2 (it is currently done using TF control_flow ops). This then assumes that the rank is static \u2013 I don't know enough about TF to know if that is a problem. \r\n\r\nIn any case, I am already using some information from get_shape to decide whether I can propagate shape information to outputs or not, so it might not be an issue. Let me know what you think, then I'll make whatever changes are needed to pass all tests.", "Do I need to do anything?\r\n\r\n@sguada ", "No you should be fine. Jenkins, test this please."]}, {"number": 6279, "title": "No attribute batch_matmul", "body": "Hello,\r\nWhen i run this following python code, i get the error, \r\n\r\n> 'module' object has no attribute 'batch_matmul'\r\n\r\n`a = tf.batch_matmul(None,None)`\r\n\r\nNB:\r\nI use the bazel build command for CPU optimization\r\n\r\n### Environment info\r\nOperating System: Debian jessy\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n9c41d5f64efedf44d30e5cc21f900ea07097395c", "comments": ["In the new documentation this OP is missing also.\r\nDo matmul support some kind of broadcasting now ?", "I think that the batch versions of some ops by making the non-batch versions handle tensors. Rasmus can correct me or add details...", "I think you are right. I did my \"batch_matmul\" with matmul correctly. I close the issue.\r\nThank you for your reply.", "Hello,\r\n\r\nAfter the update to 0.12.1, I run into some problems with the \"batch_matmul\":\r\n\r\n import tensorflow as tf\r\n a = tf.batch_matmul(None, None)\r\n Traceback (most recent call last):\r\n    File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute 'batch_matmul'\r\n\r\nIs there changed anything in the latest version ?\r\n(Maybe one of the collaborator can re-open this issue?)\r\n\r\nThanks", "Please refer to the [issue](https://github.com/tensorflow/tensorflow/issues/6560).", "from the release notes of tensorflow 1.5: \r\n\r\n> Prebuilt binaries are now built against CUDA 9 and cuDNN\"\r\n"]}, {"number": 6278, "title": "Does the op assign change the gradient computation?", "body": "I use the op \"assign\" to change the value of variables instead of \"=\", but I found the gradient I got is quite different. Could anyone tell me the difference and why? thanks!\r\nLike change `w = w1` to  `op1 = tf.assign(w, w1) sess.run(op1)`", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag"]}, {"number": 6277, "title": "save imgs in tensorboard as eps", "body": "Is there any solution to save the images visualized by tensorboard as the eps figures?(like the histogram pictures)", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag"]}, {"number": 6276, "title": "Check failed size >=0", "body": "### Environment info\r\nOperating System: WIndows 10\r\n\r\nIf installed from binary pip package, provide:\r\nTF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\nI am using a similar code provided in the wide and deep learning tutorial.\r\n\r\nLogs  :\r\nF tensorflow/core/framework/tensor_shape.cc:172] Check failed: size >= 0 (-82806 vs. 0)\r\nF tensorflow/core/framework/tensor_shape.cc:36] Check failed: NDIMS == dims() (2 vs. 1)Asking for tensor of 2dimensions from a tensor of 1 dimensions\r\n", "comments": ["Add a code example, and please ask thsi on StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag"]}, {"number": 6275, "title": "Links to windows builds in os setup documentation are bad", "body": "Build location is listed under https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows\r\n\r\nFor example when trying to retrieve:\r\n```\r\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc1-cp35-cp35m-win_amd64.whl\r\n```\r\n\r\nI get the response\r\n```xml\r\n<Error>\r\n   <Code>NoSuchKey</Code>\r\n   <Message>The specified key does not exist.</Message>\r\n</Error>\r\n```", "comments": ["It looks like the website was updated before the new packages were pushed. In the meantime, you can find a valid PIP package here:\r\n\r\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl [link edited]\r\n\r\n(...but the next RC will have a lot of bug fixes, so it's probably worth checking back!)", "The link you provided seems the same as the invalid one I listed, I assume you were meaning to link to an older build?", "Oops, sorry about that. Here's the correct link:\r\n\r\nhttps://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl", "That does the trick, thanks. BTW I really appreciate the work to get windows support. :)", "The 0.12.0rc1 links from https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows should be working now. Please give it another try. Thanks!", "Yep the new links work fine, thanks :)."]}, {"number": 6274, "title": "seq2seq checkpoints not working", "body": "I am seeing abnormal behavior when I train seq2seq...there are 3 checkpoint files being generated at each checkpoint i.e.:\r\n- seq2seq.ckpt-300.data-00000-of-00001 \r\n- seq2seq.ckpt-300.index\r\n- seq2seq.ckpt-300.meta\r\n\r\nTraining seems to progress normally with no errors but when I pause training and try to resume or test from the last checkpoint, a model with fresh parameters is created and my training is rendered useless. \r\n\r\nUPDATE:\r\nInitializing tf.train.Saver() with write_version=1 to revert back to the deprecated version appears to have fixed the problem. Now I see that only two files are created at each checkpoint i.e.:\r\n- seq2seq.ckpt-300\r\n- seq2seq.ckpt-300.meta\r\n\r\nObviously this would suggest something is going on with the new write version...any ideas?\r\n\r\n**UPDATE:**\r\nThe problem is referenced and solved here: http://stackoverflow.com/questions/40469553/tensorflow-loading-model-with-saver-v2", "comments": ["https://github.com/tensorflow/tensorflow/issues/6142\r\nthis would help, seem to be v12 updated  save format\r\n"]}, {"number": 6273, "title": "Add cwise_op_pow to iOS makefile", "body": "This fixes issue #5990. \r\n\r\nIt has been tested and works.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Tests tainted by unrelated breakage, but the important ones looks good."]}, {"number": 6272, "title": "update docker image", "body": "The docker image does not provide updated version of tensorflow. How should I upgrade to 0.12.0 cpu version?", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag"]}, {"number": 6271, "title": "Request for spatial softmax implementation", "body": "Spatial softmax is defined in [End-to-End Training of Deep Visuomotor Policies](https://arxiv.org/abs/1504.00702). This is a request for an implementation in the TensorFlow API, or if the dimension flag for the [regular softmax](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#softmax) does this, could the term and an example be explicitly added to the g3doc?\r\n\r\nIt seems this may already be implemented with tf but not yet incorporated upstream according to [Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search](https://arxiv.org/pdf/1610.00673.pdf):\r\n\r\n> This entire system, which we call asynchronous distributed\r\nguided policy search (ADGPS), was implemented in the\r\ndistributed machine learning framework TensorFlow [...snip...] \r\nOur architecture resembles prior work, with the visual features \r\nrepresented by feature points produced via a spatial softmax\r\napplied to the last convolutional\r\nresponse maps.", "comments": ["The term \"spatial softmax\" is a bit of a misnomer - it should have probably been called spatial soft-argmax, since it's function is to return the expected pixel locations of each feature map. The softmax with the dim flag is not enough in itself, but is a useful tool in implementing the spatial soft-argmax. This can be achieved using a few lines of code in stock TensorFlow. You can first compute the spatial softmax of the features over the image dimensions with a little reshaping and transposing of the input:\r\n\r\n```python\r\n# Assume features is of size [N, H, W, C] (batch_size, height, width, channels).\r\n# Transpose it to [N, C, H, W], then reshape to [N * C, H * W] to compute softmax\r\n# jointly over the image dimensions. \r\nfeatures = tf.reshape(tf.transpose(features, [0, 3, 1, 2]), [N * C, H * W])\r\nsoftmax = tf.nn.softmax(features)\r\n# Reshape and transpose back to original format.\r\nsoftmax = tf.transpose(tf.reshape(softmax, [N, C, H, W]), [0, 2, 3, 1])\r\n```\r\n\r\nThe above is the spatial softmax, which can be used as weights to compute the mean pixel locations of each channel as follows:\r\n\r\n```python\r\n# Assume that image_coords is a tensor of size [H, W, 2] representing the image\r\n# coordinates of each pixel.\r\n# Convert softmax to shape [N, H, W, C, 1]\r\nsoftmax = tf.expand_dims(softmax, -1)\r\n# Convert image coords to shape [H, W, 1, 2]\r\nimage_coords = tf.expand_dims(image_coords, 2)\r\n# Multiply (with broadcasting) and reduce over image dimensions to get the result\r\n# of shape [N, C, 2]\r\nspatial_soft_argmax = tf.reduce_sum(softmax * image_coords, reduction_indices=[1, 2])\r\n```\r\n\r\nHope this helps.", "Thanks Mrinal!  Closing since this is no longer a feature request.", "@kalakris Thanks Mrinal! Hope all is well :-). To confirm, the channels are features at that (h,w) coordinate for each problem in the batch, correct?", "Yes that's correct - I used the terms 'channels' and 'features' interchangeably in my description above.", "[spatial_softmax](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L2619) is now directly available in `layers.py`.", "Hi everyone,\r\n\r\nI have implemented the spatial softmax as well, in a similar fashion that proposed here.\r\nIt works well in the forward pass. However, whenever trying to train a network containing this layer, it appears that after a single pass, it outputs either the center point or one of the corners.\r\n\r\nDoes anyone have an idea of what might cause this behavior? Any possible fixes?\r\n\r\nThe function I use is the following:\r\n\r\n```python\r\ndef spatial_softArgmax(filters, temperature = 1.0):\r\n    \r\n    shape = tf.shape(filters)\r\n    height, width, num_channels = shape[1], shape[2], shape[3]\r\n    \r\n    posx, posy = tf.meshgrid(tf.lin_space(-1., 1., num = height), \r\n                             tf.lin_space(-1., 1., num = width))\r\n    \r\n    posx = tf.reshape(posx, [height * width])\r\n    posy = tf.reshape(posy, [height * width])\r\n    \r\n    filters = tf.reshape(tf.transpose(filters, [0, 3, 1, 2]), [-1, height * width])\r\n    \r\n    softmax_attention = tf.nn.softmax(filters / temperature)\r\n    \r\n    expected_x = tf.reduce_sum(posx * softmax_attention, 1, keep_dims = True)\r\n    expected_y = tf.reduce_sum(posy * softmax_attention, 1, keep_dims = True)\r\n    \r\n    expected_xy = tf.concat([expected_x, expected_y], axis = 1)\r\n    \r\n    feature_keypoints = tf.reshape(expected_xy, [-1, num_channels * 2])\r\n\r\n    return feature_keypoints\r\n```", "@jorisguerin did you figure it out?", "@acrosson Yes I did, first there was an error in my function, one should change the following line:\r\n```python\r\nposx, posy = tf.meshgrid(tf.lin_space(-1., 1., num = height), \r\n                             tf.lin_space(-1., 1., num = width))\r\n```\r\nto\r\n```python\r\nposx, posy = tf.meshgrid(tf.lin_space(-1., 1., num = height), \r\n                             tf.lin_space(-1., 1., num = width), \r\n                             indexing='ij')\r\n```\r\n\r\nAlso, this behavior tends to happen for some reason. I found out two possible ways of avoiding it:\r\n\r\n- Reduce your learning rate\r\n- Add dropouts after your Spacial softArgMax layer\r\n\r\nHope this helps.", "FYI spatial softmax has been available directly in tensorflow since 1.3, see:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L2606\r\n\r\nI'm also making a few small changes in https://github.com/tensorflow/tensorflow/pull/12549.", "https://github.com/tensorflow/addons/issues/1364", "Thanks @kalakris That worked great. But an issue I am facing is, since this involves a product of N * C, I am unable to use dynamic batch size since None * int is forbidden. Is this impossible or any workaround you know about? TIA"]}, {"number": 6270, "title": "Non-backwards compatible change in moving_averages.assign_moving_average", "body": "A `zero_debias` argument has recently been added to `moving_averages.assign_moving_average`. `zero_debias=True` significantly changes the behavior of `moving_averages.assign_moving_average`, in an unexpected way.\r\n\r\nThe default value for `zero_debias` is True. It should be False. The True default breaks existing code (it was breaking Keras), and it also a bad default value on its own because it introduces unexpected behavior (one would expect `assign_moving_average` to merely assign a moving average, i.e. return the equivalent of an `assign_add` op. Instead it creates weights, etc).", "comments": ["It's good to discuss this before 1.0 release.  Most people do want debiasing.  We should fix keras to use this function properly or support debiasing as well.", "(we should mention this as a breaking change in RELEASE.md)", "Is there any update on this since 1.0 is already in alpha now.", "Sorry, I'm not sure what the action item is. Keras, in particular, was fixed internally on Dec 14, 2016 and was included in the next rollout. If that was the action item, then this can be closed.\r\n\r\nIf instead, the action item was to change the default of `zero_debias` in `assign_moving_average` to `False`, then I think more discussion is required, especially since it apparently improves some of the standard implementations of batchnorm, which in turn has statistically significant quality improvements on things like benchmark inception.\r\n\r\nI will close this thread, assuming it is the first. If it is not, please reopen, assign to me, and perhaps give more details as to why the default should be moved to `false`."]}, {"number": 6269, "title": "GPU kernel for tf.random_shuffle", "body": "There doesn't seem to be a `tf.random_shuffle` impl for the GPU. Some of the work I do can utilize a GPU shuffle.\r\n\r\nRunning:\r\n  - tf0.12RC0\r\n  - CUDA8.0\r\n  - CUDNN5.1\r\n  - Titan X (non-pascal)\r\n\r\n```bash\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'RandomShuffle_8': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: RandomShuffle_8 = RandomShuffle[T=DT_FLOAT, seed=87654321, seed2=703433, _device=\"/device:GPU:0\"](transpose_16)]]\r\n```", "comments": ["When i run this code snippet: \r\n```                                                            \r\nimport tensorflow as tf\r\nimport numpy as np                                                         \r\n                                                                           \r\na = tf.placeholder(tf.float32, shape=[2, 4])                                    \r\nw = tf.Variable(tf.random_uniform([4, 4], -1., 1.), name=\"w\")                   \r\n\r\nb = tf.matmul(a, w)                                                             \r\nc = tf.random_shuffle(b)                                                        \r\n\r\nloss = tf.reduce_sum(c)                                                     \r\nopt = tf.train.GradientDescentOptimizer(0.1).minimize(loss)                     \r\n                                                                                       \r\ninit_op = tf.global_variables_initializer()                                     \r\nwith tf.Session() as sess:                                                      \r\n    sess.run(init_op)                                                              \r\n    print(sess.run(opt, feed_dict={a: np.zeros((2, 4))}))\r\n```\r\nIt gives this error:  `LookupError: No gradient defined for operation 'RandomShuffle' (op type: RandomShuffle)` which seems to mean that we cannot use the `RandomShuffle` op in a differentiable graph?", "@EloiZ : Yes, I see this as well. However, I raised this ticket merely to have a GPU kernel for shuffle.\r\n\r\nMost of the time you probably aren't going to randomly shuffle in the middle of a graph and will probably do it at the beginning of the graph (eg: data shuffling). It \"should\" be possible to apply the same logic of grabbing gradients in the slicing operators to the random shuffle operator if you do need gradients for this op.\r\n\r\nMy specific use case is that I'm trying to implement random permutations (and avoid the creation/storing of many permutation matrices).", "Yeah, sorry if that sounds like a \"thread hack\", i thought the problems were related.\r\nYes I need gradient for that op to get negative examples. That is I want to have sim(a, b) > sim(c, b) where c is \"negative sampled\" of a: c = tf.random_shuffle(b). maybe there is a better way for doing this?\r\n", "@EloiZ : I would open a separate ticket about needing a gradient for random_shuffle.", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow.", "@tfboyd : this is a feature request", "Yeah, this is a valid feature request.  Not sure how soon it will happen, but I think we can leave it open.", "No problem.  There will be a second pass to clean up FRs and put them in a different location.", "In the meantime, I'll say this is 'contributions welcome'.", "It seems like tf.tensor doesn't support slice in this way:   a[ : , [3,1,2], :, :]  or a[ :, b, :, :],  b = np.array([3,1,2]) , but numpy.array do.   Is there any alternative method ?", "@EloiZ  i meet the same problem, how you deal with it", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Is there any chance that this issue is resolved ?", "How can  solve the problem?", "> @EloiZ : I would open a separate ticket about needing a gradient for random_shuffle.\r\n\r\nWas this ticket opened? I'd love it if I could including shuffling as part of a layer in my neural net, but I don't see any immediately relevant hits on google for this particular issue being implemented. That said, I'm also not sure how one would compute the gradient through a random shuffle, so maybe there's a reason the feature request never got created.", "This issue is still persistent in tf `1.12.0`:\r\n```\r\nLookupError: gradient registry has no entry for: RandomShuffle\r\n...\r\nLookupError: No gradient defined for operation 'lambda_2/RandomShuffle' (op type: RandomShuffle)\r\n```", "Workaround with GPU gradients:\r\n\r\n```python\r\ntf.gather(batch, tf.random.shuffle(tf.range(tf.shape(batch)[0])))\r\n```", "> Workaround with GPU gradients:\r\n> \r\n> ```python\r\n> tf.gather(batch, tf.random.shuffle(tf.range(tf.shape(batch)[0])))\r\n> ```\r\n\r\nIt works for cpu too, thank you!"]}]