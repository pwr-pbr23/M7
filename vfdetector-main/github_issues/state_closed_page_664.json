[{"number": 33682, "title": "Example from tf.data.Dataset.window() raises error", "body": "**System information**\r\n- TensorFlow version (use command below):\r\nv2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version:\r\n3.7\r\n\r\n**Describe the current behavior**\r\n`tf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)` gives error, see logs.\r\n\r\n**Describe the expected behavior**\r\nShould produce `{({0, 1}, {0, 1}), ({2, 3}, {2, 3})}` according to `tf.data.Dataset.window()` docs\r\n\r\n**Code to reproduce the issue**\r\n`tf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)`\r\n\r\n**Other info / logs**\r\n\r\n\r\n```python\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-66-6565779b5dcd> in <module>\r\n----> 1 tf.data.Dataset.from_tensor_slices((range(4), range(4))).window(2)\r\n\r\n~/anaconda3/envs/tf2-ray-keras/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in from_tensor_slices(tensors)\r\n    433       Dataset: A `Dataset`.\r\n    434     \"\"\"\r\n--> 435     return TensorSliceDataset(tensors)\r\n    436 \r\n    437   class _GeneratorState(object):\r\n\r\n~/anaconda3/envs/tf2-ray-keras/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __init__(self, element)\r\n   2352   def __init__(self, element):\r\n   2353     \"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\r\n-> 2354     element = structure.normalize_element(element)\r\n   2355     batched_spec = structure.type_spec_from_value(element)\r\n   2356     self._tensors = structure.to_batched_tensor_list(batched_spec, element)\r\n\r\n~/anaconda3/envs/tf2-ray-keras/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py in normalize_element(element)\r\n    110           normalized_components.append(\r\n    111               ops.convert_to_tensor(t, name=\"component_%d\" % i))\r\n--> 112   return nest.pack_sequence_as(element, normalized_components)\r\n    113 \r\n    114 \r\n\r\n~/anaconda3/envs/tf2-ray-keras/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py in pack_sequence_as(structure, flat_sequence)\r\n    189         % (len(flat_structure), len(flat_sequence), structure, flat_sequence))\r\n    190 \r\n--> 191   _, packed = _packed_nest_with_indices(structure, flat_sequence, 0)\r\n    192   return _sequence_like(structure, packed)\r\n    193 \r\n\r\n~/anaconda3/envs/tf2-ray-keras/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py in _packed_nest_with_indices(structure, flat, index)\r\n    147     if is_sequence(s):\r\n    148       new_index, child = _packed_nest_with_indices(s, flat, index)\r\n--> 149       packed.append(_sequence_like(s, child))\r\n    150       index = new_index\r\n    151     else:\r\n\r\n~/anaconda3/envs/tf2-ray-keras/lib/python3.7/site-packages/tensorflow_core/python/data/util/nest.py in _sequence_like(instance, args)\r\n     76   else:\r\n     77     # Not a namedtuple\r\n---> 78     return type(instance)(args)\r\n     79 \r\n     80 \r\n\r\nTypeError: 'list' object cannot be interpreted as an integer\r\n```", "comments": ["The below code is working for me:\r\n\r\n```\r\nimport tensorflow as tf\r\np = tf.data.Dataset.from_tensor_slices([(range(4),range(4))]).window(2)\r\n\r\nfor i in p:\r\n  print(p)\r\n```", "@vivecalindahlericsson Is it still an issue or were you able to solve it with the solution above?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue as it has been in awaiting response for more than 14 days. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 33681, "title": "_num_elements(grad) difference between Tensor and IndexedSlices behavior", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.4.0-87-generic-x86_64-with-Ubuntu-16.04-xenial\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen training a model (which I can't make public at the moment), I've got an error: `<built-in function len> returned a result with an error set`. Debugging showed the reason was in\r\nhttps://github.com/tensorflow/tensorflow/blob/0d76b0f988d3d7b06f16da950b3ed95e4ec59e82/tensorflow/python/eager/backprop.py#L615-L624\r\nNamely, my code ended up with `grad` being an instance of `IndexedSlices`, but `grad.values._shape_tuple()` was `(None, 16)`. \r\n\r\nChanging code to\r\n```\r\n    shape_tuple = grad.values._shape_tuple()  # pylint: disable=protected-access\r\n    if shape_tuple is None or None in shape_tuple:\r\n      return 0\r\n    return functools.reduce(operator.mul, shape_tuple, 1)\r\n```\r\nby analogy with the `Tensor` case appears to work and let my model train, but I am not certain whether that's correct.\r\n\r\n**Describe the expected behavior**\r\nEither handle the case where `grad.values._shape_tuple()` is/contains `None`, or explicitly throw `ValueError` if it shouldn't be allowed.\r\n\r\n**Code to reproduce the issue**\r\n(Will try to do this later)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["That looks like it's only used here: https://github.com/tensorflow/tensorflow/blob/0250bfdb8c8f0dce3e11c91f5969c994660d21d9/tensorflow/c/eager/tape.h#L785\r\n\r\nSo safe to return 0; it's just used to estimate when to aggregate gradients, so a performance issue rather than a correctness issue. (CC @alextp )\r\n\r\nWant to send a PR with that change? Happy to review. Ideally we'd know when this case is triggered and have a unit test.", "@allenlavoie PR #33912. ", "@alexeyr \r\nI noticed that this path was not always triggered for me. It only happened when I added more layers (more variables, more gradients as a result)\r\nDoes this happen for you too? thanks!", "Yes, it's rarely triggered (enough that I still haven't written a good test). ", "@taylorchu Actually, if you have a simple enough example triggering it, that would be great.", "weight-share a huge layer more than 4 times? @alexeyr ", "@alexeyr,\r\nIs this still an issue? Could you please upgrade to TensorFlow v2.2 and check if you are still facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33681\">No</a>\n"]}, {"number": 33680, "title": "tflite:experimental:micro:riscv: Fix default target bug in Makefile.", "body": "The default target (all) should appear before any target in Makefile, to let\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu\r\nget to the intendend all target by default.\r\n\r\nThis patch fix the issue #33677 ", "comments": ["@zhoupeng Could you please check reviewer comments and keep us posted. Thanks!", "Patch updated, pls review whether I caught the review suggestions. @nkreeger\r\nhttps://github.com/tensorflow/tensorflow/compare/master...zhoupeng:fix_wrapfunction?expand=1\r\n\r\nThanks,", "@zhoupeng Can you please resolve conflicts? Thanks!", "@gbaned \r\nI saw the materials in dir \"tensorflow/lite/experimental/micro\" have been moved out to \"tensorflow/lite/micro\".\r\nI will explore whether the patches still work or necessary.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@zhoupeng Any update on this PR, please. Thanks!", "@gbaned @nkreeger  resolve conflicts as in new PR https://github.com/tensorflow/tensorflow/pull/36095\r\n\r\nIf possible, pls review as soon as possible, the tensorflow repo is very easy to get conflict because updated so quickly. \r\n\r\nThanks, "]}, {"number": 33679, "title": "TensorFlow Lite Support for LSTM Models", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs mentioned in the TensorFlow Lite 2019 roadmap, a full support for LSTM and RNN models is expected. When this support is expected to be released? will it also support LSTM layers from Keras? \r\n\r\n**Will this change the current api? How?**\r\nNot sure \r\n\r\n**Who will benefit with this feature?**\r\nEveryone which would like to convert TensorFlow LSTM models to the TensorFlow lite format \r\n\r\n**Any Other info.**\r\n", "comments": ["@deansk90,\r\nSorry for the delayed response. Can you please refer [TF Lite Guide for RNN/LSTM](https://www.tensorflow.org/lite/convert/rnn) and let us know if this is what you are looking for? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33678, "title": "tflite:experimental:micro:riscv: Fix default target bug in Makefile.", "body": "The default target (`all`) should appear before any target in Makefile, to let\r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu`\r\nget to the intendend `all` target by default.\r\n\r\nThis patch fix the issue #33677 \r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33678) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33678) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 33677, "title": "tflite:experimental:micro:riscv: the default build target is wrong.", "body": "The intended target ( `make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=riscv32_mcu`) should be `all` , as in the README.md\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/README.md.\r\nBut in fact it's not `all' by default.", "comments": ["Fixed by pull request https://github.com/tensorflow/tensorflow/pull/33680, waiting for review.", "Looks like @nkreeger has commented on your pull request.", "@zhoupeng  PR is already  merged.  Please go  ahead and close the issue if you don't have any further queries.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33677\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33677\">No</a>\n"]}, {"number": 33676, "title": "TF lite GPU delegates should use the same linker script as main library", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android/Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: ip\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): NDK r17b clang\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n**Describe the problem**\r\nCurrently libtensorflowlite.so is build with `\"-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)\"`\r\n\r\nThe same is not done for gpu delegates.\r\nIs there any particular reason for that?\r\nI believe it should be applied to gpu delegate libraries too, as currently you'll need \r\nI don't believe any symbol other than tflite related symbols are necessary.\r\nIf `visibility=hidden` by default is ok (since I think we only need C API from GPU Delegate), then shouldn't it be default flag for the library?\r\n\r\nInconsistent symbol hiding makes it confusing when building both main library and gpu delegates\r\nWould it be acceptable to hide symbols by default or use linker script as in main tflite library?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@DoumanAsh \r\n\r\nSorry for the late reply; I've been out on a conference.\r\n\r\nHm, I'm not sure what that flag does, and re: your question of:\r\n\r\n> Is there any particular reason for that?\r\n\r\nThe people owning TFLite and who delivers the TFLite GPU are different set of people with the latter (I belong here) being more agnostic of what the proper way is :p \r\n\r\nPlease feel free to send a PR to me and @jdduke ", "Hi @DoumanAsh, thanks for flagging, we should absolutely be consistent. If you want to propose a PR, feel free to, otherwise I can take a look.", "Looking over at the delegate API, I see only C API (which is marked visible in code).\r\nSo I think the simplest approach would be is to by default build with `hidden` visibility.\r\nWhich doesn't require any extra linking script (this is also what we do internally when we build delegate)\r\n\r\nSo I'd suggest it as PR with visibility hidden by default for now as all vital APIs are marked with `TFL_CAPI_EXPORT` which resolves to corresponding attribute.\r\n@jdduke does it sound good to you?", "As a side question.\r\nI noticed that 1.15 brought a new GPU Delegate.\r\nAre there any plans for which is going to be a final one?", "The new GPU delegate you are referring to probably has the logic of \"try to use OpenCL if available,  fallback to OpenGL otherwise\".  I'm not 100% certain whether we will kill off the \"old\" OpenGL delegate, but chances are high.", "Hi @DoumanAsh, the V2 delegate is going to be the default going forward. The pre-compiled GPU delegate (on JCenter/Maven) has already been updated to use this variant, as has our benchmark tooling. We're preparing a blog post, and will likely deprecate the \"V1\" API at some point.", "Good to know, I will be shifting our code base to use it then", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33676\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33676\">No</a>\n"]}, {"number": 33675, "title": "tf.keras.callbacks.ReduceLROnPlateau -  min_delta parameter should be percentage, not absolute  ", "body": "\r\n**System information**\r\n- TensorFlow version 2.0:\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRegarding tf.keras.callbacks.ReduceLROnPlateau: The min_delta parameter is currently an absolute number which indicates when a meaningful reduction in the monitored loss has accrued. It makes no sense to use an absolute number for two reasons - \r\n1. Every loss has a different dynamic range and hence a different definition for a meaningful reduction\r\n2. A \"meaningful reduction\" decreases as the training progresses. The higher the epoch the smaller of a change in loss is expected.  \r\n\r\nFor these two reasons I think that a percentage of change in the monitored loss is much more useful. \r\n\r\n**Will this change the current api? How?**\r\nMaybe, if the implantation is by adding a new parameter to the callback named \"min_delta_percent\".\r\n**Who will benefit with this feature?**\r\nEveryone using this callback.\r\n", "comments": ["Do you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "The case is whenever this callback is used. It always makes more sense to use a min_delta_percent than an absolute min delta, As explained above", "Any updates on this? I am interested in this functionality as well for the same reasons as listed above, especially reason 2.\r\n\r\nIn the problem I'm trying to solve, the loss is ~1e5 at the beginning of training, while the goal is to achieve a loss as low as 10 at the end. One can easily see that `min_delta=10` has very different meanings in the beginning and in the end.\r\n\r\nAs a workaround, I copy-pasted Tensorflow's `ReduceLROnPlateau` into my project and added an initialization parameter called `min_delta_mode`, defaulting to `\"absolute\"`. In this case, the callback works as it does presently, with\r\n```py\r\nself.monitor_op = lambda current, best: np.less(current, best - self.min_delta)\r\n```\r\nHowever, when `min_delta_mode == \"relative\"`, I define \r\n```py\r\nself.monitor_op = lambda current, best: np.less(current, (1 - self.min_delta)*best)\r\n```\r\nIt works as expected.", "@danieltomer @ruancomelli Sorry for the late response.  Did you test the feature for different models? Did you notice consistently better results with this feature?\r\n\r\nAre you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!", "@jvishnuvardhan I tested it only with the models I was studying, which are simple convolutional networks like:\r\n```py\r\nx = Input(shape=input_shape)\r\nx = LayerNormalization()(x)\r\nx = Conv2D(\r\n    32,\r\n    (5, 5),\r\n    padding='same',\r\n    activation='relu',\r\n    dtype=hidden_layers_policy,\r\n)(x)\r\nx = MaxPool2D((2, 2), strides=(2, 2), dtype=hidden_layers_policy)(x)\r\nx = Dropout(dropout, dtype=hidden_layers_policy)(x)\r\nx = Flatten(dtype=hidden_layers_policy)(x)\r\nx = Dense(200, activation='relu', dtype=hidden_layers_policy)(x)\r\nx = Dropout(dropout, dtype=hidden_layers_policy)(x)\r\nx = Dense(1, dtype=hidden_layers_policy)(x)\r\npredictions = Activation('linear', dtype=output_layer_policy)(x)\r\n\r\nmodel = Model(inputs=input_data, outputs=predictions)\r\n```\r\nand some variations adding more `Conv2D` or `Dense` layers as well as changing those layers' sizes.\r\n\r\nI noticed that using `min_delta_mode=\"relative\"` consistently improved my models' performances because of the reasons mentioned before, but I haven't tested this exhaustively with other kinds of models or datasets.\r\n\r\nI would really love to contribute to TensorFlow/Keras, yes! Do you know which kinds of tests would be required to justify such a feature, or is it enough to submit a PR containing the changes suggested before?", "@ruancomelli Please submit a PR in keras-team/keras repo. Keras-team will check it and ask you to add a unit test. Thanks for your contribution. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33674, "title": "os.path.join adding \\ to the existing path instead of /", "body": "require path : \r\nC:/Users/user/Documents/TensorFlow/workspace/training_demo/images/train/bandhook.png\r\npath formed \r\nC:/Users/user/Documents/TensorFlow/workspace/training_demo/images/train\\bandhook.png \r\nusing : os.path.join(path, '{}'.format(group.filename)) or os.path.join(path,\"\",group.filename)) as per below docs where path is C:/Users/user/Documents/TensorFlow/workspace/training_demo/images/train\r\nand group.filename is bandhook.png.\r\nFollowing below link \r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html", "comments": ["Below worked for me :\r\npath = '{}{}{}'.format(path,'/',group.filename) "]}, {"number": 33673, "title": "[tf2.0] LSTMCell get_initial_state went wrong when batch_size is a Tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\nLinux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\nNot Clear\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.0.0\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nLSTMCell get_initial_state went wrong when batch_size is a Tensor\r\n**Describe the expected behavior**\r\nI think it should gives a good result\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nimport tensorflow as tf\r\n\r\na = tf.keras.Input(shape=(10,), batch_size=2)\r\nbatch_size = tf.shape(a)[0]\r\ncell = tf.keras.layers.LSTMCell(units=1024)\r\nb = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 2355, in zeros\r\n    tensor_shape.TensorShape(shape))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in __init__\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 776, in <listcomp>\r\n    self._dims = [as_dimension(d) for d in dims_iter]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 718, in as_dimension\r\n    return Dimension(value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_shape.py\", line 193, in __init__\r\n    self._value = int(value)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1610, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Duplicate node name in graph: 'zeros/packed'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test7.py\", line 10, in <module>\r\n    b = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 2314, in get_initial_state\r\n    self, inputs, batch_size, dtype))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 2752, in _generate_zero_filled_state_for_cell\r\n    return _generate_zero_filled_state(batch_size, cell.state_size, dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 2768, in _generate_zero_filled_state\r\n    return nest.map_structure(create_zeros, state_size)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 535, in map_structure\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\", line 535, in <listcomp>\r\n    structure[0], [func(*x) for x in entries],\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py\", line 2765, in create_zeros\r\n    return array_ops.zeros(init_state_size, dtype=dtype)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 2358, in zeros\r\n    shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 1278, in _autopacking_conversion_function\r\n    return _autopacking_helper(v, dtype, name or \"packed\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 1214, in _autopacking_helper\r\n    return gen_array_ops.pack(elems_as_tensors, name=scope)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 6304, in pack\r\n    \"Pack\", values=values, axis=axis, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 793, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 548, in create_op\r\n    compute_device)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3429, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1773, in __init__\r\n    control_input_ops)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1613, in _create_c_op\r\n    raise ValueError(str(e))\r\nValueError: Duplicate node name in graph: 'zeros/packed'", "comments": ["I could reproduce the issue with TF 2.0 on colab. Please see colab [gist](https://colab.sandbox.google.com/gist/gadagashwini/bb4f6ac37f07b5246b6eeabe8cdabf10/untitled226.ipynb). Thanks!", "@Yablon Why do you want to pass batch_size as a tensor? obviously it will throw an error as it was not expecting a tensor. Please let us know any reason behind that change? Thanks!", "@jvishnuvardhan my code is a little bit complicated and I try to simplify it like the following codes. \r\n\r\nI use the official tensorflow_addons module in my code, which is in the original tf.contrib module.\r\n\r\nI have to get the initial input for the decoder, and I want to use the batch_size from the input data, because I don't want a fixed batch in my training.\r\n\r\nMy code goes wrong and I spend a day to find the lstmcell problem.\r\n\r\nDoes I use tf2 in a wrong way ? how can I make the following codes work ? \r\n\r\nThank you for spending time on my issue. You are very appreciated, and I really want to benefit from the tensorflow 2.0. Hope you give me some advices one that issue.\r\n\r\n[my simplified script here](https://github.com/Yablon/test/blob/master/test.py)\r\n", "@Yablon Can you create a standalone code to reproduce the issue? Thanks!", "@jvishnuvardhan Thank you for your reply ! \r\nAfter clearing my redundant codes, I think the problem is solved for now. I can train my model since 12 hours ago. I think TF2.0 is better to develop and debug than TF1.x.\r\nSorry for the interuption.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33673\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33673\">No</a>\n", "lstm_cell =tf.keras.layers.LSTM(units=hidden_unit)\r\nlstm_cell = tf.nn.RNNCellDropoutWrapper(lstm_cell, output_keep_prob=self.dropout_keep_prob)\r\nself._initial_state = lstm_cell.get_initial_state(128, tf.float32)\r\n\r\nGot  ERROR\r\n**********************\r\nValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.\r\nmay I know why I am getting this error?\r\n", "@niranjan8129 Please open a new issue with details of issue and a simple standalone code to reproduce the issue. thanks!"]}, {"number": 33672, "title": "Conv3D nodes not converted to float16 although automatic mixed precision is on.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): **pip install tf-nightly-gpu**\r\n- TensorFlow version (use command below): **tf-nightly-gpu-2.1.0.dev20191023**\r\n- Python version: **Python 3.7.4**\r\n- CUDA/cuDNN version: **CUDA 10.0, cuDNN 7.6.4**\r\n- GPU model and memory: **2x NVIDIA TITAN RTX**\r\n\r\n**Describe the current behavior**\r\nWhen automatic precision is active, 3D convolution ops are not converted to float16 and no performance gain is observed. \r\n\r\nIn the output of the provided example, we can see that most of the nodes are not converted to float16:\r\n`Converted 9/854 nodes to float16 precision using 2 cast(s) to float16 (excluding Const and Variable casts)`\r\n\r\nApart from the warmup delay during the first epoch of the training without AMP, there is no difference in the duration of epochs with and without AMP.\r\n\r\n**Describe the expected behavior**\r\nAccording to `tensorflow/tensorflow/core/grappler/optimizer/auto_mixed_precision_lists.h` and cuDNN release notes, 3D convolution operations on Volta architecture should benefit from AMP training with the tested versions of TF, CUDA and cuDNN.\r\n\r\nWhen debugging with `TF_CPP_MIN_VLOG_LEVEL=2`, I can see why the ops are not optimized:\r\n`I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:1076] Skipping ReadVariableOp node model/conv3d/Conv3D/ReadVariableOp because it must be preserved`\r\nI can't figure out why those items must be preserved. In the source code, I think they are marked as nodes to preserve in `GrapplerItem::NodesToPreserve()` in `tensorflow/core/grappler/grappler_item.cc`, but I could not find out the exact reason.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.python.client import device_lib\r\n\r\nprint(\"TensorFlow version is\", tf.__version__)\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\r\nNUM_SAMPLES = 1000\r\nx_train = x_train[:NUM_SAMPLES]\r\ny_train = y_train[:NUM_SAMPLES]\r\nx_test = x_test[:NUM_SAMPLES]\r\ny_test = y_test[:NUM_SAMPLES]\r\n\r\ndef fake3d(x):\r\n    return np.repeat(x[:, np.newaxis], 8, axis=1)\r\n\r\nx_train = fake3d(x_train)\r\nx_test = fake3d(x_test)\r\n\r\nnum_classes = np.max(y_train) + 1\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\ndef normalize(ndarray):\r\n    ndarray = ndarray.astype(\"float32\")\r\n    ndarray = ndarray/255.0\r\n    return ndarray\r\n\r\nx_train = normalize(x_train)\r\nx_test = normalize(x_test)\r\n\r\ndef create_model(num_classes=10):\r\n    # model parameters\r\n    act = \"relu\"\r\n    pad = \"same\"\r\n    ini = \"he_uniform\"\r\n\r\n    model = tf.keras.models.Sequential([\r\n        Conv3D(128, (3,3,3), activation=act, padding=pad, kernel_initializer=ini,\r\n               input_shape=(8,32,32,3)),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        MaxPooling3D(pool_size=(2,2,2)),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(512, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(512, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        MaxPooling3D(pool_size=(2,2,2)),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(256, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        Conv3D(128, (3,3,3), activation=act, padding=pad, kernel_initializer=ini),\r\n        MaxPooling3D(pool_size=(2,4,4)),\r\n        Flatten(),\r\n        BatchNormalization(),\r\n        Dense(512, activation='relu'),\r\n        Dense(num_classes, activation=\"softmax\")\r\n    ])\r\n\r\n    return model\r\n\r\nmodel = create_model(num_classes)\r\nmodel.summary()\r\nBATCH_SIZE = 320\r\nN_EPOCHS = 6\r\nopt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.5)\r\n\r\ndef train_model(mixed_precision, optimizer):\r\n    model = create_model(num_classes)\r\n\r\n    if mixed_precision:\r\n        import tensorflow\r\n        optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer)\r\n\r\n    model.compile(loss=\"categorical_crossentropy\",\r\n                  optimizer=optimizer,\r\n                  metrics=[\"accuracy\"])\r\n\r\n    train_start = time.time()\r\n\r\n    train_log = model.fit(x_train, y_train,\r\n                          batch_size=BATCH_SIZE,\r\n                          epochs=N_EPOCHS,\r\n                          use_multiprocessing=True,\r\n                          workers=2)\r\n\r\n    train_end = time.time()\r\n\r\n    results = {\r\n               \"train_time\": train_end-train_start,\r\n               \"train_log\": train_log}\r\n\r\n    return results\r\n\r\nfp32_results = train_model(mixed_precision=False, optimizer=opt)\r\ntrain_time = round(fp32_results[\"train_time\"], 1)\r\nprint(\"achieved in\", train_time, \"seconds\")\r\n\r\ntf.keras.backend.clear_session()\r\ntime.sleep(10)\r\n\r\nmp_results = train_model(mixed_precision=True, optimizer=opt)\r\ntrain_time = round(mp_results[\"train_time\"], 1)\r\nprint(\"achieved in\", train_time, \"seconds\")\r\n```", "comments": ["@efournie ,\r\nThank you for reporting the issue, when tried executing the given code for `tf-nightly-gpu-2.1.0.dev20191023` i got the following error, kindly check the [gist](https://colab.sandbox.google.com/gist/oanush/2eb8d6f74d21ea8897575167a06bb8cb/33672.ipynb) of colab and confirm if the same is faced even from your end.Thanks!\r\n", "Sorry, the Titan RTX GPUs have quite a lot of memory so I set the batch size a bit too high. Editing the batch size to `BATCH_SIZE = 64` in the previous program, I can run it completely. In order to run faster, I also set `NUM_SAMPLES = 400`.\r\nUnfortunately, if I am not mistaken, Google Colab notebooks use Tesla K80 GPUs which don't support automatic mixed precision. In order to reproduce the issue, the program should be run on a Volta GPU (RTX 20xx or Titan RTX or Tesla V100).", "The Windows TensorFlow builds are only built with cuDNN 7.6.0, while 7.6.2 is needed for float16 Conv3D to be used. Unfortunately, when determining whether Conv3D is float16, TensorFlow currently look at the version of cuDNN that TensorFlow is built against, not the version that is actually used at runtime, which is why you are not seeing Conv3D being done in float16 despite using 7.6.4.\r\n\r\nUntil the builds are updated to at least 7.6.2, as a workaround, you can set the environmental variable:\r\n\r\n```\r\nTF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_WHITELIST_ADD=Conv3D,Conv3DBackpropFilter,Conv3DBackpropFilterV2,Conv3DBackpropInput,Conv3DBackpropInputV2\r\n```\r\n\r\nAn easy way to do this is to add the following to the top of the Python program. Note that this will make your program a lot slower unless you are using at least cuDNN 7.6.2.\r\n\r\n```\r\nimport os\r\nos.environ['TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_WHITELIST_ADD'] = 'Conv3D,Conv3DBackpropFilter,Conv3DBackpropFilterV2,Conv3DBackpropInput,Conv3DBackpropInputV2'\r\n```", "Thanks, with the environment variable it works. \r\n\r\nAs it concerns an unreleased TF version, I guess I can close the issue now. Is there a way to  find out the cuDNN version that TensorFlow is built against?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33672\">No</a>\n", "I just committed 336efe7422384d3628000433cd9b127b48c232fd, which logs the cuDNN version. To see it, set the environmental variable TF_CPP_VMODULE to \"meta_optimizer=4\" (without quotes). The change won't be in the nightly builds until tomorrow."]}, {"number": 33671, "title": "TFLite Raspberry Pi object detection example code does not work with tensorflow.lite.python.interpreter, only works with tflite_runtime.interpreter", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry Pi 4 4GB\r\n- TensorFlow installed from (source or binary): Binary (pip3 install tensorflow)\r\n- TensorFlow version (use command below): v1.12.1-3892-g127aae0 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the behavior**\r\nI am following the TFLite object detection on Raspberry Pi guide (https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi) using a Raspberry Pi 4 4GB model. The guide says to download and install the tflite_runtime from the [Python quickstart page](https://www.tensorflow.org/lite/guide/python). However, I tried using it with my existing installation of TensorFlow since I already have it installed on my Pi 4, and the script exits with the error shown below.\r\n\r\nIt also doesn't work if I have regular TensorFlow and the tflite_runtime package installed at the same time. It does work if I uninstall TensorFlow using `pip3 uninstall tensorflow` and then install the tflite_runtime package. \r\n\r\n**Describe the expected behavior**\r\nIt would be nice if the object detection example code worked without having to remove my existing TensorFlow install.\r\n\r\n**Code to reproduce the issue**\r\nI used the example code given [here](https://github.com/tensorflow/examples/blob/master/lite/examples/object_detection/raspberry_pi/detect_picamera.py). When trying to use regular TensorFlow (rather than just the tflite_runtime package), I changed line 33 from `from tflite_runtime.interpreter import Interpreter` to `from tensorflow.lite.python.interpreter import Interpreter`.\r\n\r\nI called the script using:\r\n```\r\npython3 detect_picamera.py \\\r\n  --model /tmp/detect.tflite \\\r\n  --labels /tmp/coco_labels.txt\r\n```\r\n\r\n**Other info / logs**\r\nTraceback for the error I am getting:\r\n\r\nTraceback (most recent call last):\r\n  File \"TFLite_detection_picamera.py\", line 113, in <module>\r\n    interpreter = Interpreter(model_path=PATH_TO_CKPT)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 205, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/pi/.local/lib/python3.7/site-packages/tensorflow_core/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n", "comments": ["Update: I just tested this with TensorFlow v2.0 (downloaded and installed from https://github.com/lhelontra/tensorflow-on-arm/releases), and everything worked fine.", "Thank you @EdjeElectronics, I had exactly the same issue and this solution also worked for me.\r\n\r\n> Update: I just tested this with TensorFlow v2.0 (downloaded and installed from https://github.com/lhelontra/tensorflow-on-arm/releases), and everything worked fine.\r\n\r\n", "@petewarden will this get resolved any time soon? It looks like users are having this same error with TF v1.14, which is the latest version easily available for Raspbian Stretch (which uses Python 3.5 by default).\r\n\r\nSeems there are two solutions:\r\n\r\n1. Add wheel files for TF v2.0 on piwheels.org so users can easily install TF v2.0\r\n2. Resolve the issue with TF v1.13 / TF v1.14.\r\n\r\nA couple people following my guide for TF Lite on Raspberry Pi are encountering the same problem: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/issues/13", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33671\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33671\">No</a>\n"]}, {"number": 33670, "title": "Update metrics.py", "body": "", "comments": ["Several things:\r\n\r\n1. Please use a descriptive title for the PR\r\n1. Please describe the PR. At least justify the change\r\n1.  If there is an issue, please reference it.\r\n2. Please reference/add tests\r\n4. A one line change still triggers several CPU/GPU hours worth of CI. Please fix the issue in all cases in the file/directory so that we don't waste enormous amounts of compute hours for tiny fixes.", "Cutoff window for 2.0 is closed and at this time we only accept security vulnerabilities, so closing this PR, please open a new PR against `master` \r\nCC @mihaimaruseac"]}, {"number": 33669, "title": "tf.keras.callbacks.ProgbarLogger not works as expected", "body": "I was tried to use tf.keras instead of keras, and I found that tf.keras.callbacks.ProgbarLogger not works as expected, or as the old keras did.\r\nIn keras, at the end of one-epoch's training, the ProgbarLogger will just wait until the validation done, and the final info on the screen would be like \r\n>[training-progress-bar] training-metrics validation-metrics\r\n\r\nBut In keras, at the end of one-epoch's training, the training-progress-bar will disappear and the validation-progress-bar will show on the screen. And when validation progress finished, the final info on the screen would be like \r\n> [validation-progress-bar] validation-metrics \r\n> [training-progress-bar] training-metrics validation-metrics\r\n\r\n As I expected, the training-progress-bar should stay on the screen during validation, and the final info should be \r\n> [training-progress-bar] training-metrics\r\n> [validation-progress-bar] validation-metrics ", "comments": ["@zhangyi02, Please provide the standalone code to reproduce the reported issue. Thanks! ", "@zhangyi02, Any update code snippet.", "```python\r\nimport numpy as np\r\nimport keras\r\nprint(keras)\r\n\r\n\r\nclass FakedataSequence(keras.utils.Sequence):\r\n    def __init__(self, batch_size, num_batches, stage='tr'):\r\n        self.batch_size = batch_size\r\n        self.num_batches = num_batches\r\n        self.stage = stage\r\n\r\n    def __len__(self):\r\n        return self.num_batches\r\n\r\n    def __getitem__(self, item):\r\n        if self.stage == 'tr':\r\n            np.random.seed(item)\r\n        else:\r\n            np.random.seed(100000 + item)\r\n        data = np.random.random((self.batch_size, 10))\r\n        label = np.random.randint(0, 2, size=self.batch_size)\r\n        return data, label\r\n\r\n\r\ndef toy_model():\r\n    x = keras.layers.Input(shape=(10,))\r\n    y = keras.layers.Dense(1, activation='sigmoid')(x)\r\n    return keras.models.Model(x, y)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tr_sequence = FakedataSequence(32, 100, 'tr')\r\n    cv_sequence = FakedataSequence(32, 10, 'cv')\r\n    model = toy_model()\r\n    model.compile(optimizer='sgd', loss='mse')\r\n    history = model.fit_generator(generator=tr_sequence,\r\n                                  epochs=10,\r\n                                  verbose=1,\r\n                                  validation_data=cv_sequence,\r\n                                  workers=32)\r\n```\r\nwhile using the old keras, as `print(keras)` returns `<module 'keras' from '~/.local/lib/python3.6/site-packages/keras/__init__.py'>`, the logs looks like:\r\n>Epoch 1/10\r\n100/100 [==============================] - 0s 3ms/step - loss: 0.2649 - val_loss: 0.2828\r\nEpoch 2/10\r\n100/100 [==============================] - 0s 3ms/step - loss: 0.2648 - val_loss: 0.2820\r\nEpoch 3/10\r\n100/100 [==============================] - 0s 2ms/step - loss: 0.2641 - val_loss: 0.2809\r\nEpoch 4/10\r\n100/100 [==============================] - 0s 4ms/step - loss: 0.2659 - val_loss: 0.2802\r\nEpoch 5/10\r\n100/100 [==============================] - 0s 3ms/step - loss: 0.2638 - val_loss: 0.2797\r\nEpoch 6/10\r\n100/100 [==============================] - 0s 2ms/step - loss: 0.2648 - val_loss: 0.2790\r\nEpoch 7/10\r\n100/100 [==============================] - 0s 2ms/step - loss: 0.2627 - val_loss: 0.2787\r\nEpoch 8/10\r\n100/100 [==============================] - 0s 2ms/step - loss: 0.2634 - val_loss: 0.2781\r\nEpoch 9/10\r\n100/100 [==============================] - 0s 1ms/step - loss: 0.2634 - val_loss: 0.2778\r\nEpoch 10/10\r\n100/100 [==============================] - 0s 1ms/step - loss: 0.2632 - val_loss: 0.2776\r\n\r\nAnd if change the `import keras` to `from tensorflow import keras`, and rerun this script, while `print(keras)` returns `<module 'tensorflow._api.v1.keras' from '~/.local/lib/python3.6/site-packages/tensorflow/_api/v1/keras/__init__.py'>`, the logs looks like:\r\n>10/10 [==============================] - 0s 3ms/step - loss: 0.2566\r\n100/100 [==============================] - 1s 11ms/step - loss: 0.2571 - val_loss: 0.2566\r\nEpoch 2/10\r\n10/10 [==============================] - 0s 1ms/step - loss: 0.2566\r\n100/100 [==============================] - 0s 3ms/step - loss: 0.2568 - val_loss: 0.2566\r\nEpoch 3/10\r\n10/10 [==============================] - 0s 2ms/step - loss: 0.2640\r\n100/100 [==============================] - 0s 4ms/step - loss: 0.2562 - val_loss: 0.2640\r\nEpoch 4/10\r\n10/10 [==============================] - 0s 2ms/step - loss: 0.2491\r\n100/100 [==============================] - 0s 3ms/step - loss: 0.2554 - val_loss: 0.2491\r\nEpoch 5/10\r\n10/10 [==============================] - 0s 1ms/step - loss: 0.2500\r\n100/100 [==============================] - 0s 4ms/step - loss: 0.2542 - val_loss: 0.2500\r\nEpoch 6/10\r\n10/10 [==============================] - 0s 1ms/step - loss: 0.2598\r\n100/100 [==============================] - 0s 5ms/step - loss: 0.2545 - val_loss: 0.2598\r\nEpoch 7/10\r\n10/10 [==============================] - 0s 688us/step - loss: 0.2496\r\n100/100 [==============================] - 0s 3ms/step - loss: 0.2557 - val_loss: 0.2496\r\nEpoch 8/10\r\n10/10 [==============================] - 0s 2ms/step - loss: 0.2510\r\n100/100 [==============================] - 0s 4ms/step - loss: 0.2544 - val_loss: 0.2510\r\nEpoch 9/10\r\n10/10 [==============================] - 0s 1ms/step - loss: 0.2514\r\n100/100 [==============================] - 0s 4ms/step - loss: 0.2549 - val_loss: 0.2514\r\nEpoch 10/10\r\n10/10 [==============================] - 0s 2ms/step - loss: 0.2520\r\n100/100 [==============================] - 0s 4ms/step - loss: 0.2550 - val_loss: 0.2520\r\n\r\nWell, I notice that even \"Epoch 1/10\" also disappeared.\r\nI supposed the logs should be like:\r\n>Epoch 1/10\r\n100/100 [==============================] - 1s 11ms/step - loss: 0.2571\r\n10/10 [==============================] - 0s 3ms/step - val_loss: 0.2566\r\nEpoch 2/10\r\n....\r\n\r\n", "Issue is replicating with Tensorflow 1.15.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/9b7726a8a9845100d2f8b18a9da47ed4/untitled252.ipynb). Thanks! ", "@gadagashwini I tried TF2.0\uff0cit works just like the origin Keras. By the way, I'm using TF1.13.1. Maybe it's time change to TF2.0\u2026\u2026", "@zhangyi02 I agree that the output from `TF1.x` doesn't look as good as the output from `TF2.0`. As `TF1.15` is the last version in `TF1.x` series, I don't think there is any more updates to that branch unless there is any security related issues.\r\n\r\nIf possible, I would suggest you to start using `TF2.0`. Please close the issue, if you don't have any further questions. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33669\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33669\">No</a>\n"]}, {"number": 33668, "title": "[Simple Bug] 'name' should be a variable", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version 1.13\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nI want build two convLSTMCell and use them. like follow [Code to reproduce the issue]\r\ntensorflow talk to me: `ValueError: Variable kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at xxxx.`\r\n\r\n\r\n**Describe the expected behavior**\r\nthe code can run without error.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.rnn as rnn\r\nimport numpy as np\r\n\r\nnpa = np.array([[[[1,2,3],[4,5,6],[4,5,6]],[[7,8,9],[10,11,12],[10,11,12]],[[13,14,15],[16,17,18],[16,17,18]]]])\r\nprint(npa.shape)\r\nx = tf.constant(npa, dtype=tf.float32)\r\nprint(x)\r\n# first cell\r\ncell_encoder = rnn.Conv2DLSTMCell(input_shape=[x.shape[1], x.shape[2], x.shape[3]], output_channels=4,kernel_shape=[3, 3], name='encoder')\r\ninitial1 = cell_encoder.zero_state(batch_size=x.shape[0],dtype=tf.float32)\r\nx1, fs1 = cell_encoder.call(inputs=x, state=initial1)\r\n# second cell\r\ncell_decoder = rnn.Conv2DLSTMCell(input_shape=[x.shape[1], x.shape[2], x.shape[3]], output_channels=4, kernel_shape=[3, 3], name='decoder')\r\ninitial2 = cell_decoder.zero_state(batch_size=x.shape[0], dtype=tf.float32)\r\nx2, fs2 = cell_decoder.call(inputs=x, state=initial2)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    x = sess.run(x2)\r\n    print(x)\r\n```\r\n[minimal standalone code ]\r\n\r\n**Other info / logs**\r\nWhen system execution `[cell_decoder.call]`,  tensorflow talk to me: `ValueError: Variable kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at xxxx`.\r\nthen ,i tack the code find `tensorflow/contrib/rnn/python/ops/rnn_cell.py line: 2215 & 2228, vs.get_variable function `name of the function should be a variable, not a constant.\r\nthe issue is simple. If the name  is a constant, the second cell, cell_decoder, will use the same one variable.\r\n\r\nThanks to tensorflow for providing AI open source framework.  O(\u2229_\u2229)O\r\n\r\n\r\n", "comments": ["Looks like code is incomplete. In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Please use triple backticks before and after the code to make it easier to read. Every textbox on GitHub has a small M and a down arrow just above the Comment/Submit/etc. green button. Clicking on it will give you a tutorial on how to properly use Markdown. Please use it properly so issues can be analyzed faster", "> Looks like code is incomplete. In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!\r\n\r\ni\u2019m change the [Code to reproduce the issue]~", "I have tried on colab with TF version 1.13, 1.15 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/d49c453983908cdf90bd771bb4d4724a/untitled338.ipynb) Thanks!", "> I have tried on colab with TF version 1.13, 1.15 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/d49c453983908cdf90bd771bb4d4724a/untitled338.ipynb) Thanks!\r\n\r\n\r\n# key of the issue\r\nThere is the key of the issue.  Every `rnn.Conv2DLSTMCell` will call same one `vs.get_variable`. (`Conv2DLSTMCell `call `call`, `call `call `_conv`, `_conv `call `vs.get_variable`)\r\n[ValueError Traceback (most recent call last)] as follow:\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/rnn/python/ops/rnn_cell.py in _conv(args, filter_size, num_features, bias, bias_start)\r\n   2228   # Now the computation.\r\n   2229   kernel = vs.get_variable(\r\n-> 2230       \"kernel\", filter_size + [total_arg_size_depth, num_features], dtype=dtype)\r\n   2231   if len(args) == 1:\r\n   2232     res = conv_op(args[0], kernel, strides, padding=\"SAME\")\r\n```\r\n# reason\r\nI use twice `rnn.Conv2DLSTMCell`. The first cell is right without error, but the second is error. The reason is that name of `vs.get_variable` is a constent. The first cell gets the 'kernel' variable, and the second cell should  get a new variable which not same with the 'kernel' varibale.\r\n\r\n# modify\r\nI think the `_conv` function should be modified as follow: (the wrong code is commented out by `#` )\r\n```\r\ndef _conv(args, filter_size, num_features, bias, bias_start=0.0, name='_conv'):\r\n  \"\"\"Convolution.\r\n\r\n  Args:\r\n    args: a Tensor or a list of Tensors of dimension 3D, 4D or 5D,\r\n    batch x n, Tensors.\r\n    filter_size: int tuple of filter height and width.\r\n    num_features: int, number of features.\r\n    bias: Whether to use biases in the convolution layer.\r\n    bias_start: starting value to initialize the bias; 0 by default.\r\n\r\n  Returns:\r\n    A 3D, 4D, or 5D Tensor with shape [batch ... num_features]\r\n\r\n  Raises:\r\n    ValueError: if some of the arguments has unspecified or wrong shape.\r\n  \"\"\"\r\n\r\n  # Calculate the total size of arguments on dimension 1.\r\n  total_arg_size_depth = 0\r\n  shapes = [a.get_shape().as_list() for a in args]\r\n  shape_length = len(shapes[0])\r\n  for shape in shapes:\r\n    if len(shape) not in [3, 4, 5]:\r\n      raise ValueError(\"Conv Linear expects 3D, 4D \"\r\n                       \"or 5D arguments: %s\" % str(shapes))\r\n    if len(shape) != len(shapes[0]):\r\n      raise ValueError(\"Conv Linear expects all args \"\r\n                       \"to be of same Dimension: %s\" % str(shapes))\r\n    else:\r\n      total_arg_size_depth += shape[-1]\r\n  dtype = [a.dtype for a in args][0]\r\n\r\n  # determine correct conv operation\r\n  if shape_length == 3:\r\n    conv_op = nn_ops.conv1d\r\n    strides = 1\r\n  elif shape_length == 4:\r\n    conv_op = nn_ops.conv2d\r\n    strides = shape_length * [1]\r\n  elif shape_length == 5:\r\n    conv_op = nn_ops.conv3d\r\n    strides = shape_length * [1]\r\n\r\n  # Now the computation.\r\n  # kernel = vs.get_variable(\r\n  #     \"kernel\", filter_size + [total_arg_size_depth, num_features], dtype=dtype)\r\n  kernel = vs.get_variable(\r\n    name + \"_kernel\", filter_size + [total_arg_size_depth, num_features], dtype=dtype)\r\n  if len(args) == 1:\r\n    res = conv_op(args[0], kernel, strides, padding=\"SAME\")\r\n  else:\r\n    res = conv_op(\r\n        array_ops.concat(axis=shape_length - 1, values=args),\r\n        kernel,\r\n        strides,\r\n        padding=\"SAME\")\r\n  if not bias:\r\n    return res\r\n  # bias_term = vs.get_variable(\r\n  #     \"biases\", [num_features],\r\n  #     dtype=dtype,\r\n  #     initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\r\n  bias_term = vs.get_variable(\r\n    name + \"_biases\", [num_features],\r\n   dtype=dtype,\r\n  initializer=init_ops.constant_initializer(bias_start, dtype=dtype))\r\n  return res + bias_term\r\n```\r\n\r\nAnd, i think the `ConvLSTMCell.call` function should be modified as follow: \r\n  ```\r\ndef call(self, inputs, state, scope=None):\r\n    cell, hidden = state\r\n    new_hidden = _conv([inputs, hidden], self._kernel_shape,\r\n                       4 * self._output_channels, self._use_bias, name=self.name)\r\n    gates = array_ops.split(\r\n        value=new_hidden, num_or_size_splits=4, axis=self._conv_ndims + 1)\r\n\r\n    input_gate, new_input, forget_gate, output_gate = gates\r\n    new_cell = math_ops.sigmoid(forget_gate + self._forget_bias) * cell\r\n    new_cell += math_ops.sigmoid(input_gate) * math_ops.tanh(new_input)\r\n    output = math_ops.tanh(new_cell) * math_ops.sigmoid(output_gate)\r\n\r\n    if self._skip_connection:\r\n      output = array_ops.concat([output, inputs], axis=-1)\r\n    new_state = rnn_cell_impl.LSTMStateTuple(new_cell, output)\r\n    return output, new_state\r\n```\r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33668\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33668\">No</a>\n"]}, {"number": 33667, "title": "Potential redundancy in using np.array", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry:\r\nhttps://www.tensorflow.org/tutorials/generative/deepdream\r\n\r\n## Description of the issue (what needs changing):\r\nIn this example, we are already performing this: `original_img = np.array(original_img)` in the beginning. Then, what is the point of repeating it here: `img = tf.constant(np.array(original_img))` and here: `shift_down, shift_right, img_rolled = random_roll(np.array(original_img), 512)`? ", "comments": ["@sayakpaul Taking it up an octave and Scaling up with tiles are two different techniques.\r\n\r\nIn the 1st technique, you converted an image to tensor and then cast it to float32 to multiply it by an octave(i.e., floating point number) but in the second technique you are just passing in the original image that is a numpy array directly to the function.\r\n\r\nI don't think this is redundant information.", "@gowthamkpr \r\n\r\n> @sayakpaul Taking it up an octave and Scaling up with tiles are two different techniques.\r\n\r\nI guess I never mentioned scaling up octaves and tiling are the two same techniques. \r\n\r\nRegarding the redundancy, the order of the lines code (that I mentioned initially) in the tutorial goes like the following:\r\n\r\n```\r\noriginal_img = np.array(original_img)\r\nimg = tf.constant(np.array(original_img))\r\nshift_down, shift_right, img_rolled = random_roll(np.array(original_img), 512)\r\n```\r\n\r\nAll I am trying to say is if we are creating an `np.array` of the `original_img` in the very beginning then what's the point of doing it again for the downstream tasks as `original_img` is already an `np.array` (made in the first step)? ", "@sayakpaul Yes, I agree with you. We will take a look into it. Thanks!", "I opened a PR for the same as well. @gowthamkpr"]}, {"number": 33666, "title": "tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. \t [[node my_model/conv2d/Conv2D (defined at /wang/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_train_step_638]", "body": "Code:\r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\r\nfrom tensorflow.keras import Model\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n# Add a channels dimension\r\nx_train = x_train[..., tf.newaxis]\r\nx_test = x_test[..., tf.newaxis]\r\n\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(\r\n    (x_train, y_train)).shuffle(10000).batch(32)\r\n\r\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\r\n\r\nclass MyModel(Model):\r\n  def __init__(self):\r\n    super(MyModel, self).__init__()\r\n    self.conv1 = Conv2D(32, 3, activation='relu')\r\n    self.flatten = Flatten()\r\n    self.d1 = Dense(128, activation='relu')\r\n    self.d2 = Dense(10, activation='softmax')\r\n\r\n  def call(self, x):\r\n    x = self.conv1(x)\r\n    x = self.flatten(x)\r\n    x = self.d1(x)\r\n    return self.d2(x)\r\n\r\n# Create an instance of the model\r\nmodel = MyModel()\r\n\r\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\r\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\r\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\r\n\r\n@tf.function\r\ndef train_step(images, labels):\r\n  with tf.GradientTape() as tape:\r\n    predictions = model(images)\r\n    loss = loss_object(labels, predictions)\r\n  gradients = tape.gradient(loss, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n\r\n  train_loss(loss)\r\n  train_accuracy(labels, predictions)\r\n\r\n@tf.function\r\ndef test_step(images, labels):\r\n    predictions = model(images)\r\n    t_loss = loss_object(labels, predictions)\r\n\r\n    test_loss(t_loss)\r\n    test_accuracy(labels, predictions)\r\n\r\n\r\nEPOCHS = 5\r\n\r\nfor epoch in range(EPOCHS):\r\n  for images, labels in train_ds:\r\n    train_step(images, labels)\r\n\r\n  for test_images, test_labels in test_ds:\r\n    test_step(test_images, test_labels)\r\n\r\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\r\n  print(template.format(epoch+1,\r\n                        train_loss.result(),\r\n                        train_accuracy.result()*100,\r\n                        test_loss.result(),\r\n                        test_accuracy.result()*100))\r\n\r\n  # Reset the metrics for the next epoch\r\n  train_loss.reset_states()\r\n  train_accuracy.reset_states()\r\n  test_loss.reset_states()\r\n  test_accuracy.reset_states()\r\n\r\n```\r\n\r\n\r\n\r\n**System information**\r\n\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n    TensorFlow installed from (source or binary): Source and Binary (tried both)\r\n    TensorFlow version: 2.0\r\n    Python version: 3.7.4\r\nGPU: 1080 Ti\r\n    Installed using virtualenv? pip? conda?: conda\r\n    Bazel version (if compiling from source): 0.18\r\n    GCC/Compiler version (if compiling from source): gcc 5.4.0\r\n\r\n**Package:**\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\nabsl-py                   0.8.1                    pypi_0    pypi\r\nastor                     0.8.0                    pypi_0    pypi\r\nblas                      1.0                         mkl  \r\nca-certificates           2019.10.16                    0  \r\ncertifi                   2019.9.11                py37_0  \r\ncudatoolkit               10.1.168                      0  \r\ncudnn                     7.6.0                cuda10.1_0  \r\ncycler                    0.10.0                   py37_0  \r\ndbus                      1.13.6               h746ee38_0  \r\nexpat                     2.2.6                he6710b0_0  \r\nfontconfig                2.13.0               h9420a91_0  \r\nfreetype                  2.9.1                h8a8886c_1  \r\ngast                      0.2.2                    pypi_0    pypi\r\nglib                      2.56.2               hd408876_0  \r\ngoogle-pasta              0.1.7                    pypi_0    pypi\r\ngrpcio                    1.24.3                   pypi_0    pypi\r\ngst-plugins-base          1.14.0               hbbd80ab_1  \r\ngstreamer                 1.14.0               hb453b48_1  \r\nh5py                      2.10.0                   pypi_0    pypi\r\nicu                       58.2                 h9c2bf20_1  \r\nintel-openmp              2019.4                      243  \r\njpeg                      9b                   h024ee3a_2  \r\nkeras-applications        1.0.8                    pypi_0    pypi\r\nkeras-preprocessing       1.1.0                    pypi_0    pypi\r\nkiwisolver                1.1.0            py37he6710b0_0  \r\nlibedit                   3.1.20181209         hc058e9b_0  \r\nlibffi                    3.2.1                hd88cf55_4  \r\nlibgcc-ng                 9.1.0                hdf63c60_0  \r\nlibgfortran-ng            7.3.0                hdf63c60_0  \r\nlibpng                    1.6.37               hbc83047_0  \r\nlibstdcxx-ng              9.1.0                hdf63c60_0  \r\nlibuuid                   1.0.3                h1bed415_2  \r\nlibxcb                    1.13                 h1bed415_1  \r\nlibxml2                   2.9.9                hea5a465_1  \r\nmarkdown                  3.1.1                    pypi_0    pypi\r\nmatplotlib                3.1.1            py37h5429711_0  \r\nmkl                       2019.4                      243  \r\nmkl-service               2.3.0            py37he904b0f_0  \r\nmkl_fft                   1.0.14           py37ha843d7b_0  \r\nmkl_random                1.1.0            py37hd6b4f25_0  \r\nncurses                   6.1                  he6710b0_1  \r\nnumpy                     1.17.3                   pypi_0    pypi\r\nnumpy-base                1.17.2           py37hde5b4d6_0  \r\nopenssl                   1.1.1d               h7b6447c_3  \r\nopt-einsum                3.1.0                    pypi_0    pypi\r\npandas                    0.25.2           py37he6710b0_0  \r\npcre                      8.43                 he6710b0_0  \r\npip                       19.3.1                   py37_0  \r\nprotobuf                  3.10.0                   pypi_0    pypi\r\npyparsing                 2.4.2                      py_0  \r\npyqt                      5.9.2            py37h05f1152_2  \r\npython                    3.7.4                h265db76_1  \r\npython-dateutil           2.8.0                    py37_0  \r\npytz                      2019.3                     py_0  \r\nqt                        5.9.7                h5867ecd_1  \r\nreadline                  7.0                  h7b6447c_5  \r\nsetuptools                41.4.0                   py37_0  \r\nsip                       4.19.8           py37hf484d3e_0  \r\nsix                       1.12.0                   pypi_0    pypi\r\nsqlite                    3.30.0               h7b6447c_0  \r\ntensorboard               2.0.0                    pypi_0    pypi\r\ntensorflow-estimator      2.0.1                    pypi_0    pypi\r\ntensorflow-gpu            2.0.0                    pypi_0    pypi\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\ntk                        8.6.8                hbc83047_0  \r\ntornado                   6.0.3            py37h7b6447c_0  \r\nwerkzeug                  0.16.0                   pypi_0    pypi\r\nwheel                     0.33.6                   py37_0  \r\nwrapt                     1.11.2                   pypi_0    pypi\r\nxz                        5.2.4                h14c3975_4  \r\nzlib                      1.2.11               h7b6447c_3  \r\n\r\n\r\n**Error**\r\n2019-10-24 00:02:55.586964: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2019-10-24 00:02:55.588179: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.5.0 but source was compiled with: 7.6.0.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n2019-10-24 00:02:55.588889: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[{{node my_model/conv2d/Conv2D}}]]\r\nTraceback (most recent call last):\r\n  File \"/home/jiahao/Documents/little_fun/test.py\", line 73, in <module>\r\n    train_step(images, labels)\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"/home/jiahao/wang/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\t [[node my_model/conv2d/Conv2D (defined at /wang/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_train_step_638]\r\n\r\nFunction call stack:\r\ntrain_step\r\n", "comments": []}, {"number": 33665, "title": "[Docker] The 'tensorflow/tensorflow:1.15.0-py3' is missing on DockerHub", "body": "All other combinations of `tensorflow:1.15.0[-gpu][-py3][-jupyter]` are available.", "comments": ["Thank you for your report. It looks like our CI failed while uploading that specific image. I've re-deployed the missing image, and I'm investigating the CI system to ensure it catches future errors.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33665\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33665\">No</a>\n"]}, {"number": 33664, "title": "Some errors when running keras-yolo3 as a child process ", "body": "Hello! I am confronted with some errors and don't know how to solve them When running keras-yolo3 module as a child process. By setting breakpoints, I found erors appearing at the statement in yolo.py \"sout_boxes, out_scores, out_classes = self.sess()\". Concrete errors are shown as follows:\r\n1 [tensorflow/core/grappler/clusters/utils.cc:82] Failed to get device properties, error code: 3\r\n2 Python failed to enqueue async from host to device: cuda _error_not_initialized;\r\nGPU dst: \u2026\u2026 ; host src: \u2026\u2026 ;size: \u2026\u2026\r\n\r\nI am looking forward to getting replys from warm-hearted you, thanks!", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "@050214221 \r\n\r\nAny update please. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 33663, "title": "Can not build debug wheel due to OverflowError: Size does not fit in an unsigned int", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.13\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.19\r\n- GCC/Compiler version (if compiling from source): 4.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: \r\n\r\n\r\n**Describe the problem**\r\n\r\nI cannot create the wheel file for a TensorFlow build with debug symbols. According to #25026, it seems that the target file is too big for pip to generate. I have to debug the TensorFlow inner c++ code, is there anyone know the solution for this problem plz?\r\n\r\ncommand: \r\nbazel build --spawn_strategy=standalone --verbose_failures --config=cuda --copt=\"-fPIC\" --copt=\"-DNDEBUG\" --local_resources 11048,2.0,2.0 -c dbg --copt -g //tensorflow/tools/pip_package:build_pip_package\r\nand then\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\noutput:\r\ndata = co.compress(data) + co.flush()\r\nOverflowError: Size does not fit in an unsigned int\r\n\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 294, in <module>\r\n    keywords='tensorflow tensor machine learning',\r\n  File \"/usr/local/lib/python3.5/dist-packages/setuptools/__init__.py\", line 145, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/usr/lib/python3.5/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/usr/lib/python3.5/distutils/dist.py\", line 955, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/usr/lib/python3.5/distutils/dist.py\", line 974, in run_command\r\n    cmd_obj.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/wheel/bdist_wheel.py\", line 250, in run\r\n    wf.write_files(archive_root)\r\n  File \"/usr/local/lib/python3.5/dist-packages/wheel/wheelfile.py\", line 122, in write_files\r\n    self.write(path, arcname)\r\n  File \"/usr/local/lib/python3.5/dist-packages/wheel/wheelfile.py\", line 136, in write\r\n    self.writestr(zinfo, data, compress_type)\r\n  File \"/usr/local/lib/python3.5/dist-packages/wheel/wheelfile.py\", line 139, in writestr\r\n    ZipFile.writestr(self, zinfo_or_arcname, bytes, compress_type)\r\n  File \"/usr/lib/python3.5/zipfile.py\", line 1573, in writestr\r\n    data = co.compress(data) + co.flush()\r\nOverflowError: Size does not fit in an unsigned int\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33663\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33663\">No</a>\n", "This might be the problem between Python 3.5 and CUDA. \r\nI upgrade the Python to 3.7 and downgrade CUDA to 9 (cuDNN to 7), then the error was gone.", "upgrading your python version to 3.7+ will do"]}, {"number": 33662, "title": "How to write a dataset of serialized examples directly to a tfrecords file?", "body": "I tried asking the broader community for this, but nothing came back, so I'm asking it here.\r\n\r\nI have a `tf.data.Dataset` that contains serialized tfrecord/protobuf examples:\r\n\r\n```python\r\n>>> example = ds.make_one_shot_iterator().get_next()\r\n>>> example\r\n<tf.Tensor: id=42, shape=(), dtype=string, numpy=b'\\n\\xdd:\\n\\r\\n\\x01y\\x12\\x08\\x12\\x06\\n\\x...'>\r\n```\r\nSo each example in the dataset `ds` is just a flat byte string.\r\n\r\nWhat I'd like to do is to write these byte strings to disk (tfrecords file). The way I know to do this is to go through python land. For instance, using a tf1 session, this would look like:\r\n```python\r\nexample = ds.make_one_shot_iterator().get_next()\r\n\r\nwith tf.io.TFRecordWriter(\"/path/to/output/data.tfrecords\") as w:\r\n    with tf.Session() as sess:\r\n        w.write(sess.run(example))\r\n```\r\nThis doesn't feel very efficient to me. So my question is:\r\n\r\n- Is there a way that I can write this to a file without having to go through the python layer?\r\n", "comments": ["Does the [following guide](https://www.tensorflow.org/tutorials/load_data/tfrecord#tfrecord_files_using_tfdata) address your question?\r\n\r\ncc @MarkDaoust ", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33662\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33662\">No</a>\n"]}, {"number": 33661, "title": "Iterator initialization fails on GPU, sometimes segfaults on creating next batch", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: 2 Tesla K80s\r\n\r\n**Describe the current behavior**\r\nsession fails to initialize iterable dataset on GPU with message `tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for GPU devices compatible with node {{node compression_type}}`.  No such issue occurs on CPU.  In some cases, the dataset initializes, but fetching the next set of data causes a segfault.  I have been unable to create simple code which reproduces the second effect.\r\n\r\n**Describe the expected behavior**\r\ndataset initializes and no segfault when creating next batch\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.python.client import device_lib\r\n\r\ndef parse_line(line):\r\n    items = tf.string_split([line])\r\n    token_ids = tf.string_to_number(items.values, out_type=tf.int32)\r\n    return token_ids\r\n\r\ndef get_gpu_name():\r\n    devices = device_lib.list_local_devices()\r\n    for device in devices:\r\n        if device.device_type == \"GPU\":\r\n            return device.name\r\n\r\ndef main():\r\n    data_file = \"data_file.txt\"\r\n    with open(data_file, 'w', encoding='utf-8') as f:\r\n        f.write(\"0 1 2 3 4 5 6 7 8\")\r\n    dataset = tf.data.TextLineDataset(data_file).map(parse_line)\r\n    data_iter = dataset.make_initializable_iterator()\r\n    gpu_device = get_gpu_name()\r\n    \r\n    with tf.device(gpu_device):\r\n        features = data_iter.get_next()\r\n\r\n    with tf.Session() as session:\r\n        session.run(data_iter.initializer) # <- tensorflow fails on this line\r\n        session.run(features) # <- tensorflow sometimes segfaults on this line\r\n```\r\n\r\n**Other info / logs**\r\n[gpu.log](https://github.com/tensorflow/tensorflow/files/3765376/gpu.log)\r\n\r\n", "comments": ["I tried switching cuda versions from 10.1 to 9.0.  The error still occurs.  Can anyone give some clarity on possible causes for this bug?", "@neonrights \r\nCan you try switching to cuda 10.0 and check (The TF-GPU 1.14 prebuilt binary supports cuda 10.0) . Also, looks like code is incomplete and share sample data file `data_file.txt` to reproduce the issue in our environment. It will help us in localizing the issue faster. Thanks!", "@ravikyram  `data_file.txt` is created by the script in `main()`\r\n\r\nIt's just a file containing vectors encoded as integers separated by spaces on each line e.g.\r\n```\r\n0 1 2 3 4 5 6 7 8 9\r\n10 4 17 33\r\n4 4 4 4 4 4 4\r\n```\r\n\r\nSwitching to 10.0 fixes the issue in both the code sample above and my actual model training code.", "@neonrights \r\n\r\nPlease, let me know if we can close this issue since it looks to be fixed. Thanks!", "yes you can close the issue"]}, {"number": 33660, "title": "tf.test.compute_gradient (v2) expects wrong empty gradient shape", "body": "**System Information**\r\n\r\n- Have I written custom code: YES (see below)\r\n- OS Platform and Distribution: Ubuntu 18.04.3 LTS (inside `tensorflow/tensorflow:2.0.0-gpu`)\r\n- Mobile device: N/A\r\n- TensorFlow installed from: binary (inside `tensorflow/tensorflow:2.0.0-gpu`)\r\n- TensorFlow version: 2.0.0\r\n- Python version: 2.7.15+\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: TITANV 12GB\r\n\r\n**Current Behavior**\r\nWhen the gradient tensors are totally empty (i.e. all their dimensions are zero), and more than one input is considered at the same time,  the shape of an expected gradient tensor does not match the actual shape produced by the op's gradient function. This case is not currently tested in `tensorflow/python/ops/gradient_checker_v2_test.py` (see `testEmptyMatMul`).\r\n\r\n*UPDATE*: I also tried the configuration in `testEmptyMatMul` (see additional code below) and found that this also tickles the apparent bug. Surprisingly, `//tensorflow/python:gradient_checker_v2_test` passes when running on tag v2.0.0; I don't currently understand why.\r\n\r\nI have tested\r\n  * with two different ops, each with two inputs (both fail).\r\n  * cases where not all of the gradient tensors are empty (as in `testEmptyMatMul`).\r\n  * `tf.compat.v1.test.compute_gradient` with more than one empty input does **not** exhibit this behavior, so it was probably not present in `tensorflow/python/ops/gradient_checker.py` but was introduced in `tensorflow/python/ops/gradient_checker_v2.py`. See code at the end, in the \"TensorFlow Version 1 API\" section, that demonstrates this.\r\n\r\nI have **not** tested\r\n  * using ops with more than two inputs\r\n\r\n**Expected Behavior**\r\nThese exceptions should not be produced when using built-in ops. Either the ops' gradient functions are producing the wrong shaped tensors (which would be a bug), or, more likely, the code in `gradient_checker_v2.py` is expecting the wrong shape.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef empty(rank):\r\n  shape = (0,) * rank\r\n  return np.array([]).reshape(shape)\r\n\r\n# comment-out the first to run the second\r\ntf.test.compute_gradient(tf.nn.bias_add, [empty(3), empty(1)])\r\ntf.test.compute_gradient(tf.linalg.matmul, [empty(2), empty(3)])\r\n```\r\n\r\nThe following is essentially the same as the code in `testEmptyMatMul` in `tensorflow/python/ops/gradient_checker_v2_test.py`: \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef random_tensor(shape):\r\n  return tf.constant(np.random.random_sample(shape))\r\n\r\ndef f(x, y):\r\n  return tf.linalg.matmul(x, y)\r\n\r\nx = random_tensor((0, 3))\r\ny = random_tensor((3, 4))\r\n\r\njacobians = tf.test.compute_gradient(f, [x, y])\r\n```\r\n\r\n**Other info / logs**\r\n\r\nOutput from the above code:\r\n```\r\nTraceback (most recent call last):\r\n  File \"repro_eager_issue.py\", line 24, in <module>\r\n    tf.test.compute_gradient(tf.nn.bias_add, [empty(3), empty(1)])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 332, in compute_gradient\r\n    return _compute_gradient_list(f, x, delta)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 293, in _compute_gradient_list\r\n    xs, i, delta) for i in range(len(xs))])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 278, in _compute_gradient\r\n    xs, param)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 187, in _compute_theoretical_jacobian\r\n    (x.shape, grad.shape))\r\nValueError: Empty gradient has wrong shape: expected (0,), got (0, 0, 0)\r\n\r\nTraceback (most recent call last):\r\n  File \"repro_eager_issue.py\", line 25, in <module>\r\n    tf.test.compute_gradient(tf.linalg.matmul, [empty(2), empty(3)])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 332, in compute_gradient\r\n    return _compute_gradient_list(f, x, delta)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 293, in _compute_gradient_list\r\n    xs, i, delta) for i in range(len(xs))])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 278, in _compute_gradient\r\n    xs, param)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 187, in _compute_theoretical_jacobian\r\n    (x.shape, grad.shape))\r\nValueError: Empty gradient has wrong shape: expected (0, 0, 0), got (0, 0)\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_issue_33660.py\", line 77, in <module>\r\n    existing_test_repro()\r\n  File \"tf_issue_33660.py\", line 71, in existing_test_repro\r\n    jacobians = tf.test.compute_gradient(f, [x, y])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 332, in compute_gradient\r\n    return _compute_gradient_list(f, x, delta)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 293, in _compute_gradient_list\r\n    xs, i, delta) for i in range(len(xs))])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 278, in _compute_gradient\r\n    xs, param)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/gradient_checker_v2.py\", line 187, in _compute_theoretical_jacobian\r\n    (x.shape, grad.shape))\r\nValueError: Empty gradient has wrong shape: expected (3, 4), got (0, 3)\r\n```\r\n\r\n**Work-Around**\r\n\r\n`compute_gradient` can be called multiple times, once for each input, to get the analytical and numerical jacobians for each input separately. As, follows:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef empty(rank):\r\n  shape = (0,) * rank\r\n  return np.array([]).reshape(shape)\r\n\r\ninput_val = empty(3)\r\nbias_val = empty(1)\r\n\r\ndef bias_add_1(input_val):\r\n  return tf.nn.bias_add(input_val, bias_val)\r\n\r\ndef bias_add_2(bias_val):\r\n  return tf.nn.bias_add(input_val, bias_val)\r\n\r\ninput_jacobians = tf.test.compute_gradient(bias_add_1, [input_val])\r\nbias_jacobians = tf.test.compute_gradient(bias_add_2, [bias_val])\r\n```\r\n\r\n**TensorFlow Version 1 API**\r\n\r\nThe following code **does not** throw an exception, demonstrating that this problem does not exist with `tf.compat.v1.test.compute_gradient`.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef empty_tensor(shape):\r\n  return tf.constant([], shape=shape)\r\n\r\ntf.compat.v1.disable_eager_execution()\r\ninput_shape = output_shape = (0, 0, 0)\r\nbias_shape = (0,)\r\ninput_tensor = empty_tensor(input_shape)\r\nbias_tensor = empty_tensor(bias_shape)\r\noutput_tensor = tf.nn.bias_add(input_tensor, bias_tensor)\r\nwith tf.compat.v1.Session() as sess:\r\n  jacobians = tf.compat.v1.test.compute_gradient(\r\n      [input_tensor, bias_tensor], [input_shape, bias_shape], output_tensor,\r\n      output_shape)\r\n```", "comments": ["Issue replicating for TF-2.0, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/839b2999d53195a5dd8c98f22c31ab50/33660.ipynb) of the colab.Thanks!", "I just added an update to the description: \"I also tried the configuration in `testEmptyMatMul` (see additional code) and found that this also tickles the apparent bug. This suggests that `tensorflow/python/ops/gradient_checker_v2_test.py` is not being run as part of the CI.\"\r\n\r\nI also added some code to the end of the description that demonstrates that `tf.compat.v1.test.compute_gradient` does not exhibit this behavior.", "Surprisingly, `//tensorflow/python:gradient_checker_v2_test` passes when running on tag v2.0.0; I don't currently understand why.", "@duncanriach, I tried with Tf 2.1, its working without any error.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/0c11957c6fca4f94461cd57b63040389/untitled456.ipynb) and please close if issue is resolved. Thanks", "Thank you, @gadagashwini. I can confirm that this issue is resolved in TensorFlow version 2.1.0. Closing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33660\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33660\">No</a>\n"]}, {"number": 33659, "title": "[INTEL MKL] Fixed MKL QuantizeV2 operator.", "body": "There has been a new attribute added to QuantizeV2 (commit id: 4a8064512ca53c). Since MKL version was not updated accordingly, all quantization models with MKLDNN have been failing. This PR updates the MKL version to adapt those changes.\r\n\r\nThis PR also contains a small fix to `tensorflow/core/graph/mkl_layout_pass.cc` that was causing an abort in debug mode.", "comments": []}, {"number": 33658, "title": "Allow Keras ModelCheckpoint to save the best N checkpoints", "body": "The callback tf.keras.callbacks.ModelCheckpoint allows to save the best checkpoint using save_best_only=True. In the Estimator API we can use tf.estimator.BestExporter that saves the top N checkpoints (parameter exports_to_keep). \r\n\r\nIt seems using the Estimator API is becoming discouraged, for example I need to use compat v1 symbols to use the BestExporter.\r\n\r\nIs it possible to update the Keras ModelCheckpoint class to allow to save the top N models instead of just the best?\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\nYes, the constructor of tf.keras.callbacks.ModelCheckpoint needs a new parameter which is the equivalent of tf.estimator.BestExporter exports_to_keep. We can keep save_best_only but the two parameters can't be used as the same time.\r\n\r\n**Who will benefit with this feature?**\r\nEverybody that wants to save several models. For example for model averaging.\r\n\r\n**Any Other info.**\r\n", "comments": ["I wrote a [ModelCheckpoint subclass](https://github.com/schustmi/tf_utils/blob/915fe5e231ca302b28cd02dc8ac2e4c772a62e0b/tf_utils/callbacks.py#L34) that implements this if you're still looking for this functionality.", "@schustmi Thanks for the implementation.\r\n\r\n@iteal Sorry for the late response. Do you think @schustmi 's custom callback will work for you?\r\n\r\nAre you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33657, "title": "TPUStrategy breaks on subclassed keras models", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab TPU\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.7\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n%tensorflow_version 2.x\r\n\r\n# setup\r\n\r\nimport tensorflow as tf\r\nimport os\r\n\r\ntpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\r\ntf.config.experimental_connect_to_cluster(cluster_resolver)\r\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\r\nSTRATEGY = tf.compat.v1.distribute.experimental.TPUStrategy(cluster_resolver)\r\n\r\n# define a subclassed keras model\r\nclass Simple(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, x, training=None):\r\n        return self.dense(x, training=training)\r\n\r\n# a fake dataset\r\ndata = tf.data.Dataset.from_tensor_slices((\r\n    tf.random.uniform((10, 10)),\r\n    tf.random.uniform((10, 10))\r\n)).batch(10).repeat()\r\n\r\n# outside the TPUStrategy scope, the model works fine...\r\nmodel = Simple()\r\nmodel.compile(loss='categorical_crossentropy')\r\nmodel.evaluate(data, steps=1)\r\n# ==> 1/1 [==============================] - 0s 62ms/step - loss: 40.6148\r\n\r\n# but if we try and use the model in the scope...\r\nwith STRATEGY.scope():\r\n    model = Simple()\r\n    model.compile(loss='categorical_crossentropy')\r\n\r\nmodel.evaluate(data, steps=1)\r\n\r\n# we get an error:\r\n# \r\n# ValueError                                Traceback (most recent call last)\r\n# \r\n# <ipython-input-17-56ee0b4381e4> in <module>()\r\n#      29     model = Nested(model)\r\n#      30     model.compile(loss='categorical_crossentropy')\r\n# ---> 31 model.evaluate(data, steps=1)\r\n# \r\n# ... 12 frames ...\r\n# \r\n# /tensorflow-2.0.0/python3.6/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n#     903           except Exception as e:  # pylint:disable=broad-except\r\n#     904             if hasattr(e, \"ag_error_metadata\"):\r\n# --> 905               raise e.ag_error_metadata.to_exception(e)\r\n#     906             else:\r\n#     907               raise\r\n# \r\n# ValueError: in converted code:\r\n#     relative to /tensorflow-2.0.0/python3.6/tensorflow_core/python/keras:\r\n# \r\n#     distribute/distributed_training_utils.py:862 distributed_function  *\r\n#         x, y, sample_weights = input_fn()\r\n#     engine/training_arrays.py:516 get_distributed_inputs\r\n#         model, inputs, targets, sample_weights, mode)\r\n#     distribute/distributed_training_utils.py:638 _prepare_feed_values\r\n#         inputs, targets, sample_weights = _get_input_from_iterator(inputs, model)\r\n#     distribute/distributed_training_utils.py:616 _get_input_from_iterator\r\n#         x, y, sample_weights = next_element\r\n# \r\n#     ValueError: not enough values to unpack (expected 3, got 2)\r\n```\r\n\r\n**Expected behavior**\r\n\r\nThis should not throw any error.\r\n", "comments": ["Update: found a simpler repro.", "Issue is replicating on colab with TF 2.0. Please take a look at [gist](https://colab.sandbox.google.com/gist/gadagashwini/4060275d458dc7547a3ab4a6a670f126/untitled225.ipynb). Thanks!", "TPUs are not officially supported in TF2.0. Does the code work on 1.15 or a nightly version?", "@frankchn TF1.15.0 throws a different error as shown below. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/13ae47a9d0131514db9e760eec777e9f/untitled225.ipynb). \r\n\r\nError trace\r\n```\r\nINFO:tensorflow:Initializing the TPU system: 10.104.18.114:8470\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Querying Tensorflow master (grpc://10.104.18.114:8470) for TPU system metadata.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 12994194352412773709)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3779494438081468292)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7546761505036203465)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 15889901405504100012)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16304530786141444431)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 15091146302727302848)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 4279064904853472805)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 13012250068625670504)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 15180363532189083818)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 1141987253231946822)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2127150505038211523)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-b944ab9ba609> in <module>()\r\n     34     model.compile(loss='categorical_crossentropy')\r\n     35 \r\n---> 36 model.evaluate(data, steps=1)\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    235       except Exception as e:  # pylint:disable=broad-except\r\n    236         if hasattr(e, 'ag_error_metadata'):\r\n--> 237           raise e.ag_error_metadata.to_exception(e)\r\n    238         else:\r\n    239           raise\r\n\r\nValueError: in converted code:\r\n    relative to /usr/local/lib/python3.6/dist-packages/tensorflow_core/python:\r\n\r\n    keras/engine/training_distributed.py:333 _test_step_fn\r\n        _build_model, args=(model, mode, inputs, targets)))\r\n    distribute/distribute_lib.py:1940 merge_call\r\n        return self._merge_call(merge_fn, args, kwargs)\r\n    distribute/distribute_lib.py:1947 _merge_call\r\n        return merge_fn(self._strategy, *args, **kwargs)\r\n    keras/engine/training_distributed.py:58 _build_model\r\n        targets)\r\n    keras/distribute/distributed_training_utils.py:783 _build_distributed_network\r\n        args=(model, mode, inputs, targets))\r\n    distribute/distribute_lib.py:1810 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    distribute/tpu_strategy.py:372 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    keras/distribute/distributed_training_utils.py:743 _build_network_on_replica\r\n        model, input_tensors=inputs, layer_fn=models.share_weights)\r\n    keras/models.py:166 _clone_functional_model\r\n        raise ValueError('Expected `model` argument '\r\n\r\n    ValueError: Expected `model` argument to be a functional `Model` instance, but got a subclass model instead.\r\n```", "@rxsang can you help take a look? I am OOO today", "Side note, this can be worked around easily enough w/ custom training loops.", "@kazimuth,\r\nThe issue seems to be resolved with TensorFlow v2.4. \r\n\r\nI was able to run the code without any issues, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8cb7247ff498cb58347d0b161a174fe3/33657.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33657\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33657\">No</a>\n"]}, {"number": 33656, "title": "Error: No OpKernel was registered to support Op 'NcclAllReduce'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code/ but copy from the stock example (nothing out of the ordinary)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win10 1903\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.0.0\r\n- **Python version**: 3.7\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 10.0/7.6\r\n- **GPU model and memory**: 4x GTX780ti 3GB each\r\n- **Exact command to reproduce**: model_distributed.fit(X_train, y_train, epochs=4, batch_size=25)\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. 4x GPUs are recognised. but failed to train. The train on the CPU works well, but then when moving to do it with the distributed strategy shows the error below. I've uninstalled and re-installed \"pip install tensorflow-gpu\" twice. Updated drivers. nothing seems to help. Code below. Any help will be highly appreciated. Thanks.\r\n\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\nX_train = X_train / 255.\r\nX_test = X_test / 255.\r\n\r\nX_train = X_train.reshape(-1, 28*28)\r\nX_test = X_test.reshape(-1, 28*28)\r\n\r\nmodel_normal = tf.keras.models.Sequential()\r\nmodel_normal.add(tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)))\r\nmodel_normal.add(tf.keras.layers.Dropout(rate=0.2))\r\nmodel_normal.add(tf.keras.layers.Dense(units=10, activation='softmax'))\r\nmodel_normal.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\n\r\n\r\nwith strategy.scope():\r\n  model_distributed = tf.keras.Sequential([\r\n      tf.keras.layers.Dense(units=128, activation='relu', input_shape=(784,)),\r\n      tf.keras.layers.Dropout(rate=0.2),\r\n      tf.keras.layers.Dense(units=10, activation='softmax'),\r\n      ])\r\n  model_distributed.compile(loss='sparse_categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\n\r\nstart_time = time.time()\r\nmodel_normal.fit(X_train, y_train, epochs=4, batch_size=25)\r\nprint(\"Normal training took: {}\".format(time.time() - start_time))\r\n\r\nstart_time = time.time()\r\nmodel_distributed.fit(X_train, y_train, epochs=4, batch_size=25)\r\nprint(\"Distributed training took: {}\".format(time.time() - start_time))\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n2019-10-24 07:50:25.464075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2.0.0\r\n2019-10-24 07:50:29.987980: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-10-24 07:50:30.090327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.15\r\npciBusID: 0000:01:00.0\r\n2019-10-24 07:50:30.097223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:02:00.0\r\n2019-10-24 07:50:30.104573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0845\r\npciBusID: 0000:03:00.0\r\n2019-10-24 07:50:30.111754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.15\r\npciBusID: 0000:04:00.0\r\n2019-10-24 07:50:30.118501: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-10-24 07:50:30.126709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-10-24 07:50:30.131151: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-10-24 07:50:30.809583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.15\r\npciBusID: 0000:01:00.0\r\n2019-10-24 07:50:30.816475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:02:00.0\r\n2019-10-24 07:50:30.823594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0845\r\npciBusID: 0000:03:00.0\r\n2019-10-24 07:50:30.831552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.15\r\npciBusID: 0000:04:00.0\r\n2019-10-24 07:50:30.839507: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-10-24 07:50:30.848211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-10-24 07:50:32.420376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-24 07:50:32.465449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 2 3\r\n2019-10-24 07:50:32.491610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N N N\r\n2019-10-24 07:50:32.494431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N N N\r\n2019-10-24 07:50:32.506160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 2:   N N N N\r\n2019-10-24 07:50:32.544545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 3:   N N N N\r\n2019-10-24 07:50:32.572199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2154 MB memory) -> physical GPU (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:01:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:32.614321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 2154 MB memory) -> physical GPU (device: 1, name: GeForce GTX 780 Ti, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:32.670375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 2154 MB memory) -> physical GPU (device: 2, name: GeForce GTX 780 Ti, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:32.725417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 2154 MB memory) -> physical GPU (device: 3, name: GeForce GTX 780 Ti, pci bus id: 0000:04:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:33.211267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.15\r\npciBusID: 0000:01:00.0\r\n2019-10-24 07:50:33.310413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0455\r\npciBusID: 0000:02:00.0\r\n2019-10-24 07:50:33.383430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.0845\r\npciBusID: 0000:03:00.0\r\n2019-10-24 07:50:33.423598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties:\r\nname: GeForce GTX 780 Ti major: 3 minor: 5 memoryClockRate(GHz): 1.15\r\npciBusID: 0000:04:00.0\r\n2019-10-24 07:50:33.446267: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-10-24 07:50:33.463455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-10-24 07:50:33.467760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-10-24 07:50:33.472212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 2 3\r\n2019-10-24 07:50:33.475053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N N N\r\n2019-10-24 07:50:33.478236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N N N\r\n2019-10-24 07:50:33.481176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 2:   N N N N\r\n2019-10-24 07:50:33.486383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 3:   N N N N\r\n2019-10-24 07:50:33.492208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 2154 MB memory) -> physical GPU (device: 0, name: GeForce GTX 780 Ti, pci bus id: 0000:01:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:33.501615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 2154 MB memory) -> physical GPU (device: 1, name: GeForce GTX 780 Ti, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:33.510891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:2 with 2154 MB memory) -> physical GPU (device: 2, name: GeForce GTX 780 Ti, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2019-10-24 07:50:33.520186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:3 with 2154 MB memory) -> physical GPU (device: 3, name: GeForce GTX 780 Ti, pci bus id: 0000:04:00.0, compute capability: 3.5)\r\nNumber of devices: 4\r\nTrain on 60000 samples\r\nEpoch 1/4\r\n2019-10-24 07:50:35.783397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll\r\n2019-10-24 07:50:35.958561: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2019-10-24 07:50:35.963318: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2019-10-24 07:50:35.967782: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n2019-10-24 07:50:35.974357: E tensorflow/stream_executor/cuda/cuda_blas.cc:238] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED\r\n60000/60000 [==============================] - 10s 165us/sample - loss: 0.2815 - sparse_categorical_accuracy: 0.9183\r\nEpoch 2/4\r\n60000/60000 [==============================] - 8s 132us/sample - loss: 0.1353 - sparse_categorical_accuracy: 0.9603\r\nEpoch 3/4\r\n60000/60000 [==============================] - 8s 133us/sample - loss: 0.1052 - sparse_categorical_accuracy: 0.9674\r\nEpoch 4/4\r\n60000/60000 [==============================] - 8s 131us/sample - loss: 0.0841 - sparse_categorical_accuracy: 0.9741\r\nNormal training took: 33.73987317085266\r\nTrain on 60000 samples\r\nEpoch 1/4\r\n   25/60000 [..............................] - ETA: 3:18:14Traceback (most recent call last):\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\ptvsd_launcher.py\", line 119, in <module>\r\n    vspd.debug(filename, port_num, debug_id, debug_options, run_as)\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\debugger.py\", line 39, in debug\r\n    run()\r\n  File \"c:\\program files (x86)\\microsoft visual studio\\2019\\community\\common7\\ide\\extensions\\microsoft\\python\\core\\Packages\\ptvsd\\__main__.py\", line 316, in run_file\r\n    runpy.run_path(target, run_name='__main__')\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\Javier\\Desktop\\Personal files\\VS\\TensorFlow 2.0 Complete guide\\distributed training.py\", line 53, in <module>\r\n    model_distributed.fit(X_train, y_train, epochs=4, batch_size=25)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 728, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 324, in fit\r\n    total_epochs=epochs)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 123, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 86, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 520, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'NcclAllReduce' used by {{node Adam/NcclAllReduce}}with these attrs: [reduction=\"sum\", shared_name=\"c1\", T=DT_FLOAT, num_devices=4]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n         [[Adam/NcclAllReduce]] [Op:__inference_distributed_function_33245]\r\nPress any key to continue . . .\r\n", "comments": ["@PepeLotudo, Please take a look at [this link](https://www.tensorflow.org/guide/gpu#using_multiple_gpus) for multiple GPUs. `tf.config.experimental.set_memory_growth`  which attempts to allocate only as much GPU memory as needed for the runtime allocations. Please update your code accordingly and try. Please let us know how it progresses. Thanks!", "Hi gadagashwini,\r\n\r\nmanaged to get it to work if I replace \"strategy = tf.distribute.MirroredStrategy()\" with \"strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\".\r\n\r\nBut the performance is way slower than doing it directly on the CPU.\r\n\r\nWhen on CPU only: 39secs\r\nWhen on GPU:1 only: 18.5 secs\r\nWhen on tf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()): 65 secs\r\nWhen on tf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce(), device=GPU:1): 21 secs\r\n\r\nAll GPUs are the same. a bit confused why this is happening and wondering is getting it to work with the NCCL will make any difference.\r\n\r\nI did implemented the tf.config.experimental.set_memory_growth(device=GPUdevice,enable=True)... all I noticed was the GPU memory was not allocated 100%. apart from that no difference.\r\n\r\nAre you suggesting I create several logical GPUs inside one GPU? I want to be able to have better performance over 4x GPUs than a single one.\r\n\r\nRegards,", "```\r\n2019-10-24 07:50:33.475053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N N N N\r\n2019-10-24 07:50:33.478236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1: N N N N\r\n2019-10-24 07:50:33.481176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 2: N N N N\r\n2019-10-24 07:50:33.486383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 3: N N N N\r\n```\r\n\r\nTensorFlow logs device connectivity, and what this is saying is that there is no peer-to-peer connections on your GPUs. For reference on a 4xV100 cloud instance the topology is:\r\n```\r\n...] 0:   N Y Y Y\r\n...] 1:   Y N Y Y\r\n...] 2:   Y Y N Y\r\n...] 3:   Y Y Y N\r\n```\r\n\r\nNCCL is Nvidia's library for cross device reduction. The main point is to take advantage of the peering connections and bypass the CPU / PCI, so even if you got it working (I suspect that you still have a misconfigured nccl, but unfortunately don't have time to debug your setup) I would expect that you would get similar performance to hierarchical copy because there's nothing a library can do but go through the host if there are no peering connections. A simple way to check this is with the tensorflow profiler (https://www.tensorflow.org/api_docs/python/tf/summary/trace_on?version=stable). If you see a giant block of reductions and cross device copies in the timeline, then you're probably device interconnect bound. Hope that helps.", "This is fixed latest tf-nightly. Thanks!", "Hi.  I am getting the same error, yet I am using the tf-nightly.  The workaround works for me as well.", "`tf.distribute.MirroredStrategy()` uses NCCL in default. Apparently NCCL is not installed in your system, tha's why your have to change the call to `tf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())`. See the documentation about `cross_device_ops` parameter [https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy](url). can you confirm?", "I'll give it a try.\n\nOn Tue, Sep 15, 2020 at 9:22 AM Hicham Bellafkir <notifications@github.com>\nwrote:\n\n>\n>\n> tf.distribute.MirroredStrategy() uses NCCL in default. Apparently NCCL is\n> not installed in your system, tha's why your have to change the call to\n> tf.distribute.mirrorstrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()).\n> See the documentation about cross_device_ops parameter\n> https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy\n> <http://url>. can you confirm?\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-692710505>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJLIJ7EXEDA6KEQV3J6F77DSF5TANANCNFSM4JEM4IXQ>\n> .\n>\n>\n>\n", "@PepeLotudo,\r\nWith respect to [this comment](https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-601958566), can you please confirm if the issue is resolved? If you face problems please check if you have installed **`NCCL`** as mentioned in [this comment](https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-692710505). Thanks!", "I dont know if it was fixed or not. Not using TF anymore. I moved to\npytorch.\nRegards\n\n\nOn Mon, 31 May 2021, 20:17 rmothukuru, ***@***.***> wrote:\n\n> @PepeLotudo <https://github.com/PepeLotudo>,\n> With respect to this comment\n> <https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-601958566>,\n> can you please confirm if the issue is resolved? If you face problems\n> please check if you have installed *NCCL* as mentioned in this comment\n> <https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-692710505>.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-851451158>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ANPXRY7ZLGBLKWKEB4NLNJLTQN45JANCNFSM4JEM4IXQ>\n> .\n>\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I will test it this week.\n\nOn Mon, Jun 7, 2021, 8:51 AM google-ml-butler[bot] ***@***.***>\nwrote:\n\n> This issue has been automatically marked as stale because it has not had\n> recent activity. It will be closed if no further activity occurs. Thank you.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/33656#issuecomment-855897412>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJLIJ7HMBMSOMZWYD727YXTTRS6FBANCNFSM4JEM4IXQ>\n> .\n>\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33655, "title": "Exception in thread \"main\" java.lang.IllegalStateException: Table not initialized.", "body": "When training model in tensorflow using tf.contrib.lookup.HashTable and tf.contrib.lookup.KeyValueTensorInitializer, and then save model as .pb, it works when load .pb and predict in tensorflow,\r\nbut when importGraphDef in java and predict, an error occurred as below:\r\n\r\n2019-10-24 07:24:29.753767: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753768: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753784: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753840: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753852: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753865: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753852: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753905: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753905: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\n2019-10-24 07:24:29.753933: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at lookup_table_op.cc:674 : Failed precondition: Table not initialized.\r\nException in thread \"main\" java.lang.IllegalStateException: Table not initialized.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n"]}, {"number": 33654, "title": "Strange behavior when importing `from` TF submodules", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but have tested in a standalone bare Python script\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X 10.14\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n```python\r\nfrom tensorflow import data  # ==> imports the tf.data submodule\r\nfrom tensorflow.data import Dataset  # ==> Raises: ImportError: No module named 'tensorflow.data'\r\n```\r\n\r\n**Describe the expected behavior**\r\nI would expect the Dataset class to be imported.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\nfrom tensorflow.data import Dataset\r\n```", "comments": ["@jvmancuso ,\r\nHello, yes for TF 2.0 version please try using \r\n`from tensorflow import                                                                                                                      \r\n data data=data.dataset` it should work fine.\r\nFind [gist](https://colab.sandbox.google.com/gist/oanush/a21d362af3a3c38024364fea7dab3237/33654.ipynb) colab  for the same.Thanks!\r\n", "Yes, that works!  However, that seems to be a workaround/anti-pattern and not a fix, so still seems like a bug to me.", "See https://github.com/tensorflow/tensorflow/issues/32957#issuecomment-543819065\r\n\r\nTL;DR: the only supported API usage is\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.get.here.all.the.attributes.you.want\r\n```\r\n\r\nAny other API usage can break at any time and fixing it is a best effort job.\r\n\r\n(I linked to that comment as it proves that if you can do `from module import name` and `name.other_name` that doesn't imply you can also do `from module.name import other_name` always)"]}, {"number": 33653, "title": "1.14", "body": "", "comments": []}]