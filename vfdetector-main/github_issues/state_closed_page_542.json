[{"number": 37464, "title": "[TF XLA AOT] Make sure required sources are included in pip package.", "body": "(bugfix) We thought the previous CL ensured this, but looks like there\r\nwere some missing pieces in the genrule & MANIFEST files.\r\n\r\nPiperOrigin-RevId: 298715688\r\nChange-Id: Ib0a20cb10fb3b36686419b02c2f8ab6afd438d57", "comments": []}, {"number": 37463, "title": "ImportError: No module named builtins", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): Pip3 \r\n- TensorFlow version (use command below): 2.1.0  \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n![Screenshot from 2020-03-10 08-30-00](https://user-images.githubusercontent.com/41910134/76275645-c9089780-62a9-11ea-8798-686a3a8d8412.png)\r\n\r\nI am unable to complete the util test.\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nbazel test ${flags} //tensorflow/python/keras\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ayushmankumar7 \r\nAs there is an existing PR monitoring the issue, proceeding to move this issue to closed status as it would be a duplicate for #37425 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37463\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37463\">No</a>\n"]}, {"number": 37462, "title": "Custom rnn migration to tensorflow 2.x", "body": "I want to migrate tensorflow 1.x code to 2.x. I have tried migration script and refererd migration guide. But still its unclear that how to migrate \r\n1.place holder with shape such as [x,None,y] . \r\n2. Convert tf_get_variable with xavier initialization\r\nPlease find the stackoverflow post link (not answered)\r\n\r\nhttps://stackoverflow.com/questions/60601770/custom-rnn-migration-to-tensorflow-2-0", "comments": ["@dynamicwebpaige ", "@npv0 did you try compat.v1 ?\r\n", "> I want to migrate tensorflow 1.x code to 2.x. I have tried migration script and refererd migration guide. But still its unclear that how to migrate\r\n> 1.place holder with shape such as [x,None,y] .\r\n\r\nIn idiomatic TF2 code, you should convert any `tf.Placeholder` objects to function arguments to a `@tf.function` that replaces your `tf.compat.v1.Session()`. \r\n\r\n> 2. Convert tf_get_variable with xavier initialization\r\n\r\nAs stated in the [Migration Guide](https://www.tensorflow.org/guide/migrate), replace all `tf.get_variable()` calls with `tf.Variable()` and the `tf.variable_scope`s should be wrapped in python objects like [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer). Where you can also specify which initializer you want to use. \r\n\r\nIf you don't want to build a layer, you can also use `tf.Module`, which is more general than a layer. \r\n\r\n    class MyModel(tf.Module):\r\n      def __init__(self, ins, outs):\r\n       initializer = tf.initializers.GlorotNormal() # Xavier initialization\r\n       self.W = tf.Variable(initializer([ins, outs]))\r\n       self.B = tf.Variable(tf.zeros([outs], dtype = tf.float32))\r\n\r\nEdit:\r\n\r\nI also answered to your Stack Overflow question. I hope this helps you.\r\n\r\n\r\n", "@npv0,\r\nCould you please check @RobinBaumann's comment and let us know if it works? Thanks!", "Any updates regarding this issue? Thanks!", "@RobinBaumann @ayushmankumar7 Thanks a lot for the suggestions. I am trying both methods. \r\n@amahendrakar sorry I took some time to provide an update here as I was travelling from my city to hometown.\r\nI will post my detailed update this week.", "Please find the below 1.x code and my attempt to convert it to 2.x using tf.Module.\r\nThe 2.x code is still under development and the implementation of recurrence part is not working as of now.\r\ntf 1.x code:  https://gist.github.com/np000/a9123dcc64fdf1f4f13b1c1e56ee4e6e\r\nand my conversion attempt : https://gist.github.com/np000/92d79cb1e73ea06a55e0db355da5437a", "can anyone please check my gist file and share me a feedback.\r\n@RobinBaumann @mihaimaruseac @qlzh727 @goldiegadde \r\nThanks", "@ayushmankumar7 I have tried tf.compat.v1, this worked. My goal is to make a custom tf.module or tf.keras model and then train rnn using autograd or model.fit.\r\nAs I am using tf.scan in 1.x code in the recurrence section, I was wondering how it could be used to train a custom rnn tf.keras model or tf.module ", "i see now the issue got assigned to @gowthamkpr , just following up now. any feedback\r\nthanks", "@np000 Is your issue resolved or are you still facing the same issue? Thanks!", "@gowthamkpr still facing the same issue! ", ">  the implementation of recurrence part is not working as of now.\r\n\r\nHi @np000, \r\n\r\nI think you just need to practice reading your error messages. When I copy-paste your code into colab it says you're passing an unexpected argument to the constructor. \"Training\" is an argument for when you run a layer or model not when you build it. Keep working at it, I think these are errors you can handle yourself.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-2-263240bbee7e> in <module>()\r\n----> 1 main()\r\n\r\n<ipython-input-1-56c6ddde70bd> in main()\r\n     75 \r\n     76     model = Rnn(features=features, n_hid=n_hid, init_state=init_state, training=training,\r\n---> 77                        n_out=n_out)\r\n     78     logits = model.get_logits()\r\n     79     final_logits = model.get_final_logits()\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'training'\r\n```\r\n\r\n\r\n\r\n"]}, {"number": 37461, "title": "Installation for Python 3.8.1?", "body": "Hey , just wanted to confirm if you now have the support for Python 3.8.1? If not, I want to use tensorflow 1.15, which python version works best with it?\r\n\r\n", "comments": ["@ali-chaudhry8 As I know, TensorFlow 1.15 doesn't support Python 3.8, you could use Python 3.7 to work with it\r\nrelated to #33374 ", "We don't back-release old releases on newer versions of python.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37461\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37461\">No</a>\n"]}, {"number": 37460, "title": "map_fn + @tf.function + tf.nn.conv2d throws error when strides to conv2d are supplied in the elems argument for map_fn", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu GNU/Linux 18.04, Debian GNU/Linux bullseye/sid x86_64\r\n\r\n- TensorFlow installed from (source or\r\nbinary): Binary\r\n- TensorFlow version (use command below): 2.0.0, 2.1.0\r\n- Python version: 3.7.3, 3.5.2\r\n- CUDA/cuDNN version: 10.0, ROCm 3.0\r\n- GPU model and memory: Titan V, Radeon VII\r\n\r\n**Describe the current behavior**\r\nSupplying the strides for tf.nn.conv2d inside elems when using tf.map_fn leads to the following error ONLY when executing in graph mode/inside @tf.function:\r\n\r\nTypeError: len is not well defined for symbolic Tensors. (TensorArrayV2Read_2/TensorListGetItem:0) Please call `x.shape` rather than `len(x)` for shape information.\r\n\r\n\r\n**Describe the expected behavior**\r\nIt should run successfully, as is the case without @tf.function\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef g(a,b):\r\n    return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],[2,2],\"VALID\",\"NCHW\"), [a,b], dtype = a.dtype, parallel_iterations = 16)\r\n\r\ndef g2(a,b,s):\r\n    return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n\r\n@tf.function\r\ndef g3(a,b,s):\r\n    return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n\r\nq = tf.random.uniform((16,5,100,100))\r\nr = tf.random.uniform((16,5,5,5,2))\r\nstri = tf.tile(tf.constant([[2,2]]),[16,1])\r\n\r\nprint(g(q,r).shape) #runs fine\r\nprint(g2(q,r,stri).shape) #runs fine\r\nprint(g3(q,r,stri).shape) #error\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n### When using TF2.0 + CUDA 10.0 on Titan V\r\n```\r\nTraceback (most recent call last):\r\n  File \"mapfn.py\", line 19, in <module>\r\n    print(g3(q,r,stri))\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 503, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    mapfn.py:12 g3  *\r\n        return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n    /home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/map_fn.py:268 map_fn\r\n        maximum_iterations=n)\r\n    /home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py:1913 conv2d_v2\r\n        name=name)\r\n    /home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py:2000 conv2d\r\n        strides = _get_sequence(strides, 2, channel_index, \"strides\")\r\n    /home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py:68 _get_sequence\r\n        current_n = len(value)\r\n    /home/ago14/.conda/envs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:741 __len__\r\n        \"shape information.\".format(self.name))\r\n\r\n    TypeError: len is not well defined for symbolic Tensors. (TensorArrayV2Read_2/TensorListGetItem:0) Please call `x.shape` rather than `len(x)` for shape information.\r\n```\r\n\r\n### When using TF2.1 + ROCm 3.0 on Radeon VII\r\n```\r\nTraceback (most recent call last):\r\n  File \"mapfn.py\", line 20, in <module>\r\n    print(g3(q,r,stri).shape) #error\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nTypeError: in converted code:\r\n\r\n    mapfn.py:12 g3  *\r\n        return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/map_fn.py:268 map_fn\r\n        maximum_iterations=n)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:1914 conv2d_v2\r\n        name=name)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:2001 conv2d\r\n        strides = _get_sequence(strides, 2, channel_index, \"strides\")\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:69 _get_sequence\r\n        current_n = len(value)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py:733 __len__\r\n        \"shape information.\".format(self.name))\r\n\r\n    TypeError: len is not well defined for symbolic Tensors. (TensorArrayV2Read_2/TensorListGetItem:0) Please call `x.shape` rather than `len(x)` for shape information.\r\n```\r\n\r\n", "comments": ["@aligirayhanozbay \r\nPLease refer to this [link](https://github.com/wau/keras-rl2/pull/9), [link2](https://github.com/keras-rl/keras-rl/issues/348) and [link3](https://stackoverflow.com/questions/59682542/typeerror-len-is-not-well-defined-for-symbolic-tensors-activation-3-identity), let us know if this helps resolve your issue.", "Dear @Saduf2019 ,\r\nI am not using keras-rl, so I believe this issue is not directly related to the issues you linked. May I ask why a len() check is being done on the strides argument of tf.nn.conv2d only?", "@aligirayhanozbay \r\nCould you please check [this link](https://stackoverflow.com/questions/34642595/tensorflow-strides-argument) in reference to len().\r\n\r\nAlso This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "@aligirayhanozbay\r\nplease update on the above comment", "Dear @Saduf2019,\r\n\r\nSorry for the belated reply - I did not have access to the machines I was testing this on for a little while. I read the stackoverflow link you posted, but I disagree that there is no bug involved here. It seems like tf.nn.conv2d has been written to accept a list (and not a tensor) for the strides argument, but map_fn cannot supply a list to conv2d. The number of elements supplied to conv2d does not make a difference - I get precisely the same errors if I replace the line `stri = tf.tile(tf.constant([[2,2]]),[16,1])` with `stri = tf.tile(tf.constant([[1,1,2,2]]),[16,1])`.\r\n\r\nSupplying a list causes different errors since map_fn is not capable of dealing with them properly. Running this snippet of code\r\n```python\r\n@tf.function\r\ndef g3(a,b,s):\r\n    return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n\r\nq = tf.random.uniform((16,5,100,100))\r\nr = tf.random.uniform((16,5,5,5,2))\r\n\r\nstri = [[1,1,2,2] for _ in range(16)]\r\n\r\nprint(g3(q,r,stri).shape) #error\r\n```\r\nleads to the following traceback\r\n```\r\nTraceback (most recent call last):\r\n  File \"mapfn.py\", line 23, in <module>\r\n    print(g3(q,r,stri).shape) #error\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 968, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nValueError: in converted code:\r\n\r\n    mapfn.py:13 g3  *\r\n        return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/map_fn.py:268 map_fn\r\n        maximum_iterations=n)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:1914 conv2d_v2\r\n        name=name)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:2001 conv2d\r\n        strides = _get_sequence(strides, 2, channel_index, \"strides\")\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:78 _get_sequence\r\n        name, n, n + 2, current_n))\r\n\r\n    ValueError: strides should be of length 1, 2 or 4 but was 16\r\n\r\n```\r\nwhich seems to imply to me that it tried to pass the whole list into each slice map_fn is operating on. A variation with a list of 16 integers supplied leads to the same error. However, things are not so straightforward since running the above snippet with `stri=[1,1,2,2]` leads to \r\n```\r\nmapfn.py:13 g3  *\r\n        return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NCHW\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/map_fn.py:268 map_fn\r\n        maximum_iterations=n)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:1914 conv2d_v2\r\n        name=name)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/nn_ops.py:2011 conv2d\r\n        name=name)\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py:943 conv2d\r\n        strides = [_execute.make_int(_i, \"strides\") for _i in strides]\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py:943 <listcomp>\r\n        strides = [_execute.make_int(_i, \"strides\") for _i in strides]\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/execute.py:174 make_int\r\n        (arg_name, repr(v)))\r\n\r\n    TypeError: Expected int for argument 'strides' not <tf.Tensor 'TensorArrayV2Read_2/TensorListGetItem:0' shape=() dtype=int32>.\r\n```\r\nJust for the sake of completeness, I'd like to emphasize that supplying any sort of tensor (e.g. tf.constant([2 for _ in range(16)])) also lead to a traceback similar to one in the original post. \r\n\r\nTo recap, it seems that tf.nn.conv2d expects to see a list (at least in graph mode, oddly enough) to specify the strides argument. However, map_fn is unable to supply this argument as a list (supplying [[1,1,2,2] for _ in range(16)] does not end up in passing [1,1,2,2] to each slice map_fn is operating on) which leads to an error. It is also impossible to call list() on the argument inside tf.function.", "i have replicated the code shared, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/a23c29bfb3df4256d881f54ed7fbe66a/37460.ipynb#scrollTo=xqR-X3yVQ6wo)", "Looks like the nightly version returns a different error. I suppose it because it tries to interpret the list as a tensor somehow and fails? Since there seems to be some changes in the nightly, may there be a way to change this code so it'll be possible to pass different strides to each slice in map_fn?\r\n\r\nAlso, even on nightly, when a tf.Tensor with the correct rank (in this case 2, since we want 1 dimension for the batching for map_fn and another for the integers containing the strides) is given, the original error is still there in the nightly version:\r\n```python\r\n@tf.function\r\ndef g3(a,b,s):\r\n    return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NHWC\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n\r\nq = tf.random.uniform((16,100,100,5))\r\nr = tf.random.uniform((16,5,5,5,2))\r\n\r\n#stri = [[1,1,2,2] for _ in range(16)]\r\nstri = tf.tile(tf.constant([[2,2]]),[16,1])\r\n\r\nprint(g3(q,r,stri).shape) #error\r\n```\r\nreturns \r\n```\r\n---------------------------------------------------------------------------\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-13-5a62a04d5f8e> in <module>()\r\n      9 stri = tf.tile(tf.constant([[2]]),[16,1])\r\n     10 \r\n---> 11 print(g3(q,r,stri).shape) #error\r\n\r\n8 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-7-477a1376b36e>:3 g3  *\r\n        return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NHWC\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:1925 conv2d_v2  **\r\n        name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:2012 conv2d\r\n        strides = _get_sequence(strides, 2, channel_index, \"strides\")\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py:67 _get_sequence\r\n        current_n = len(value)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:897 __len__\r\n        \"shape information.\".format(self.name))\r\n\r\n    TypeError: len is not well defined for symbolic Tensors. (map/while/TensorArrayV2Read_2/TensorListGetItem:0) Please call `x.shape` rather than `len(x)` for shape information.\r\n```\r\n\r\nSimilarly to the example in the original post, commenting out the @tf.function line, we get the intended result:\r\n```python\r\n#@tf.function\r\ndef g3(a,b,s):\r\n    return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0],0),x[1],x[2],\"VALID\",\"NHWC\"), [a,b,s], dtype = a.dtype, parallel_iterations = 16)\r\n\r\nq = tf.random.uniform((16,100,100,5))\r\nr = tf.random.uniform((16,5,5,5,2))\r\n\r\n#stri = [[1,1,2,2] for _ in range(16)]\r\nstri = tf.tile(tf.constant([[2,2]]),[16,1])\r\n\r\nprint(g3(q,r,stri).shape) #error\r\n```\r\nwith the correct shape `(16, 1, 48, 48, 2)` printed.\r\n\r\n(Please note that I changed the tensor format to NHWC and, correspondingly, the shape of the tensor q because colab seems to run on CPU, which for some reason cannot do NCHW)", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210524, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/19ca969122707b95b89ba9a25bac6c85/untitled7.ipynb). Thanks!", "The underlying problem here is that the strides argument for `tf.nn.conv2d` cannot be a Tensor (https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). It needs to be a static int or list of ints. In Eager mode (without tf.function), a Tensor input is as good as something static as we know its value but in tf.function we don't support that.\r\n\r\nIf you want tf.function, then I'm afraid the only solution here is to use g() where the strides argument is hard coded. Or you can do something like\r\n\r\n```python\r\n@tf.function\r\ndef g4(a, b, s):\r\n  return tf.map_fn(lambda x: tf.nn.conv2d(tf.expand_dims(x[0], 0), x[1], s, \"VALID\", \"NCHW\"), [a, b], dtype=a.dtype, parallel_iterations=16)\r\n\r\nq = tf.random.uniform((16,100,100,5))\r\nr = tf.random.uniform((16,5,5,5,2))\r\nstri = [2, 2]\r\n\r\nprint(g4(q,r,stri).shape) \r\n```\r\n\r\nEverytime you change stri to something else, we'll most likely recompile the tf.function as it'll change the function cache. ", "@rohan100jain I solved my issue over the past year with a similar approach to the snippet in your comment; please feel free to close the issue if the behaviour of `tf.nn.conv2d` won't be changed in graph mode to accommodate tensors as the stride argument.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37460\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37460\">No</a>\n"]}, {"number": 37459, "title": "Export Keras image preprocessing layers into the experimental namespace.", "body": "PiperOrigin-RevId: 299956491\r\nChange-Id: I2f5a5ea74cbd2ffcfbb254d99c9d54a4bbd81f34", "comments": []}, {"number": 37458, "title": "how to extract tflite_hexagon_nn_skel_v1.14.run", "body": "**System information**\r\n- OS Platform and Distribution - Debian based but tried on Ubuntu 18.04 too\r\n\r\nPer the instructions here\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/hexagon_delegate.md#step-2-add-hexagon-libraries-to-your-android-app\r\n\r\nwhen I try to run the extractor I'm presented with a colon. I've tried 'yes', 'accept', but nothing works. It just says \"Aborting extraction\" and bails.\r\n```\r\nroot@linaro-alip:~/hexagon# ./tflite_hexagon_nn_skel_v1.14.run\r\nCreating directory hexagon_nn_skel_v1.14\r\nVerifying archive integrity...  100%   All good.\r\nUncompressing Hexagon NN Shared Libs  100%  \r\n\r\n:\r\nAborting extraction.. Done\r\nroot@linaro-alip:~/hexagon# \r\n```\r\nIs there some trick to extracting the files?\r\n\r\n", "comments": ["`I ACCEPT`", "@karimnosseir it seems, on some Linux platform, no message is displayed just like what @arrow53 showed. It's a bit confusing.", "Looking", "@arrow53 I just tried it on a debian based Linux and the license agreement shows and ask you to use \"I ACCEPT\"\r\n\r\n\"\r\n....\r\nType \"I ACCEPT\" if you agree to the terms of the license:\r\nYou didn't accept the license. Extraction aborted.\r\n\"\r\n\r\nCan you provide more information on the environment you used ?\r\n@freedomtan Can you also share some information\r\n\r\nThanks", "@arrow53 does it happen also on the v 1.10.3 or only on v1.14\r\n\r\nThanks", "@karimnosseir I tested it on Ubuntu 18.04 LTS. I figured out why. It's dash's problem (on Ubuntu /bin/sh is a symbolic link to dash, which is a sh clone). `rm /bin/sh; ln -s /bin/bash /bin/sh`  then it works.", "Thanks @freedomtan. @arrow53 Can you try what @freedomtan suggested.\r\n\r\nThanks", "@freedomtan thank you so much!!!\r\n@karimnosseir yep works great.\r\n\r\nI'm going to close this issue as the solution can be searched now. I'm not sure why the license agreement didn't pop up for me on Ubuntu 18.04. But, maybe just something wrong with my system.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37458\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37458\">No</a>\n", "> @arrow53 does it happen also on the v 1.10.3 or only on v1.14\r\n> \r\n> Thanks\r\n\r\n@karimnosseir yes, I see the same on both versions on Ubuntu 18.04 and my embedded Debian system (Dragonboard 820c)."]}, {"number": 37457, "title": "pip install tensorflow could not find a version", "body": "**System information**\r\n- macOS Catalina version 10.15.3\r\n- pip install tensorflow\r\n- GPU model and memory: Intel Iris Plus Graphics 645 1536 MB\r\n\r\n**The problem**\r\nBoth `pip install tensorflow` and `pip install tf-nightly` lead to the following error messages:\r\n\r\n> ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\n\r\n> ERROR: No matching distribution found for tensorflow\r\n\r\n", "comments": ["Did you try updating your pip version?", "@alexcreature.\r\n\r\nAs per the [documentation](https://www.tensorflow.org/install) TensorFlow 2 packages require a pip version >19.0.\r\n\r\nCould you please try the following commands and let us know if it works.\r\n`python3 -m pip install --upgrade pip` or `pip install --upgrade pip` and then try \r\n\r\n`pip install tensorflow`\r\n\r\nThanks!", "@alexcreature Try updating pip version by:\r\n``` \r\npip install --upgrade pip\r\n```\r\nand the you can again install tensorflow by:\r\n```\r\npip install tensorflow\r\n```\r\nor you can specify version which you want to install.", "This seems to be the same as an issue I had previously with both Windows 10 and Ubuntu 18 and Ubuntu 16, no matter the operating system.\r\nThe previous issue ( #37205 ) was closed because I proposed a WORK AROUND of downloading the .whl and installing Tensorflow through the downloaded wheel, which worked just fine for me.\r\n\r\nSomething is broken in the pypi pipeline of installation and needs to be taken care of...", "Yes, the issue is that systems use old `pip` / `setuptools` by default and these don't know how to install packages with manylinux2010 tags. That's why you need to upload `pip` and `setuptools` first (and always install in a virtualenv, not globally)", "@alexcreature,\r\nAny updates regarding this issue? Thanks!", "> Yes, the issue is that systems use old `pip` / `setuptools` by default and these don't know how to install packages with manylinux2010 tags. That's why you need to upload `pip` and `setuptools` first (and always install in a virtualenv, not globally)\r\n\r\nOkay so here are ALL of the things I have tried in my attempts to simply install tensorflow via pip install:\r\n\r\npip 20.0.2\r\nPython 3.7.6\r\nWin10, globally\r\nUbuntu 18.04, globally (source built python 3.7.6 and alias setup to avoid breaking apt)\r\nUbuntu 16.xx, globally, same as above\r\nWin10 pipenv environment (most recent version of pipenv)\r\nUbuntu 16, 18 pipenv environment (most recent version of pipenv)\r\n\r\nI have tried and tested everything I can possibly come up with to make things different but nothing seems to install Tensorflow 2.1 for Python 3.7 for me, in any way shape or form. I have tried pip install tensorflow I have tried pip install tensorflow==2.1 and the absolute only thing I can do is install by downloading the wheel and installing that wheel directly.\r\n\r\nIs there something we can do with pip or python source to get this back to working properly? I don't have ANY issues with any other packages I use across the board, except for Black, but even then sometimes that will install without a hitch and other times I get the same error, though it's been a long time since I tried and that is not relevant to Tensorflow...\r\n\r\nEdit: I cannot find any PEP which shows a breaking change in manylinux tags and pip installs... The most relevant is 571 which states that any pip that is capable of installing manylinux2010 tags will be backwards compatible, and 600 which shows a new naming convention for manylinux tags, but also states that old tags should still work, if I'm reading correctly.\r\nPEP 599 also adds the manylinux2014 tag, however in true python fashion, retains support for manylinux1 and manylinux2010 tags, so this definately should not be the issue, however, I am no source developer and am simply relaying the information I have found.....\r\n\r\nThese are the most relevant PEP's I have found regarding manylinux wheel tags.", "I think the best way forward now would be to open an issue on PyPI https://github.com/pypa/pip\r\n\r\nIf you do, please link against this issue and/or tag me too to follow up", "> @alexcreature,\r\n> Any updates regarding this issue? Thanks!\r\n\r\nAutomatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37457\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37457\">No</a>\n", "upgrading my pip worked for me. \r\n\r\npip install --upgrade pip"]}, {"number": 37456, "title": "Ops with SparseTensor on GPU gives result in CPU.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  custom code\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): binary (conda)\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model and memory: NVIDIA GTX 1060 6 GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen executing operations on sparse tensors in the GPU (within a `tf.device` block), the result is stored in the CPU. This is causing a huge slowdown in my program, as the data is being copied around between GPU and GPU multiple times.\r\n\r\nI have verified that this occurs with the functions `tf.sparse.sparse_dense_matmul` and `tf.sparse.reduce_sum`. It may also occur with others. Is this the intended behavior? I am missing something?\r\n\r\n**Describe the expected behavior**\r\nThe result of an operation on a sparse tensor with the GPU should stay in the GPU.\r\n\r\n**Standalone code to reproduce the issue** \r\nSee [this Colab notebook](https://colab.research.google.com/drive/1BpG8P8dEzDDmtMBIGgSA2wOMzEm4bCzL).\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThanks in advance.\r\n", "comments": ["Was able to replicate the issue with Tf 2.1.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/71a5af89e786fbc5a935c1b53dc87576/untitled435.ipynb). Thanks!", "Unfortunately this is by design, SparseReduceSum OP is not implemented for GPU, see declaration:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_reduce_op.cc\r\n\r\nVery few sparse ops are supported on GPU.", "See #24404", "Thanks for the update. In this case I'm closing the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37456\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37456\">No</a>\n"]}, {"number": 37455, "title": "Add and populate leaf_id in boosted_trees.DebugOutput", "body": "Adds leaf_id field to boosted_trees.DebugOutput, and populates it in BoostedTreesExampleDebugOutputsOp.\r\n\r\nMotivation:\r\n*Leaf ids are quite useful in circumstances where we're using boosted trees to generate features for a subsequent model, eg in https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/.\r\n*This PR also (partially) resolves a TODO.\r\n\r\nTested:\r\nUpdated unit tests to validate correct leaf_ids are returned.", "comments": ["@gbaned @jaingaurav \r\nHi there! Could you please take a look, or re-assign? Thanks!\r\n-Brendan", "Adding @tanzhenyu instead as I'm not qualified to review this", " Hi there @tanzhenyu -- could you please take a look, or re-assign? Thanks!", "Friendly ping @tanzhenyu ", "@tanzhenyu Can you please review this PR ? Thanks!", "@tanzhenyu Can you please review this PR ? Thanks!", "@tanzhenyu Can you please review this PR ? Thanks!", "ping @gbaned @mihaimaruseac @ebrevdo @tanzhenyu \r\n\r\nThis has been pending for more than 7 months now.", "As mentioned, I am not qualified to review this. @tanzhenyu can you take a look or reassign please?", "I don't have the bandwidth to review this, un-assigning myself.", "@bmc2-stripe  Can you please take a look on the above comment from @nataliaponomareva. Thanks!"]}, {"number": 37454, "title": "<release 2.2>-<rc0> cherry-pick request: Export minimum runtime string in MLIR  tflite converter", "body": "This change fixes the issue for missing min_runtime_version in flatbuffers exported by MLIR converter. Specifically:\r\n\r\n*Export `min_runtime_version` in metadata of flatbuffer (fixed-length byte array).\r\n\r\n*Update all related mlir tests to have metadata.\r\n\r\nPiperOrigin-RevId: 299894972\r\nChange-Id: Ic79f3ab05b593882362f5baf62493861961acbe3", "comments": []}, {"number": 37453, "title": "<release 2.2>-<rc0> cherry-pick request: export min runtime version in MLIR converter", "body": "This change enables the MLIR TF lite converter to populate minimum required runtime string.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37453) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 37452, "title": "[ROCm] Update Tensorflow ROCm to build with hipclang compiler", "body": "This PR updates the Tensorflow master branch to build with ROCm hipclang compiler. ", "comments": ["CC @whchung @deven-amd ", "/cc @nvining-work ", "gentle ping"]}, {"number": 37451, "title": "Make while_v2_indexed_slices_writers compatible with tensor equality.", "body": "PiperOrigin-RevId: 299619903\r\nChange-Id: Ia829ce5942d08fb6c55d83d5180f0a49e80bfdc3", "comments": []}, {"number": 37450, "title": "tensorflow-gpu 2.0.1 hangs mid-epoch", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): The only modification i have made is that i've forked ReduceLRonPlateau and use this modified version. The only change is that i reduce the weight decay along with the learning rate. Seems highly unlikely this is relevant.\r\n\r\nReduceLRonPlateau\r\n```\r\n old_lr = float(K.get_value(self.model.optimizer.lr))\r\n                    old_wd = float(K.get_value(self.model.optimizer.weight_decay))\r\n                    if old_lr > self.min_lr:\r\n                        new_lr = old_lr * self.factor\r\n                        new_wd = old_wd * self.factor\r\n                        new_lr = max(new_lr, self.min_lr)\r\n                        K.set_value(self.model.optimizer.weight_decay, new_wd)\r\n                        K.set_value(self.model.optimizer.lr, new_lr)\r\n```\r\n\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Windows 10\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  pip\r\n- Python version: - Bazel\r\nversion (if compiling from source):  3.6\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory: 1060 6GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nUpon calling model.fit() TF will hang at seemingly random points. (5 epochs in, mid epoch for example after we've trained on 20% of our batches). I am doing hyper parameter optimization, so sometimes a trial will complete fully, other times it will hang indefinitely. I was having no issues originally when doing a regression problem. This issue only started happenign when i switched to categorical classification.\r\n\r\nMy output node is 3 units because i have 3 class. Using a softmax function and built in categorical cross entropy. Those are the only changes from when it was working to now.\r\n\r\nTried changing batch size, does not seem to have affect. Not sure if i can see a pattern if certain parameter configurations cause the fit to fail.\r\n\r\nI notice the CPU spikes during a stall.\r\n\r\n**Describe the expected behavior**\r\n\r\nTF does not stall or at least provided some information as to why it is stalled.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Upgrading to 2.1 seemed to fix whatever bug this is.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37450\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37450\">No</a>\n", "Issue still happens with tensorflow 2.1", "@ben-arnao, Please provide the complete standalone code to replicate the reported issue. Thanks!", "Do you have a situation that occupies two cores?", "@ben-arnao, Please post the code snippet to analyze the issue. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 37449, "title": "Resource temporarily unavailable, ENOMEM on CPU with memory rlimit", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes, trivial example\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Debian buster\r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.73\r\n- Bazel version (if compiling from source): ?\r\n- GCC/Compiler version (if compiling from source): 8.3.0\r\n- CUDA/cuDNN version: - GPU model and memory: not relevant\r\n\r\n**Describe the current behavior**\r\n\r\nThe system has 12 GB total physical memory, with /etc/security/limits.conf set to limt per-process max memory to 10 GB as follows:\r\n\r\n```\r\n* hard rss 10485760\r\n* hard as 10485760\r\n```\r\nA trivial code example runs out of memory. The last failling syscall obtained with strace is\r\n\r\nmmap(NULL, 8392704, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\n\r\nLooking at mmap invocations from strace confirms that it is indeed allocating close to 10 GB memory before fail. This does not happen when the resource limits are removed. I suspect that it's ignoring the rlimit and trying to allocate all physical memory.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe library should respect the current max memory rlimit (or better yet, not try to allocate all system memory?) or provide some configuration parameter to limit memory (I found none for the CPU-based version).\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.config.set_visible_devices([], 'GPU')\r\nmodel = tf.keras.Sequential([ tf.keras.layers.Dense(units=1, input_shape=[1]) ])\r\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\r\n\r\nxs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\r\nys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\r\n\r\nmodel.fit(xs, ys, epochs=1)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nStrace output:\r\n\r\n```\r\n...\r\nwrite(1, \"Train on 6 samples\\n\", 19Train on 6 samples\r\n)    = 19\r\nioctl(1, TCGETS, {B38400 opost isig icanon echo ...}) = 0\r\nsched_getaffinity(0, 128, [0, 1, 2, 3, 4, 5, 6, 7]) = 64\r\nsysinfo({uptime=8660, loads=[12896, 10528, 13536], totalram=12518699008, freeram=2526597120, sharedram=950390784, bufferram=1171718144, totalswap=4898942976, freeswap=3857969152, procs=741, totalhigh=0, freehigh=0, mem_unit=1}) = 0\r\nmmap(NULL, 8392704, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = -1 ENOMEM (Cannot allocate memory)\r\nfutex(0x7f5ca3c481a0, FUTEX_WAKE_PRIVATE, 2147483647) = 0\r\nwrite(2, \"terminate called after throwing \"..., 48terminate called after throwing an instance of ') = 48\r\nwrite(2, \"std::system_error\", 17std::system_error)       = 17\r\nwrite(2, \"'\\n\", 2'\r\n)                      = 2\r\nwrite(2, \"  what():  \", 11  what():  )             = 11\r\nwrite(2, \"Resource temporarily unavailable\", 32Resource temporarily unavailable) = 32\r\nwrite(2, \"\\n\", 1\r\n)                       = 1\r\n```\r\n", "comments": ["@jploski,\r\nI was able to run the code without any issues and did not observe any memory issues. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/2ee4ab61d311029962a3fecb4e969f33/37449.ipynb). Thanks!", "@amahendrakar What does \"ulimit -m\" and \"ulimit -v\" report in your test environment? Did you set a memory limit in /etc/security/limits.conf (and logged in again to make it effective)? This code sample only crashes for me if there is a configured memory limit, and it works otherwise (sorry if that was not clear from the original bug report).", "@jploski,\r\nThe output of both `ulimit -m` and `ulimit -v` is `unlimited`. Please check the gist of it [here](https://colab.research.google.com/gist/amahendrakar/2bf653011ac8556161be9848d73949f0/37449.ipynb). \r\n\r\nUnfortunately, I am unable to set the memory limit in the `limits.conf` file. Thanks!", "@amahendrakar I just checked that I can also use set the rlimit in shell without without changing limits.conf:\r\n```\r\nulimit -v 10485760\r\nulimit -m 10485760\r\npython3 example.py\r\n```\r\n(The value should be something below the total available memory in kB.)", "@jploski,\r\nI've tried running the above commands and reset the runtime. Didn't observe any changes, got the same output as before.", "TensorFlow does not detect the amount of memory available via rlimits. Crashing when allocating memory above these limits is working as intended. This happens with most programs:\r\n\r\n```c\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n\r\nint main () {\r\n        char *a = malloc(SZ);\r\n        printf(\"Allocated %u bytes to %p\\n\", SZ, a);\r\n        *a = '0';\r\n        printf(\"Set the start of %p to 0\\n\", a);\r\n        return 0;\r\n}\r\n```\r\n\r\n```\r\nroot@7b8dce9800a9:/work# ulimit -v 4504; ./a.out\r\nAllocated 10000 bytes to 0x55890a43e260\r\nSet the start of 0x55890a43e260 to 0\r\nroot@7b8dce9800a9:/work# ulimit -v 4503; ./a.out\r\nAllocated 10000 bytes to (nil)\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nAlthough there exist calls to detect available memory limits and trim allocation, this would have to happen before every allocation and would still be vulnerable to TOCTOU. Plus, it is not portable, thus not something we would prioritize.\r\n\r\nFurthermore, there is a [`tf.config.experimental.set_memory_growth`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth?hl=en) API point.", "Of course, the issue here is not that a crash occurs when you allocate memory above limit. The issue is that you attempt to allocate more memory than is available, which most programs don't do.\r\n\r\nNot fixing this issue means that the non-GPU version is unusable in presence of a rlimit. Not sure how it behaves with other kinds of limits (e.g. cgroup limits imposed by Docker containers), but I predict it would fall on its face as well. (A similar issue existed for a time in Java VM and got fixed to make it behave properly.)\r\n\r\nIf you can't handle it so as to allocate only the amount of memory actually available to the process, I suggest that you at least display some warning if a memory limit is detected.", "I'm going to add it to the security list of issues for further prioritization.", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! "]}, {"number": 37448, "title": "Increase BiasGrad size:  \"BiasGrad requires tensor size <= int32 max\"", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nWe've using 3D U-Net on large images (e.g. 512x512x256x4) and have found that when the input size increases above a certain size we get the error:  \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  BiasGrad requires tensor size <= int32 max\r\n\t [[node BiasAddGrad_3 (defined at testing.py:640) ]] [Op:__inference_distributed_function_2785]\r\n```\r\n\r\nI'm pretty sure that this is because the BiasGrad function is using an INT32 datatype and we're declaring more than the datatype can represent as INT32. \r\n\r\nI'd like to request that the datatype be change to something larger-- possible INT64 or at least an unsigned INT32. On CPU we can easily use 100s of GB of RAM for the TensorFlow graph so these are now valid workloads for TF to support.\r\n\r\n**Will this change the current api? How?**\r\n\r\nNo. Other than to change the datatype for BiasGrad to something larger.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nWe've got customers now using 3D networks with very large images (512x512x256x4) and moderately large batch sizes (2-16). The memory footprint for these TF models can fit into CPU memory (~ 1.5 TB) if the datatype is extended to larger integers. \r\n\r\n**Any Other info.**\r\n", "comments": ["@tonyreina \r\nWould you please refer to[ this link](https://stackoverflow.com/questions/60414562/how-to-solve-the-biasgrad-requires-tensor-size-int32-max-invalidargumenterr) and let us know if it helps", "@tonyreina\r\nplease update as per above comment", "Yes. That link confirms the issue. The problem is that we are seeing customers with networks (e.g. 3D U-Net with large batches) that use this many elements. On CPU we are able to handle TB of RAM and in particular with high resolution images we are seeing the need for larger networks. So we're requesting that the int32 be increased to handle the new use cases.", "I wanted to circle back on this one. We're starting to get use cases where this is important on medical imaging. Would it be possible to make things at least uint32 to get some extra headroom?  ", "@tonyreina,\r\nSorry for the delayed response. Can you please refer the latest [documentation of BiasAddGrad](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/bias-add-grad) and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 37447, "title": "Cross-compile instructions for TFLite XNNPACK delegate on ARM", "body": "Are there any instructions on how to cross-compile the XNNPACK delegate for ARM?\r\n\r\n\r\nI've tried the following:\r\nexport CC=arm-linux-armeabief-gcc\r\nexport CPP=arm-linux-armeabief-g++\r\nbazel-2.1.0 build //tensorflow/lite/delegates/xnnpack:xnnpack_delegate\r\n\r\nERROR: /home/andy/.cache/bazel/_bazel_andy/ff7061ef3fb000e53210886b26774f34/external/XNNPACK/BUILD.bazel:1654:1: C++ compilation of rule '@XNNPACK//:sse2_ukernels' failed (Exit 1)\r\narm-linux-gnueabihf-gcc: error: unrecognized command line option '-msse2'\r\nTarget //tensorflow/lite/delegates/xnnpack:xnnpack_delegate failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 26.516s, Critical Path: 14.13s\r\nINFO: 28 processes: 28 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nUbuntu 14.04 \r\nBazel 2.1.0\r\nGNU 4.9.3\r\nTargeting Raspberry Pi 3B / Compute Module 3\r\n", "comments": ["@WillNichols726 \r\nCould you please refer to [this link](https://www.tensorflow.org/lite/guide/build_arm64) and let us know if it helps.\r\nAlso please share the tensorflow version.", "Tensorflow 2.1.0\r\n\r\nI was able to use the link below to successfully cross-compile libtensorflow_lite.a\r\nhttps://www.tensorflow.org/lite/guide/build_rpi\r\n\r\nDo these instructions also compile the XNNPACK delegate? From what I could see rooting around in the Makefile at tensorflow/lite/tools/make, it didn't look like the XNNPACK delegate was included in the build, nor were there any options to include it.", "We don't have XNNPACK support with ./tensorflow/lite/tools/make/build_rpi_lib.sh script yet.\r\nWe need to update tensorflow/lite/tools/make/Makefile to build XNNPACK.", "I've just checked in ARM crossbuild toolchains (https://github.com/tensorflow/tensorflow/commit/4961f18733ca3967198393abf419e14476b4a85c)  for Bazel build.\r\n\r\nYou can build armhf, aarch64 targets (include XNNPACK) with the following commands. \r\n\r\n```sh\r\n$ bazel build --config=elinux_armhf  //tensorflow/lite/tools/benchmark:benchmark_model \r\n$ bazel build --config=elinux_aarch64  //tensorflow/lite/tools/benchmark:benchmark_model \r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37447\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37447\">No</a>\n"]}, {"number": 37446, "title": "Python crashes when computing max/min of a complex tensor", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 \r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nWhen taking (by mistake) the max value of a tensor with complex dtype, tf crashes python.\r\n\r\n\r\n**Describe the expected behavior**\r\nTf should fail gracefully, i.e. throwing an error.\r\n\r\n**Standalone code to reproduce the issue** \r\n```python\r\nimport tensorflow as tf\r\ntf.reduce_max(tf.ones((9,), dtype=tf.complex64))\r\n```\r\n\r\n**Other info / logs** \r\n```\r\n2020-03-09 15:40:04.604547: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-03-09 15:40:04.604625: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-03-09 15:40:04.604723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (zaccharie-Latitude-7490): /proc/driver/nvidia/version does not exist\r\n2020-03-09 15:40:04.605857: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-03-09 15:40:04.645752: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2112000000 Hz\r\n2020-03-09 15:40:04.647241: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4c32870 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-03-09 15:40:04.647293: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-03-09 15:40:04.671854: F tensorflow/compiler/xla/literal_util.cc:212] C64 element type has no minimum value\r\n[1]    23558 abort (core dumped)  ipython\r\n```\r\n\r\nI tried on machine with and without GPU for the same result.\r\n", "comments": ["Hi @zaccharieramzi, I think the input type of tf.complex64 is not support in reduce_max.\r\nHere make a PR to pick out this condition and avoid core_dump.", "yes of course, you can't use `reduce_max` on a complex tensor, but I mean there should be a way to fail nicely: hopefully your PR does the job.\r\nWhat is the new output with this fix?", "@zaccharieramzi you will failed to create the max op and see the error message as this:\r\n```\r\nOP_REQUIRES failed at reduction_ops.cc:88 : Invalid argument: Unsupported PrimitiveType of MaxOp: 'C64'\r\n```", "Was able to reproduce the issue with TF-2.1 and TF-nightly. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/2fa295dab88e61fe31881f421504e3ac/37446-tf-nightly.ipynb). Thanks!", "@Leslie-Fang Can you please update related doc also? Thanks!", "@jvishnuvardhan Thanks for the remind. I think it's already in the doc  https://github.com/tensorflow/tensorflow/blob/78b09356f5c1a17d869a2fbc571bd720b3450d9b/tensorflow/python/ops/math_ops.py#L2409", "@Leslie-Fang You could add that it raises an exception in the style of https://github.com/tensorflow/tensorflow/blob/78b09356f5c1a17d869a2fbc571bd720b3450d9b/tensorflow/python/ops/math_ops.py#L376-L378", "Hi @zaccharieramzi Thanks for the suggestions. I think this PR (https://github.com/tensorflow/tensorflow/pull/37483) has been merged.  Please take a look when feasible. ", "Ha, I didn't take the time to review the PR, sorry. Had I done it, I would have suggested to do the same also for the `reduce_min` operator which suffers from the same problem (just tested). I am going to change the title of the issue to reflect that the problem is for both operators and leave this open. \r\n\r\nI wanted to make the PR, but I thought I was just going to copy-paste what you did. Maybe it's best you do it, so that you can refactor that in the `XlaReductionOp` rather than having it duplicated in `MinOp` and `MaxOp`.", "I would like to work on this issue", "The same problem occurs for `tf.keras.backend.max` and `tf.keras.backend.min` when passing a complex dtype tensor for the argument `x`.", "I like to contribute in this project. ", "@jvishnuvardhan, is it feasible to support complex numbers instead? How much work will it take?", "@Rendok what would it mean to compute the max/min of a complex tensor? ", "@zaccharieramzi, agree, there is no total order in complex space although you can order them by modulus in this case", "Now the issue produces proper error message in both Tensorflow GPU 2.4.1 and Tf Nightly 2.6 version, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/690f5d93dc63d1f9edf39e6bd34fc63a/untitled7.ipynb).", "Closing this issue since the error message is proper now, feel free to reopen the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37446\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37446\">No</a>\n"]}, {"number": 37445, "title": "Document purpose for tf.keras.preprocessing module", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe module documentation is very terse and only states that it contains \"preprocessing utils\". It does not state the specific purpose.\r\n\r\nIt would be helpful if it defined the intended use of the `tf.keras.preprocessing` module (_e.g._ to clean up or transform tf.data.Datasets before they are fed to the model).\r\n\r\nAlso, since the `tf.feature_column` module has similar functionality, it would be nice to describe when to use one or the other, or how they are intended to be used together.", "comments": ["I'll start working on it...\r\nThanks!", "@ManishAradwad Can you please link the PR with this issue? Thanks!"]}, {"number": 37443, "title": "Apply check for avoiding printout broken of Fit method", "body": "The out-prints of in the fit method in version 2.1 as per mentioned in issue  #36332.\r\nThis PR applied a check for the same case.", "comments": ["@rchao please review this change", "\"We are closing this PR as it has been copied from somebody else. This behavior is unethical and runs contrary to our TensorFlow Code of Conduct. Please do not do this. If you continue to plagiarize work, we will ban you from the project.\"", "Hey @shubham769, we see that you are again trying to plagiarize PRs from other persons. Please stop."]}, {"number": 37442, "title": "support broadcast in xla_legalize_to_linalg", "body": "This pr is  mainly a discussion to find a suitable way which can lower implicit broadcast in hlo dialect\r\nnow, for code like:\r\n```\r\n \"xla_lhlo.add\"(%29, %14, %30) {broadcast_dimensions = dense<[]> : tensor<0xi64>} : (memref<?xi64>, memref<i64>, memref<?xi64>) -> ()\r\n```\r\ncannot be lowered in to linalg dialect in current implementation.\r\nThis pr purposed a way to transform implicit broadcast in lhlo by adding convertor in xla_legalize_to_linalg pass, that is, when constructing loop by linalg.generic, use a LoadOp to load the actually value from  scalar tensor and passed it into the block which compose the linalg.generic. This way prevents the additional  memory allocation in converting implicit broadcast to explicit broadcast , so the code above will be lowered to \r\n```\r\n %44 = load %14[] : memref<i64>\r\n    linalg.generic {args_in = 1 : i64, args_out = 1 : i64, indexing_maps = [#map0, #map0], iterator_types = [\"parallel\"]} %35, %43 {\r\n    ^bb0(%arg1: i64, %arg2: i64):   // no predecessors\r\n      %120 = addi %arg1, %44 : i64\r\n      linalg.yield %120 : i64\r\n    }: memref<?xi64>, memref<?xi64>\r\n```\r\n\r\nAlso, for shape derivation, for elementwise op, the SameShapeAsFirstOperand may be not enough for code like\r\n```\r\n   \"xla_lhlo.add\"(%21, %56, %61) {broadcast_dimensions = dense<[]> : tensor<0xi64>} : (memref<i64>, memref<?xi64>, memref<?xi64>) -> ()\r\n```\r\nbecause the first operand is scalar . For this is also a broadcast case, I extended  SameShapeAsFirstOperand to first find the first dynamic operand, and then use it as the shape as result shape .(by doing this SameShapeAsFirstOperand may be not suitable, so a new name is needed afterwards)", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37442) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F37442) for more info**.\n\n<!-- ok -->", "I have only briefly looked at the code but have some higher-level concerns. You have implemented dedicated support for implicit broadcasting where one of the operands is a scalar. This only handles one case for broadcasting, so we will still need to be able to handle the general case and remove the need for temporary buffers there, as well. From that perspective, it does not gain us much to handle the special casing but introduces extra complexity and additional lowering paths. For that reason, I would prefer to not add the special casing.\r\nWe have also looked into this and our current approach has been to convert all operations with implicit broadcasts into a sequence of explicit broadcasts followed by an operation without implicit broadcasting. For context, an example would be\r\n```mlir\r\nfunc @biasadd(%arg0: tensor<?x?xf32>, %arg1: tensor<?xf32>) -> tensor<?x?xf32> {\r\n\u00a0\u00a0%0 = \"xla_hlo.add\"(%arg0, %arg1) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<?x?xf32>, tensor<?xf32>) -> tensor<?x?xf32>\r\n\u00a0\u00a0return %0 : tensor<?x?xf32>\r\n}\r\n```\r\nwhich gets transformed into\r\n```mlir\r\nfunc @biasadd(%arg0: tensor<?x?xf32>, %arg1: tensor<?xf32>) -> tensor<?x?xf32> {\r\n  %0 = dim %arg0, 0 : tensor<?x?xf32>\r\n  %1 = index_cast %0 : index to i32\r\n  %c1 = constant 1 : index\r\n  %2 = dim %arg0, 1 : tensor<?x?xf32>\r\n  %3 = dim %arg1, 0 : tensor<?xf32>\r\n  %4 = cmpi \"eq\", %2, %c1 : index\r\n  %5 = select %4, %2, %3 : index\r\n  %6 = index_cast %5 : index to i32\r\n  %7 = \"xla_hlo.scalars_to_dimension_tensor\"(%1, %6) : (i32, i32) -> tensor<2xi32>\r\n  %8 = \"xla_hlo.dynamic_broadcast_in_dim\"(%arg0, %7) {broadcast_dimensions = dense<[0, 1]> : tensor<2xi64>} : (tensor<?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %9 = \"xla_hlo.dynamic_broadcast_in_dim\"(%arg1, %7) {broadcast_dimensions = dense<1> : tensor<1xi64>} : (tensor<?xf32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %10 = xla_hlo.add %8, %9 : tensor<?x?xf32>\r\n  return %10 : tensor<?x?xf32>\r\n}\r\n```\r\nThis code assumes that the broadcast is valid. Note that the result shape now depends on the value of the input shape at certain positions. This also has the effect that we can no longer have a simple affine map for the input of a corresponding linalg operation that computes the broadcast. This is something to figure out.\r\nHowever, assuming we can get to linalg (for example assuming that the shape at runtime has no `1` component and we hence never need to expand; the scalar case you use would be an example for this), we can lower this to three linalg operations that we then fuse. That approach composes much nicer than having a special case.\r\nAvoiding the extra buffer for the broadcast can then be done in two ways: Either you fuse on linalg before buffer allocation or you fuse the linalg operations after buffer allocation and apply [store forwarding](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/mlir_gpu/kernel_lowering.cc#L146) + [dead buffer removal](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/mlir_gpu/kernel_lowering.cc#L212). Both of these optimizations need to be build proper but are needed for other fusion rewrites on the buffer level that cannot be special cased level anyway.\r\n\r\nDo you have a special use case in mind that is blocked by this? I will update the linked lowering pipeline in the coming days with a newer version we have been playing with that supports at least this scalar case.", "@qqsun8819 Can you please check sherhut's comments and resolve conflicts?. Thanks!", "> @qqsun8819 Can you please check sherhut's comments and resolve conflicts?. Thanks!\r\n\r\nok\uff0c@sherhut 's comment is very useful, and I'm working on refactor my code according to his comments, also with file conflict resloved", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 37441, "title": "ValueError: Cannot convert a Tensor of dtype resource to a NumPy array", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  N/A, as it can be reproduced in Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device:  N/A\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1\r\n- Python version: - Bazel\r\nversion (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from\r\nsource): N/A\r\n- CUDA/cuDNN version: - GPU model and memory: N/A\r\n\r\n**Describe the current behavior**: It is resulting in Error, `InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.`, while running the First Code but is working fine when `tf.keras.Input` is replaced with `tf.Variable` in the Second Code.\r\n\r\n**Describe the expected behavior**: Code should work fine with `tf.keras.Input` as well\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n      **Code with Error:**\r\n\r\nimport tensorflow as tf\r\n\r\nnum_uids = 50\r\n#input_uid = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, batch_size = 32)\r\ninput_uid = tf.keras.layers.Input(shape=(1,), dtype=tf.int32)\r\nparams = tf.Variable(tf.random.normal((num_uids, 9)), trainable=True)\r\n\r\nparam = tf.gather_nd(params, input_uid)\r\n\r\n#input_shared_features = tf.keras.layers.Input(shape=(128,), dtype=tf.float32, batch_size = 32)\r\ninput_shared_features = tf.keras.layers.Input(shape=(128,), dtype=tf.float32)\r\ncombined = tf.concat([param, input_shared_features], axis=-1)\r\n\r\nnet = tf.keras.layers.Dense(128)(combined)\r\n\r\n      **Working Code:**\r\n\r\nimport tensorflow as tf\r\n\r\nnum_uids = 50\r\ninput_uid = tf.Variable(tf.ones((32, 1), dtype=tf.int32))\r\nparams = tf.Variable(tf.random.normal((num_uids, 9)), trainable=True)\r\n\r\nparam = tf.gather_nd(params, input_uid)\r\n\r\ninput_shared_features = tf.Variable(tf.ones((32, 128), dtype=tf.float32))\r\ncombined = tf.concat([param, input_shared_features], axis=-1)\r\n\r\nnet = tf.keras.layers.Dense(128)(combined)\r\n\r\nPlease find the [Github Gist](https://colab.sandbox.google.com/gist/rmothukuru/b3427c06cab54beed19a381339c932e0/so_59962509.ipynb).\r\n\r\nThere is a [Stack Overflow Question](https://stackoverflow.com/questions/59962509/valueerror-cannot-convert-a-tensor-of-dtype-resource-to-a-numpy-array) also, associated with this issue.", "comments": ["I have replicated this issue in nightly as well and the issue persist, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/4a455a71f944fba5432b96ce7cae23e6/37441.ipynb).", "I have the same problem. I've been confused for a long time\u00b7\u00b7\u00b7", "Same problem here.\r\nTensorflow version \r\n\r\n`'2.2.0-rc3'`\r\n\r\nCode to reproduce the issue:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_datasets as tfds\r\n\r\nembedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\r\nhub_layer = hub.KerasLayer(embedding, input_shape=[], \r\n                           dtype=tf.string, trainable=True)\r\n\r\nDense = tf.keras.layers.Dense\r\nfc_model = tf.keras.Sequential()\r\nfc_model.add(hub_layer)\r\nfc_model.add(Dense(4096, activation=tf.nn.swish))\r\nfc_model.add(Dense(1))\r\n\r\nds_train, ds_val, ds_test = tfds.load(\r\n    name=\"imdb_reviews\", \r\n    split=('train[:60%]', 'train[60%:]', 'test'),\r\n    as_supervised=True)\r\n\r\nfc_model.compile(optimizer='adam',\r\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n\r\nfc_model.fit(ds_train.shuffle(10000).batch(512), validation_data=ds_test.batch(512), epochs=5, verbose=1)\r\n\r\ndef representative_dataset_gen():\r\n  for _ in range(num_calibration_steps):\r\n    # Get sample input data as a numpy array in a method of your choosing.\r\n    yield [input]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(fc_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\ntflite_quant_model = converter.convert()\r\n```\r\n\r\nThe issue:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n<ipython-input-31-b07abdccac46> in <module>()\r\n      8 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      9 converter.representative_dataset = representative_dataset_gen\r\n---> 10 tflite_quant_model = converter.convert()\r\n\r\n6 frames\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    457     frozen_func, graph_def = (\r\n    458         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n--> 459             self._funcs[0], lower_control_flow=False))\r\n    460     input_tensors = [\r\n    461         tensor for tensor in frozen_func.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n    704   \"\"\"\r\n    705   graph_def, converted_inputs = _convert_variables_to_constants_v2_impl(\r\n--> 706       func, lower_control_flow, aggressive_inlining)\r\n    707   frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)\r\n    708   return frozen_func, graph_def\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _convert_variables_to_constants_v2_impl(func, lower_control_flow, aggressive_inlining)\r\n    455 \r\n    456   # Get mapping from node name to variable value.\r\n--> 457   tensor_data = _get_tensor_data(func)\r\n    458 \r\n    459   # Get mapping from function name to argument types.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _get_tensor_data(func)\r\n    215       data = map_index_to_variable[idx].numpy()\r\n    216     else:\r\n--> 217       data = val_tensor.numpy()\r\n    218     tensor_data[tensor_name] = {\r\n    219         \"data\": data,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n    959     \"\"\"\r\n    960     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n--> 961     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n    962     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n    963 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n    927       return self._numpy_internal()\r\n    928     except core._NotOkStatusException as e:\r\n--> 929       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n    930 \r\n    931   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```", "I am facing the same issue. Any updates on a potential fix? Thanks.", "I had the same issue. The code worked when I manually entered data using tf.Variable but failed when I tried to build a model.", "Same issue, any update?", "I also had the same issue. Are there any solutions?", "I have posted a solution in the stackoverflow question associated with this issue - https://stackoverflow.com/a/62161525/2352424\r\nHope that helps", "I had the same issue, and was able to work around it like this:\r\n\r\n```py\r\nindices = Input(name='indices', shape=(), dtype='int32')\r\nparams = tf.Variable(params)\r\n\r\nclass GatherLayer(keras.layers.Layer):\r\n    def call(self, indices, params):\r\n        return tf.gather(params, indices)\r\n\r\noutput = GatherLayer()(indices, params)\r\n```\r\n\r\nRather unintuitively, the critical part is actually that the function arguments (`params` and `indices`) are swapped in the custom layer. If you don't do that, you still get the same error. I didn't dig deeper into why this is the case.", "> I had the same issue, and was able to work around it like this:\r\n> \r\n> ```python\r\n> indices = Input(name='indices', shape=(), dtype='int32')\r\n> params = tf.Variable(params)\r\n> \r\n> class GatherLayer(keras.layers.Layer):\r\n>     def call(self, indices, params):\r\n>         return tf.gather(params, indices)\r\n> \r\n> output = GatherLayer()(indices, params)\r\n> ```\r\n> \r\n> Rather unintuitively, the critical part is actually that the function arguments (`params` and `indices`) are swapped in the custom layer. If you don't do that, you still get the same error. I didn't dig deeper into why this is the case.\r\n\r\nAs obtuse as this might be, this worked for me too! ", "I have the same issue:\r\n```\r\ndef makeSimpleModelNoFeatures(\r\n    hp = {\r\n        'max_tokens':    5000, \r\n        'ngrams':        (1, 2), \r\n        'units':         64, \r\n        'learning_rate': 1e-3,\r\n        'dropout':       0.2,\r\n        'layers':        2\r\n    }\r\n):\r\n    vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n        max_tokens        = int(hp.get('max_tokens')),\r\n        ngrams            = hp.get('ngrams') if type(hp.get('ngrams')) == tuple else int(hp.get('ngrams')),\r\n        output_mode       = 'tf-idf',\r\n        pad_to_max_tokens = True,\r\n    )\r\n    vectorizer.adapt(TEXTS)\r\n    \r\n    textInput     = tf.keras.Input(shape=(1,),  name = 'text', dtype = tf.string)\r\n    \r\n    x = vectorizer(textInput)\r\n    x = tf.keras.layers.Dropout(hp.get('dropout'))(x)\r\n    \r\n    for index in range(hp.get('layers')):\r\n        x = tf.keras.layers.Dense(hp.get('units'), activation = 'relu')(x)\r\n        x = tf.keras.layers.Dropout(hp.get('dropout'))(x)\r\n    \r\n    output = tf.keras.layers.Dense(\r\n        1, \r\n        activation       = 'sigmoid',\r\n        bias_initializer = tf.keras.initializers.Constant(INITIAL_BIAS) if INITIAL_BIAS else None\r\n    )(x)\r\n    \r\n    model = tf.keras.Model(inputs = textInput, outputs = output)\r\n    model.compile(\r\n        optimizer = tf.keras.optimizers.Adam(lr = hp.get('learning_rate')),\r\n        loss      = tf.keras.losses.BinaryCrossentropy(),\r\n        metrics   = METRICS,\r\n    )\r\n\r\n    return model\r\n\r\nEPOCHS       = 10\r\nBATCH_SIZE   = 3000\r\ncheckpointer = getCheckpointer('ngrams, no features, no weigths')\r\n\r\nmodel   = makeSimpleModelNoFeatures()\r\nhistory = model.fit(\r\n    x               = trainInput['text'],\r\n    y               = trainLabels,\r\n    batch_size      = BATCH_SIZE,\r\n    epochs          = EPOCHS,\r\n    validation_data = (validationInput['text'], validationLabels),\r\n    callbacks       = [checkpointer],\r\n    class_weight    = None,\r\n    verbose         = 2,\r\n)\r\n\r\ntfliteModel = tf.lite.TFLiteConverter.from_keras_model(model).convert()\r\n\r\nwith tf.io.gfile.GFile('ngrams_20200628.tflite', 'wb') as f:\r\n    f.write(tfliteModel)\r\n```\r\n\r\n```\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```", "@rakeshmothukuru1 were you able to resolve the issue? ", "@MeghnaNatraj,\r\nSorry for the delayed response. Yes, it is resolved. Thank you @anjany for the workaround.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37441\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37441\">No</a>\n", "for reference, I was coming across the same problem within a custom layer I wrote. Changing the order of the inputs fixed it, though this seems like a problem that may need a real fix.", "@stephen-systemfriend  What version of TF are you running?\r\n\r\n\r\nFor TF 2.3.0, I get no errors for the following code snippet:\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=(1,), dtype=tf.int32, batch_size = 32)\r\nparams = tf.Variable(tf.random.normal((50, 9)), trainable=True)\r\n\r\n# Version 1\r\noutput = tf.gather_nd(params, input)\r\n# No error\r\n\r\n# Version 2\r\nclass UpdatedTFGather(tf.keras.layers.Layer):\r\n    def call(self, input, params):\r\n        return tf.gather_nd(params, input)\r\noutput = UpdatedTFGather()(input, params)\r\n# No error\r\n```\r\n\r\nAlso, the `tf.gather_nd` function arguments are expected to be of type \"tf.Tensor\" and not \"tf.keras.layers.Layer\". Thus, it is best to use the following code which will work across all TF versions:\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.Variable(tf.ones((32, 1), dtype=tf.int32))\r\nparams = tf.Variable(tf.random.normal((50, 9)), trainable=True)\r\noutput = tf.gather_nd(params, input)\r\n```\r\n\r\n", "TensorFlow 2.2.0\r\nThe code in question was a custom implementation of attention, but if I took the inputs as (value, query) then I would hit this error for some set of inputs. In the same structure it already was successfully called 5 times for other inputs, and changing the order to query, value fixed it\r\nThere are no explicit calls to `tf.gather_nd`", "@stephen-systemfriend  Thank you for the details. As it's not a blocker and you have a workaround, I hope that should do for now. If you face a major blocker, feel free to re-open this issue with further information.", "> Same problem here.\r\n> Tensorflow version\r\n> \r\n> `'2.2.0-rc3'`\r\n> \r\n> Code to reproduce the issue:\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> import tensorflow_hub as hub\r\n> import tensorflow_datasets as tfds\r\n> \r\n> embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\r\n> hub_layer = hub.KerasLayer(embedding, input_shape=[], \r\n>                            dtype=tf.string, trainable=True)\r\n> \r\n> Dense = tf.keras.layers.Dense\r\n> fc_model = tf.keras.Sequential()\r\n> fc_model.add(hub_layer)\r\n> fc_model.add(Dense(4096, activation=tf.nn.swish))\r\n> fc_model.add(Dense(1))\r\n> \r\n> ds_train, ds_val, ds_test = tfds.load(\r\n>     name=\"imdb_reviews\", \r\n>     split=('train[:60%]', 'train[60%:]', 'test'),\r\n>     as_supervised=True)\r\n> \r\n> fc_model.compile(optimizer='adam',\r\n>               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n>               metrics=['accuracy'])\r\n> \r\n> fc_model.fit(ds_train.shuffle(10000).batch(512), validation_data=ds_test.batch(512), epochs=5, verbose=1)\r\n> \r\n> def representative_dataset_gen():\r\n>   for _ in range(num_calibration_steps):\r\n>     # Get sample input data as a numpy array in a method of your choosing.\r\n>     yield [input]\r\n> \r\n> \r\n> converter = tf.lite.TFLiteConverter.from_keras_model(fc_model)\r\n> converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n> converter.representative_dataset = representative_dataset_gen\r\n> tflite_quant_model = converter.convert()\r\n> ```\r\n> \r\n> The issue:\r\n> \r\n> ```\r\n> \r\n> ---------------------------------------------------------------------------\r\n> \r\n> InvalidArgumentError                      Traceback (most recent call last)\r\n> \r\n> <ipython-input-31-b07abdccac46> in <module>()\r\n>       8 converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n>       9 converter.representative_dataset = representative_dataset_gen\r\n> ---> 10 tflite_quant_model = converter.convert()\r\n> \r\n> 6 frames\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n>     457     frozen_func, graph_def = (\r\n>     458         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n> --> 459             self._funcs[0], lower_control_flow=False))\r\n>     460     input_tensors = [\r\n>     461         tensor for tensor in frozen_func.inputs\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n>     704   \"\"\"\r\n>     705   graph_def, converted_inputs = _convert_variables_to_constants_v2_impl(\r\n> --> 706       func, lower_control_flow, aggressive_inlining)\r\n>     707   frozen_func = _construct_concrete_function(func, graph_def, converted_inputs)\r\n>     708   return frozen_func, graph_def\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _convert_variables_to_constants_v2_impl(func, lower_control_flow, aggressive_inlining)\r\n>     455 \r\n>     456   # Get mapping from node name to variable value.\r\n> --> 457   tensor_data = _get_tensor_data(func)\r\n>     458 \r\n>     459   # Get mapping from function name to argument types.\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _get_tensor_data(func)\r\n>     215       data = map_index_to_variable[idx].numpy()\r\n>     216     else:\r\n> --> 217       data = val_tensor.numpy()\r\n>     218     tensor_data[tensor_name] = {\r\n>     219         \"data\": data,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n>     959     \"\"\"\r\n>     960     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n> --> 961     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n>     962     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n>     963 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n>     927       return self._numpy_internal()\r\n>     928     except core._NotOkStatusException as e:\r\n> --> 929       six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n>     930 \r\n>     931   @property\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n> \r\n> InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n> ```\r\n\r\nThe problem comes from the model that has been pulled from tensorflow-hub originally. When you check the `hub_layer.signatures['serving_default'].__dict__` you can see the `_capture_inputs` was stored as `dtype=resource, numpy=<unprintable>`. Therefore, through tflite converter, it can not convert the weight"]}, {"number": 37440, "title": "AutoGraph error with FOR loop in Keras loss", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, x64\r\n- TensorFlow installed from (source or binary): Binary (pip)\r\n- TensorFlow version (use command below): TF 2.1.0 as well as TF 2.2.0 (2.2.0.dev20200304)\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n- GPU model and memory: Bug appears on several computers with different GPU\r\n\r\n**Describe the current behavior**\r\n\r\nError \"**tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.**\" when using a FOR loop over a Tensor dimension in a custom Keras loss in AutoGraph mode.\r\n\r\nNotice that when running eagerly, the bug does not appear. \r\nNotice also that using a similar FOR loop in a custom Keras model works both in AutoGraph and Eager modes. The bug is specific to Keras Loss.\r\nAs expected, replacing the loop with a call to tf.map_fn works correctly.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe behavior should be the same as running eagerly, without any error.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n# Custom loss with a FOR loop. Raises an error in AutoGraph mode. \r\n# A similar FOR loop in a Keras model works as expected.\r\nclass CustomLoss(keras.losses.Loss):\r\n\tdef call(self, y_true, y_pred):\r\n\t\tx = y_true + y_pred\r\n\t\tfor i in tf.range(tf.shape(y_true)[0]): # The error is reaised here.\r\n\t\t\tx += 1\r\n\t\treturn tf.reduce_mean(x)\r\n\r\nif __name__ == \"__main__\" :\r\n\tdata = np.random.random((1000, 3)).astype(np.float32)\r\n\t\r\n\tinputs = tf.keras.Input(shape=(1000,3))\r\n\toutputs = tf.keras.layers.Dense(3)(inputs)\r\n\tmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\t\r\n\tmodel.compile(loss=CustomLoss()) # does NOT work\r\n\t# model.compile(loss=CustomLoss(), run_eagerly = True) # works\r\n\t\r\n\tmodel.fit(x=data, y=data)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nLog in attachment.\r\n[log.txt](https://github.com/tensorflow/tensorflow/files/4306163/log.txt)\r\n", "comments": ["I have tried on colab with TF nightly and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/4f9dbb6a4b25213e7aec3edbe830f75f/untitled722.ipynb).Thanks", "I just noticed that the same bug also appears even if the loss is a simple callable instead of a subclass of keras.losses.Loss.", "@pavithrasv AutoGraph is not applied in custom losses, I believe. Should we add it, for consistency with the Layer behavior?", "Yes, i think it's a good idea to explore autographing custom Loss/Metric.", "Your answer seems surprising to me. I expected that the whole training loop was AutoGraphed in the fit method, thus including any custom loss.\r\n\r\nThanks for your feedback.", "This is a fair point. The training loop is indeed autographed. However, AutoGraph does not descend into internal TensorFlow APIs, and Keras is considered an internal API (it's part of the `tensorflow` module). In other places, like Keras layers, tf.data, etc., the internal APIs applies AutoGraph to all user-supplied functions. We need to do something consistent here.\r\n\r\nSo from that perspective, this can be considered a bug.\r\n\r\nOn the long term, the hope is to let AutoGraph step inside internal TF modules and remove issues like these, but we're not there yet.", "Note that this might be a regression, though it's unclear how - has this loss been working in the past?", "I cannot know as I only recently moved to TF 2.", "Same issue is unfortunately also present with metrics on the last nightly build:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nclass CustomMetric(keras.metrics.Metric):\r\n\tdef __init__(self):\r\n\t\tsuper(CustomMetric, self).__init__()\r\n\t\tself._metric = self.add_weight(name='metric', initializer='zeros', shape=())\r\n\t\r\n\tdef update_state(self, y_pred, y_true, sample_weights=None):\r\n\t\tbatchSize = tf.shape(y_pred)[0]\r\n\t\tfor b in range(batchSize):\r\n\t\t\tself._metric.assign_add(1.0)\r\n\t\r\n\tdef result(self):\r\n\t\treturn self._metric\r\n\t\r\n\tdef reset_states(self):\r\n\t\tself._metric.assign(0.0)\r\n\r\nif __name__ == \"__main__\" :\r\n\tdata = np.random.random((1000, 3)).astype(np.float32)\r\n\t\r\n\tinputs = tf.keras.Input(shape=(1000, 3))\r\n\toutputs = tf.keras.layers.Dense(3)(inputs)\r\n\tmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n\t\r\n\tmodel.compile(loss=\"mse\", metrics=CustomMetric()) #Does NOT WORK\r\n\t# model.compile(loss=\"mse\", metrics=CustomMetric(), run_eagerly=True) #WORKS\r\n\t\r\n\tmodel.fit(x=data, y=data)\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37440\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37440\">No</a>\n"]}, {"number": 37439, "title": "TypeError: '_UserObject' object is not callable,  why tf.saved_model.load() failed?", "body": "**System information** \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:/Pyfile/tensorflow2.X/load_model.py\", line 12, in <module>\r\n    print(model(tf.random.normal((1, 3))))\r\nTypeError: '_UserObject' object is not callable\r\n```\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\nmodel = Model()\r\n\r\n# high-level api\r\n# model.predict(tf.random.normal((2, 3)))\r\n# model.save(\"save/high\", save_format=\"tf\")\r\n\r\n# low-level api\r\ntf.saved_model.save(model, 'save/low')\r\n\r\n---------------------------------------------\r\n\r\nimport os\r\nimport tensorflow as tf\r\n\r\n# high-level api\r\n# model = tf.keras.models.load_model('save/high')\r\n\r\n# low-level api\r\nmodel = tf.saved_model.load('save/low')\r\n\r\nprint(model(tf.random.normal((1, 3))))   #  error!!!\r\n```\r\nIf I user the high-level api to save and load, it runs successfully. If I use tf.saved_mode.save()\uff0cit can save successfully with warning\r\n'''\r\nWARNING:tensorflow:Skipping full serialization of Keras model <__main__.Model object at 0x0000028C0515F6D8>, because its inputs are not defined.\r\n2020-03-09 18:20:37.304479: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x0000028C1CB87CC0>, because it is not built.\r\n''' \r\nthen load failed, So what does  '_UserObject' object is not callable mean?  How can I fix this, thanks for any help", "comments": ["@xiaoyangnihao,\r\nOn running the above code, I got an error stating `NameError: name 'model' is not defined`. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/0e172b1fa3c2202b97ea6978ec78a304/37439.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "> @xiaoyangnihao,\r\n> On running the above code, I got an error stating `NameError: name 'model' is not defined`. Please find the gist of it [here](https://colab.sandbox.google.com/gist/amahendrakar/0e172b1fa3c2202b97ea6978ec78a304/37439.ipynb).\r\n> \r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!\r\n\r\nSorry for missing some codes. I update the description above, so you can reproduce the issue, thanks", "The tensorflow2.0 use dynamic graphs. \r\nthe model can auto infer the input shape by your input. \r\nBut in your \"low-level api\" code, the Dense layer has not been inputed, so it can't auto infer the shape of input to build model. So, you need input some data like  your ''high-level api\" code, then save model.\r\n\r\nI suggest you  follow the code behind like keras, to let the model know the  shape of your inputs.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\n\r\nmymodel = Model()\r\ninput = tf.keras.layers.Input(shape=(3,))\r\nout = mymodel(input)\r\nmodel=tf.keras.models.Model(inputs=input, outputs=out)\r\n```\r\n\r\n\r\n\r\n", "> The tensorflow2.0 use dynamic graphs.\r\n> the model can auto infer the input shape by your input.\r\n> But in your \"low-level api\" code, the Dense layer has not been inputed, so it can't auto infer the shape of input to build model. So, you need input some data like your ''high-level api\" code, then save model.\r\n> \r\n> I suggest you follow the code behind like keras, to let the model know the shape of your inputs.\r\n> \r\n> ```\r\n> import os\r\n> import tensorflow as tf\r\n> \r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n> \r\n> class Model(tf.keras.Model):\r\n>     def __init__(self):\r\n>         super(Model, self).__init__()\r\n>         self.d = tf.keras.layers.Dense(2)\r\n> \r\n>     def call(self, x, training=True, mask=None):\r\n>         return self.d(x)\r\n> \r\n> \r\n> mymodel = Model()\r\n> input = tf.keras.layers.Input(shape=(3,))\r\n> out = mymodel(input)\r\n> model=tf.keras.models.Model(inputs=input, outputs=out)\r\n> ```\r\n\r\nYeah, this does work. \r\nBut In my real project, I use tf.saved_model.save() to save the model in checkpoint_interval, I think **after running serveral batch_sizes, the input_shape should be configured**, but when I use tf.saved_model.load() to load the model and call the input, TypeError: '_UserObject' object is not callable arises. I wonder why ? ", "```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(1)\r\n\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\nmodel = Model()\r\n\r\noptimizer = tf.keras.optimizers.SGD()\r\nwith tf.GradientTape() as tape:\r\n    out = model(tf.random.normal((2, 3)))\r\n    grads = tape.gradient(out, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads,model.trainable_weights))\r\n\r\ntf.saved_model.save(model, 'save/low')\r\n\r\nmodel_load= tf.saved_model.load('save/low')\r\nprint(model_load(tf.random.normal((2, 3))))\r\n\r\n```\r\nyeah, in this way ,it really have  TypeError: '_UserObject' object is not callable arises. \r\n\r\nAccording my understand , a keras model need input_layer.  If you have not input_layer, you  saved model that is not really model .it is just a layer .\r\n\r\nI also want to know how you  explain this issue.  @amahendrakar\r\n\r\n", "@amahendrakar any suggestions?", "# build model\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(Model, self).__init__(**kwargs)\r\n        self.d = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.ones())\r\n\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\nmodel = Model()\r\noptimizer = tf.keras.optimizers.SGD()\r\nwith tf.GradientTape() as tape:\r\n    out = model(tf.ones((2, 3)))\r\n    grads = tape.gradient(out, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n\r\nprint(model(tf.ones((2, 3))))\r\n```\r\n# save and load model\r\n## Approach 1\r\nhttps://www.tensorflow.org/guide/keras/save_and_serialize#approach_1\r\n\r\n```\r\nmodel.save_weights('save/')\r\nmodel_load = Model()\r\nmodel_load.load_weights('save/')\r\nprint(model_load(tf.ones((2, 3))))\r\n```\r\n**It works well**.\r\n\r\n\r\n## Approach2\r\nhttps://www.tensorflow.org/guide/keras/save_and_serialize#approach_2\r\n\r\n```\r\nmodel.save('save')\r\nmodel_load = tf.keras.models.load_model('save/')\r\nprint(model_load(tf.ones((2, 3))))\r\n```\r\n**ValueError**: Model <__main__.Model object at 0x4441057d0> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call ### model._set_inputs(inputs).\r\n\r\n## Approach3\r\nhttps://www.tensorflow.org/guide/keras/save_and_serialize#approach_3\r\n```\r\ntf.saved_model.save(model,'save')\r\nmodel_load = tf.keras.models.load_model('save')\r\nprint(model_load(tf.ones((2, 3))))\r\n```\r\n**AttributeError**: '_UserObject' object has no attribute 'call_and_return_conditional_losses'\r\n\r\n\r\nIn the eager mode, just **Approach 1** `save_weights` can work well.  But if model is trained  by using model.fit(), These methods all work well .\r\nI think this might be  a bug.  But if not\uff0cI think these methods of saving and loading models should be distinguished in the docs.  @amahendrakar \r\n", "> # build model\r\n> ```\r\n> import os\r\n> import tensorflow as tf\r\n> \r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n> \r\n> \r\n> class Model(tf.keras.Model):\r\n>     def __init__(self, **kwargs):\r\n>         super(Model, self).__init__(**kwargs)\r\n>         self.d = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.ones())\r\n> \r\n>     def call(self, x, training=True, mask=None):\r\n>         return self.d(x)\r\n> \r\n> model = Model()\r\n> optimizer = tf.keras.optimizers.SGD()\r\n> with tf.GradientTape() as tape:\r\n>     out = model(tf.ones((2, 3)))\r\n>     grads = tape.gradient(out, model.trainable_weights)\r\n>     optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n> \r\n> print(model(tf.ones((2, 3))))\r\n> ```\r\n> \r\n> # save and load model\r\n> ## Approach 1\r\n> https://www.tensorflow.org/guide/keras/save_and_serialize#approach_1\r\n> \r\n> ```\r\n> model.save_weights('save/')\r\n> model_load = Model()\r\n> model_load.load_weights('save/')\r\n> print(model_load(tf.ones((2, 3))))\r\n> ```\r\n> \r\n> **It works well**.\r\n> \r\n> ## Approach2\r\n> https://www.tensorflow.org/guide/keras/save_and_serialize#approach_2\r\n> \r\n> ```\r\n> model.save('save')\r\n> model_load = tf.keras.models.load_model('save/')\r\n> print(model_load(tf.ones((2, 3))))\r\n> ```\r\n> \r\n> **ValueError**: Model <**main**.Model object at 0x4441057d0> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call ### model._set_inputs(inputs).\r\n> \r\n> ## Approach3\r\n> https://www.tensorflow.org/guide/keras/save_and_serialize#approach_3\r\n> \r\n> ```\r\n> tf.saved_model.save(model,'save')\r\n> model_load = tf.keras.models.load_model('save')\r\n> print(model_load(tf.ones((2, 3))))\r\n> ```\r\n> \r\n> **AttributeError**: '_UserObject' object has no attribute 'call_and_return_conditional_losses'\r\n> \r\n> In the eager mode, just **Approach 1** `save_weights` can work well. But if model is trained by using model.fit(), These methods all work well .\r\n> I think this might be a bug. But if not\uff0cI think these methods of saving and loading models should be distinguished in the docs. @amahendrakar\r\n\r\n@SmileTM  The following method will be OK, \r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\nprint(tf.executing_eagerly())\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(64)\r\n\r\n    def call(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\n\r\n@tf.function\r\ndef train(inputs, targets, model, optimizer, loss_fn):\r\n    with tf.GradientTape() as tape:\r\n        predicts = model(inputs)\r\n        loss = loss_fn(targets, predicts)\r\n    \r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n\r\n\r\ndef inference(tests):\r\n    model = tf.saved_model.load('save/demo-1')\r\n    print(model(tests))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    model = Model()\r\n    optimizer = tf.optimizers.Adam()\r\n    loss_fn = tf.losses.MeanSquaredError()\r\n\r\n    inputs = tf.random.normal((8, 10, 16))\r\n    targets = tf.random.normal((8, 10, 1))\r\n\r\n    train(inputs, targets, model, optimizer, loss_fn)\r\n    tf.saved_model.save(model, 'save/demo-1')\r\n\r\n    tests = tf.random.normal((1, 10, 16))\r\n    inference(tests)\r\n\r\n```", "@xiaoyangnihao That's actually a rather easy fix which is more related to Python than to TF:\r\n\r\nThe method in `Model` must be named `__call__` rather than `call` for the model to be callable (i.e. you can do `model(x)`). As it is right now you would have to do `model.call(x)`.\r\n\r\nApparently this was wrong in the documentation at some point because we also had this problem in some places. It's correct now though.", "> @xiaoyangnihao That's actually a rather easy fix which is more related to Python than to TF:\r\n> \r\n> The method in `Model` must be named `__call__` rather than `call` for the model to be callable (i.e. you can do `model(x)`). As it is right now you would have to do `model.call(x)`.\r\n> \r\n> Apparently this was wrong in the documentation at some point because we also had this problem in some places. It's correct now though.\r\n\r\n@jjedele  My current version is tf2.1, does I need to update lasted version ?  Can you give some examples or codes about above questions? I don't think it is solved.\r\n\r\n```\r\n!pip uninstall tensorflow\r\n!pip install tf-nightly\r\n\r\nimport os\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    def __call__(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\nmodel = Model()\r\n\r\n# low-level api\r\ntf.saved_model.save(model, 'save/low')\r\n\r\n# low-level api\r\nmodel = tf.saved_model.load('save/low')\r\nprint(model.call(tf.random.normal((1, 3))))   #  error!!!\r\n```\r\n\r\n```\r\n2.2.0-dev20200312\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <__main__.Model object at 0x7f0cbc971080>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.core.Dense object at 0x7f0c1c8f6978>, because it is not built.\r\nINFO:tensorflow:Assets written to: save/low/assets\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-451ee428c4e1> in <module>()\r\n     30 model = tf.saved_model.load('save/low')\r\n     31 \r\n---> 32 print(model.call(tf.random.normal((1, 3))))   #  error!!!\r\n\r\nAttributeError: '_UserObject' object has no attribute 'call'\r\n```\r\n", "@xiaoyangnihao Sorry, I was squeezing too much information into my answer. Now you fixed the one part and broke the other \ud83d\ude04\r\n\r\nDo the following:\r\n\r\n```python\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.d = tf.keras.layers.Dense(2)\r\n\r\n    # \"dunder\"-methods (__...__) typically are called implicitly by Python\r\n    def __call__(self, x, training=True, mask=None):\r\n        return self.d(x)\r\n\r\n...\r\n\r\nmodel(tf.random.normal((1, 3))) # no explicit .call here\r\n```", "Was able to reproduce the issue with TF2.1 and TF-nightly. Please find the Gist [here](https://colab.research.google.com/gist/amahendrakar/f89c323afb0b7e10e26a8f17a39c0508/37439.ipynb). Thanks!", "We intentionally don't deal with Keras types in tf.saved_model.save/tf.saved_model.load so the libraries are layered.\r\n\r\nBut @k-w-w has added SavedModel APIs which do save and restore Keras types: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#save (tf.keras.model.save(..., save_format='tf'))", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37439\">No</a>\n"]}, {"number": 37438, "title": "Minor refactoring and robust mode check for imagenet_utils.py", "body": "Minor refactoring of code for better readability of conditional statements (using `elif` instead of multiple `if`s if the statements check for conditions on the same object). Also added robust `mode` argument check to prevent silent failure.", "comments": ["@jaketae Can you please check build failures. Thanks!", "@gbaned Thank you for the friendly reminder. I realized that one of the files had a missing argument required for docstring formatting. I've pushed a new commit, so hopefully it goes through. ", "@jaketae Could you please resolve the conflicts? Thanks!"]}, {"number": 37437, "title": "Update bucketized_column documentation comment to fix markdown", "body": "The documentation generator was merging the prose with the preceding and ensuing code blocks, resulting in rather curious formatting.\r\n\r\nThis tries to fix it by starting the non-code sentence with a something that does not have code markdown (back-plings).", "comments": ["@mjul Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 37433, "title": "AttributeError: module 'tensorboard' has no attribute 'lazy'", "body": "I've been trying to import tensorflow, downloading Anaconda 3, creating virtual environment, use pip install tensorflow, pip install keras and when I try to import tensorflow an error occured  \"Failed to load the native TensorFlow runtime.\" then I tried to resolve it by downloading cygwin to update the gcc. After that, another error occured \r\n\r\n```\r\n2020-01-21 16:18:17.197417: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-01-21 16:18:17.197832: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nTried to solve it by downloading CUDA 10.1 version and now the problem is\r\n\r\n```\r\n2020-03-09 14:17:07.528838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n```\r\n\r\nThis is a problem because after that all process finished with exit code 1 and print('test') cannot even be processed after this.\r\n\r\nSystem information:\r\nAnaconda 3\r\nWindows 10\r\nTensorflow 2.1\r\nInteil Core I5\r\nGtx 1050 ti\r\nCUDA version 10.1\r\npython version 3.6.1\r\n\r\n\r\nDescribe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe errors when I try to import tensorflow:\r\n\r\n```\r\n2020-03-09 14:17:07.528838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nTraceback (most recent call last):\r\n  File \"C:/Users/User/PycharmProjects/tensortest/test.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 46, in <module>\r\n    from . _api.v2 import compat\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 32, in <module>\r\n    from . import compat\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py\", line 29, in <module>\r\n    from tensorflow._api.v2.compat.v1 import app\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\__init__.py\", line 39, in <module>\r\n    from . import v1\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\__init__.py\", line 32, in <module>\r\n    from . import compat\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\__init__.py\", line 40, in <module>\r\n    from . import v2\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py\", line 30, in <module>\r\n    from tensorflow._api.v2.compat.v2 import audio\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v2\\__init__.py\", line 33, in <module>\r\n    from . import compat\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v2\\compat\\__init__.py\", line 40, in <module>\r\n    from . import v2\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorflow_core\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py\", line 320, in <module>\r\n    from tensorboard.summary._tf import summary\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorboard\\summary\\__init__.py\", line 31, in <module>\r\n    from tensorboard.summary import v2\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorboard\\summary\\v2.py\", line 24, in <module>\r\n    from tensorboard.plugins.audio.summary_v2 import audio\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorboard\\plugins\\audio\\summary_v2.py\", line 30, in <module>\r\n    from tensorboard.compat import tf2 as tf\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorboard\\compat\\__init__.py\", line 28, in <module>\r\n    import tensorboard.lazy as _lazy\r\nAttributeError: module 'tensorboard' has no attribute 'lazy'\r\n\r\nProcess finished with exit code 1\r\n```\r\n", "comments": ["@PreVizsla, Did you install cuDNN? If not install cuDNN >=7.6 version. \r\nPlease take a look at the [Tensorflow doc](https://www.tensorflow.org/install/gpu#software_requirements). Thanks", "yes, followed this https://www.easy-tensorflow.com/tf-tutorials/install/cuda-cudnn\r\nand downloaded the 7.6 cuDNN and still same error", "Will installing an older version of tensorflow helps the problem? Since I have no idea what kind of problem makes this error", "i have like this problem and i checked cudnn version but i have still this problem how i can fix ?\r\n\r\n```\r\n\"I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\"\r\n```", "Hi, did anyone find a solution to this? I am having the same trouble.", "trying to run : \r\n\r\n```\r\nfrom keras.preprocessing.image import load_img, img_to_array\r\n```\r\n\r\nand getting the same error", "same thing here, any one has any solutions?", "I am facing the same thing, can anyone please help!", "Yep facing same issues, installed it and worked before in the past but with this fresh install I'm running into the same problems others are having.", "There is more than just errors printed to stderr. The log format is\r\n\r\n```\r\ndate time: FEWID message\r\n```\r\n\r\nwhere `FEWID` is one of `F` (fatal error, program stops), `E` (error, program continues but with different behavior), `W` (warning), `I` (info), `D` (debug). The last 3 steps are useful for understanding behavior but are **NOT** errors\r\n\r\nThus:\r\n\r\n```\r\n2020-01-21 16:18:17.197417: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-01-21 16:18:17.197832: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nDoesn't mean that you need to install CUDA to use TF. You can use TF without having CUDA, it will just use CPU.\r\n\r\nAlso,\r\n\r\n```\r\n2020-03-09 14:17:07.528838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n```\r\n\r\nonly means that the CUDA library was loaded. It is not an error.\r\n\r\nThe real error is \r\n\r\n```\r\n...\r\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensortest\\lib\\site-packages\\tensorboard\\compat\\__init__.py\", line 28, in <module>\r\n    import tensorboard.lazy as _lazy\r\nAttributeError: module 'tensorboard' has no attribute 'lazy'\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nwhich makes me think you have an mismatched version of tensorboard and tensorflow. What does `pip list` print?", "Update your 'NVIDIA Graphics Driver' with latest and compatible version for your PC.\r\nDownload Driver: https://www.nvidia.com/Download/index.aspx#\r\nTo find out required driver: https://youtu.be/a6ZdiNBPCC8", "please give the solution., I am also facing same issue\r\nplease.....\r\n>>> import tensorflow\r\n2020-05-20 17:55:45.825216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\dipta\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\dipta\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 84, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\Users\\dipta\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"C:\\Users\\dipta\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\", line 23, in <module>\r\n    from tensorflow.python.keras import backend as K\r\n  File \"C:\\Users\\dipta\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 5948, in <module>\r\n    assert _image_data_format in {'channels_last', 'channels_first'}\r\nAssertionError", "@nfelt could you take a look?\r\nWe suspect this is due to a mismatched TF/tensorboard version, but could not confirm.", "How can I determine which versions of TF and tensorboard are compatible with each other?  I have the same problem.  I have TF 2.2.0 and tensorboard 2.2.2.", "I'm getting the same error:\r\n2020-06-10 20:15:45.369670: W tensorflow/stream_executor/platform/default/dso_loader.cc:", "actually I can now use the older version of tensor flow with no problem, I am using tensorflow 1.7 right now. But I haven't try to use the 2.0 version again.", "@johnLIC You should use the same versions for TF, estimator and tensorboard\r\n\r\n@brandonssmith that's a warning, not an error\r\n\r\n@PreVizsla yes, you can use any released TF but if you encounter a bug in a version we no longer support we won't be able to help you. And we won't issue security updates and patches to old versions.", "@mihaimaruseac how to check tensorboard version? And do we need the exact same version for both TF and tensorboard?\r\n", "Surprisingly, After creating new conda virtual environment and downloading tensorflow and keras, I am able to run tensorflow 2.1 in pycharm with 20 Info notice in the console, but the script and tf works fine.", "@mihaimaruseac If it keeps me from progressing it's an error, I could care less how it is classified.", "@brandonssmith What @mihaimaruseac meant was that message will not block your progress. There must be another issue that is blocking your progress. If you need help, please file a new issue providing all the details in the template.\r\n\r\nAlso, please make sure to abide by our community guidelines in your comments (referring to your deleted comment).", "@PreVizsla sorry, was away. `pip list` should display the version of all installed pip packages.\r\n\r\nInstalling in a new environment ensures that all dependencies are installed at a corresponding version", "I have a similar issue in google colab:\r\n\r\n- tensorflow version 2.2.0\r\n- tensorboard version 2.2.2\r\n\r\n```\r\n%tensorboard --logdir drive/...\r\n```\r\n\r\ngives `AttributeError: module 'tensorboard' has no attribute 'lazy'`", "@irinawetransfer can you please post output of `pip list` and `pip debug --verbose`? Also, if possible, a minimal code that reproduces the issue?", "I have the same problem.\r\n\r\n<details><summary>the output of `pip list`:</summary>\r\n\r\n```\r\nPackage                  Version\r\n------------------------ -----------\r\nabsl-py                  0.9.0\r\nastunparse               1.6.3\r\ncachetools               4.1.0\r\ncertifi                  2020.6.20\r\nchardet                  3.0.4\r\ngast                     0.3.3\r\ngoogle-auth              1.18.0\r\ngoogle-auth-oauthlib     0.4.1\r\ngoogle-pasta             0.2.0\r\ngrpcio                   1.30.0\r\nh5py                     2.10.0\r\nidna                     2.9\r\nKeras-Preprocessing      1.1.2\r\nMarkdown                 3.2.2\r\nnumpy                    1.19.0\r\noauthlib                 3.1.0\r\nopt-einsum               3.2.1\r\npip                      19.2.3\r\nprotobuf                 3.12.2\r\npyasn1                   0.4.8\r\npyasn1-modules           0.2.8\r\nrequests                 2.24.0\r\nrequests-oauthlib        1.3.0\r\nrsa                      4.6\r\nscipy                    1.4.1\r\nsetuptools               41.2.0\r\nsix                      1.15.0\r\ntensorboard              2.2.2\r\ntensorboard-plugin-wit   1.6.0.post3\r\ntensorflow-gpu           2.2.0\r\ntensorflow-gpu-estimator 2.2.0\r\ntermcolor                1.1.0\r\nurllib3                  1.25.9\r\nWerkzeug                 1.0.1\r\nwheel                    0.34.2\r\nwrapt                    1.12.1\r\n```\r\n</details>\r\n\r\n<details><summary>the output of `pip debug --verbose`:</summary>\r\n\r\n```\r\npip version: pip 19.2.3 from c:\\users\\ghassen\\downloads\\lisadetection\\lisadetection\\venv2\\lib\\site-packages\\pip (python 3.8)\r\nsys.version: 3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:37:02) [MSC v.1924 64 bit (AMD64)]\r\nsys.executable: c:\\users\\ghassen\\downloads\\lisadetection\\lisadetection\\venv2\\scripts\\python.exe\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: cp1252\r\nsys.platform: win32\r\nsys.implementation:\r\n  name: cpython\r\nConfig variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\r\nCompatible tags: 15\r\n  cp38-cp38-win_amd64\r\n  cp38-none-win_amd64\r\n  py3-none-win_amd64\r\n  cp38-none-any\r\n  cp3-none-any\r\n  py38-none-any\r\n  py3-none-any\r\n  py37-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```\r\n\r\n</details>\r\n\r\nFor `import tensorflow as tf` this is the output:\r\n\r\n```\r\n2020-06-24 14:50:11.230153: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-06-24 14:50:11.236957: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n```\r\n\r\nand for `tf.test.is_gpu_available()` the output is:\r\n\r\n<details>\r\n```\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-06-24 14:51:00.146205: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2020-06-24 14:51:00.171655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21868deee70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-06-24 14:51:00.182635: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-06-24 14:51:00.190956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-06-24 14:51:01.031439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0\r\ncoreClock: 1.176GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 74.65GiB/s\r\n2020-06-24 14:51:01.045344: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-06-24 14:51:01.051842: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2020-06-24 14:51:01.058515: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2020-06-24 14:51:01.065241: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2020-06-24 14:51:01.072266: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2020-06-24 14:51:01.080353: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2020-06-24 14:51:01.087954: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found\r\n2020-06-24 14:51:01.095638: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-06-24 14:51:01.186835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-06-24 14:51:01.194906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0\r\n2020-06-24 14:51:01.198498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N\r\n2020-06-24 14:51:01.206694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x21876e35be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-06-24 14:51:01.214517: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0\r\nFalse\r\n```\r\n</details>\r\n\r\nI tried also with Tensorboard 2.2.0, the same problem remains", "@GhassenBenMakhlouf I don't see the `AttributeError: module 'tensorboard' has no attribute 'lazy'` error in your post. All I see are warning and information logs, not errors. The other log messages can be safely ignored, they are there to provide you with context about what is happening and to help debugging.", "@mihaimaruseac the problem is that I can not use GPU because this error: Could not load dynamic library 'cudart64_101.dll' \r\n\r\nI know it is an information log or warning because the program can still use the CPU.\r\nBut I want to use GPU, please help me solve this issue. please refer me to another issue if you have the solution somewhere.", "That is not the same problem and it should actually be opened as a new issue. Let's keep this issue to the `AttributeError: module 'tensorboard' has no attribute 'lazy'` error", "> That is not the same problem and it should actually be opened as a new issue. Let's keep this issue to the `AttributeError: module 'tensorboard' has no attribute 'lazy'` error\r\n\r\nhere you are:\r\nhttps://github.com/tensorflow/tensorflow/issues/40804", "`pip uninstall tensorflow-tensorboard`\r\n`pip install tensorboard`\r\n\r\nhad the same problem and this fixed it, hope it helps. Also keep an eye on versions and your virtual env", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37433\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37433\">No</a>\n", "For more context please refer [comment](https://github.com/tensorflow/tensorboard/issues/1862#issuecomment-782518739)"]}, {"number": 37432, "title": "Encountered unresolved custom op: BatchMatMulV2.", "body": "**System information**\r\n- OS Platform and Distribution (Pop!_OS 19.10):\r\n- TensorFlow installed from (source or binary):python3 -m pip install\r\n- TensorFlow version - 2.2.0-dev20200308\r\n- TensorFlowLite version [0.0.0-nightly](https://bintray.com/google/tensorflow/tensorflow-lite/0.0.0-nightly#)\r\n\r\nI was trying to convert a Transformer model to TfLite. It converted successfully with Python using the `converter.experimental_new_converter = True` and `converter.allow_custom_ops = True` but can't run it on android.\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Encountered unresolved custom op: BatchMatMulV2.\r\n2020-03-09 12:09:21.727 6025-6025/com.meiteimayek.transformertest W/System.err: Node number 31 (BatchMatMulV2) failed to prepare.\r\n2020-03-09 12:09:21.727 6025-6025/com.meiteimayek.transformertest W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n2020-03-09 12:09:21.727 6025-6025/com.meiteimayek.transformertest W/System.err:     at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:152)\r\n2020-03-09 12:09:21.727 6025-6025/com.meiteimayek.transformertest W/System.err:     at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n```java\r\n\r\nimport android.app.Activity;\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.util.ArrayMap;\r\nimport android.util.Log;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\n\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\nimport java.util.Arrays;\r\nimport java.util.Map;\r\n\r\npublic class Model {\r\n  \r\n  private Interpreter mInterpreter;\r\n  \r\n  public Model(Activity activity) throws IOException {\r\n    float[] inp = {4, 25,  7, 25, 14, 15, 18, 20,  7,  5,  0,  0,  0,  0,  0,  0,  0,\r\n      0,  0,  0,  0,  0,  0,  0};\r\n    \r\n    String name = \"model.tflite\";\r\n    ByteBuffer input = ByteBuffer.allocate(4 * 24);\r\n    ByteBuffer oInput = ByteBuffer.allocate(4 * 23);\r\n    \r\n    input.order(ByteOrder.nativeOrder());\r\n    oInput.order(ByteOrder.nativeOrder());\r\n    \r\n  \r\n    for(float v : inp) {\r\n      input.putFloat(v);\r\n    }\r\n    float[] predicted = new float[1];\r\n    \r\n    oInput.putFloat(1);\r\n    for(int i = 1; i < 23; i++) {\r\n      oInput.putFloat(0);\r\n    }\r\n    Interpreter.Options op = new Interpreter.Options();\r\n    \r\n    mInterpreter = new Interpreter(getModel(activity, name), op);\r\n\r\n    Map<Integer, Object> result = new ArrayMap<>();\r\n    result.put(0, predicted);\r\n    mInterpreter.runForMultipleInputsOutputs(new ByteBuffer[]{input, oInput}, result);\r\n    Log.d(\"TAG\", \"Model: Output \" + Arrays.toString((float[])result.get(0)));\r\n  }\r\n  \r\n  private MappedByteBuffer getModel(Activity activity, String name) throws IOException {\r\n    AssetFileDescriptor f = activity.getAssets().openFd(name);\r\n    return new FileInputStream(f.getFileDescriptor())\r\n      .getChannel()\r\n      .map(FileChannel.MapMode.READ_ONLY, f.getStartOffset(), f.getDeclaredLength());\r\n  }\r\n}\r\n\r\n```\r\n\r\nTFlite Model Link - https://drive.google.com/open?id=1URF3USAEkOFaKCCOB7ctl_b2FxTVDWPx\r\n", "comments": ["We plan on adding support for BatchMatMul in the TF 2.3 release.", "So, when can we expect TF 2.3 release.", "> So, when can we expect TF 2.3 release.\r\n\r\nMid Q2. We'll post here when it's available in the nightly builds.\r\n\r\n", "@GyanendroKh Could you please try on latest stable version of tf 2.5 or 2.4.1 and let us know if this is still an issue.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I believe we supported BatchMatMul in TensorFlow Lite natively long time ago. \r\nThis should be already resolved closing. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37432\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37432\">No</a>\n"]}, {"number": 37431, "title": "undefined reference error while building Lite for ESP32 \"person_detection\" example", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win7\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): Latest version\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP32\r\n\r\n**Describe the problem**\r\nI git the whole latest tensorflow branch on my WIN7 PC. I intended to use \"Tensorflow Lite for Micro\". I wanted to try the included examples \"Person Detection\" first to make sure everything works out of box. So I followed the readme instruction by running \"make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project\". According to the Makefile, five 3rd party downloads should be downloaded first. However, I got the MD5 mismatch error for each and every single one of them. So I had to 'hack' the Makefile to disable the MD5 checksum checking and manually downloaded those required 5 downloads and installed in the 'C:\\Projects\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\downloads' folder. \r\nThen the make seemed to run for a few minutes, the stopped at the following error:\r\n[888/894] Building CXX object esp-idf/...core/api/flatbuffer_conversions.cc.obj\r\ncc1plus.exe: warning: command line option '-std=c11' is valid for C/ObjC but not\r\n for C++\r\n[893/894] Linking CXX executable person_detection.elf\r\nFAILED: person_detection.elf\r\ncmd.exe /C \"cd . && C:\\Users\\Tianhao\\.espressif\\tools\\xtensa-esp32-elf\\esp-2019r\r\n2-8.2.0\\xtensa-esp32-elf\\bin\\xtensa-esp32-elf-g++.exe  -mlongcalls -Wno-frame-ad\r\ndress  -nostdlib @CMakeFiles\\person_detection.elf.rsp  -o person_detection.elf\r\n&& cd .\"\r\nc:/users/tianhao/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32\r\n-elf/bin/../lib/gcc/xtensa-esp32-elf/8.2.0/../../../../xtensa-esp32-elf/bin/ld.e\r\nxe: esp-idf/main/libmain.a(main_functions.cc.obj):(.literal.loop+0x14): undefine\r\nd reference to `RespondToDetection(tflite::ErrorReporter*, unsigned char, unsign\r\ned char)'\r\nc:/users/tianhao/.espressif/tools/xtensa-esp32-elf/esp-2019r2-8.2.0/xtensa-esp32\r\n-elf/bin/../lib/gcc/xtensa-esp32-elf/8.2.0/../../../../xtensa-esp32-elf/bin/ld.e\r\nxe: esp-idf/main/libmain.a(main_functions.cc.obj): in function `loop':\r\nc:\\projects\\tensorflow\\tensorflow\\lite\\micro\\tools\\make\\gen\\esp_xtensa-esp32\\prj\r\n\\person_detection\\esp-idf\\build/../main/main_functions.cc:111: undefined referen\r\nce to `RespondToDetection(tflite::ErrorReporter*, unsigned char, unsigned char)'\r\n\r\ncollect2.exe: error: ld returned 1 exit status\r\nninja: build stopped: subcommand failed.\r\nninja failed with exit code 1\r\n\r\nI also attached full log file for reference. Please show me how to fix this problem and get at least the example built and running. Thanks!\r\n\r\n[build_log.txt](https://github.com/tensorflow/tensorflow/files/4304902/build_log.txt)\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\n", "comments": ["@Hazeline2018 \r\nCould you please refer to existing old issues related to this topic.\r\nplease let us know if [this helps](https://github.com/tensorflow/tensorflow/issues/28105#issuecomment-490139562)", "@Saduf2019 \r\nthanks for your prompt response. However, I read through the post you suggested but did not see anywhere could help with my own issue. I'm trying to make for ESP32 mcu on WIN7 machine. But the link you suggested was for Android build. Please help pointing to the right direction. Thanks!", "**Edit:** \r\nTo correct my original problem description: the make step was successful after I ''hacked\" the MD5 checksum and manually downloaded those 5 3rd party downloads. \r\n\r\nBut this **'undefined reference to `RespondToDetection(tflite::ErrorReporter*, unsigned char, unsigned char)'** error came when I tried to build the example prj using \"**idf.py build**\" cmd.\r\n\r\nJust as a reference, I also tried building 'hello_world' example with same procedure, that went successfully. So I guess there might be something missing in the 'person_detection' example.\r\n\r\nHope that clear things up. Thanks!\r\n\r\n", "Can anyone please help with this problem? I'm stuck here for a couple of days and I've exhausted almost the whole build link but couldn't see any problem. This 'undefined reference' error seems to be missing a file which I couldn't figure out\r\n\r\nAny suggestion or advise would be greatly appreciated!", "Update: I've figured out. This can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37431\">No</a>\n", "Ran into same problem. Turned out, for some reason *detection_responder.cc* was an empty file. Re-gen example project fixed it.\r\n```\r\n-rw-rw-r-- 1 mhuang mhuang       0 Aug 19 17:42 detection_responder.cc\r\n```"]}]