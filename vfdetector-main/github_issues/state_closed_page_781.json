[{"number": 30121, "title": "About `Evaluator` in TF_CONFIG", "body": "**System information**\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- TensorFlow installed from (source or binary): source\n- TensorFlow version (use command below): master\n- Python version: 2.7\n- Bazel version (if compiling from source):\n- GCC/Compiler version (if compiling from source):\n- CUDA/cuDNN version: \n- GPU model and memory:\n\n**Describe the current behavior**\nThe `evaluator` in `TF_CONFIG` makes me confused. In `RunConfig` document, I found the `evaluator` should not be in `cluster`. For example, \n```  \ncluster = {'chief': ['host0:2222'],\n             'ps': ['host1:2222', 'host2:2222'],\n             'worker': ['host3:2222', 'host4:2222', 'host5:2222']}\nos.environ['TF_CONFIG'] = json.dumps(\n      {'cluster': cluster,\n       'task': {'type': 'evaluator', 'index': 0}})\n```\nThis means the `evaluator` is not part of training cluster.  So it is not going to be in the `cluster_spec`.\nHowever, in `DistributionStrategy` there is a check to find `evaluator` node in `cluster_spec`. For example, in `tensorflow/python/distribute/multi_worker_util.py`, there is a function named `_validate_cluster_spec`. \n```\n  if task_type not in (\"chief\", \"worker\", \"evaluator\", \"ps\"):\n    raise ValueError(\n        \"Unrecognized task_type: %r, valid task types are: \\\"chief\\\", \"\n        \"\\\"worker\\\", \\\"evaluator\\\" and \\\"ps\\\".\" % task_type)\n\n  if task_type and task_type not in cluster_spec:\n    raise ValueError(\"`task_type` %r not found in cluster_spec.\" % task_type)\n\n```\nThat means if the `task_type` is `evaluator`, it should be in `cluster_spec`. This is inconsistent with what is stated in the document above.  So what should `TF_CONFIG` be like when I want to use `DistributionStrategy` in training and single mode in evaluation? I try many times but failed because of this check. So it is better to give an example about how to construct `TF_CONFIG`. Thanks very much.\n@yuefengz @anj-s \n", "comments": ["I'm facing the same issue. I failed to run distributed evaluation when setting `cluster_spec` like the document does. Now I'm using `train_and_evaluate` for my model. The `train_and_evaluate` document mentions that when using 'train_and_evaluate', the evaluator should not use `cluster_spec` but use `remote_cluster` instead. It does eliminate `ValueError`. However the model doesn't work as expected. I am curious whether `RunConfig` document made mistakes?", "I guess it is clearer to have evaluator job in the cluster spec if you want to run side-car evaluation. Does that work for Estimator + distribution strategy? ", "@yuefengz can you give an example of how they should setup their TF_CONFIG?", "Yes, it is better to give an example. Thanks. ", "Another example would look like:\r\n```\r\ncluster = {'chief': ['host0:2222'],\r\n                 'evaluator': ['host6:2222'],\r\n                 'ps': ['host1:2222', 'host2:2222'],\r\n                 'worker': ['host3:2222', 'host4:2222', 'host5:2222']}\r\n```", "> Another example would look like:\r\n> \r\n> ```\r\n> cluster = {'chief': ['host0:2222'],\r\n>                  'evaluator': ['host6:2222'],\r\n>                  'ps': ['host1:2222', 'host2:2222'],\r\n>                  'worker': ['host3:2222', 'host4:2222', 'host5:2222']}\r\n> ```\r\n\r\nThanks for your reply.", "@yuefengz \r\nif evaluator in cluster, chief worker will wait evaluator start session like (CreateSession still waiting for response from worker: /job:evaluator/replica:0/task:0)\r\nBut evaluator wait chief's checkpoint file, can't start like(Waiting 100.000000 secs before starting eval, Estimator is not trained yet. Will start an evaluation when a checkpoint is ready.).\r\nMay I miss something ? how to solve it? Thanks.", "> @yuefengz\r\n> if evaluator in cluster, chief worker will wait evaluator start session like (CreateSession still waiting for response from worker: /job:evaluator/replica:0/task:0)\r\n> But evaluator wait chief's checkpoint file, can't start like(Waiting 100.000000 secs before starting eval, Estimator is not trained yet. Will start an evaluation when a checkpoint is ready.).\r\n> May I miss something ? how to solve it? Thanks.\r\n\r\nI'm facing exactly the same issue. Any hints/answers?", "@wangsiyu @liyi193328 @sahiltyagi4 I guess all of you have explicitly offered the `session_config` parameter when constructing a `tf.estimator.RunConfig`.\r\nAccording to source of [run_config.py#L589](https://github.com/tensorflow/estimator/blob/r1.14/tensorflow_estimator/python/estimator/run_config.py#L589), a `tf.estimator.Estimator` will automatically add device filters to the created `tf.ConfigProto` in distributed training when the field `session_config` is unset.\r\n```python\r\nif self._task_type == TaskType.MASTER:\r\n  device_filters = ['/job:ps', '/job:master']\r\nelif self._task_type == TaskType.CHIEF:\r\n  device_filters = ['/job:ps', '/job:chief']\r\nelif self._task_type == TaskType.WORKER:\r\n  device_filters = ['/job:ps', '/job:worker/task:%d' % self._task_id]\r\nelif self._task_type == TaskType.PS:\r\n  device_filters = ['/job:ps', '/job:worker', '/job:chief', '/job:master']\r\n```\r\nThus, the solution to prevent unnecessary sync between all intances is easy. Just provide a correct device filter to your declared `tf.ConfigProto`.", "@shishaochen Got it, Thanks a lot, Nice to you", "hi~  do you solve the problem? I encountered the same problem with you\uff0cbut I dont understand how to set tf.ConfigProto.  Can you give me a example\uff1f\uff1f Thanks", "Agreed with @lebinlebin, the solution is unclear. Can someone explictly provide an example of:\r\n\r\n1. The TF_CONFIG set on the worker nodes. \r\n2. The TF_CONFIG set on the evaluate node. \r\n3. The full tf.estimator.RunConfig used.", "@yuefengz  please take a look at TF_CONFIG env issue in kubeflow/tf-operator", "> @yuefengz please take a look at TF_CONFIG env issue in kubeflow/tf-operator\r\n\r\nHI, @yuefengz , any update about this issue? https://github.com/kubeflow/tf-operator/issues/1139", "hi, I am using the elder version of tf-operator, and I have the same issue. Has it been fixed yet?", "> @shishaochen Got it, Thanks a lot, Nice to you\r\n\r\nHey, bro\uff0cdid you have a solution there\uff1f I met the same issue with version 1.14. And there is another confusing issue is that my chief node has no variable file just meta-graph files. The evaluator also told me \"Estimator not trained yet\". Thx", "Hi There,\r\n\r\nWe are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.we will get you the right help.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30121\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30121\">No</a>\n"]}, {"number": 30120, "title": "Attempt to convert a value (1.0) with an unsupported type (<class 'numpy.float32'>) to a Tensor.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo, I have used the example script from [https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor](url)\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip install (binary)\r\n- TensorFlow version (use command below):\r\n2.0.0-beta1\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n10.0\r\n- GPU model and memory:\r\nGeForce GTX 1050, 4GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI was trying to convert a numpy array to a tensorflow tensor as in the example code.\r\n**Describe the expected behavior**\r\nShould give back tensor (as it works properly in earlier versions)\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n`import numpy as np\r\n\r\ndef my_func(arg):\r\n  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\r\n  return tf.matmul(arg, arg) + arg\r\n\r\n#The following calls are equivalent.\r\n`import numpy as np\r\n\r\ndef my_func(arg):\r\n  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\r\n  return tf.matmul(arg, arg) + arg\r\n\r\n#The following calls are equivalent.\r\nvalue_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]])) \r\nvalue_2 = my_func([[1.0, 2.0], [3.0, 4.0]]) \r\nvalue_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)) `\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n`---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-f167d18ea32e> in <module>\r\n      9 value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))\r\n     10 value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])\r\n---> 11 value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))\r\n\r\n<ipython-input-1-f167d18ea32e> in my_func(arg)\r\n      3 \r\n      4 def my_func(arg):\r\n----> 5   arg = tf.convert_to_tensor(arg, dtype=tf.float32)\r\n      6   return tf.matmul(arg, arg) + arg\r\n      7 \r\n\r\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n   1156       name=name,\r\n   1157       preferred_dtype=dtype_hint,\r\n-> 1158       as_ref=False)\r\n   1159 \r\n   1160 \r\n\r\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)\r\n   1235 \r\n   1236     if ret is None:\r\n-> 1237       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1238 \r\n   1239     if ret is NotImplemented:\r\n\r\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    303                                          as_ref=False):\r\n    304   _ = as_ref\r\n--> 305   return constant(v, dtype=dtype, name=name)\r\n    306 \r\n    307 \r\n\r\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in constant(value, dtype, shape, name)\r\n    244   \"\"\"\r\n    245   return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n--> 246                         allow_broadcast=True)\r\n    247 \r\n    248 \r\n\r\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)\r\n    252   ctx = context.context()\r\n    253   if ctx.executing_eagerly():\r\n--> 254     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    255     if shape is None:\r\n    256       return t\r\n\r\n~\\Anaconda3\\envs\\Tensorflow_2_0_Beta\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    113     return t\r\n    114   else:\r\n--> 115     return ops.EagerTensor(value, handle, device, dtype)\r\n    116 \r\n    117 \r\n\r\nValueError: Attempt to convert a value (1.0) with an unsupported type (<class 'numpy.float32'>) to a Tensor.\r\n`", "comments": ["I have tried the code snippet in Colab with TF GPU version 2.0beta1 and it executed without error.  Please help us to reproduce the issue. Thanks!", "Thanks for the reply, I have re-installed the tensorflow 2.0beta1, and now the problem is not present anymore. \r\nDon't know the reasons why, sorry.", "Great to know the issue is resolved. Thanks!", "Hi, \r\n\r\nI also came across this error.\r\n\r\nI believe there is an **issue with compatibility with latest version of numpy (1.17.3)**\r\n- **SOLUTION** :  install previous version of numpy    1.16.4\r\n\r\n- tensorlfow version : 2.1.0-dev20191023 \r\n- numpy:  1.17.3 (install the previous version 1.16.4)\r\n-  python: 3.7\r\n\r\n", "Installing numpy 1.16.4 worked for me. Thanks!", "reverting to numpy 1.16.4 fixes the problem for me, thanks."]}, {"number": 30119, "title": "Saver's restore method uses deprecated checkpoint_exists method.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, I have written custom Python code.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nmacOS Mojave 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nBinary using pip\r\n- TensorFlow version (use command below):\r\n('v1.9.0-0-g25c197e023', '1.9.0')\r\n- Python version:\r\nPython 3.6.5\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\nNA\r\n- CUDA/cuDNN version:\r\nNA\r\n- GPU model and memory:\r\nIntel Iris Pro 1536 MB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI try to restore previously saved checkpoint variables and I get a method deprecated error.\r\n\r\n**Describe the expected behavior**\r\nI could restore checkpoints.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\ngraph_path = os.path.join('plant_disease_classification/ckpts/', 'plants-disease-model.meta')\r\ncheckpoint_path = os.path.join('plant_disease_classification/ckpts/', 'plants-disease-model')\r\n\r\nsession = tf.compat.v1.Session()\r\nsaver = tf.compat.v1.train.import_meta_graph(graph_path)\r\n# get error on line below\r\nsaver.restore(session, checkpoint_path)\r\n# and try this as well same depracation error\r\nsaver.restore(session, tf.train.latest_checkpoint('plant_disease_classification/ckpts/'))\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0625 14:28:02.910645 4579825088 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n```", "comments": ["Just to verify whether you are able to restore previously saved checkpoint variables .Deprecation warning gives information about the function which will not be available in future version and it makes the user aware of this information.", "Sadly I can't restore my previously saved checkpoints.", "That's not an error, it's just a warning. It won't stop program execution.", "@abdullahselek \r\nCan we close the issue since this is not a bug. Let us know. Thanks!", "@ravikyram yes we can close, I'm closing right now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30119\">No</a>\n"]}, {"number": 30118, "title": "Op type not registered 'LeakyRelu' in binary running on some PC.", "body": "**Tensorflow version:**\r\nDue to some problems according to the production environment, I only can use the tensorflow 1.9 or 1.10.\r\n\r\n**Problem description**\r\nI'm trying to deploy the yolov3 trained model through the c++ api. The trained model are coming from these two repositories, [`tensorflow-yolov3`](https://github.com/YunYang1994/tensorflow-yolov3) and  [`keras-yolo3`](https://github.com/qqwweee/keras-yolo3). \r\nHowever, when I was creating the session by using the graph definition, the session create status report that \"Op type not registered 'LeakyRelu' in binary running on my pc\".\r\n`    auto session = std::unique_ptr<tensorflow::Session>();\r\n    session.reset(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n    Status session_create_status = session->Create(graph_def);\r\n    if (!session_create_status.ok())\r\n    {\r\n        throw std::runtime_error(session_create_status.error_message());\r\n    }`\r\n\r\n**Operating System Information**\r\nI test my code on both ubuntu 18.04 and Windows10. The error is the same.\r\n**Some research**\r\nI tried to use the tensorflow 1.9 and tensorflow 1.10 at the same time, but the problem is the same. I read the tensorflow api and I found that the tf.nn module has the leaky relu operation but the c++ api does not.\r\nIs there any possibility to add the support of leaky relu throught c++ api on the old version of tensorflow?", "comments": ["@EternalSaga Can you try recent TF 1.14.0 version and let us know how it progresses. Also, please fill the issue template [here](https://github.com/tensorflow/tensorflow/issues/new/choose). thanks!", "> \r\n> \r\n> @EternalSaga Can you try recent TF 1.14.0 version and let us know how it progresses. Also, please fill the issue template [here](https://github.com/tensorflow/tensorflow/issues/new/choose). thanks!\r\n\r\nThank you for your reply. I compiled the TF 1.13.1 successfully (I don't want to upgrade the bazel to compile 1.14.0). However, I cannot organize the header files successfully. In fact, the header files for TF 1.9 and 1.10 come from this repository [tensorflow-windows-wheel](https://github.com/fo40225/tensorflow-windows-wheel). This repo does not provide the orgnized headers for TF which versions after 1.10. Aditionally, I cannot find any official guide for using tensorflow_cc.so and it's c++ api as a standalone library for other c++ project without using bazel.", "I got the same issue and update 1.14.0 solved this\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-1.14.0.zip\r\nhttps://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.14.0.zip", "@hksonngan Thanks for providing the resolution.\r\n\r\nI am closing this issue. Please feel free to reopen if the issue persists again. Thanks!"]}, {"number": 30117, "title": "Makefile build broken on macOS", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.14.5 (18F203)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 8e423e3d56390671f0d954c90f4fd163ab02a9c1\r\n- Python version: 2.7 (but not building pip package)\r\n- Bazel version (if compiling from source): not used\r\n- GCC/Compiler version (if compiling from source):\r\n```\r\nApple LLVM version 10.0.1 (clang-1001.0.46.4)\r\nTarget: x86_64-apple-darwin18.6.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\n**Describe the problem**\r\n\r\nI am followin https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile on macOS Mojave.\r\n\r\nRunning\r\n```\r\nbash tensorflow/contrib/makefile/build_all_linux.sh\r\n```\r\nterminates with the error\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"_CFRelease\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFStringGetCString\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFStringGetLength\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFStringGetMaximumSizeForEncoding\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFTimeZoneCopyDefault\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFTimeZoneGetName\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\n```\r\n\r\nMay this come from not having XCode installed? I assume the (seemingly missing) Core Foundation libraries are shipped with macOS regardless. But it is listed as a dependency for iOS, so maybe macOS should be mentioned thereabouts as well?\r\n\r\nUpdate: The makefile seems to link the CoreFoundation framework only for iOS, the comment notwithstanding.\r\nhttps://github.com/tensorflow/tensorflow/blob/2f4121b1a7266aef4007457aa348c5f7f01eb539/tensorflow/contrib/makefile/Makefile#L912\r\n\r\nI added\r\n```\r\nifeq ($(TARGET),OSX)\r\n    HOST_LDOPTS += -framework CoreFoundation\r\nendif\r\n```\r\nand the compilation proceeds at least (can't say whether it will succeed yet).", "comments": ["The proposed solution alone did not help. I _instead_ modified the block beginning in line 242 of the Makefile to always pass `-framework CoreFoundation` to the compiler options.\r\n```\r\n# If we're on OS X, make sure that globals aren't stripped out.\r\nifeq ($(TARGET),OSX)\r\nifeq ($(HAS_GEN_HOST_PROTOC),true)\r\n\tLIBFLAGS += -L$(MAKEFILE_DIR)/gen/protobuf-host/lib\r\n\texport LD_LIBRARY_PATH=$(MAKEFILE_DIR)/gen/protobuf-host/lib\r\nendif\r\n\tLDFLAGS += -all_load\r\n\tCXXFLAGS += -framework CoreFoundation   #  <--- here \r\nendif\r\n```\r\n\r\nThis yields a lot of warnings about unused linker args, but ultimately seems to work (I'm not quite sure if I had to add the initial `HOST_LDOPTS` modifications as well).", "Makefile build is no longer supported, it's povided on a best-effort basis.\r\n\r\nI currently don't have access to a Mac machine to try and fix this, unfortunately", "I think I have the fix ready, I can hopefully submit a PR.", "That would be awesome. Please assign me as reviewer to PR", "See https://github.com/tensorflow/tensorflow/pull/30214", "Looks like https://github.com/tensorflow/tensorflow/pull/30279 is already merged. closing.", "Still having this problem. i am build static library for android using MacOS system.\r\n```\r\nUndefined symbols for architecture x86_64:\r\n  \"_CFRelease\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFStringGetCString\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFStringGetLength\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFStringGetMaximumSizeForEncoding\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFTimeZoneCopyDefault\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\n  \"_CFTimeZoneGetName\", referenced from:\r\n      absl::time_internal::cctz::local_time_zone() in time_zone_lookup.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [/Users/granjur/Documents/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\n```\r\ni have tried building with both command\r\n`tensorflow/contrib/makefile/build_all_android.sh`\r\nand \r\n`make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID`\r\nbut same result", "I think I also had the problem after the merge, not quite sure though. It could be that `CoreFoundation` gets linked into the library alright, but not the benchmark app that gets built as well.", "so how to solve this?\r\ni only need to build library, not the benchmark app", "If you just need it once, you can hardcode `-framework CoreFoundation` to the makefile targets `$(LIB_PATH)` and `$(BENCHMARK_NAME)`.\r\n\r\nBut if that works with android, I have no idea. I only tested macos and linux builds.", "thanks for the hint, can you please direct me exactly where (what line number) i should add this.\r\nNote: I am using tensorflow v1.14.0 "]}, {"number": 30116, "title": "Error in training ssd_mobilenet_v1_pets.config for my own dataset", "body": "Hi,\r\nIam getting this error when i run the command-  python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n\r\n\r\n/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/summary/writer/writer.py:386: UserWarning: Attempting to use a closed FileWriter. The operation will be a noop unless the FileWriter is explicitly reopened.\r\n  warnings.warn(\"Attempting to use a closed FileWriter. \"\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 184, in <module>\r\n    tf.app.run()\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"train.py\", line 180, in main\r\n    graph_hook_fn=graph_rewriter_fn)\r\n  File \"/opt/tensorflow/models/research/object_detection/legacy/trainer.py\", line 416, in train\r\n    saver=saver)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 790, in train\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\", line 839, in stop\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 257, in _run\r\n    enqueue_callable()\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1279, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/home/shilpa.j/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: image_size must contain 3 elements[4]\r\n         [[{{node cond_1/RandomCropImage/sample_distorted_bounding_box/SampleDistortedBoundingBoxV2}}]]\r\nshilpa.j@Theorem-TrainUP:/opt/tensorflow/models/research/object_detection$\r\n", "comments": ["`tensorflow.python.framework.errors_impl.InvalidArgumentError: image_size must contain 3 elements[4]`\r\n\r\nI think you should review your train CSV files CSV files to see if there are any entries with Width and Height as 0 or missing some value.\r\n", "\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nIf you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nCan you please provide full code snippet to reproduce the issue \r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "> \r\n> \r\n> `tensorflow.python.framework.errors_impl.InvalidArgumentError: image_size must contain 3 elements[4]`\r\n> \r\n> I think you should review your train CSV files CSV files to see if there are any entries with Width and Height as 0 or missing some value.\r\n\r\nThanks @Arritmic .Yes this has resolved my problem.", "This is my system config -\r\n\r\nSystem information\r\n\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 18.04): Ubuntu\r\n    TensorFlow installed from (source or binary):\r\n    TensorFlow version: 1.14.0\r\n    Python version: 3.5.0\r\n    Installed using virtualenv? pip? conda?: virtualenv\r\n    GCC/Compiler version (if compiling from source):\r\n    CUDA/cuDNN version: Using Linux server 8GB RAM with 4 cored on windows machine\r\n    Windows OS config:\r\n \tprocessor:i3 CPU@3.40GHz \r\n\tRAM:4GB\r\n\tSystem type:64 bit\r\n      GPU model and memory: N/A\r\n I solved it by reviewing csv data for train set and test set for zeros in width column and height column.\r\nThank you all"]}, {"number": 30115, "title": "Tensorboard embeddings mnist example crashes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution Windows 10 Pro 64-bit\r\n- TensorFlow installed from binary\r\n- TensorFlow version: b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 10.0/7.6.0\r\n- GPU model and memory: GeForce GTX 1070 8Mb\r\n\r\n**Describe the current behavior**\r\nWhen attempting to run the following example, https://keras.io/examples/tensorboard_embeddings_mnist/, I get a crash after the first epoch.\r\n\r\nI get the following error message:\r\n\r\n```\r\n2019-06-25 11:33:16.177546: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at strided_slice_op.cc:308 : Not found: Resource localhost/features_embedding/class tensorflow::Var does not exist.\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/features_embedding/class tensorflow::Var does not exist.\r\n\t [[{{node strided_slice/_assign}}]]\r\n\t [[{{node ReadVariableOp_1}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py\", line 84, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 370, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs, mode=mode)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 251, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1204, in on_epoch_end\r\n    K.sess.run(self.assign_embeddings, feed_dict=feed_dict)\r\nAttributeError: module 'tensorflow.python.keras.backend' has no attribute 'sess'\r\n```\r\n\r\nThis was fixed using the following change set tensorflow/tensorflow@d994300\r\n\r\nI now get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py\", line 84, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 370, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs, mode=mode)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 251, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1204, in on_epoch_end\r\n    sess.run(self.assign_embeddings, feed_dict=feed_dict)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/features_embedding/class tensorflow::Var does not exist.\r\n\t [[node strided_slice/_assign (defined at X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py:84) ]]\r\n\t [[node ReadVariableOp_1 (defined at X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py:84) ]]\r\n\r\nCaused by op 'strided_slice/_assign', defined at:\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1664, in <module>\r\n    main()\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1658, in main\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\pydevd.py\", line 1068, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py\", line 84, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 880, in fit\r\n    validation_steps=validation_steps)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\", line 215, in model_iteration\r\n    mode=mode)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 106, in configure_callbacks\r\n    callback_list.set_model(callback_model)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 178, in set_model\r\n    callback.set_model(model)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1051, in set_model\r\n    embedding_input)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 224, in assign\r\n    return ref.assign(value, name=name)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 844, in assign\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1163, in _strided_slice_assign\r\n    shrink_axis_mask=shrink_axis_mask))\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 8576, in resource_strided_slice_assign\r\n    name=name)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\ProgramData\\Anaconda2\\envs\\py36new\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nNotFoundError (see above for traceback): Resource localhost/features_embedding/class tensorflow::Var does not exist.\r\n\t [[node strided_slice/_assign (defined at X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py:84) ]]\r\n\t [[node ReadVariableOp_1 (defined at X:/CoreTech/LTI791_FaceEncoding/Research/Python/Faces/Encoding/Scripts/MNISTExample.py:84) ]]\r\n```\r\n\r\n**Describe the expected behavior**\r\nMy aim is to find a working example that allows me to visualize weights/gradients etc in tensorboard. I would expect this example to run multiple epochs saving the embedding layer data to a log file for each epoch so I can visualize it using tensorboard. \r\n\r\n**Code to reproduce the issue**\r\nhttps://keras.io/examples/tensorboard_embeddings_mnist/\r\n\r\nplus the following fix:\r\ntensorflow/tensorflow@d994300\r\n\r\n**Other info / logs**\r\nThe example works fine if embeddings_freq is set to 0.\r\n", "comments": ["cc @omalleyt12 do you have any idea what might be going awry here?", "Same in tf 1.14\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/features_embedding/N10tensorflow3VarE does not exist.\r\n\t [[{{node strided_slice/_assign}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ben/.PyCharmCE2019.2/config/scratches/scratch_2.py\", line 82, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 419, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 311, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/callbacks_v1.py\", line 446, in on_epoch_end\r\n    sess.run(self.assign_embeddings, feed_dict=feed_dict)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/home/ben/venvs/base/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/features_embedding/N10tensorflow3VarE does not exist.\r\n\t [[node strided_slice/_assign (defined at /.PyCharmCE2019.2/config/scratches/scratch_2.py:82) ]]\r\n\r\nOriginal stack trace for 'strided_slice/_assign':\r\n  File \"/.PyCharmCE2019.2/config/scratches/scratch_2.py\", line 82, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 780, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\", line 213, in model_iteration\r\n    mode=mode)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 105, in configure_callbacks\r\n    callback_list.set_model(callback_model)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py\", line 231, in set_model\r\n    callback.set_model(model)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/keras/callbacks_v1.py\", line 277, in set_model\r\n    embedding_input)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 228, in assign\r\n    return ref.assign(value, name=name)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 885, in assign\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1446, in _strided_slice_assign\r\n    shrink_axis_mask=shrink_axis_mask))\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 7829, in resource_strided_slice_assign\r\n    name=name)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\r\n    op_def=op_def)\r\n  File \"/venvs/base/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n```python\r\n\r\nfrom __future__ import print_function\r\n\r\nfrom os import makedirs\r\nfrom os.path import exists, join\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.callbacks import TensorBoard\r\nfrom tensorflow.keras.datasets import mnist\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\r\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\r\nfrom tensorflow.keras import backend as K\r\n\r\nimport numpy as np\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\nlog_dir = './logs'\r\n\r\nif not exists(log_dir):\r\n    makedirs(log_dir)\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# save class labels to disk to color data points in TensorBoard accordingly\r\nwith open(join(log_dir, 'metadata.tsv'), 'w') as f:\r\n    np.savetxt(f, y_test)\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\ntensorboard = TensorBoard(batch_size=batch_size,\r\n                          embeddings_freq=1,\r\n                          embeddings_layer_names=['features'],\r\n                          embeddings_metadata='metadata.tsv',\r\n                          embeddings_data=x_test)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu', name='features'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          callbacks=[tensorboard],\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n```\r\n", "Any updates or timeline for fix on this issue? ", "I'm experiencing the same problem. The Keras-only version works fine. However, the tensorflow.keras version does not. I've tensorflow versions 1.14.0 and 1.15.0 and they have the same problem.\r\nYour help is greatly appreciated.", "I have the same problem. Any solution?", "No fix.\nNeeds to be fixed in Tensorboard.\n\nA Hui Hou,\nGreg Martin\ngregory.james.martin@gmail.com\n\n\n\n\n> On Mar 20, 2020, at 5:05 PM, suziW <notifications@github.com> wrote:\n> \n> \n> I have the same problem. Any solution?\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/30115#issuecomment-601984367>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAIG7HRA3E3X6MUP27F5F6LRIQVGZANCNFSM4H3HBCKA>.\n> \n\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "I upgraded to TensorFlow 2.2.\nNo more issues.\nThanks!\n\nA Hui Hou,\nGreg Martin\ngregory.james.martin@gmail.com\n\n\n\n\n> On Feb 1, 2021, at 4:03 AM, Alfred Sorten Wolf <notifications@github.com> wrote:\n> \n> \n> Hi There,\n> \n> We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help.\n> \n> This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/30115#issuecomment-770879688>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAIG7HWQ5GZWPI2YHTYLOGTS42YEZANCNFSM4H3HBCKA>.\n> \n\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30115\">No</a>\n"]}, {"number": 30114, "title": "Illegal instruction (core dumped)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04.02):\r\n- TensorFlow installed from (source or binary):\r\n\r\npip install --upgrade tensorflow==2.0.0-beta1\r\n\r\n- Python version:\r\npython3 --version\r\nPython 3.6.8\r\n\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\npython -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\r\nIllegal instruction (core dumped)\r\n\r\n", "comments": ["I have tried on Colab with TF CPU version 2.0beta1 and was able to execute it successfully. Can you please help us to reproduce the issue if we are missing out something. Thanks!", "@Elmit2015 what's the output of `cat /proc/cpuinfo`?", "```\r\ncat /proc/cpuinfo\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Atom(TM) x5-Z8350  CPU @ 1.44GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x411\r\ncpu MHz\t\t: 659.590\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 4\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat md_clear\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 mds\r\nbogomips\t: 2880.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Atom(TM) x5-Z8350  CPU @ 1.44GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x411\r\ncpu MHz\t\t: 711.097\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 1\r\ncpu cores\t: 4\r\napicid\t\t: 2\r\ninitial apicid\t: 2\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat md_clear\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 mds\r\nbogomips\t: 2880.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 2\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Atom(TM) x5-Z8350  CPU @ 1.44GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x411\r\ncpu MHz\t\t: 1256.603\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 2\r\ncpu cores\t: 4\r\napicid\t\t: 4\r\ninitial apicid\t: 4\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat md_clear\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 mds\r\nbogomips\t: 2880.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 3\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Atom(TM) x5-Z8350  CPU @ 1.44GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x411\r\ncpu MHz\t\t: 1796.532\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 3\r\ncpu cores\t: 4\r\napicid\t\t: 6\r\ninitial apicid\t: 6\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat md_clear\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 mds\r\nbogomips\t: 2880.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\n\r\n```", "@Elmit2015 That's expected. Your CPU does not support the AVX instruction set, which is enabled by default since v1.6. To use TF on your laptop, either downgrade to TF 1.5, or build TF from source.", "Thanks for your help.\r\nHow do I uninstall 2.0 and install 1.5 instead?\r\nI am not going to compile on that mini notebook from source.", "Just do `pip install tensorflow-gpu==1.5.0`.\r\n\r\nIf you are not using a GPU, change to `pip install tensorflow==1.5.0`.", "@Elmit2015 : Let us know if you are still stuck. Thanks!", "I tried:\r\n```\r\npip install tensorflow==1.5.0\r\nCollecting tensorflow==1.5.0\r\n  ERROR: Could not find a version that satisfies the requirement tensorflow==1.5.0 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\r\nERROR: No matching distribution found for tensorflow==1.5.0\r\n```\r\n\r\n", "That probably because your Python version is too high. Python 3.7 support was added not long time ago. Could you try using Python 2?", "@Elmit2015 Is this still an issue? Thanks!", "I give up to install Tensor flow on that litle notebook. I will use colab instead if needed. I have successfully installed Tensor flow 2.0.0beta1 on a bigger notebook.", "> @Elmit2015 That's expected. Your CPU does not support the AVX instruction set, which is enabled by default since v1.6. To use TF on your laptop, either downgrade to TF 1.5, or build TF from source.\r\n\r\nI have the same problem, how can I know if my CPU supports AVX or not?\r\nThis is the output of cat /proc/cpuinfo\r\n```\r\ncat /proc/cpuinfo \r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Xeon(R) CPU           W3520  @ 2.67GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x1d\r\ncpu MHz\t\t: 2490.495\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 4\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds\r\nbogomips\t: 5333.60\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Xeon(R) CPU           W3520  @ 2.67GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x1d\r\ncpu MHz\t\t: 2628.877\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 1\r\ncpu cores\t: 4\r\napicid\t\t: 2\r\ninitial apicid\t: 2\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds\r\nbogomips\t: 5333.60\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 2\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Xeon(R) CPU           W3520  @ 2.67GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x1d\r\ncpu MHz\t\t: 2225.469\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 2\r\ncpu cores\t: 4\r\napicid\t\t: 4\r\ninitial apicid\t: 4\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds\r\nbogomips\t: 5333.60\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 3\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 26\r\nmodel name\t: Intel(R) Xeon(R) CPU           W3520  @ 2.67GHz\r\nstepping\t: 5\r\nmicrocode\t: 0x1d\r\ncpu MHz\t\t: 2599.159\r\ncache size\t: 8192 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 3\r\ncpu cores\t: 4\r\napicid\t\t: 6\r\ninitial apicid\t: 6\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds\r\nbogomips\t: 5333.60\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\n```", "@Kati07, Please refer this [link](https://superuser.com/questions/1251865/is-there-a-way-to-tell-if-my-hardware-supports-specific-instructions) to know the CPU instructions set. ", "Hi! I'm having the same problem with Tensorflow 2.0 (both cpu and gpu versions).\r\nFrom what I gather here, my cpu would need AVX instructions and in fact they don't show in my cpuinfo. \r\nHowever, on the same machine I can smoothly run Tensorflow 1.14 (both cpu and gpu). Any idea on what's going on? Is AVX actually the problem?", "@davidc9320sg,\r\nCan you please post a new issue by providing the information asked by the [template](https://github.com/tensorflow/tensorflow/issues/new/choose)?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "This is happening to me also but my tensorflow version is 1.14.\r\nPython 3.6\r\n\r\ncat /proc/cpuinfo\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 23\r\nmodel name\t: Intel(R) Core(TM)2 Duo CPU     E7500  @ 2.93GHz\r\nstepping\t: 10\r\nmicrocode\t: 0xa0b\r\ncpu MHz\t\t: 2546.724\r\ncache size\t: 3072 KB\r\nphysical id\t: 0\r\nsiblings\t: 2\r\ncore id\t\t: 0\r\ncpu cores\t: 2\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 5850.33\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 23\r\nmodel name\t: Intel(R) Core(TM)2 Duo CPU     E7500  @ 2.93GHz\r\nstepping\t: 10\r\nmicrocode\t: 0xa0b\r\ncpu MHz\t\t: 2231.318\r\ncache size\t: 3072 KB\r\nphysical id\t: 0\r\nsiblings\t: 2\r\ncore id\t\t: 1\r\ncpu cores\t: 2\r\napicid\t\t: 1\r\ninitial apicid\t: 1\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 13\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good nopl cpuid aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm pti tpr_shadow vnmi flexpriority dtherm\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 5850.33\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nPlease solve my problem.", "No AVX support, please check #19584 or build from source", "Same problem:\r\n\r\ncat /proc/cpuinfo\r\nprocessor\t: 0\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Pentium(R) CPU  N3710  @ 1.60GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x40e\r\ncpu MHz\t\t: 1568.756\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 0\r\ncpu cores\t: 4\r\napicid\t\t: 0\r\ninitial apicid\t: 0\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 3200.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 1\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Pentium(R) CPU  N3710  @ 1.60GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x40e\r\ncpu MHz\t\t: 1473.475\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 1\r\ncpu cores\t: 4\r\napicid\t\t: 2\r\ninitial apicid\t: 2\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 3200.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 2\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Pentium(R) CPU  N3710  @ 1.60GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x40e\r\ncpu MHz\t\t: 586.230\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 2\r\ncpu cores\t: 4\r\napicid\t\t: 4\r\ninitial apicid\t: 4\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 3200.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n\r\nprocessor\t: 3\r\nvendor_id\t: GenuineIntel\r\ncpu family\t: 6\r\nmodel\t\t: 76\r\nmodel name\t: Intel(R) Pentium(R) CPU  N3710  @ 1.60GHz\r\nstepping\t: 4\r\nmicrocode\t: 0x40e\r\ncpu MHz\t\t: 608.606\r\ncache size\t: 1024 KB\r\nphysical id\t: 0\r\nsiblings\t: 4\r\ncore id\t\t: 3\r\ncpu cores\t: 4\r\napicid\t\t: 6\r\ninitial apicid\t: 6\r\nfpu\t\t: yes\r\nfpu_exception\t: yes\r\ncpuid level\t: 11\r\nwp\t\t: yes\r\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 movbe popcnt tsc_deadline_timer aes rdrand lahf_lm 3dnowprefetch epb pti tpr_shadow vnmi flexpriority ept vpid tsc_adjust smep erms dtherm ida arat\r\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2\r\nbogomips\t: 3200.00\r\nclflush size\t: 64\r\ncache_alignment\t: 64\r\naddress sizes\t: 36 bits physical, 48 bits virtual\r\npower management:\r\n", "hey dude...\r\nproblem with Illegal instruction (core dumped)...?\r\n\r\ni solve with \"conda install -c conda-forge tensorflow\"", "Check this repo for more unofficial wheels: https://github.com/yaroslavvb/tensorflow-community-wheels/issues\r\nI found the right one for our server. YAY!"]}, {"number": 30113, "title": "tf.image.encode_png doesn't support 16 bit and inconsistent behavior in eager mode", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tested with 1.9.0, 1.12.0 and 1.14.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nCreating a numpy array with uint16 datatype and passing it to tf.image.encode_png() yields different results in eager execution mode. The first time the array is passed it somehow gets transformed to a uint8 array and for the following encodings it works as expected. \r\n\r\nUsing a tf.session the uint16 input is always transformed to uint8\r\n\r\n**Describe the expected behavior**\r\n\r\nJust return a bytestring of a 16bit PNG\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n#tf.enable_eager_execution()\r\n\r\nnp.random.seed(1)\r\nA = np.random.randint(low=0, high=65535, size=100, dtype=np.uint16).reshape(10,10,1)\r\nB = A.copy()\r\nnp.allclose(A,B) # is true\r\n\r\n# Eager Execution\r\na_encoded = tf.image.encode_png(A).numpy()\r\nb_encoded = tf.image.encode_png(B).numpy()\r\n\r\nprint(len(a_encoded),len(b_encoded)) # prints 178 and 278, 278 expected both times\r\nassert(a_encoded == b_encoded) # Fails\r\n\r\n# Session Mode\r\nencode_a = tf.image.encode_png(A)\r\nencode_b = tf.image.encode_png(B)\r\n\r\nwith tf.Session() as sess:\r\n    a_encoded = sess.run(encode_a)\r\n    b_encoded = sess.run(encode_b)\r\n\r\nprint(len(a_encoded),len(b_encoded)) # prints 178 and 178 but 278 expected \r\nassert(a_encoded == b_encoded) # True\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I am able to reproduce the issue with eager execution using TF 1.12 &TF 1.9 but in session i am getting the below error.\r\nRuntimeError: The Session graph is empty. Add operations to the graph before calling run().\r\n\r\nBut in TF1.14 i am able to reproduce the issue with session mode but i am getting below error with eager execution.\r\nValueError: tf.enable_eager_execution must be called at program startup.", "Apologies for the delay in response. I get following results using TF 1.14. Can you please confirm?Thanks!\r\nEager Mode \r\n``` 278, 278```\r\nSession Mode\r\n```178, 178```", "Hi, thanks for having a look at it.\r\n\r\nThe PNG header of the encoded image should be something around 60-70 bytes from looking at the outputs.\r\nSo for the 10x10px uint8 image the expected byte length is ~ 100+70 and for the uint16 it should be around 2*100+70.\r\n\r\nWith a fresh install of TF 1.14.0 and Python 3.6 I've got\r\n\r\n**Eager Mode**\r\n`178`,`278 `\r\n**Session Mode**\r\n`178``,``178`\r\n\r\nBut it should be **278 always**, in both modes. So in session mode the encoding doesn't work and in eager mode it works after the first encoding which is currently my workaround.\r\n\r\n**Note:** I executed it in a notebook and after a kernel restart I get the 178, 278 result for eager execution and if I execute the cell again, it will be 278, 278. So I guess there is some kind of initialization going on that sets the correct datatype after the first faulty execution of `tf.image.encode`.\r\n", "It seems the problem exists also in TF 2.0 (if the usage hasn't changed).\r\n\r\n```python \r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>>\r\n>>> print(tf.__version__)\r\n2.0.0-beta1\r\n>>>\r\n>>> np.random.seed(1)\r\n>>> A = np.random.randint(low=0, high=65535, size=100, dtype=np.uint16).reshape(10,10,1)\r\n>>> B = A.copy()\r\n>>> np.allclose(A,B) # is true\r\nTrue\r\n>>>\r\n>>> a_encoded = tf.image.encode_png(A).numpy()\r\n>>> b_encoded = tf.image.encode_png(B).numpy()\r\n>>>\r\n>>> print(len(a_encoded),len(b_encoded)) # prints 178 and 278, 278 expected both times\r\n178 278\r\n```", "I could reproduce the issue with 2.0 and master, and could see the issue lies in the generated code of `encode_png_eager_fallback` (and to `args_to_matching_eager`).\r\n\r\nThe issue is that in `encode_png_eager_fallback`, eager tensor for input image is created with \"preferred_dtype\" passed as uint8 (since `EncodePng`'s defined `T` type is UINT8).\r\n\r\nHowever, input image itself is a numpy array with dtype of uint16. So, the second time it runs, input image is converted to a tensor as uint16. The reason is that the second time `encode_png_eager_fallback` is not called, so `T` type of UINT8 is not used implicitly.\r\n\r\nI think the behavior may need to be fixed one way or another, though I it might also break many other places if the fix is not careful.\r\n\r\nNot sure the best way to get around it, though @53RT if you convert input numpy array to tensor first then the example will run correctly.\r\n\r\n@tensorflow/api-owners ", "@akshaym do you understand what is happening here?", "Sorry for the delayed response. \r\n\r\nEager internally has 2 paths (and you've found a place where they don't agree - a discrepancy I will remove soon). \r\n\r\nUnfortunately, this means removing the behavior of being 278 is unfortunately going to go away in eager, and all the above cases will return 178.\r\n\r\nAs suggested by @yongtang, the best thing to do is to convert the numpy array to a tensor explicitly.", "@53RT, Looks like its fixed in TF-nightly (2.1).\r\nPlease find the [gist](https://colab.sandbox.google.com/gist/gadagashwini/7bd1e30565e22d6f8a9087006016e52f/untitled327.ipynb) and let us know. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@53RT, Issue is fixed in Tf-nightly. \r\nCan you confirm. Thanks", "@gadagashwini sorry for not responding.\r\nYes I could reproduce the results and PNG encoding/decoding did worked for 8 and 16 bit with \r\n\r\n```\r\ntf.__version__\r\n'2.2.0-dev20200112'\r\n```\r\nThanks for fixing it", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30113\">No</a>\n"]}, {"number": 30112, "title": "[TF2.0] How to do image data augmentation using Dataset", "body": "Hello everyone,\r\n\r\nsince TF2.0, it seems that image data augmentation is made more difficult than in 1.x. I admit I am now not able to do such a simple thing as adding random rotation to my pipeline. \r\n\r\nUsing Dataset, I `map` my preprocess function to the pipeline. I can perform augmentations available in tf.image, but these are rather limited in range, e.g. I am not happy with rot90. I would like to add a TF->PIL->rotate->TF code, but it always raises exception during eager because obviously the input dimensions are not known.\r\n\r\n```\r\nimage = tf.io.read_file(path)\r\nimage = tf.image.decode_image(image, channels=3)\r\nimage = tf.image.random_flip_left_right(image)\r\nimage = SOMEHOWROTATE(image, angle)\r\n```\r\n\r\nSeems that keras ImageDataGenerator should have been used, but I am not able to integrate it into Dataset pipeline. Previously in TF1, I used py_func in which I was able to convert tensors to pil images and rotate. Anyone can help?\r\n\r\nPlease do not forward this to stackoverflow, as user base of TF2.0 is very sparse there. It seems to be very difficult to find examples of using TF2.0.\r\n\r\nr", "comments": ["Did you get the right image. I use map function, and I can't even get the string of \"path\".", "in dataset.map:\r\n1. do random brightness/saturation/hue with tf.image functions.\r\n2. wrap OpenCV operation with tf.py_function(rotation with warp affine [https://cristianpb.github.io/blog/image-rotation-opencv](url) ).\r\nbut currently I'm having lots of trouble dealing with tf.py_function when distribute strategy is employed. see: [https://github.com/tensorflow/tensorflow/issues/31117](url)", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30112\">No</a>\n", "This is still a massive problem. Can you guide us to a workaround/solution?", "TF2.0's rotate pipeline solution is this code.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport tensorflow_addons as tfa\r\nimport numpy as np \r\n@tf.function\r\ndef rotate_tf(image):\r\n    if image.shape.__len__() ==4:\r\n            \r\n        random_angles = tf.random.uniform(shape = (tf.shape(image)[0], ), minval = -np\r\n        .pi / 4, maxval = np.pi / 4)\r\n    if image.shape.__len__() ==3:\r\n        random_angles = tf.random.uniform(shape = (), minval = -np\r\n        .pi / 4, maxval = np.pi / 4)\r\n\r\n    return tfa.image.rotate(image,random_angles)\r\n\r\n\r\n(train_x,train_y),(test_x,test_y) =  keras.datasets.mnist.load_data()\r\ntrain_x=train_x.reshape(-1,28,28,1)\r\ntrain_ds=tf.data.Dataset.from_tensor_slices(train_x)\r\ntrain_ds=train_ds.batch(512).map(rotate_tf) # map first is too slow in first time\r\nfor x in train_ds:\r\n    X = x.numpy().reshape((-1,28,28))\r\n    break\r\nplt.figure(facecolor=\"white\")\r\nfor i in range(16):\r\n    plt.subplot(4,4,i+1)\r\n    plt.imshow(X[i,:])\r\n    plt.grid(False)\r\nplt.show()\r\n```\r\n![mnist_rotate](https://user-images.githubusercontent.com/33369460/68120578-16b27380-ff49-11e9-8713-1f6eadb1acd0.png)\r\n"]}, {"number": 30111, "title": "Unable to build tensorflow from source on CentOS", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.4.9 and 3.6.6\r\n- Installed using virtualenv? pip? conda?: -\r\n- Bazel version (if compiling from source): 0.26.0 from Vincent Batts (Fedora COPR)\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nHello everyone,\r\n\r\nfor my current project I want to use Tensorflow's C API on a cluster running on CentOS. I was able to use the downloaded C API version (from: https://www.tensorflow.org/install/lang_c) on the cluster. However the last version I downloaded (on June 14th) does not support AVX / AVX2. Simultaneously I did the same thing on one cluster node running on Ubuntu. There I built Tensorflow from source, which automatically supported AVX, and therefore decreased the inference time by 66% which is great. I now want to use AVX also on CentOS. However here I am experiencing several issues that I am unable to handle:\r\n\r\n__________________________________________________________________________\r\n\r\n1) If I want to use the shared libraries (.so) and the header files for the C API that were generated during building Tensorflow from source on Ubuntu my code does not compile. I get the following error message:\r\n```\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::assign(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::domain_error::domain_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace_aux(unsigned long, unsigned long, unsigned long, char)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow_framework.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::operator>><char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_stringstream()@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::rfind(char, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__exception_ptr::exception_ptr::exception_ptr(void*)@CXXABI_1.3.11'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::invalid_argument::invalid_argument(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::erase(unsigned long, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `powf@GLIBC_2.27'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::substr(unsigned long, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::domain_error::domain_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::_M_sync(char*, unsigned long, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(unsigned long, unsigned long, char const*, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(unsigned long, unsigned long, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `VTT for std::__cxx11::basic_stringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__throw_out_of_range_fmt(char const*, ...)@GLIBCXX_3.4.20'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::underflow_error::underflow_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow_framework.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::append(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `__cxa_throw_bad_array_new_length@CXXABI_1.3.8'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::push_back(char)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `expf@GLIBC_2.27'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::underflow_error::underflow_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::overflow_error::overflow_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `logf@GLIBC_2.27'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::append(char const*, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::length_error::length_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::basic_ostringstream(std::_Ios_Openmode)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::thread::_M_start_thread(std::unique_ptr<std::thread::_State, std::default_delete<std::thread::_State> >, void (*)())@GLIBCXX_3.4.22'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_erase(unsigned long, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `__cxa_init_primary_exception@CXXABI_1.3.11'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__atomic_futex_unsigned_base::_M_futex_notify_all(unsigned int*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::thread::_State::~_State()@GLIBCXX_3.4.22'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::rfind(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(std::logic_error const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `typeinfo for std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(unsigned long, unsigned long, char const*) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `VTT for std::__cxx11::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::resize(unsigned long, char)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_istringstream()@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_last_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_istringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::operator=(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_first_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >::~basic_ostringstream()@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::basic_istream<char, std::char_traits<char> >& std::getline<char, std::char_traits<char>, std::allocator<char> >(std::basic_istream<char, std::char_traits<char> >&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&, char)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::str() const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::swap(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__future_base::_State_baseV2::_Make_ready::_M_set()@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::length_error::length_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_first_not_of(char const*, unsigned long, unsigned long) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `typeinfo for std::thread::_State@GLIBCXX_3.4.22'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `lgammaf@GLIBC_2.23'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(std::runtime_error const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::runtime_error::runtime_error(std::runtime_error const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct(unsigned long, char)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::invalid_argument::invalid_argument(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::logic_error::logic_error(std::logic_error const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::random_device::_M_init(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::out_of_range::out_of_range(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `VTT for std::__cxx11::basic_ostringstream<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::range_error::range_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::out_of_range::out_of_range(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::overflow_error::overflow_error(char const*)@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `vtable for std::__cxx11::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >@GLIBCXX_3.4.21'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `lgamma@GLIBC_2.23'\r\n/home/elias/OpenFOAM/elias-4.1/platforms/linux64GccDPInt64Opt/lib/libtensorflow.so: undefined reference to `std::range_error::range_error(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)@GLIBCXX_3.4.21'\r\n```\r\n\r\nObviously there is an issue with GLIBCXX / GLIBC / CXXABI. The compilation seems to need:\r\n```\r\nGLIBCXX_3.4.20\r\nGLIBCXX_3.4.21\r\nGLIBCXX_3.4.22\r\nGLIBC_2.27\r\nGLIBC_2.23\r\nCXXABI_1.3.11\r\nCXXABI_1.3.8\r\n```\r\n\r\nSo i checked the available ones, and I get:\r\n```\r\nstrings /usr/lib64/libstdc++.so.6 | grep GLIBC\r\n\r\nGLIBCXX_3.4\r\nGLIBCXX_3.4.1\r\nGLIBCXX_3.4.2\r\nGLIBCXX_3.4.3\r\nGLIBCXX_3.4.4\r\nGLIBCXX_3.4.5\r\nGLIBCXX_3.4.6\r\nGLIBCXX_3.4.7\r\nGLIBCXX_3.4.8\r\nGLIBCXX_3.4.9\r\nGLIBCXX_3.4.10\r\nGLIBCXX_3.4.11\r\nGLIBCXX_3.4.12\r\nGLIBCXX_3.4.13\r\nGLIBCXX_3.4.14\r\nGLIBCXX_3.4.15\r\nGLIBCXX_3.4.16\r\nGLIBCXX_3.4.17\r\nGLIBCXX_3.4.18\r\nGLIBCXX_3.4.19\r\nGLIBC_2.3\r\nGLIBC_2.2.5\r\nGLIBC_2.14\r\nGLIBC_2.4\r\nGLIBC_2.3.2\r\n```\r\n\r\n```\r\nstrings /usr/lib64/libstdc++.so.6 | grep CXXABI\r\n\r\nCXXABI_1.3\r\nCXXABI_1.3.1\r\nCXXABI_1.3.2\r\nCXXABI_1.3.3\r\nCXXABI_1.3.4\r\nCXXABI_1.3.5\r\nCXXABI_1.3.6\r\nCXXABI_1.3.7\r\nCXXABI_TM_1\r\n```\r\n\r\nFor comparison reasons I pulled the current official docker image for CentOS and it provided exactly the same CXXABIs and GLIBCs / GLIBCXXs. \r\nCan I do something to enable the compilation with the version from Ubuntu?\r\n__________________________________________________________________________\r\n\r\n2) Assuming that the Ubuntu version will never work on CentOS I decided to go another way and try to build Tensorflow directly on CentOS. The first problem was the unavailability of Bazel for CentOS. Thanks to the support of Vincent Batts who provides unofficial Bazel versions for CentOS I was able to install Bazel 0.26.0 both in my CentOS docker container as well as on the cluster running on CentOS. Entering \r\n\r\n```\r\nbazel version\r\n```\r\n\r\nGives me this on the cluster:\r\n```\r\nWARNING: Output base '/home/elias/.cache/bazel/_bazel_elias/d41d8cd98f00b204e9800998ecf8427e' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.26.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 11 17:32:09 2019 (1560274329)\r\nBuild timestamp: 1560274329\r\nBuild timestamp as int: 1560274329\r\n```\r\n\r\nand this in my docker container:\r\n```\r\n$TEST_TMPDIR defined: output root default is '/tmp/bazel' and max_idle_secs default is '15'.\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\n.bazelrc\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/tmp/bazel/_bazel_root/install/3e27460bd3939411a8a3e57865ad7ae0/_embedded_binaries/A-server.jar) to field java.nio.Buffer.address\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.26.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 11 17:32:09 2019 (1560274329)\r\nBuild timestamp: 1560274329\r\nBuild timestamp as int: 1560274329\r\n```\r\n\r\nI am worried about the cluster warning me regarding the NFS, however if do not really know how to solve this. I assume it does not matter where I put bazel on the cluster, as long as it is on the cluster I will always get this error?\r\n\r\nHowever, in both cases I seem to have an installed version of Bazel 0.26.0 which should be able to build Tensorflow from source. Unfortunately building Tensorflow fails, I will describe the exact commands / steps i executed:\r\n\r\n1) Installing the requirements\r\n```\r\nyum install -y https://centos7.iuscommunity.org/ius-release.rpm\r\nyum update\r\nyum install -y python36u python36u-libs python36u-devel python36u-pip\r\npip install -U --user pip six numpy wheel setuptools mock future>=0.17.1\r\npip install -U --user keras_applications==1.0.6 --no-deps\r\npip install -U --user keras_preprocessing==1.0.5 --no-deps\r\n```\r\n\r\n2) Installing bazel\r\n```\r\nyum install https://copr-be.cloud.fedoraproject.org/results/vbatts/bazel/epel-7-x86_64/00934552-bazel/bazel-0.26.0-2.el7.x86_64.rpm\r\n```\r\n\r\n3) Downloading the Tensorflow source code:\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n```\r\n\r\n4) Configure the build:\r\n```\r\n./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: default\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python2.7/site-packages]: /usr/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nDo you wish to build TensorFlow with MPI support? [y/N]: y\r\nPlease specify the MPI toolkit folder. [Default is /opt/openmpi/3.1.3/gcc]: y\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: default\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n```\r\n\r\n5) Trying to build according to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md\r\n```\r\nbazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\n```\r\n\r\n6) ERROR MESSAGE: (There are some german messages, I will provide a fully english version asap)\r\n```\r\nbazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'test' from /tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages --python_path=/usr/bin/python --action_env TF_CONFIGURE_IOS=0\r\nINFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium --test_tag_filters=-benchmark-test,-no_oss,-oss_serial --build_tag_filters=-benchmark-test,-no_oss --test_tag_filters=-gpu --build_tag_filters=-gpu\r\nINFO: Found applicable config definition build:opt in file /tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nINFO: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl:252:18):\r\n - /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n - /tensorflow/WORKSPACE:36:1\r\nERROR: An error occurred during the fetch of repository 'io_bazel_rules_docker':\r\n   Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external\r\n+ rm -rf /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\n+ git clone '' https://github.com/bazelbuild/rules_docker.git /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\nToo many arguments.\r\n\r\nusage: git clone [options] [--] <repo> [<dir>]\r\n\r\n    -v, --verbose         be more verbose\r\n    -q, --quiet           be more quiet\r\n    --progress            force progress reporting\r\n    -n, --no-checkout     don't create a checkout\r\n    --bare                create a bare repository\r\n    --mirror              create a mirror repository (implies bare)\r\n    -l, --local           to clone from a local repository\r\n    --no-hardlinks        don't use local hardlinks, always copy\r\n    -s, --shared          setup as shared repository\r\n    --recursive           initialize submodules in the clone\r\n    --recurse-submodules  initialize submodules in the clone\r\n    --template <template-directory>\r\n                          directory from which templates will be used\r\n    --reference <repo>    reference repository\r\n    -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n    -b, --branch <branch>\r\n                          checkout <branch> instead of the remote's HEAD\r\n    -u, --upload-pack <path>\r\n                          path to git-upload-pack on the remote\r\n    --depth <depth>       create a shallow clone of that depth\r\n    --single-branch       clone only one branch, HEAD or --branch\r\n    --separate-git-dir <gitdir>\r\n                          separate git dir from working tree\r\n    -c, --config <key=value>\r\n                          set config inside the new repository\r\n\r\n+ git clone https://github.com/bazelbuild/rules_docker.git /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external\r\n+ rm -rf /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\n+ git clone '' https://github.com/bazelbuild/rules_docker.git /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\nToo many arguments.\r\n\r\nusage: git clone [options] [--] <repo> [<dir>]\r\n\r\n    -v, --verbose         be more verbose\r\n    -q, --quiet           be more quiet\r\n    --progress            force progress reporting\r\n    -n, --no-checkout     don't create a checkout\r\n    --bare                create a bare repository\r\n    --mirror              create a mirror repository (implies bare)\r\n    -l, --local           to clone from a local repository\r\n    --no-hardlinks        don't use local hardlinks, always copy\r\n    -s, --shared          setup as shared repository\r\n    --recursive           initialize submodules in the clone\r\n    --recurse-submodules  initialize submodules in the clone\r\n    --template <template-directory>\r\n                          directory from which templates will be used\r\n    --reference <repo>    reference repository\r\n    -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n    -b, --branch <branch>\r\n                          checkout <branch> instead of the remote's HEAD\r\n    -u, --upload-pack <path>\r\n                          path to git-upload-pack on the remote\r\n    --depth <depth>       create a shallow clone of that depth\r\n    --single-branch       clone only one branch, HEAD or --branch\r\n    --separate-git-dir <gitdir>\r\n                          separate git dir from working tree\r\n    -c, --config <key=value>\r\n                          set config inside the new repository\r\n\r\n+ git clone https://github.com/bazelbuild/rules_docker.git /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': Traceback (most recent call last):\r\n\tFile \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 234\r\n\t\t_clone_or_update(ctx)\r\n\tFile \"/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 74, in _clone_or_update\r\n\t\tfail((\"error cloning %s:\\n%s\" % (ctx....)))\r\nerror cloning io_bazel_rules_docker:\r\n+ cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external\r\n+ rm -rf /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\n+ git clone '' https://github.com/bazelbuild/rules_docker.git /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\nToo many arguments.\r\n\r\nusage: git clone [options] [--] <repo> [<dir>]\r\n\r\n    -v, --verbose         be more verbose\r\n    -q, --quiet           be more quiet\r\n    --progress            force progress reporting\r\n    -n, --no-checkout     don't create a checkout\r\n    --bare                create a bare repository\r\n    --mirror              create a mirror repository (implies bare)\r\n    -l, --local           to clone from a local repository\r\n    --no-hardlinks        don't use local hardlinks, always copy\r\n    -s, --shared          setup as shared repository\r\n    --recursive           initialize submodules in the clone\r\n    --recurse-submodules  initialize submodules in the clone\r\n    --template <template-directory>\r\n                          directory from which templates will be used\r\n    --reference <repo>    reference repository\r\n    -o, --origin <name>   use <name> instead of 'origin' to track upstream\r\n    -b, --branch <branch>\r\n                          checkout <branch> instead of the remote's HEAD\r\n    -u, --upload-pack <path>\r\n                          path to git-upload-pack on the remote\r\n    --depth <depth>       create a shallow clone of that depth\r\n    --single-branch       clone only one branch, HEAD or --branch\r\n    --separate-git-dir <gitdir>\r\n                          separate git dir from working tree\r\n    -c, --config <key=value>\r\n                          set config inside the new repository\r\n\r\n+ git clone https://github.com/bazelbuild/rules_docker.git /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker reset --hard 251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch '' origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\nusage: git [--version] [--help] [-c name=value]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\nINFO: Elapsed time: 4.263s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nNow I saw here (https://github.com/tensorflow/tensorflow/issues/28824) that adding something to the WORKSPACE file might help. It says: \" managed to fix this by adding the following to the top of my WORKSPACE file: [...] If you do the same, you probably want to ensure you get the latest version here https://github.com/bazelbuild/rules_docker and copy the code snippet from the README instead of from here.\"\r\n\r\nI now ran \"update_deps.sh\" and added the respective part from here(https://github.com/bazelbuild/rules_docker/blob/master/README.md), now my WORKSPACE file looks like this:\r\n\r\n```\r\nworkspace(name = \"org_tensorflow\")\r\n\r\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\")\r\n\r\nhttp_archive(\r\n    name = \"io_bazel_rules_docker\",\r\n    sha256 = \"3556d4972571f288f8c43378295d84ed64fef5b1a875211ee1046f9f6b4258fa\",\r\n    strip_prefix = \"rules_docker-0.8.0\",\r\n    urls = [\"https://github.com/bazelbuild/rules_docker/archive/v0.8.0.tar.gz\"],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"io_bazel_rules_closure\",\r\n    sha256 = \"5b00383d08dd71f28503736db0500b6fb4dda47489ff5fc6bed42557c07c6ba9\",\r\n    strip_prefix = \"rules_closure-308b05b2419edb5c8ee0471b67a40403df940149\",\r\n    urls = [\r\n\t\"http://mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",\r\n        \"https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",  # 2019-06-13\r\n    ],\r\n)\r\n\r\n# Load tf_repositories() before loading dependencies for other repository so\r\n# that dependencies like com_google_protobuf won't be overridden.\r\nload(\"//tensorflow:workspace.bzl\", \"tf_repositories\")\r\n# Please add all new TensorFlow dependencies in workspace.bzl.\r\ntf_repositories()\r\n\r\nload(\"@io_bazel_rules_closure//closure:defs.bzl\", \"closure_repositories\")\r\n\r\nclosure_repositories()\r\n\r\nload(\"//third_party/toolchains/preconfig/generate:archives.bzl\",\r\n     \"bazel_toolchains_archive\")\r\n\r\nbazel_toolchains_archive()\r\n\r\nload(\r\n    \"@bazel_toolchains//repositories:repositories.bzl\",\r\n    bazel_toolchains_repositories = \"repositories\",\r\n)\r\n\r\nbazel_toolchains_repositories()\r\n\r\nload(\r\n    \"@io_bazel_rules_docker//repositories:repositories.bzl\",\r\n    container_repositories = \"repositories\",\r\n)\r\n\r\ncontainer_repositories()\r\n\r\nload(\"//third_party/toolchains/preconfig/generate:workspace.bzl\",\r\n     \"remote_config_workspace\")\r\n\r\nremote_config_workspace()\r\n\r\n# Apple and Swift rules.\r\nhttp_archive(\r\n    name = \"build_bazel_rules_apple\",\r\n    sha256 = \"23792cd999f97fc97284d1c44cb1324bfdd0bc54aa68ad513fa3705aca3b1f9e\",\r\n    urls = [\"https://github.com/bazelbuild/rules_apple/releases/download/0.15.0/rules_apple.0.15.0.tar.gz\"],\r\n)  # https://github.com/bazelbuild/rules_apple/releases\r\nhttp_archive(\r\n    name = \"build_bazel_apple_support\",\r\n    sha256 = \"7356dbd44dea71570a929d1d4731e870622151a5f27164d966dda97305f33471\",\r\n    urls = [\"https://github.com/bazelbuild/apple_support/releases/download/0.6.0/apple_support.0.6.0.tar.gz\"],\r\n)  # https://github.com/bazelbuild/apple_support/releases\r\nhttp_archive(\r\n    name = \"bazel_skylib\",\r\n    sha256 = \"2ef429f5d7ce7111263289644d233707dba35e39696377ebab8b0bc701f7818e\",\r\n    urls = [\"https://github.com/bazelbuild/bazel-skylib/releases/download/0.8.0/bazel-skylib.0.8.0.tar.gz\"],\r\n)  # https://github.com/bazelbuild/bazel-skylib/releases\r\nhttp_archive(\r\n    name = \"build_bazel_rules_swift\",\r\n    sha256 = \"9efe9699e9765e6b4a5e063e4a08f6b163cccaf0443f775d935baf5c3cd6ed0e\",\r\n    urls = [\"https://github.com/bazelbuild/rules_swift/releases/download/0.9.0/rules_swift.0.9.0.tar.gz\"],\r\n)  # https://github.com/bazelbuild/rules_swift/releases\r\nhttp_archive(\r\n    name = \"com_github_apple_swift_swift_protobuf\",\r\n    type = \"zip\",\r\n    strip_prefix = \"swift-protobuf-1.5.0/\",\r\n    urls = [\"https://github.com/apple/swift-protobuf/archive/1.5.0.zip\"],\r\n)  # https://github.com/apple/swift-protobuf/releases\r\nhttp_file(\r\n    name = \"xctestrunner\",\r\n    executable = 1,\r\n    urls = [\"https://github.com/google/xctestrunner/releases/download/0.2.7/ios_test_runner.par\"],\r\n)  # https://github.com/google/xctestrunner/releases\r\n# Use `swift_rules_dependencies` to fetch the toolchains. With the\r\n# `git_repository` rules above, the following call will skip redefining them.\r\nload(\"@build_bazel_rules_swift//swift:repositories.bzl\", \"swift_rules_dependencies\")\r\nswift_rules_dependencies()\r\n\r\n# We must check the bazel version before trying to parse any other BUILD\r\n# files, in case the parsing of those build files depends on the bazel\r\n# version we require here.\r\nload(\"//tensorflow:version_check.bzl\", \"check_bazel_version_at_least\")\r\ncheck_bazel_version_at_least(\"0.19.0\")\r\n\r\nload(\"//third_party/android:android_configure.bzl\", \"android_configure\")\r\nandroid_configure(name=\"local_config_android\")\r\nload(\"@local_config_android//:android.bzl\", \"android_workspace\")\r\nandroid_workspace()\r\n\r\n# If a target is bound twice, the later one wins, so we have to do tf bindings\r\n# at the end of the WORKSPACE file.\r\nload(\"//tensorflow:workspace.bzl\", \"tf_bind\")\r\ntf_bind()\r\n\r\nhttp_archive(\r\n    name = \"inception_v1\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"7efe12a8363f09bc24d7b7a450304a15655a57a7751929b2c1593a71183bb105\",\r\n    urls = [\r\n\t\"http://storage.googleapis.com/download.tensorflow.org/models/inception_v1.zip\",\r\n        \"http://download.tensorflow.org/models/inception_v1.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"mobile_ssd\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"bddd81ea5c80a97adfac1c9f770e6f55cbafd7cce4d3bbe15fbeb041e6b8f3e8\",\r\n    urls = [\r\n\t\"http://storage.googleapis.com/download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip\",\r\n        \"http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_android_export.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"mobile_multibox\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"859edcddf84dddb974c36c36cfc1f74555148e9c9213dedacf1d6b613ad52b96\",\r\n    urls = [\r\n\t\"http://storage.googleapis.com/download.tensorflow.org/models/mobile_multibox_v1a.zip\",\r\n        \"http://download.tensorflow.org/models/mobile_multibox_v1a.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"stylize\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"3d374a730aef330424a356a8d4f04d8a54277c425e274ecb7d9c83aa912c6bfa\",\r\n    urls = [\r\n\t\"http://storage.googleapis.com/download.tensorflow.org/models/stylize_v1.zip\",\r\n        \"http://download.tensorflow.org/models/stylize_v1.zip\",\r\n    ],\r\n)\r\n\r\nhttp_archive(\r\n    name = \"speech_commands\",\r\n    build_file = \"//:models.BUILD\",\r\n    sha256 = \"c3ec4fea3158eb111f1d932336351edfe8bd515bb6e87aad4f25dbad0a600d0c\",\r\n    urls = [\r\n\t\"http://storage.googleapis.com/download.tensorflow.org/models/speech_commands_v0.01.zip\",\r\n        \"http://download.tensorflow.org/models/speech_commands_v0.01.zip\",\r\n    ],\r\n)\r\n```\r\nHowever, now my error looks like this:\r\n\r\n```\r\nbazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=204\r\nINFO: Reading rc options for 'test' from /home/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone --strategy=Genrule=standalone -c opt --announce_rc --define=grpc_no_ares=true --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include\r\nINFO: Reading rc options for 'test' from /home/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --host_force_python=PY2 --action_env PYTHON_BIN_PATH=/usr/bin/python --action_env PYTHON_LIB_PATH=/usr/lib/python2.7/site-packages --python_path=/usr/bin/python --action_env TF_CONFIGURE_IOS=0\r\nINFO: Reading rc options for 'test' from /home/tensorflow/.tf_configure.bazelrc:\r\n  'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium --test_tag_filters=-benchmark-test,-no_oss,-oss_serial --build_tag_filters=-benchmark-test,-no_oss --test_tag_filters=-gpu --build_tag_filters=-gpu\r\nINFO: Found applicable config definition build:opt in file /home/tensorflow/.tf_configure.bazelrc: --copt=-march=native --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\nINFO: Call stack for the definition of repository 'local_config_git' which is a git_configure (rule definition at /home/tensorflow/third_party/git/git_configure.bzl:63:17):\r\n - /home/tensorflow/tensorflow/workspace.bzl:71:5\r\n - /home/tensorflow/WORKSPACE:26:1\r\nERROR: An error occurred during the fetch of repository 'local_config_git':\r\n   Traceback (most recent call last):\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nERROR: /home/tensorflow/tensorflow/core/BUILD:2810:1: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\n and referenced by '//tensorflow/core:version_info_gen'\r\nERROR: /home/tensorflow/tensorflow/core/BUILD:2810:1: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\n and referenced by '//tensorflow/core:version_info_gen'\r\nERROR: /home/tensorflow/tensorflow/core/BUILD:2810:1: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\n and referenced by '//tensorflow/core:version_info_gen'\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow_test' failed; build aborted: no such package '@local_config_git//': Traceback (most recent call last):\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 61\r\n\t\t_fail(result.stderr)\r\n\tFile \"/home/tensorflow/third_party/git/git_configure.bzl\", line 14, in _fail\r\n\t\tfail((\"%sGit Configuration Error:%s %...)))\r\nGit Configuration Error: Traceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/98bd3f3dfbd0725c619a9932413c587d/external/org_tensorflow/tensorflow/tools/git/gen_git_source.py\", line 29, in <module>\r\n    from builtins import bytes  # pylint: disable=redefined-builtin\r\nImportError: No module named builtins\r\n\r\nINFO: Elapsed time: 48.450s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (141 packages loaded, 7475 targets configured)\r\nFAILED: Build did NOT complete successfully (141 packages loaded, 7475 targets configured)\r\n```\r\nHowever I am getting further than before I think. \r\nMy git version is: 2.16.5\r\n\r\nPS: if it helps I can provide my CentOS docker image where I try to get this done!\r\n__________________________________________________________________________\r\n\r\n3) Alternative: Would it be possible for the development team to provide the C API libraries and headers (like here: https://www.tensorflow.org/install/lang_c) with AVX support? I would hope that if you compile it I may be able to run it also on CentOS, or is that improbable?\r\n\r\nThanks in advance!", "comments": ["I have met the same problem you got in step 6 once:\r\n```\r\n+ git -C /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/io_bazel_rules_docker fetch origin 251f6a68b439744094faff800cd029798edf9faa:251f6a68b439744094faff800cd029798edf9faa\r\nUnknown option: -C\r\n```\r\nafter I upgrade the git version to 2.X. It fix the problem.\r\nBut since your git version is already 2.16.5, it's quite strange", "Hi Leslie,\r\n\r\nI think the problem with io_bazel_rules is solved by changing the WORKSPACE file. However I intended to solve the \"no such package '@local_config_git\" error by updating to git 2.X, however I was not sucessful.\r\n\r\nEDIT: I want to point out to something I noticed regarding git. As I had to install bazel from Fedora COPR I now have 0.26.0 running, however typing \"bazel version\" gives me:\r\n\r\n```\r\nbazel version\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/root/.cache/bazel/_bazel_root/install/3e27460bd3939411a8a3e57865ad7ae0/_embedded_binaries/A-server.jar) to field java.nio.Buffer.address\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.26.0- (@non-git)\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jun 11 17:32:09 2019 (1560274329)\r\nBuild timestamp: 1560274329\r\nBuild timestamp as int: 1560274329\r\n```\r\nYou can see: Build label: 0.26.0- (@non-git)\r\n\r\nI do not know what \"(@non-git)\" means but maybe there is an issue with that?", "I have created a docker image exactly reproducing my issue. Please pull as follows:\r\n```\r\ndocker pull elias1995/c093cd2c6cbc\r\n```\r\nThe tensorflow directory can be found under:\r\n```\r\n/home/tensorflow\r\n```\r\nPlease notice that it contains the original WORKSPACE file. The tensorflow directory was directly cloned from github.\r\n\r\n\r\nSpecifications of the image:\r\n```\r\nCentOS Linux release 7.6.1810 (Core)\r\ngit version 2.16.5\r\nOpen MPI: 3.1.3\r\nbazel: 0.26.0- (@non-git) from Vincent Batts (Fedora COPR)\r\n```\r\n\r\nSteps to reproduce my error:\r\n```\r\ncd /home/tensorflow\r\n./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: default\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python2.7/site-packages]: usr/lib/python3.6/site-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nDo you wish to build TensorFlow with MPI support? [y/N]: y\r\nPlease specify the MPI toolkit folder. [Default is /opt/openmpi/3.1.3/gcc]: default\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: default\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\n```\r\nThen:\r\n```\r\nbazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\n```\r\n\r\nPlease remark that I have installed openmpi version 3.1.3 in the same directory as it is on the cluster, as I must use it later on the cluster. If I would be able to build tensorflow as configured above in this docker container, do you think I can use it on the cluster? I would prefer this procedure as I do not have the rights on the cluster. I could get support on that but preferably the docker container should be used to build Tensorflow's C API.\r\n\r\nCluster specifications:\r\n```\r\nCentOS Linux release 7.6.1810 (Core)\r\ngit version 1.8.3.1\r\nOpen MPI: 3.1.3\r\n0.26.0- (@non-git)\r\n```\r\n", "@el1995 \r\nPlease try below commands, I have **built tf in your docker**.\r\n```\r\nsudo pip install future\r\n```\r\n```\r\n[root@0a5b99e5f8ba tensorflow]# ./configure\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: Waiting for server process to terminate (waited 5 seconds, waiting at most 60)\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/root/.cache/bazel/_bazel_root/install/3e27460bd3939411a8a3e57865ad7ae0/_embedded_binaries/A-server.jar) to field java.nio.Buffer.address\r\nWARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.26.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python2.7/site-packages\r\n  /usr/lib64/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python2.7/site-packages]\r\n/usr/lib/python2.7/site-packages:/usr/lib64/python2.7/site-packages\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: Y\r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: N\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: N\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: N\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=noignite       # Disable Apache Ignite support.\r\n        --config=nokafka        # Disable Apache Kafka support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n[root@0a5b99e5f8ba tensorflow]# bazel test --config opt //tensorflow/tools/lib_package:libtensorflow_test\r\n```\r\nAs for the TF multinode MPI support question,  please refer to this issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/26610\r\n\r\n\r\n", "Hi @Leslie-Fang,\r\n\r\nthat solved the issue right away, thank you so much! The built versions worked on the CentOS cluster, I did not run into further errors with OpenMPI!\r\n\r\nThanks for the support!", "@el1995 My Pleasure.\r\nJust curious how you use the openmpi in your tensorflow application?\r\n\r\n", "Hi @Leslie-Fang,\r\n\r\nmy application is basically no classic deep learning one. We use TensorFlow for inference purposes in simulations of turbulent combustion, e.g. in liquid rocket engines. In the last years it was shown that predicting the chemical source terms via Artificial Neural Networks is way faster than direct computation. Our Computational Fluid Dynamics software relies heavily on a large amount of parallelization, we use openmpi to run the cases on up to 1500 cores.\r\n\r\nWhat I did was to install openmpi in the container at the exact same position as we have installed it on our computing cluster. Therefore, while configuring the TensorFlow build, I was able to input exactly the same path to the binary as I would have on the cluster. Probably that helped me to avoid any issues!"]}, {"number": 30110, "title": "PYYAML not found", "body": "Running ```yaml_string = model.to_yaml()``` on mac is giving me this error :\r\n```\r\nTraceback (most recent call last):\r\n  File \"check-keras.py\", line 197, in <module>\r\n    yaml_string = model.to_yaml()\r\n  File \"/Users/vipulsharma/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/network.py\", line 1560, in to_yaml\r\n    'Requires yaml module installed (`pip install pyyaml`).')\r\nImportError: Requires yaml module installed (`pip install pyyaml`)\r\n```\r\nWhen I run pip install, it says that the package already exists.\r\n```\r\nVipuls-MacBook-Air:Keras vipulsharma$ pip install pyyaml\r\nRequirement already satisfied: pyyaml in /Users/vipulsharma/Library/Python/2.7/lib/python/site-packages (5.1.1)\r\n```\r\n\r\nSo I figured in the above terminal output\r\n``` File \"/Users/vipulsharma/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/network.py\", line 1560,``` \r\n\r\nThis version of Python is different than the version here :\r\n```Requirement already satisfied: pyyaml in /Users/vipulsharma/Library/Python/2.7/lib/python/site-packages (5.1.1)```\r\n\r\nSo I tried installing yaml in \r\n```/Users/vipulsharma/Library/Python/3.7/lib/python/site-packages/```\r\n\r\n by doing a \r\n```pip install --target=/Users/vipulsharma/Library/Python/3.7/lib/python/site-packages/ pyyaml```\r\n\r\n It installed yaml in  ```/Users/vipulsharma/Library/Python/3.7/lib/python/site-packages/```  successfully, but when I run my Python file it gives me the same error.\r\n\r\nHow am I supposed to convert it to Yaml?", "comments": ["This question on the install of pyyaml is better asked in stackoverflow then here. ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30109, "title": "Using keras.layers.BatchNormalization inside keras.layers.TimeDistributed causes bad validation loss/accuracy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu 1.13.1\r\n- Python version: 3.6.7\r\n- CUDA/cuDNN version: v10\r\n- GPU model and memory: GTX 980M\r\n\r\n**Describe the current behavior**\r\nUsing BatchNorm inside a TimeDistributed layer results in poor validation performance. A model trained and validated on identical data can reach 100% train accuracy but never reach that when validating against the same sample. Removing the Batchnorm layer resolves the issue.\r\n\r\nSee the training results from a toy example:\r\n```\r\n500/500 loss: 1.6910 - sparse_categorical_accuracy: 0.2400 - val_loss: 1.7292 - val_sparse_categorical_accuracy: 0.2000\r\nEpoch 2/100\r\n\r\n500/500 loss: 1.2619 - sparse_categorical_accuracy: 0.5600 - val_loss: 1.6112 - val_sparse_categorical_accuracy: 0.2000\r\nEpoch 3/100\r\n\r\n500/500 loss: 0.9130 - sparse_categorical_accuracy: 0.9200 - val_loss: 1.5113 - val_sparse_categorical_accuracy: 0.2000\r\nEpoch 4/100\r\n\r\n500/500  loss: 0.6420 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.4297 - val_sparse_categorical_accuracy: 0.2000\r\nEpoch 5/100\r\n...\r\n...\r\n...\r\n500/500 loss: 8.2955e-04 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1042 - val_sparse_categorical_accuracy: 0.2000\r\nEpoch 100/100\r\n```\r\n\r\n**Describe the expected behavior**\r\nWhen training and validating on an identical sample, the model should get the same accuracy during both training and validation. To eliminate the possibility that this is caused by a difference between the running mean/variance and the batchwise mean/variance, a very small value of BatchNorm Momentum was trialed. It was observed that modifying Momentum appears to have no effect. BatchNorm should not affect a model's ability to make good predictions.\r\n\r\n**Code to reproduce the issue**\r\nThe following code shows the issue. Run it a few times and watch the validation accuracy/loss.\r\n```\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\n\r\ntime_steps = 5\r\nsample_width = 20\r\nkernel_size = 3\r\nnum_filters = 5\r\nnum_classes = 5\r\n\r\ndef build_graph():\r\n    with tf.name_scope(\"SequenceProcess\"):\r\n        sequence_input = keras.layers.Input(shape=(time_steps, sample_width))\r\n\r\n        with tf.name_scope(\"TimeDistributed\"):\r\n            sample_input = keras.layers.Input(shape=(sample_width,))\r\n\r\n            conv = keras.layers.Reshape([sample_width, 1])(sample_input)\r\n            conv = keras.layers.BatchNormalization(momentum=0.01)(conv)\r\n            conv = keras.layers.Conv1D(num_filters, kernel_size, padding='same')(conv)\r\n            encoded_sample = keras.layers.Reshape((num_filters * sample_width,))(conv)\r\n\r\n            sample_model = keras.models.Model(sample_input, encoded_sample)\r\n\r\n        processed_samples = keras.layers.TimeDistributed(sample_model)(sequence_input)\r\n        fc = keras.layers.Dense(num_classes, activation='softmax')(processed_samples)\r\n\r\n        sequence_model = keras.models.Model(sequence_input, fc)\r\n        sequence_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\r\n\r\n    return sequence_model\r\n\r\ndef generate_data():\r\n    steps = []\r\n    for i in range(time_steps):\r\n        sample_data = np.random.rand(sample_width)\r\n        steps.append(sample_data)\r\n    steps = np.array(steps)\r\n\r\n    labels = []\r\n    for i in range(time_steps):\r\n        label = random.randint(0, num_classes - 1)\r\n        labels.append([label])\r\n    labels = np.array(labels)\r\n\r\n    input_data = []\r\n    input_labels = []\r\n\r\n    for i in range(1000):\r\n        input_data.append(steps)\r\n        input_labels.append(labels)\r\n\r\n    return np.array(input_data), np.array(input_labels)\r\n\r\nif __name__ == '__main__':\r\n    data, labels = generate_data()\r\n    model = build_graph()\r\n    model.fit(data, labels, 10, 100, validation_split= 0.5)\r\n```\r\n", "comments": ["@LukeBolly I am able to reproduce the issue on colab with Tensorflow-gpu 1.13.1 by executing multiple times.", "It turns out the running mean and variance aren't being updated:\r\nif you add these lines after the fit\r\n```\r\n    weights = model.layers[1].layer.layers[2].get_weights()\r\n    print('\\ngamma:\\t\\t' + str(weights[0][0]) + '\\nrunning mean:\\t\\t' + str(weights[2][0]) + '\\n')\r\n    print('beta:\\t' + str(weights[1][0]) + '\\nrunning variance:\\t' + str(weights[3][0]) + '\\n')\r\n```\r\n\r\nyou'll get the output:\r\n```\r\ngamma:\t\t1.1386212\r\nrunning mean:\t\t0.0\r\n\r\nbeta:\t0.06730438\r\nrunning variance:\t1.0\r\n```\r\n\r\nHere are some possibly related issues:\r\nhttps://github.com/keras-team/keras/issues/7466#issuecomment-500002720\r\nhttps://github.com/keras-team/keras/issues/12400\r\nhttps://github.com/keras-team/keras/pull/7467\r\n\r\nIf you've got an idea how I could work around this in the meantime, it would be much appreciated.", "The running mean and variance update in 1.14, though the results still don't seem right\r\n```\r\nEpoch 1/5\r\nloss: 1.2191 - sparse_categorical_accuracy: 0.5640 - val_loss: 0.9704 - val_sparse_categorical_accuracy: 0.6000\r\nEpoch 2/5\r\nloss: 0.2219 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.9911 - val_sparse_categorical_accuracy: 0.4000\r\nEpoch 3/5\r\nloss: 0.0514 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0707 - val_sparse_categorical_accuracy: 0.4000\r\nEpoch 4/5\r\nloss: 0.0236 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0686 - val_sparse_categorical_accuracy: 0.4000\r\nEpoch 5/5\r\nloss: 0.0139 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0246 - val_sparse_categorical_accuracy: 0.4000\r\n\r\ngamma:\t\t\t1.1039916\r\nrunning mean:\t\t0.18822414\r\n\r\nbeta:\t\t\t0.15830743\r\nrunning variance:\t0.63675237\r\n```\r\n\r\nGiven that the Momentum value is basically 0, validation_accuracy should be roughly equal to train_accuracy", "It turns out momentum is more complicated than I thought. A value of 0.5 makes the train and validation accuracies match.\r\n\r\nUsing a value close to 0 causes the moving averages to overshoot, and you end up with something like this:\r\n```\r\nrunning mean:\t\t1.0374619\t\trunning variance:\t-0.82485986\r\nrunning mean:\t\t0.010333061\t\trunning variance:\t0.9818245\r\nrunning mean:\t\t1.0272316\t\trunning variance:\t-0.80686545\r\nrunning mean:\t\t0.020461679\t\trunning variance:\t0.9640094\r\nrunning mean:\t\t1.017204\t\trunning variance:\t-0.7892276\r\nrunning mean:\t\t0.03038919\t\trunning variance:\t0.94654715\r\nrunning mean:\t\t1.0073754\t\trunning variance:\t-0.7719393\r\nrunning mean:\t\t0.040119976\t\trunning variance:\t0.9294311\r\nrunning mean:\t\t0.99774146\t\trunning variance:\t-0.7549938\r\nrunning mean:\t\t0.04965794\t\trunning variance:\t0.9126543\r\n```\r\nwhich explains the unstable validation loss.\r\n\r\nLong story short, don't use a Momentum below 0.5 and don't use BatchNorm inside TimeDistributed in 1.13.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30109\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30109\">No</a>\n"]}, {"number": 30108, "title": "CPU memory is not released after calling sess.run and closing session", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): not sure\r\n- TensorFlow version (use command below): v1.13.1-0-g6612da8\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0\r\n- GPU model and memory: Tesla V100, 16130MiB\r\n\r\n**Describe the current behavior**\r\nI observed increase of cpu memory usage (and not releasing memory) when fetching big tensors with `sess.run` from gpu and also not releasing cpu memory when closing session and resetting graph.\r\n\r\nI attached example and memory profiler logs. I tested 3 situations:\r\n1) Create two variables on gpu and fetch them in one `sess.run` call: memory increases by 1057MiB and is not released after closing session and resetting graph)\r\n2) Create two variables on gpu and fetch them one by one with `sess.run` call: memory increases by 528MiB and is not released after closing session and resetting graph)\r\n3) Create two variables on \u0441pu and fetch them: memory isn\u2019t increased and released after closing session.\r\n\r\n**Describe the expected behavior**\r\nMemory is released after `sess.run` and closing session, like in case 3 above but for gpu.\r\n\r\n**Code to reproduce the issue**\r\nhttps://gist.github.com/fgvbrt/7e9163267e1bc1fc501916125e09cd6d\r\n\r\n**Other info / logs**\r\nLogs with run commands for 3 situation above:\r\n[profile.txt](https://github.com/tensorflow/tensorflow/files/3323401/profile.txt)\r\n", "comments": ["Hello, I just wanted to check if anyone had a chance to look at this error?\r\nI am working on model pruning and to fully utilize pruned architecture I rebuild graph during training. And this error is causing a lot of problems and inconvenience", "Since we no longer support Tensorflow 1.x issues, could you please update your code in Tensorflow 2.x and let us know if you are still facing issue, Thank you!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30108\">No</a>\n"]}, {"number": 30107, "title": "Make tape only watch the tensor with the floating dtype", "body": "This PR fixes #28248 by making the tape only watch the tensor with the floating dtype and removes the log info that may make users confused.", "comments": ["@alextp Thanks for your explanation! I rewrite this PR. Could you please have a look at the changes (https://github.com/tensorflow/tensorflow/pull/30107/commits/fbc212cfd666b205abf01bc1d79e1ea7844389f4) when you get a chance?", "Oops, yes.\n\nOn Thu, Oct 17, 2019 at 10:28 PM RJ Skerry-Ryan <notifications@github.com>\nwrote:\n\n> *@rryan* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/script_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/30107#discussion_r336323652>\n> :\n>\n> >        for tensor in args:\n> -        tape.watch(tensor)\n> +        for t in nest.flatten(tensor):\n> +          if t.dtype.is_floating:\n>\n> It looks like we should have also watched complex types here?\n> (context: #32774 <https://github.com/tensorflow/tensorflow/issues/32774>)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30107?email_source=notifications&email_token=AAABHRIRFKOJBEYDQNWGYVDQPFCOLA5CNFSM4H3D4Z52YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCIMZ73A#pullrequestreview-303669228>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAABHRN2YK4EDIL7AGJAJYDQPFCOLANCNFSM4H3D4Z5Q>\n> .\n>\n\n\n-- \n - Alex\n", "Can you send a PR?\n\nOn Tue, Oct 29, 2019 at 10:48 AM Alexandre Passos <apassos@google.com>\nwrote:\n\n> Oops, yes.\n>\n> On Thu, Oct 17, 2019 at 10:28 PM RJ Skerry-Ryan <notifications@github.com>\n> wrote:\n>\n>> *@rryan* commented on this pull request.\n>> ------------------------------\n>>\n>> In tensorflow/python/ops/script_ops.py\n>> <https://github.com/tensorflow/tensorflow/pull/30107#discussion_r336323652>\n>> :\n>>\n>> >        for tensor in args:\n>> -        tape.watch(tensor)\n>> +        for t in nest.flatten(tensor):\n>> +          if t.dtype.is_floating:\n>>\n>> It looks like we should have also watched complex types here?\n>> (context: #32774 <https://github.com/tensorflow/tensorflow/issues/32774>)\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/30107?email_source=notifications&email_token=AAABHRIRFKOJBEYDQNWGYVDQPFCOLA5CNFSM4H3D4Z52YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCIMZ73A#pullrequestreview-303669228>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AAABHRN2YK4EDIL7AGJAJYDQPFCOLANCNFSM4H3D4Z5Q>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n"]}, {"number": 30106, "title": "Fix math type setting in CudnnConvolutionDescriptor", "body": "The ctor of `CudnnConvolutionDescriptor` sets its  `cudnnMathType` to `CUDNN_TENSOR_OP_MATH` by default.  Within `DoConvolve` in `cuda_dnn.cc`, the `CudnnConvolutionDescriptor` has its math type set to `CUDNN_TENSOR_OP_MATH` irrespective of what the `AlgorithmDesc` says  because a new `CudnnConvolutionDescriptor` is created and passed to be used in the cudnn API call. The idea was to use the same ref of `CudnnConvolutionDescriptor` that gets set correctly while computing the workspace size. However, based on the current design this doesn't happen. Currently, the workspace gets computed with `use_tensor_math_op` flag (say) `false` but the convolution actually occurs with the flag set to `true` (Note: `use_tensor_math_op` is the flag that sets the math type to either `CUDNN_TENSOR_OP_MATH` or `CUDNN_DEFAULT_MATH`). This works most of the times but there are some exceptions. For example, in mobilenet, convolution with stride 2 picks up a slower algorithm since cudnn call fails due to this mismatch. \r\nIt should be noted that `DoFusedConvolveImpl` however, sets the flag correctly since the same  `CudnnConvolutionDescriptor` ref is used for workspace computation and the actual convolution. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30106) for more info**.\n\n<!-- need_sender_cla -->", "> I signed it!\r\n\r\n", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30106) for more info**.\n\n<!-- ok -->", "Thanks for your contribution , let me know once it is ready."]}, {"number": 30105, "title": "Dataset.cache() Followed by Dataset.zip() Throws AlreadyExistsError", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Mojave 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pipenv install --pre tensorflow==2.0.0-beta1`\r\n- TensorFlow version (use command below): `2.0.0-beta1`\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n`1v2.0.0-beta0-16-g1d91213fe7`\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n1. Cache a dataset\r\n2. Derive other datasets from it\r\n3. Zip the derived datasets into a single dataset\r\n4. See cache error\r\n\r\n```\r\nAlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists\r\n```\r\n\r\n**Describe the expected behavior**\r\nOnly cache the dataset once and not throw an error.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef map_file_to_xy_dataset(filename, params):\r\n    # Generate a dataset from the filename\r\n    csv_dataset = tf.data.experimental.CsvDataset(...)\r\n\r\n    # Cache at unique file location (i.e filename)\r\n    cached_dataset = csv_dataset.cache(...)\r\n\r\n    # Generate x dataset (all but id column)\r\n    x_dataset = cached_dataset.map(lambda *x: x[0:-1])\r\n\r\n    # Generate y dataset (last column of each file)\r\n    y_dataset = cached_dataset.map(lambda *x: x[-1])\r\n\r\n    # ZIP the X, Y datasets \r\n    # This will throw AlreadyExistsError \r\n    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\r\n\r\n    return xy_dataset\r\n\r\n\r\ndef generate_dataset():\r\n    filenames_dataset = tf.data.Dataset.list_files(data_glob)\r\n\r\n    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)\r\n\r\n    return dataset\r\n\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I have tried to reproduce with below code snippet in Colab with TF CPU version 2.0beta1 and was not able to reproduce the error. \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef map_file_to_xy_dataset(filename):\r\n    # Generate a dataset from the filename\r\n    csv_dataset = tf.data.experimental.CsvDataset(\"Iris.csv\",[\"float32\",\"float32\",\"float32\",\"float32\",\"float32\"])\r\n\r\n    # Cache at unique file location (i.e filename)\r\n    cached_dataset = csv_dataset.cache(\"Iris.csv\")\r\n\r\n    # Generate x dataset (all but id column)\r\n    x_dataset = csv_dataset.map(lambda *x: x[0:-1])\r\n\r\n    # Generate y dataset (last column of each file)\r\n    y_dataset = csv_dataset.map(lambda *x: x[-1])\r\n\r\n    # ZIP the X, Y datasets \r\n    # This will throw AlreadyExistsError \r\n    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\r\n\r\n    return xy_dataset\r\n\r\n\r\ndef generate_dataset():\r\n    filenames_dataset = tf.data.Dataset.list_files(\"Iris.csv\")\r\n\r\n    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)\r\n\r\n    return dataset\r\n\r\ndataset=generate_dataset()\r\ndataset\r\n```\r\nPlease let us know if we are missing out something. Thanks!", "@achandraa Thanks for looking into this! Did you iterate over the dataset? I'm still able to reproduce with your code by adding the following:\r\n\r\n```python\r\nfor x,y in dataset:\r\n    print(x,y)\r\n```\r\n\r\nIf this still doesn't work, the only other difference is the `filenames_dataset` I'm using has multiple files. ", "@devstein I am not able to reproduce the issue with above code and where to add these two lines \r\n```\r\nfor x,y in dataset:\r\n    print(x,y) . \r\n\r\n```Can you help us to reproduce the issue. Thanks!", "@gadagashwini Apologies there was a mistake in the demo code, it wasn't mapping over the `cached_dataset`. Here is the fixed version below:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef map_file_to_xy_dataset(filename):\r\n    # Generate a dataset from the filename\r\n    csv_dataset = tf.data.experimental.CsvDataset(\"Iris.csv\",[\"float32\",\"float32\",\"float32\",\"float32\",\"float32\"])\r\n\r\n    # Cache at unique file location (i.e filename)\r\n    cached_dataset = csv_dataset.cache(\"Iris.csv\")\r\n\r\n    # Generate x dataset (all but id column)\r\n    x_dataset = cached_dataset.map(lambda *x: x[0:-1])\r\n\r\n    # Generate y dataset (last column of each file)\r\n    y_dataset = cached_dataset.map(lambda *x: x[-1])\r\n\r\n    # ZIP the X, Y datasets \r\n    # This will throw AlreadyExistsError \r\n    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\r\n\r\n    return xy_dataset\r\n\r\n\r\ndef generate_dataset():\r\n    filenames_dataset = tf.data.Dataset.list_files(\"Iris.csv\")\r\n\r\n    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)\r\n\r\n    return dataset\r\n\r\ndataset=generate_dataset()\r\ndataset\r\n```\r\n\r\n\r\nOtherwise did you try using a `tf.data.Dataset.list_files(\"*.csv\")` where there are multiples CSV files in the file path? This is the only difference between my code and the example ", "@devstein Tried reproducing the issue and I received error saying `<DatasetV1Adapter shapes: (((), (), (), ()), ()), types: ((tf.float32, tf.float32, tf.float32, tf.float32), tf.float32)>`. Thanks!", "@gadagashwini What line did you receive this error? Did you try \r\n\r\n```python\r\nfor x,y in dataset:\r\n  print(x,y)\r\n```", "@devstein @gadagashwini This error message `AlreadyExistsError: There appears to be a concurrent caching iterator running - cache lockfile already exists` looks reasonable. It is caused by that multiple datasets are trying to be cached in the same file. You can change the line `cached_dataset = csv_dataset.cache(\"Iris.csv\")` to be `cached_dataset = csv_dataset.cache(\"cached_\" + filename)` so that each cached dataset has a unique file name.\r\n\r\n", "@feihugis I am caching to a unique location for every file. This is my exact code for generating each filename:\r\n\r\n```python\r\ndef generate_cache_filename(filename, directory):\r\n    filename_no_slashes = tf.strings.regex_replace(filename, r\"\\/\", \"_\")\r\n\r\n    cache_filename = tf.strings.join([directory, filename_no_slashes],\r\n                                     separator=\"/\")\r\n\r\n    return cache_filename\r\n``` \r\n\r\nAlso for context, this caching code doesn't throw an error unless I call `Dataset.zip` on the two derived datasets ", "@devstein Could you print the filenames used for the cache to make sure they are unique?\r\n\r\nThe following code works for me:\r\n```Python\r\n  def map_file_to_xy_dataset(filename):\r\n    # Generate a dataset from the filename\r\n    csv_dataset = tf.data.experimental.CsvDataset(filename,\r\n                                                  [\"float32\", \"float32\",\r\n                                                   \"float32\", \"float32\",\r\n                                                   \"float32\"],\r\n                                                  header=True,)\r\n\r\n    # Cache at unique file location (i.e filename)\r\n    cached_dataset = csv_dataset.cache(\"cached_\"+filename)\r\n\r\n    # Generate x dataset (all but id column)\r\n    x_dataset = csv_dataset.map(lambda *x: x[0:1])\r\n\r\n    # Generate y dataset (last column of each file)\r\n    y_dataset = csv_dataset.map(lambda *x: x[-1])\r\n\r\n    # ZIP the X, Y datasets\r\n    # This will throw AlreadyExistsError\r\n    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\r\n\r\n    return xy_dataset\r\n\r\n  def generate_dataset(file_pattern):\r\n    filenames_dataset = tf.data.Dataset.list_files(file_pattern)\r\n    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)\r\n    return dataset\r\n\r\n  dataset = generate_dataset(\"/Users/fei/Desktop/csv_test/*.csv\")\r\n  for x, y in dataset:\r\n    print(x, y)\r\n```", "@feihugis \r\n\r\nCan you re-run with the corrected code below? I run into the issue when mapping over the `cached_dataset` not the `csv_dataset`\r\n\r\n```python\r\n  def map_file_to_xy_dataset(filename):\r\n    # Generate a dataset from the filename\r\n    csv_dataset = tf.data.experimental.CsvDataset(filename,\r\n                                                  [\"float32\", \"float32\",\r\n                                                   \"float32\", \"float32\",\r\n                                                   \"float32\"],\r\n                                                  header=True,)\r\n\r\n    # Cache at unique file location (i.e filename)\r\n    cached_dataset = csv_dataset.cache(\"cached_\"+filename)\r\n\r\n    # Generate x dataset (all but id column)\r\n    x_dataset = cached_dataset.map(lambda *x: x[0:1])\r\n\r\n    # Generate y dataset (last column of each file)\r\n    y_dataset = cached_dataset.map(lambda *x: x[-1])\r\n\r\n    # ZIP the X, Y datasets\r\n    # This will throw AlreadyExistsError\r\n    xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\r\n\r\n    return xy_dataset\r\n\r\n  def generate_dataset(file_pattern):\r\n    filenames_dataset = tf.data.Dataset.list_files(file_pattern)\r\n    dataset = filenames_dataset.flat_map(map_file_to_xy_dataset)\r\n    return dataset\r\n\r\n  dataset = generate_dataset(\"/Users/fei/Desktop/csv_test/*.csv\")\r\n  for x, y in dataset:\r\n    print(x, y)\r\n\r\n```\r\n\r\nThanks for the help!", "Oh, my bad. Now I got this error `tensorflow.python.framework.errors_impl.NotFoundError: cached_/Users/fei/Desktop/csv_test/3.csv_0.lockfile; No such file or directory [Op:IteratorGetNextSync]`. It is different from yours. \r\n\r\nI guess the problem you met is caused by the cache file name. [After changing `cached_dataset = csv_dataset.cache(\"cached_\"+filename)` to `cached_dataset = csv_dataset.cache(\"cached_\")`, I met the same error with yours.]\r\n\r\n I am looking into the error I met.", "@feihugis Thanks! ", "@devstein Were you able to resolve the issue? Thanks!", "@gadagashwini @jsimsa I am working on the fix. The issue can be reproduced by the simplified code below:\r\n```\r\n  csv_dataset = tf.data.Dataset.range(0, 1000, 1)\r\n  cached_dataset = csv_dataset.cache(\"cache_test\")\r\n  x_dataset = cached_dataset.map(lambda x: x)\r\n  y_dataset = cached_dataset.map(lambda x: x * 2)\r\n  xy_dataset = tf.data.Dataset.zip((x_dataset, y_dataset))\r\n\r\n  for x, y in xy_dataset:\r\n    print(x, y)\r\n```\r\n\r\n@devstein The workaround solution is to change `cached_dataset = csv_dataset.cache(\"cache_test\")` to be `cached_dataset = csv_dataset.cache()` which will use the memory cache instead of file cache.", "This behavior is expected because your input pipeline definition is trying to create the same cache twice (once for each component of the zip) and it will not be reused.\r\n\r\nInstead, you should do the following (which will also be more efficient):\r\n\r\n```\r\ndef map_file_to_dataset(filename, params):\r\n    # Generate a dataset from the filename\r\n    dataset = tf.data.experimental.CsvDataset(...)\r\n\r\n    return dataset.cache(...).map(lambda *x: (x[0:-1], x[-1]))\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30105\">No</a>\n", "@jsimsa I separated the two operations because I window the `x_dataset` afterward and not the `y_dataset`, but I'll switch it to split the datasets after I window. Appreciate the help! "]}, {"number": 30104, "title": "[WIP] Create a gradient function for RGB to HSV", "body": "", "comments": []}, {"number": 30103, "title": "Limit the number of statements in exception raising test blocks to 1.", "body": "We may locate the exact statement where the exception is raiased.", "comments": ["@karmel can you please review new changes ?", "@autoih thank you , there are still conflicts to resolve.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30103) for more info**.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30103) for more info**.\n\n<!-- cla_yes -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30103) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30103) for more info**.\n\n<!-- ok -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30103) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30103) for more info**.\n\n<!-- ok -->", "I've tried to resolve the conflicts in Line `669-672` and `694-697` (remove them), but once I removed them, CLA issue raises up. ", "@autoih are there any other contributors to this PR , if yes they need to sign CLA.", "Can one of the admins verify this patch?", "> @autoih are there any other contributors to this PR , if yes they need to sign CLA.\r\n\r\nThanks @rthadur, I think it may be the username. The other PR works okay. ", "@autoih can you please resolve conflicts ?", "@rthadur, I've tried before, but after I removed the conflicts, then I got no CLA. In this case, should I close this pr and create a new one? What do you think? ", "Please use a new PR ", "Thanks @rthadur, the new one is https://github.com/tensorflow/tensorflow/pull/31997"]}, {"number": 30102, "title": "Fix the deadlock when freeing the shared resources", "body": "This PR tries to fix #29695. \r\n\r\nThe root cause is that for a shared iterator, the resource is not private to the kernel, so the resource cannot be released by [~IteratorHandleOp](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/iterator_ops.cc#L438-L450), and the [ResourceMgr::Clear()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_mgr.cc#L109-L118) needs to be called to free the shared resources. \r\n\r\nFor the case in #29695, the session close() tries to release the shared resources by calling `ResourceMgr::Clear()`, during which, as the generator dataset did not finish the iteration yet, it needs to delete the python generator by calling the finalizing function [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/generator_dataset_op.cc#L95). However,  when running the finalizing function, [the done function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/captured_function.cc#L632) triggers the [ResourceMgr::CleanUp()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_mgr.cc#L218-L243) which causes the deadlock with `ResourceMgr::Clear()`.\r\n\r\nHere is the Traceback:\r\n```\r\nframe #0: 0x00007fff7565686a libsystem_kernel.dylib`__psynch_cvwait + 10\r\n    frame #1: 0x00007fff7571556e libsystem_pthread.dylib`_pthread_cond_wait + 722\r\n    frame #2: 0x00007fff72750a0a libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18\r\n    frame #3: 0x000000011c13c61b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p(nsync::nsync_semaphore_s_*) + 123\r\n    frame #4: 0x000000011c13a578 libtensorflow_framework.so`nsync::nsync_mu_lock_slow_(nsync::nsync_mu_s_*, nsync::waiter*, unsigned int, nsync::lock_type_s*) + 296\r\n    frame #5: 0x000000011c13a650 libtensorflow_framework.so`nsync::nsync_mu_lock(nsync::nsync_mu_s_*) + 80\r\n    frame #6: 0x000000011bbe9cf7 libtensorflow_framework.so`tensorflow::ResourceMgr::Cleanup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 39\r\n    frame #7: 0x0000000111fc5666 _pywrap_tensorflow_internal.so`std::__1::__function::__func<tensorflow::data::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*)::$_4, std::__1::allocator<tensorflow::data::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*)::$_4>, void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>::operator()(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 54\r\n    frame #8: 0x0000000111fc4ca6 _pywrap_tensorflow_internal.so`tensorflow::ScopedStepContainer::~ScopedStepContainer() + 38\r\n    frame #9: 0x0000000111fc36a0 _pywrap_tensorflow_internal.so`tensorflow::data::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 976\r\n    frame #10: 0x0000000111eb1085 _pywrap_tensorflow_internal.so`tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 101\r\n    frame #11: 0x0000000111eb0d3e _pywrap_tensorflow_internal.so`tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14\r\n    frame #12: 0x0000000111eaea89 _pywrap_tensorflow_internal.so`tensorflow::data::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105\r\n    frame #13: 0x00007fff72753d42 libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 40\r\n    frame #14: 0x0000000111ec3755 _pywrap_tensorflow_internal.so`tensorflow::data::IteratorResource::~IteratorResource() + 149\r\n    frame #15: 0x0000000111ec366e _pywrap_tensorflow_internal.so`tensorflow::data::IteratorResource::~IteratorResource() + 14\r\n    frame #16: 0x000000011bbe7fa4 libtensorflow_framework.so`tensorflow::ResourceMgr::Clear() + 116\r\n    frame #17: 0x00000001139f362f _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 655\r\n    frame #18: 0x00000001139f3aae _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14\r\n    frame #19: 0x00007fff72753d42 libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 40\r\n    frame #20: 0x00000001112ceb7f _pywrap_tensorflow_internal.so`tensorflow::SessionRef::Close() + 223\r\n    frame #21: 0x000000011147f40b _pywrap_tensorflow_internal.so`TF_CloseSession + 27\r\n```\r\n\r\ncc: @jsimsa \r\n", "comments": ["@alextp could you please take a look, the ResourceMgr change is not specific to tf.data", "+1 to Jiri's suggestion\n\nOn Thu, Jun 27, 2019 at 11:07 AM Jiri Simsa <notifications@github.com>\nwrote:\n\n> *@jsimsa* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/core/kernels/data/captured_function.cc\n> <https://github.com/tensorflow/tensorflow/pull/30102#discussion_r298299873>\n> :\n>\n> > @@ -629,7 +629,9 @@ Status InstantiatedCapturedFunction::RunInstantiated(\n>    FunctionLibraryRuntime::Options f_opts;\n>    ScopedStepContainer step_container(\n>        f_opts.step_id, [this](const string& name) {\n> -        lib_->device()->resource_manager()->Cleanup(name).IgnoreError();\n> +        if (!lib_->device()->resource_manager()->IsClearingResource()) {\n>\n> Looking at the code base, ScopedStepContainer is used by ops that create\n> resources whose life-time is scoped within a step and the method that is\n> passed to the ScopedStepContainer is used to cleanup these resources.\n> However, the argument to the cleanup method is that name of the container\n> (so that all resources it contains are cleaned up), not a resource handle.\n>\n> It seems that the ResourceMgr::Clear does not expect that a destructor it\n> triggers could in turn call methods on the ResourceMgr object.\n>\n> I think a better way to fix this problem would be to make modify Clear to\n> move the contents of containers_ to a temporary local variable under the\n> mutex (and then perform that actual unref-ing and deallocation without\n> holding the lock).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30102?email_source=notifications&email_token=AAABHRNBVPKPRNK4I6FLXTTP4T6UPA5CNFSM4H3C7JVKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB44OXPI#discussion_r298299873>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRI52363VVV6ILW4I3LP4T6UPANCNFSM4H3C7JVA>\n> .\n>\n\n\n-- \n - Alex\n", "@jsimsa @alextp Thanks for your suggestions! This PR has been rewritten. Could you please have a look at the changes (https://github.com/tensorflow/tensorflow/pull/30102/commits/d32693fd0479d1a0c38948703b483c2f6d25038e) when you get a chance?", "This PR is revised base on Jiri's suggestion. Could you (@jsimsa @alextp) please take another look (https://github.com/tensorflow/tensorflow/pull/30102/commits/a41b23415c75b562535273139e8722b546525405)?", "@feihugis  can you please check build failures ?", "@rthadur The failures seem to be caused by this PR (https://github.com/tensorflow/tensorflow/commit/c205be168295893f7f49be3ed8c9d5d14e0307a4), which has been reverted a few hours ago. Could you please re-trigger the tests? ", "@alextp Thanks for triggering the tests. Ubuntu Sanity check failed but it passed last time. As the log link is not available, could you help check the log?", "@alextp The internal checks failed. Could you help paste the log details here? Thanks!", "the internal checks are still running"]}, {"number": 30101, "title": "Tf-TRT minor test fixes", "body": "- Check if checkpoints are available in TF-TRT quantization test\r\n- Use constant input test tensors instead of random tensors Otherwise the test sometimes fails depending on the generated random values.", "comments": ["Hi @pooyadavoodi, would you please fix this error?\r\n```\r\ntensorflow/python/compiler/tensorrt/test/const_broadcast_test.py:21: [W0611(unused-import), ] Unused numpy imported as np\r\n```", "> Hi @pooyadavoodi, would you please fix this error?\r\n> \r\n> ```\r\n> tensorflow/python/compiler/tensorrt/test/const_broadcast_test.py:21: [W0611(unused-import), ] Unused numpy imported as np\r\n> ```\r\n\r\nDone", "> * Increase tolerance of TF-TRT tests for fp32 and fp16\r\n> * Check if checkpoints are available in TF-TRT quantization test\r\n> * Use constant input test tensors instead of random tensors Otherwise the test sometimes fails depending on the generated\r\n> * random values.\r\n\r\n@pooyadavoodi could you please update the description?", "> > * Increase tolerance of TF-TRT tests for fp32 and fp16\r\n> > * Check if checkpoints are available in TF-TRT quantization test\r\n> > * Use constant input test tensors instead of random tensors Otherwise the test sometimes fails depending on the generated\r\n> > * random values.\r\n> \r\n> @pooyadavoodi could you please update the description?\r\n\r\nDone"]}, {"number": 30100, "title": "nccl.reduce_sum() throws an error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes script for testing nccl.reduce_sum, attached below.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: Python 3.6\r\n- CUDA/cuDNN version: 10.1 / 7\r\n- GPU model and memory: v100 16Gb\r\n\r\n**Describe the current behavior**\r\nnccl.reduce_sum() throws and error about feed_devices or fetch_devices not being found in the graph. nccl.all_reduce() works.\r\n\r\n**Describe the expected behavior**\r\nExpect to get a reduced tensor. \r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import nccl_ops as nccl\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n    a = tf.constant([1,2,3,4,5], dtype=tf.float32)\r\n\r\nwith tf.device('/gpu:1'):\r\n    b = tf.constant([6,7,8,9,10], dtype=tf.float32)\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n    c = nccl.reduce_sum([a, b])\r\n\r\nsess = tf.Session()\r\nprint(sess.run(c)) \r\n```\r\n\r\n**Other info / logs**\r\n```\r\nubuntu@ip-172-31-26-69:~/tensorpack/examples/ResNet$ python test_nccl.py\r\n2019-06-24 20:41:57.077685: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-24 20:41:57.803050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-24 20:41:57.828402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-24 20:41:57.841325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-24 20:41:57.852582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-06-24 20:41:57.853980: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4a9c130 executing computations on platform CUDA. Devices:\r\n2019-06-24 20:41:57.854028: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2019-06-24 20:41:57.854053: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2019-06-24 20:41:57.854077: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2019-06-24 20:41:57.854101: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2019-06-24 20:41:57.860344: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300060000 Hz\r\n2019-06-24 20:41:57.865200: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x4bbd9f0 executing computations on platform Host. Devices:\r\n2019-06-24 20:41:57.865270: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-24 20:41:57.866085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:00:1b.0\r\ntotalMemory: 15.75GiB freeMemory: 6.38GiB\r\n2019-06-24 20:41:57.866225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:00:1c.0\r\ntotalMemory: 15.75GiB freeMemory: 3.71GiB\r\n2019-06-24 20:41:57.866329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 2 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:00:1d.0\r\ntotalMemory: 15.75GiB freeMemory: 3.71GiB\r\n2019-06-24 20:41:57.866442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 3 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 15.75GiB freeMemory: 6.68GiB\r\n2019-06-24 20:41:57.866498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1, 2, 3\r\n2019-06-24 20:41:57.872295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-06-24 20:41:57.872370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 2 3\r\n2019-06-24 20:41:57.872395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N Y Y Y\r\n2019-06-24 20:41:57.872413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   Y N Y Y\r\n2019-06-24 20:41:57.872425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 2:   Y Y N Y\r\n2019-06-24 20:41:57.872440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 3:   Y Y Y N\r\n2019-06-24 20:41:57.872826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6203 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\r\n2019-06-24 20:41:57.873247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 3496 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\r\n2019-06-24 20:41:57.873562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 3498 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\r\n2019-06-24 20:41:57.873856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 6496 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Tensor NcclReduce:0, specified in either feed_devices or fetch_devices was not found in the Graph\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_nccl.py\", line 17, in <module>\r\n    print(sess.run(c))\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/home/ubuntu/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Tensor NcclReduce:0, specified in either feed_devices or fetch_devices was not found in the Graph\r\n```", "comments": ["I could reproduce the issue with `tensorflow-gpu==1.14.0rc1` and `tensorflow-gpu==1.13.1`. Thanks!", "@dubey can you take a look?", "Adding @chsigg who is more familiar with nccl_ops.  Christian, do you have any idea why this is happening?", "Hi Vishnuvardhan, could you try to wrap the output tensor `c` with `array_ops.identity()`?", "@chsigg when I modify my script to: `c = tf.identity(nccl.reduce_sum([a, b]))` it works. Thanks for the tip!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30100\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30100\">No</a>\n"]}, {"number": 30099, "title": "deeplab on ios+real time segmentation", "body": "i tried to implement deeplab model on ios but what i got is very slow capture \r\nios:12.3.1\r\niphone:6+\r\nxcode:10.1\r\ni tried to improve the fps by:\r\ndesiredFrameRate = 30\r\n```\r\ncaptureDevice.activeVideoMinFrameDuration = CMTimeMake(1, Int32(desiredFrameRate))\r\n captureDevice.activeVideoMaxFrameDuration = CMTimeMake(1, Int32(desiredFrameRate))\r\n```\r\ni have changed desiredFrameRate to 60 and 240 but i got an error\r\n\r\n> uncaught exception of type NSException\r\n\r\ndid anyone know how i can improve my result", "comments": []}, {"number": 30098, "title": "usage example added in extract glimpse", "body": "Added usage example in extract glimpse. Issue raised on https://github.com/tensorflow/tensorflow/issues/30097", "comments": []}, {"number": 30097, "title": "[TF 2.0 API Docs] tf.image.extract_glimpse", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/extract_glimpse\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Raises listed and defined\r\n\r\nRaises are not listed and defined\r\n\r\n### Usage example\r\n\r\nNo usage example is given\r\n\r\n### Pull Request\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/30098", "comments": ["Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!"]}, {"number": 30096, "title": "Keras load_model fails to load models with BatchNormalization layer when saved in non-eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): \r\ntf version = 2.0.0-dev20190622\r\ntf git version = v1.12.1-4759-g9856697d8b\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nThis issue is very similar to #3628, but it only happens in non-eager mode in TF 2.0.\r\nYou can save a model (having batch norm layer) in **non-eager mode** successfully. However, you cannot load the model in any mode.  Load model fails with:\r\n```\r\nValueError: Node 'AssignMovingAvg/sub/x' expects to be colocated with unknown node 'bn_layer/moving_mean'\r\n```\r\nSeems like the model is not saved correctly.\r\n\r\n**Describe the expected behavior**\r\nSave model should produce a correct SavedModel in non-eager mode so that it can be loaded later.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n## model architecture\r\ninput_layer = tf.keras.Input(shape=(28, 28, 1), name='image_input')\r\nlayer = tf.keras.layers.ZeroPadding2D(padding=(3, 3), name='initial_padding')(input_layer)\r\n# add convolutional layer\r\nlayer = tf.keras.layers.Conv2D(\r\n    filters=16,\r\n    kernel_size=8,\r\n    padding='same',\r\n    name='conv_layer'\r\n)(layer)\r\n# batch normalization\r\nlayer = tf.keras.layers.BatchNormalization(axis=3, name='bn_layer')(layer)\r\n# activation\r\nlayer = tf.keras.layers.Activation('relu', name='activation_layer')(layer)\r\n# down sample\r\nnet = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(layer)\r\n# flatten\r\nnet = tf.keras.layers.Flatten(name='flatten_layer')(net)\r\n# dense layer with ReLU-activation.\r\nnet = tf.keras.layers.Dense(64, activation='relu', name='dense_layer')(net)\r\n# dropout layer\r\nnet = tf.keras.layers.Dropout(0.2, name='dropout_layer')(net)\r\n# last fully-connected / dense layer with softmax-activation so it can be used for classification.\r\noutput_layer = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(net)\r\n# creating the model\r\nmodel = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='test1')\r\n\r\n# loading data\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n# add channel to x; also divide by 255 to normalize the data\r\nx_train = x_train.reshape(60000, 28, 28, 1)#.astype('float32') / 255\r\nx_test = x_test.reshape(10000, 28, 28, 1)#.astype('float32') / 255\r\n\r\n# create the training dataset\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n# shuffle, batch and prefetch for optimizing io reads\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(64).prefetch(1024)\r\n# create the validation dataset.\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n# batch and prefetch for optimizing io reads\r\nval_dataset = val_dataset.batch(64).prefetch(1024)\r\n\r\n# compile the model\r\nmodel.compile(\r\n    loss='sparse_categorical_crossentropy',\r\n    optimizer='rmsprop',\r\n    metrics=['accuracy']\r\n)\r\n\r\n# fit the model\r\nhistory = model.fit(\r\n    train_dataset,\r\n    validation_data=val_dataset,\r\n    epochs=2,\r\n    steps_per_epoch=50\r\n)\r\n\r\n# the returned \"history\" object holds a record of the loss values and metric values during training\r\nprint('\\nhistory dict:', history.history)\r\n\r\nmodel.save('mnist_model')\r\n\r\ndel model\r\n# recreate the exact same model purely from the file:\r\nmodel = tf.keras.models.load_model('mnist_model')\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0624 13:37:50.545702 140736272085888 deprecation.py:506] From /temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1624: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2019-06-24 13:37:51.383301: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-24 13:37:51.398677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f90837c85b0 executing computations on platform Host. Devices:\r\n2019-06-24 13:37:51.398709: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-24 13:37:51.728750: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nW0624 13:37:52.842435 140736272085888 deprecation.py:323] From /temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\nTrain on 50 steps, validate on 157 steps\r\nEpoch 1/2\r\n50/50 [==============================] - 4s 73ms/step - loss: 1.8029 - accuracy: 0.5256 - val_loss: 0.5635 - val_accuracy: 0.8345\r\nEpoch 2/2\r\n50/50 [==============================] - 3s 58ms/step - loss: 0.6426 - accuracy: 0.7937 - val_loss: 0.4020 - val_accuracy: 0.8787\r\n\r\nhistory dict: {'loss': [1.8028993177413941, 0.6426236051321029], 'accuracy': [0.525625, 0.79375], 'val_loss': [0.563538867861602, 0.40199054775249426], 'val_accuracy': [0.8345, 0.8787]}\r\n2019-06-24 13:38:02.261349: W tensorflow/python/util/util.cc:268] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nTraceback (most recent call last):\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 427, in import_graph_def\r\n    graph._c_graph, serialized, options)  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'AssignMovingAvg/sub/x' expects to be colocated with unknown node 'bn_layer/moving_mean'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"error_showcase_graph.py\", line 71, in <module>\r\n    model = tf.keras.models.load_model('mnist_model')\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\", line 142, in load_model\r\n    return saved_model_load.load(filepath, compile)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 86, in load\r\n    model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 506, in load_internal\r\n    export_dir)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py\", line 102, in __init__\r\n    super(KerasObjectLoader, self).__init__(*args, **kwargs)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py\", line 111, in __init__\r\n    meta_graph.graph_def.library))\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\", line 301, in load_function_def_library\r\n    copy, copy_functions=False)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/framework/function_def_to_graph.py\", line 64, in function_def_to_graph\r\n    importer.import_graph_def(graph_def, name=\"\")\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\", line 431, in import_graph_def\r\n    raise ValueError(str(e))\r\nValueError: Node 'AssignMovingAvg/sub/x' expects to be colocated with unknown node 'bn_layer/moving_mean'\r\n```\r\n", "comments": ["I have tried on Colab with TF version 2.0.0-dev20190622 and was able to reproduce it on only non eager mode.", "Thanks for reporting this issue! This should now be fixed with this change: https://github.com/tensorflow/tensorflow/commit/769900b011cf6a31f8e8f919e97b70900f9825d8#diff-5738cc9149537416bc9a07c7cfefae42", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30096\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30096\">No</a>\n", "Thank you @k-w-w \r\nI confirm that the issue is resolved in the tf-nightly"]}, {"number": 30095, "title": "Update Callback subclass in callbacks.py", "body": "See #29958 - Suggested improvements:\r\n\r\n- _Expanded the description:_\r\nTo make things clear, added more information so that a user knows they can use `Callback` as a custom callback during training, evaluating and inference when the existing built-in callback is not enough.\r\n\r\n- _Added a `Callback` example:_\r\nThis callback doc needed an example. It was adapted/modified from the Write Custom Callbacks [guide](https://www.tensorflow.org/beta/guide/keras/custom_callback) (r2.0b). Please note that `steps_per_epoch` in the `.fit()` method during training may be GPU memory hungry, so a larger batch size was used (128) on a training set (55000, Fashion MNIST) to mitigate it when testing this custom callback example.", "comments": ["> Doc review looks good to me, thanks!\r\n\r\n@lamberta can we undo \"ready to pull\" if it needs to be done, fixed \"X_train\" -> \"data\" thanks to @rchao 's eagle eye \ud83d\udc4d as per his earlier comment", "Can one of the admins verify this patch?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 30094, "title": "Tensorboard callback with histogram_freq=1 crashes after one epoch in non-eager mode ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):\r\ntf version = 2.0.0-dev20190622\r\ntf git version = v1.12.1-4759-g9856697d8b\r\n- Python version: 3.6.4\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\nCalling the Keras ```fit``` function in non-eager mode fails after one epoch, if specifying a tensorboard callback which has ```histogram_freq=1```. \r\n\r\n**Describe the expected behavior**\r\nI think the ```fit``` function should continue training without crashing.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# model architecture\r\ninputs = tf.keras.Input(shape=(784,), name='flattened_image')\r\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\r\nx = tf.keras.layers.Dense(64, activation='relu')(x)\r\noutputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\r\n# creating the model\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs, name='test1')\r\n\r\n# loading data\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\n# create the training dataset\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n# shuffle, batch and prefetch for optimizing io reads\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(64).prefetch(1024)\r\n# create the validation dataset.\r\nval_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\n# batch and prefetch for optimizing io reads\r\nval_dataset = val_dataset.batch(64).prefetch(1024)\r\n\r\n# compile the model\r\nmodel.compile(\r\n    loss='sparse_categorical_crossentropy',\r\n    optimizer='rmsprop',\r\n    metrics=['accuracy'],\r\n)\r\n\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n    log_dir='./tensorboard_logs/',\r\n    # enabling histogram will crash the learninig in non-eager mode\r\n    histogram_freq=1, # every epoch\r\n    write_images=True, # visualize model weights in image form\r\n    update_freq='batch', # this can be 'epoch' to make training faster (less logs)\r\n)\r\n\r\n# fit the model\r\nhistory = model.fit(\r\n    train_dataset,\r\n    validation_data=val_dataset,\r\n    callbacks=[tensorboard_callback],\r\n    epochs=2,\r\n    steps_per_epoch=50\r\n)\r\n\r\n# the returned \"history\" object holds a record of the loss values and metric values during training\r\nprint('\\nhistory dict:', history.history)\r\n```\r\n\r\n**Other info / logs**\r\nHere is the stack trace...\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0624 13:13:30.813938 140736272085888 deprecation.py:506] From /temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1624: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nIf using Keras pass *_constraint arguments to layers.\r\n2019-06-24 13:13:34.039928: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-06-24 13:13:34.055276: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fda14c310d0 executing computations on platform Host. Devices:\r\n2019-06-24 13:13:34.055294: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-06-24 13:13:35.799524: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\nW0624 13:13:43.409096 140736272085888 deprecation.py:323] From /temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:460: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nApply a constraint manually following the optimizer update step.\r\nTrain on 50 steps, validate on 157 steps\r\nEpoch 1/2\r\n30/50 [=================>............] - ETA: 0s - loss: 1.3660 - accuracy: 0.6500  Traceback (most recent call last):\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_resource_variable_ops.py\", line 570, in read_variable_op\r\n    \"dtype\", dtype)\r\ntensorflow.python.eager.core._FallbackException: This function does not handle the case of the path where all inputs are not already EagerTensors.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"error_showcase_tb.py\", line 50, in <module>\r\n    steps_per_epoch=50\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 669, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 669, in fit\r\n    steps_name='steps_per_epoch')\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\", line 444, in model_iteration\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\", line 296, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\", line 1612, in on_epoch_end\r\n    self._log_weights(epoch)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\", line 1691, in _log_weights\r\n    weight = K.get_value(weight)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 3016, in get_value\r\n    return x.numpy()\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 580, in numpy\r\n    return self.read_value().numpy()\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 633, in read_value\r\n    value = self._read_variable_op()\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\", line 611, in _read_variable_op\r\n    self._dtype)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_resource_variable_ops.py\", line 575, in read_variable_op\r\n    resource, dtype=dtype, name=name, ctx=_ctx)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_resource_variable_ops.py\", line 613, in read_variable_op_eager_fallback\r\n    attrs=_attrs, ctx=_ctx, name=name)\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 71, in quick_execute\r\n    raise e\r\n  File \"/temp/v36/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\", line 61, in quick_execute\r\n    num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: dense/kernel:0\r\n```", "comments": ["Same problem in tf 1.14.0", "I'm also having this problem! TensorFlow version 1.14, running on CPU", "@omalleyt12 could you take a look?  I'm not sure how Keras is supposed to behave here in TF 2.0 + non-eager mode.", "This issue still exists. Any updates?", "Same problem in tf 1.14.0", "the same problem in tf2.0.0", "Issue is replicating with Tf 2.2.rc0.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/2b70167d52f21e4247bbbd965997dafe/untitled451.ipynb). Thanks!", "@mahzoon Generally it is not recommended to combine 1.x and 2.x functionality. \r\nIn general, if you want to maintain v1 behavior, it's best to use the v1 apis as well: I updated first two lines of your code as follows.\r\n\r\n```\r\n#import tensorflow as tf\r\nimport tensorflow.compat.v1 as tf\r\n\r\n#tf.compat.v1.disable_eager_execution()\r\n```\r\nWith this change, everything works as expected. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/8c2f3b08d3ff0ff011676e4d0505f283/untitled50.ipynb). Thanks!\r\n\r\nPlease close the issue if this was resolved for you. Thanks!\r\n", "@mahzoon Can you please check my last comment? Thanks!\r\n\r\nIf this was resolved for you, then please close the issue. Thanks!", "@mahzoon I am closing this as this was resolved in recent `tf-nightly`. Please feel free to reopen if this was not resolved for you. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30094\">No</a>\n"]}, {"number": 30093, "title": "Compiling protos for Golang", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: v1.14.0\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem** I'm trying to compile the protos using `protoc` so I can import the MetaGraph Go struct. Some of the protos (tensorflow/core/protobuf/saved_object_graph.proto, tensorflow/core/protobuf/trackable_object_graph.proto, tensorflow/core/protobuf/struct.proto) that are depended on lack `option go _package`, so `protoc` generates Go files with different package names (ending in protobuf and tensorflow) in the same directory, making both packages non-importable. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. `protoc` on all protos in tensorflow/core/lib/core/\r\n2. `protoc` on all protos in tensorflow/core/framework/\r\n3. `protoc` on protos in tensorflow/core/protobuf: saver.proto, struct.proto, trackable_object_graph.proto, saved_object_graph.proto, meta_graph.proto, saved_model.proto\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Just to verify which OS are you using ? ", "I've tried this on Ubuntu and OSX", "@asimshankar Can you PTAL? Thanks!", "Unfortunately, I'm no longer working on TensorFlow. Perhaps @jhseu might be able to help.\r\nRegardless, seems like it should be fine to add the package statements like those added in #17262", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30093\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30093\">No</a>\n"]}, {"number": 30092, "title": "AttributeError: module 'tensorflow._api.v2.train' has no attribute 'FtrlOptimizer'", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/estimator/LinearClassifier\r\n\r\n## Description of issue (what needs changing):\r\n\r\n`FtrlOptimizer` is not accessible from `tf.train`:\r\n\r\n```python\r\n# Or estimator using the FTRL optimizer with regularization.\r\nestimator = LinearClassifier(\r\n    feature_columns=[categorical_column_a,\r\n                     categorical_feature_a_x_categorical_feature_b],\r\n    optimizer=tf.train.FtrlOptimizer(\r\n      learning_rate=0.1,\r\n      l1_regularization_strength=0.001\r\n    )\r\n    ### should be optimizer=tf.keras.optimizers.Ftrl(...)\r\n)\r\n\r\n> AttributeError: module 'tensorflow._api.v2.train' has no attribute 'FtrlOptimizer'\r\n```\r\n\r\n### Clear description\r\n\r\nN/A\r\n\r\n### Correct links\r\n\r\nN/A\r\n\r\n### Parameters defined\r\n\r\nN/A\r\n\r\n### Returns defined\r\n\r\nN/A\r\n\r\n### Raises listed and defined\r\n\r\nN/A\r\n\r\n### Usage example\r\n\r\nN/A\r\n\r\n### Request visuals, if applicable\r\n\r\nN/A\r\n\r\n### Submit a pull request?\r\n\r\nN/A\r\n", "comments": ["Can you please provide your full code snippet to reproduce the issue.Thanks!", "Sure, here's my MWE:\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\n> 2.0.0-beta1\r\n```\r\n\r\n```python\r\nfrom tensorflow.estimator import LinearClassifier\r\n\r\nfeature_columns = []\r\n\r\nfor feature_name in ['some_feature_a', 'some_feature_b']:\r\n    feature_columns.append(\r\n        tf.feature_column.numeric_column(feature_name, dtype=tf.float32)\r\n    )\r\n\r\nestimator = LinearClassifier(\r\n    feature_columns=feature_columns,\r\n    optimizer=tf.train.FtrlOptimizer(\r\n      learning_rate=0.1,\r\n      l1_regularization_strength=0.001\r\n    )\r\n)\r\n\r\n> AttributeError: module 'tensorflow._api.v2.train' has no attribute 'FtrlOptimizer'\r\n```\r\n\r\nThe `Ftrl` optimizer (or any other optimizer) can be called from `tf.optimizers` or `tf.keras.optimizers`:\r\n\r\n```python\r\nestimator = LinearClassifier(\r\n    feature_columns=feature_columns,\r\n    optimizer=tf.optimizers.Ftrl(\r\n      learning_rate=0.1,\r\n      l1_regularization_strength=0.001\r\n    )\r\n)\r\n```", "I have similar issue: \r\n\r\nVersion: tensorflow._api.v2.version\r\n             tensorflow-gpu==2.0Beta1 \r\n\r\nCode:\r\n\r\n\t\texample = tf.train.example(features=tf.train.features(feature={\r\n\t\t\t'height': _int64_feature(rows),\r\n\t\t\t'width': _int64_feature(cols),\r\n\t\t\t'depth': _int64_feature(depth),\r\n\t\t\t'label_raw': _bytes_feature(label_raw),\r\n\t\t\t'image_raw': _bytes_feature(image_raw)}))\r\n\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_v1.1.py\", line 236, in <module>\r\n    main(sys.argv)\r\n  File \"train_v1.1.py\", line 226, in main\r\n    convert_to(train_images, train_labels, datafolder)\r\n  File \"train_v1.1.py\", line 136, in convert_to\r\n    example = tf.train.example(features=tf.train.features(feature={\r\n**AttributeError: module 'tensorflow._api.v2.train' has no attribute 'example'**\r\n", "I found a way to get past this issue. \r\n\r\n\r\nef _parse_function(example_proto):\r\n\tfeature_description = {\r\n\t\t'height': tf.io.FixedLenFeature([], tf.int64),\r\n\t\t'width':  tf.io.FixedLenFeature([], tf.int64),\r\n\t\t'depth':  tf.io.FixedLenFeature([], tf.int64),\r\n\t\t'label_raw':  tf.io.FixedLenFeature([], tf.string),\r\n\t\t'image_raw':  tf.io.FixedLenFeature([], tf.string),\r\n\t}\r\n\tparsed_features = tf.io.parse_single_example(example_proto, feature_description)\r\n\treturn parsed_features[\"height\"], parsed_features[\"width\"], parsed_features[\"depth\"], parsed_features[\"image_raw\"], parsed_features[\"label_raw\"]\r\n\r\n...\r\n...\r\nserialized_example = tf.data.TFRecordDataset(filenames)\r\nserialized_example = serialized_example.map(_parse_function)\r\n\r\nCalling tf.io.parse_single_example inside tf.Data.map() function as above works without error. \r\n\r\nThank you.", "@keurcien Thanks for catching this. Sent a PR to address this issue.", "First, `tf.train.FtrlOptimizer` is not exposed in TF 2.0, and you may use tf.compat.v1 to access it. Secondly, LinearClassifier in TF 2.0 supports tf.keras.optimziers.* (and tf.optimizers.*) only, and you cannot use `tf.train.*Optimizer` here. \r\n\r\nSo this is the intended behavior, not an actual issue.", "Ah, just notice this is about the doc string issue. Yes, that's right. Thank you!", "hi yhliang2018, do u know how to use tf.contrib.keras.optimizers.Adamax? ", "@xiNA2019 Hi, Adamax optimizer has been exposed in TF 2.0. Please check https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax", "I had the same problem here:\r\n\r\n```\r\n!pip install scikit-learn\r\n!pip install tensorflow-gpu\r\n!pip install tensorflow-hub\r\n!pip install bert-tensorflow\r\n```\r\n\r\n```python\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nfrom datetime import datetime\r\n\r\nimport bert\r\nfrom bert import run_classifier\r\nfrom bert import optimization\r\nfrom bert import tokenization\r\n```\r\n\r\n```bash\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-9-1ccb11d8dffa> in <module>\r\n      1 import bert\r\n----> 2 from bert import run_classifier\r\n      3 from bert import optimization\r\n      4 from bert import tokenization\r\n\r\n~/anaconda3/envs/py37/lib/python3.7/site-packages/bert/run_classifier.py in <module>\r\n     23 import os\r\n     24 from bert import modeling\r\n---> 25 from bert import optimization\r\n     26 from bert import tokenization\r\n     27 import tensorflow as tf\r\n\r\n~/anaconda3/envs/py37/lib/python3.7/site-packages/bert/optimization.py in <module>\r\n     85 \r\n     86 \r\n---> 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):\r\n     88   \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\r\n     89 \r\n\r\nAttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer'\r\n```\r\n\r\nwhile using\r\n```\r\n!pip install tensorflow-gpu==1.15.0\r\n```\r\n\r\nI get a deprecation warning only\r\n\r\n```\r\nWARNING:tensorflow:From /home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n```\r\n\r\nsee this notebook: https://github.com/loretoparisi/bert-movie-reviews-sentiment-classifier/blob/master/src/bert_sentiment_classifier.ipynb\r\n\r\n\r\n", "Are you in TF 1.x or TF 2.0? In general, `tf.train.Optimizer` has been deprecated in TF 2.0, and you need to use [`tf.compat.v1.Optimizer`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer) (then the deprecation message shows up but it's a warning only). In TF 2.0,  the Keras optimziers [`tf.keras.optimizers.*`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) are recommended to use.", "@loretoparisi I'm getting a similar error trying to run the Bert example here in my own Jupyter notebook running on EC2 (using TF2.0 on an Ubuntu deep learning AMI):\r\n\r\nhttps://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=IhJSe0QHNG7U\r\n\r\n`from bert import tokenization`\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-24-f9c3f4b9c6d5> in <module>()\r\n----> 1 from bert import optimization\r\n\r\n~/bert/optimization.py in <module>()\r\n     85 \r\n     86 \r\n---> 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):\r\n     88   \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\r\n     89 \r\n\r\nAttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer'\r\n```\r\n\r\nEven running the code in the Colab notebook gives a similar warning (although not an error):\r\n`WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n`", "Thank you!", "> @loretoparisi I'm getting a similar error trying to run the Bert example here in my own Jupyter notebook running on EC2 (using TF2.0 on an Ubuntu deep learning AMI):\r\n> \r\n> https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=IhJSe0QHNG7U\r\n> \r\n> `from bert import tokenization`\r\n> \r\n> ```\r\n> AttributeError                            Traceback (most recent call last)\r\n> <ipython-input-24-f9c3f4b9c6d5> in <module>()\r\n> ----> 1 from bert import optimization\r\n> \r\n> ~/bert/optimization.py in <module>()\r\n>      85 \r\n>      86 \r\n> ---> 87 class AdamWeightDecayOptimizer(tf.train.Optimizer):\r\n>      88   \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\r\n>      89 \r\n> \r\n> AttributeError: module 'tensorflow_core._api.v2.train' has no attribute 'Optimizer'\r\n> ```\r\n> \r\n> Even running the code in the Colab notebook gives a similar warning (although not an error):\r\n> `WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. `\r\n\r\nEvanZ, try to change the code in the 87 line,\r\n`tf.train.Optimizer` \r\n-> ` tf.keras.optimizers.Optimizer`", "Any updates on this one?", "@senthilmk What is your question? This issue is already answered and closed. Please open a new one if necessary."]}]