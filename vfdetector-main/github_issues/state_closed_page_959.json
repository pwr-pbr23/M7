[{"number": 24658, "title": "[BUG] r1.12.0-rc2  :StringPiece has no member named 'starts_with'", "body": "**System information**\r\n- target-cpu  arm-v8\r\n- tensorflow  r1.12.0-rc2\r\n- local gcc 5.4.0\r\n\r\nThis is the entry point to the tensorflow utest program\r\n```C\r\nGTEST_API_ int main(int argc, char** argv) {\r\n  std::cout << \"Running main() from test_main.cc\\n\";\r\n\r\n  tensorflow::testing::InstallStacktraceHandler();\r\n  testing::InitGoogleTest(&argc, argv);\r\n  for (int i = 1; i < argc; i++) {\r\n    if (tensorflow::StringPiece(argv[i]).starts_with(\"--benchmarks=\")) {\r\n      const char* pattern = argv[i] + strlen(\"--benchmarks=\");\r\n      tensorflow::testing::Benchmark::Run(pattern);\r\n      return 0;\r\n    }\r\n  }\r\n  return RUN_ALL_TESTS();\r\n}\r\n```\r\nIf I try to compile any test files, I get the following error:\r\n```\r\ntest_main.cc: In function 'int main(int, char**)':\r\ntest_main.cc:40:42: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'starts_with'\r\n     if (tensorflow::StringPiece(argv[i]).starts_with(\"--benchmarks=\")) {\r\n\r\n```\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24657, "title": "Print type of the optimizer class name in case of ValueError", "body": "This fix adds `type` to the optimizer class so that the class\r\nname is pretty printted (vs. the object name and memory address itself, previously).\r\n\r\nBefore the fix:\r\n```\r\nTraceback (most recent call last):\r\n...\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 260, in compile\r\n    '%s' % optimizer)\r\nValueError: When running a model in eager execution, the optimizer must be \\\r\n  an instance of  tf.train.Optimizer. Received: \\\r\n  <tensorflow.python.keras.optimizers.SGD object at 0x7f9a99088b38>\r\n```\r\n\r\nAfter the fix:\r\n```\r\nValueError: When running a model in eager execution, the optimizer must be \\\r\n  an instance of tf.train.Optimizer. Received: \\\r\n  <class 'tensorflow.python.keras.optimizers.SGD'>\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thank you for the PR. Printing the full object rather than its class name or type makes the debugging experience better, I would think. So we won't merge this change (besides, with the fusion of `tf.keras.optimizers` with `tf.train`, this error will disappear)."]}, {"number": 24656, "title": "Add tensor forest classification train in core", "body": "According to https://github.com/tensorflow/community/blob/master/rfcs/20180626-tensor-forest.md\r\n\r\nThis is a first step. And the whole process is tracked by #21830\r\n\r\nWe are adding an canned TensorForestClassifier and TensorForestRegressor into core.\r\n\r\nThe Pr is only a third step implementing TensorForestClassifier with train only functionality.", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24655, "title": "new committ", "body": "", "comments": ["jan 1"]}, {"number": 24654, "title": "Bugfix: mutex_lock acquired for output_frame", "body": "Calling function 'ActivateNodes' requires holding mutex 'output_frame->mu'", "comments": ["@ymodak could you please assign some reviewers\r\n"]}, {"number": 24653, "title": "tf.log() is missing although it is still used in many code examples", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n`tf.log()` is missing.  It is only available in `tf.math.log()`.\r\n\r\n**Describe the expected behavior**\r\nThe log function is fundamental and used all the time, it should be available in `tf.log()` (and also in `tf.math.log()`. Especially considering that `tf.exp()` is available. Note that many code examples still show `tf.log()` (try searching for `'tf.log('` in the code base, you will find 12 matches across 8 files).\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.log(42.)\r\n```\r\n\r\n**Other info / logs**\r\nI also checked the changes listed in the API names RFC, and I noticed a few discrepancies (I can file a separate issue if necessary):\r\n\r\n```\r\nNot deleted: tf.Event\r\nNot deleted: tf.losses\r\nNot deleted: tf.space_to_batch\r\n\r\nNot moved: tf.floormod => tf.math.floormod\r\nNot moved: tf.realdiv => tf.math.realdiv\r\nNot moved: tf.SummaryMetadata => tf.summary.SummaryMetadata\r\nNot moved: tf.truncatediv => tf.math.truncatediv\r\nNot moved: tf.truncatemod => math.truncatemod\r\n\r\nNot added: tf.batch_to_space_nd\r\nNot added: tf.debugging.is_finite\r\nNot added: tf.debugging.is_inf\r\nNot added: tf.debugging.is_non_decreasing\r\nNot added: tf.debugging.is_strictly_increasing\r\nNot added: tf.debugging.Print\r\nNot added: tf.debugging.verify_tensor_all_finite\r\nNot added: tf.dtypes.bitcast\r\nNot added: tf.initializers.glorot_normal\r\nNot added: tf.initializers.orthogonal_initializer\r\nNot added: tf.initializers.tables_initializer\r\nNot added: tf.initializers.uniform_unit_scaling\r\nNot added: tf.initializers.variance_scaling\r\nNot added: tf.io.PaddingFIFOQueue\r\nNot added: tf.io.PriorityQueue\r\nNot added: tf.io.QueueBase\r\nNot added: tf.io.RandomShuffleQueue\r\nNot added: tf.io.tf_record_iterator\r\nNot added: tf.linalg.matrix_band_part\r\nNot added: tf.linalg.matrix_inverse\r\nNot added: tf.linalg.matrix_solve_ls\r\nNot added: tf.linalg.self_adjoint_eig\r\nNot added: tf.linalg.self_adjoint_eigvals\r\nNot added: tf.math.mod\r\nNot added: tf.math.reduce_join\r\nNot added: tf.quantization.quantize_v2\r\nNot added: tf.random.get_seed\r\nNot added: tf.random.multinomial\r\nNot added: tf.random.random_gamma\r\nNot added: tf.random.random_poisson\r\nNot added: tf.random.set_random_seed\r\nNot added: tf.saved_model.build_tensor_info\r\nNot added: tf.saved_model.get_tensor_from_tensor_info\r\nNot added: tf.saved_model.LEGACY_INIT_OP_KEY\r\nNot added: tf.saved_model.load\r\nNot added: tf.saved_model.main_op\r\nNot added: tf.saved_model.MAIN_OP_KEY\r\nNot added: tf.saved_model.main_op_with_restore\r\nNot added: tf.saved_model.maybe_saved_model_directory\r\nNot added: tf.saved_model.SavedModelBuilder\r\nNot added: tf.saved_model.TRAINING\r\nNot added: tf.sparse.matmul\r\nNot added: tf.sparse.merge\r\nNot added: tf.sparse.placeholder\r\nNot added: tf.sparse.reduce_max_sparse\r\nNot added: tf.sparse.reduce_sum_sparse\r\nNot added: tf.sparse.SparseTensorValue\r\nNot added: tf.summary.HistogramProto\r\nNot added: tf.train.confusion_matrix\r\n```\r\n\r\nAnd `tf.spectral` is called `tf.signal`.", "comments": ["The code examples will be updated soon, and it isn't surprising that they don't yet work on TF2 (though of course, it would be nice if they did). \r\n\r\nWe decided to remove tf.log because there's an unfortunate ambiguity with tf.logging.log. \r\n\r\nThank you for sending the differences. We have made some changes, and at first glance, these look fine to me. We do have to update the RFC though.", "Thanks Martin.  I didn't think about `tf.logging.log()`, I see your point, although I would strongly vote in favor of keeping `tf.log()` (math), as I believe way more people are going to be frustrated by `tf.log()` missing than people frustrated by `tf.log()` being math instead of logging, given that:\r\n* TF 1.x had `tf.log()`, and it was math, not logging.\r\n* `exp()` and `log()` always go hand in hand,\r\n* NumPy has `np.log()` (math)\r\nJust my 2 cents.", "@dynamicwebpaige you have the latest on the symbol map. Assigning this issue to you to close when we have an official map we can point to.", "FWIW, I agree with @ageron. `tf.log` should have stayed the logarithm.\r\n\r\nHaving `tf.exp` next to `tf.math.log` just looks weird.", "@ageron,\r\nIn the latest version of `Tensorflow`, even **`tf.exp`** has been replaced with [**`tf.math.exp`**](https://www.tensorflow.org/api_docs/python/tf/math/exp) and for Logarithms, the API is [**`tf.math.log`**](https://www.tensorflow.org/api_docs/python/tf/math/log).\r\n\r\nSince both `Exponential Function` and `Logarithmic Function` are present in **`tf.math`**, can you please confirm if we can close this issue? Thanks!", "Thanks for the update, @rmothukuru. Well I guess it's consistent now, with `log` and `exp` treated the same way, but to be honest I would have much preferred to see a `tf.log()` alias for `tf.math.log()` created instead. I imagine that `tf.exp()` is used all over the place, so it's going to require a lot of search and replace, with potentially a bit of breakage here and there, not to mention code examples being outdated (including in books). Conversely, adding `tf.log()` would break no code at all.\r\nBut it's just my opinion of course! I guess there's an effort to make the `tf` namespace as clean as possible, and that's definitely a good thing. Closing the issue."]}, {"number": 24652, "title": "SynchronousMemcpy changed to SynchronousMemcpyD2H and SynchronousMemcpyH2D", "body": "Updated deprecated SynchronousMemcpy to SynchronousMemcpyD2H and SynchronousMemcpyH2D", "comments": ["@aaroey Thanks for pointing it out, printing 'result' during error will help in easy identification of the issue. Its updated.", "@aaroey, Thanks for the review, Changed result.error_message() to result"]}, {"number": 24651, "title": "Unused variable removed from softmax_op_functor.h", "body": "The below issue is resolved.\r\n\r\n```\r\nIn file included from tensorflow/core/kernels/softmax_op.cc:26:\r\n./tensorflow/core/kernels/softmax_op_functor.h:60:45: warning: unused variable 'depth_dim' [-Wunused-variable]\r\n    Eigen::IndexList<Eigen::type2index<1> > depth_dim;\r\n```\r\n", "comments": []}, {"number": 24650, "title": "MKL convolution throws exception for simple code", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):b'v1.12.0-5324-g3ae375aa92' 1.12.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):19.2\r\n- GCC/Compiler version (if compiling from source):5.3.0\r\n- CUDA/cuDNN version:n/a\r\n- GPU model and memory:n/a\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32, shape=[1, 3, None, None])\r\nxpad = tf.pad(x, [[0, 0], [0, 0], [2, 3], [2, 3]])\r\nW = tf.random_normal([7, 7, 3, 64])\r\nout = tf.nn.conv2d(xpad, W, [1, 1, 2, 2], padding=\"VALID\", data_format='NCHW')\r\n\r\nsess = tf.Session()\r\nwith sess.as_default():\r\n    sess.run(out, feed_dict=\r\n            {x: np.random.rand(1, 3, 600, 800)})\r\n```\r\nThis simple code snippet is legal and runs well on a GPU. However, when run on a CPU with MKL build it throws:\r\n```\r\n2019-01-01 04:17:00.402605: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at mkl_conv_ops.cc:1128 : Aborted: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1125\r\nTraceback (most recent call last):\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1125              \r\n         [[{{node Conv2D}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"a.py\", line 15, in <module>\r\n    {x: np.random.rand(1, 3, 600, 800)})\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.AbortedError: Operation received an exception:Status: 3, message: could not create a dilated convolution forward descriptor, in file tensorflow/core/kernels/mkl_conv_ops.cc:1125              \r\n         [[node Conv2D (defined at a.py:10) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node Conv2D:\r\n random_normal/stddev (defined at a.py:9)\r\n Placeholder (defined at a.py:7)\r\n Pad/paddings (defined at a.py:8)\r\n\r\nOriginal stack trace for 'Conv2D':\r\n  File \"a.py\", line 10, in <module>\r\n    out = tf.nn.conv2d(xpad, W, [1, 1, 2, 2], padding=\"VALID\", data_format='NCHW')\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1026, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 501, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"/HOME/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```\r\n\r\nThe code will not throw if the padding is [2, 2] instead of [2, 3]. ", "comments": ["Hi,\r\nI tried your code on TensorFlow installed from https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl. There was no error thrown out.\r\nCould you try this version, please?", "The version your provided does not throw an error. However I was testing with a later version which means the bug may be introduced after 1.12.\r\n\r\nFrom the behavior it seems very likely that the bug comes from the recent graph transformation optimization added in `mkl_layout_pass` which merges padding and convolution to a single op.\r\n@nhasabni who seems to be the author of this feature.", "Thanks for the information. I'll check this.", "May I know the commit id of \"b'v1.12.0-5324-g3ae375aa92' 1.12.0\", please?", "3ae375aa92, as part of the version string.", "Noted.", "I've requested the dev team to investigate this issue.\r\nI'll update you later once it is fixed.", "This bug is in the 1.13.1 release.\r\n\r\nAny updates about it?", "The problem can not be reproduced with a build from the latest \"master\" branch", "Thanks! I just verified that the problem does not exist in latest master any more.", "Hi Yuxin, thank you for the update.  Guozhong"]}, {"number": 24649, "title": "Why tf.matmul doesn't get zero?", "body": "<em>I got a problem when using TensorFlow eager mode that tf.matmul result of two matrixes should obvious be zeros but now different small number as results.</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution:Linux Ubuntu 16.04, also on macOS\r\n- TensorFlow installed from:binary\r\n- TensorFlow version :1.12\r\n- Python version:3.6\r\n- GPU model and memory: no GPU, 8GB memory\r\n\r\n**Describe the current behavior**\r\nIt should be all zeros of the output, but it shows random small values\r\n\r\n**Code to reproduce the issue**\r\n` index=np.random.randint(10, size=20)`\r\n ` grad = tf.one_hot(index, 10, on_value=-0.045, off_value=0.005)`\r\n`  w=tf.constant(0.01, shape=[100, 10])`\r\n`  print(tf.matmul(grad, tf.transpose(w)))`\r\n\r\n\r\nIs this a bug or something else?\r\n", "comments": ["The results should be zeros mathematically but standard floating point numbers simply do not have enough precision to compute the exact results. This behavior is common and expected.\r\n\r\ne.g.:\r\n```\r\n$python3 -c 'print(sum([0.005 for k in range(6)]))'\r\n0.030000000000000002\r\n```", "> The results should be zeros mathematically but standard floating point numbers simply do not have enough precision to compute the exact results. This behavior is common and expected.\r\n> \r\n> e.g.:\r\n> \r\n> ```\r\n> $python3 -c 'print(sum([0.005 for k in range(6)]))'\r\n> 0.030000000000000002\r\n> ```\r\n\r\nPrecision was my first thought, but it have different result for the same calculation as follow.\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow.contrib.eager as tfe\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n  tfe.enable_eager_execution()\r\n  np.set_printoptions(threshold=np.nan)\r\n  index=np.random.randint(10, size=20)\r\n  grad = tf.one_hot(index, 10, on_value=-0.045, off_value=0.005)\r\n  w=tf.constant(0.01, shape=[100, 10])\r\n  print(tf.matmul(grad, tf.transpose(w)))\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\nAnd floating point shouldn't get the problem of representing 0.005. Calculating 0.005 * 0.01 * 9 - 0.045 * 0.01=0.", "Your code contains randomness, so getting different result is no surprise.\r\n\r\nfloat32 has no problem representing 0.005 (CORRECTION: even this is not true). But as I have shown above, it does have problems representing something like this which is very likely to be an intermediate result of your computation:\r\n```\r\n$python3 -c 'print(sum([0.005 * 0.01 for k in range(6)]))'\r\n0.00030000000000000003\r\n```", "> Your code contains randomness, so getting different result is no surprise.\r\n> \r\n> float32 has no problem representing 0.005. But as I have shown above, it does have problems representing something like this which is very likely to be an intermediate result of your computation:\r\n> \r\n> ```\r\n> $python3 -c 'print(sum([0.005 * 0.01 for k in range(6)]))'\r\n> 0.00030000000000000003\r\n> ```\r\n\r\nThe randomness is only about the place of -0.045 in vector when multiplying in my code, which affect the order of adding, but they show different results.\r\n\r\nI asked another question in numpy. It seems the problem from original python. C++ calculating 0.05*6  with double get 0.3 instead of 0.30000000000000004 in python.\r\n[https://github.com/numpy/numpy/issues/12643](url)", "> which affect the order of adding\r\n\r\nand the order of adding may affect results due to precision.", "> > which affect the order of adding\r\n> \r\n> and the order of adding may affect results due to precision.\r\n\r\nThanks for answering and sorry for my stubborn. Finally I figured out IEEE754 can not represent 0.05 accurately. I forgot the rule to generate binary representation back to university.", "Closing this issue since its solved. Please feel free to reopen if have any further questions. Thanks!"]}, {"number": 24648, "title": "Removed kOutput variable which was not used", "body": "tensorflow/lite/kernels/expand_dims.cc:30:15: warning: unused variable 'kOutput' [-Wunused-const-variable]\r\nconstexpr int kOutput = 0;", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 77 days with no activity and the `awaiting review` label has been applied.", "@aselle Could you PTAL and approve", "@gbaned I'm not an owner of this code -- I'm just in the history because I moved it. Can you find a more fitting reviewer for TF lite, please?"]}, {"number": 24647, "title": "Update eigen library to 88fc23324517 to fix 24457", "body": "This fix updates eigen library to 88fc23324517 so that the issue raised in #24457 could be fixed.\r\n\r\nThis fix fixes #24457.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Sorry I'm not familiar with this change. Please find someone else. @ymodak "]}, {"number": 24646, "title": "TensorFlow Lite conversion fails", "body": "**System information**\r\nWindows 7/64bit\r\n- TensorFlow installed from binary:\r\n1.12 from pip\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\io\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\lite\\python\\lite.py\", line 453, in\r\nconvert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\io\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 342,\r\nin toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"C:\\Users\\io\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 135,\r\nin toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\io\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\site-packages\\\\tensor\r\nflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, descrip\r\ntion = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\io\\\\anaconda3\\\\lib\\\\imp.py\"\r\n, line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'\r\n_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (mos\r\nt recent call last):\\r\\n  File \"c:\\\\users\\\\io\\\\anaconda3\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__mai\r\nn__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\io\\\\anaconda3\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals\r\n)\\r\\n  File \"C:\\\\Users\\\\io\\\\Anaconda3\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"C:\\\\U\r\nsers\\\\io\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.p\r\ny\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\User\r\ns\\\\io\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.\r\npy\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\Users\\\\io\\\\AppData\\\\Roaming\r\n\\\\Python\\\\Python36\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_i\r\nmport_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n\r\n\r\nGithub's Continuous build status says: \r\nWin GPU nightly failing, \r\nso I did not try retraining and reconverting.\r\n\r\n\r\n\r\n", "comments": ["@ioavator Was there any issue in the installation? Could you provide any code to reproduce the error? Please provide any details that you think will help in finding the root cause of the issue. Thanks!", "@jvishnuvardhan\r\n>>Was there any issue in the installation? \r\nNope. pip install tensorflow-gpu went smoothly.\r\nModel training/testing with or without custom estimators works on Win7.\r\nApparently my problem is - toco and tensorflow lite converters are not working\\supported in Windows environment.\r\n\r\n>>Could you provide any code to reproduce the error? \r\nAn instance of this:\r\nhttps://github.com/tensorflow/tensorflow/issues/15805#issuecomment-407298717\r\n\r\nI had to use Mac-OS to do conversion.\r\nMac-Os is fine. Win7 is not.\r\n\r\n\r\n", "Does this still repro with the latest 1.13.1 TF release?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24646\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24646\">No</a>\n"]}, {"number": 24645, "title": "Add take_while experimental dataset op", "body": "Addresses #24105 ", "comments": ["Still need to add test cases, but any changes required in the current commits? \r\n\r\n@jsimsa @mrry Most other dataset op that take in a function as a parameter, check for short circuits using `ComputeShortCircuitIndices`, will I need to do that here as well?\r\n\r\nAlso, right now `take_while` is state-less, similar to `map`. Do I need to also build the `scan` equivalent for `take_while`, i.e. `take_while` with a `state`? ", "I've made the required changes. Adding tests in the next couple of hours.", "@jsimsa I've added tests and made the changes mentioned. Take a look, and let me know if further changes are required.", "@jsimsa Made the changes and fixed whatever was failing on the tests. I wasn't exactly sure what the api_def should look like for this, so I copied and edited `api_def_FilterDataset`, so can you take a look and tell me whether it's right or wrong? \r\n\r\nAlso, what's the difference between `base_api` and `python_api`? ", "@jsimsa Made the changes you asked and fixed whatever was causing the CI to fail. I think it should be good to go unless I've overlooked something.", "@jsimsa Can you help me out a bit?\r\n\r\nUbuntu Sanity fails on Step 5 of 12: `do_bazel_nobuild (bazel nobuild)`\r\n```\r\nERROR: error loading package 'tensorflow/lite/experimental/swift': Extension file not found. \r\nUnable to load file '@bazel_skylib//lib:new_sets.bzl': file doesn't exist or isn't a file\r\n```\r\n\r\nRunning`bazel build --nobuild  -- //tensorflow/... -//tensorflow/lite/java/demo/app/... -//tensorflow/lite/examples/android/... -//tensorflow/lite/schema/...` locally on `master` branch fails with the same warning for me. \r\n\r\nIs there something I'm missing, because I haven't edited those files at all.", "@Squadrick can you try pull the `master` branch and then merging it into your PR? I am pretty sure the issue is unrelated to your change.", "@jsimsa This is good to go, right? Nothing else to change? ", "Yes, I believe @ymodak is trying to get the presubmits to pass at which point it should move on to an internal review (should be a no-op) and then submitted.", "@Squadrick one of the test failures `tensorflow/tools/api/tests:api_compatibility_test` is in fact related:\r\n\r\ncan you run the following:\r\n\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\n\r\nand update the PR", "@jsimsa I pulled `master` and ran the commands. This is what my `git status` looks like now.\r\n```\r\nOn branch take-while\r\nChanges not staged for commit:\r\n  (use \"git add/rm <file>...\" to update what will be committed)\r\n  (use \"git checkout -- <file>...\" to discard changes in working directory)\r\n\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.data.experimental.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-baseline-classifier.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-baseline-estimator.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-baseline-regressor.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-boosted-trees-classifier.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-boosted-trees-regressor.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-checkpoint-saver-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-checkpoint-saver-listener.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-d-n-n-classifier.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-d-n-n-estimator.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-d-n-n-linear-combined-classifier.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-d-n-n-linear-combined-estimator.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-d-n-n-linear-combined-regressor.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-d-n-n-regressor.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-estimator.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-feed-fn-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-final-ops-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-global-step-waiter-hook.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-linear-classifier.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-linear-estimator.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-linear-regressor.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-logging-tensor-hook.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.-mode-keys.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-nan-loss-during-training-error.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-nan-tensor-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-profiler-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-second-or-step-timer.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-session-run-args.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-session-run-context.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-session-run-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-session-run-values.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-step-counter-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-stop-at-step-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.-summary-saver-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.experimental.-in-memory-evaluator-hook.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.experimental.-linear-s-d-c-a.pbtxt\r\n\tdeleted:    tensorflow/tools/api/golden/v2/tensorflow.estimator.experimental.pbtxt\r\n\tmodified:   tensorflow/tools/api/golden/v2/tensorflow.estimator.pbtxt\r\n\r\nUntracked files:\r\n  (use \"git add <file>...\" to include in what will be committed)\r\n\r\n\ttensorflow/tools/api/golden/v2/tensorflow.estimator.inputs.pbtxt\r\n\r\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n```\r\n\r\nI also got this warning:\r\n```\r\nWARNING:tensorflow:Golden file update requested!\r\nAll test failures have been skipped, see the logs for detected diffs.\r\nThis test is now going to write new golden files.\r\nMake sure to package the updates together with your change.\r\n\r\nYou will need an explicit API approval. This may take longer than a normal\r\nreview.\r\n```\r\n\r\nAnd this seems to be the `diff`:\r\n\r\n```\r\n+   member_method {\r\n+     name: \"take_while\"\r\n+     argspec: \"args=[\\'predicate\\'], varargs=None, keywords=None, defaults=None\"\r\n+   }\r\n```\r\n\r\nI have no idea how to proceed. How do I go about getting API approval? ", "The only file `pbtxt` file you should add to your PR is `tensorflow/tools/api/golden/v2/tensorflow.data.experimental.pbtxt`. Ignore all the `tensorflow/tools/api/golden/v2/tensorflow.estimator.*` (I am not sure why they showed up).\r\n\r\nI will take care of the API approval (it happens during the internal review of the PR).", "@jsimsa Running the update script didn't change any of the `v1` scripts.\r\n\r\nIn `tensorflow/api_template.__init__.py`, on line `56`, there's `_compat.enable_v2_behavior()`, this sets `_force_enable` in `tensorflow/python/tf2.py` to `True`, and every time `tf2.enabled()` is called it returns `True`. So setting `TF2_BEHAVIOR=0` has no effect on which TensorFlow version to run. `tensorflow/api_template.__init__.py` is one of the `srcs` for `api_compatibility_test`, so I had to manually remove `@test_util.run_v1_only('b/120545219')` in `api_compatibility_tests.py` on line `317` and `336`, to get v1 golden API files to update.\r\n\r\nIs there something I'm overlooking? ", "@jsimsa Pushed the requested changes. ", "@Squadrick thanks, I will resume the internal review."]}, {"number": 24644, "title": "I replaced keras to tensorflow.keras in my project and got OOM with same batch size.", "body": "I would like to completely remove keras dependency from my project and replace it to tensorflow.keras, but it is significantly degrades performance.  \r\nOut Of Memory occurs when I just replace keras to tensorflow.keras with same batch size.\r\nAlso epoch train time increased.\r\nLooks like tensorflow.keras unoptimized.\r\nWill tensorflow.keras be optimized in future?", "comments": ["@iperov, please provide the code and also follow the guidelines to fill the required details in the [template](https://github.com/tensorflow/tensorflow/issues/new?template=00-bug-performance-issue.md). Without the code and other details, it is difficult to find rootcause of the problem. If possible, mention your intention behind removing keras dependency?", "sorry I cannot provide reproduce right now, because my project is large enough.\r\n\r\nPossible reproduce : \r\ncreate deep image autoencoder for ~500MB total disk space. \r\nFind largest not OOM batch size for training for your videocard.\r\nReplace keras to tf.keras and expect issue.\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 24643, "title": "Tensorboard histogram_freq is not using GPU: slows down training", "body": "**System information**\r\n- No custom code\r\n- Ubuntu 18.04\r\n- TensorFlow version 1.11.0\r\n- tf.COMPILER_VERSION = v1.11.0-0-gc19e29306c\r\n- Python version 3.6\r\n- CUDA/cuDNN version 9.0\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\nWhen I run Tensorboard with `histogram_freq = x` where `x != 0`, it uses the CPU at the end of each epoch when Tensorboard writes to logs. \r\n\r\nUsually the GPU is used for the entirety of training and writing to logs and this is also true when `histogram_freq = 0`. \r\n\r\nThis [issue] (https://github.com/keras-team/keras/issues/3358) has been mentioned in previous questions but it is dissimilar in that their issue occurs when validation data is passed via a data generator and that histograms are not created - no reference to CPU or GPU. \r\n\r\nI do not use a data generator for validation and histograms appear to be created. This is an issue as writing to logs using CPU seems to take an eternity making the training process up to five times longer.\r\n\r\nNotes: \r\n\r\nTraining carried out by Keras `fit_generator`.\r\n\r\nTraining dataset passed by `ImageDataGenerator`.\r\n\r\nValidation dataset is passed without data generator.\r\n\r\n    tensorboard = TensorBoard(log_dir='../logs/vv2', histogram_freq=1, write_graph=True, write_images=True)\r\n    model.fit_generator(train_generator, \r\n                                  steps_per_epoch=batch_size, \r\n                                  epochs=epochs, \r\n                                  validation_data=(x_test,y_test),\r\n                                  callbacks=[tensorboard])\r\n", "comments": ["This issue is more suitable on TenosrFlow Tensorboard repo. Please post it on tensorboard repo from [here](https://github.com/tensorflow/tensorboard/issues). Thanks!"]}, {"number": 24642, "title": "Failed to load the native TensorFlow runtime in Mac OSX", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Mac OSX 10.11.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed with pip\r\n- TensorFlow version:  1.12.0\r\n- Python version: 3.6.6\r\n\r\n**Describe the problem**\r\n\r\nEverything was ok with an older version, but with the current version install pass ok but when try to import, many errors are produced.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`import tensorflow\r\n`\r\n\r\n**Any other info / logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\n  Referenced from: /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/mikko/dev/wip/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _clock_gettime\r\n  Referenced from: /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so (which was built for Mac OS X 10.12)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Users/mikko/dev/wip/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```", "comments": ["I suspect your operating system does not follow AVX instruction sets. TensorFlow official release binaries version 1.6 and higher are prebuilt with AVX instruction sets. In the case of macOS, TF is tested and supported against macOS 10.12.6 (Sierra) or later. To use Tensorflow on old CPUs you'll need to build a binary on your own.", "Thanks @ymodak. \r\n\r\nSome useful workarounds to this issue for macOS X versions < 10.12.6: \r\n\r\n1) install a specific version (at least 1.5 confirmed to work) \r\n\r\n`pip install tensorflow==1.5`\r\n\r\n2) Use conda\r\n\r\n`conda install tensorflow`\r\n\r\nBoth tested and solve the issue.", "Thanks for the workarounds!", "I am getting the same error, While trying to pip install 1.5 tensorflow i get this error.\r\n\r\nCollecting tensorflow==1.5\r\n  Could not find a version that satisfies the requirement tensorflow==1.5 (from versions: 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 2.0.0a0, 2.0.0b0, 2.0.0b1, 2.0.0rc0, 2.0.0rc1, 2.0.0rc2, 2.0.0)\r\nNo matching distribution found for tensorflow==1.5\r\n\r\nis there any other workaround?"]}, {"number": 24641, "title": "tf.contrib.quantize.create_training_graph with tf.keras.activations.relu quantizes before the relu cliping op", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nThe use of the param 'max_value' in https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu produces the a quantized graph with the FakeQuant op just after the relu op and not after the automatically inserted clip op.\r\n\r\n![Generated graph](https://c2.staticflickr.com/8/7889/31598991447_a5d540f672_b.jpg)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe FakeQuant op should be added after the clip op\r\n\r\n**Code to reproduce the issue**\r\n```\r\n...\r\nnet = tf.layers.dense(inputs=net,\r\n                              units=units,\r\n                              activation=None,\r\n                              name='dense_' + str(ly_idx),\r\n                              )\r\nnet = tf.keras.activations.relu(net,max_value=params['clip_activations'])\r\n...\r\n```\r\n\r\n\r\n\r\nThanks,\r\n", "comments": ["Hello @fchollet , user requests that quantization occur after clipping of Relu, not before as shown. Can you please review? Thanks.", "@fgr1986,\r\nSorry for the delayed response. Can you please confirm if the issue still persists? If so, please provide a reproducible code so that we can look into it. \r\n\r\nPlease refer to the latest [Documentation of Relu Activation function](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) and the [Documentation of Quantize](https://www.tensorflow.org/api_docs/python/tf/quantization/quantize) too!\r\n\r\nThanks!", "Shall I try on 1.12? or in the newest 1.X branch? \r\n\r\nThanks", "@fgr1986,\r\nThank you for your response. It would be great if you could try in the latest `2.x` branch as `1.x` is not actively supported.  ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24641\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24641\">No</a>\n"]}, {"number": 24640, "title": "Add model_to_dot and print_summary to tf.keras.utils", "body": "Fixes #24639", "comments": ["Thanks for approving the changes.  Not sure why the build is failing, any idea?"]}, {"number": 24639, "title": "model_to_dot() missing from tf.keras although it is documented on keras.io", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nThe official Keras API is documented at keras.io, and it shows an example using `model_to_dot()` ([here](https://keras.io/visualization/#model-visualization)). The example uses `keras.utils.vis_utils.model_to_dot()`. However, this example does not work when using `tf.keras`, since there is no `tf.keras.utils.vis_utils` package.\r\nI can work around this issue by using: `from tensorflow.python.keras.utils.vis_utils import model_to_dot`, but I believe importing from `tensorflow.python` is frowned upon and may break in the future.\r\n\r\n**Describe the expected behavior**\r\nI expect `model_to_dot()` to be available in `tf.keras.utils.vis_utils`, or at least in `tf.keras.utils`.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom tensorflow import keras\r\nkeras.utils.vis_utils.model_to_dot\r\n```\r\n\r\n**Other info / logs**\r\nOn a related note, `keras.utils.print_summary` is part of the Keras API ([here](https://keras.io/utils/#print_summary)), but absent from `tf.keras.utils`.\r\n\r\n`keras.utils.np_utils.to_categorical` is documented [here](https://keras.io/losses/) but absent from `tf.keras.utils.np_utils` (it's in `tf.keras.utils`). This might be an issue with the Keras API rather than with tf.keras.\r\n```", "comments": ["I would suggest:\r\n* fixing the Keras API to replace `keras.utils.vis_utils.model_to_dot` with `keras.utils.model_to_dot`\r\n* fixing the Keras API to replace `keras.utils.np_utils.to_categorical` with `keras.utils.to_categorical`\r\n* Adding `model_to_dot` and `print_summary` to `tf.keras.utils`.\r\nI will submit PRs to that effect.", "I filed a corresponding issue in keras-team/keras: https://github.com/keras-team/keras/issues/11959\r\n", "Hi @ymodak , Please kindly complete review for this PR submitted by @ageron . Thanks.", "I just submitted a new PR because it seems that my previous PR was insufficient to fix this issue, I also needed to add a `@keras_export` decorator to the `model_to_dot` and `print_summary` function definitions."]}, {"number": 24638, "title": "Fix C++14-compat warning in pattern_matcher.h", "body": "The below issue is fixed\r\n\r\n```\r\nINFO: From Compiling tensorflow/compiler/xla/service/hlo_graph_dumper.cc [for host]:\r\nIn file included from tensorflow/compiler/xla/service/hlo_graph_dumper.cc:42:\r\n./tensorflow/compiler/xla/service/pattern_matcher.h:1880:18: warning: 'constexpr' non-static member function will not be implicitly 'const' in C++14; add 'const' to avoid a change in behavior [-Wconstexpr-not-const]\r\n  constexpr auto WithShapeEqualTo(const ::xla::Shape* shape)\r\n                 ^\r\n                                                             const\r\n./tensorflow/compiler/xla/service/pattern_matcher.h:1888:18: warning: 'constexpr' non-static member function will not be implicitly 'const' in C++14; add 'const' to avoid a change in behavior [-Wconstexpr-not-const]\r\n  constexpr auto WithShapeCompatibleTo(const ::xla::Shape* shape)\r\n                 ^\r\n\r\n```", "comments": ["Patch LGTM, but would you be willing to change the PR title to be more specific than \"compilation issue\"?  Maybe \"Fix C++14-compat warning in pattern_matcher.h\"?\r\n\r\nThe PR title and description is what forms the commit's message (I think!).", "@jlebar  I have changed it. Thanks for the review."]}, {"number": 24637, "title": "Fix model_to_dot() for Sequential models", "body": "The model_to_dot() function does not display the input layer of `Sequential` models correctly. This change (just one character added, haha) fixes that. See https://github.com/keras-team/keras/issues/10638\r\nNote: this fix seems to have been applied already in keras-team/keras, but not in tf.keras.", "comments": ["Thanks for accepting the change.  Not sure why the build is failing, any idea?"]}, {"number": 24636, "title": "Tensorflow can find right cudnn in one python file but fail in another", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution :  `windows 10`\r\n- TensorFlow installed from : `binary`\r\n- TensorFlow version: `1.12.0`\r\n- Python version: `3.5`\r\n- CUDA/cuDNN version: `cuda 9.0 ` & `cudnn 7.4.2`\r\n- GPU model and memory: `Geforce 1060`\r\n\r\n**Describe the current behavior**\r\nI am trying to use tensorflow gpu version to train and test my deep learning model. But here comes the problem. When I train my model in one python file things go on well. Tensorflow-gpu can be used properly. Then I save my model as a pretrained on as `grapg.pb` format and try to reuse it in another python file. \r\n\r\nThen I got the following error messages.\r\n\r\n**Describe the expected behavior**\r\nRun tensorflow well in both files.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n E tensorflow/stream_executor/cuda/cuda_dnn.cc:363] Loaded runtime CuDNN \r\n    library: 7.1.4 but source was compiled with: 7.2.1.  CuDNN library major \r\n    and minor version needs to match or have higher minor version in case of \r\n    CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN \r\n    library.  If building from sources, make sure the library loaded at runtime \r\n    is compatible with the version specified during compile configuration.\r\n```\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hello @ZhangYuef , since you are running on Windows 10, you may need to adjust you cuDNN version (major & minor) for windows such that both save to .pb file program and restore from .pb file program work correctly. Please experiment with latest major/minor version until it works. Let us know. Thanks.", "@ZhangYuef i had the exact same problem. although i had cudnn 7.4.2 i found i had the cudnn 7.1.4 anaconda module. i did conda remove and reinstalled tensorflow-gpu. problem solved", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24635, "title": "Replace deprecated test_session with cached_session in quantile_ops_test", "body": "\r\nThis fix replace deprecated test_session with cached_session in\r\nquantile_ops_test.py to remove warnings during tests\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Nagging Reviewer @tanzhenyu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24634, "title": "Unsupported data type in placeholder op", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS Mojave\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow\r\n- TensorFlow version (or github SHA if from source):\r\n1.12\r\n\r\n```\r\nRuntimeError: TOCO failed see console for info.\r\nb\"2018-12-30 15:40:54.449737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:189] Unsupported data type in placeholder op: 2\\n2018-12-30 15:40:54.450020: F tensorflow/contrib/lite/toco/import_tensorflow.cc:2137] Check failed: status.ok() Unexpected value forattribute 'T'. Expected 'DT_FLOAT'\\n\"\r\n```\r\n\r\nTo save the model, I have:\r\n```\r\n// model is an Estimator instance\r\ndef export(model):\r\n  model.export_saved_model(\"tmp/export\", serving_input_receiver_fn)\r\n```\r\n\r\nand:\r\n```\r\ndef serving_input_receiver_fn():\r\n  features = { 'x': tf.placeholder(shape=[1, 100, 100, 1], dtype=tf.as_dtype(np.int32)) }\r\n  return tf.estimator.export.ServingInputReceiver(features, features)\r\n```\r\n\r\nAs you can see, dtype is np.int32, so I attempt to cast that to a tf type. \r\n\r\nI can attach the full model def on request. \r\n\r\nThanks.", "comments": ["The solution to this was not in the placeholder op itself, but in the model declaration. I was using a float64 input type. Switching to float32, and setting dtype=float32 in the placeholder, solved my issue.", "Please I don't understand how to apply your solution to my problem here\r\n\r\n> ConverterError: See console for info.\r\n2021-07-03 18:46:15.428635: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 19\r\n2021-07-03 18:46:15.433220: F tensorflow/lite/toco/import_tensorflow.cc:2690] Check failed: status.ok() Unexpected value for attribute 'T'. Expected 'DT_FLOAT'\r\nAborted (core dumped)\r\n\r\nI am currently using Google Colab and the following cells produced the error\r\n\r\n`xception_fold_1 = keras.models.load_model('/content/drive/MyDrive/Datasets/project_models_graphs/xception fold-1.h5')\r\n`\r\n\r\n`converter = tf.lite.TFLiteConverter.from_keras_model(xception_fold_1)\r\nconverter.experimental_new_converter = False\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE, tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True`\r\n\r\n`tflite_model = converter.convert()`\r\n\r\n`with open('/content/drive/MyDrive/Datasets/project_models_graphs/xception-fold-1.tflite', 'wb') as f:\r\n  f.write(tflite_model)`\r\n "]}, {"number": 24633, "title": "Fix keras mode evaluate() progress bar issue", "body": "This fix tries to address the issue raised in #24593 where the display of evaluate() progress bar is incorrect at the last batch in keras mode.\r\n\r\nThe issue was that on_batch_end() was relying on on_epoch_end for the last step processing:\r\nhttps://github.com/tensorflow/tensorflow/blob/8f60a381d210478f21762a6cf14f547a05e98878/tensorflow/python/keras/callbacks.py#L724-L727\r\nbut on_epoch_end was only called for train mode:\r\nhttps://github.com/tensorflow/tensorflow/blob/8f60a381d210478f21762a6cf14f547a05e98878/tensorflow/python/keras/engine/training_arrays.py#L375-L378\r\n\r\nThis fix addresses the issue.\r\n\r\nThis fix fixes #24593.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the PR. As it happens, this has been fixed in a recent commit. Your fix was however correct."]}, {"number": 24632, "title": "Interrupting tf.keras training while using the TensorBoard callback wreaks havoc", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nUsing tf.keras in Jupyter (or a Python shell) with the `TensorBoard` callback, some problems occur if I interrupt training. These problems did not occur in TF 1.12:\r\n\r\n1. I get an exception if I call `fit()` again on the same model:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource\r\nlocalhost/logdir:logs/run1/N10tensorflow22SummaryWriterInterfaceE does not exist.\r\n[Op:WriteScalarSummary] name: epoch_loss/\r\n```\r\nI can workaround this problem by recompiling the model.\r\n\r\n2. I also get an exception if I interrupt training, then I delete the logs directory, then I try to use the `TensorBoard` callback on the same logs directory again:\r\n```\r\ntensorflow.python.framework.errors_impl.UnknownError: The events file logs/run1/events.out.tfevents.1546185456.macmix.local.v2 has disappeared.\r\n\tFailed to flush 1 events to logs/run1/events.out.tfevents.1546185456.macmix.local.v2\r\n\tCould not flush events file. [Op:FlushSummaryWriter]\r\n```\r\nThis one is more severe: sometimes it recovers by itself after a while. Sometimes is doesn't and I cannot find any way to manually recover from this error, other than restarting the Jupyter kernel (or the Python shell).\r\n\r\n**Describe the expected behavior**\r\nI expect the `TensorBoard` callback to gracefully handle these issues, perhaps display a warning, but do not force a recompile or a kernel restart.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport shutil\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nX_train = np.random.rand(1000, 10)\r\ny_train = np.random.rand(1000)\r\nmodel = keras.models.Sequential([keras.layers.Dense(1)])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\ntensorboard_cb = keras.callbacks.TensorBoard(\"logs/run1\")\r\nmodel.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])\r\n# NOTE: you must interrupt training (Ctrl-C) before it finishes\r\n\r\n# For issue #1, try this:\r\nmodel.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])\r\n\r\n# For issue #2, try this (you may need to interrupt and retry a few times):\r\nshutil.rmtree(\"logs\")\r\nmodel = keras.models.Sequential([keras.layers.Dense(1)])\r\nmodel.compile(loss=\"mse\", optimizer=\"sgd\")\r\ntensorboard_cb = keras.callbacks.TensorBoard(\"logs/run1\")\r\nmodel.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])\r\n```\r\n\r\n**Other info / logs**\r\nHere is a gist with the full session output:\r\nhttps://gist.github.com/ageron/1d430d4a7716c7a2bae44ee82c321f01", "comments": ["Note: I checked, this issue does not occur with TF 1.12.", "Hello @martinwicke, In this issue @ageron has outlined that for r2.0 (a) tf.Keras model training when interrupted by TensorBoard callback and then resume training generates an exception (b) interrupting training and deleting logs and invoking TensorBoard callback generates an exception. Thanks. ", "@omalleyt12 are you working on TensorBoard integration? Can you take a look at this?", "@martinwicke Yep, will do", "Hi @ageron, to clarify, this issue is occurring after you've `Ctrl-C`'d a cell in the Jupyter notebook?", "Hi @omalleyt12 ,\r\nIt's either in the Python shell (command line) after I press Ctrl-C during training, or in a Jupyter notebook after I interrupt the kernel during training.\r\nHope this helps.", "I can confirm that I could reproduce the issue in a Python shell and in a Jupyter notebook, using the latest tf-nightly-2.0-preview (version '2.0.0-dev20190126', \"b'v1.12.0-6726-g5522d670af'\").", "Thanks for the reports!\r\n\r\nIssue 1 has the root cause #25707 which is actually independent of Keras.\r\n\r\nIssue 2 is not strictly speaking the same problem, but the proposed solution to #25707 would also fix issue 2, because it would mean each execution of the callback would create a new event file (the same as the 1.x behavior).", "The root cause #25707 has been fixed in 826027dbd4277a2636fc2935ed245700fd01e7cd, and propagated into the Keras callback by @wchargin in 059ea3ba68db861e40d750eba688281011d2735f.\r\n\r\nIssue 1 should be completely fixed, and issue 2 should only occur if you delete the event files in the middle of training (without interrupting the keras callback, as described in the OP) while the writer is still open and then it tries to flush to the no-longer-existing file.  In that case I think it's reasonable to report the error and I believe that matches previous behavior."]}, {"number": 24631, "title": "[XLA] Unimplemented inst in call fusion", "body": "If an unimplemented instruction is embedded inside a call or fusion operation then currently the system aborts having done a ConsumeValueOrDie.  This change allows the various clients of the evaluator to process a graph containing calls/fusions safely.\r\n\r\nApplies to all XLA systems.", "comments": ["is there a danger that the constant subexpression evaluator will eval the kRng instructions and eliminate them from the graph?  ", "used a custom-call", "> is there a danger that the constant subexpression evaluator will eval the kRng instructions and eliminate them from the graph?\r\n\r\nCSE doesn't evaluate instructions, it just commons them (i.e. replaces two instances of the same thing with one instance).  So the evaluator isn't used in CSE, and however it worked before it should continue working.\r\n\r\nCSE skips side-effecting ops, and kRNG is marked as side-effecting.  As it happens @bixia1 is working on changing this and making the side-effects more explicit in the HLO graph, e.g. passing the state (i.e. the seed) explicitly between kRNG ops.", "@jlebar sorry - i meant the constant subexpression removal step (which uses the evaluator) rather than the common subexpression removal.", "> sorry - i meant the constant subexpression removal step (which uses the evaluator) rather than the common subexpression removal.\r\n\r\nAh, what we call \"constant folding\".  `hlo_constant_folding.cc` explicitly skips kRNG ops.  Which I suppose is a bit of a hack, but there you go.", "@DavidNorman Can you please resolve the merge conflicts? Thanks!", "@DavidNorman gentle ping"]}, {"number": 24630, "title": "[XLA] Allow the reducewindow -> reduce optimization to be disabled", "body": "Add a flag to allow the reducewindow->reduce optimization to be disabled.\r\n\r\nThis applies to all platforms and all XLA backends, but by default the optimization is enabled, meaning no change in behaviour apart from for backends which specifically disable it.\r\n\r\n", "comments": []}, {"number": 24629, "title": "Fix incorrect display of keras model summary", "body": "This fix fixes the issue raised in #24627 where the display of keras model summary is incorrect (regression from 1.12). The reason was that `layer.name` (vs. layer object itself) should be used,\r\n\r\nThis fix fixes #24627.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}]