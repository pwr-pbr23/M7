[{"number": 22587, "title": "Fix validating callback type for distributed Keras", "body": "", "comments": ["@fchollet I have added some unit tests.", "@fchollet Can you review this again? Thanks.", "Gently pinging @fchollet ", "@EFanZh please resolve conflicts", "Hello, the content of this PR has been added in https://github.com/tensorflow/tensorflow/commit/c89c25b1f82394ebcefb7bf14dd8e96791aab328#diff-f107d83c80db7670b1808be2ebbf9999 and thus I'll close this PR. Thank you for the contribution."]}, {"number": 22586, "title": "GPU Memory Allocator OOM Before Limit Reached", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Sort of, the issue seems to be occurring with the ELMo tfhub model\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/a\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.11.0 (git version v1.11.0-0-gc19e29306c)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/a\r\n- **GCC/Compiler version (if compiling from source)**: N/a\r\n- **CUDA/cuDNN version**: 9.2/7.2.1\r\n- **GPU model and memory**: 1080 Ti (11177 MiB)\r\n- **Exact command to reproduce**:  \r\n```\r\nestimator = tf.estimator.DNNClassifier(\r\n                           hidden_units=[500, 100],\r\n                           feature_columns=[hub.text_embedding_column(key=\"cleaned\", module_spec=\"https://tfhub.dev/google/elmo/2\")],\r\n                           n_classes=2,\r\n                           optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.003))\r\nestimator.train(input_fn=train_input_fn, steps=1000)\r\n```\r\nYou'll need to provide the data, but this should reproduce the error.\r\n\r\n### Describe the problem\r\nAccording to the memory trace, only 4.47 GiB have been allocated before the OOM error. Tensorflow has 8.74 GiB available. Barely over half of the available memory is used when tensorflow throws the OOM.\r\n\r\n### Source code / logs\r\n```\r\n2018-09-27 22:58:14.599824: W tensorflow/core/graph/graph_constructor.cc:1263] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\r\n2018-09-27 22:58:16.160346: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-27 22:58:16.241355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-27 22:58:16.241872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 8.74GiB\r\n2018-09-27 22:58:16.241896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-27 22:58:16.465458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-27 22:58:16.465495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-27 22:58:16.465500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-27 22:58:16.465757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8437 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-27 22:58:22.105125: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.97GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-09-27 22:58:22.167820: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.03GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-09-27 22:58:25.227506: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-09-27 22:58:25.293991: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.03GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-09-27 22:58:29.024543: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.21GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-09-27 22:58:31.129676: W tensorflow/core/common_runtime/bfc_allocator.cc:215] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.43GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n[I 22:58:31.786 NotebookApp] Saving file at /Tensorflow Spam Classification.ipynb\r\n2018-09-27 22:58:41.991342: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB.  Current allocation summary follows.\r\n2018-09-27 22:58:41.991374: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (256):   Total Chunks: 54, Chunks in use: 54. 13.5KiB allocated for chunks. 13.5KiB in use in bin. 720B client-requested in use in bin.\r\n2018-09-27 22:58:41.991384: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (512):   Total Chunks: 8, Chunks in use: 7. 4.2KiB allocated for chunks. 3.8KiB in use in bin. 3.3KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991391: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1024):  Total Chunks: 3, Chunks in use: 3. 3.2KiB allocated for chunks. 3.2KiB in use in bin. 3.0KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991399: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2048):  Total Chunks: 7, Chunks in use: 4. 15.5KiB allocated for chunks. 8.2KiB in use in bin. 8.0KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991405: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4096):  Total Chunks: 6, Chunks in use: 2. 27.8KiB allocated for chunks. 8.0KiB in use in bin. 8.0KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991412: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8192):  Total Chunks: 7, Chunks in use: 5. 68.0KiB allocated for chunks. 48.0KiB in use in bin. 44.0KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991419: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16384):         Total Chunks: 2, Chunks in use: 0. 40.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-09-27 22:58:41.991424: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (32768):         Total Chunks: 2, Chunks in use: 1. 81.0KiB allocated for chunks. 32.0KiB in use in bin. 32.0KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991431: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (65536):         Total Chunks: 7, Chunks in use: 4. 540.0KiB allocated for chunks. 298.0KiB in use in bin. 256.0KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991439: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (131072):        Total Chunks: 3, Chunks in use: 1. 525.5KiB allocated for chunks. 195.5KiB in use in bin. 195.3KiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991445: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (262144):        Total Chunks: 4, Chunks in use: 4. 1.46MiB allocated for chunks. 1.46MiB in use in bin. 1.38MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991451: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (524288):        Total Chunks: 2, Chunks in use: 0. 1.66MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-09-27 22:58:41.991457: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (1048576):       Total Chunks: 1, Chunks in use: 1. 1.95MiB allocated for chunks. 1.95MiB in use in bin. 1.95MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991463: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-09-27 22:58:41.991468: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (4194304):       Total Chunks: 2, Chunks in use: 1. 8.00MiB allocated for chunks. 4.00MiB in use in bin. 4.00MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991475: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (8388608):       Total Chunks: 4, Chunks in use: 4. 32.00MiB allocated for chunks. 32.00MiB in use in bin. 32.00MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991481: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (16777216):      Total Chunks: 5, Chunks in use: 4. 84.00MiB allocated for chunks. 64.00MiB in use in bin. 64.00MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991488: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (33554432):      Total Chunks: 3, Chunks in use: 1. 115.59MiB allocated for chunks. 41.80MiB in use in bin. 41.80MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991494: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (67108864):      Total Chunks: 4, Chunks in use: 4. 256.00MiB allocated for chunks. 256.00MiB in use in bin. 256.00MiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991500: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-09-27 22:58:41.991506: I tensorflow/core/common_runtime/bfc_allocator.cc:610] Bin (268435456):     Total Chunks: 6, Chunks in use: 3. 7.75GiB allocated for chunks. 4.08GiB in use in bin. 4.08GiB client-requested in use in bin.\r\n2018-09-27 22:58:41.991512: I tensorflow/core/common_runtime/bfc_allocator.cc:626] Bin for 2.30GiB was 256.00MiB, Chunk State: \r\n2018-09-27 22:58:41.991523: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 615.25MiB | Requested Size: 225.70MiB | in_use: 0, prev:   Size: 41.80MiB | Requested Size: 41.80MiB | in_use: 1, next:   Size: 615.25MiB | Requested Size: 615.25MiB | in_use: 1\r\n2018-09-27 22:58:41.991532: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 1.17GiB | Requested Size: 1.17GiB | in_use: 0, prev:   Size: 615.25MiB | Requested Size: 615.25MiB | in_use: 1, next:   Size: 1.17GiB | Requested Size: 1.17GiB | in_use: 1\r\n2018-09-27 22:58:41.991539: I tensorflow/core/common_runtime/bfc_allocator.cc:632]   Size: 1.90GiB | Requested Size: 2.0KiB | in_use: 0, prev:   Size: 2.30GiB | Requested Size: 2.30GiB | in_use: 1\r\n2018-09-27 22:58:41.991545: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000000 of size 1280\r\n2018-09-27 22:58:41.991550: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000500 of size 256\r\n2018-09-27 22:58:41.991554: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000600 of size 256\r\n2018-09-27 22:58:41.991558: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000700 of size 256\r\n2018-09-27 22:58:41.991562: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000800 of size 256\r\n2018-09-27 22:58:41.991566: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000900 of size 256\r\n2018-09-27 22:58:41.991570: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000a00 of size 256\r\n2018-09-27 22:58:41.991575: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000b00 of size 256\r\n2018-09-27 22:58:41.991579: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000c00 of size 256\r\n2018-09-27 22:58:41.991583: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000d00 of size 256\r\n2018-09-27 22:58:41.991587: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000e00 of size 256\r\n2018-09-27 22:58:41.991591: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0000f00 of size 256\r\n2018-09-27 22:58:41.991595: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001000 of size 256\r\n2018-09-27 22:58:41.991599: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001100 of size 256\r\n2018-09-27 22:58:41.991603: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001200 of size 256\r\n2018-09-27 22:58:41.991607: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001300 of size 256\r\n2018-09-27 22:58:41.991611: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001400 of size 256\r\n2018-09-27 22:58:41.991615: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001500 of size 256\r\n2018-09-27 22:58:41.991619: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001600 of size 256\r\n2018-09-27 22:58:41.991623: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001700 of size 256\r\n2018-09-27 22:58:41.991627: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001800 of size 256\r\n2018-09-27 22:58:41.991631: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001900 of size 256\r\n2018-09-27 22:58:41.991635: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001a00 of size 256\r\n2018-09-27 22:58:41.991639: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001b00 of size 256\r\n2018-09-27 22:58:41.991644: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001c00 of size 512\r\n2018-09-27 22:58:41.991648: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001e00 of size 256\r\n2018-09-27 22:58:41.991652: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0001f00 of size 256\r\n2018-09-27 22:58:41.991656: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002000 of size 256\r\n2018-09-27 22:58:41.991660: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002100 of size 256\r\n2018-09-27 22:58:41.991664: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002200 of size 256\r\n2018-09-27 22:58:41.991668: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002300 of size 256\r\n2018-09-27 22:58:41.991672: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002400 of size 256\r\n2018-09-27 22:58:41.991676: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002500 of size 256\r\n2018-09-27 22:58:41.991680: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002600 of size 256\r\n2018-09-27 22:58:41.991684: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002700 of size 256\r\n2018-09-27 22:58:41.991688: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002800 of size 256\r\n2018-09-27 22:58:41.991692: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002900 of size 256\r\n2018-09-27 22:58:41.991696: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002a00 of size 256\r\n2018-09-27 22:58:41.991700: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002b00 of size 256\r\n2018-09-27 22:58:41.991704: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002c00 of size 256\r\n2018-09-27 22:58:41.991708: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002d00 of size 256\r\n2018-09-27 22:58:41.991712: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002e00 of size 256\r\n2018-09-27 22:58:41.991716: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0002f00 of size 256\r\n2018-09-27 22:58:41.991720: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003000 of size 256\r\n2018-09-27 22:58:41.991724: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003100 of size 256\r\n2018-09-27 22:58:41.991727: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003200 of size 256\r\n2018-09-27 22:58:41.991731: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003300 of size 512\r\n2018-09-27 22:58:41.991735: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003500 of size 256\r\n2018-09-27 22:58:41.991739: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003600 of size 256\r\n2018-09-27 22:58:41.991743: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003700 of size 256\r\n2018-09-27 22:58:41.991747: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003800 of size 768\r\n2018-09-27 22:58:41.991752: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003b00 of size 512\r\n2018-09-27 22:58:41.991756: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0003d00 of size 256\r\n2018-09-27 22:58:41.991760: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0003e00 of size 91136\r\n2018-09-27 22:58:41.991764: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a001a200 of size 8192\r\n2018-09-27 22:58:41.991768: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a001c200 of size 16640\r\n2018-09-27 22:58:41.991772: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0020300 of size 1024\r\n2018-09-27 22:58:41.991779: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0020700 of size 24576\r\n2018-09-27 22:58:41.991786: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0026700 of size 8192\r\n2018-09-27 22:58:41.991793: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0028700 of size 50176\r\n2018-09-27 22:58:41.991799: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0034b00 of size 256\r\n2018-09-27 22:58:41.991805: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0034c00 of size 12288\r\n2018-09-27 22:58:41.991812: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0037c00 of size 4096\r\n2018-09-27 22:58:41.991818: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0038c00 of size 512\r\n2018-09-27 22:58:41.991823: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0038e00 of size 512\r\n2018-09-27 22:58:41.991829: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a0039000 of size 4096\r\n2018-09-27 22:58:41.991834: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a003a000 of size 4096\r\n2018-09-27 22:58:41.991839: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a003b000 of size 256\r\n2018-09-27 22:58:41.991845: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a003b100 of size 74752\r\n2018-09-27 22:58:41.991850: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a004d500 of size 512\r\n2018-09-27 22:58:41.991855: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a004d700 of size 8192\r\n2018-09-27 22:58:41.991860: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a004f700 of size 8192\r\n2018-09-27 22:58:41.991866: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a0051700 of size 308736\r\n2018-09-27 22:58:41.991871: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a009cd00 of size 256\r\n2018-09-27 22:58:41.991876: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a009ce00 of size 198656\r\n2018-09-27 22:58:41.991882: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00cd600 of size 2048\r\n2018-09-27 22:58:41.991887: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a00cde00 of size 2048\r\n2018-09-27 22:58:41.991893: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00ce600 of size 256\r\n2018-09-27 22:58:41.991898: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00ce700 of size 256\r\n2018-09-27 22:58:41.991903: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a00ce800 of size 4352\r\n2018-09-27 22:58:41.991909: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00cf900 of size 2304\r\n2018-09-27 22:58:41.991914: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a00d0200 of size 67108864\r\n2018-09-27 22:58:41.991920: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40d0200 of size 12288\r\n2018-09-27 22:58:41.991925: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40d3200 of size 256\r\n2018-09-27 22:58:41.991930: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40d3300 of size 65536\r\n2018-09-27 22:58:41.991936: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a40e3300 of size 8388608\r\n2018-09-27 22:58:41.991941: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a48e3300 of size 81920\r\n2018-09-27 22:58:41.991946: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a48f7300 of size 67108864\r\n2018-09-27 22:58:41.991952: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a88f7300 of size 65536\r\n2018-09-27 22:58:41.991957: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a8907300 of size 8388608\r\n2018-09-27 22:58:41.991963: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9107300 of size 2048000\r\n2018-09-27 22:58:41.991968: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a92fb300 of size 2048\r\n2018-09-27 22:58:41.991973: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a92fbb00 of size 12288\r\n2018-09-27 22:58:41.991979: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a92feb00 of size 3328\r\n2018-09-27 22:58:41.991985: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a92ff800 of size 32768\r\n2018-09-27 22:58:41.991991: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9307800 of size 301056\r\n2018-09-27 22:58:41.991996: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9351000 of size 92160\r\n2018-09-27 22:58:41.992001: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9367800 of size 512\r\n2018-09-27 22:58:41.992007: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a9367a00 of size 2048\r\n2018-09-27 22:58:41.992012: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9368200 of size 1024\r\n2018-09-27 22:58:41.992017: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a9368600 of size 7424\r\n2018-09-27 22:58:41.992022: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a936a300 of size 2048\r\n2018-09-27 22:58:41.992028: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a936ab00 of size 4352\r\n2018-09-27 22:58:41.992033: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a936bc00 of size 200192\r\n2018-09-27 22:58:41.992038: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a939ca00 of size 917504\r\n2018-09-27 22:58:41.992044: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a947ca00 of size 81920\r\n2018-09-27 22:58:41.992049: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94a9490a00 of size 139264\r\n2018-09-27 22:58:41.992054: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a94b2a00 of size 458752\r\n2018-09-27 22:58:41.992060: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9522a00 of size 458752\r\n2018-09-27 22:58:41.992065: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94a9592a00 of size 67108864\r\n2018-09-27 22:58:41.992070: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94ad592a00 of size 819200\r\n2018-09-27 22:58:41.992075: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94ad65aa00 of size 8388608\r\n2018-09-27 22:58:41.992081: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94ade5aa00 of size 67108864\r\n2018-09-27 22:58:41.992086: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b1e5aa00 of size 8388608\r\n2018-09-27 22:58:41.992092: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b265aa00 of size 4194304\r\n2018-09-27 22:58:41.992097: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94b2a5aa00 of size 4194304\r\n2018-09-27 22:58:41.992102: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b2e5aa00 of size 16777216\r\n2018-09-27 22:58:41.992108: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94b3e5aa00 of size 20971520\r\n2018-09-27 22:58:41.992113: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b525aa00 of size 16777216\r\n2018-09-27 22:58:41.992118: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94b625aa00 of size 33554432\r\n2018-09-27 22:58:41.992123: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b825aa00 of size 16777216\r\n2018-09-27 22:58:41.992129: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94b925aa00 of size 16777216\r\n2018-09-27 22:58:41.992134: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94ba25aa00 of size 43827200\r\n2018-09-27 22:58:41.992139: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94bcc26a00 of size 43827200\r\n2018-09-27 22:58:41.992145: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f94bf5f2a00 of size 645136384\r\n2018-09-27 22:58:41.992150: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f94e5d32a00 of size 645136384\r\n2018-09-27 22:58:41.992155: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f950c472a00 of size 1262223360\r\n2018-09-27 22:58:41.992161: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f9557832a00 of size 1262223360\r\n2018-09-27 22:58:41.992166: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Chunk at 0x7f95a2bf2a00 of size 2468347904\r\n2018-09-27 22:58:41.992172: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Free  at 0x7f9635df2a00 of size 2038319360\r\n2018-09-27 22:58:41.992177: I tensorflow/core/common_runtime/bfc_allocator.cc:651]      Summary of in-use Chunks by size: \r\n2018-09-27 22:58:41.992184: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 54 Chunks of size 256 totalling 13.5KiB\r\n2018-09-27 22:58:41.992190: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 6 Chunks of size 512 totalling 3.0KiB\r\n2018-09-27 22:58:41.992196: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 768 totalling 768B\r\n2018-09-27 22:58:41.992202: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 1024 totalling 2.0KiB\r\n2018-09-27 22:58:41.992208: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 1280 totalling 1.2KiB\r\n2018-09-27 22:58:41.992213: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 3 Chunks of size 2048 totalling 6.0KiB\r\n2018-09-27 22:58:41.992219: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2304 totalling 2.2KiB\r\n2018-09-27 22:58:41.992225: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 4096 totalling 8.0KiB\r\n2018-09-27 22:58:41.992231: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 3 Chunks of size 8192 totalling 24.0KiB\r\n2018-09-27 22:58:41.992238: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 12288 totalling 24.0KiB\r\n2018-09-27 22:58:41.992244: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 32768 totalling 32.0KiB\r\n2018-09-27 22:58:41.992250: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 65536 totalling 128.0KiB\r\n2018-09-27 22:58:41.992256: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 81920 totalling 80.0KiB\r\n2018-09-27 22:58:41.992262: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 92160 totalling 90.0KiB\r\n2018-09-27 22:58:41.992268: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 200192 totalling 195.5KiB\r\n2018-09-27 22:58:41.992274: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 301056 totalling 294.0KiB\r\n2018-09-27 22:58:41.992280: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 308736 totalling 301.5KiB\r\n2018-09-27 22:58:41.992286: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 2 Chunks of size 458752 totalling 896.0KiB\r\n2018-09-27 22:58:41.992292: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2048000 totalling 1.95MiB\r\n2018-09-27 22:58:41.992298: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 4194304 totalling 4.00MiB\r\n2018-09-27 22:58:41.992304: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 8388608 totalling 32.00MiB\r\n2018-09-27 22:58:41.992310: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 16777216 totalling 64.00MiB\r\n2018-09-27 22:58:41.992316: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 43827200 totalling 41.80MiB\r\n2018-09-27 22:58:41.992322: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 4 Chunks of size 67108864 totalling 256.00MiB\r\n2018-09-27 22:58:41.992328: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 645136384 totalling 615.25MiB\r\n2018-09-27 22:58:41.992333: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 1262223360 totalling 1.17GiB\r\n2018-09-27 22:58:41.992339: I tensorflow/core/common_runtime/bfc_allocator.cc:654] 1 Chunks of size 2468347904 totalling 2.30GiB\r\n2018-09-27 22:58:41.992345: I tensorflow/core/common_runtime/bfc_allocator.cc:658] Sum Total of in-use chunks: 4.47GiB\r\n2018-09-27 22:58:41.992352: I tensorflow/core/common_runtime/bfc_allocator.cc:660] Stats: \r\nLimit:                  8847717172\r\nInUse:                  4797028096\r\nMaxInUse:               7450003456\r\nNumAllocs:                   54412\r\nMaxAllocSize:           2468347904\r\n\r\n2018-09-27 22:58:41.992366: W tensorflow/core/common_runtime/bfc_allocator.cc:275] ******_______********_____________*******************************************_______________________\r\n2018-09-27 22:58:41.992384: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at conv_ops.cc:693 : Resource exhausted: OOM when allocating tensor with shape[128,1024,107,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n```", "comments": ["Thanks for posting with the full log!\r\n\r\nIt is failing while trying to allocate 2.30GiB memory that is not available in any of the following three chunks (from the log) because of the fragmentation.\r\n\r\nSize: 615.25MiB | Requested Size: 225.70MiB | in_use: 0, prev:   Size: 41.80MiB | Requested Size: 41.80MiB | in_use: 1, next:   Size: 615.25MiB | Requested Size: 615.25MiB | in_use: 1\r\nSize: 1.17GiB | Requested Size: 1.17GiB | in_use: 0, prev:   Size: 615.25MiB | Requested Size: 615.25MiB | in_use: 1, next:   Size: 1.17GiB | Requested Size: 1.17GiB | in_use: 1\r\nSize: 1.90GiB | Requested Size: 2.0KiB | in_use: 0, prev:   Size: 2.30GiB | Requested Size: 2.30GiB | in_use: 1\r\n\r\nConsider reducing size of the network and/or batch size so that it does not allocate very large tensors. If you still run into issues or need further advice, try Stack Overflow for further help and support."]}, {"number": 22585, "title": "Tensorflow Class Diagram for android", "body": "Hi,\r\n\r\nI could not find any info on Tensorflow Class Diagram for android, which helps in understanding what are classes specific for TF android Project and Understanding the process flow in TF Android Project.\r\n\r\nCan any one help me out with the Class Diagram of the TF  Android Project or any details about the working structure of TF in Android.\r\n\r\nThank You.\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@ICL-Developer Hi, you can go through [this](https://www.tensorflow.org/lite/tfmobile/android_build) for detailed information about building Tensorflow on Android.\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22584, "title": "MappedByteBuffer is not a valid flatbuffer model TFLITE Model_conversion_issue", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: NVIDIA 12 GB\r\n- **Exact command to reproduce**:\r\n\r\nI have a model in which i have connected two mobilenet models . These are the following steps:\r\n\r\n1. Train the model. - Success\r\n2. Freeze the graph using eval graph - Success\r\n///Defiine the graph then \r\n` \r\n  with tf.Session() as sess:\r\n\r\n        saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\r\n        g = tf.get_default_graph()\r\n        tf.contrib.quantize.create_eval_graph(input_graph=g)\r\n        sess.run(tf.global_variables_initializer())\r\n        saver.restore(sess, '/home/ubuntu/raj/research/trimap_mobnet/model.ckpt-205740')\r\n        input_graph_def = graph.as_graph_def()\r\n        output_graph_def = graph_util.convert_variables_to_constants(\r\n            sess,\r\n            input_graph_def,\r\n           [_OUTPUT_NAME])        \r\n        with tf.gfile.GFile(output_graph_name, \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())`\r\n3. Toco convert -Success\r\n\r\ntflite_convert  --output_file=./lite.tflite  --input=./frozen.pb --input_arrays=image_ph --output_arrays=SemanticPredictions --input_shapes=1,481,481,3\r\n\r\n4 . Load the model on phone - Error\r\n\r\nWhile loading the lite model on phone by running the command\r\n\r\n`var localSource = FirebaseLocalModelSource.Builder(\"my_local_model\")\r\n               .setAssetFilePath(\"lite.tflite\")\r\n               .build()\r\n       FirebaseModelManager.getInstance().registerLocalModelSource(localSource)\r\n        var options = FirebaseModelOptions.Builder()\r\n               .setLocalModelName(\"my_local_model\")\r\n               .build()\r\n       firebaseInterpreter = FirebaseModelInterpreter.getInstance(options)\r\n       inputOutputOptions = FirebaseModelInputOutputOptions.Builder()\r\n               .setInputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3))\r\n               .setOutputFormat(0, FirebaseModelDataType.FLOAT32, intArrayOf(1, 481, 481, 3))\r\n               .build()`\r\n\r\n i get the following error:\r\nMappedByteBuffer is not a valid flatbuffer model\r\n\r\nAny reason why am i getting this error or is it a tensorflow converter bug?`", "comments": ["In detail here is what i am doing:\r\n\r\nIMAGE >> MOBILENET MODEL1 >>ARGMAX>> OUTPUT1 >> SCALAR MULTIPLICATION OF OUTPUT1 with 255 >> CONCAT THIS WITH IMAGE >>MOBILENET MODEL2 >>OUTPUT2>>SCALAR MULTIPLICATION OF OUTPUT2 WITH 255 >> CONCAT THIS WITH IMAGE>> 3 LAYERS OF 3*3 CONVOLUTIONS>>  SIGMOID >> PREDICTION >> SPLIT IMAGE IN 3 CHANNELS >> MULTIPLY PREDICTION WITH ALL 3 CHANNELS >> CONCAT 3 CHANNELS  >> FINAL OUTPUT", "After removing Deconvolution and replacing it with bilinear resizing , i get the following error during toco convert : \r\n\r\nCheck failed : input_array_shape.dimension_count()==output_array_shape.dimension_count() (5 vs 4)", "Any update on this ??", "@Raj-08 You would need to check whether there are some ops in your model not supported by tflite. To replace Deconvolution, you can apply `tf.nn.conv2d_transpose` rather than bilinear resizing. Also make sure the shape of input and output array match. Lastly, use the latest TensorFlow version.", "Sure will try that out !!", "I used tf.nn.conv2d_transpose everywhere replacing it with bilinear resizing. \r\nStill i get the same error  MappedByteBuffer is not a valid flatbuffer model", "I was able to convert a simple model with `tf.nn.conv2d_transpose` without running errors.  You may have other ops in your model that are not supported by tflite. ", "Is argmax supported ?", "@Raj-08 Yes, argmax and argmin are supported in TensorFlow 1.11.0", "Closing this, feel free to reopen if issue persists.", "\r\n> I was able to convert a simple model with `tf.nn.conv2d_transpose` without running errors. You may have other ops in your model that are not supported by tflite.\r\n\r\nSorry to post on the closed issue.\r\n@wt-huang  did you test your tf.nn.conv2d_transpose outputs, i couldn't get expected results as in https://github.com/tensorflow/tensorflow/issues/25462, thanks."]}, {"number": 22583, "title": "Tensorflow 1.11.0 build failed with ngraph", "body": "\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.5\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nNo\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)** :\r\n1.11.0\r\n\r\n- **Python version**:\r\n3.5\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.17.2\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4.0\r\n\r\n- **CUDA/cuDNN version**:\r\ncuda10\r\n\r\nCudnn 7.3\r\n\r\n- **GPU model and memory**:\r\nGTX1070\r\n\r\n- **Exact command to reproduce**:\r\nconfigure  TensorFlow with nGraph support\r\n\r\nThen build\r\n```\r\nbazel build --config=opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package\r\n\r\n```\r\n```\r\nERROR: /home/bernard/.cache/bazel/_bazel_bernard/b48040ee72f8a3090b277569ef55e694/external/ngraph_tf/BUILD.bazel:19:1: C++ compilation of rule '@ngraph_tf//:ngraph_tf' failed (Exit 1)\r\nIn file included from external/org_tensorflow/tensorflow/core/framework/common_shape_fns.h:22:0,\r\n                 from external/org_tensorflow/tensorflow/core/framework/resource_mgr.h:24,\r\n                 from external/org_tensorflow/tensorflow/core/common_runtime/device.h:43,\r\n                 from external/org_tensorflow/tensorflow/core/common_runtime/device_set.h:23,\r\n                 from external/org_tensorflow/tensorflow/core/common_runtime/optimization_registry.h:25,\r\n                 from external/ngraph_tf/src/ngraph_encapsulate_pass.cc:23:\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {\r\n                                             ^\r\nexternal/ngraph_tf/src/ngraph_encapsulate_pass.cc: In member function 'tensorflow::Status ngraph_bridge::NGraphEncapsulatePass::EncapsulateFunctions(tensorflow::Graph*)':\r\nexternal/ngraph_tf/src/ngraph_encapsulate_pass.cc:393:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n           if (i < node->requested_inputs().size()) {\r\n                 ^\r\nexternal/ngraph_tf/src/ngraph_encapsulate_pass.cc:414:42: error: 'class absl::string_view' has no member named 'ToString'\r\n             cluster_idx, tensor_id.first.ToString(), tensor_id.second));\r\n                                          ^\r\nIn file included from external/org_tensorflow/tensorflow/core/platform/default/logging.h:24:0,\r\n                 from external/org_tensorflow/tensorflow/core/platform/logging.h:25,\r\n                 from external/org_tensorflow/tensorflow/core/lib/core/refcount.h:22,\r\n                 from external/org_tensorflow/tensorflow/core/platform/tensor_coding.h:21,\r\n                 from external/org_tensorflow/tensorflow/core/framework/resource_handle.h:19,\r\n                 from external/org_tensorflow/tensorflow/core/framework/allocator.h:24,\r\n                 from external/org_tensorflow/tensorflow/core/common_runtime/device.h:35,\r\n                 from external/org_tensorflow/tensorflow/core/common_runtime/device_set.h:23,\r\n                 from external/org_tensorflow/tensorflow/core/common_runtime/optimization_registry.h:25,\r\n                 from external/ngraph_tf/src/ngraph_encapsulate_pass.cc:23:\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:452:47:   required from here\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n                             ^\r\nexternal/org_tensorflow/tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attributes.size())\r\n   ^\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:461:54:   required from here\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n                             ^\r\nexternal/org_tensorflow/tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'\r\n #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n                                               ^\r\nexternal/org_tensorflow/tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'\r\n   CHECK(index >= 0 && index < dimension_attribute.size())\r\n   ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3531.592s, Critical Path: 131.48s\r\nINFO: 5128 processes: 5128 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["@aaroey Could you loop in Intel on this?", "@gunan Sure! @avijit-nervana do you have an idea?", "@aaroey and @gunan - we will look into it and report back\r\n", "@beew @aaroey This compilation issue was fixed and merged to master on 9/18:\r\nhttps://github.com/tensorflow/tensorflow/commit/08af8cac22af4cc430e092b6218ca77736efb82c\r\nBy that time the release branch for 1.11 was already cut - so this fix is not available in the 1.11.0 release. \r\n\r\nHowever, if you pick up the latest master or the next TensorFlow release - this will go away. Please let us know if you have any questions/comments.", "As this issue has invited community support, please remove the assignee. Otherwise, remove the `community support` label. Thank you.", "Closing this issue, as it seems to be resolved at head.\r\nThank you very much @avijit-nervana !"]}, {"number": 22582, "title": "Modify code in docs to conform to Python syntax", "body": "Corrected a very few code in the docs to let it work only by copy-and-paste.", "comments": ["Thanks for the fix @rnarkk ", "@av8ramit I just resolved the conflict and the differences remain superficial. Could you re-review this PR?", "Seems the same a little conflict has happened so I will resolve it and request someone's review again?", "Closing this since it was merged in https://github.com/tensorflow/tensorflow/commit/73177c09d6ef3716aa449c1f0d7afd992c91dcd7"]}, {"number": 22581, "title": "Calling `tf.image.non_max_suppression()` in parallel `tf.while_loop()` causes crash", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2 (and also Windows 10)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary (via `pip install tensorflow-gpu`)\r\n- **TensorFlow version (use command below)**: v1.11.0-0-gc19e29306c 1.11.0\r\n- **Python version**: 3.6.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.1.4\r\n- **GPU model and memory**: NVIDIA GeForce GTX 1070 (8 GiB)\r\n- **Exact command to reproduce**: `python nmsstest.py`; see below for the content of `nmstest.py`\r\n\r\n### Describe the problem\r\nThe code in the following section, which calls `tf.image.non_max_suppression()` in `tf.while_loop()` many times, crashes abnormally.\r\nCrash reason (and loop count) varies from time to time, for example:\r\n* `F tensorflow/core/common_runtime/bfc_allocator.cc:384] Check failed: h != kInvalidChunkHandle` at loop `i == 140`\r\n* `F tensorflow/core/common_runtime/bfc_allocator.cc:462] Check failed: c->in_use() && (c->bin_num == kInvalidBinNum)` at loop `i == 76`\r\n* `Bus error` at loop `i == 34`\r\n* heap corruption reported by libc (see the following section) at loop `i == 35`\r\n* sometimes it crashes sliently without any logs (on Windows)\r\n\r\nI notice that:\r\n* it's since TensorFlow 1.11.0rc0; TF 1.10.1 was okay\r\n* it also reproduces on Windows\r\n* it also reproduces on CPU version (`pip install tensorflow`)\r\n* it does **not** reproduce if `num_threads=1`; calling `tf.image.non_max_suppression()` in parallel seems the trigger\r\n* even when I gave a fixed seed to `tf.random_uniform()`, crash cause varied\r\n\r\n### Source code / logs\r\n\r\nThe code to reproduce the problem is as follows:\r\n```\r\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    # crashes iff num_threads > 1 on TensorFlow >= 1.11.rc0\r\n    num_threads = 10\r\n\r\n    top_k = 1\r\n    batch_size = 32\r\n    num_boxes = 10000\r\n    boxes_op = tf.random_uniform((batch_size,num_boxes,4), 0, 1)\r\n    scores_op = tf.random_uniform((batch_size,num_boxes), 0, 1)\r\n    indices_op = tf.while_loop(\r\n        (lambda b, ta: True),\r\n        (lambda b, ta: (b+1, ta.write(b, tf.image.non_max_suppression(boxes_op[b], scores_op[b], top_k, iou_threshold=0.3)))),\r\n        (tf.constant(0),\r\n         tf.TensorArray(tf.int32, size=batch_size, infer_shape=False, element_shape=(top_k,))),\r\n        back_prop=False,\r\n        parallel_iterations=num_threads,\r\n        maximum_iterations=batch_size)[1].stack()\r\n\r\n    with tf.Session() as session:\r\n        for i in range(1000):\r\n            indices = session.run(indices_op)\r\n            print(f'#{i}: {indices.shape}')\r\n```\r\n\r\n```\r\n$ python nmstest.py\r\n2018-09-28 13:18:42.760545: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-28 13:18:43.123328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-28 13:18:43.124208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 7.83GiB\r\n2018-09-28 13:18:43.124232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-28 13:18:43.351813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-28 13:18:43.351847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-28 13:18:43.351859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-28 13:18:43.352078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7558 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n#0: (32, 1)\r\n#1: (32, 1)\r\n#2: (32, 1)\r\n#3: (32, 1)\r\n#4: (32, 1)\r\n#5: (32, 1)\r\n#6: (32, 1)\r\n#7: (32, 1)\r\n#8: (32, 1)\r\n#9: (32, 1)\r\n#10: (32, 1)\r\n#11: (32, 1)\r\n#12: (32, 1)\r\n#13: (32, 1)\r\n#14: (32, 1)\r\n#15: (32, 1)\r\n#16: (32, 1)\r\n#17: (32, 1)\r\n#18: (32, 1)\r\n#19: (32, 1)\r\n#20: (32, 1)\r\n#21: (32, 1)\r\n#22: (32, 1)\r\n#23: (32, 1)\r\n#24: (32, 1)\r\n#25: (32, 1)\r\n#26: (32, 1)\r\n#27: (32, 1)\r\n#28: (32, 1)\r\n#29: (32, 1)\r\n#30: (32, 1)\r\n#31: (32, 1)\r\n#32: (32, 1)\r\n#33: (32, 1)\r\n#34: (32, 1)\r\n#35: (32, 1)\r\n*** Error in `python': malloc(): smallbin double linked list corrupted: 0x00007fb6c001c960 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6(+0x7f5e4)[0x7fb9816b75e4]\r\n/lib64/libc.so.6(+0x82d00)[0x7fb9816bad00]\r\n/lib64/libc.so.6(__libc_malloc+0x4c)[0x7fb9816bd84c]\r\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(_Znwm+0x16)[0x7fb936087084]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorENS_19AllocatorAttributesE+0x48)[0x7fb93f0c0608]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow15OpKernelContext15allocate_outputEiRKNS_11TensorShapeEPPNS_6TensorE+0xc5)[0x7fb93f0c0785]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7MergeOp7ComputeEPNS_15OpKernelContextE+0xa4)[0x7fb9423854b4]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice13ComputeHelperEPNS_8OpKernelEPNS_15OpKernelContextE+0x37d)[0x7fb93f23ac9d]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0x8d)[0x7fb93f23b1dd]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63a4bc)[0x7fb93f2844bc]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(+0x63ae2a)[0x7fb93f284e2a]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZN5Eigen26NonBlockingThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x21a)[0x7fb93f2f296a]\r\n/home/hyabe/anaconda3/envs/work/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x32)[0x7fb93f2f1a12]\r\n/home/hyabe/anaconda3/envs/work/bin/../lib/libstdc++.so.6(+0xb8678)[0x7fb9360a2678]\r\n/lib64/libpthread.so.0(+0x7e25)[0x7fb981a0ce25]\r\n/lib64/libc.so.6(clone+0x6d)[0x7fb981736bad]\r\n======= Memory map: ========\r\n200000000-200200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n200200000-200400000 ---p 00000000 00:00 0 \r\n200400000-200404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n200404000-200600000 ---p 00000000 00:00 0 \r\n200600000-200a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n200a00000-201200000 ---p 00000000 00:00 0 \r\n201200000-201204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201204000-201400000 ---p 00000000 00:00 0 \r\n201400000-201800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201800000-201804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201804000-201a00000 ---p 00000000 00:00 0 \r\n201a00000-201e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201e00000-201e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n201e04000-202000000 ---p 00000000 00:00 0 \r\n202000000-202400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202400000-202404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202404000-202600000 ---p 00000000 00:00 0 \r\n202600000-202a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202a00000-202a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n202a04000-202c00000 ---p 00000000 00:00 0 \r\n202c00000-203000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203000000-203004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203004000-203200000 ---p 00000000 00:00 0 \r\n203200000-203600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203600000-203604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203604000-203800000 ---p 00000000 00:00 0 \r\n203800000-203c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203c00000-203c04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n203c04000-203e00000 ---p 00000000 00:00 0 \r\n203e00000-204200000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204200000-204204000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204204000-204400000 ---p 00000000 00:00 0 \r\n204400000-204800000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204800000-204804000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204804000-204a00000 ---p 00000000 00:00 0 \r\n204a00000-204e00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204e00000-204e04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n204e04000-205000000 ---p 00000000 00:00 0 \r\n205000000-205400000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205400000-205404000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205404000-205600000 ---p 00000000 00:00 0 \r\n205600000-205a00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205a00000-205a04000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n205a04000-205c00000 ---p 00000000 00:00 0 \r\n205c00000-206000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206000000-206004000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206004000-206200000 ---p 00000000 00:00 0 \r\n206200000-206600000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206600000-206604000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206604000-206800000 ---p 00000000 00:00 0 \r\n206800000-206c00000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n206c00000-206e00000 ---p 00000000 00:00 0 \r\n206e00000-207000000 rw-s 00000000 00:05 25687                            /dev/nvidiactl\r\n207000000-300200000 ---p 00000000 00:00 0 \r\n10000000000-10204000000 ---p 00000000 00:00 0 \r\n5596eb5dd000-5596eb89c000 r-xp 00000000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\r\n5596eba9c000-5596eba9f000 r--p 002bf000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\r\n5596eba9f000-5596ebb02000 rw-p 002c2000 fd:02 86155766                   /home/hyabe/anaconda3/envs/work/bin/python3.6\r\n5596ebb02000-5596ebb33000 rw-p 00000000 00:00 0 \r\n5596eda67000-5597016ee000 rw-p 00000000 00:00 0                          [heap]\r\n7fb698000000-7fb69803a000 rw-p 00000000 00:00 0 \r\n7fb69803a000-7fb69c000000 ---p 00000000 00:00 0 \r\n7fb69c000000-7fb69c021000 rw-p 00000000 00:00 0 \r\n7fb69c021000-7fb6a0000000 ---p 00000000 00:00 0 \r\n7fb6a0000000-7fb6a0036000 rw-p 00000000 00:00 0 \r\n7fb6a0036000-7fb6a4000000 ---p 00000000 00:00 0 \r\n7fb6a4000000-7fb6a4049000 rw-p 00000000 00:00 0 \r\n7fb6a4049000-7fb6a8000000 ---p 00000000 00:00 0 \r\n7fb6a8000000-7fb6a8039000 rw-p 00000000 00:00 0 \r\n7fb6a8039000-7fb6ac000000 ---p 00000000 00:00 0 \r\n7fb6b0000000-7fb6b0036000 rw-p 00000000 00:00 0 \r\n7fb6b0036000-7fb6b4000000 ---p 00000000 00:00 0 \r\n7fb6b4000000-7fb6b403b000 rw-p 00000000 00:00 0 \r\n7fb6b403b000-7fb6b8000000 ---p 00000000 00:00 0 \r\n7fb6b8000000-7fb6b8037000 rw-p 00000000 00:00 0 \r\n7fb6b8037000-7fb6bc000000 ---p 00000000 00:00 0 \r\n7fb6c0000000-7fb6c003c000 rw-p 00000000 00:00 0 \r\n7fb6c003c000-7fb6c4000000 ---p 00000000 00:00 0 \r\n7fb6c5400000-7fb6c5a00000 rw-p 00000000 00:00 0 \r\n7fb6c5ad4000-7fb6c5ad5000 ---p 00000000 00:00 0 \r\n7fb6c5ad5000-7fb6c6326000 rw-p 00000000 00:00 0 \r\n7fb6c6326000-7fb6c6327000 ---p 00000000 00:00 0 \r\n7fb6c6327000-7fb6c6b78000 rw-p 00000000 00:00 0 \r\n7fb6c6b78000-7fb6c6b79000 ---p 00000000 00:00 0 \r\n7fb6c6b79000-7fb6d6000000 rw-p 00000000 00:00 0 \r\n7fb6d6000000-7fb8ae800000 ---p 00000000 00:00 0 \r\n7fb8ae800000-7fb8aea00000 rw-s 00000000 00:04 31968                      /dev/zero (deleted)\r\n7fb8aea00000-7fb8aec00000 rw-s 00000000 00:04 29039                      /dev/zero (deleted)\r\n7fb8aec00000-7fb8b0000000 ---p 00000000 00:00 0 \r\n7fb8b0000000-7fb8b0021000 rw-p 00000000 00:00 0 \r\n7fb8b0021000-7fb8b4000000 ---p 00000000 00:00 0 \r\n7fb8b4000000-7fb8b4021000 rw-p 00000000 00:00 0 \r\n7fb8b4021000-7fb8b8000000 ---p 00000000 00:00 0 \r\n7fb8b8000000-7fb8bc000000 ---p 00000000 00:00 0 \r\n7fb8bc000000-7fb8bc021000 rw-p 00000000 00:00 0 \r\n7fb8bc021000-7fb8c0000000 ---p 00000000 00:00 0 \r\n7fb8c0000000-7fb8c0001000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0001000-7fb8c0002000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0002000-7fb8c0003000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0003000-7fb8c0004000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0004000-7fb8c0005000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0005000-7fb8c0006000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0006000-7fb8c0007000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0007000-7fb8c0008000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0008000-7fb8c0009000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0009000-7fb8c000a000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000a000-7fb8c000b000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000b000-7fb8c000c000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000c000-7fb8c000d000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000d000-7fb8c000e000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000e000-7fb8c000f000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c000f000-7fb8c0010000 rw-s 00000000 00:05 25688                      /dev/nvidia0\r\n7fb8c0010000-7fb8d0000000 ---p 00000000 00:00 0 \r\n7fb8d0000000-7fb8d0021000 rw-p 00000000 00:00 0 \r\n7fb8d0021000-7fb8d4000000 ---p 00000000 00:00 0 \r\n7fb8d4200000-7fb8d4600000 rw-p 00000000 00:00 0 \r\n7fb8d470a000-7fb8d470b000 ---p 00000000 00:00 0 \r\n7fb8d470b000-7fb8d4f5c000 rw-p 00000000 00:00 0 \r\n7fb8d4f5c000-7fb8d4f5d000 ---p 00000000 00:00 0 \r\n7fb8d4f5d000-7fb8d57ae000 rw-p 00000000 00:00 0 \r\n7fb8d57ae000-7fb8d57af000 ---p 00000000 00:00 0 \r\n7fb8d57af000-7fb8d6000000 rw-p 00000000 00:00 0 \r\n7fb8d6000000-7fb8d6200000 ---p 00000000 00:00 0 \r\n7fb8d6200000-7fb8d6400000 rw-s 00000000 00:04 31965                      /dev/zero (deleted)\r\n7fb8d6400000-7fb8d6600000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\r\n7fb8d6600000-7fb8d6800000 rw-s 00000000 00:04 31966                      /dev/zero (deleted)\r\n7fb8d6800000-7fb8d6a00000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\r\n7fb8d6a00000-7fb8d6c00000 ---p 00000000 00:00 0 \r\n7fb8d6c00000-7fb8d6ed6000 rw-s 00000000 00:05 25687                      /dev/nvidiactl\r\n7fb8d6ed6000-7fb8d8000000 ---p 00000000 00:00 0 \r\n7fb8d8000000-7fb8d8021000 rw-p 00000000 00:00 0 \r\n7fb8d8021000-7fb8dc000000 ---p 00000000 00:00 0 \r\n7fb8dc200000-7fb8dc600000 rw-p 00000000 00:00 0 \r\n7fb8dc70a000-7fb8dc70b000 ---p 00000000 00:00 0 \r\n7fb8dc70b000-7fb8dcf5c000 rw-p 00000000 00:00 0 \r\n7fb8dcf5c000-7fb8dcf5d000 ---p 00000000 00:00 0 \r\n7fb8dcf5d000-7fb8dd7ae000 rw-p 00000000 00:00 0 \r\n7fb8dd7ae000-7fb8dd7af000 ---p 00000000 00:00 0 \r\n7fb8dd7af000-7fb8de000000 rw-p 00000000 00:00 0 \r\n7fb8de000000-7fb8e4000000 ---p 00000000 00:00 0 \r\n7fb8e4000000-7fb8e4021000 rw-p 00000000 00:00 0 \r\n7fb8e4021000-7fb8e8000000 ---p 00000000 00:00 0 \r\n7fb8e8000000-7fb8e8021000 rw-p 00000000 00:00 0 \r\n7fb8e8021000-7fb8ec000000 ---p 00000000 00:00 0 \r\n7fb8ec000000-7fb8ec021000 rw-p 00000000 00:00 0 \r\n7fb8ec021000-7fb8f0000000 ---p 00000000 00:00 0 \r\n7fb8f0000000-7fb8f0021000 rw-p 00000000 00:00 0 \r\n7fb8f0021000-7fb8f4000000 ---p 00000000 00:00 0 \r\n7fb8f4200000-7fb8f4400000 rw-p 00000000 00:00 0 \r\n7fb8f45c2000-7fb8f45c3000 ---p 00000000 00:00 0 \r\n7fb8f45c3000-7fb8f4e14000 rw-p 00000000 00:00 0 \r\n7fb8f4e14000-7fb8f4e15000 ---p 00000000 00:00 0 \r\n7fb8f4e15000-7fb8f5666000 rw-p 00000000 00:00 0 \r\n7fb8f5666000-7fb8f5667000 ---p 00000000 00:00 0 \r\n7fb8f5667000-7fb8f5eb8000 rw-p 00000000 00:00 0 \r\n7fb8f5eb8000-7fb8f5eb9000 ---p 00000000 00:00 0 \r\n7fb8f5eb9000-7fb8f670a000 rw-p 00000000 00:00 0 \r\n7fb8f670a000-7fb8f670b000 ---p 00000000 00:00 0 \r\n7fb8f670b000-7fb8f6f5c000 rw-p 00000000 00:00 0 \r\n7fb8f6f5c000-7fb8f6f5d000 ---p 00000000 00:00 0 \r\n7fb8f6f5d000-7fb8f77ae000 rw-p 00000000 00:00 0 \r\n7fb8f77ae000-7fb8f77af00Aborted\r\n```", "comments": ["@hyabe Can you try with the tensorflow nightly build and let us know if you still face this issue ?", "@harshini-gadige yes, it still reproduces on today's nightly (1.12.0-dev20181003 b'v1.9.0-rc2-5200-g3d452dbcf7').", "This bug is not evident in `<=1.10`, but I am having this issue when upgrading to `==1.11`.", "@hyabe TensorFlow sometimes has multiple implementations for various ops. As a temporary workaround, you can use the (presumably) previous implementation of `non_max_suppression`: `non_max_suppression_v2`:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops import gen_image_ops\r\n\r\ntf.image.non_max_suppression = gen_image_ops.non_max_suppression_v2 # or v3, v4\r\n# ... your code\r\n```\r\n\r\nTested on Py3.6.5 + CPU Only!\r\n\r\n(Reference [non_max_suppression tests](https://github.com/tensorflow/tensorflow/blob/28eeda839f124cf5ba648576e86214b38141e4ab/tensorflow/python/ops/image_ops_test.py#L3666-L3705))", "@wenkesj Thank you for your suggestion.  `non_max_suppression_v2` works for me too on the following TensorFlow versions:\r\n* 1.11.0 (GPU and CPU) on Linux\r\n* Nightly used in [the comment above](https://github.com/tensorflow/tensorflow/issues/22581#issuecomment-426868736) (GPU) on Linux\r\n* 1.11.0 (GPU and CPU) on Windows\r\n\r\nNote on 22 Oct 2018: it works fine **without** the temporary workaround on today's nightly; the problem seems resolved (maybe) by fix for #22750.", "Closing this issue as it is resolved. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Today's nightly works fine for me **without a workaround** by @wenkesj.\r\nThe issue seems duplicate of #22750.\r\nThanks to all of you.", "this issue also exists in tf-serving 1.12.0"]}, {"number": 22580, "title": "Modify: fix name problem when export and import graph def of boosted_\u2026", "body": "**tensorflow/python/framework/importer.py**\r\n\r\n```python\r\n  with ops.name_scope(name, 'import', input_map.values()) as scope:\r\n    # Save unique prefix generated by name_scope\r\n    if scope:\r\n      assert scope.endswith('/')\r\n      prefix = scope[:-1]\r\n    else:\r\n      prefix = ''\r\n```\r\n\r\nWhen a variable or saveable objects endswith '/', it will fix it. This will cause a restoring error. Variable or tensor not found.\r\n\r\n> \"The name 'HzfHomepageWnd/StatsAccumulatorfeature0/' refers to an Operation not in the graph.\"", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "The error message is:\r\n\r\n> \"The name 'HzfHomepageWnd/StatsAccumulatorfeature0/' refers to an Operation not in the graph.\"\r\n\r\n`'HzfHomepageWnd/StatsAccumulatorfeature0/'` is generated by accumulator.\r\n\r\nThe error happens when you use `SavedModelBuilder` to save the trained model and then restore it using `tf.saved_model.loader`.\r\n\r\n", "Can you add a test demonstrating the problem and showing that it is fixed?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 22579, "title": "save tensorflow  model to .pb file ", "body": "Hi I wonder , how do I save model to .pb file to retrain model  ? Below ,there is sample model for experiment. Thanks for helping\r\n\r\n#######################################################################\r\nx=tf.placeholder(tf.float32,shape=[None,2],name=\"x\")\r\ny=tf.placeholder(tf.float32,shape=[None,1],name=\"y\")\r\n\r\nwith tf.name_scope(\"dnn\"):\r\n      layer1=tf.layers.dense(x,15,activation=tf.nn.relu,\r\n                           kernel_initializer= tf.initializers.truncated_normal(),name=\"layer1\")\r\n    layer2=tf.layers.dense(layer1,20,activation=tf.nn.relu,\r\n                           kernel_initializer= tf.initializers.truncated_normal(),name=\"layer2\")\r\n    logits=tf.layers.dense(layer2,1,\r\n                           kernel_initializer= tf.initializers.truncated_normal(),name=\"logits\")\r\n    out=tf.nn.sigmoid(logits,name=\"out\")\r\n    \r\nwith tf.name_scope(\"train\"):\r\n    loss=tf.losses.sigmoid_cross_entropy(multi_class_labels=y,logits=logits)\r\n    train_op=tf.train.AdamOptimizer().minimize(loss)\r\n\r\nwith tf.name_scope(\"eval\"):\r\n    correct=tf.equal(tf.round(out),y)\r\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\r\n\r\nsaver=tf.train.Saver()\r\n\r\nepochs=2000\r\nwith tf.Session( ) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    for epoch in range(epochs+1):\r\n        sess.run(train_op,feed_dict={x:trainx,y:trainy})\r\n        if epoch%1000==0:\r\n            l,a=sess.run([loss,accuracy],feed_dict={x:trainx,y:trainy})\r\n            print(\"Epoch {} | Accuracy : {:.2f} | Loss :{:.2f}\".format(epoch,a,l))\r\n            saver.save(sess,\"./moon_model\")\r\n    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'moon_model.pbtxt', as_text=True)\r\n\r\n#########################################################################3\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device"]}, {"number": 22578, "title": "Tensorflow top 1 prediction drop on the /example/label_image/label_image.py  with mobile net . ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NO\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.17.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI compared official code for mobilenet v1 on both python and C++ and find some performance drop.  \r\nmodel: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet\r\nI use mobilenet_v1_224\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\nThe model and input data are **EXACTLY SAME** \r\n\r\nI doubt the problem is the mean and std in label_image.py but I couldn't make the prediction as same as the above inference. \r\n\r\nThe label_image.py comes from tensorflow/example/label_image/label_image.py. Just change inception to mobilenet. \r\n\r\n```\r\nimport tensorflow as tf\r\ntf_model_path = './frozen_graph.pb'\r\nwith open(tf_model_path, 'rb') as f:\r\n    serialized = f.read()\r\ntf.reset_default_graph()\r\noriginal_gdef = tf.GraphDef()\r\noriginal_gdef.ParseFromString(serialized)\r\n\r\nimport numpy as np\r\nimport PIL\r\nimport requests\r\nfrom io import BytesIO\r\nfrom matplotlib.pyplot import imshow\r\nimg_url = 'https://upload.wikimedia.org/wikipedia/commons/9/93/Golden_Retriever_Carlos_%2810581910556%29.jpg'\r\nresponse = requests.get(img_url)\r\n%matplotlib inline\r\nimg = PIL.Image.open(BytesIO(response.content))\r\nimshow(np.asarray(img))\r\nimg = img.resize([224,224], PIL.Image.ANTIALIAS)\r\n\r\nimg_np = np.array(img).astype(np.float32)\r\nprint( 'image shape:', img_np.shape)\r\nprint( 'first few values: ', img_np.flatten()[0:4], 'max value: ', np.amax(img_np))\r\nimg_tf = np.expand_dims(img_np, axis = 0) #now shape is [1,224,224,3] as required by TF\r\nimg_tf = (2.0/255.0) * img_tf - 1\r\n\r\n\r\ntf_input_name = 'input:0'\r\ntf_output_name = 'MobilenetV1/Predictions/Reshape_1:0'\r\nwith tf.Session(graph = g) as sess:\r\n    tf_out = sess.run(tf_output_name, \r\n                      feed_dict={tf_input_name: img_tf})\r\ntf_out = tf_out.flatten()    \r\nidx = np.argmax(tf_out)\r\nlabel_file = 'labels.txt' \r\nwith open(label_file) as f:\r\n    labels = f.readlines()\r\n    \r\nprint('\\n')\r\nprint(\"TF prediction class = {}, probability = {}\".format(labels[idx],\r\n                                            str(tf_out[idx])))\r\n```\r\n\r\nit shows: \r\nimage shape: (224, 224, 3)\r\nfirst few values:  [39. 33. 18. 42.] max value:  255.0\r\nTF prediction class = 208:golden retriever\r\n, probability = 0.9581951\r\n\r\n#while for when  I use tensorflow/example/label_image/label_image.py the result is different:\r\n\r\n```\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef load_graph(model_file):\r\n  graph = tf.Graph()\r\n  graph_def = tf.GraphDef()\r\n\r\n  with open(model_file, \"rb\") as f:\r\n    graph_def.ParseFromString(f.read())\r\n  with graph.as_default():\r\n    tf.import_graph_def(graph_def)\r\n\r\n  return graph\r\n\r\n\r\ndef read_tensor_from_image_file(file_name,\r\n                                input_height=224,\r\n                                input_width=224,\r\n                                input_mean=0,\r\n                                input_std=255):\r\n  input_name = \"file_reader\"\r\n  output_name = \"normalized\"\r\n  file_reader = tf.read_file(file_name, input_name)\r\n  if file_name.endswith(\".png\"):\r\n    image_reader = tf.image.decode_png(\r\n        file_reader, channels=3, name=\"png_reader\")\r\n  elif file_name.endswith(\".gif\"):\r\n    image_reader = tf.squeeze(\r\n        tf.image.decode_gif(file_reader, name=\"gif_reader\"))\r\n  elif file_name.endswith(\".bmp\"):\r\n    image_reader = tf.image.decode_bmp(file_reader, name=\"bmp_reader\")\r\n  else:\r\n    image_reader = tf.image.decode_jpeg(\r\n        file_reader, channels=3, name=\"jpeg_reader\")\r\n  float_caster = tf.cast(image_reader, tf.float32)\r\n  dims_expander = tf.expand_dims(float_caster, 0)\r\n  resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\r\n  normalized = tf.subtract(tf.divide(resized,[input_std]),[input_mean])\r\n  sess = tf.Session()\r\n  result = sess.run(normalized)\r\n\r\n  return result\r\n\r\n\r\ndef load_labels(label_file):\r\n  label = []\r\n  proto_as_ascii_lines = tf.gfile.GFile(label_file).readlines()\r\n  for l in proto_as_ascii_lines:\r\n    label.append(l.rstrip())\r\n  return label\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  file_name = \"./data/test.jpg\"\r\n  model_file = \\\r\n    \"./data/frozen_graph.pb\"\r\n  label_file = \"./data/labels.txt\"\r\n  input_height = 224\r\n  input_width = 224\r\n  input_mean = 1\r\n  input_std = 255/2\r\n  input_layer = \"input\"\r\n  output_layer = \"MobilenetV1/Predictions/Reshape_1\"\r\n\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument(\"--image\", help=\"image to be processed\")\r\n  parser.add_argument(\"--graph\", help=\"graph/model to be executed\")\r\n  parser.add_argument(\"--labels\", help=\"name of file containing labels\")\r\n  parser.add_argument(\"--input_height\", type=int, help=\"input height\")\r\n  parser.add_argument(\"--input_width\", type=int, help=\"input width\")\r\n  parser.add_argument(\"--input_mean\", type=int, help=\"input mean\")\r\n  parser.add_argument(\"--input_std\", type=int, help=\"input std\")\r\n  parser.add_argument(\"--input_layer\", help=\"name of input layer\")\r\n  parser.add_argument(\"--output_layer\", help=\"name of output layer\")\r\n  args = parser.parse_args()\r\n\r\n  if args.graph:\r\n    model_file = args.graph\r\n  if args.image:\r\n    file_name = args.image\r\n  if args.labels:\r\n    label_file = args.labels\r\n  if args.input_height:\r\n    input_height = args.input_height\r\n  if args.input_width:\r\n    input_width = args.input_width\r\n  if args.input_mean:\r\n    input_mean = args.input_mean\r\n  if args.input_std:\r\n    input_std = args.input_std\r\n  if args.input_layer:\r\n    input_layer = args.input_layer\r\n  if args.output_layer:\r\n    output_layer = args.output_layer\r\n\r\n  graph = load_graph(model_file)\r\n  t = read_tensor_from_image_file(\r\n      file_name,\r\n      input_height=input_height,\r\n      input_width=input_width,\r\n      input_mean=input_mean,\r\n      input_std=input_std)\r\n\r\n  input_name = \"import/\" + input_layer\r\n  output_name = \"import/\" + output_layer\r\n  input_operation = graph.get_operation_by_name(input_name)\r\n  output_operation = graph.get_operation_by_name(output_name)\r\n\r\n  with tf.Session(graph=graph) as sess:\r\n    results = sess.run(output_operation.outputs[0], {\r\n        input_operation.outputs[0]: t\r\n    })\r\n  results = np.squeeze(results)\r\n\r\n  top_k = results.argsort()[-5:][::-1]\r\n  labels = load_labels(label_file)\r\n  for i in top_k:\r\n    print(labels[i], results[i])\r\n```\r\n`208:golden retriever 0.83966106\r\n209:Labrador retriever 0.033579364\r\n213:English setter 0.024712995\r\n274:dingo, warrigal, warragal, Canis dingo 0.01596725\r\n217:clumber, clumber spaniel 0.010608253\r\n\r\nif I change std =255 mean=0 it shows 208:golden retriever 0.7953789\r\n213:English setter 0.03927898\r\n217:clumber, clumber spaniel 0.028547956\r\n209:Labrador retriever 0.025497263\r\n221:Sussex spaniel 0.013308002", "comments": ["@ywang370 A small difference in label prediction is expected but looking at your code snippets, at least part of the discrepancies originate from the downsampling filter you applied in the customized code: `PIL.Image.ANTIALIAS`.  ", "Closing this for now, feel free to reopen if any additional questions."]}, {"number": 22577, "title": "[Bug] TensorRT conversion error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:No\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: 1.11.0 dev build from dockerhub, last commit c19e29306ce1777456b2dbb3a14f511edf7883a8\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: GTX 1080 with 8G memory\r\n- **Exact command to reproduce**: python minimal_graph.py \r\n\r\n### Describe the problem\r\nMet with another conversion error when using latest tensorRT conversion pipeline. After localization of the problem, it seems that the problem is caused by NHWC -> NCHW conversion: If we have a constant of shape (C) (usually bias add term) added after the convolution, after the NHWC -> NCHW conversion, we should expect it to be of shape (C, 1, 1) in order to be broadcast-able, yet current pipeline failed to do so.\r\nMy guess may be incorrect, hopefully this would be helpful though.\r\n\r\n@aaroey \r\n\r\n### Source code / logs\r\nTest case:\r\n```\r\n import numpy as np\r\n import tensorflow as tf\r\n from tensorflow.contrib import tensorrt as trt\r\n \r\n def build_graph_from_def(graph_def, input_nodes, output_nodes):\r\n     \"\"\"\r\n     build the actual graph from definition\r\n     \"\"\"\r\n     tf.reset_default_graph()\r\n     graph = tf.Graph()\r\n     with graph.as_default():\r\n         return_tensors = [operation_name + \":0\" for operation_name in input_nodes + output_nodes]\r\n         tensors = tf.import_graph_def(graph_def=graph_def, name=\"\",\r\n                                       return_elements=return_tensors)\r\n         input_tensor_list = tensors[:len(input_nodes)]\r\n         output_tensor_list = tensors[len(input_nodes):]\r\n \r\n     return graph, input_tensor_list, output_tensor_list\r\n \r\n \r\n def main():\r\n \r\n     with tf.variable_scope(\"Net\"):\r\n         inp = tf.placeholder(tf.float32, shape=(1, 32, 32, 3), name=\"input_image\")\r\n         weight = tf.zeros([3,3,3,8] ,tf.float32)\r\n         conv = tf.nn.conv2d(inp, weight, strides=[1,1,1,1], padding=\"SAME\")\r\n         bn = conv + tf.ones((8,), tf.float32, name=\"add\")\r\n         bn = tf.nn.relu(bn, name=\"output\")\r\n \r\n     input_nodes = [\"Net/input_image\"]\r\n     output_nodes = [\"Net/output\"]\r\n \r\n     with tf.Session() as sess:\r\n         sess.run(tf.global_variables_initializer())\r\n         const_graph_def = tf.graph_util.convert_variables_to_constants(\r\n             sess, sess.graph.as_graph_def(), output_nodes)\r\n \r\n     optimized_graph_def = trt.create_inference_graph(\r\n         input_graph_def=const_graph_def,\r\n         outputs=output_nodes,\r\n         max_batch_size=1,\r\n         max_workspace_size_bytes=1 << 25)\r\n     graph, input_tensors, output_tensors = build_graph_from_def(\r\n         optimized_graph_def, input_nodes, output_nodes)\r\n \r\n     with tf.Session(graph=graph) as sess:\r\n         output_value = sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 32,    32, 3))})\r\n     print(\"output:{}\".format(output_value.shape))\r\n \r\n if __name__ == \"__main__\":\r\n     main()\r\n```\r\nSample Output:\r\n```\r\n$ python minimal_graph2.py\r\n2018-09-27 17:44:41.357261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-27 17:44:41.357805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 5.67GiB\r\n2018-09-27 17:44:41.357817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-27 17:44:41.632033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-27 17:44:41.632053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-27 17:44:41.632058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-27 17:44:41.632205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5419 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-27 17:44:41.689349: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-09-27 17:44:41.689390: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-09-27 17:44:41.689539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0\r\n2018-09-27 17:44:41.689552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-27 17:44:41.689557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 \r\n2018-09-27 17:44:41.689562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N \r\n2018-09-27 17:44:41.689643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5419 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-27 17:44:41.693257: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2952] Segment @scope 'Net/', converted to graph\r\n2018-09-27 17:44:41.693268: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\npython: customWinogradConvActLayer.cpp:48: nvinfer1::cudnn::WinogradConvActLayer::WinogradConvActLayer(const string&, const EngineTensors&, const EngineTensors&, const nvinfer1::ConvolutionParameters&, bool, const std::vector<float>&): Assertion `matchNbDims(inputs[0], outputs[0]) && (inputs.size() == 1 || inputs[1].extent == outputs[0].extent)' failed.\r\n[1]    28090 abort (core dumped)  python minimal_graph2.py\r\n\r\n```\r\n", "comments": ["@pooyadavoodi", "I've been running into the same issue with an almost similar scenario and exactly same environment. Can we please get some activity going  on for this issue? ", "@mike199515 I ran the above code snippet without running into any issues. Try to install TensorFlow from binary. Also, please post your `env.txt` here after running `tf_env_collect.sh`.", "Closing this issue, feel free to reopen if any error comes up.", "Hello, I tried running on latest 1.11.0 tensorflow-gpu installed from pip, and I still got the same output as described above.\r\nMy env.txt looks like the following:\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux yiwei-desktop 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux yiwei-desktop 4.15.0-36-generic #39~16.04.1-Ubuntu SMP Tue Sep 25 08:59:23 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy                              1.15.2                \r\nprotobuf                           3.6.0                 \r\ntensorflow-gpu                     1.11.0                \r\ntensorflow-tensorboard             0.1.8                 \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.11.0\r\ntf.GIT_VERSION = v1.11.0-0-gc19e29306c\r\ntf.COMPILER_VERSION = v1.11.0-0-gc19e29306c\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nThu Nov  1 18:42:17 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\r\n| 29%   46C    P2    42W / 180W |   1391MiB /  8112MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      2190      G   /usr/lib/xorg/Xorg                           582MiB |\r\n|    0      5166      G   compiz                                       493MiB |\r\n|    0      6354      G   ...-token=6EB0A193120E130143DC89992F94A977   196MiB |\r\n|    0      7163      C   /usr/lib/libreoffice/program/soffice.bin     107MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.252\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n```\r\n\r\nHope it helps.", "@wt-huang I switched my model to NCHW format to avoid this problem, yet it would still be helpful if we can fix this? I think I don't have the permission to re-open the issue yet..", "@mike199515 This should work with NCHW format, which is more the norm. We will note down to add NHWC -> NCHW conversion to make it backward compatible. ", "To be clear, it's not an issue with the NHWC -> NCHW conversion, rather this is a problem with the NHWC format which is the default in tensorflow, so I would expect this to affect the majority of use cases", "I faced the same issue, do we have some updates?", "I encounter with the same problem, is there any official resolution ?", "sorry for the delayed response for this issue.\r\n\r\nI was able to reproduce this issue with TF 1.12, TensorRT 4 and CUDA 9.0 using the minimal example provided in https://github.com/tensorflow/tensorflow/issues/23467#issue-376975195. But, the good news is that the issue seems to be fixed with nightly TensorFlow (and soon to be released TF 1.13), TensorRT 5 and CUDA 10.0.\r\n\r\nPlease let us know know if that does not fix the issue for you. Otherwise, will close this issue after waiting for a week.\r\n\r\nAlso, adding @pooyadavoodi @trevor-m to shed light on what actually fixed the issue if that is already known.", "Good news, thanks very much.  : )"]}, {"number": 22576, "title": "Confusing information in the tf.data Op kernel files", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.2.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nA bunch of files for tf.data Ops under the directory `tensorflow/core/kernels/data/` has the following doc:\r\n\r\n```\r\n// See documentation in ../ops/dataset_ops.cc for a high-level\r\n// description of the following op.\r\n```\r\nHowever, I could not find this file at the referred location (`../ops/dataset_ops.cc`). Does it mean the file `tensorflow/core/ops/dataset_ops.cc`? If yes, I would like to submit a PR to update them.\r\n", "comments": ["Yes, we moved the kernel files down one directory, and forgot to update those comments. A contribution would be welcome, thanks!"]}, {"number": 22575, "title": "Preferred way of installing TensorRT with Tensorflow 1.11 on Ubuntu?", "body": "Back when TensorRT support was first announced back in version 1.7, for TensorRT to work it was necessary that you install TensorRT 3.0.4 for Ubuntu 14.04, regardless of your version. For 1.11, which version of TensorRT should be used when installing the tensorflow-gpu pip package?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "According to [Deep Learning SDK documentation](https://docs.nvidia.com/deeplearning/sdk/tensorrt-release-notes/rel_5-0-RC.html#rel_5-0-RC) TensorRT 5.0 RC has been tested with TensorFlow 1.9.\r\nHowever you can install TensorFlow 1.11 yourself with TensorRT 5.0.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I know I can build from source with the tensorrt 5, to clarify I\u2019m wondering what version the tensorflow-gpu pip package was built against"]}, {"number": 22574, "title": "1.11.0 is missing the package for Python 3.6 on macOS", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra (10.13.6)\r\n- **TensorFlow installed from (source or binary)**: binary (attempted)\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.6.4 (Anaconda)\r\n- **Exact command to reproduce**: `pip install tensorflow==1.11.0`\r\n\r\n### Describe the problem\r\n\r\n1.11.0 is missing the package for Python 3.6 on macOS. See https://pypi.org/project/tensorflow/1.11.0/#files -- it has many combinations of {Windows, Linux, macOS} and Python {2.7, 3.3, 3.4, 3.5, 3.6}, but it's missing macOS + Python 3.6. (It's also missing Windows + older Python versions, but that's been that way for a while.)\r\n\r\n### Source code / log\r\nHere's what https://pypi.org/project/tensorflow/1.11.0/#files currently lists:\r\n\r\n\r\nFilename, size & hash\u00a0SHA256 hash help | File type | Python version | Upload date\r\n-- | -- | -- | --\r\ntensorflow-1.11.0-cp27-cp27m-macosx_10_11_x86_64.whl\u00a0(59.4 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp27 | Sep 27, 2018\r\ntensorflow-1.11.0-cp27-cp27mu-manylinux1_x86_64.whl\u00a0(63.0 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp27 | Sep 27, 2018\r\ntensorflow-1.11.0-cp33-cp33m-macosx_10_11_x86_64.whl\u00a0(59.9 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp33 | Sep 27, 2018\r\ntensorflow-1.11.0-cp33-cp33m-manylinux1_x86_64.whl\u00a0(63.5 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp33 | Sep 27, 2018\r\ntensorflow-1.11.0-cp34-cp34m-macosx_10_11_x86_64.whl\u00a0(59.9 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp34 | Sep 27, 2018\r\ntensorflow-1.11.0-cp34-cp34m-manylinux1_x86_64.whl\u00a0(63.5 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp34 | Sep 27, 2018\r\ntensorflow-1.11.0-cp35-cp35m-macosx_10_11_x86_64.whl\u00a0(59.3 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp35 | Sep 27, 2018\r\ntensorflow-1.11.0-cp35-cp35m-manylinux1_x86_64.whl\u00a0(63.0 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp35 | Sep 27, 2018\r\ntensorflow-1.11.0-cp35-cp35m-win_amd64.whl\u00a0(46.9 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp35 | Sep 27, 2018\r\ntensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl\u00a0(63.0 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp36 | Sep 27, 2018\r\ntensorflow-1.11.0-cp36-cp36m-win_amd64.whl\u00a0(46.9 MB)\u00a0\u00a0Copy SHA256 hashSHA256 | Wheel | cp36 | Sep 27, 2018\r\n\r\n", "comments": ["Taking a look now.", "@angersson any ideas?", "This should be fixed now. Thanks @daniel-ziegler for bringing this to our attention. \r\n\r\n[Link](https://files.pythonhosted.org/packages/70/78/cd74769027b6249e45807637c1aa3ef212b9492349cca4b87e5de1a10548/tensorflow-1.11.0-cp36-cp36m-macosx_10_11_x86_64.whl)", "Works, thanks!\r\n"]}, {"number": 22573, "title": "TensorFlow 1.11.0 requires scipy", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: `pip install tensorflow==1.11.0`\r\n\r\n### Describe the problem\r\n\r\nTensorFlow 1.11.0 requires `keras-preprocessing>=1.0.3`, which in turn requires `scipy>=0.14`.  So now scipy is a dependency of TensorFlow.  This seems like a rather large change, and I didn't see anything about it in the release notes, so I'm wondering whether it was intentional or not.\r\n\r\nNote that this same change makes all of Keras a dependency of TensorFlow now, as noted in #22426\r\n", "comments": ["For a concrete example of the impact of these new dependencies, the size of a clean tensorflow installation went from 375MB (1.9.0), to 379MB (1.10.0), to 516MB (1.11.0).", "The updated install docs at: https://www.tensorflow.org/install/source\r\nnow state the following as dependencies, but to use the --no-deps flags:\r\npip install -U --user keras_applications==1.0.5 --no-deps\r\npip install -U --user keras_preprocessing==1.0.3 --no-deps", "I think we have to consider that the vast majority of users will simply be doing `pip install tensorflow`, so this change makes scipy an install requirement (even if there is technically a way for users to manually work around the standard packaging system behaviour).\r\n\r\nAlso consider all the downstream packages that have `tensorflow` as a requirement.  They would all now need to add \r\n```\r\npip install -U --user keras_applications==1.0.5 --no-deps\r\npip install -U --user keras_preprocessing==1.0.3 --no-deps\r\n```\r\nto their installation instructions (which would be pretty confusing to users, since those aren't even dependencies of that downstream package) if they want to avoid adding a scipy dependency to their own installation process.", "Oops, you're right. I inadvertently conflated installing/building from source with simply installing the pre-built package.", "It seems to create a little problems because tensorflow depends on keras, see #22601, #22486.  cc @fchollet \r\n\r\nAs `tf.keras` has been in tensorflow core, I'm curious about why keras library is still necessary for tf.", "I think this is resolved for 1.12 and future releases. This dependency was happening through keras_applications.\r\n\r\nThanks for reporting. Unfortunately, for 1.11 we will leave this bug in because we have already started working on 1.12."]}, {"number": 22572, "title": "[nGraph] Fixed the broken unit test build", "body": "Due to some recent changes in the bazel files for MKL, the nGraph unit test build was broken. This pull request fixes that and upgrdes the nGraph library references to latest (which contains some bug fixes). ", "comments": []}, {"number": 22571, "title": "[INTEL MKL] Removing dead code in slice. ", "body": "With the addition of mkl slice with this commit https://github.com/tensorflow/tensorflow/commit/08a6cfed1cf0cccc8ff35448266f44fbc55be0bc  the openMP  based code will not longer be executed", "comments": []}, {"number": 22570, "title": "Keras eager execution: Accessing the model DeferredTensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\n- **TensorFlow installed from (source or binary)**: Colab\r\n- **TensorFlow version (use command below)**:  1.11.0-rc2\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Colab\r\n- **GCC/Compiler version (if compiling from source)**: Colab\r\n- **CUDA/cuDNN version**: Colab\r\n- **GPU model and memory**: Colab\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a custom loss function which requires some specific tensors to be computed based on other tensors generated from the model itself.  Unfortunately, this can not be done in eager mode.\r\n\r\nPlease find below a toy example that illustrates the issue. The code runs without problems in the graph mode.\r\nI really need a workaround. Could you please help.\r\n\r\nThanks.\r\n\r\n### Source code \r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense, Input, Layer\r\nfrom tensorflow.keras.models import Model\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef custom_loss_wrapper(input_tensor):\r\n    def custom_loss(y_true, y_pred):\r\n        return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\r\n    return custom_loss\r\n  \r\ninput_tensor = Input(shape=(10,))\r\nhidden = Dense(100, activation='relu')(input_tensor)\r\nout = Dense(1, activation='sigmoid')(hidden)\r\nmodel = Model(input_tensor, out)\r\nmodel.compile(loss=custom_loss_wrapper(input_tensor), optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\n\r\nnp.random.seed(0)\r\nX = np.random.random((3, 10)).astype(np.float32)\r\nY1 = np.random.random((3, 1)).astype(np.float32)\r\nmodel.fit(x=X, y=Y1, batch_size=1, epochs=10)\r\n```\r\n\r\n### logs\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-9dbf3db39e42> in <module>()\r\n     22 X = np.random.random((3, 10)).astype(np.float32)\r\n     23 Y1 = np.random.random((3, 1)).astype(np.float32)\r\n---> 24 model.fit(x=X, y=Y1, batch_size=1, epochs=10)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1577           initial_epoch=initial_epoch,\r\n   1578           steps_per_epoch=steps_per_epoch,\r\n-> 1579           validation_steps=validation_steps)\r\n   1580     elif self._distribution_strategy:\r\n   1581       return training_distributed.fit_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)\r\n    696           validation_steps=validation_steps,\r\n    697           do_validation=do_validation,\r\n--> 698           batch_size=batch_size)\r\n    699       callbacks.on_epoch_end(epoch, epoch_logs)\r\n    700       if callbacks.model.stop_training:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in iterator_fit_loop(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)\r\n    248     # Train model.\r\n    249     outs, loss, loss_metrics, masks = _process_single_batch(\r\n--> 250         model, x, y, sample_weights=sample_weights, training=True)\r\n    251     outs = generic_utils.to_list(outs)\r\n    252 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)\r\n    502           targets,\r\n    503           sample_weights=sample_weights,\r\n--> 504           training=training)\r\n    505       if loss is None:\r\n    506         raise ValueError('The model cannot be run '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)\r\n    110       with backend.name_scope(model.output_names[i] + '_loss'):\r\n    111         output_loss = weighted_masked_fn(\r\n--> 112             targets[i], outs[i], weights, mask=mask)\r\n    113       # If the number of outputs is 1 then we don't append the loss metric\r\n    114       # associated with each model output. When there are multiple outputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in weighted(y_true, y_pred, weights, mask)\r\n    569     \"\"\"\r\n    570     # score_array has ndim >= 2\r\n--> 571     score_array = fn(y_true, y_pred)\r\n    572     if mask is not None:\r\n    573       mask = math_ops.cast(mask, y_pred.dtype)\r\n\r\n<ipython-input-3-9dbf3db39e42> in custom_loss(y_true, y_pred)\r\n     10 def custom_loss_wrapper(input_tensor):\r\n     11     def custom_loss(y_true, y_pred):\r\n---> 12         return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\r\n     13     return custom_loss\r\n     14 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in mean(x, axis, keepdims)\r\n   1727   if x.dtype.base_dtype == dtypes_module.bool:\r\n   1728     x = math_ops.cast(x, floatx())\r\n-> 1729   return math_ops.reduce_mean(x, axis, keepdims)\r\n   1730 \r\n   1731 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    486                 'in a future version' if date is None else ('after %s' % date),\r\n    487                 instructions)\r\n--> 488       return func(*args, **kwargs)\r\n    489     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    490                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_mean(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\r\n   1488                                    input_tensor,\r\n   1489                                    _ReductionDims(input_tensor, axis,\r\n-> 1490                                                   reduction_indices),\r\n   1491                                    keepdims,\r\n   1492                                    name=name))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _ReductionDims(x, axis, reduction_indices)\r\n   1270 \r\n   1271     # Otherwise, we rely on Range and Rank to do the right thing at run-time.\r\n-> 1272     return range(0, array_ops.rank(x))\r\n   1273 \r\n   1274 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank(input, name)\r\n    366   @end_compatibility\r\n    367   \"\"\"\r\n--> 368   return rank_internal(input, name, optimize=True)\r\n    369 \r\n    370 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank_internal(input, name, optimize)\r\n    386       return gen_array_ops.size(input.dense_shape, name=name)\r\n    387     else:\r\n--> 388       input_tensor = ops.convert_to_tensor(input)\r\n    389       input_shape = input_tensor.get_shape()\r\n    390       if optimize and input_shape.ndims is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n   1046       name=name,\r\n   1047       preferred_dtype=preferred_dtype,\r\n-> 1048       as_ref=False)\r\n   1049 \r\n   1050 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1142 \r\n   1143     if ret is None:\r\n-> 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1145 \r\n   1146     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    226                                          as_ref=False):\r\n    227   _ = as_ref\r\n--> 228   return constant(v, dtype=dtype, name=name)\r\n    229 \r\n    230 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    176   ctx = context.context()\r\n    177   if ctx.executing_eagerly():\r\n--> 178     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    179     if shape is None:\r\n    180       return t\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    111     return t\r\n    112   else:\r\n--> 113     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\r\n    114 \r\n    115 \r\n\r\nValueError: Attempt to convert a value (<DeferredTensor 'input_2' shape=(?, 10) dtype=float32>) with an unsupported type (<class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>) to a Tensor.", "comments": ["@fchollet I think something in Keras is using DeferredTensor incorrectly. Can you take a look?", "@fchollet @alextp  any update?", "Hi, I encountered the similar problem (see below) but in different context.:\r\n\r\n```python\r\nValueError: Attempt to convert a value (<DeferredTensor 'None' shape=(?, 64, 64, 128) dtype=float32>) with an unsupported type (<class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>) to a Tensor.\r\n```\r\n\r\nI did some quick checking code to know if there's a need to upgrade tensorflow or relevant code. I found that the problem is the incorrect dtype assignment.\r\n\r\nIn tensorflow/python/eager/execute.py(180)args_to_matching_eager funciton\r\n\r\n```python\r\n    177   # Is some input already a Tensor with a dtype?\r\n    178   dtype = None\r\n    179   for t in l:\r\n    180     if isinstance(t, EagerTensor):\r\n    181       dtype = t.dtype\r\n    182       break\r\n    183 \r\n```\r\nBecause the instance of **`DeferredTensor`** cannot pass the isinstance testing, the dtype remaining None. As a result, the conversion cannot be done as dtype is None.    \r\nHave checked out **`DeferredTensor`** class which inherit from *object* only and fails the isinstance testing as a consequence.      \r\n\r\nAll of the above is observed on the release build and  **TensorFlow version**: 1.9.0   \r\nNot sure if this information is helpful.  But keep me in the loop and inform me if I could provide any help. Thanks!", "DeferredTensor is no longer used in tf.keras so closing this as obsolete. Please reopen if you can reproduce on nightly."]}, {"number": 22569, "title": "How debug model during inference in mlEngine", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I want to run the inference using tensorflow serving on ml engine. But the model fails without giving any stack trace. \r\n\r\nLike to know if there is any debug level logging I can enable during inference", "@Giribushan Please refer to this [documentation](https://cloud.google.com/ml-engine/docs/tensorflow/troubleshooting) for logs.", "Thanks @ymodak \r\n\r\nI have referred to this document already. Current I am doing online prediction. As per the documentation..\r\n\r\nOnline prediction logs\r\nYour online prediction requests don't generate logs by default. You can enable Stackdriver Logging when you create your model resource:\r\n\r\nSo I have used the tag \"--enable-logging\" when creating the model, and I am able to do the prediction using \"gcloud local\". But when I exported the model to ml-engine, and doing online prediction, I am getting some variable uninitialization error as below.\r\n```\r\n{\r\n  \"error\": \"Prediction failed: Error during model execution: AbortionError(code=StatusCode.FAILED_PRECONDITION, details=\\\"Attempting to use uninitialized value shadow/LSTMLayers/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias\\n\\t [[Node: shadow/LSTMLayers/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias/read = Identity[T=DT_FLOAT, _output_shapes=[[2048]], _device=\\\"/job:localhost/replica:0/task:0/device:CPU:0\\\"](shadow/LSTMLayers/stack_bidirectional_rnn/cell_0/bidirectional_rnn/fw/basic_lstm_cell/bias)]]\\\")\"\r\n}\r\n```\r\n\r\nThis error is quite unclear, and also there is no stack trace provided in logs.\r\nIf anyone had success with gcloud ml-engine prediction logs, please help.\r\n", "What version of TensorFlow are you using?", "I am using Tensorflow 1.8 both in gcloud and in my local machine. Do you have a working example for gcloud ml engine..", "Please take a look at [Google Cloud Platform repo](https://github.com/GoogleCloudPlatform/cloudml-samples) for gcloud ml engine example.\r\nAlso can you please open your issue in [GoogleCloudPlatform/ml-on-gcp repo](https://github.com/GoogleCloudPlatform/ml-on-gcp) since this issue is related to Machine Learning on Google Cloud Platform."]}, {"number": 22568, "title": "Tensorflow CPU computation of NCHW data_format.", "body": "I have trained my model on GPU\r\nBut I wanna launch it at my server, which has only CPU\r\nWhat can i do?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "Can we implement some operations which use NCHW on CPU ? "]}, {"number": 22567, "title": "Is upper constraint for setuptools still required?", "body": "Currently ``tensorflow`` has an upper limit for ``setuptools`` in install requirements, as I understand it's related to https://github.com/tensorflow/tensorflow/pull/19820 and https://github.com/tensorflow/tensorflow/pull/19818. However recent setuptools releases have certain number of bug fixes which could solve that issues. May you please check if constraint for setuptools to be <= 39.1.0 is still required? Also removing this constraint may help with https://github.com/tensorflow/tensorflow/issues/22082 because at some of ``40.*`` releases setuptools include the fix for namespaces packages related to that problem.\r\n\r\nI ask because I'd like to use newer setuptools and this constraint prevents me from doing this. Also I checked installation of tensorflow with latest setuptools and imports work.", "comments": ["@daa thanks for opening this up. I'll look into this.", "Was this by any chance fixed by https://github.com/tensorflow/tensorflow/commit/4ecce5aa64587afe1cd07ee4c92bbb5ce2cf85df already? I was trying to figure out if there was a pull request for it but it seems that tensorflow has its own workflow different from everybody else's.", "Yes, I believe that will resolve this. I'll close this issue once 1.12 rc0 builds.", "Thank you.", "Nagging Assignee @av8ramit: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This has been resolved."]}, {"number": 22566, "title": "Added chief to the default device_filters for /job:ps", "body": "This behaviour matches the description in the _get_default_session_config_distributed docstring, and restores the symmetry of the default device_filters.", "comments": ["Nagging Assignee @caisq: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@superbobry please fix:\r\n```\r\n\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/python/estimator/run_config_test.py:1200: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\ntensorflow/python/estimator/run_config_test.py:1201: [C0330(bad-continuation), ] Wrong hanging indentation (add 2 spaces).\r\n```"]}, {"number": 22565, "title": "Tensorflow example, Object Detection: Failed to find input Node 'image_tensor", "body": "I try to run tensorflow object Detection example on android with my own '.pb' file.\r\nI change `TF_OD_API_MODEL_FILE` and `TF_OD_API_LABELS_FILE` to my own files.\r\nWhen I run it I've got the error:\r\n\r\n```\r\njava.lang.RuntimeException: Failed to find input Node 'image_tensor'\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:102)\r\n```\r\nBut by graph HAS input node name 'image_tensor'!! To check it, I open it with tensorboard and check it.\r\n\r\nI don't know what versions and libraries should i write here, maybe someone has same issue? \r\n \r\nPS - maybe some error got because I have only one class (and label) to detect? Or it doesn't mean?\r\n\r\n_________\r\n\r\nEDIT: answer on questions from @tensorflowbutler \r\n\r\n**Have I written custom code**\r\nI'm using only code example from [`official page`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android)\r\nI only change name's of files to my own pb - `TF_OD_API_MODEL_FILE` and `TF_OD_API_LABELS_FILE`.\r\n\r\n**OS Platform and Distribution**\r\nMAC OS High Sierra 10.13.6\r\n\r\n**TensorFlow installed from**\r\nTensorflow installed from pip\r\n\r\n**TensorFlow version**\r\n1.10.1\r\n\r\n**Bazel version**\r\nBuild label: 0.16.1\r\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Aug 13 13:42:50 2018 (1534167770)\r\nBuild timestamp: 1534167770\r\nBuild timestamp as int: 1534167770\r\n\r\n**CUDA/cuDNN version**\r\nno\r\n\r\n**GPU model and memory**\r\nno\r\n\r\n**Exact command to reproduce**\r\nRun app in android studio, start application 'TF Detect', crash on starting.\r\n\r\n**Mobile device**\r\ngoogle Nexus 5, api23", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@tensorflowbutler I've edited my question and add this info to it, thank you\r\n\r\nps - also i changed in build.gradle this line to ('none' or 'cmake' - i try both)\r\n```\r\ndef nativeBuildSystem = 'none'\r\n```\r\n\r\nbecause none of other variants (bazel, makefile) not worked - have many errors that I couldn't resolve ", "when I try to build and run app with option `cmake` + I comment line \r\n```\r\napply from: \"download-models.gradle\"\r\n```\r\nin build.gradle file, app runs and don't fall with error of wrong `input_name`, but now it have another error: with input labels size..  \r\n```\r\n java.lang.ArrayIndexOutOfBoundsException: length=2; index=2\r\n...\r\nat org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.recognizeImage(TensorFlowObjectDetectionAPIModel.java:194)\r\n```\r\n\r\nHow does params `outputScores` and `outputClasses` collected in code (they length are setted to `MAX_RESULTS` which by default = 100 in sample)? Where can i change it ? \r\n\r\nps - now I don't understand: does code use my own pb file or not? Labels are parsed, I can see it while debugging, but about pb file I can't understand how to check. ", "Maybe someone can tell me - what should I do - to use TF DETECT demo app with my own pb model file? And all my errors will 'go out' ) \r\nI have only one class to detect, file pb with exported frozen graph - and what I need to do next?\r\n@jch1 @tensorflowbutler ", "So, after I compile and build project with `cmake`, change paths `TF_OD_API_MODEL_FILE` and `TF_OD_API_LABELS_FILE`, create file `labels.txt` with:\r\n\r\n```\r\n???\r\nyour_object_class_name\r\n```\r\n\r\nI can run working example app.\r\nWhy don't I find any explanation about this moment with `???` on the top of labels file in Tensorflow tutorials? I think it must be included somewhere, because without this moment no will be worked.\r\n\r\nSo, the problem can be closed, but I think someone need to add small tutorial to Tensorflow Example of Object Detection about file with users custom labels @tensorflowbutler ", "Hi, I have the same issue\r\nI followed the following tutorial to convert it to TFLITE\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md\r\n\r\nThen I had tflite model from my custom dataset. using Mobilenet network.\r\n\r\nNow Im having the same error you had When i run TF Detect.\r\nhere is the android log\r\n\r\n11-19 14:58:53.492 29465-29465/? I/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480\r\n    CameraConnectionFragment: Valid preview sizes: [1920x1080, 1440x1080, 1280x720, 720x480, 640x480]\r\n    CameraConnectionFragment: Rejected preview sizes: [320x240]\r\n    CameraConnectionFragment: Exact size match found.\r\n11-19 14:58:53.507 29465-29465/? W/tensorflow: ImageUtils: Native library not found, native RGB -> YUV conversion may be unavailable.\r\n11-19 14:58:53.695 29465-29465/org.tensorflow.demo W/tensorflow: TensorFlowObjectDetectionAPIModel: positive\r\n    TensorFlowObjectDetectionAPIModel: negative\r\n11-19 14:58:53.699 29465-29465/org.tensorflow.demo I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\n11-19 14:58:53.699 29465-29465/org.tensorflow.demo E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n11-19 14:58:53.699 29465-29465/org.tensorflow.demo I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n11-19 14:58:53.796 29465-29465/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n11-19 14:58:54.082 29465-29465/org.tensorflow.demo I/TensorFlowInferenceInterface: Model load took 27ms, TensorFlow version: 1.12.0\r\n11-19 14:58:54.083 29465-29465/org.tensorflow.demo I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/detect300300.tflite'\r\n11-19 14:58:54.084 29465-29465/org.tensorflow.demo E/tensorflow: CameraActivity: Exception!\r\n    java.lang.RuntimeException: Failed to find input Node 'image_tensor'\r\n        at org.tensorflow.demo.TensorFlowObjectDetectionAPIModel.create(TensorFlowObjectDetectionAPIModel.java:102)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:169)\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1150)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:148)\r\n        at android.app.ActivityThread.main(ActivityThread.java:5461)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)\r\n11-19 14:59:16.527 29465-29465/org.tensorflow.demo D/tensorflow: CameraActivity: onPause org.tensorflow.demo.DetectorActivity@9bf609f\r\n    CameraActivity: Requesting finish\r\n11-19 14:59:16.580 29465-29465/org.tensorflow.demo D/tensorflow: CameraActivity: onStop org.tensorflow.demo.DetectorActivity@9bf609f\r\n11-19 14:59:16.584 29465-29465/org.tensorflow.demo D/tensorflow: CameraActivity: onDestroy org.tensorflow.demo.DetectorActivity@9bf609f\r\n\r\n\r\none of the solution is to change the tensorflow since it is not compatible with the one that android is using .. but still the same results .. i have the latest tensorflow  like this\r\n\r\ndependencies {\r\n    if (nativeBuildSystem == 'cmake' || nativeBuildSystem == 'none') {\r\n        compile 'org.tensorflow:tensorflow-android:1.12.0'\r\n        //compile 'org.tensorflow:tensorflow-android:+'\r\n    }\r\n}\r\n\r\nbut the same error!\r\nany hint?", "I run summarize_graph to see the input and changed the input name to normalized_input_image_tensor but still the same error\r\n\r\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph --in_graph=/home/dell/TFObject-detection/tensorflow/models/research/object_detection/Output2/tflite_graph.pb\r\n\r\nFound 1 possible inputs: (name=normalized_input_image_tensor, type=float(1), shape=[1,300,300,3]) \r\nNo variables spotted.\r\nFound 1 possible outputs: (name=TFLite_Detection_PostProcess, op=TFLite_Detection_PostProcess) \r\nFound 4035720 (4.04M) const parameters, 0 (0) variable parameters, and 0 control_edges\r\nOp types used: 411 Identity, 340 Const, 60 FusedBatchNorm, 55 Conv2D, 43 Relu6, 17 DepthwiseConv2dNative, 12 BiasAdd, 12 Reshape, 10 Add, 2 ConcatV2, 1 Placeholder, 1 RealDiv, 1 Sigmoid, 1 Squeeze, 1 TFLite_Detection_PostProcess\r\nTo use with tensorflow/tools/benchmark:benchmark_model try these arguments:\r\nbazel run tensorflow/tools/benchmark:benchmark_model -- --graph=/home/dell/TFObject-detection/tensorflow/models/research/object_detection/Output2/tflite_graph.pb --show_flops --input_layer=normalized_input_image_tensor --input_layer_type=float --input_layer_shape=1,300,300,3 --output_layer=TFLite_Detection_PostProcess\r\n", "\r\n \r\n@SteveIb \r\n\r\nHave you seen my answer about labels.txt file structure (first row of it must contain \"???\" and after that, on the second row - you real label(s) should be written)? Maybe it can help you. ", "@SheptunovaAA \r\nHi,\r\nYest I tried it .. didnt work either!", "I have the same issue using a .tflite file that I converted from the ssd_mobilenet_v1 model from http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz\r\n`    private static final String TF_OD_API_LABELS_FILE = \"file:///android_asset/coco_labels_list.txt\";\r\n`\r\nA .tflite conversion of a the ssd_mobilenet_v1_android_export.pb also results in that error.\r\n\r\nHere I have already left a comment:\r\nhttps://github.com/tensorflow/tensorflow/issues/15633#issuecomment-456860149", "@digital-prime Is this still an issue ? We see that you are using old version of tensorflow  1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors in TF v2, we will get you the right help . Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 22564, "title": " ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu, **CPU**\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: binary (pip install through anaconda)\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: \r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nHi, I want to covert a .pb model to .tflite one. The model is train with tensorflow object detection API. The input tensor shape is (None, None, None, 3) but it seems that tflite_convert doesn't support this kind of input.\r\nSource code / logs\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'.\r\n\r\n@pkulzc Do you think this error is caused because we don't provide the input shape for in the []( models/research/object_detection/export_inference_graph.py ) ? If so, should we edit this file and set the input shape (dimension of our pictures) ?\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@wt-huang Can you please help me to solve it? I really nead to convert my .pb file to .tflite.\r\n\r\nExact Command:\r\ntoco\r\n--input_file=mobilenet_v1_1.0_224/teste/sfrozen_inference_graph.pb\r\n--input_format=TENSORFLOW_GRAPHDEF\r\n--output_file=/tmp/mobilenet_v1_1.0_224.tflite\r\n--input_shape=-1,-1,-1,3\r\n--input_array=image_tensor\r\n--output_array=detection_boxes,detection_scores,detection_classes,detection_nums \\", "@Elites2017 Currently Toco is not supported on Windows 10. Please try to convert the model on Linux or Mac. It should work.", "> \r\n> \r\n> @Elites2017 Currently Toco is not supported on Windows 10. Please try to convert the model on Linux or Mac? It should work.\r\n\r\n@wt-huang I use toco on Ubuntu 16.04. I convert the frozen graph from the mobilenet v1_1.0_224. the problem is after training, I got a new frozen graph with different parameters (from the default frozen graph coming with that model) such as: input_type=image_tensor, input_arrays=[None, None, None, 3], output_arrays=detection_boxes,detection_scores,detection_classes,detection_nums. I've got this error:\r\n**ValueError: None is only supported in the 1st dimension. Tensor 'image_tensor:0' has invalid shape '[None, None, None, 3]'**", "For SSD models trained with detection API, you should use this [binary](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py) to convert the graph.", "> \r\n> \r\n> For SSD models trained with detection API, you should use this [binary](https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py) to convert the graph.\r\n\r\n@pkulzc I've used images of size 256 x 256. in the binary file you mentioned, they say:\r\n**Inputs:\r\n'normalized_input_image_tensor': a float32 tensor of shape\r\n[1, height, width, 3] containing the normalized input image. Note that the\r\nheight and width must be compatible with the height and width configured in\r\nthe fixed_shape_image resizer options in the pipeline config proto.**\r\n\r\nI haven't done anything in that pipeline config proto, where is that file? Do I need to pass the dimension of the pictures used to trained my model in that file? \r\n\r\nWith the binary mentioned, will I have to convert the frozen graph I will have after exportation to tflite format in order to use it for mobile devices or that frozen graph can be directly used on mobile devices without conversion? ", "Please read the comments carefully. It says \r\n\r\n> The exported graph has the following input and output nodes.\r\n> Inputs:\r\n> 'normalized_input_image_tensor': ...\r\n\r\nSo when you export the graph, you don't need to provide this.\r\n\r\nSee this [post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) for some more details.", "> \r\n> \r\n> Please read the comments carefully. It says\r\n> \r\n> > The exported graph has the following input and output nodes.\r\n> > Inputs:\r\n> > 'normalized_input_image_tensor': ...\r\n> \r\n> So when you export the graph, you don't need to provide this.\r\n> \r\n> After this exporting you don't need to convert again. See this [post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) for some more details.\r\n\r\n@pkulzc  Maybe I misunderstood whe you said that I don't need to convert the frozen graph obtained to tflite after exportation. I thouth I could use that graph directly into an android app.\r\n\r\nAbout the link you mentionned, where do I get those parameters **(input_shapes=1,300,300,3)** or **should I replace the 300 x 300 by 256 x 256 which are the width and height of the images that I've used to trained the model or those are deflautf parameters**? What about the std_value, mean_value?\r\n\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- \\\r\n--input_file=$OUTPUT_DIR/tflite_graph.pb \\\r\n--output_file=$OUTPUT_DIR/detect.tflite \\\r\n--input_shapes=1,300,300,3 \\\r\n--input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  \\\r\n--inference_type=QUANTIZED_UINT8 \\\r\n--mean_values=128 \\\r\n--std_values=128 \\\r\n--change_concat_input_ranges=false \\\r\n--allow_custom_ops", "Yes you still need to run toco to get tflite flatbuffer format  file. \r\n\r\n@achowdhery  could you please comment on the toco questions?", "> \r\n> \r\n> Yes you still need to run toco to get tflite flatbuffer format file.\r\n> \r\n> @achowdhery could you please comment on the toco questions?\r\n\r\n@pkulzc What about the parameters, where can I find them in my case?", "@pkulzc By running toco to get tflite flatbuffer format file, I got the following error:\r\n\r\n**F tensorflow/contrib/lite/toco/tooling_util.cc:1589] Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\nAborted (core dumped)**\r\n\r\nAny idea..?", "@pkulzc the (tflite_graph.pb) file is converted to .tflite with the following warning.\r\n**Ignoring unsupported attribute type with key '_output_types'** should I worry about it ?\r\n\r\nExact comand used:\r\nbazel run -c opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/david/pro/tflite_graph.pb --output_file=/home/david/pro/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess --output_arrays=TFLite_Detection_PostProcess:1 --output_arrays=TFLite_Detection_PostProcess:2 --output_arrays=TFLite_Detection_PostProcess:3 --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6", "Now I got this problem when trying to build the demo app on Android Studio. (Windows 10)\r\n\r\n**\\contrib\\lite\\examples\\android\\BUILD\\android-profile**", "Closing as this is a duplicate, feel free to reopen if any error comes up.", "We are running into the exact same issue. @pkulzc we are not able to use the export_tflite_ssd_graph.py since we only have the frozen inference graph. With our frozen .pb file knowing the input and output arrays based on tensorboard, we are attempting to use toco and convert to tflite file.\r\n\r\nThis is our error:\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n", "Are you trying to use a float model or quantized model?", "> Are you trying to use a float model or quantized model?\r\n\r\nThis is our input type.\r\ndtype\r\n{\"type\":\"DT_UINT8\"}", "@achowdhery Basically, I have an frozen inference graph (.pb) that I am attempting to turn into a tflite model.\r\n\r\nThis is my command line input:\r\n\r\ntflite_convert \r\n--output_file=/home/aurash/Documents/detect.tflite \r\n--graph_def_file=/home/aurash/Downloads/frozen_inference_graph.pb \r\n--input_arrays=image_tensor \r\n--output_arrays=detection_boxes,detection_scores,detection_classes,num_detections\r\n\r\nThis is the error I am seeing:\r\nValueError: None is only supported in the 1st dimension. Tensor 'image_tensor' has invalid shape '[None, None, None, 3]'.\r\n\r\nIf I add the flag --input_shapes={1,300,300,3}\r\nI get the following error:\r\nValueError: The shape of tensor 'image_tensor' cannot be changed from (?, ?, ?, 3) to [3]. Shapes must be equal rank, but are 4 and 1\r\n\r\n\r\nI am not sure what to do at this point?\r\n", "For the quantized model SSD MobileNet you can use the instructions at the end in this [blog post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)", "> For the quantized model SSD MobileNet you can use the instructions at the end in this [blog post](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193)\r\n\r\nI am familiar with this blog post, yet this post assumes that we have access to the ckpt and pipeline.config files to generate a .pb file that is compatible with tflite. Basically, I would like to ask if it is possible to skip this step and use a .pb file that has been given to me. As shown above, I have changed the input and output array parameters as my frozen graph is not the same as the one used in the blog post.", "\r\nIn the documentation for toco it gives the following example:\r\n\r\n```\r\ntflite_convert \\\r\n  --output_file=/tmp/mobilenet_v1_1.0_224.tflite \\\r\n  --graph_def_file=/tmp/mobilenet_v1_0.50_128/frozen_graph.pb \\\r\n  --input_arrays=input \\\r\n  --output_arrays=MobilenetV1/Predictions/Reshape_1\r\n\r\nThe frozen_graph.pb file used here is available for download. Setting the input_array and output_array arguments is not straightforward. The easiest way to find these values is to explore the graph using TensorBoard. Reuse the arguments for specifying the output nodes for inference in the freeze_graph step.\r\n```\r\nWe also used tensorboard to determine our input and output, yet we are running into the issue I mentioned above. I appreciate your help.", "Then you probably need to use input_arrays=Preprocessor/sub and output_arrays= concat, concat_1, Please add visualization of the frozen graph (say from a tool such as [this](https://github.com/lutzroeder/netron)). Your model will convert as long as all ops are supported in Tensorflow Lite between the input and the output nodes.", "[Here is a link to my frozen graph from tensorboard.](https://drive.google.com/file/d/1XvCyiO7Fr-lc70KUPDMQf1OTxsXVxrPr/view?usp=sharing)\r\n\r\nI tried your changes and got the following output:\r\n\r\n/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\r\n  warnings.warn(warning, RequestsDependencyWarning)\r\n2019-01-31 16:52:40.318160: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-01-31 16:52:40.340290: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz\r\n2019-01-31 16:52:40.340878: I tensorflow/compiler/xla/service/service.cc:161] XLA service 0x3e399b0 executing computations on platform Host. Devices:\r\n2019-01-31 16:52:40.340930: I tensorflow/compiler/xla/service/service.cc:168]   StreamExecutor device (0): <undefined>, <undefined>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py\", line 191, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 442, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py\", line 205, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\r\n  warnings.warn(warning, RequestsDependencyWarning)\r\n2019-01-31 16:52:41.597453: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.604910: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.605006: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-31 16:52:41.605029: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.605037: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.605044: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.605052: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.605064: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.605073: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LoopCond\r\n2019-01-31 16:52:41.605087: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.605093: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.605099: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.605105: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-31 16:52:41.605479: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.605521: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.605532: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-31 16:52:41.605553: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2019-01-31 16:52:41.605572: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-31 16:52:41.605596: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-31 16:52:41.617073: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617117: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617128: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617134: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617141: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617147: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617173: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-31 16:52:41.617203: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-31 16:52:41.617230: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayScatterV3\r\n2019-01-31 16:52:41.617243: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617249: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617255: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617261: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617267: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617274: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617282: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayV3\r\n2019-01-31 16:52:41.617287: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617293: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617301: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617309: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617316: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617323: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617339: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617347: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: LoopCond\r\n2019-01-31 16:52:41.617371: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617379: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617385: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617391: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-31 16:52:41.617398: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617403: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617408: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617415: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-31 16:52:41.617423: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617428: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.617434: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617441: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayReadV3\r\n2019-01-31 16:52:41.617531: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-01-31 16:52:41.617557: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.617601: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-01-31 16:52:41.617640: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: NonMaxSuppression\r\n2019-01-31 16:52:41.617677: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-01-31 16:52:41.617733: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Where\r\n2019-01-31 16:52:41.617769: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: NonMaxSuppression\r\n2019-01-31 16:52:41.617814: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Size\r\n2019-01-31 16:52:41.618202: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.618212: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.618219: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-31 16:52:41.618229: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.618236: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.618243: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-31 16:52:41.618251: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.618258: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.618264: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-31 16:52:41.618272: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Enter\r\n2019-01-31 16:52:41.618279: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-01-31 16:52:41.618285: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayWriteV3\r\n2019-01-31 16:52:41.618301: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2019-01-31 16:52:41.618310: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2019-01-31 16:52:41.618316: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2019-01-31 16:52:41.618322: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: Exit\r\n2019-01-31 16:52:41.618329: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-31 16:52:41.618347: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-31 16:52:41.618356: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-31 16:52:41.618373: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-31 16:52:41.618383: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-31 16:52:41.618399: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-31 16:52:41.618407: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArraySizeV3\r\n2019-01-31 16:52:41.618424: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: TensorArrayGatherV3\r\n2019-01-31 16:52:41.661650: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1461 operators, 2604 arrays (0 quantized)\r\n2019-01-31 16:52:41.697924: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 556 operators, 893 arrays (0 quantized)\r\n2019-01-31 16:52:41.715714: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 556 operators, 893 arrays (0 quantized)\r\n2019-01-31 16:52:41.739096: F tensorflow/lite/toco/tooling_util.cc:627] Check failed: dim >= 1 (0 vs. 1)\r\nAborted (core dumped)", "You are running a MobileNet model or MobileNet SSD model. Those are different and input/output will change.", "mobilenet SSD", "Did you try using input_arrays=Preprocessor/sub and output_arrays= concat, concat_1, Please add visualization of the frozen graph (say from a tool such as this). Your model will convert as long as all ops are supported in Tensorflow Lite between the input and the output nodes.", "yes, please refer to my message above.", "https://github.com/tensorflow/tensorflow/issues/22564#issuecomment-459538809", "Then you probably need to identify which input and output nodes will work. The model trained from OSS Tensorflow Object Detection API does not have these nodes.\r\nThe other check is to make sure you have evaluation graph and not training graph.", "i have the same issue, how did you solve this @aurashn ?\r\n", "@achowdhery how can i determine the input and output nodes that will work on converting to tflite?", " @achowdhery  i have an error saying `Check failed: GetOpWithOutput(model, output_array) Specified output array \"TFlite_Detection_PostProcess\" is not produced by any op in this graph. Is it a typo? To silence this message, pass this flag:  allow_nonexistent_arrays\r\n` \r\n\r\neven though it is in my graph. which is this \r\n![image](https://user-images.githubusercontent.com/40930782/56848402-340fe080-691b-11e9-9a77-8dc3ed6cc25b.png)  obtained from tflite_graph.pb. how can i solve this?\r\n", "@inakaaay @achowdhery We are having a similar error message. Does anyone know how to solve this?\r\n\r\n<img width=\"1244\" alt=\"Screen Shot 2019-05-02 at 6 42 58 PM\" src=\"https://user-images.githubusercontent.com/24554/57115131-49fe1680-6d0a-11e9-9a4e-b77812b52e25.png\">\r\n\r\n`F tensorflow/lite/toco/tooling_util.cc:917] Check failed: GetOpWithOutput(model, output_array) Specified output array \"TFLite_Detection_PostProcess\" is not produced by any op in this graph. Is it a typo? This should not happen. If you trigger this error please send a bug report (with code to reporduce this error), to the TensorFlow Lite team.`\r\n\r\n", "Solved it with extra parameters inside code:\r\n\r\n    import tensorflow as tf\r\n\r\n    graph_def_file = \"mask_rcnn_resnet50_atrous_coco_2018_01_28/frozen_inference_graph.pb\"\r\n    input_arrays = [\"image_tensor\"]\r\n    output_arrays = [\"detection_scores\",\"detection_boxes\",\"detection_classes\",\"detection_masks\"]\r\n\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(\r\n            graph_def_file, input_arrays, output_arrays,input_shapes={\"image_tensor\":[1,600,600,3]})\r\n    tflite_model = converter.convert()\r\n    open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n\r\nAlthough it cannot produce .tflite because it raises kmerge error \"OperatorType::kMerge Found Sub as non-selected output from Switch, but only Merge supported\"", "Model ssd_mobile_v2.\r\n# First, you convert using export_tflite_ssd_graph.py. Pay attention to max_detections to get good performance on mobile.\r\n\r\npython export_tflite_ssd_graph.py  --input_type image_tensor --pipeline_config_path training_ssd_v2_config --trained_checkpoint_prefix ../output/model.ckpt-124960 --output_directory ../output/frozen_tflite/frozen_inference_graph.pb -add_postprocessing_op True --max_detections 10\r\n\r\n--> It will  generate 2 files: **tflite_graph.pb** to be used below and tflite_graph.pbtxt\r\n\r\n# Second, use tflite_convert with tensorflow (or tensorflow-gpu) **version 1.11**:\r\ntflite_convert --output_file=./tflite/detect.tflite \\\r\n--graph_def_file=/home/nguyen/ssd/Foods/dataset/OD_food_non_food/ssd_model/output/frozen_tflite/frozen_inference_graph.pb/tflite_graph.pb \\\r\n--input_arrays=normalized_input_image_tensor  \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n--input_shape=1,300,300,3  \\\r\n--allow_custom_ops\r\n\r\nVoila :dango: ", "hi.\r\nmy code is:\r\n\r\nfrom tensorflow.contrib import lite\r\nconverter = lite.TFLiteConverter.from_keras_model_file( r'/content/drive/My Drive/inceptionv3-transfer-learning__fine_tune.model' ) # Your model's name\r\nmodel = converter.convert()\r\nfile = open( 'model.tflite' , 'wb' )\r\nfile.write( model )\r\n\r\nValueError: None is only supported in the 1st dimension. Tensor 'input_1' has invalid shape '[None, None, None, 3]'.\r\n\r\nwhat will my input_array,input_shape and output_array be?", "> Model ssd_mobile_v2.\r\n> \r\n> # First, you convert using export_tflite_ssd_graph.py. Pay attention to max_detections to get good performance on mobile.\r\n> python export_tflite_ssd_graph.py --input_type image_tensor --pipeline_config_path training_ssd_v2_config --trained_checkpoint_prefix ../output/model.ckpt-124960 --output_directory ../output/frozen_tflite/frozen_inference_graph.pb -add_postprocessing_op True --max_detections 10\r\n> \r\n> --> It will generate 2 files: **tflite_graph.pb** to be used below and tflite_graph.pbtxt\r\n> \r\n> # Second, use tflite_convert with tensorflow (or tensorflow-gpu) **version 1.11**:\r\n> tflite_convert --output_file=./tflite/detect.tflite\r\n> --graph_def_file=/home/nguyen/ssd/Foods/dataset/OD_food_non_food/ssd_model/output/frozen_tflite/frozen_inference_graph.pb/tflite_graph.pb\r\n> --input_arrays=normalized_input_image_tensor\r\n> --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'\r\n> --input_shape=1,300,300,3\r\n> --allow_custom_ops\r\n> \r\n> Voila \ud83c\udf61\r\n\r\nThis worked. :)\r\nThanks a lot "]}, {"number": 22563, "title": "RuntimeError: TOCO failed see console for info.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Closing this issue due to lack of activity. Feel free to reopen this issue when new information is available.\r\nThanks!."]}, {"number": 22562, "title": "[WIP] Load CUDA libraries dynamically", "body": "Prior to this commit, TensorFlow was built in two flavours: CPU-only and CPU/GPU. The latter version was dynamically linked with CUDA and therefore required CUDA libraries to be installed in the host system. This change replaces dynamic linking with dynamic loading for all of the CUDA libraries but cudart. This would allow to ship a single TensorFlow build which works for both cases.\r\n\r\nNote that TensorFlow already used dynamic loading for CUDA libraries prior to version 1.1.0. In 1.1.0 however, dynamic loading has been changed to dynamic linking, see commit 191658d54f90ac03c15b339326129cd52d1f56a3.\r\n\r\nCloses #16184", "comments": ["@gunan a few questions to drop the [WIP] status:\r\n\r\n* Are you OK with having the same copy-pasted shim in both cuda_driver.cc and gpu_cudamalloc_allocator.cc? As an alternative, I can make gpu_cudamalloc_allocator use  cuda_driver, or extract the wrappers into a separate file.\r\n* What would be the best way to eliminate the remaining dependency on cudart? The usages are scattered through multiple subsystems, so the local shim trick does not apply. I could create a wrapper in a separate file, e.g. (common_runtime/gpu/gpu_cudart.*) and change the usages to call the wrapped functions.\r\n* Ideally, I'd like to ensure that if CUDA is not available, one cannot register a GPU device. Could you point me to the part of the code responsible for that?", "@jlebar @martinwicke to answer some of these questions.", "cc @Artem-B ", "@superbobry Is this still WIP?", "@caisq it is, since it does not remove CUDA libraries entirely (see questions in the comment above). I got some of the answers from @Artem-B over email a couple of weeks ago, but did not have time to revisit the PR since.", "I tried up-streaming this change to head, but now I'm getting errors `libtensorflow_framework.so: undefined reference to `cuOccupancyMaxPotentialBlockSize'`. Any idea what else might have changed at head?", "Looks like #21958 added the usage of symbol cuOccupancyMaxPotentialBlockSize, thanks to @gunan's detective work.", "@gunan @yifeif fixed all your grievances here #24860, let's do it", "This needs a StreamExecutor owner to review -- probably @timshen91.  It also needs @Artem-B's sign-off. ", "Closing this as the corresponding changes have been submitted. Thanks @superbobry!"]}, {"number": 22561, "title": "tflite inceptionv3      the output is greater than 1.", "body": "HI\uff0c\r\n\r\ndemo :  tflite inceptionv3 \r\n\r\nMay I ask why the output is greater than 1.\r\n\r\nThanks.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Hi\uff0c\r\n\r\nthe ouput is tensorflow/tensorflow/contrib/lite/java/demo/  on adnroid device.\r\n\r\nThanks.", "@ZeroZxj Hi, could you please describe the problem clearly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22560, "title": "Problem while building tensorflow", "body": "Sry, my mistake", "comments": []}, {"number": 22559, "title": "Fix grpc+gdr compile error introduced in 33170cc6.", "body": "@poxvoculi Most CPU tensors in PS are still managed by cpu_allocator, and I have added a separate path to copy them back to a CPU tensor managed by ProcessState::GetCPUAllocator.\r\n\r\nI believe there is no harm (maybe a little bit larger page table on NIC?) to register the same memory region multiple times, so it is not required for the memory managers to be singletons.", "comments": ["@byronyi Sorry for breaking this directory.  I don't have an environment in which I can compile and test it, so I ended up submitting a best guess.\r\n\r\nWith the latest change, the value returned by cpu_allocator() should not be directly passed to any devices if ProcessState::AddCPUAllocVisitor is called prior to any CPU devices being created.   It's important when RDMA is going to be used that visitors be registered very early in process startup, so that all uses get the correct allocators.    I tried to achieve that by putting the new RegMemVisitors in the GdrMemoryManager constructor, which gets called in the GdrServer constructor.  The devices are not created until GrpcServer::Init, [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc#L162).  Are you seeing the PS processes using the cpu_allocator() values directly, and not the allocator with SubAllocator using the registered visitors, created [here?](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/process_state.cc#L91)\r\n\r\nWhether it's a problem to duplicate-register the same region depends on the networking libraries in use.  For us it's a fatal error, but I can imagine another library ignoring a duplicate registration or making a duplicate table entry.", "It's quite alright. In fact, with Ubuntu 18.04 or any Linux distro with kernel > 4.14, we could build the package without using any specific hardware or proprietary drivers/libraries, like in a GCP VM or a plain container (all necessary libraries are free software and maintained by distro). Testing is also possible with SoftRoCE (with in-tree driver rdma-rxe). I will add detailed docs after we move the current code to tensorflow/networking.\r\n\r\nAs to the tensors allocated by cpu_allocator(), I found many of them falls into one of the following categories:\r\n\r\n1. boolean flag, like `report_uninitialized_variables/boolean_mask/Prod` \r\n2. global step counter, like `edge_138_inc_global_step/AssignAdd`\r\n3. single aggregated value, like `edge_133_average_loss/Mean`\r\n4. other vectorised tensors, like `append_apply_gradient_ops/GradientDescent/update_v/cg/affine2/biases/ApplyGradientDescent` and `tower_0/v/gradients/AddN_1_S33`\r\n\r\n You could take a look of the detailed log of [PS](https://gist.github.com/byronyi/568998c100615dfc9cada5112c7fd05c) and the log of the corresponding [worker](https://gist.github.com/byronyi/e4ba356dfb40c135dc673acc36b9424f). I may have looked the wrong place, but I could find many of them by  ```grep \"allocator_name:\\ \\\"cpu\\\"\"```.", "By the way, should we use 0 or -1 for unknown NUMA node? I would like to stay consistent to the rest of core.", "Thanks for the logs.  What's going on with the direct use of cpu_allocator is that there are still a few uses in the code base that haven't been cleaned up yet, for example in Tensor::FromProto.  I'll be fixing them soonish, but it may be some weeks before they're all taken care of, so if you can handle doing duplicate registration for now, that's probably best.\r\n\r\nFor unknown NUMA node, you should be using port::kNUMANoAffinity, defined in tensorflow/core/platform/numa.h.  For the present time this will continue to resolve to CPU:0.  In the future it will indicate a \"don't care\" or \"no particular affinity\" choice.", "@poxvoculi Hi Paul, I rebased to current master. Let me know if you have any comments. Thanks!\r\n\r\nBtw, I was wrong about singleton visitors: we are working on porting GDR to multi-rail network platform, and non-singleton visitors are necessary with multiple NICs/devices. ", "@caisq Is this ready to pull? ", "@poxvoculi Hi Paul, is this ready to pull?", "Yes, I think it's ready.  I'll try to find out who's supposed to be doing this.", "@byronyi The PR will be merged automatically when it passes tests and gets submitted internally.", "hi @byronyi, @poxvoculi \r\nI tried running the new GDR from latest master but I encountered an error:\r\nCheck failed: 0 == cpu_allocators_.size() (0 vs. 1)AddCPUAllocVisitor must be called prior to first call to ProcessState::GetCPUAllocator\r\n\r\nI am also getting the same error on my the patch I am working on to fix the verbs contribution.\r\n\r\nMy understanding is the the GPU \"base device\" is created in [this line](https://github.com/tensorflow/tensorflow/blob/aab3c53e1484404a70565324d1231c4e6ead7425/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1111), trigerring the call to GetCPUAllocator before the visitors are registered, which is an error.\r\nI am still trying to figure out how the tensorflow core is managing/syncing those event (GPU context creation and memory manager initialization). ", "@yanivbl6 Hi Yaniv, could you elaborate on your running environment? Is it PS/worker, and is GPU involved?", "The error occurs on the worker with the visible GPU (V-100 in my case), but will also occur on a parameter server if I expose the GPU. \r\nI use CUDA 9.0, ubuntu 16.04, and build with NCCL2.2 for cuda 9.0 in addition to GDR.", "This could be an affect of the organization of your program at the python level.  Because variables have a lifespan that outlasts sessions, allocator initialization happens once per process.  It's important that all session creation in the python expressed TF program be consistent, i.e. if the program is going to run distributed then don't ever create a local-only session.  If you create a local-only session then later a distributed session, something like this error might occur.", "I don't think it's coming from the program, since I was running the upstream tf_cnn benchmark (synthetic run), and tried reverting back to older versions (back till tf1.5 compatible), with the same error.\r\n\r\nAlso- the error is happening on initialization, so the variables don't yet have a session to outlast.\r\nI will try investigating the issue a bit further and will open an issue once I know more.\r\n\r\nEdit:\r\nI do think there is a problem here, since GdrServer::Init is calling GrpcServer::Init that initialize the GPU (And calls GetCPUAllocator) **before** calling to the memory registration that registers the suballocator.\r\n\r\nEdit2:\r\nThe tensorflow/benchmark is not off the hook just yet- I see they use some dummy session that creates the session and is immediately deleted. This could be what 'confuses' the suballocator. I think that both of the issues may exist simultaneously .", "@yanivbl6 I could reproduce that on a DGX-1V. Will take a few days to fix it.", "thanks for checking it- I am also working on it and I will make sure to patch GDR if I find a solution."]}, {"number": 22558, "title": "@fft2d//:fft2d bazel ", "body": "fisrt build \r\nERROR: /private/var/tmp/_bazel_dsr/f1e96f4f92f1092791bdd5610d5595fa/external/com_google_absl/absl/strings/BUILD.bazel:84:1: C++ compilation of rule '@com_google_absl//absl/strings:internal' failed (Exit 1)\r\n\r\n\r\n\r\nafter build\r\nERROR: /private/var/tmp/_bazel_dsr/f1e96f4f92f1092791bdd5610d5595fa/external/fft2d/BUILD.bazel:26:1: C++ compilation of rule '@fft2d//:fft2d' failed (Exit 1)\r\n\r\nmacos 10.13.6\r\nndk 16.0.4442984 \r\n \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@dingshaoran It appears that something is off in the tool chain. Please fill out the issue template so we can better help you.", "Closing this for now, please open a new issue if any error comes up."]}]