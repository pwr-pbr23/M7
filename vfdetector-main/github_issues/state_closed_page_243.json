[{"number": 47229, "title": "Building TFLite WHL on Pi Zero (armv6l) - \"warning: requested alignment 16 is larger than 8 [-Wattributes]\"", "body": "**System information**\r\n- OS Platform and Distribution: Raspbian GNU/Linux 10 (buster) - **Fresh install**\r\n- Pi Zero (armv6l)\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7.3\r\n- Installed with pip\r\n\r\n\r\n\r\nUpon trying to build the TFLite Python wheel for my Pi Zero, `build_pip_package.sh ` spits out warnings and I believe causes my Pi to crash (green light is flashing consistently, my unix terminal I'm using to ssh to the Pi is unresponsive or extremely slow). I let it go the 2nd time around, and the Pi recovered, but I couldn't find the wheel anywhere on the Pi. I'm assuming it didn't actually finish and crashed instead, with ample recovery time after.\r\n\r\nHere's what I did to try to build TFLite:\r\n```\r\nsudo apt install swig libjpeg-dev zlib1g-dev python3-dev python3-numpy\r\npip install numpy pybind11 && pip3 install numpy pybind11\r\n./tensorflow/lite/tools/make/download_dependencies.sh #sh command did not work for these commands, error included in log\r\n./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n```\r\n\r\n\r\n**Logs**\r\n```\r\n./tensorflow/lite/tools/make/download_dependencies.sh \r\ndownloading https://gitlab.com/libeigen/eigen/-/archive/011e0db31d1bed8b7f73662be6d57d9f30fa457a/eigen-011e0db31d1bed8b7f73662be6d57d9f30fa457a.tar.gz\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100 2604k    0 2604k    0     0  1851k      0 --:--:--  0:00:01 --:--:-- 1849k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/eigen\r\n/tmp/tmp.KDDmNAnmqI/eigen-011e0db31d1bed8b7f73662be6d57d9f30fa457a.tar.gz: OK\r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/gemmlowp/archive/fda83bdc38b118cc6b56753bd540caa49e570745.zip\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  914k  100  914k    0     0  1793k      0 --:--:-- --:--:-- --:--:-- 1797k\r\nchecking sha256 of tensorflow/lite/tools/make/downloads/gemmlowp\r\n/tmp/tmp.ZohJqyHeDH/fda83bdc38b118cc6b56753bd540caa49e570745.zip: OK\r\nArchive:  /tmp/tmp.ZohJqyHeDH/fda83bdc38b118cc6b56753bd540caa49e570745.zip\r\n```\r\n...\r\n```\r\npi@raspberrypi:~/tensorflow $ sh tensorflow/lite/tools/pip_package/build_pip_package.sh\r\ntensorflow/lite/tools/pip_package/build_pip_package.sh: 18: tensorflow/lite/tools/pip_package/build_pip_package.sh: Bad substitution\r\n+ cd \r\n+ pwd\r\n+ SCRIPT_DIR=/home/pi/tensorflow\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/home/pi/tensorflow/../../../..\r\n+ TENSORFLOW_LITE_DIR=/home/pi/tensorflow/../../../../tensorflow/lite\r\n+ + + grepcut _VERSION =  /home/pi/tensorflow/../../../../tensorflow/tools/pip_package/setup.py -d=sed\r\n -f2 s/[ '-]//g\r\n\r\ngrep: /home/pi/tensorflow/../../../../tensorflow/tools/pip_package/setup.py: No such file or directory\r\n+ TENSORFLOW_VERSION=\r\n+ export PACKAGE_VERSION=\r\n+ BUILD_DIR=/home/pi/tensorflow/gen/tflite_pip/python3\r\n+ rm -rf /home/pi/tensorflow/gen/tflite_pip/python3\r\n+ mkdir -p /home/pi/tensorflow/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/debian /home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/setup.py /home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /home/pi/tensorflow/../../../../tensorflow/lite/python/interpreter_wrapper /home/pi/tensorflow/gen/tflite_pip/python3\r\ncp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/debian': No such file or directory\r\ncp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/setup.py': No such file or directory\r\ncp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in': No such file or directory\r\ncp: cannot stat '/home/pi/tensorflow/../../../../tensorflow/lite/python/interpreter_wrapper': No such file or directory\r\n```\r\n...\r\n```\r\npi@raspberrypi:~/tensorflow $ ./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n+++ dirname ./tensorflow/lite/tools/pip_package/build_pip_package.sh\r\n++ cd ./tensorflow/lite/tools/pip_package\r\n++ pwd\r\n+ SCRIPT_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package\r\n+ PYTHON=python3\r\n+ VERSION_SUFFIX=\r\n+ export TENSORFLOW_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../..\r\n+ TENSORFLOW_LITE_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite\r\n++ cut -d= -f2\r\n++ grep '_VERSION = ' /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py\r\n++ sed 's/[ '\\''-]//g'\r\n+ TENSORFLOW_VERSION=2.4.0\r\n+ export PACKAGE_VERSION=2.4.0\r\n+ PACKAGE_VERSION=2.4.0\r\n+ BUILD_DIR=/home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ rm -rf /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ mkdir -p /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ cp -r /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup.py /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ cp /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime\r\n+ echo '__version__ = '\\''2.4.0'\\'''\r\n++ git -C /home/pi/tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe\r\nfatal: not a git repository (or any of the parent directories): .git\r\n+ echo '__git_version__ = '\\'''\\'''\r\n+ cd /home/pi/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3\r\n+ case \"${TENSORFLOW_TARGET}\" in\r\n+ [[ -n '' ]]\r\n+ python3 setup.py bdist bdist_wheel\r\nrunning bdist\r\nrunning bdist_dumb\r\nrunning build\r\nrunning build_py\r\nrunning build_ext\r\nmake: Entering directory '/home/pi/tensorflow'\r\ng++ -O3 -DNDEBUG -DCPU_SETSIZE=__CPU_SETSIZE -fPIC  --std=c++11  -DTFLITE_WITHOUT_XNNPACK -fPIC -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -pthread -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ruy -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/fp16/include -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/include -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/cpuinfo/deps/clog/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/linux_armv6l/obj/tensorflow/lite/kernels/activations.o\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:94,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h: In member function \u2018Packet Eigen::internal::UniformRandomGenerator<T>::packetOp(Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h:213:40: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX T values[packetSize];\r\n                                        ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h: In member function \u2018Packet Eigen::internal::NormalRandomGenerator<T>::packetOp(Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorRandom.h:322:40: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX T values[packetSize];\r\n                                        ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:104,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In member function \u2018Eigen::TensorReductionEvaluatorBase<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::PacketReturnType Eigen::TensorReductionEvaluatorBase<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::packet(Eigen::TensorReductionEvaluatorBase<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:811:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:107,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h:271:54: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];\r\n                                                      ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h: In member function \u2018void Eigen::TensorEvaluator<Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorConcatenationOp<Axis, LhsXprType, RhsXprType>, Device>::Index, const PacketReturnType&)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConcatenation.h:384:54: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];\r\n                                                      ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:108,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function \u2018typename Eigen::internal::enable_if<(Eigen::internal::unpacket_traits<Packet>::size == packet_size), PacketT>::type Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::load(Index, Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:327:44: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX Scalar data[packet_size];\r\n                                            ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function \u2018typename Eigen::internal::enable_if<(Eigen::internal::unpacket_traits<Packet>::size != packet_size), PacketT>::type Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, packet_size, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::load(Index, Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:347:54: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX Scalar data[requested_packet_size];\r\n                                                      ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function \u2018PacketT Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, 1, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::loadPacket(Index, Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:393:34: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX Scalar data[1];\r\n                                  ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h: In member function \u2018PacketT Eigen::internal::BaseTensorContractionMapper<Scalar, Index, side, Tensor, nocontract_t, contract_t, 1, inner_dim_contiguous, inner_dim_reordered, Alignment, MakePointer_>::load(Index, Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorContractionMapper.h:399:34: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX Scalar data[1];\r\n                                  ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:113,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h: In member function \u2018TgtPacket Eigen::PacketConverter<TensorEvaluator, SrcPacket, TgtPacket, 1, TgtCoeffRatio>::packet(Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:160:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::unpacket_traits<TgtPacket>::type values[TgtPacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h: In static member function \u2018static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, ActuallyVectorize, IsSameT>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:260:88: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<TargetType>::type values[PacketSize];\r\n                                                                                        ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h: In static member function \u2018static TargetPacket Eigen::internal::PacketConv<SrcPacket, TargetPacket, LoadMode, false, true>::run(const Eigen::TensorEvaluator<ArgType, Device>&, Eigen::Index)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConversion.h:292:88: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<TargetType>::type values[PacketSize];\r\n                                                                                        ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:114,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConvolution.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorConvolutionOp<Dimensions, InputXprType, KernelXprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorConvolutionOp<Dimensions, InputXprType, KernelXprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorConvolutionOp<Dimensions, InputXprType, KernelXprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorConvolution.h:448:45: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX Scalar data[PacketSize];\r\n                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:116,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorPatchOp<PatchDim, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorPatchOp<PatchDim, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorPatchOp<PatchDim, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPatch.h:250:56: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX CoeffReturnType values[PacketSize];\r\n                                                        ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:117,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorImagePatchOp<Rows, Cols, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorImagePatchOp<Rows, Cols, XprType>, Device>::packetWithPossibleZero(Eigen::TensorEvaluator<const Eigen::TensorImagePatchOp<Rows, Cols, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h:543:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:118,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorVolumePatchOp<Planes, Rows, Cols, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorVolumePatchOp<Planes, Rows, Cols, XprType>, Device>::packetWithPossibleZero(Eigen::TensorEvaluator<const Eigen::TensorVolumePatchOp<Planes, Rows, Cols, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorVolumePatch.h:546:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:119,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetOneByNByOne(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:375:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetOneByN(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:429:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetNByOne(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:447:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetColMajor(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:521:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::packetRowMajor(Eigen::TensorEvaluator<const Eigen::TensorBroadcastingOp<Broadcast, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorBroadcasting.h:579:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:120,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorChippingOp<DimId, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorChippingOp<DimId, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorChippingOp<DimId, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:243:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:263:97: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n         EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                                 ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h: In member function \u2018void Eigen::TensorEvaluator<Eigen::TensorChippingOp<DimId, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorChippingOp<DimId, XprType>, Device>::Index, const PacketReturnType&)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:469:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorChipping.h:489:97: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n         EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                                 ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:121,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorInflationOp<Strides, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorInflationOp<Strides, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorInflationOp<Strides, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorInflation.h:205:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:123,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:628:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[packetSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h: In member function \u2018void Eigen::TensorEvaluator<Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorSlicingOp<StartIndices, Sizes, XprType>, Device>::Index, const PacketReturnType&)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:823:56: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX CoeffReturnType values[packetSize];\r\n                                                        ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:124,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorPaddingOp<PaddingDimensions, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorPaddingOp<PaddingDimensions, XprType>, Device>::packetWithPossibleZero(Eigen::TensorEvaluator<const Eigen::TensorPaddingOp<PaddingDimensions, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorPadding.h:683:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:125,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h:238:78: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n                                                             values[PacketSize];\r\n                                                                              ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h: In member function \u2018void Eigen::TensorEvaluator<Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorReverseOp<ReverseDimensions, XprType>, Device>::Index, const PacketReturnType&)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReverse.h:470:54: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX CoeffReturnType values[PacketSize];\r\n                                                      ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:126,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h: In static member function \u2018static Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketLoader<LoadMode, Self, ImplPacketAccess>::Run(const Self&, Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::Index)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h:211:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h: In static member function \u2018static Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::PacketLoader<LoadMode, Self, true>::Run(const Self&, Eigen::TensorEvaluator<const Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::Index)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h:228:97: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n         EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                                 ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h: In member function \u2018void Eigen::TensorEvaluator<Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorShufflingOp<Shuffle, XprType>, Device>::Index, const PacketReturnType&)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorShuffling.h:416:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:127,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorStridingOp<Strides, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorStridingOp<Strides, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorStridingOp<Strides, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h:212:95: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n                                                                                               ^\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h: In member function \u2018void Eigen::TensorEvaluator<Eigen::TensorStridingOp<Strides, XprType>, Device>::writePacket(Eigen::TensorEvaluator<Eigen::TensorStridingOp<Strides, XprType>, Device>::Index, const PacketReturnType&)\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorStriding.h:347:47: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n       EIGEN_ALIGN_MAX Scalar values[PacketSize];\r\n                                               ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:131,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorGeneratorOp<Generator, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorGenerator.h:159:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[packetSize];\r\n                                                                                             ^\r\nIn file included from /home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/Tensor:134,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:40,\r\n                 from tensorflow/lite/kernels/activations.cc:29:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h: In member function \u2018Eigen::TensorEvaluator<const Eigen::TensorTraceOp<Dims, XprType>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorTraceOp<Dims, XprType>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorTraceOp<Dims, XprType>, Device>::Index) const\u2019:\r\n/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorTrace.h:250:93: warning: requested alignment 16 is larger than 8 [-Wattributes]\r\n     EIGEN_ALIGN_MAX typename internal::remove_const<CoeffReturnType>::type values[PacketSize];\r\n```\r\n", "comments": ["RPI zero has 512MB ram. It might not be sufficient to build it natively.\r\nYou'd better use ARM cross compilation with Docker.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_pip", "I figured as much. The continuous flashing green LED is probably telling me it's out of memory. No wonder it takes so long to recover from that crash after killing the process.\r\n\r\nI was already trying to make a Docker-deployed model work on the Pi Zero by recompiling the wheel. I'll work on that in the next few days and post an update to the thread. ", "Compilation of the pip package to actually compile the wheel in fails on my machine. Failed on a fresh install of Ubuntu 20.04, and Mac OS, returning the same errors:\r\n```\r\nIn file included from /home/user/tensorflow_src/tensorflow/lite/python/interpreter_wrapper/numpy.cc:17:\r\n/home/user/tensorflow_src/tensorflow/lite/python/interpreter_wrapper/numpy.h:51:10: fatal error: numpy/arrayobject.h: No such file or directory\r\n   51 | #include \"numpy/arrayobject.h\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[3]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/build.make:89: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/numpy.cc.o] Error 1\r\nmake[3]: *** Waiting for unfinished jobs....\r\nIn file included from /home/user/tensorflow_src/tensorflow/lite/python/interpreter_wrapper/interpreter_wrapper.cc:30:\r\n/home/user/tensorflow_src/tensorflow/lite/python/interpreter_wrapper/numpy.h:51:10: fatal error: numpy/arrayobject.h: No such file or directory\r\n   51 | #include \"numpy/arrayobject.h\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nmake[3]: *** [CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/build.make:63: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/python/interpreter_wrapper/interpreter_wrapper.cc.o] Error 1\r\nmake[3]: Leaving directory '/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'\r\nmake[2]: *** [CMakeFiles/Makefile2:1091: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/all] Error 2\r\nmake[2]: Leaving directory '/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'\r\nmake[1]: *** [CMakeFiles/Makefile2:1098: CMakeFiles/_pywrap_tensorflow_interpreter_wrapper.dir/rule] Error 2\r\nmake[1]: Leaving directory '/home/user/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'\r\nmake: *** [Makefile:186: _pywrap_tensorflow_interpreter_wrapper] Error 2\r\n```\r\nI was sure to `pip3 apt install numpy` and the same error occured. I also checked the solution given [here](https://stackoverflow.com/questions/44888925/fatal-error-numpy-arrayobject-h-no-such-file-or-directory/44935933) for the arrayobject.h file not being found and got the same result. ", "Are you using tensorflow/tools/ci_build/ci_build.sh ? It's recommended for cross compilation to avoid such a dependency issue. ", "I'm not, but I can try it instead of trying to build the pip environment. I followed the instructions precisely here: https://www.tensorflow.org/lite/guide/build_cmake_pip", "The page says you should use tensorflow/tools/ci_build/ci_build.sh for cross compilation.\r\nAre you building natively on RPI0? You'd better not.", "My bad, I think something was miscommunicated here. I _am_ building using the ci_build.sh. However, ci_build.sh fails I'm guessing because the pip environment build fails with the missing numpy header error I followed up with.", "What's the command you used?\r\nI've just tested with the follow command and it works well for me.\r\n\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh rpi0\r\n```", "I did use that, but it failed in the end and broke something, since it didn't run again. \nHere's what I've done:\n```\nsudo apt-get install cmake\ngit clone https://github.com/tensorflow/tensorflow.git tensorflow_src\nPYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh native #this fails\n```\nThe build command I used was:\n```\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\n  tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh rpi0 \n```\n\nI will try running the full process in Ubuntu including the wheel build again, despite any errors that come up in building the pip package and see what happens.\n", "Ran a fresh install of Ubuntu, installed pybind11, installed Docker using the instructions [here](https://docs.docker.com/engine/install/ubuntu/), and ran \r\n```\r\nsudo tensorflow/tools/ci_build/ci_build.sh PI-PYTHON38 \\\r\n  tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh rpi0 \r\n```\r\nThis worked fine and reached the `exit 0` line. \r\nI'm not exactly sure what I did differently. Weird. \r\nI know some of the errors I was getting were coming from not having used sudo before running the commands, which is beyond me since it's in `~/` but I'm not sure about the other ones. It's possible that when I tried building on my Mac, there were conflicts in pip linking with python, given I installed Python using Anaconda. Building on a fresh install of Ubuntu (which came preloaded with python3) and installing dependencies as needed was the most helpful. Closing as the wheel compiled fine now. Thanks for your help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47229\">No</a>\n", "Reopening as there was an issue with installing the wheel. The wheel built successfully, but doesn't want to install on my RPi. I built the wheel for armv6l and Python3.8, both of which are what's on my Pi. \r\n```\r\npip3 install ~/tflite_runtime-2.5.0-cp38-cp38-linux_armv6l.whl\r\nDefaulting to user installation because normal site-packages is not writeable\r\nLooking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple\r\nERROR: tflite_runtime-2.5.0-cp38-cp38-linux_armv6l.whl is not a supported wheel on this platform.\r\n```\r\nI have no clue what the issue is now. ", "Are you trying to install it to RPI zero with Python 3.8?\r\n\r\nYou need to check\r\n1. Python interpreter version (python3 --version)\r\n2. Target CPU & ABI https://www.tensorflow.org/lite/guide/build_cmake_arm#check_your_target_environment", "I did install Python 3.8.3 on my Pi Zero in the middle of this thread. Even when building for Python 3.7, I still had issues. I've given up on trying to cross-compile my application for a Pi Zero, and ordered a set of Pi v4s instead which should alleviate the issue. I'll open a new thread and post a comment linking to it here should I have any issues building TFLite.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47229\">No</a>\n"]}, {"number": 47228, "title": "Update bazel command to `bazel run` instead of `bazel test`.", "body": "`bazel test` puts the error logs into a file instead of on the terminal, whereas `bazel run` will show all the logs on the terminal, which is nicer when debugging locally.\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47227, "title": "Documentation for how to add new optimized kernels to TFLM.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47226, "title": "[Grappler] Add support for QuantizeAndDequantizeV4", "body": "52df91c5634e6c666843849a1c6ff29b3d2676be added `QuantizeAndDequantizeV4` which is used in `tf.quantization.quantize_and_dequantize_v2` and deprecated `QuantizeAndDequantizeV2`.\r\n\r\nThis PR adds  `QuantizeAndDequantizeV4` support to grappler which can be a simple alias to `QuantizeAndDequantizeV2` since they share the same forward kernel implementation:\r\nhttps://github.com/tensorflow/tensorflow/blob/e43be76009614be88454d2fdf2fe702acc5bab77/tensorflow/core/kernels/quantize_and_dequantize_op.cc#L419-L422\r\n\r\nThese changes should be covered by the existing unittests.", "comments": ["@andyly Thanks for the fast review. Do you mind also taking a look at #47225?"]}, {"number": 47225, "title": "[XLA] Support QuantizeAndDequantizeV4", "body": "@pkanwar23 added  `QuantizeAndDequantizeV4` in 52df91c5634e6c666843849a1c6ff29b3d2676be which is used in `tf.quantization.quantize_and_dequantize_v2` and deprecated `tf.quantization.quantize_and_dequantize` which relied on `QuantizeAndDequantizeV2`.\r\n\r\nThis PR adds  `QuantizeAndDequantizeV4` support for XLA which can be a simple alias to the `QuantizeAndDequantizeV2` kernel implementation since `QuantizeAndDequantizeV4` only changed the gradient and kept the same forward pass:\r\nhttps://github.com/tensorflow/tensorflow/blob/e43be76009614be88454d2fdf2fe702acc5bab77/tensorflow/core/kernels/quantize_and_dequantize_op.cc#L419-L422\r\n\r\nTo make this work with the MLIR bridge, the op also needed to be added to `tf_generated_ops.td`. This file is autogenerated, but unfortunately I couldn't find any documentation on how this is done so I manually added the op for now.\r\n\r\nOnce this is merged, I am also happy to add support for this in the MLIR TFLite converter which should be straight forward as well.", "comments": ["@joker-eph @gbaned Any updates on this PR? It would be cool if this could get reviewed before the 2.5 branch get's cut.", "Any updates on this? Would be great to still get this into the 2.5 branch cut.", "@gbaned any updates on this PR?", "> @gbaned any updates on this PR?\r\n\r\n@lgeiger This PR is waiting for the reviewer approval. Thanks!", "> Thanks for the contributions!\r\n\r\n@smit-hinsu Thanks a lot for the review!\r\n\r\nDo you mind also reviewing followup PR that I just pushed: #48410"]}, {"number": 47221, "title": "Update avgpooling_op.cc for GPU kernel", "body": "This pull request fixes the similar issue as in pr [#46838](https://github.com/tensorflow/tensorflow/pull/46838) but for the GPU kernel.", "comments": []}, {"number": 47220, "title": "Does TensorFlow do some optimization (like merge a conv and BN into one conv) when converting to TFLite? If yes, is there any other layers merge optimization Tensorflow did?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWhat kind of layers merge does Tensorflow do when converting to TFLite? (Like merge consecutive conv and BN into a new conv). I am working on some optimization part of TFLite and If Tensorflow already did some layers merge optimization part then I do not need to redo this part.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nAnyone who would like to do optimizations to TFLite\r\n**Any Other info.**\r\n", "comments": ["In the high level of the view point, if your model is applied to the optimization and serialized with the optimized format, the TFLite model will have the same structure.\r\n\r\nDuring TF -> TFL conversion, we have a sort of optimizations like dilated conversion and integrated conv + activation patterns. However, there is no optimization rule for merging conv + batchnom patterns yet.", "> In the high level of the view point, if your model is applied to the optimization and serialized with the optimized format, the TFLite model will have the same structure.\r\n> \r\n> During TF -> TFL conversion, we have a sort of optimizations like dilated conversion and integrated conv + activation patterns. However, there is no optimization rule for merging conv + batchnom patterns yet.\r\n\r\nCould you provide a list of what kind of optimizations Tensorflow already done? Therefore I could skip repeatable work.", "@jwj04ok I am not familiar with grappler or TF optimizations. Please refer to https://www.tensorflow.org/guide/graph_optimization", "I think it's folded. (fused) batch norm is decomposed to mul, add, div and some sort of operators in `tensorflow/compiler/mlir/lite/transforms/prepare_tf.cc`, and then those ops are folded into conv in `tensorflow/compiler/mlir/lite/transforms/optimize.cc`. You can convert and visualize the model\r\n\r\n```python\r\nmodel = tf.keras.Sequential([\r\n  tf.keras.layers.Conv2D(16, 3, batch_input_shape=(1, 128, 128, 3)),\r\n  tf.keras.layers.BatchNormalization()\r\n])\r\n```\r\n\r\nand check if it's folded."]}, {"number": 47219, "title": "Add reminder on MacOS Catalina", "body": "Same as   #47153 which is applicable to another location of different applications (micro_speech) using same board (STM32 DISCO_F746NG).\r\n\r\nARM mbed MacOS installer (mbed-cli-v0.0.10.dmg from https://github.com/ARMmbed/mbed-cli-osx-installer/releases/tag/v0.0.10) creates error during installation.\r\n\r\nIt is caused by MacOS Catalina moving path of terminal app from /Applications/Utilities/Terminal.app to /System/Applications/Utilities/Terminal.app. ) ARM mbed installer scripts still use /Applications/Utilities/Terminal.app.\r\n\r\nSo add a reminder, and a link to a solution from ARM mbed github.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 47218, "title": "I have problem with python code in google colab", "body": "I write this code:\r\nbasemodel.fit(X_train,y_train,epochs=20,validation_split=.1,callbacks=call_back)\r\nbut don't work this error:\r\nEpoch 1/20\r\nValueError Traceback (most recent call last)\r\nin ()\r\n----> 1 basemodel.fit(X_train,y_train,epochs=20,validation_split=0.1,callbacks=call_back)\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n975 except Exception as e: # pylint:disable=broad-except\r\n976 if hasattr(e, \"ag_error_metadata\"):\r\n--> 977 raise e.ag_error_metadata.to_exception(e)\r\n978 else:\r\n979 raise", "comments": ["@walaamm,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. \r\n\r\nAlternatively, you can also share the Colab notebook you are running, with us. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47218\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47218\">No</a>\n"]}, {"number": 47217, "title": "DSP HiFi4 support (delegate) for TF-Lite", "body": "I wonder if there is any support (experimental) for HiFi4 DSP of Cadance ([Xtensa HiFi4 Audio DSP](https://ip.cadence.com/uploads/928/TIP_PB_HiFi_DSP_FINAL-pdf)), and possibly check there is an execution mechanism, probably delegate executor for this DSP.  \r\nSo question or request is: Is there any support available or would be possible to add HiFi4 delegate or another mechanism to leverage from HiF4 DSP?", "comments": ["Hi @peter197321, can you say a little more about what platform/OS you're using for deployment? And what the size/type of your model is? Thanks.", "Hello Jared,\r\n\r\nMy platform/OS is Linux.\r\nI'd deploy the models for audio recognition, and to have the sound classification models.\r\n\r\nThank you.\r\n\r\nB.T.W. It seems the HiFi4 is supported with https://github.com/foss-xtensa/nnlib-hifi4 but it's targeted for TFLM.\r\n\r\nOn Tue, Feb 23, 2021 at 6:25 PM Jared Duke <notifications@github.com> wrote:\r\n\r\n> Hi @peter197321 <https://github.com/peter197321>, can you say a little\r\n> more about what platform/OS you're using for deployment? And what the\r\n> size/type of your model is? Thanks.\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/47217#issuecomment-784368170>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ALGLVQSXVFYXEDWDLGU25Y3TAPQJJANCNFSM4XYQT5KA>\r\n> .\r\n>\r\n", "Is there a reason you cannot use TFLM?\r\n\r\nOn the TFLite side, there are no plans to build such a delegate, though you could potentially leverage some of the optimized kernels that have already been written for TFLM. It really depends on whether you expect graph execution to run entirely on the DSP, or if you want the host CPU in the loop.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47216, "title": "TypeError if set the weights to the current weights via `set_weights`", "body": "\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 20.04 `Linux XXX 5.8.0-43-generic #49~20.04.1-Ubuntu SMP Fri Feb 5 09:57:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux`\r\n- TensorFlow installed from (source or binary): `pip install tensorflow`\r\n- TensorFlow version (use command below): `v2.4.0-49-g85c8b2a817f 2.4.1`\r\n- Python version: `3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0]`\r\n\r\n**Describe the current behavior**\r\nI get the error `TypeError: __array__() takes 1 positional argument but 2 were given`. Hence it might be related to bug #46840. \r\n\r\n\r\n**Describe the expected behavior**\r\nShould just set the weight to what they are right now.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\ndenseLayer = tf.keras.layers.Dense(1,activation=\"relu\")\r\ndenseLayer.build(input_shape=(4))\r\n\r\ndenseLayer.set_weights(denseLayer.weights)\r\n```\r\nLink: [Colab](https://colab.research.google.com/drive/1bwHmMvktEsLzOK-x8fsmIJBiNJGfsM9F?usp=sharing)\r\n\r\n**Other info / logs** \r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-21-99e968378f7a> in <module>\r\n      4 denseLayer.build(input_shape=(4))\r\n      5 \r\n----> 6 denseLayer.set_weights(denseLayer.weights)\r\n\r\n~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in set_weights(self, weights)\r\n   1875         weight_index += 1\r\n   1876 \r\n-> 1877     backend.batch_set_value(weight_value_tuples)\r\n   1878 \r\n   1879   def get_weights(self):\r\n\r\n~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/tensorflow/python/keras/backend.py in batch_set_value(tuples)\r\n   3704   if ops.executing_eagerly_outside_functions():\r\n   3705     for x, value in tuples:\r\n-> 3706       x.assign(np.asarray(value, dtype=dtype(x)))\r\n   3707   else:\r\n   3708     with get_graph().as_default():\r\n\r\n~/Projects/NotebooksEnvs/py38Env/lib/python3.8/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)\r\n     81 \r\n     82     \"\"\"\r\n---> 83     return array(a, dtype, copy=False, order=order)\r\n     84 \r\n     85 \r\n\r\nTypeError: __array__() takes 1 positional argument but 2 were given\r\n```\r\n", "comments": ["It turns out, that it works with `get_weights()` but still, this should yield a reasonable error message.", "Was able to reproduce the issue with  TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/ravikyram/be8bceb82372692740dc74da02749466/untitled674.ipynb). Thanks!", "Hi, I want to work on creating a more suitable error message for this scenario. I was thinking something like\r\nTypeError: set_weights() expects a list of  expected_num_weights weights in the form of arrays, but instead received  list of tf.variable        ", "@shivaditya-meduri,\r\nWith [respect to this comment in the PR](https://github.com/tensorflow/tensorflow/pull/47489#issuecomment-833838923) can you please let us know if you are working on the fix? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47216\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47216\">No</a>\n"]}, {"number": 47215, "title": "TF-TRT Improve matrix multiplication conversion and enable dynamic shape mode", "body": "This PR improves the MatMul and BatchMatMul converters.\r\n\r\n- Unnecessary transpose of weights are removed. Transposing weights for `IMatrixMultiplyLayer` is not necessary, because `IMatrixMultiplyLayer` can directly pass the transpose flags to the underlaying GEMM call, which can use it to access elements with the correct stride without any actual transposition. \r\n- Restrictions caused by previous weight transpose ops eliminated.\r\n- Enabled explicit batch and dynamic shape input.\r\n- `IFullyConnectedLayer` (FC) usage fixed:\r\n  - FC layer is preferred over `IMatrixMultiply` because it is expected to give better performance. Moreover, currently only FC layer supprorts INT8 precision.\r\n  - Fixed and relaxed FC layer conversion condition.\r\n  - Fixed input tensor_a shaped handling. In dynamic shape mode care has to be taken to retain static dim where available and not to confuse unknown dims with -1 wildcard.\r\n  - Enabled rank > 2 for weights.\r\n  - Fixed conversion of `BatchMatMul` to FC: broadcast now preserves the information whether the input is tensor or weight, so that we can correctly check FC compatibility condition.\r\n\r\n\r\n`BatchMatMul` involves a potential broadcast step. TRT requires that the input tensors have the same rank, with 1 values filled in the dimensions which need to be broadcasted. A helper function `BroadcastTensors` was added to make the tensors match in rank. In dynamic shape mode we need shape inference for this step. The `DynamicReshape` function was modified to allow insertion of multiple singleton dimensions.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.\r\nTracker: #45481", "comments": ["I have fixed the missing comment."]}, {"number": 47214, "title": "ValueError: No gradients provided for any variable: ['conv1_conv/kernel:0', 'conv1_conv/bias:0', 'conv2_block1_preact_bn/gamma:0', 'conv2_block1_preact_bn/beta:0', 'conv2_block1_1_conv/kernel:0',", "body": "I cannot understand how I can solve this issue.  I would be very grateful for help!  \r\n\r\nhttps://colab.research.google.com/drive/1ZFrfT7BPV7S75WD0082VTswNkdYqYSWf?usp=sharing", "comments": ["@kitenko,\r\nOn running the code, the `class_names` variable prints an empty list. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f8164d9bf4e5f22fab27a90649c248ec/47214.ipynb).\r\n\r\nCould you please provide the dataset and all the necessary files required to run the code, so that we can reproduce the issue on our end. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47214\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47214\">No</a>\n"]}, {"number": 47212, "title": "tensorflow.python.framework.errors_impl.NotFoundError: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringB5cxx11EPNS_15OpKernelContextEb", "body": "I encounter a bug that occurs when I try to import tensorflow in a conda environment (python 3.8). All my attempts so far, to even revert to a previous version or recreate the environment from scratch with different version of tensorflow, failed so far.\r\nThe issue is similar to https://github.com/tensorflow/tensorflow/issues/42084 but I didn't manage to get a fix out of it, even some others have the same issue (and the end was \"It's better to open a new issue and post the full details.\"), so here I am!\r\n\r\n# About the system\r\nA Kubuntu (Linux) computer, version 20.04. The installation step are close to https://www.tensorflow.org/install/pip#ubuntu-macos, adapted to the current version of Kubuntu for the nvidia dependencies.\r\nPilots Nvidia Updated\r\nCUDA/cuDNN version: 11.00\r\nTF installed from conda (Binary)\r\nTF version 2.2.0 or 2.3.0\r\n\r\n# Step to reproduce\r\nLaunch python 3.8\r\n```\r\n>>> import tensorflow\r\n2021-02-17 10:35:57.687845: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/me/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 436, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"/home/me/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py\", line 153, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/me/.conda/envs/CasComplexe/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringB5cxx11EPNS_15OpKernelContextEb\r\n```\r\n\r\n", "comments": ["@dmidge8 \r\n\r\nPlease, see tested build configurations from [here](https://www.tensorflow.org/install/source#gpu).Can you try with CUDA 10.1 and see if the issue still persists. Thanks!", "Hi @ravikyram ,\r\nThank you for your answer. I had CUDA 10.1 and it was not working. It was complaining that it couldn't find libcudart.so.11.0 (which is normal since it CUDA 10.1 was installed, but I have no idea why it was looking for CUDA 11.0). Anyway, fast forward, I installed the correct version of CUDA and the drivers following the Tensorflow tutorial. And the warning on libcudart disappeared.\r\n\r\nPrior to that (around one month ago), my environment was working, but for some reason, it broke. I remember that I played to have tensorboard working in the environment, but I can't remember what I did that could have broke tensorflow, thus I started over with a clean environment and can't get it to work since then, with various version of tensorflow. I tried to clean the conda cache, but it is not helping. I guess that it is my upper environment that has some mismatched *.so that for some reason, the tensorflow in conda is looking for.\r\n\r\nAnyway, the crash happen even when I don't want to use CUDA (I am just importing TF), thus I don't even have the CPU environment working.", "@dmidge8 Generally we don't support Anaconda built TF packages. TF built 2.2 and 2.3 requires CUDA10.1. However, we don't have any idea about Conda 2.3 package. \r\n\r\nIs it possible to install `TF2.4` or `tf-nightly` using `pip`? As you already have `CUDA11`, installing TF using `pip` should work. Also, can you please try different python version such as `3.6` or `3.7`? I asking this because I notice others also facing similar issue where they have `TF2.3` and `python3.8`. Thanks!", "Hi @jvishnuvardhan ,\r\nThanks for your interest in the issue!\r\nI tried to install it with pip in the conda environment, and it didn't solve anything. Even with various versions. The odd part though is that installing it through pip *outside* of the conda environment did solve the problem *inside* the conda environment (I had to go through another loop of installation though in the conda environment, with the conda pip installation). It is odd... It is kink of if the conda package tries to reach for a package that is outside of the environment and can't see it if it is only present inside the environment....\r\nHowever, I didn't try with `tf-nightly`...", "@dmidge8 Please try with `tf-nighly` also and close the issue if it was resolved for you. Generally we don't support Anaconda based issues. If you can post them in their repo, you might get faster resolutions related those issue. Thanks!", "I am closing this issue as this is more related Anaconda repository than TensorFlow. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47212\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47212\">No</a>\n"]}, {"number": 47211, "title": "Support for tf.where without x and y arguments in XLA", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 10 / Linux Ubuntu 20.04 / Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: conda 3.8.5\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: RTX 2080 Super\r\n\r\n**Describe the current behavior**\r\nThe following error is raised, either the code is run on CPU or GPU:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 895, in _call\r\n    filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\r\n  File \"/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 560, in call\r\n    ctx=ctx)\r\n  File \"/opt/anaconda3/envs/jplu-transformers/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_where_7}} = __inference_where_7[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003CPU\\020\\001\\n\\007\\n\\003GPU\\020\\0012\\005*\\0010J\\0008\\001\\202\\001\\000\", executor_type=\"\"](dummy_input).\r\nUncompilable nodes:\r\nWhere: unsupported op: No registered 'Where' OpKernel for XLA_GPU_JIT devices compatible with node {{node Where}}\r\n        Stacktrace:\r\n                Node: __inference_where_7, function:\r\n                Node: Where, function: __inference_where_7\r\n [Op:__inference_where_7]\r\n```\r\n\r\n**Describe the expected behavior**\r\nI'm expecting to have the Where op compliant with XLA on GPU and CPU as stated by @blakehechtman in https://github.com/tensorflow/tpu/issues/584 and get the following result:\r\n```python\r\n<tf.Tensor: shape=(2, 1), dtype=int64, numpy=\r\narray([[0],\r\n       [1]], dtype=int64)>\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nHere a piece of code to reproduce the issue:\r\n```python\r\nimport tensorflow as tf\r\n\r\nmask = tf.constant([True, True, False])\r\n\r\n@tf.function(experimental_compile=True)\r\ndef test():\r\n  return tf.where(mask)\r\n\r\ntest()\r\n```\r\n\r\nThe context is that I need to have the indices of the `True` values in a mask in order to be able to use the `tf.gather_nd`, `tf.scatter_nd` and `tf.tensor_scatter_nd_update` methods.", "comments": ["Was able to reproduce the issue with TF v2.3, TF v2.4 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4088eeedda640a3629f8223af5f97f5f/47211.ipynb). Thanks!", "This is supported on XLA-TPU but not on XLA-GPU yet. ", "Oh, I see. Thanks for the answer! Any ETA for it to come on XLA-GPU (CPU)? Can we expect to have it provided with TF 2.5?", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210603, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/720f64c46754ecec93f9695797b559d5/47211.ipynb). Thanks!", "@jplu Could you please have a look at the colab  [gist](https://colab.research.google.com/gist/sushreebarsa/2a8ef5dafb0e1f1d9e61da0e3855bd31/untitled68.ipynb) here ?Please let us know if  this issue is resolved for you.Thanks!", "I do confirm, this is properly fixed in 2.8.0. Thanks a lot!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47211\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47211\">No</a>\n"]}, {"number": 47210, "title": "Use of tensorflow_addons instead of tf.contrib", "body": "I am using tensorflow2. As we know tensorflow2 is not supporting tf.contrib and tf.contrib is move to tensorflow_addons.\r\nBut I don't know how to use tesorflow_addons instead of tf.contrib\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\nslim_example_decoder = tf.contrib.slim.tfexample_decoder\r\n```\r\n\r\nI want to write `slim_example_decoder = tf.contrib.slim.tfexample_decoder` using **tesorflow_addons**.\r\ncan anyone have any idea?", "comments": ["This question belongs to Stack Overflow. it is not a bug report in the TF library code nor is it a feature request."]}, {"number": 47209, "title": "[XLA] Add Nx to XLA documentation frontends", "body": "Hi everyone, for the last 3 months we've been working on a numerical computing library for the Elixir programming language. We support pluggable compilers, but we started by writing a compiler using XLA first. Nx supports JIT compilation using XLA to CPU and GPU right now. We hope to add TPU support in the future. We we're hoping we could get Nx added to the list of supported XLA frontends. Thank you!", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47209) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 47208, "title": "Replacing activation in a pretrained network", "body": "I'm trying to replace swish activation with relu activation in pretrained TF model EfficientNetB0. EfficientNetB0 uses swish activation in Conv2D and Activation layers. I could not find any relevant resources on how to do this. Is there any TF API to do this? Below is the code that I'm working on to replace swish activations with relu activations. Any pointers/help on replacing the activation is much appreciated.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import ReLU\r\n\r\ndef replace_swish_with_relu(model):\r\n    '''\r\n    Modify passed model by replacing swish activation with relu\r\n    '''\r\n    for layer in tuple(model.layers):\r\n        layer_type = type(layer).__name__\r\n        if hasattr(layer, 'activation') and layer.activation.__name__ == 'swish':\r\n            print(layer_type, layer.activation.__name__)\r\n            if layer_type == \"Conv2D\":\r\n                # conv layer with swish activation.\r\n                # Do something\r\n                layer.activation = ReLU() # This didn't work\r\n            else:\r\n                # activation layer\r\n                # Do something\r\n                layer = tf.keras.layers.Activation('relu', name=layer.name + \"_relu\") # This didn't work\r\n    return model\r\n\r\n# load pretrained efficientNet\r\nmodel = tf.keras.applications.EfficientNetB0(\r\n    include_top=True, weights='imagenet', input_tensor=None,\r\n    input_shape=(224, 224, 3), pooling=None, classes=1000,\r\n    classifier_activation='softmax')\r\n\r\n# convert swish activation to relu activation\r\nmodel = replace_swish_with_relu(model)\r\nmodel.save(\"efficientNet-relu\")\r\n```", "comments": ["@mrtpk123,\r\nCan you please let us know why are you trying to do so? Also, if you change the Activation Function, will the purpose of Transfer Learning be fulfilled, i.e..I don't think we will be able to Freeze the Layers or reuse the Weights. Thanks!", "Hi @rmothukuru, Thank you for the reply. \r\n\r\nActually, I'm in search of an EfficientNet pretrained model with ReLU activations. My plan is to retrain the existing model after replacing the swish activations. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47208\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47208\">No</a>\n"]}, {"number": 47207, "title": "TFLite converter produces wrong output shape_signature if RNN / LSTM output layer in model (becomes all unknown dimensions)", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly==2.5.0-dev20210216 and tensorflow>=2.4.0\r\n\r\n### 2. Code\r\n\r\n- [Collab notebook](https://colab.research.google.com/drive/1csQPhZpdidop2OS3JOGEGtCTU1JWwiWU?usp=sharing)\r\n\r\n- Code:\r\n```py\r\nimport tensorflow as tf\r\nfrom pprint import pprint\r\n\r\ndef convertModelToTFLite(path, outPath):\r\n\r\n    print(f\"[INFO] Using tensorflow v{tf.__version__}\")\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(path)\r\n    converter.experimental_new_converter = True\r\n    tfliteModel = converter.convert()\r\n\r\n    # Save the model\r\n    with open(outPath, 'wb') as f:\r\n        f.write(tfliteModel)\r\n\r\nunits = 256\r\nsavePath = \"my_model\"\r\n\r\n# creating model\r\ni = tf.keras.Input(shape=(1, 521), name='input')\r\nx = tf.keras.layers.Dense(units)(i)\r\nx = tf.keras.layers.LSTM(units, return_sequences=True)(x)\r\n# x = tf.keras.layers.Dense(units)(x)  # <-------- Having a dense output layer gives correct  output signature \r\n\r\n# saving as SavedModel\r\nmodel = tf.keras.models.Model(inputs=[i], outputs=[x])\r\nmodel.save(savePath, save_format='tf')\r\n\r\nconvertModelToTFLite(savePath, f\"{savePath}.tflite\", args.optimize)\r\nip = tf.lite.Interpreter(f\"{savePath}.tflite\")\r\n\r\nprint(model.summary())\r\npprint(ip.get_output_details())\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\n- Conversion **is** successful and model **works** if I resize the output tensor to expected dimensions and reallocate tensors\r\n- **But** I expect the shape signature of the output to be correct, and have the last dimension (feature dimension) be known and equal to the number of units / cells in the LSTM.\r\n\r\n- With an LSTM or RNN layer as the output layer to a `keras.Model`, the TFLite model has the unexpected output shape signature :\r\n```\r\n  'shape_signature': array([-1, -1, -1], dtype=int32),      <---------- All -1s\r\n``` \r\n\r\n- However with a Dense layer, the output has the expected shape signature:\r\n```\r\n  'shape_signature': array([ -1,  -1, 256], dtype=int32),\r\n```\r\n\r\n\r\n### 5. (optional) Any other info / logs\r\n- Output details and model summary **with a RNN / LSTM output layer**\r\n\r\n```\r\n[{'dtype': <class 'numpy.float32'>,\r\n  'index': 42,\r\n  'name': 'StatefulPartitionedCall:0',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n                              'scales': array([], dtype=float32),\r\n                              'zero_points': array([], dtype=int32)},\r\n  'shape': array([1, 1, 1], dtype=int32),\r\n  'shape_signature': array([-1, -1, -1], dtype=int32),      <---------- All -1s\r\n  'sparsity_parameters': {}}]\r\n\r\n___________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput (InputLayer)           [(None, 1, 521)]          0         \r\n_________________________________________________________________\r\ndense_3 (Dense)              (None, 1, 256)            133632    \r\n_________________________________________________________________\r\nlstm_3 (LSTM)                (None, 1, 256)            525312    \r\n=================================================================\r\n```\r\n\r\n- Output details and model summary **with Dense output layer**\r\n\r\n```\r\n[{'dtype': <class 'numpy.float32'>,\r\n  'index': 51,\r\n  'name': 'StatefulPartitionedCall:0',\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'quantized_dimension': 0,\r\n                              'scales': array([], dtype=float32),\r\n                              'zero_points': array([], dtype=int32)},\r\n  'shape': array([  1,   1, 256], dtype=int32),\r\n  'shape_signature': array([ -1,  -1, 256], dtype=int32),\r\n  'sparsity_parameters': {}}]\r\n\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\n\r\ninput (InputLayer)           [(None, 1, 521)]          0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 1, 256)            133632\r\n_________________________________________________________________\r\nlstm (LSTM)                  (None, 1, 256)            525312\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1, 256)            65792\r\n=================================================================\r\n```", "comments": ["Was able to reproduce the issue with [TF v2.4](https://colab.research.google.com/gist/amahendrakar/2d7fea304b1ddf2acef0a27816c9d40c/47207.ipynb) and TF-nightly.\r\n\r\nWhereas with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a02ee07350673687a1e976aa70e8c285/47207.ipynb), I am facing an error while converting the model to `tflite` format. Please check the linked gist for reference. Thanks!", "Seems fixing the input size, as described in this [colab](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb) helps.\r\n\r\n```python\r\nrun_model = tf.function(lambda x: model(x))\r\n# This is important, let's fix the input size.\r\nBATCH_SIZE = 1\r\nINPUT_SIZE = 521\r\nconcrete_func = run_model.get_concrete_function(\r\n    tf.TensorSpec([BATCH_SIZE, 1, INPUT_SIZE], model.inputs[0].dtype))\r\nmodel.save(savePath, save_format='tf', signatures=concrete_func)\r\n```", "> Seems fixing the input size, as described in this [colab](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/experimental_new_converter/Keras_LSTM_fusion_Codelab.ipynb) helps.\r\n> \r\n> ```python\r\n> run_model = tf.function(lambda x: model(x))\r\n> # This is important, let's fix the input size.\r\n> BATCH_SIZE = 1\r\n> INPUT_SIZE = 521\r\n> concrete_func = run_model.get_concrete_function(\r\n>     tf.TensorSpec([BATCH_SIZE, 1, INPUT_SIZE], model.inputs[0].dtype))\r\n> model.save(savePath, save_format='tf', signatures=concrete_func)\r\n> ```\r\n\r\nYeah, if I define all dimensions of the input shape, it works. But if I leave either the batch size or timestep size as `None`, the problem returns. \r\n\r\nAlso I realized, at least, after the first call to `interpreter.invoke()`, the output dimensions become expected values, but the signature dims remain (-1, -1, -1).", "Fixed in [Tensorflow v2.5.0](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0), thanks !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47207\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47207\">No</a>\n"]}, {"number": 47206, "title": "Prevent BackupAndRestore cleanup from throwing if dir doesn't exist", "body": "If `model.fit()` is called with `epochs=0` (which is allowed by Keras) [`BackupAndRestore.on_train_end`](https://github.com/tensorflow/tensorflow/blob/9663abe4c9037030b0b497c68cc4b2ba991967dd/tensorflow/python/keras/callbacks.py#L1648-L1656) would throw due to the fact that the checkpoint directory doesn't exist since [`on_epoch_end`](https://github.com/tensorflow/tensorflow/blob/9663abe4c9037030b0b497c68cc4b2ba991967dd/tensorflow/python/keras/callbacks.py#L1658-L1660) never get's called. This PR makes sure that `BackupAndRestore` doesn't throw in cases where no cleanup is necessary.", "comments": []}, {"number": 47205, "title": "\"No matching distribution found for tensorflow\" when installing with pip on macOS 11", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11.2.1, x86-64\r\n- TensorFlow installed from (source or binary): `pip3 install tensorflow`\r\n- TensorFlow version: latest\r\n- Python version: 3.8\r\n- Installed using: pip3\r\n\r\n**Describe the problem**\r\n\r\nInstallation with `pip` fails with:\r\n\r\n```shell\r\n$ pip3 install tensorflow\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```bash\r\n# TensorFlow apparently does not yet support 3.9, so fall back to 3.8\r\nbrew install python@3.8\r\nbrew link --overwrite python@3.8\r\n\r\npip3 install tensorflow\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nHere is verbose output:\r\n\r\n```shell\r\n$ pip3 install --upgrade -v tensorflow\r\nUsing pip 20.2.4 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)\r\nNon-user install because site-packages writeable\r\nCreated temporary directory: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-ephem-wheel-cache-v942l1u1\r\nCreated temporary directory: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu\r\nInitialized build tracking at /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu\r\nCreated build tracker: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu\r\nEntered build tracker: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu\r\nCreated temporary directory: /private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-install-wju8g2vw\r\n1 location(s) to search for versions of tensorflow:\r\n* https://pypi.org/simple/tensorflow/\r\nFetching project page and analyzing links: https://pypi.org/simple/tensorflow/\r\nGetting page https://pypi.org/simple/tensorflow/\r\nFound index url https://pypi.org/simple\r\nLooking up \"https://pypi.org/simple/tensorflow/\" in the cache\r\nRequest header has \"max_age\" as 0, cache bypassed\r\nStarting new HTTPS connection (1): pypi.org:443\r\nhttps://pypi.org:443 \"GET /simple/tensorflow/ HTTP/1.1\" 304 0\r\n  Skipping link: none of the wheel's tags match: cp27-cp27m-macosx_10_11_x86_64: https://files.pythonhosted.org/packages/90/cf/1d1e12f9f39b6a0ed1c49792ef5ce7615dddc2ce7287fc83ede0dddb9b3c/tensorflow-0.12.0rc0-cp27-cp27m-macosx_10_11_x86_64.whl#sha256=feaf06c7df5c0a480654bf1f38dd4d3b809c7315502a7d9f295033f9d2bd9b13 (from https://pypi.org/simple/tensorflow/)\r\n... OMITTING EXTREMELY LONG LIST OF WHEELS ...\r\n  Skipping link: none of the wheel's tags match: cp38-cp38-win_amd64: https://files.pythonhosted.org/packages/ad/fc/fccaa149d7ccc165de01d62d19e5e9492e87ad23a7106f6dfe132800ca6f/tensorflow-2.4.1-cp38-cp38-win_amd64.whl#sha256=eedcf578afde5e6e69c75d796bed41093451cd1ab54afb438760e40fb74a09de (from https://pypi.org/simple/tensorflow/)\r\nGiven no hashes to check 0 links for project 'tensorflow': discarding no candidates\r\nException information:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 228, in _main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 182, in wrapper\r\n    return func(self, options, args)\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/commands/install.py\", line 323, in run\r\n    requirement_set = resolver.resolve(\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 183, in resolve\r\n    discovered_reqs.extend(self._resolve_one(requirement_set, req))\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 388, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 339, in _get_abstract_dist_for\r\n    self._populate_link(req)\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 305, in _populate_link\r\n    req.link = self._find_requirement_link(req)\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/resolution/legacy/resolver.py\", line 270, in _find_requirement_link\r\n    best_candidate = self.finder.find_requirement(req, upgrade)\r\n  File \"/usr/local/lib/python3.8/site-packages/pip/_internal/index/package_finder.py\", line 928, in find_requirement\r\n    raise DistributionNotFound(\r\npip._internal.exceptions.DistributionNotFound: No matching distribution found for tensorflow\r\nRemoved build tracker: '/private/var/folders/7g/m6tkc5n56l7ds_6dyr28jvb00000gn/T/pip-req-tracker-vv4x81gu'\r\n```\r\n\r\nThe cause seems to be that it expects at least one wheel whose tags match my machine, but none do. My guess is that this is due to using macOS 11. All lines look like `macosx_10_*_x86_64`, but I guess it's looking for a line matching `macosx_11_*_x86_64`. (Apologies: I know nothing about the python ecosystem; this is just my educated guess.)", "comments": ["I just found this, which looks closely related to my theory that this is due to macOS 11: https://github.com/tensorflow/tensorflow/issues/45120", "[PEP 425](https://www.python.org/dev/peps/pep-0425/) seems to confirm that it is checking against the filename e.g. `tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl` to check compatibility.", "To test, I changed the filename, and it starts installing:\r\n\r\n```bash\r\nwget https://files.pythonhosted.org/packages/2f/33/4b330de553d80ae6fa123ec9aee90ee7d679d297b1595c9c6ce2b3c7a967/tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl\r\nmv tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl tensorflow-2.4.1-cp38-cp38-macosx_11_2_x86_64.whl\r\npip3 install tensorflow-2.4.1-cp38-cp38-macosx_11_2_x86_64.whl\r\n```\r\n\r\nHowever, it runs long and hot, apparently compiling many dependencies from source, e.g. `grpcio`, some of which fail. But this seems to confirm that it's the lack of a `macosx_11_2` wheel that causes the issue.", "I get the feeling that macOS 11 is actually not supported, and [the requirements on the install page](https://www.tensorflow.org/install) are out-of-date.", "@jameshfisher \r\nCan you please verify, if your system satisfies these [requirement](https://github.com/tensorflow/tensorflow/issues/42367#issuecomment-674531556) once.", "Hi @Saduf2019, my CPU does support AVX instructions:\r\n\r\n```shell\r\n$ sysctl -a | grep 'cpu.*features'\r\nmachdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C\r\nmachdep.cpu.leaf7_features: RDWRFSGS TSC_THREAD_OFFSET SGX BMI1 AVX2 SMEP BMI2 ERMS INVPCID FPU_CSDS MPX RDSEED ADX SMAP CLFSOPT IPT MDCLEAR TSXFA IBRS STIBP L1DF SSBD\r\nmachdep.cpu.extfeatures: SYSCALL XD 1GBPAGE EM64T LAHF LZCNT PREFETCHW RDTSCP TSCI\r\n```\r\n\r\nIt's an unmodified Macbook Air from early 2020; here's my hardware overview from System Information:\r\n\r\n```\r\nHardware Overview:\r\n\r\n  Model Name:\tMacBook Air\r\n  Model Identifier:\tMacBookAir8,2\r\n  Processor Name:\tDual-Core Intel Core i5\r\n  Processor Speed:\t1.6 GHz\r\n  Number of Processors:\t1\r\n  Total Number of Cores:\t2\r\n  L2 Cache (per Core):\t256 KB\r\n  L3 Cache:\t4 MB\r\n  Hyper-Threading Technology:\tEnabled\r\n  Memory:\t16 GB\r\n  System Firmware Version:\t1554.80.3.0.0 (iBridge: 18.16.14346.0.0,0)\r\n  Serial Number (system):\tFVFC50AVLYWT\r\n  Hardware UUID:\t6550BBA2-DE9D-5DBE-8839-F8BF5BD445BC\r\n  Provisioning UDID:\t6550BBA2-DE9D-5DBE-8839-F8BF5BD445BC\r\n  Activation Lock Status:\tEnabled\r\n```", "Here is the entire output from `pip3 install --upgrade -v tensorflow`, which includes the long list of wheels that it checks: [pip_output.txt](https://github.com/tensorflow/tensorflow/files/6001868/pip_output.txt)\r\n\r\n(It's frustrating that pip is not more explicit about why it is skipping a wheel. It would be very helpful to see which tags don't match. I'll see if there's any way to get that info.)", "So based on @8bitmp3's suggestion, I tried installing via virtualenv, and it does find and install a wheel! The wheel `tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl`, with standard `pip3` from Homebrew, is rejected with \"none of the wheel's tags match\", but with virtualenv, I get the output:\r\n\r\n```\r\nFound link https://files.pythonhosted.org/packages/2f/33/4b330de553d80ae6fa123ec9aee90ee7d679d297b1595c9c6ce2b3c7a967/tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl#sha256=4a04081647b89a8fb602895b29ffc559e3c20aac8bde1d4c5ecd2a65adce5d35 (from https://pypi.org/simple/tensorflow/), version: 2.4.1\r\n```\r\n\r\nIt then goes on to install this wheel.\r\n\r\nSo now there are two mysteries:\r\n\r\n* Why does pip3 under virtualenv install `tensorflow-2.4.1-cp38-cp38-macosx_10_11_x86_64.whl`, despite my OS being version 11.2 rather than 10.11? Shouldn't these be incompatible?\r\n* Why does pip3 from Homebrew fail to match this wheel, if not due to the OS? And why won't it give me more specific information about what failed to match, so that I can debug it? I've asked about this at https://github.com/pypa/pip/issues/9621", "So with some help from @8bitmp3 (https://github.com/tensorflow/docs/pull/1820) and @pfmoore (https://github.com/pypa/pip/issues/9621), I found the root cause.\r\n\r\npip from `brew install python@3.8` is version 20.2.4, which on macOS 11 only accepts `macosx_11` wheels. But pip 20.3+  includes [a change to accept `macosx_10` when running on macOS 11](https://github.com/pypa/pip/issues/9138).\r\n\r\nThe fact remains that the TensorFlow installation instructions on macOS are broken. To fix it, at least one of these solutions needs to happen:\r\n\r\n* TensorFlow installation instructions should demand pip 20.3+.\r\n* TensorFlow should support Python 3.9. Then we wouldn't have to `brew install python@3.8`, dragging in an old version of pip.\r\n* TensorFlow should publish wheels explicitly supporting `macos_11`, as per https://github.com/tensorflow/tensorflow/issues/45120.\r\n* Homebrew should update the pip in `python@3.8` to 20.3+.", "I've proposed documentation change here: https://github.com/tensorflow/docs/pull/1826\r\n\r\nThis leaves it up to the user how to get pip 20.3+. It's not easy, because TF demands Python <3.9 but pip 20.3+, which can't be satisfied with Homebrew packages (AFAICS). My workaround is to use virtualenv for now.", "TF cannot yet support Py3.9 (#44485)\r\n\r\nMacOS support is hard to obtain as we depend on a different team which provides us with VMs but they are older macs", "Note: Homebrew now seems to have updated `python@3.8` to include `pip3` version `21.0.1`. So the following now seems to work:\r\n\r\n```\r\nbrew install python@3.8\r\npip3 install tensorflow\r\n```", "Awesome. What else is needed for this issue?", "I think nothing, so I'll close it!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/47205\">No</a>\n", "Hit this coming from homebrew python@3.9. For anyone else ending up here;\r\n\r\n`brew install python@3.8` installs alongside 3.9 without linking. Manually pointing to the needed installation worked for us `/usr/local/opt/python@3.8/bin/python3 -m venv venv; ...`", "Doesn't work on Macbook Air M1, with macOS Big Sur 11.6, Python 3.8.12 and pip 21.3", "Google does not have access to develop on M1, so until that happens we cannot provide any support there"]}, {"number": 47204, "title": "AttributeError: module 'tensorflow' has no attribute 'contrib'", "body": "I am learning object-detection from [this link](https://pythonprogramming.net/training-custom-objects-tensorflow-object-detection-api-tutorial/).\r\n\r\nBut I **got an error** while run command in **anaconda prompt**:\r\n\r\n(base) R:\\SEMESTER 8\\SC\\eye-detection\\models\\research\\object_detection\\legacy>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n2021-02-17 15:30:15.419573: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-02-17 15:30:15.419775: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 49, in <module>\r\n    from object_detection.builders import dataset_builder\r\n  File \"R:\\Anaconda\\anaconda\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 27, in <module>\r\n    from object_detection.data_decoders import tf_example_decoder\r\n  File \"R:\\Anaconda\\anaconda\\lib\\site-packages\\object_detection\\data_decoders\\tf_example_decoder.py\", line 27, in <module>\r\n    slim_example_decoder = tf.contrib.slim.tfexample_decoder\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'\r\n\r\n### What I have tried:\r\n\r\n- I tried to automatically upgrade script to tensorflow2 using [single python file command](https://www.tensorflow.org/guide/upgrade#single_file). But didn't work for me.\r\n(base) R:\\Anaconda\\anaconda\\Lib\\site-packages\\object_detection\\data_decoders>**tf_upgrade_v2 --infile tf_example_decoder.py --outfile tf_example_decoder.py**\r\n2021-02-17 15:14:35.610773: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-02-17 15:14:35.611176: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nERROR line 27:23: Using member tf.contrib.slim.tfexample_decoder in deprecated module tf.contrib. tf.contrib.slim.tfexample_decoder cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\nERROR line 62:23: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\nERROR line 63:20: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\nERROR line 71:31: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\nERROR line 72:20: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\nTensorFlow 2.0 Upgrade Script\r\n**Converted 1 files\r\nDetected 5 issues that require attention\r\nFile: tf_example_decoder.py**\r\ntf_example_decoder.py:27:23: ERROR: Using member tf.contrib.slim.tfexample_decoder in deprecated module tf.contrib. tf.contrib.slim.tfexample_decoder cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\ntf_example_decoder.py:62:23: ERROR: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\ntf_example_decoder.py:63:20: ERROR: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\ntf_example_decoder.py:71:31: ERROR: Using member tf.contrib.lookup.HashTable in deprecated module tf.contrib. tf.contrib.lookup.HashTable cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\ntf_example_decoder.py:72:20: ERROR: Using member tf.contrib.lookup.KeyValueTensorInitializer in deprecated module tf.contrib. tf.contrib.lookup.KeyValueTensorInitializer cannot be converted automatically. tf.contrib will not be distributed with TensorFlow 2.0, please consider an alternative in non-contrib TensorFlow, a community-maintained repository such as tensorflow/addons, or fork the required code.\r\nMake sure to read the detailed log 'report.txt'\r\n\r\n- tried to install downgrade tesorflow version but not supported in python 3.9.1\r\n- tried to run this code in cmd also but got same error.\r\n\r\n**_Tensorflow Version: 2.4.1\r\npython version: 3.9.1\r\nwindows: 10\r\nconda 4.9.2_**\r\n\r\nCan anyone know proper solution for this?", "comments": ["Use [tf_slim](https://github.com/google-research/tf-slim) instead", "@fsx950223 instead of which I have to use tf_slim?", "You should pull tensorflow models' latest codebase since they have updated the code.", "Hey @fsx950223 thanks for your help. But I need your help one more time. To install updated model directory I follow [this link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md) and its work properly. but I am not moving forward because of following error.\r\n\r\nR:\\SEMESTER 8\\SC\\models\\research\\object_detection\\legacy>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 186, in <module>\r\n    tf.app.run()\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 340, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"train.py\", line 169, in main\r\n    trainer.train(\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\object_detection\\legacy\\trainer.py\", line 302, in train\r\n    training_optimizer, optimizer_summary_vars = optimizer_builder.build(\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\object_detection\\builders\\optimizer_builder.py\", line 153, in build\r\n    return build_optimizers_tf_v1(config, global_step)\r\n  File \"R:\\minianaconda\\minianaconda\\lib\\site-packages\\object_detection\\builders\\optimizer_builder.py\", line 86, in build_optimizers_tf_v1\r\n    optimizer = tf_opt.MovingAverageOptimizer(\r\nNameError: name 'tf_opt' is not defined\r\n\r\n**This error come from** [optimizer_builder.py](https://github.com/tensorflow/models/blob/master/research/object_detection/builders/optimizer_builder.py).\r\n\r\nPlease help me to solve this.", "@rutvi1462 \r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!", "@ravikyram Thanks I will post there."]}, {"number": 47202, "title": "[MLIR] Remove hardcoded element type in lowering of SpaceToBatchND", "body": "Lowering of SpaceToBatchND currently creates a Pad Op with\r\ntype f32 irrespective of the element type of the input\r\ntensor. This commit fixes this bug, and also changes a test\r\ncase for SpaceToBatchND Op that would catch such a bug.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47202) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47202) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47202) for more info**.\n\n<!-- need_sender_cla -->", "@vinayaka-polymage  Can you please sign CLA. Thanks!", "@googlebot I signed it!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F47202) for more info**.\n\n<!-- need_sender_cla -->", "> @vinayaka-polymage Can you please sign CLA. Thanks!\r\n\r\nI am facing problems while we are trying to sign CLA as corporate signers. I created this PR, and after that we (PolyMage Labs) signed the CLA. We will troubleshoot this.", "@googlebot I signed it!", "@smit-hinsu @gbaned Looks like the build failure is unrelated and someone else broke it.\r\n\r\n```\r\n /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/PowerPCCodeGen/PPCAsmPrinter.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/PowerPCCodeGen/PPCAsmPrinter.pic.o' -fPIC -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/AMDGPU -isystem external/llvm-project/llvm/lib/Target/PowerPC -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/PowerPC -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -Iexternal/llvm-project/llvm/lib/Target/PowerPC -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/llvm-project/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp -o bazel-out/k8-opt/bin/external/llvm-project/llvm/_objs/PowerPCCodeGen/PPCAsmPrinter.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/llvm-project/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp:86:14: error: specialization of 'template<class T> struct llvm::DenseMapInfo' in different namespace [-fpermissive]\r\n struct llvm::DenseMapInfo<std::pair<const MCSymbol *, MCSymbolRefExpr::VariantKind>> {\r\n              ^\r\nIn file included from external/llvm-project/llvm/include/llvm/ADT/DenseMapInfo.h:16:0,\r\n                 from external/llvm-project/llvm/include/llvm/ADT/DenseMap.h:16,\r\n                 from external/llvm-project/llvm/include/llvm/MC/MCAsmLayout.h:12,\r\n                 from external/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCMCExpr.h:12,\r\n                 from external/llvm-project/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp:19:\r\nexternal/llvm-project/llvm/include/llvm/ADT/APInt.h:34:30: error:   from definition of 'template<class T> struct llvm::DenseMapInfo' [-fpermissive]\r\n template <typename T> struct DenseMapInfo;\r\n                              ^\r\n\u001b[32m[13,837 / 14,611]\u001b[0m Compiling tensorflow/core/kernels/matmul_op_real.cc; 288s local ... (32 actions, 31 running)\r\n\u001b[32m[16,863 / 18,594]\u001b[0m 19 / 1198 tests, \u001b[31m\u001b[1m19 failed\u001b[0m;\u001b[0m Compiling tensorflow/compiler/mlir/tensorflow/translate/import_model.cc; 54s local ... (31 actions, 30 running)\r\n\u001b[32m[18,505 / 18,594]\u001b[0m 19 / 1198 tests, \u001b[31m\u001b[1m19 failed\u001b[0m;\u001b[0m Compiling tensorflow/core/kernels/mkl/mkl_cwise_ops_common.cc; 205s local ... (32 actions, 31 running)\r\n```", "> @smit-hinsu @gbaned Looks like the build failure is unrelated and someone else broke it.\r\n\r\nLet's retry the submission as all the builds are now green in an internal dashboard so this should go through now.", "Hey, can you please let me know if more changes are needed on this ? ", "Will get this approved by Mehdi internally."]}, {"number": 47201, "title": "RE: Some questions", "body": "Hi I have some questions in regards to this project:\r\n\r\n1. How does this differ to OpenCV?\r\n2. Am I able to also code in C++?\r\n3. Does it support AMD OpenCL?", "comments": ["@Joe23232 \r\n\r\n1. Tensorflow is a framework used (with Keras) to perform ML-related tasks. OpenCV is primarily used for Computer Vision and Image processing related tasks. \r\n2. This library is written in C++ and python both. \r\n3. Follow this [Link](https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/). Also, refer this [issue #22](https://github.com/tensorflow/tensorflow/issues/22)\r\n\r\nI hope this helps !!", "Oh ok thanks mate\n\nOn Wed, Feb 17, 2021 at 10:35 PM Adwait <notifications@github.com> wrote:\n\n> @Joe23232 <https://github.com/Joe23232>\n>\n>    1. Tensorflow is a framework used (with Keras) to perform ML-related\n>    tasks. OpenCV is primarily used for Computer Vision and Image processing\n>    related tasks.\n>    2. This library is written in C++ and python both.\n>    3. Read this Link\n>    <https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47201#issuecomment-780496430>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIKO7IIOCPRKJWG3B3HBRK3S7OSXBANCNFSM4XXRMABQ>\n> .\n>\n", "@Joe23232 \r\n1.OpenCv does not have automatic differentiation engine and many more differences, these are largely different in their own scope.\r\n2.Both frameworks have programmable interface in c++\r\n3.Please refer to this [link](https://community.amd.com/t5/hsa/tensorflow-with-amd-gpu/td-p/199925).\r\n\r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there, please move this to close status and open an issue on stackoverflow.\r\n\r\n", "Closing as this should be asked on Stack Overflow."]}, {"number": 47200, "title": "How can I get min/max information from TFlite Intepreter?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):2.4.1\r\n- Are you willing to contribute it (Yes/No):Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI can see the min/max information by using netron open .tflite file. I need those two information to calculate radix. However, I cannot get min/max from any API provided in TFLite Intepreter. I guess to use get_tensor_details(). However I can only get the scale and zero point. Is there any way I can access min/max?\r\n\r\n**Will this change the current api? How?**\r\nYes, adding one or two items in get_tensor_details()\r\n**Who will benefit with this feature?**\r\nAnyone that wants to calculate radix from quantized models in TFLite\r\n**Any Other info.**\r\n", "comments": ["Can you describe your use case? (As you might be aware) you can calculate min/max from scale and zero points, like `((DTYPE_MIN - zero_point) * scale, (DTYPE_MAX - zero_point) * scale)`", "> Can you describe your use case? (As you might be aware) you can calculate min/max from scale and zero points, like `((DTYPE_MIN - zero_point) * scale, (DTYPE_MAX - zero_point) * scale)`\r\n\r\nOh, thanks for your explanation.Now I know how to calculate scale and zero points. "]}, {"number": 47199, "title": "Use xa_nnlib for quantize for Fusion F1.", "body": "Copied the relevant function call from https://github.com/pnikam-cad/tensorflow/blob/a737c1e3945bc70022259479ad24133a343ec906/tensorflow/lite/micro/kernels/xtensa_hifi/quantize.cc\r\n\r\nLatency for the first quantize op (int16->int8) in the keyword_benchmark went from 3758 ticks to 800 ticks. Overall latency went from 38516 ticks to 34253 ticks.\r\n\r\nTested with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade OPTIMIZED_KERNEL_DIR=xtensa run_keyword_benchmark -j8\r\n```\r\n\r\nFull output (for completeness):\r\n```\r\nInitializeKeywordRunner took 160568 ticks (160 ms).\r\n\r\nKeywordRunNIerations(1) took 34253 ticks (34 ms)\r\nQUANTIZE took 800 ticks (0 ms).\r\nSVDF took 4753 ticks (4 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 4211 ticks (4 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 3145 ticks (3 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 4211 ticks (4 ms).\r\nFULLY_CONNECTED took 1353 ticks (1 ms).\r\nSVDF took 2890 ticks (2 ms).\r\nSVDF took 3583 ticks (3 ms).\r\nSVDF took 3054 ticks (3 ms).\r\nFULLY_CONNECTED took 1091 ticks (1 ms).\r\nSOFTMAX took 749 ticks (0 ms).\r\nQUANTIZE took 354 ticks (0 ms).\r\n```\r\n\r\nAlso tested that the kernel test passes with:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade OPTIMIZED_KERNEL_DIR=xtensa test_kernel_quantize_test -j8\r\n```\r\n\r\nProgress towards http://b/177457688\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "tagging @pnikam-cad @nyadla-sys @bhanuprakashbv"]}, {"number": 47198, "title": "pip Install Error: don't could find version that satisfies the requirement tensorflow", "body": "------------------------\r\n\r\n### System information\r\n\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n-   **Python version**: 3.9.1 64x\r\n\r\nI tried to \r\n\r\n```bash\r\npip install https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n```\r\nor\r\n```bash\r\npip install tensorflow\r\n```\r\non VirtualEnv created via VSCode and I am getting the following errors:\r\n\r\n```bash\r\nERROR: Could not find a version that satisfies the requirement tensorflow\r\nERROR: No matching distribution found for tensorflow\r\n```\r\nor\r\n\r\n```bash\r\nCollecting https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n  Using cached https://github.com/tensorflow/tensorflow/archive/v2.4.1.tar.gz\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'c:\\users\\venic\\onedrive\\kaggle\\fashion_ai\\.venv\\scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\venic\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-72uc4pr3\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\venic\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-72uc4pr3\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base 'C:\\Users\\venic\\AppData\\Local\\Temp\\pip-pip-egg-info-z0d8x7xg'\r\n    Complete output (5 lines):\r\n      File \"C:\\Users\\venic\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tokenize.py\", line 392, in open\r\n        buffer = _builtin_open(filename, 'rb')\r\n    FileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\venic\\\\AppData\\\\Local\\\\Temp\\\\pip-req-build-72uc4pr3\\\\setup.py'\r\n    ----------------------------------------\r\n```\r\n\r\nPip version = 21.0.1\r\n", "comments": ["@veniciuss TensorFlow has not supported Python3.9 yet.  See https://github.com/tensorflow/tensorflow/issues/44485. As a workaround, can you try to install Python3.8 or lower version?", "Closing as duplicate of #44485"]}, {"number": 47197, "title": "Multi-thread support for TF2 ParameterServerStrategy", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter building a sample model based on tutorial(https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy), the performance is way less compare to TF 1 PS strategy(multi client version) and checking the coordinator resource utilization, it turns out that only single core is heavily utilized in a multi-core machine for coordinator. It seems that even though in workers are maintained in separate thread(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/coordinator/cluster_coordinator.py#L742), the actual executions are still sequential. \r\n\r\nIs this caused by GIL? what's the recommended approach to improve the performance?\r\n\r\n**Describe the expected behavior**\r\nMulti-thread / multi-core support for ParameterServerStrategy coordinator.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nN/A\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["The function execution will release GIL therefore so far it has not been a big issue for our internal users. Have you tried multi-step packing: https://www.tensorflow.org/tutorials/distribute/parameter_server_training#performance_improvement?", "Thanks Yuefeng, I tried but performance wise it's still not on par with TF 1 API. Do you mind sharing some details regrading how to set up the benchmark you and Haoyu used in this video(https://www.youtube.com/watch?v=B2Tpv_N7wkg&list=PLv-somlsqZU6VDnVcOof75RgT0I6v1Vg4&index=1&ab_channel=TensorFlow)?", "cc @haoyuz as well.\r\n\r\nAfter we enabled multi-step packing (like ~10 steps per schedule), we tried to see whether there is any imbalance between PS in terms of CPU usage. We saw some PS has much higher CPU load and identified issues with variable sharding and optimizer variables being used by all other PS in a single step. Therefore, we added the tip 1 and 2 in the tutorial.", "We created a synthetic model which resemble a wide-n-deep for benchmarking.", "> we tried to see whether there is any imbalance between PS in terms of CPU usage\r\n\r\nFor our particular case, PS CPU usage is low and the bottleneck is on coordinator throughput. ", "Could you try packing more steps into a single function?", "If you have a profiling with python call stacks enabled, that would be wonderful.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 47196, "title": "I have problem with python code in google colab", "body": "I write this code:\r\nbasemodel.fit(X_train,y_train,epochs=20,validation_split=.1,callbacks=call_back)\r\nbut don't work this error:\r\nEpoch 1/20\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-34-337a90849ebd> in <module>()\r\n----> 1 basemodel.fit(X_train,y_train,epochs=20,validation_split=0.1,callbacks=call_back)\r\n\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    975           except Exception as e:  # pylint:disable=broad-except\r\n    976             if hasattr(e, \"ag_error_metadata\"):\r\n--> 977               raise e.ag_error_metadata.to_exception(e)\r\n    978             else:\r\n    979               raise", "comments": ["\"I have a problem\" type of questions are better asked on Stack Overflow. This looks like a problem with the code you wrote, not with TF itself. As such, to minimize TF developers distraction time, please ask this on SO.", "I don't know how to deal with this website, but I really need to solve this\nproblem. It is related to my master's project, which will be discussed in\ntwo weeks.\n\nOn Tue, Feb 16, 2021 at 9:16 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> \"I have a problem\" type of questions are better asked on Stack Overflow.\n> This looks like a problem with the code you wrote, not with TF itself. As\n> such, to minimize TF developers distraction time, please ask this on SO.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/47196#issuecomment-780025181>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AN3JHKOUZKJKIVBJ56LKJV3S7KZBRANCNFSM4XW4ROSA>\n> .\n>\n\n\n-- \nTo be successful, we must first believe that we can\ud83d\udc9e\n"]}, {"number": 47195, "title": "Error while starting the training for custom dataset using Tensor flow object detection API", "body": "Trying to train a model with custom dataset using the link :\r\nhttps://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\r\nthis Object detection API. \r\nI followed all the steps mentioned in the documentation and the training should have been stated after running the command:\r\n>python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\nbut instead this error is coming up. \r\nPlease help \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip installed\r\n- TensorFlow version (use command below): 2.4\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: 11.0.2, CUDnn 8.0.5\r\n- GPU model and memory: Nvidia Quadro T2000\r\n\r\n\r\nError log from cmd: \r\n\r\n\r\n(tensorflow) C:\\Saurav\\Work\\SESA\\YardHealthStick\\tensorflow\\workspace\\training_demo>python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config\r\n2021-02-16 21:41:58.726218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-16 21:42:03.204172: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-16 21:42:03.208849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-02-16 21:42:03.255184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro T2000 computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2021-02-16 21:42:03.262012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-16 21:42:03.275486: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-16 21:42:03.279686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-16 21:42:03.287813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-16 21:42:03.294632: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-16 21:42:03.312945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-16 21:42:03.322076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-02-16 21:42:03.328164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-16 21:42:03.330916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-16 21:42:03.333579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-02-16 21:42:03.346264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro T2000 computeCapability: 7.5\r\ncoreClock: 1.785GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s\r\n2021-02-16 21:42:03.353121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-02-16 21:42:03.356164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-02-16 21:42:03.358468: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-02-16 21:42:03.361200: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-02-16 21:42:03.365114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-02-16 21:42:03.367316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-02-16 21:42:03.370458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-02-16 21:42:03.373566: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-02-16 21:42:03.376127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-02-16 21:42:04.480554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-02-16 21:42:04.484849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0\r\n2021-02-16 21:42:04.487720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N\r\n2021-02-16 21:42:04.490792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2903 MB memory) -> physical GPU (device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2021-02-16 21:42:04.499549: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nI0216 21:42:04.510176 15720 mirrored_strategy.py:350] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\r\nINFO:tensorflow:Maybe overwriting train_steps: None\r\nI0216 21:42:04.516160 15720 config_util.py:552] Maybe overwriting train_steps: None\r\nINFO:tensorflow:Maybe overwriting use_bfloat16: False\r\nI0216 21:42:04.517184 15720 config_util.py:552] Maybe overwriting use_bfloat16: False\r\nWARNING:tensorflow:From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:530: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nW0216 21:42:04.714892 15720 deprecation.py:333] From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py:530: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nrename to distribute_datasets_from_function\r\nINFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']\r\nI0216 21:42:04.721902 15720 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']\r\nINFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']\r\nI0216 21:42:04.723216 15720 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']\r\nINFO:tensorflow:Number of filenames to read: 1\r\nI0216 21:42:04.723867 15720 dataset_builder.py:81] Number of filenames to read: 1\r\nWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\r\nW0216 21:42:04.724863 15720 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\r\nWARNING:tensorflow:From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nW0216 21:42:04.730878 15720 deprecation.py:333] From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\r\nWARNING:tensorflow:From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nW0216 21:42:04.758770 15720 deprecation.py:333] From C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.Dataset.map()\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 104, in main\r\n    model_lib_v2.train_loop(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 530, in train_loop\r\n    train_input = strategy.experimental_distribute_datasets_from_function(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 340, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1143, in experimental_distribute_datasets_from_function\r\n    return self.distribute_datasets_from_function(dataset_fn, options)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1134, in distribute_datasets_from_function\r\n    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py\", line 545, in _distribute_datasets_from_function\r\n    return input_lib.get_distributed_datasets_from_function(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 161, in get_distributed_datasets_from_function\r\n    return DistributedDatasetsFromFunction(dataset_fn, input_workers,\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1272, in __init__\r\n    _create_datasets_from_function_with_input_context(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\distribute\\input_lib.py\", line 1936, in _create_datasets_from_function_with_input_context\r\n    dataset = dataset_fn(ctx)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 521, in train_dataset_fn\r\n    train_input = inputs.train_input(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\inputs.py\", line 893, in train_input\r\n    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 251, in build\r\n    dataset = dataset_map_fn(dataset, decoder.decode, batch_size,\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\builders\\dataset_builder.py\", line 236, in dataset_map_fn\r\n    dataset = dataset.map_with_legacy_function(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 340, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 2679, in map_with_legacy_function\r\n    ParallelMapDataset(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 4242, in __init__\r\n    self._map_func = StructuredFunctionWrapper(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3493, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 546, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 378, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 400, in _create_definition_if_needed_impl\r\n    temp_graph = func_graph_from_py_func(\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 971, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3485, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3453, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n  File \"C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 670, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\nNotImplementedError: in user code:\r\n\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\object_detection\\data_decoders\\tf_example_decoder.py:524 default_groundtruth_weights  *\r\n        [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\r\n        return target(*args, **kwargs)\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3120 ones\r\n        output = _constant_if_small(one, shape, dtype, name)\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2804 _constant_if_small\r\n        if np.prod(shape) < 1000:\r\n    <__array_function__ internals>:5 prod\r\n\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3030 prod\r\n        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:87 _wrapreduction\r\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n    C:\\Users\\ytl2tnk\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:852 __array__\r\n        raise NotImplementedError(\r\n\r\n    NotImplementedError: Cannot convert a symbolic Tensor (cond_2/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n\r\n", "comments": ["The GPU functionality is working well as i tried to run the example using pre-trained model in real time using object_detection_camera.py\r\n\r\n", "@KumarSaurav366,\r\nIssues related to Object Detection API are tracked in tensorflow/models repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/models/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Sure, thanks for the suggestion. I'll do the same \r\n"]}]