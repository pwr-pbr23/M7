[{"number": 19230, "title": "Fix links on the community/swift page.", "body": "They were broken rendered on https://www.tensorflow.org/community/swift.", "comments": []}, {"number": 19229, "title": "[Intel MKL] Fix for convrnn unit test failure", "body": "Adding a fixup pass (+unit test) to handle incorrectly linked Mkl metadata\r\nedges\r\n\r\nThis graph pass is needed because a graph may have some input Mkl metadata\r\nedges incorrectly setup after node merge and rewrite passes. This could\r\nhappen because GetReversePostOrder function may not provide a topologically\r\nsorted order if a graph contains cycles.\r\n\r\nMinor style issue left over from earlier commits to the graph pass are also\r\nhandled (but not related to the fixup pass).", "comments": ["The failures are known to us and unrelated to your change. Merging.", "@nhasabni Thanks for the fix!", "Thanks!"]}, {"number": 19228, "title": "optimize_for_inference.py has conversion error for float16 Conv2D", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**: Linux Mint 18.3\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: v1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.13.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 9.1, V9.1.85\r\n- **GPU model and memory**: 1080 Ti\r\n\r\n### Describe the problem\r\n`optimize_for_inference.py` has conversion error for `float16` `Conv2D`.\r\n\r\nI have a model with all floats as `float16`. After optimization by `optimize_for_inference.py` and then loaded in C++, the following error occurs.\r\n\r\n### Source code / logs\r\n```\r\nInvalid argument: No OpKernel was registered to support Op 'FusedPadConv2D' with these attrs.  Registered devices: [CPU,GPU,XLA_CPU,XLA_GPU], Registered kernels:\r\n  device='CPU'; T in [DT_FLOAT]\r\n\r\n\t [[Node: Conv2D = FusedPadConv2D[T=DT_HALF, mode=\"REFLECT\", padding=\"VALID\", strides=[1, 1, 1, 1]](MirrorPad_5, MirrorPad_6/paddings, Deep-Q-learning/conv1)]]\r\n```\r\n\r\nThe format of the output of `Registered kernels` is so weird.\r\n\r\nI have read the source code of `optimize_for_inference.py` but found no clue.\r\n", "comments": ["The root cause is in https://github.com/tensorflow/tensorflow/blob/f318765ad5a50b2fbd7cc08dd4ebc249b3924270/tensorflow/core/ops/nn_ops.cc#L449, it expects the input T to be type float, whereas your input is half.\r\n", "@qlzh727 So this is a bug at that line, or I did something wrong?\r\nWhy `T` of `FusedPadConv2D` can only be `float`?", "Nagging Assignee @tatatodd: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatatodd: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Sorry for the long delay, should be fixed by https://github.com/tensorflow/tensorflow/commit/46cf73f0214bc6208295e36650f1a8ffde4abdd7."]}, {"number": 19227, "title": "Fix warning caused by squeeze_dims", "body": "The `squeeze_dims` in `tf.squeeze` has been deprecated in favor of `axis`. This fix fixes the `squeeze_dims` in text_classification_cnn.py so that the warning could be removed.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks!"]}, {"number": 19226, "title": "Use AvailableArrayName for sanity", "body": "Array name must only have digits after colon.\r\nBut the code is implemented to add \"_reordered\" string without check.\r\nIt makes error while using toco.", "comments": ["Could you please add label for unit tests?", "@pillarpond sorry for the delay. Testing now.", "I think that the fails is not related with my changes. What should I do?", "Nagging Assignee @rmlarsen: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Cloud you please let me know why this commit is not merged?\r\nShould I wait more?", "It seems to fail the build internally. Trying again."]}, {"number": 19225, "title": "Add complex support for tf.segment_mean", "body": "While using `tf.segment_mean` I noticed that it does not have the complex support like `tf.segment_sum`. I think it makes sense to support complex for it. This fix adds the complex support for `tf.segment_mean`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Thanks!"]}, {"number": 19224, "title": "Fix misleading cupti.h error message", "body": "This fix tries to address the issue raised in #19223 where the cupti.h eror message was misleading. The following error:\r\n```\r\nCuda Configuration Error: Cannot find cupti.h under /usr/local/cuda-9.0\r\n```\r\nis not the true path searched.\r\n\r\nThis fix updates the bzl file to print out the complete searched paths when error occurs. Below is the new output:\r\n```\r\nCuda Configuration Error: Cannot find cupti.h under /usr/local/cuda-9.0/extras/CUPTI/include/, /usr/local/cuda-9.0/include/cuda/CUPTI/\r\n```\r\n\r\nThis fix fixes #19223.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Thanks!"]}, {"number": 19223, "title": "Misleading \"cupti.h\" error message", "body": "i am trying to build tensorflow 1.7.1 with cuda support on debian.\r\nWhile doing so, i received the error message\r\n```\r\nCuda Configuration Error: Cannot find cupti.h under /usr/lib/cuda\r\n```\r\ni symlinked cupti.h to the written location, even copied it there but nothing happened. after reading the buildscript ```third_party/gpus/cuda_configure.bzl```, it became clear that the script did actually not search for cupti.h in ```/usr/lib/cuda/``` but in two subfolders below that one. This error message is disleading and should display the actual path that bazel uses to look up the files.", "comments": ["Added PR #19224 for the fix.", "wow that was quick :) very much appreciated. thx"]}, {"number": 19222, "title": "Support float16 for depthwise convolutions on GPU and CPU", "body": "The current implementation supports only bfloat16, float32, float64, but not float16.\r\n\r\nStack trace:\r\n```\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 536, in _DepthwiseConv2dNativeGrad\r\n    data_format=op.get_attr(\"data_format\")),\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2325, in depthwise_conv2d_native_backprop_input\r\n    name=name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 609, in _apply_op_helper\r\n    param_name=input_name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 60, in _SatisfiesTypeConstraint\r\n    \", \".join(dtypes.as_dtype(x).name for x in allowed_list)))\r\nTypeError: Value passed to parameter 'filter' has DataType float16 not in list of allowed values: bfloat16, float32, float64\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: built from sources\r\nTensorFlow version: 1.8\r\nBazel version: 0.10.0\r\nCUDA/cuDNN version: 9.1/7.1.2\r\nGPU model and memory: Tesla v100, 16GB\r\nExact command to reproduce\r\n```\r\nnet = tf.cast(net, tf.float16)\r\nnet = tf.contrib.layers.separable_convolution2d(net, 512, 3, 1)\r\n\r\n```", "@alextp Do you known the rationale for the supported types?", "@poxvoculi no idea. @vrv might know?", "@chsigg is our depthwise guru and probably knows best!", "Sorry for the delay. Yes, this looks like an omission. I will get to it soon.", "@ygoncharov It looks like the float16 support has been added in commit https://github.com/tensorflow/tensorflow/commit/5ad9e4588874f30d0d079acc60e07f2eddc0480f\r\n\r\nI think this issue might be closed?", "Yes, it works in v1.9\r\nI noted that the performance of depthwise convolutions on GPU (v100) for fp16 is worse than for fp32 (it is obviously not a case for regular convolutions). Is it a known problem? What is a possible explanation for this?", "Nagging Assignee @chsigg: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The depthwise convolution kernel does not use the tensor cores. The regular convolutions forward to cuDNN, which does use the tensor cores.\r\n\r\nIn the medium term, we want to use cuDNN for depthwise convolutions as well, which will fix this discrepancy.", "Thanks for the update. Is there an issue I can follow for using the tensor cores for depthwise convolutions?"]}, {"number": 19221, "title": "fix docstring", "body": "Confusing, since container.variables is a method. Looks similar to tf.keras.Sequential().variables, which returns the list of all layer variables/weights.\r\n\r\nCompare with \r\nhttps://github.com/tensorflow/models/blob/master/official/mnist/mnist_eager.py#L73", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 19220, "title": "upsampling op for 5-D tensor with shape [batch, height, width, depth, channels] (feature request)", "body": "My target is 3D medical image.\r\nFor 4-D tensor B with shape [batch, height, width, channels] use tf.image.resize_* for upsampling.\r\nFor 5-D tensor A with shape [batch, height, width, depth, channels], tf.nn.conv3d_transpose can be used for upsampling, but I don't want extra weights for training.\r\n\r\nIs there an direct op for this? instead of\r\n\r\n1.  Note: This seem to be wrong !\r\n     [batch, height, width, depth, channels] = tf.shape(A)\r\n     A1 = tf.reshape(A, [batch, height, width * depth, channels]) \r\n     A2 = tf.image.resize_bilinear(A1, [2 * height, 4 * width * depth])\r\n     A3 = tf.reshape(A2, [batch, 2 * height, 2 * width, 2 * depth, channels]\r\n\r\n2. split the last dimension -> reshape -> resize_bilinear -> reshape -> concat.\r\n\r\n3. tf.nn.con3d_transpose()\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code\uff1ayes\r\nOS Platform and Distribution\uff1aUbuntu16.04\r\nTensorFlow installed from\uff1apip\r\nTensorFlow version\uff1a1.7.0\r\nBazel version\uff1aNone\r\nCUDA/cuDNN version\uff1a9.0/7.0\r\nGPU model and memory\uff1a11G", "Since there may be a workaround that does not involve creating a new op, this question is better asked on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow), where the answer may better help other developers. There is also a larger community that answers questions there.\r\n\r\nIf you ask a question there and find that there is no solution other than to create a new op, please file another issue with a link to the question. Thanks!", "Why is this issue closed?\r\nTF completely lacks any 3D upsampling  operations, which is very common unpooling op for 3D (volumentric) images. \r\n\r\nE.g. here PyTorch implements nearest neighbor and linear upsampling for 3D images\r\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.Upsample\r\n\r\nPlease reopen this issue, and add support for 3D nearest neighbor and linear upsampling.\r\n\r\n", "Need to know if there was any follow-up to this. I cannot find 3D Upsampling with interpolation anywhere in TensorFlow.", "I cannot believe that tensorflow doesn't have the upsampling or resize function for 3d data. ", "Come on Google, you are better than garbage fb"]}, {"number": 19219, "title": "unorderable types: str() < tuple() in /tensorflow/python/feature_column/feature_column.py", "body": "Google Tensorflow forum guy asked me to open this issue:\r\n\r\n### System information\r\n- **Have I written custom code **:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\npip3 install tensorflow --upgrade\r\n- **TensorFlow version (use command below)**:\r\nTF 1.7.0.\r\n- **Python version**: \r\nPython 3.4.3 \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nrun time error in python: unorderable types: str() < tuple() in /tensorflow/python/feature_column/feature_column.py\r\n\r\n### Source code / logs\r\nhttps://github.com/werowe/tripAdvisorNeuralNetworkTensorFlow/blob/master/tripadvisorNN.py\r\ndata:   https://raw.githubusercontent.com/werowe/tripAdvisorNeuralNetworkTensorFlow/master/tripAdvisorFL.csv\r\n\r\n\r\ndictionary {'Nrhotelreviews': <tf.Tensor 'DecodeCSV:2' shape=() dtype=int32>, 'Casino': <tf.Tensor 'DecodeCSV:11' shape=() dtype=int32>, 'Travelertype': <tf.Tensor 'DecodeCSV:6' shape=() dtype=int32>, 'Helpfulvotes': <tf.Tensor 'DecodeCSV:3' shape=() dtype=int32>, 'Usercontinent': <tf.Tensor 'DecodeCSV:16' shape=() dtype=int32>, 'Tenniscourt': <tf.Tensor 'DecodeCSV:9' shape=() dtype=int32>, 'Usercountry': <tf.Tensor 'DecodeCSV:0' shape=() dtype=int32>, 'Reviewmonth': <tf.Tensor 'DecodeCSV:18' shape=() dtype=int32>, 'Hotelname': <tf.Tensor 'DecodeCSV:13' shape=() dtype=int32>, 'Nrrooms': <tf.Tensor 'DecodeCSV:15' shape=() dtype=int32>, 'Freeinternet': <tf.Tensor 'DecodeCSV:12' shape=() dtype=int32>, 'Nrreviews': <tf.Tensor 'DecodeCSV:1' shape=() dtype=int32>, 'Gym': <tf.Tensor 'DecodeCSV:8' shape=() dtype=int32>, 'Memberyears': <tf.Tensor 'DecodeCSV:17' shape=() dtype=int32>, 'Periodofstay': <tf.Tensor 'DecodeCSV:5' shape=() dtype=int32>, 'Pool': <tf.Tensor 'DecodeCSV:7' shape=() dtype=int32>, 'Reviewweekday': <tf.Tensor 'DecodeCSV:19' shape=() dtype=int32>, 'Spa': <tf.Tensor 'DecodeCSV:10' shape=() dtype=int32>, 'Hotelstars': <tf.Tensor 'DecodeCSV:14' shape=() dtype=int32>}  label =  Tensor(\"DecodeCSV:4\", shape=(), dtype=int32)\r\nINFO:tensorflow:Calling model_fn.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 355, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 824, in _train_model\r\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py\", line 805, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 354, in _model_fn\r\n    config=config)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 184, in _dnn_model_fn\r\n    logits = logit_fn(features=features, mode=mode)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/estimator/canned/dnn.py\", line 92, in dnn_logit_fn\r\n    features=features, feature_columns=feature_columns)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/feature_column/feature_column.py\", line 274, in input_layer\r\n    trainable, cols_to_vars)\r\n  File \"/home/walker/tf3/lib/python3.4/site-packages/tensorflow/python/feature_column/feature_column.py\", line 192, in _internal_input_layer\r\n    for column in sorted(feature_columns, key=lambda x: x.name):\r\nTypeError: unorderable types: str() < tuple()\r\n", "comments": ["`key` argument must be a string. You have provide a tuple in some of your feature columns, such as:\r\n`tf.feature_column.numeric_column((\"Usercountry\",47))`"]}, {"number": 19218, "title": "DOC: Fix python code in for invalid code", "body": "There is an error in python code in the documentation. There is no class called `tf.estimator.Estimator.LinearClassifier` I think it was a typo and the author meant `tf.estimator.LinearClassifier`.", "comments": ["```\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.VERSION\r\n'1.8.0'\r\n>>> tf.estimator.Estimator.LinearClassifier\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: type object 'Estimator' has no attribute 'LinearClassifier'\r\n>>> tf.estimator.LinearClassifier\r\n<class 'tensorflow.python.estimator.canned.linear.LinearClassifier'>\r\n```\r\n\r\nYou can notice that the code as written in the documentation will raise an exception. To fix that you should access `LinearClassifier` as `tf.estimator.LinearClassifier`\r\n\r\n@rmlarsen for reference, this is the official documentation for `LinearClassifier`:\r\nhttps://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier", "@TwistedHardware Thanks!", "@rmlarsen I don't know if this is normal but I just wanted to get your attention to one of the checks that has been running for several days.\r\n`Windows CMake Pending \u2014 Internal CI build started.`", "@TwistedHardware Thanks!"]}, {"number": 19217, "title": "R0.11", "body": "MNIST", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "5.11"]}, {"number": 19216, "title": "import tensorflow as tf error", "body": "installed from anaconda using\r\n`pip  install  tensorflow`\r\n\r\nOS Platform:\r\nWindows 7 64bit\r\nintel processor\r\nintel hd graphic graphic card\r\npython version 3.6.4\r\n\r\nHave I written custom code: No\r\nBazel version : I don't have\r\nCUDA/cuDNN version : I don't have\r\nGPU model and memory :N\\A\r\nExact command to reproduce : import tensorflow as tf using jupyter\r\n\r\n\r\n```\r\n(base) C:\\Users\\david>pip install tensorflow\r\nCollecting tensorflow\r\n  Downloading https://files.pythonhosted.org/packages/f4/88/980d7032b7408fcca5b0\r\nb8d420fcd97919197a9e7acf280ab74fc7db6993/tensorflow-1.8.0-cp36-cp36m-win_amd64.w\r\nhl (34.4MB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 34.4MB 13kB/s\r\nCollecting astor>=0.6.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/b2/91/cc9805f1ff7b49f62013\r\n6b3a7ca26f6a1be2ed424606804b0fbcf499f712/astor-0.6.2-py2.py3-none-any.whl\r\nCollecting gast>=0.2.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e\r\n789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\r\nRequirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-\r\npackages (from tensorflow)\r\nCollecting tensorboard<1.9.0,>=1.8.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b\r\n2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1\r\nMB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 3.1MB 682kB/s\r\nRequirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\sit\r\ne-packages (from tensorflow)\r\nCollecting termcolor>=1.1.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2\r\na4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\r\nRequirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-\r\npackages (from tensorflow)\r\nCollecting grpcio>=1.8.6 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/80/7e/d5ee3ef92822b01e3a27\r\n4230200baf2454faae64e3d7f436b093ff771a17/grpcio-1.11.0-cp36-cp36m-win_amd64.whl\r\n(1.4MB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 1.4MB 787kB/s\r\nCollecting protobuf>=3.4.0 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/32/cf/6945106da76db9b62d11\r\nb429aa4e062817523bb587018374c77f4b63200e/protobuf-3.5.2.post1-cp36-cp36m-win_amd\r\n64.whl (958kB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 962kB 787kB/s\r\nCollecting absl-py>=0.1.6 (from tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/90/6b/ba04a9fe6aefa56adafa\r\n6b9e0557b959e423c49950527139cb8651b0480b/absl-py-0.2.0.tar.gz (82kB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 92kB 708kB/s\r\nCollecting bleach==1.5.0 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d\r\n4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\r\nRequirement already satisfied: werkzeug>=0.11.10 in c:\\programdata\\anaconda3\\lib\r\n\\site-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\nCollecting markdown>=2.6.8 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/6d/7d/488b90f470b96531a3f5\r\n788cf12a93332f543dbab13c423a5e7ce96a0493/Markdown-2.6.11-py2.py3-none-any.whl (7\r\n8kB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 81kB 682kB/s\r\nCollecting html5lib==0.9999999 (from tensorboard<1.9.0,>=1.8.0->tensorflow)\r\n  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfa\r\nf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\r\n    100% |\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6\u00a6| 890kB 787kB/s\r\nRequirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-p\r\nackages (from protobuf>=3.4.0->tensorflow)\r\nBuilding wheels for collected packages: gast, termcolor, absl-py, html5lib\r\n  Running setup.py bdist_wheel for gast ... done\r\n  Stored in directory: C:\\Users\\david\\AppData\\Local\\pip\\Cache\\wheels\\9a\\1f\\0e\\3c\r\nde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\r\n  Running setup.py bdist_wheel for termcolor ... done\r\n  Stored in directory: C:\\Users\\david\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc\r\n84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\r\n  Running setup.py bdist_wheel for absl-py ... done\r\n  Stored in directory: C:\\Users\\david\\AppData\\Local\\pip\\Cache\\wheels\\23\\35\\1d\\48\r\nc0a173ca38690dd8dfccfa47ffc750db48f8989ed898455c\r\n  Running setup.py bdist_wheel for html5lib ... done\r\n  Stored in directory: C:\\Users\\david\\AppData\\Local\\pip\\Cache\\wheels\\50\\ae\\f9\\d2\r\nb189788efcf61d1ee0e36045476735c838898eef1cad6e29\r\nSuccessfully built gast termcolor absl-py html5lib\r\nInstalling collected packages: astor, gast, html5lib, bleach, markdown, protobuf\r\n, tensorboard, termcolor, grpcio, absl-py, tensorflow\r\n  Found existing installation: html5lib 1.0.1\r\n    Uninstalling html5lib-1.0.1:\r\n      Successfully uninstalled html5lib-1.0.1\r\n  Found existing installation: bleach 2.1.2\r\n    Uninstalling bleach-2.1.2:\r\n      Successfully uninstalled bleach-2.1.2\r\nSuccessfully installed absl-py-0.2.0 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-\r\n1.11.0 html5lib-0.9999999 markdown-2.6.11 protobuf-3.5.2.post1 tensorboard-1.8.0\r\n tensorflow-1.8.0 termcolor-1.1.0\r\nYou are using pip version 9.0.1, however version 10.0.1 is available.\r\nYou should consider upgrading via the 'python -m pip install --upgrade pip' comm\r\nand.\r\n```\r\n\r\n\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     13         try:\r\n---> 14             return importlib.import_module(mname)\r\n     15         except ImportError:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py in _gcd_import(name, package, level)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py in _find_and_load(name, import_)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py in _load_unlocked(spec)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py in module_from_spec(spec)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap_external.py in create_module(self, spec)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nModuleNotFoundError                       Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>()\r\n     16             return importlib.import_module('_pywrap_tensorflow_internal')\r\n---> 17     _pywrap_tensorflow_internal = swig_import_helper()\r\n     18     del swig_import_helper\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     15         except ImportError:\r\n---> 16             return importlib.import_module('_pywrap_tensorflow_internal')\r\n     17     _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py in import_module(name, package)\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-306-64156d691fe5> in <module>()\r\n----> 1 import tensorflow as tf\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>()\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 # pylint: disable=wildcard-import\r\n     26 from tensorflow.tools.api.generator.api import *  # pylint: disable=redefined-builtin\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>()\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>()\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 14, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 17, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 16, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version", "Might be duplicate of #17386. This might fix your issue. https://github.com/tensorflow/tensorflow/issues/17386#issuecomment-370129452", "It's most likely the case that you purchased your Intel CPU before 2012. You have the following options on your existing hardware:\r\n\r\n1. Build TF from source.\r\n2. Use a slightly older TF release binary (when we supported k8-sse3+)\r\n3. Install an unofficial binary release created a community member: https://github.com/tensorflow/tensorflow/issues/17386#issuecomment-370129452", "I could solve it by installing an earlier version\r\n`pip install tensorflow == 1.5.0`\r\n\r\nThank", "yes, install old version solved this. Thanks @loboere .and after trying 1.5.0 , I have installed 1.9.0 and it's work too", "Maybe it's from your CPU is rather old to support AVX instructions ...\r\nchange your tensorflow version to 1.5.0 by  \"pip install tensorflow==1.5.0\"\r\nor search for an unofficial pre-built binary of TF >= 1.6 that does not support AVX\r\nlook at this issue in StackOverflow ...\r\nhttps://stackoverflow.com/questions/50430086/issue-installing-tensorflow-not-a-cuda-cudnn-issue", "The problem is that I think there is an imcompatibility issue.\r\nWhen installing tensorflow-gpu directly, did not work. \r\nI solved the problem as. \r\n\r\nFirst, I created the conda environment as following:\r\n**_conda create -n myenv  python=3.6\r\nconda activate myenv_** \r\nThen I installed the keras-gpu=2.2.2 \r\n**_conda install keras-gpu=2.2.2_** #this command also installed the tensorflow  ", "I have faced these type of  problems ,\r\n-Like Tensorflow =2.0.0 (latest version )  incompatible   with 3.7 python .\r\n-Like in python 3.7 version tensorflow doesn't work properly so install python 3.6 version or before version.\r\nAnd working windows 7 version laptops and desktops (manufactured before 2012 or 2013)\r\nSolution  for It.\r\n### command in anacoda prompt\r\n**$ _conda create -n tensorflow_cpu pip python=3.6_**  (to to make a virtual environment in same place with version)\r\n$ **_activate tensorflow_cpu_**   (these command is used to activate the enviroment)\r\n### installation of tensorlflow\r\npip install --ignore-installed --upgrade tensorflow==1.5.0\r\n.. \r\n\r\n\"THESE WILL WORK PROBABLY\"  if yes or no comment ..\r\n\r\n\r\n", "> I have faced these type of problems ,\r\n> -Like Tensorflow =2.0.0 (latest version ) incompatible with 3.7 python .\r\n> -Like in python 3.7 version tensorflow doesn't work properly so install python 3.6 version or before version.\r\n> And working windows 7 version laptops and desktops (manufactured before 2012 or 2013)\r\n> Solution for It.\r\n> \r\n> ### command in anacoda prompt\r\n> **$ _conda create -n tensorflow_cpu pip python=3.6_** (to to make a virtual environment in same place with version)\r\n> $ **_activate tensorflow_cpu_** (these command is used to activate the enviroment)\r\n> \r\n> ### installation of tensorlflow\r\n> pip install --ignore-installed --upgrade tensorflow==1.5.0\r\n> ..\r\n> \r\n> \"THESE WILL WORK PROBABLY\" if yes or no comment ..\r\n\r\nthis works for me", "buen d\u00eda tengo el siguiente error, su ayuda ser\u00eda de mucha ayuda.\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-29f498619f5c> in <module>\r\n      4 import keras\r\n      5 import matplotlib.pyplot as plt\r\n----> 6 import tensorflow as tf\r\n      7 from keras import applications\r\n      8 from keras.utils import to_categorical\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     26 \r\n     27 # pylint: disable=g-bad-import-order\r\n---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     29 from tensorflow.python.tools import module_util as _module_util\r\n     30 \r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     81 from tensorflow.python import data\r\n     82 from tensorflow.python import distribute\r\n---> 83 from tensorflow.python import keras\r\n     84 from tensorflow.python.feature_column import feature_column_lib as feature_column\r\n     85 from tensorflow.python.layers import layers\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py in <module>\r\n     25 \r\n     26 from tensorflow.python.keras import activations\r\n---> 27 from tensorflow.python.keras import applications\r\n     28 from tensorflow.python.keras import backend\r\n     29 from tensorflow.python.keras import callbacks\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\__init__.py in <module>\r\n     24 from tensorflow.python.keras import backend\r\n     25 from tensorflow.python.keras import engine\r\n---> 26 from tensorflow.python.keras import layers\r\n     27 from tensorflow.python.keras import models\r\n     28 from tensorflow.python.keras import utils\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py in <module>\r\n     27 \r\n     28 # Advanced activations.\r\n---> 29 from tensorflow.python.keras.layers.advanced_activations import LeakyReLU\r\n     30 from tensorflow.python.keras.layers.advanced_activations import PReLU\r\n     31 from tensorflow.python.keras.layers.advanced_activations import ELU\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\advanced_activations.py in <module>\r\n     25 from tensorflow.python.keras.engine.base_layer import Layer\r\n     26 from tensorflow.python.keras.engine.input_spec import InputSpec\r\n---> 27 from tensorflow.python.keras.utils import tf_utils\r\n     28 from tensorflow.python.ops import math_ops\r\n     29 from tensorflow.python.util.tf_export import keras_export\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\__init__.py in <module>\r\n     37 from tensorflow.python.keras.utils.layer_utils import print_summary\r\n     38 from tensorflow.python.keras.utils.losses_utils import squeeze_or_expand_dimensions\r\n---> 39 from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n     40 from tensorflow.python.keras.utils.np_utils import normalize\r\n     41 from tensorflow.python.keras.utils.np_utils import to_categorical\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\multi_gpu_utils.py in <module>\r\n     20 from tensorflow.python.framework import ops\r\n     21 from tensorflow.python.keras import backend as K\r\n---> 22 from tensorflow.python.keras.engine.training import Model\r\n     23 from tensorflow.python.ops import array_ops\r\n     24 from tensorflow.python.util.tf_export import keras_export\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in <module>\r\n     38 from tensorflow.python.keras import optimizers\r\n     39 from tensorflow.python.keras.distribute import distributed_training_utils\r\n---> 40 from tensorflow.python.keras.engine import network\r\n     41 from tensorflow.python.keras.engine import training_arrays\r\n     42 from tensorflow.python.keras.engine import training_distributed\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py in <module>\r\n     37 from tensorflow.python.keras import backend\r\n     38 from tensorflow.python.keras import callbacks\r\n---> 39 from tensorflow.python.keras import saving\r\n     40 from tensorflow.python.keras.engine import base_layer\r\n     41 from tensorflow.python.keras.engine import base_layer_utils\r\n\r\n~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\__init__.py in <module>\r\n     31 from tensorflow.python.keras.saving.save import load_model\r\n     32 from tensorflow.python.keras.saving.save import save_model\r\n---> 33 from tensorflow.python.keras.saving.saved_model import export_saved_model\r\n     34 from tensorflow.python.keras.saving.saved_model import load_from_saved_model\r\n     35 from tensorflow.python.keras.saving.saving_utils import trace_model_call\r\n\r\nImportError: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model' (C:\\Users\\DANTE\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\saved_model\\__init__.py)\r\n"]}, {"number": 19215, "title": "Improve the shape function for ParameterizedTruncatedNormal", "body": "The parameters of ParameterizedTruncatedNormal should\r\nbe 0-D or 1-D, which is checked in ther kernel functions.\r\nThere is no check in the shape function of the ops.\r\n\r\nThis fix improves the shape function and checks the\r\nparameters of ParameterizedTruncatedNormal whever possible.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19214, "title": "for loop not working in tf.contrib.autograph", "body": "The following example for autograph is not working (see my system-info below):\r\n\r\n```\r\nfrom tensorflow.contrib import autograph\r\n\r\ndef computation():\r\n    for k in range(4):\r\n        print(k)\r\n    return tf.no_op()\r\n\r\ncomputation_autographd = autograph.to_graph(computation, verbose=True)\r\n```\r\n\r\nIf I remove the for loop, it works fine.\r\nWith the loop, it fails with:\r\n\r\n```\r\nFile \"/opt/project/examples/_dict_in_autograph_example.py\", line 66, in <module>\r\nverbose=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/impl/api.py\", line 245, in to_graph\r\n    compiled_node, compiled_src = compiler.ast_to_object(module)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/pyct/compiler.py\", line 75, in ast_to_object\r\n    return imp.load_source(module_name, f.name), source\r\n  File \"/usr/lib/python3.5/imp.py\", line 172, in load_source\r\n    module = _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 693, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 673, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 661, in exec_module\r\n  File \"<frozen importlib._bootstrap_external>\", line 767, in get_code\r\n  File \"<frozen importlib._bootstrap_external>\", line 727, in source_to_code\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\n  File \"/tmp/tmpqdvecllf.py\", line 21\r\n    () = __ops.for_loop(autograph_utils.dynamic_builtin(range, 4),\r\n       ^\r\nSyntaxError: can't assign to ()\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\npython3 --version\r\nPython 3.5.2\r\n\r\npython3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 5a7b67a966f3 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 5a7b67a966f3 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.8.0\r\ntf.GIT_VERSION = v1.8.0-0-g93bc2e2072\r\ntf.COMPILER_VERSION = v1.8.0-0-g93bc2e2072\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Sorry for the delay! This is a known issue -- we still need to properly handle the cases where a code block (in this case, the body of the for loop) doesn't calculate any numeric values.\r\n\r\nFor now, you can work around this limitation by having the loop perform some computation on an existing variable, like for instance:\r\n\r\n```\r\ndef computation():\r\n    s = 0\r\n    for k in range(tf.constant(4)):\r\n        print(k)\r\n        s += k\r\n    return s\r\n```\r\n\r\nSide note: we also need to make sure the argument passed to range is a tensor, otherwise the loop will be statically unrolled. We're still debating whether to keep this behavior because it's been a source of confusion.", "Quick update: this now works in TF2 with [@tf.function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function) (which uses a stable subset of autograph):\r\n\r\n```\r\n@tf.function\r\ndef computation():\r\n    for k in tf.range(4):\r\n        print(k)\r\n```", "Final update - the loop uses the type to decide whether to run as a Python or TF loop. So, if the iterated is a Tensor, then the loop will run in TF. Otherwise it stays in Python.\r\n\r\nExamples:\r\n\r\n```\r\nfor k in range(4):  # Python loop\r\nfor k in [1, 2, 3]:  # Python loop\r\n\r\nfor k in tf.range(4):  # TF loop\r\nfor k in tf.data.Dataset.range(4):  # TF loop\r\nfor k in tf.constant([1, 2, 3]):  # TF loop\r\n```\r\n\r\nWe found that this is the way that's most consistent with other instances, like for instance overloaded operators (e.g. `a + b` runs in TF if either `a` or `b` is a tensor).\r\n\r\n"]}, {"number": 19213, "title": "Improve tf.constant() for float16 with fast_tensor_util", "body": "This fix tries to fix the issue raised in #19180 where tf.constant() for float16 is too slow as there is no\r\nfast_tensor_util support.\r\n\r\nThis fix adds fast_tensor_util by casting float16 to uint16. Since numpy natively stores float16 as uint16, the conversion should be ok.\r\n\r\nThis fix fixes #19180.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Close the PR for now as there is another PR pending."]}, {"number": 19212, "title": "Add `AppendFloat16ArrayToTensorProto` to acclerate `tf.constant` for float16", "body": "Related with #19180.\r\nIt seems that cython doesn't support `np.float16_t` by now, we use `np.uint16` instead. \r\n\r\nThe conversion still be time-consuming (25 times slower than float32), however it is better than original slow implementation (260 times slower than float32).\r\n\r\nPerformance comparison:\r\n```\r\nbefore:\r\nfloat32: 0.0535 sec\r\nfloat16: 13.7567 sec\r\n\r\nafter:\r\nfloat32: 0.0496 sec\r\nfloat16: 1.0815 sec\r\n```\r\n\r\nscript:\r\n```python\r\nimport time\r\nimages = np.random.rand(128, 100, 100, 3)\r\nimagesFloat16 = images.astype(np.float16)\r\nimagesFloat32 = images.astype(np.float32)\r\n\r\nstart = time.time()\r\nconstant_op.constant(imagesFloat32)\r\nend = time.time()\r\nprint(\"float32: {0:.4f} sec\".format(end - start))\r\n\r\nstart = time.time()\r\nconstant_op.constant(imagesFloat16)\r\nend = time.time()\r\nprint(\"float16: {0:.4f} sec\".format(end - start))\r\n```", "comments": ["@facaiy Thanks for the improvement!", "The failure seems unrelated?\r\n\r\n```python\r\n======================================================================\r\n261\r\nFAIL: testPostPruningOfAllNodes (__main__.UpdateTreeEnsembleOpTest)\r\n262\r\nTest growing an ensemble with post-pruning, with all nodes are pruned.\r\n263\r\n----------------------------------------------------------------------\r\n264\r\nTraceback (most recent call last):\r\n265\r\n  File \"T:/src/github/tensorflow/tensorflow/python/kernel_tests/boosted_trees/training_ops_test.py\", line 1393, in testPostPruningOfAllNodes\r\n266\r\n    \"\"\", res_ensemble)\r\n267\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 881, in assertProtoEquals\r\n268\r\n    self._AssertProtoEquals(expected_message, message, msg=msg)\r\n269\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 858, in _AssertProtoEquals\r\n270\r\n    compare.assertProtoEqual(self, a, b, normalize_numbers=True, msg=msg)\r\n271\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\util\\protobuf\\compare.py\", line 107, in assertProtoEqual\r\n272\r\n    msg=msg)\r\n273\r\nAssertionError: 'tree[570 chars]4.0143\\n  }\\n}\\ntree_metadata {\\n}\\ngrowing_me[88 chars]n}\\n' != 'tree[570 chars]4.014299\\n  }\\n}\\ntree_metadata {\\n}\\ngrowing_[90 chars]n}\\n'\r\n274\r\nDiff is 849 characters long. Set self.maxDiff to None to see it. :\r\n275\r\n276\r\n----------------------------------------------------------------------\r\n```", "Thanks!"]}, {"number": 19211, "title": "TensorRT engine requires consistent batch size", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, custom SSD model\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: nvidia gpu cloud docker container 18.04 for V100\r\n                                                                                from source for 1080 Ti\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9 / 7\r\n- **GPU model and memory**: V100 (16GB) 1080Ti (11GB)\r\n- **Exact command to reproduce**: N/A \r\n\r\n### Source code / logs\r\nRun TF-TRT with custom SSD and got the following message:\r\nF tensorflow/contrib/tensorrt/shape_fn/trt_shfn.cc:52] TensorRT engine requires consistent batch size\r\n\r\nAfter commenting out the line 52 in trt_shfn.cc and recompile with bazel, the model ran properly and output expected results. Suspect that this dimension check has a bug and does not cover all the cases. \r\n\r\nUse the code example from [here](https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz) and modify to load custom SSD model.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "Just updated the field. Thanks!", "/CC @aaroey", "@samikama ", "The error there seems to be a little more restrictive than it is. An engine can run a smaller batch size than it is built for, but with less efficiency than it could if it was built for that batch size. However it can not run a batch size bigger than it is built for so it is not really wrong. We are going to change these code soon for other reasons and I will update the logic there then.", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This will be fixed in upcoming PRs.", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "With #19871 we have less dependence on to the batch size and will be able to create a new engine on the fly at the expense of compute time.", "Nagging Assignee @samikama: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @samikama: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This should be fixed with recent updates, #20350 specifically."]}, {"number": 19210, "title": "Fix cublas wrap macro for cublasGemmBatchedEx", "body": "PR #18436 breaks tensorflow build for cuda 9.1. It uses PERFTOOLS_GPUTOOLS_CUBLAS_WRAP instead of STREAM_EXECUTOR_CUBLAS_WRAP. This PR fixes that issue. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "The failing test lools unrelated. Merging."]}, {"number": 19209, "title": "Use passed name for leaky relu tensor op", "body": "I was experimenting with different activation functions for the final layer of my graph recently when I noticed that the output graph was failing to save because it couldn't find a tensor by a name I had provided, e.g. (`my_final_tensor_op`).\r\n\r\nIt worked correctly with an activation function like `tf.nn.sigmoid`:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\r\nsigmoid_tensor = tf.nn.sigmoid(sample_values, name='my_final_tensor_op')\r\nsigmoid_tensor.name\r\n>>> 'my_final_tensor_op:0'\r\n```\r\n\r\nBut for `tf.nn.leaky_relu` I noticed that a `/Maximum` gets appended to whatever `name` value is passed:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\r\nleaky_relu_tensor = tf.nn.leaky_relu(sample_values, name='my_final_tensor_op')\r\nleaky_relu_tensor.name\r\n>>> 'my_final_tensor_op/Maximum:0'\r\n```\r\n\r\nI suspect the reason why is that the `name` parameter [is not passed in the call](https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/nn_ops.py#L1604) to the `math_ops.maximum` function:\r\n```\r\nreturn math_ops.maximum(alpha * features, features)\r\n```\r\n\r\nCompare this to the case of `tf.nn.sigmoid`, which [does pass in](https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/math_ops.py#L2342) the `name` parameter into the function call that it returns:\r\n```\r\nreturn gen_math_ops.sigmoid(x, name=name)\r\n```\r\n\r\nThis PR makes the change to have the `leaky_relu` function pass `name` to the `math_ops.maximum` function so that the desired name for the op carries down. I also added a unit test that addresses this specific functionality.\r\n\r\nOne potential issue that could come up is if there's a lot of existing code that expects the `/Maximum` string to be appended, such as in the case where no `name` is set and the tensor op's name becomes `LeakyRelu/Maximum:0`. If that's the case, I would at least like to change the method's documentation so that the caller is aware of the `/Maximum` string concatenation side effect.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I made one small change (\"with ops.name_scope(...) as name:\") based on the other functions in this file work, but otherwise, looks good.  You probably have to sign the CLA before we can accept this change.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@vrv thanks for the review.", "There was a small bug in the test - think I got confused between `nn` and `np`. \ud83d\ude1d \r\n\r\nAnyhow, I corrected the function call so that it matches how it works in the other leaky relu tests.", "@jdgumz Please address the linter errors: https://source.cloud.google.com/results/invocations/bd5cab36-e615-4bb3-859e-0c6bd51d9869/log", "@rmlarsen Done!\r\n\r\nLooks like the pylint errors were:\r\n```\r\nFAIL: Found 2 non-whitelited pylint errors:\r\ntensorflow/python/ops/nn_test.py:967: [C0301(line-too-long), ] Line too long (99/80)\r\ntensorflow/python/ops/nn_test.py:969: [C0301(line-too-long), ] Line too long (81/80)\r\n```\r\n\r\nI shortened the lines in the test and ran pylint locally before pushing the latest commit. I didn't see the failures come up again, so hopefully it's resolved now.", "@jdgumz Thanks!", "@rmlarsen \r\n@vrv \r\n\r\nI took a look at the `Windows CMake` build failure and it seems like it's under the `python/kernel_tests/boosted_trees` test coverage. I don't think my changes are related to the failure, but I wonder if perhaps my branch is behind and is missing a commit that addresses the test failure. Will I need to rebase the branch to perhaps fix the problem?", "@jdgumz Thanks. The failure seems unrelated to your change."]}, {"number": 19208, "title": "rnn_cell not Checkpointable", "body": "tf-nightly: 1.8.0.dev20180329 (latest Windows 10 build)\r\n\r\nSample Code:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\nfrom tensorflow.python.training import checkpointable\r\n\r\n\r\nclass MyModel(checkpointable.Checkpointable):\r\n\r\n  def __init__(self):\r\n    self.cell = tf.nn.rnn_cell.DropoutWrapper(\r\n        tf.nn.rnn_cell.BasicLSTMCell(10), 0.5)\r\n    self.cell2 = tf.nn.rnn_cell.BasicLSTMCell(10)\r\n\r\n\r\nmodel = MyModel()\r\nstate = tf.nn.rnn_cell.LSTMStateTuple(\r\n    c=tf.constant([[10] * 10] * 10, dtype=tf.float32),\r\n    h=tf.constant([[10] * 10] * 10, dtype=tf.float32))\r\nmodel.cell(tf.constant([[10]] * 10, dtype=tf.float32), state)\r\nmodel.cell2(tf.constant([[10]] * 10, dtype=tf.float32), state)\r\n\r\ncheckpoint_path = 'modeldir/test/'\r\ncheckpoint = tfe.Checkpoint(model=model)\r\nsaver = tfe.CheckpointableSaver(checkpoint)\r\n\r\nfrom tensorflow.contrib.eager.python import checkpointable_utils\r\nprint(checkpointable_utils._serialize_object_graph(saver._root_checkpointable))\r\n\r\n```\r\n\r\nPrints:\r\n```\r\n({'model/cell2/kernel/.ATTRIBUTES/VARIABLE_VALUE': <tf.Variable 'basic_lstm_cell_1/kernel:0' shape=(11, 40) dtype=float32_ref>, 'model/cell2/bias/.ATTRIBUTES/VARIABLE_VALUE': <tf.Variable 'basic_lstm_cell_1/bias:0' shape=(40,) dtype=float32_ref>}, nodes {\r\n  children {\r\n    node_id: 1\r\n    local_name: \"model\"\r\n  }\r\n}\r\nnodes {\r\n  children {\r\n    node_id: 2\r\n    local_name: \"cell\"\r\n  }\r\n  children {\r\n    node_id: 3\r\n    local_name: \"cell2\"\r\n  }\r\n}\r\nnodes {\r\n}\r\nnodes {\r\n  children {\r\n    node_id: 4\r\n    local_name: \"kernel\"\r\n  }\r\n  children {\r\n    node_id: 5\r\n    local_name: \"bias\"\r\n  }\r\n}\r\nnodes {\r\n  attributes {\r\n    name: \"VARIABLE_VALUE\"\r\n    full_name: \"basic_lstm_cell_1/kernel\"\r\n    checkpoint_key: \"model/cell2/kernel/.ATTRIBUTES/VARIABLE_VALUE\"\r\n  }\r\n}\r\nnodes {\r\n  attributes {\r\n    name: \"VARIABLE_VALUE\"\r\n    full_name: \"basic_lstm_cell_1/bias\"\r\n    checkpoint_key: \"model/cell2/bias/.ATTRIBUTES/VARIABLE_VALUE\"\r\n  }\r\n}\r\n)\r\n```\r\n\r\nExpected to contain `bias` and `kernel` for both `cell` and `cell2` but only contains them for the cell without `DropoutWrapper`.", "comments": ["@allenlavoie Maybe that is something for you. I am not sure.", "Same goes for `tf.contrib.rnn.DeviceWrapper`.", "Thank you for the report! I've sent a fix out for review (and then it'll sync to github and mark this as closed).\r\n\r\nFor now I'll just have wrappers depend on their cells. It would be nice to have them inherit their cell's dependencies too, so that adding a wrapper around a cell doesn't break an existing checkpoint [contributions welcome :)]. But when adding a wrapper you can add a skip dependency on the cell to get the same effect (or going the other way can add a random Checkpointable object in place of the wrapper).", "Great to here, can't wait to use it.", "Now, when trying to print the checkpoint graph with `checkpointable_utils._serialize_object_graph(ckpt, None)` it will throw:\r\n```\r\n/python3.5/site-packages/tensorflow/python/keras/_impl/keras/engine/base_layer.py\", line 196, in name\r\n    return self._name\r\nAttributeError: 'DropoutWrapper' object has no attribute '_name'\r\n```", "Oh I see, DropoutWrapper isn't calling its parent constructor so get_config() fails. Easy enough to fix. Thank you for the reports!", "You're welcome :)", "Would this be a valid approach to support lists of variables for checkpointing?\r\n```python\r\nclass MyModel(checkpointable.Checkpointable):\r\n\r\n  def __init__(self):\r\n    variables = [tf.get_variable('var{}'.format(i), []) for i in range(10)]\r\n    for var in variables:\r\n      setattr(self, var.name, var)\r\n```\r\n\r\nTo answer my own question, probably not due to scope names with `/`.\r\n\r\nUpdate: Interestingly this works `setattr` seems to circumvent the python naming conventions and your restoration code does not care about the `/` and `:`.", "Yeah, this is what several of the eager examples do: e.g. https://github.com/tensorflow/tensorflow/blob/d8f9538ab48e3c677aaf532769d29bc29a05b76e/tensorflow/contrib/eager/python/examples/rnn_colorbot/rnn_colorbot.py#L211\r\n\r\nIt's fine, but we should add checkpointable data structures (which also forward Layers). I'm almost done with an append-only list datastructure. Whether we support regular Python lists is another interesting question (somewhat tricky semantics with restore-on-create, but doable...).", "Btw. `checkpoint.save()` also throws the dropout name error. Seems to be related to `get_config`.\r\nThank you for the link.", "Yep, will add some end-to-end tests for saving wrappers this time.\r\n\r\nI'd note on the setattr() issue that using \"var.name\" as a dependency name is rather fragile. If you make another instance of the object in the same Graph, the name will be different, which means the objects will be checkpoint incompatible (and changes elsewhere in the program that change \"var.name\" might break checkpoints for this object). ", "Good point, I will change that in my code.", "Do you have a date for the name fix? I would really like to use checkpoint and this bug puts me on hold. ", "Should be whenever the next push is. In the meantime, the fix is just to add `super(DropoutWrapper, self).__init__()` as the [first line of `DropoutWrapper`'s `__init__`](https://github.com/tensorflow/tensorflow/blob/a9761960e282cdcf0822951dec86372181f0e88e/tensorflow/python/ops/rnn_cell_impl.py#L981). Once it syncs, the change will add that to a few other wrappers and add some unit tests.", "How can this error occur? And what exactly does it mean? I am not sure how I caused it.\r\n```\r\nload_status.initialize_or_restore(self.session)\r\n  File \"/home/lib/python3.5/site-packages/tensorflow/python/training/checkpointable_utils.py\", line 677, in initialize_or_restore\r\n    checkpointable_objects = list_objects(self._root_checkpointable)\r\n  File \"/home/lib/python3.5/site-packages/tensorflow/python/training/checkpointable_utils.py\", line 472, in list_objects\r\n    object_names=object_names)\r\n  File \"/home/lib/python3.5/site-packages/tensorflow/python/training/checkpointable_utils.py\", line 291, in _serialize_slot_variables\r\n    \"A slot variable was re-used as a dependency of a \"\r\nNotImplementedError: A slot variable was re-used as a dependency of a Checkpointable object. This is not currently allowed. File a feature request if this limitation bothers you.\r\n```\r\n\r\nUpdate: \r\nSeems to be related to some `AdamOptimizer` variables.\r\nSo does that mean I cannot use `Adam` with Checkpoints?\r\n", "@allenlavoie I have some remarks to the current solution.\r\nWhen using e.g. `DropoutWrapper` then it is no longer possible to switch between not using it e.g. during `inference` while using it during `training`. There are a lot of implementations currently that are programmed like this that do not work anymore. Same goes for `DeviceWrapper`. There either has to be an updated documentation stating that is important to keep these object wrappers when using checkpointing or a different solution is necessary that skips the wrapper in the dependency tree.\r\n\r\n```python\r\nif training:\r\n   cell = DropoutWrapper(cell, prob)\r\nelse:\r\n   # leave cell as is\r\n```\r\nThis code does not work when doing checkpointing!", "[Totally agree](https://github.com/tensorflow/tensorflow/issues/19208#issuecomment-388218570), we should do dependency forwarding so that removing a wrapper doesn't break the checkpoint. It's on my list.\r\n\r\nIsn't that a bit of an unusual use-case, though? My expectation is that the model is created in an __init__ before training/not training is known, then `training` is passed through to DropoutWrapper's call(). Creating the object conditionally (in call?) seems a bit wasteful, although I guess it doesn't matter so much unless eager execution is enabled.", "This issue also stemmed from the problem mentioned in one of my previous issues where overriding e.g. an LSTM cell caused checkpointing errors. The problem I see with the wrappers is that sometimes people implement somethings like a wrapper function to apply all kinds of wrappers to a cell and then get that cell from that function. (e.g. https://github.com/tensorflow/nmt/blob/365e7386e6659526f00fa4ad17eefb13d52e3706/nmt/model_helper.py#L427).\r\nThey probably did not have the checkpointing API in mind when they programmed this but for me it shows that some misunderstanding can happen when using both APIs (wrapper & checkpointing)", "In the NMT example, it's still only an issue if the wrappers are changed, right? I guess ideally MultiRNNCell would have the dependencies forwarded from its first child (so you could add a second cell, causing it to wrap in a MultiRNNCell, and still load the first cell from an existing checkpoint).", "Yes, If you would run inference where training is false it would probably fail to restore because they did not add the DropoutWrapper in that case. I myself would also not want to add the overhead of another wrapper I don\u2018t even use in the inference training just to make sure checkpointing works. Only objects (BasicLSTMCell) that add something that is worth restoring e.g. Variables should be considered during restoration. All other dependencies (here Wrapper) should not be mandatory during restoration"]}, {"number": 19207, "title": "AttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'", "body": "### System information\r\n- I am trying to use export_inference_graph.py within tensorflow/research/object_detection\r\n- Windows 10\r\n- Tensorflow installed from binary\r\n- Tensorflow Version b'v1.8.0-0-g93bc2e2072' 1.8.0\r\n- Python 3.6.4\r\n### Describe the problem\r\nI am trying to train a detector using a custom data set.  I have run through the training and produced the model.ckpt files (meta, index, etc).  The issue I'm running into is creating the frozen inference graph.  Running export_inference_graph.py always fails out with the error:\r\n\r\nAttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'\r\n\r\nAfter some mild searching online, I didn't uncover any hits or paths to go down to address this.  What might I be missing?\r\n\r\n### Source code / logs\r\ntensorflow\\research\\object_detection> python export_inference_graph.py --input_type i\r\nmage_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt\r\n-89076 --output_directory inference_graph\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 147, in <module>\r\n    tf.app.run()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"export_inference_graph.py\", line 143, in main\r\n    FLAGS.output_directory, input_shape)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\exporter.py\", line 447, in export_inference_graph\r\n    is_training=False)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\builders\\model_builder.py\", line 98, in build\r\n    add_background_class)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\builders\\model_builder.py\", line 178, in _build_ssd_model\r\n    ssd_config.anchor_generator)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\builders\\anchor_generator_builder.py\", line 59, in build\r\n    if ssd_anchor_generator_config.height_stride:\r\nAttributeError: 'SsdAnchorGenerator' object has no attribute 'height_stride'\r\n\r\n### Edit 5/14/2018:\r\nHave I written custom code: not for this instance.\r\nOS Platform : Windows 10\r\nBasel Version: NA\r\nCuda version l: NA\r\nGpu Model: NA\r\nCommand: tensorflow\\research\\object_detection> python export_inference_graph.py --input_type i\r\nmage_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt\r\n-89076 --output_directory inference_graph", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: not for this instance. \r\nOS Platform : Windows 10\r\nBasel Version: NA\r\nCuda version l: NA\r\nGpu Model: NA\r\nCommand: tensorflow\\research\\object_detection> python export_inference_graph.py --input_type i\r\nmage_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt\r\n-89076 --output_directory inference_graph\r\n", "Checking in on rohan100jain.", "Bumping @rohan100jain ", "Issue solved. It appears to be the mobilenet SSD V1 coco model doesn't have a height or width stride. Re training using another model proceeded with no issues. ", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19206, "title": "Can't pass eager tensors to Model.train_on_batch", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nOSX, and Fedora 28\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.8.0\r\n\r\n- **Python version**: \r\n3.6\r\n\r\nBazel version\r\nN/A\r\n\r\nCUDA/cuDNN version\r\nN/A\r\n\r\nGPU model and memory\r\nN/A\r\n\r\nExact command to reproduce\r\nSee description of the problem below.\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI can't pass eager tensors to Model.train_on_batch. I had to pass numpy arrays instead.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 37 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19205, "title": "Branch 196146716", "body": "", "comments": []}, {"number": 19204, "title": "Use passed name for leaky relu tensor op", "body": "I was experimenting with different activation functions for the final layer of my graph recently when I noticed that the output graph was failing to save because it couldn't find a tensor by a name I had provided, e.g. (`my_final_tensor_op`).\r\n\r\nIt worked correctly with an activation function like `tf.nn.sigmoid`:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\r\nsigmoid_tensor = tf.nn.sigmoid(sample_values, name='my_final_tensor_op')\r\nsigmoid_tensor.name\r\n>>> 'my_final_tensor_op:0'\r\n```\r\n\r\nBut for `tf.nn.leaky_relu` I noticed that a `/Maximum` gets appended to whatever `name` value is passed:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nsample_values = np.array([1.0, 2.0, 3.0], dtype=np.float64)\r\nleaky_relu_tensor = tf.nn.leaky_relu(sample_values, name='my_final_tensor_op')\r\nleaky_relu_tensor.name\r\n>>> 'my_final_tensor_op/Maximum:0'\r\n```\r\n\r\nI suspect the reason why is that the `name` parameter [is not passed in the call](https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/nn_ops.py#L1604) to the `math_ops.maximum` function:\r\n```\r\nreturn math_ops.maximum(alpha * features, features)\r\n```\r\n\r\nCompare this to the case of `tf.nn.sigmoid`, which [does pass in](https://github.com/tensorflow/tensorflow/blob/f91bd2f8c4b263dd5460e4398b94ad4823ce7a18/tensorflow/python/ops/math_ops.py#L2342) the `name` parameter into the function call that it returns:\r\n```\r\nreturn gen_math_ops.sigmoid(x, name=name)\r\n```\r\n\r\nThis PR makes the change to have the `leaky_relu` function pass `name` to the `math_ops.maximum` function so that the desired name for the op carries down. I also added a unit test that addresses this specific functionality.\r\n\r\nOne potential issue that could come up is if there's a lot of existing code that expects the `/Maximum` string to be appended, such as in the case where no `name` is set and the tensor op's name becomes `LeakyRelu/Maximum:0`. If that's the case, I would at least like to change the method's documentation so that the caller is aware of the `/Maximum` string concatenation side effect.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Will reopen pull request from a different forked repo (that isn't my personal one)."]}, {"number": 19203, "title": "Build Tensorflow 1.7 on Cuda 8.0", "body": "Hi I am trying to build tensorflow r1.7 branch with the following settings:\r\n\r\n* cuda 8.0\r\n* cudnn 7.0\r\n* gcc 5.4\r\n\r\nbut i got the following error:\r\n\r\n```\r\ntensorflow/core/kernels/neon/neon_depthwise_conv_op.cc:138:42:   required from here\r\n./tensorflow/core/kernels/neon/depthwiseconv_float.h:602:7: warning: variable 'fixed_input_depth' set\r\n but not used [-Wunused-but-set-variable]\r\n   int fixed_input_depth = 0;\r\n       ^\r\nINFO: From Compiling tensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc:\r\ntensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(113): warning: function \"tensorflow::<unnamed>::IdentityOp::operator()\" was declared but never referenced\r\n\r\ntensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(155): warning: function \"tensorflow::<unnamed>::BoundedOutputIterator::operator*\" was declared but never referenced\r\n\r\ntensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(165): warning: function \"tensorflow::<unnamed>::BoundedOutputIterator::operator+\" was declared but never referenced\r\n\r\ntensorflow/core/kernels/dynamic_partition_op_gpu.cu.cc(171): warning: function \"tensorflow::<unnamed>::BoundedOutputIterator::operator-\" was declared but never referenced\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_\r\nexpect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::Eigen::half, int, (bool)1> \") is n\r\not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_\r\nexpect\") from a __global__ function(\"tensorflow::GatherOpKernel<float, int, (bool)1> \") is not allowe\r\nd\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_\r\nexpect\") from a __global__ function(\"tensorflow::GatherOpKernel<double, int, (bool)1> \") is not allow\r\ned\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_\r\nexpect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<float> , int, (bool)1\r\n> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_\r\nexpect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<double> , int, (bool)\r\n1> \") is not allowed\r\n\r\n5 errors detected in the compilation of \"/tmp/tmpxft_00004427_00000000-7_dynamic_partition_op_gpu.cu.\r\ncpp1.ii\".\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:1702:1: output 'tensorflow/core/kernels/\r\n_objs/dynamic_partition_op_gpu/tensorflow/core/kernels/dynamic_partition_op_gpu.cu.pic.o' was not cre\r\nated\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:1702:1: not all outputs were created or\r\nvalid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 770.936s, Critical Path: 154.34s\r\nINFO: 1470 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Which version of bazel are you using?", "I am experiencing the same problem on CentOS 7 with\r\n- CUDA 8.0\r\n- cuDNN 7.0\r\n- GCC 4.8.5\r\n- Bazel 0.10.1 (also tried 0.12.0).\r\n\r\nEDIT: Forgot to mention I am using TF v1.7.1", "@chsigg Would TF still be compatible with CUDA8 or is that over?", "@chsigg  Does it has to do something with 3c245f52912612ed9d8e20245ddb5de055680969 ?", "As I stated in a reply in #19249, I'm having this problem, as well.\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.7\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):.13.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version: 8.0 / 6.0\r\n- GPU model and memory: Tesla P100, 16GB\r\n- Exact command to reproduce: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package", "I tried compiling TF 1.6.0 this morning, and here the problem is absent. The code compiled as expected.", "I had the same problem, but could successfully compile the whole package with following quick fix. Please change ./tensorflow/core/platform/macros.h\r\n\r\nfrom\r\n#define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n#define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n\r\nto\r\n\r\n#define TF_PREDICT_FALSE(x) (x)\r\n#define TF_PREDICT_TRUE(x) (x)\r\n\r\n", "@mrlhansen I tried compiling TF 1.6.0 \r\n\r\n```\r\nERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:2696:1: C++ compilation of rule '//tensorflow/core/kernels:argmax_op' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10529.210s, Critical Path: 2071.40s\r\nINFO: 4345 processes, local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "@karta0807913 This looks like an error in GCC and not something related to TF. I suspect you might be running out of memory. Try limiting the number of bazel jobs by adding e.g. --jobs=10 to the bazel command (after the build argument).", "@twjang Your quickfix also worked for me. Thanks for that.\r\nCan you explain, why this happens? Or is it just \"empirical fixing\"? What changed, that 2 year old code stops working? ", "So I believe I know what's going on here, and I think #19249 is indeed a dupe of this issue.\r\n\r\nThe issue is that in 3c245f52912612ed9d8e20245ddb5de055680969, the change from\r\n\r\n    #if defined(COMPILER_GCC3)\r\n\r\nto \r\n\r\n    #if TF_HAS_BUILTIN(__builtin_expect) || (defined(__GNUC__) && __GNUC__ >= 3)\r\n\r\nmeans that the `TF_PREDICT_*` macros are now defined to use `__builtin_expect`. When compiling for CPU, this isn't a problem.\r\n\r\nHowever, in the case of `--config=cuda`, it seems that `nvcc` doesn't recognize `__builtin_expect` as a compiler builtin, and simply assumes it's some function defined for the host, leading to the compilation error [above](https://github.com/tensorflow/tensorflow/issues/19203#issue-321998060).\r\n\r\nI believe a quick fix here would be to avoid using the prediction logic in the case of GPUs, eg by making the predicate something like\r\n\r\n    #if (!defined(__NVCC__)) && (TF_HAS_BUILTIN(__builtin_expect) || (defined(__GNUC__) && __GNUC__ >= 3))\r\n\r\n@jart WDYT? (If that sounds good, I'm happy to send a PR.)", "Wow, great find @craigcitro; I find that super surprising. I'll gladly approve a PR making the change you suggested.", "The patch works, and I finally compiled Tensorflow 1.8 for CUDA 8.0 version without errors. https://github.com/ghostplant/tensorflow-cuda8-cudnn6/releases/tag/tf1.8-py35-cuda8-cudnn6", "@zheng-xq I think this patch should be disabled for CUDA 9.0, otherwise kernel functions won't benefit this optimization on CUDA 9.0 then.", "@zheng-xq @ghostplant The patch does not work for me. I am compiling TF1.8 on CUDA 8.0GA2 with CUDNN 6.0.", "@ziruizhuang You can refer this script for compilation. https://github.com/ghostplant/tensorflow-cuda8-optimized/blob/master/Dockerfile.tf18-py35-cuda8-cudnn6021", "mark", "The patch works for Tensorflow 1.8 with \r\nCUDA 8.0\r\ncuDNN 7.0\r\nGCC 5.4.0\r\nBazel 0.13.1", "This issue is resolved. Closing.", "@craigcitro, thanks very much!"]}, {"number": 19202, "title": "[XLA] Allow for disabling of some XLA Dot tests", "body": "These 8 tests are a recent addition to the XLA CC test suite.  They are not using the XLA_TEST_F macro, which allows tests to be selectively disabled by a manifest.\r\n\r\n", "comments": []}, {"number": 19201, "title": "Add citation", "body": "", "comments": []}]