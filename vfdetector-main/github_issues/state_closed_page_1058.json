[{"number": 21548, "title": "how to build .tflite file and two outputs", "body": "this is my command:\r\nbazel build //tensorflow/contrib/lite/toco:toco\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco  \r\n --input_file=/tmp/frozen_graph.pb  \r\n --input_format=TENSORFLOW_GRAPHDEF  \r\n --output_format=TFLITE  \r\n --output_file=/tmp/mobilenet_v1_224.tflite  \r\n --inference_type=FLOAT  \r\n --input_arrays=input \r\n --output_arrays=MobilenetV1/Predictions/Reshape_1 \r\n --output_arrays=MobilenetV1/Logists \r\n --input_shares=1,224,224,3\r\n\r\n\r\nadd this .tflite in android demo , but only find one output. maybe  the .tflite  file load one output,  I  think this problem is this last output_arrays over coverage the next output_arrays. Could anyone can sovle this problem , very appreciate ", "comments": ["I'm a little confused. The command you are running doesn't seem to match your title? Where is the second pb? What are you trying to accomplish. Thanks!", "sorry , it`s my fault. it should be one lite file and two outputs. for example,\r\n intput:  Tensor(\"input:0\", shape=(1, 384, 384, 3), dtype=float32)\r\noutput: \r\nTensor(\"Depth/output:0\", shape=(192, 192, 1), dtype=float32) \r\nTensor(\"Saliency/output:0\", shape=(192, 192), dtype=float32)\r\n\r\nthanks @ aselle ", "You should use \r\n\r\n`--output_arrays=\"MobilenetV1/Predictions/Reshape_1\",\"MobilenetV1/Logists\"`", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please refer to @michaeltinsley's comment. Thanks!"]}, {"number": 21547, "title": "tf.image.decode_image cannot decode b64-encoded string in Python3.6", "body": "```python\r\nimport tensorflow as tf\r\nimport base64\r\n\r\nwith open('~/001.jpg', 'r') as img:\r\n\tencoded_string = base64.b64encode(img.read())\r\n\timage_tensor = tf.image.decode_image(encoded_string, channels=3)\r\n\tprint(image_tensor)\r\n```\r\n\r\nWhen run above code with Python 2, it is ok, the result is:\r\nTensor(\"decode_image/cond_jpeg/Merge:0\", dtype=uint8)\r\n\r\nHowever, when run with Python 3, it shows:\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte", "comments": ["It seems that [API](https://www.tensorflow.org/api_docs/python/tf/image/decode_image) declare its contents must be byte string:\r\n\r\n> contents: 0-D string. The encoded image bytes.", "This issue has completely nothing to do with tensorflow. This is a python usage issue.\r\n\r\nhttps://stackoverflow.com/questions/42339876/error-unicodedecodeerror-utf-8-codec-cant-decode-byte-0xff-in-position-0-in/42340744", "Hi, @ppwwyyxx  and @facaiy \r\n\r\nThanks for your reply. Solved it by explicitly adding the mode 'b' when open the image file:\r\n```python\r\nwith open('/home/lzhang/tmp/0000045/001.jpg', 'rb') as img:\r\n```"]}, {"number": 21546, "title": "[Intel MKL] Adding keras pip packages to MKL container builds.", "body": "@gunan @av8ramit Adding this package pulls in all of the other dependencies: \r\n\r\n- keras>=2.1.6\r\n- keras-preprocessing==1.0.2\r\n- pyyaml-3.13", "comments": []}, {"number": 21545, "title": "Update Kafka and boringssl library", "body": "This fix updates librdkafka to 0.11.5 and boringssl to 7f63442.\r\n\r\nRelated change with librdkafka: https://github.com/edenhill/librdkafka/releases/tag/v0.11.5\r\n\r\nBoringssl update is necessary here as well, as it contains OpenSSL API SSL[_CTX]_set1_sigalgs[_list]:\r\nhttps://boringssl-review.googlesource.com/c/boringssl/+/30304\r\n\r\nThe need for SSL[_CTX]_set1_sigalgs[_list] is in: edenhill/librdkafka#1896\r\n(same issue with previous version of openssl and libressl)", "comments": []}, {"number": 21544, "title": "OutOfRangeError: read less bytes than requested", "body": "The training works fine on my local computer and did not work on my Ubuntu Amazon EC2 server when I increased the size of training data. \r\n\r\n**The first error: [W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184: (out of range, Read less bytes than requested).**\r\n\r\nMy Ubuntu Amazon EC2 server has less memory, less CPU. Reading the error, I am not sure whether that is the problem. \r\n\r\nAny clue? Thanks.", "comments": ["Did you perhaps run out of disk space while writing the checkpoint?", "Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I think maybe it is out of memory error. After I increased the instance memory, the error went away. Thanks.", "Hey,\r\nI got the same error when I uploaded the zip files on Google Colab and then unzipped them using the !unzip command. Then I deleted each file keeping the directory intact and manually uploaded the files in each sub-directory. However, the error is still the same.\r\n![error_ocr](https://user-images.githubusercontent.com/48624180/61220699-813e6a00-a734-11e9-92f7-04280d3bff56.png)\r\n", "Same i also get the same error when i try to retrain the model using previous checkpoint in google colab.Any one solved it?", "I got the error because the process of uploading these files to server machine was interrupted, so **the files are corrupted**, after I successfully uploaded them again, the error ran away. So if you downloaded the corrupted files, **try to download them again and make sure the files are intact**. Hope it may help you!", "i also get the same error when i restored the model from hdfs.But when download model file to my local disk.it can restore successfully.i do not why", "I tried to interrupt while copying the saved_model. And got \"read than expected\".\r\nCopying again worked fine and smooth. I hope you download the right saved model too.", "> I got the error because the process of uploading these files to server machine was interrupted, so **the files are corrupted**, after I successfully uploaded them again, the error ran away. So if you downloaded the corrupted files, **try to download them again and make sure the files are intact**. Hope it may help you!\r\n\r\nThat's the case for me, I didn't even notice. Thx, bro."]}, {"number": 21543, "title": "ConditionalBijectors not Chainable due to Bijector _Mapping not supporting deep dicts", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX High Sierra 10.13.1\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: n/a\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: Python 3.6.5 :: Anaconda, Inc.\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\nI would like to use the [bijectors.Chain](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/chain.py) to create a flow of bijector that extend [bijectors.ConditionalBijector](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/conditional_bijector.py#L28). It looks like Chain [already supports conditioning under the hood](https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/bijectors/chain.py#L263), but the other parts of Bijector are not fully compatible with it.\r\n\r\nSuppose we have the following test function:\r\n\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ntfb = tfp.bijectors\r\n\r\n\r\ndef test_chained_condition_bijector(ChainClass):\r\n    flow = ChainClass([\r\n        BijectorWithSupportForConditioning(name=\"bijector_1\"),\r\n        BijectorWithSupportForConditioning(name=\"bijector_2\")\r\n    ])\r\n\r\n    x = tf.random_uniform([5])\r\n    flow.forward(\r\n        x,\r\n        bijector_1={'conditions': tf.random_uniform([2])},\r\n        bijector_2={'conditions': tf.random_uniform([3])},\r\n    )\r\n```\r\n\r\nCalling `test_chained_condition_bijector` with `bijector.Chain`, we get the following error:\r\n\r\n```python\r\n>>> test_chained_condition_bijector(tfb.Chain)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/kristian/code/softqlearning-private/broken_conditional_bijector.py\", line 52, in test_chained_condition_bijector\r\n    'bijector_2': { 'conditions': tf.random_uniform([3]) },\r\nTypeError: forward() got an unexpected keyword argument 'bijector_1'\r\n```\r\n\r\nThis error can be overcome by creating a `ConditionalChain` class, which extends `ConditionalBijector`:\r\n```python\r\nclass ConditionalChain(tfb.ConditionalBijector, tfb.Chain):\r\n    pass\r\n```\r\n\r\nNow, if we run the same code again, this time with ConditionalChain, we get the following:\r\n\r\n```python\r\n>>> test_chained_condition_bijector(ConditionalChain)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/kristian/code/softqlearning-private/broken_conditional_bijector.py\", line 52, in test_chained_condition_bijector\r\n    'bijector_2': { 'conditions': tf.random_uniform([3]) },\r\n  File \"/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow/python/ops/distributions/util.py\", line 1461, in _fn\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/conditional_bijector.py\", line 35, in forward\r\n    return self._call_forward(x, name, **condition_kwargs)\r\n  File \"/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bijector_impl.py\", line 750, in _call_forward\r\n    mapping = self._lookup(x=x, kwargs=kwargs)\r\n  File \"/Users/kristian/anaconda3/envs/softlearning/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bijector_impl.py\", line 1014, in _lookup\r\n    return self._from_x.get(mapping.x_key, mapping)\r\nTypeError: unhashable type: 'dict'\r\n```\r\n\r\nThis happens because the [_deep_tuple](https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/distributions/bijector_impl.py#L126), which is used to cache Bijector inputs/outputs, doesn't support nested dicts.\r\n\r\nI would expect the code to run normally with `ConditionalChain`. Not sure if this is a feature or a bug. If it's a bug, then this is a bug report, and if it's a feature, then this is a feature request to change the behavior. It's also possible that I'm misusing the ConditionalBijectors, in which case I should move this to stackoverflow.\r\n\r\nHere's the full code snippet to test the behavior:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ntf.enable_eager_execution()\r\n\r\n\r\ntfb = tfp.bijectors\r\n\r\n\r\nclass BijectorWithSupportForConditioning(tfb.ConditionalBijector):\r\n    def __init__(self, validate_args=False, name=\"exp\"):\r\n        super(BijectorWithSupportForConditioning, self).__init__(\r\n            validate_args=validate_args,\r\n            forward_min_event_ndims=0,\r\n            name=name)\r\n\r\n    def _concat_input(self, x, **condition_kwargs):\r\n        return tf.concat(\r\n            [x] + [condition_kwargs[k] for k in sorted(condition_kwargs)],\r\n            axis=-1)\r\n\r\n    def _forward(self, x, **condition_kwargs):\r\n        return tf.exp(self._concat_input(x, **condition_kwargs))\r\n\r\n    def _inverse(self, y, **condition_kwargs):\r\n        return tf.log(self._concat_input(y, **condition_kwargs))\r\n\r\n    def _inverse_log_det_jacobian(self, y, **condition_kwargs):\r\n        return -self._forward_log_det_jacobian(self._inverse(\r\n            self._concat_input(y, **condition_kwargs)))\r\n\r\n    def _forward_log_det_jacobian(self, x, **condition_kwargs):\r\n        return self._concat_input(x, **condition_kwargs)\r\n\r\n\r\nclass ConditionalChain(tfb.ConditionalBijector, tfb.Chain):\r\n    pass\r\n\r\n\r\ndef test_chained_condition_bijector(ChainClass):\r\n    flow = ChainClass([\r\n        BijectorWithSupportForConditioning(name=\"bijector_1\"),\r\n        BijectorWithSupportForConditioning(name=\"bijector_2\")\r\n    ])\r\n\r\n    x = tf.random_uniform([5])\r\n    flow.forward(\r\n        x,\r\n        bijector_1={'conditions': tf.random_uniform([2])},\r\n        bijector_2={'conditions': tf.random_uniform([3])},\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_chained_condition_bijector(tfb.Chain)\r\n    # test_chained_condition_bijector(ConditionalChain)\r\n```", "comments": ["I agree this is an \"API bug.\"  I'll try to address it ASAP but can't give an ETA at the moment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=21543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=21543\">No</a>\n"]}, {"number": 21542, "title": "fatal: not a git repository: 'E:/tensorflow/tensorflow/contrib/cmake/build/nsync/src/nsync/.git'", "body": "i want to use cmake+VS2015+win10 to compile tensorflow-CPU,to get tensorflow.dll and tensorflow.lib\uff0cin total 6 errors , \r\nbut always happen :\r\nCreating directories for 'zlib'\r\n  Building Custom Rule E:/tensorflow/tensorflow/contrib/cmake/CMakeLists.txt\r\n  CMake does not need to re-run because E:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/generate.stamp is up-to\r\n  -date.\r\n  Performing download step (git clone) for 'zlib'\r\n  Cloning into 'zlib'...\r\n  fatal: Out of memory, malloc failed (tried to allocate 947912704 bytes)\r\n  Cloning into 'zlib'...\r\n  fatal: Out of memory, malloc failed (tried to allocate 947912704 bytes)\r\n  Cloning into 'zlib'...\r\n  fatal: Out of memory, malloc failed (tried to allocate 947912704 bytes)\r\n  -- Had to git clone more than once:\r\n  CMake Error at E:/tensorflow/tensorflow/contrib/cmake/build/zlib/tmp/zlib-gitclone.cmake:66 (message):\r\n    Failed to clone repository: 'https://github.com/madler/zlib'\r\n 3 times.\r\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \u201ccmd.exe\u201d\u5df2\u9000 \u51fa\uff0c\r\n\u4ee3\u7801\u4e3a 1\u3002 [E:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\zlib.vcxproj]\r\n\r\n\r\nhow to solve it??thanks,please\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21541, "title": "Fix nn_test.py on AVX512 builds", "body": "This patch modifies the nn_test test case L2LossTest.testGradient\r\nso that it passes on AVX512 builds.  The test case is failing\r\nas the error tolerance used in the test case is too strict.\r\nThe test case compares the difference of pairs of tensor reductions\r\nto an expected result.  If the comparison is out by more than 1e-11\r\nthe test case fails.  The problem here is that the results of a\r\nsummation reduction of doubles of the same tensor can differ slightly\r\non different builds.  AVX2, AVX512 and non vectorized versions of the\r\ntensor contraction algorithm add the tensor's contents together in\r\ndifferent orders and this different ordering can produce slightly\r\ndifferent results due to rounding errors.\r\n\r\nThe accuracy of AVX512 tensor reduction is no worse than the AVX2\r\nimplementation.  In fact, it's only luck that this test case passes\r\non AVX2 builds and fails on AVX512 builds.  If the seed at the start of\r\nthe test is changed from 1 to 3, the test passes on AVX512 builds and\r\nfails on AVX2 builds.  Rather than trying to find a seed that allows\r\nthe test case to pass on all CPU architectures, it is better to relax\r\nthe test criteria a little bit.\r\n\r\nSigned-off-by: Mark Ryan <mark.d.ryan@intel.com>", "comments": []}, {"number": 21540, "title": "The nn_test unit test testGradient.L2LossTest fails on AVX512 builds", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.8.0-6288-g335336a', '1.10.0-rc1')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: [bazel release 0.15.0]\r\n- **GCC/Compiler version (if compiling from source)**: g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A \r\n- **Exact command to reproduce**:\r\n\r\nbazel test --config=opt -- //tensorflow/python:nn_test\r\n\r\n### Describe the problem\r\n\r\nThe test case fails on AVX512 builds.  See the logs below.\r\n\r\n### Source code / logs\r\n```\r\n======================================================================\r\nFAIL: testGradient (__main__.L2LossTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py\", line 224, in testGradient\r\n    self.assertLess(err, err_tolerance)\r\nAssertionError: 1.275390903998641e-11 not less than 1e-11\r\n\r\n----------------------------------------------------------------------\r\nRan 81 tests in 7.372s\r\n\r\nFAILED (failures=1)\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\nL2Loss gradient err = 1.27539e-11 \r\nL2Normalize gradient err = 4.2424e-08 \r\nL2Normalize gradient err = 5.45829e-07 \r\nL2Normalize gradient err = 7.61142e-05 \r\n```\r\n", "comments": ["I've submitted a [patch](https://github.com/tensorflow/tensorflow/pull/21541) to fix the issue.  The underlying problem is with the test case itself which is too stringent.  This issue can be reproduced on AVX2 builds if the random seed set at the start of the test is modified.  The patch fixes the issue by reducing the error tolerance from 1e-11 to 1e-10.  For more details see the commit message.\r\n", "https://github.com/tensorflow/tensorflow/pull/21541 is now merged so this can be closed."]}, {"number": 21539, "title": "def_file_filter: Fix a bug when VC path is not found", "body": "`find_vc_path(repository_ctx)` returns `None` if VC path is not found.\r\n@gunan", "comments": []}, {"number": 21538, "title": "why the results of tf.contrib.crf.viterbi_decode is different from tf.contrib.crf.crf_decode?", "body": "### Describe the problem\r\nFor viterbi decode,i have try the both api:\r\ntf.contrib.crf.viterbi_decode and tf.contrib.crf.crf_decode.\r\nI got different results.\r\n\r\n### Source code \r\nthe way i use  tf.contrib.crf.viterbi_decode:\r\n            for logit, sequence_length in zip(logits, sequence_lengths):\r\n                logit = logit[:sequence_length] # keep only the valid steps\r\n                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\r\n                        logit, trans_params)\r\nthe way i use  tf.contrib.crf.crf_decode:\r\n viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(self.logits, self.trans_params,self.sequence_lengths)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "It has been 36 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21537, "title": "problem when num_enqueue == batch_size", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 18.04 \r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource \r\n\r\n- **TensorFlow version (use command below)**: \r\nv1.08, v1.10\r\n\r\n- **Python version**:\r\n2.7\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.16.0\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\n6.4.0 \r\n\r\n- **CUDA/cuDNN version**:\r\n9.2\r\n\r\n- **GPU model and memory**:\r\nNVIDIA 1080 Ti\r\n\r\n- **Exact command to reproduce**:\r\n```\r\n      image_list = [ tf.image.crop_to_bounding_box(image, 16, 16, 224, 224),\r\n                    tf.image.crop_to_bounding_box(image, 0, 0, 224, 224),\r\n                    tf.image.crop_to_bounding_box(image, 32, 0, 224, 224),\r\n                    tf.image.crop_to_bounding_box(image, 0, 32, 224, 224),\r\n                    tf.image.crop_to_bounding_box(image, 32, 32, 224, 224),\r\n                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 16, 16, 224, 224)),\r\n                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 0, 0, 224, 224)),\r\n                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 32, 0, 224, 224)),\r\n                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 0, 32, 224, 224)),\r\n                    tf.image.flip_left_right(tf.image.crop_to_bounding_box(image, 32, 32, 224, 224)) ]\r\n      image = tf.stack([image_list[x] for x in range(10)], axis=0)\r\n      label = tf.stack([label for x in range(10)], axis=0)\r\n\r\n    #Make batches\r\n    if objective == \"test\":  x, y_ = tf.train.batch([image, label], batch_size=10,        capacity=200,  num_threads=2, enqueue_many=True, allow_smaller_final_batch=True)\r\n```\r\nAnd then evaluate accuracy with:\r\n```\r\n  for i in range(int(math.ceil(nTest))):\r\n    pred, trueLabel, top_five_ = sess.run([test_y, test_y_, top_five])\r\n    pred = int(np.argmax(np.mean(pred, axis=0)))\r\n    truth = int(np.argmax(trueLabel[0]))\r\n    test_accuracy_ = (1. if pred == truth else 0.)\r\n    accuracies.append(test_accuracy_)\r\naccuracy = np.mean(accuracies)\r\n```\r\n\r\n### Describe the problem\r\nWhen there are 10 images being enqueued into a tf.train.batch() with batch_size = 10, the reported accuracy is artificially low (e.g., 5%). If you change the batch_size to a higher number (e.g., 100) [and adjust the accuracy calculation accordingly], the reported accuracy is much higher (e.g., 50%). Seems there's some bug about num_enqueued == batch_size. \r\n\r\n", "comments": ["Its hard for me to say here without understanding your model / problem but if you change the batch size, it'll depend on your optimizer etc. and the learned weights etc. would change leading to different accuracy results. So this isn't that surprising perhaps. ", "No, I'm saying that batchsize = 10 with num_enqueue = 10 leads to artificially low error, but batchsize=11 works fine. I ran this past a few other ML experts in my office and we all agreed this is something weird on the TF side.", "So we're moving away from the tf.train.batch type API's that use queues and more towards tf.data \r\n\r\nhttps://www.tensorflow.org/guide/datasets. Would it be possible for you to give that a try?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "> So we're moving away from the tf.train.batch type API's that use queues and more towards tf.data\r\n> \r\n> https://www.tensorflow.org/guide/datasets. Would it be possible for you to give that a try?\r\n\r\n@cag51  Any update on this ?", "In \"awaiting response\" status for more than 3 days. Hence closing this. Please post the updates here if any, we will reopen the issue. Thanks !"]}, {"number": 21536, "title": "Support placeholders without shape in build_raw_serving_input_receiver_fn", "body": "Fix #21178.", "comments": []}, {"number": 21535, "title": "typo fix for rpi.md", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it! ; )", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21534, "title": "Bijector mapping  deep tuple fix", "body": "Right now, Bijector _Mapping's _deep_tuple doesn't allow nested dicts. This problem causes some of the conditioning not to work. See:  #21543. This PR is a possible way to fix the issue.", "comments": ["@jvdillon ptal.", "@ebrevdo @jvdillon Does this PR make sense? If it does, would it make sense for me to move it under tfp?"]}, {"number": 21533, "title": "Fix incorrect doc in tf.layers.dense", "body": "This fix tries to address the issue raised in #21525 where\r\nthe doc in tf.layers.dense is not correct. Specifically\r\n`outputs = activation(inputs.kernel + bias) Where`\r\nshould be:\r\n`outputs = activation(inputs * kernel + bias) where`\r\n\r\nThis fix fixes #21525.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 21532, "title": "How to take weight from file checkpoint ckpt ?", "body": "i want to take weight parameter from model object detection api ? how to do that ?", "comments": ["[tf.train.load_variable](https://www.tensorflow.org/api_docs/python/tf/train/load_variable), and [tf.train.list_variables](https://www.tensorflow.org/api_docs/python/tf/train/list_variables) if you don't know the name. Probably a better question for StackOverflow."]}, {"number": 21531, "title": "Build from source with python3.7 failed(prototype changed)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:arch\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:1.10.0\r\n- **Python version**:python-3.7\r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**: gcc-7\r\n- **CUDA/cuDNN version**:cuda-9.2,cudnn-7.1\r\n- **GPU model and memory**:24G\r\n- **Exact command to reproduce**:\r\n\r\nbuild from newest source with Python3.7 failed at \r\n`external/protobuf_archive/python/google/protobuf/pyext/descriptor_containers.cc:69:42: error: invalid conversion from 'const char*' to 'char*'`\r\nAs the https://docs.python.org/3.7/c-api/unicode.html#c.PyUnicode_AsUTF8AndSize said \r\n`Changed in version 3.7: The return type is now const char * rather of char *` for `PyUnicode_AsUTF8AndSize() prototype` \r\nMaybe you should update it accordingly.", "comments": ["https://github.com/tensorflow/tensorflow/issues/20690", "As this is the first hit on google: Protobuf has already been patched. if you apply the changes from the linked commit the conversion error will be solved: https://github.com/protocolbuffers/protobuf/pull/4862/files\r\n"]}, {"number": 21530, "title": "[Bug] Broken Combination: Non-SGD Optimizer, tf.Variable(), and Estimator Framework", "body": "The following combination is broken when simultaneously used:\r\n* Optimizer other than SGD\r\n* Estimator framework\r\n* using `tf.Variable` rather than `tf.get_variable`\r\nThe bug might be more specific, since I am basing this directly off of the full setup found in the official resnet example in `models/`\r\n\r\nI have made a 3-line modification to the official resnet example illustrate problem:\r\nhttps://github.com/tensorflow/models/compare/master...liuyipei:BUG_NonSGDOptimizer_Variable_Estimator\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI can illustrate the bug with minimal change to an official example.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\ncentos-release-7-4.1708.el7.centos.x86_64\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**:\r\n3.4.5\r\n- **CUDA/cuDNN version**:\r\nCUDA 9, cuDNN 7\r\n- **Bazel version**:\r\nN/A\r\n- **Mobile device**:\r\nN/A\r\n- **GPU model and memory**:\r\nNvidia GTX1080\r\n- **Exact command to reproduce**:\r\n```\r\ncd models/official/resnet\r\ngit remote add liuyipei git@github.com:liuyipei/models.git\r\ngit fetch liuyipei\r\ngit checkout liuyipei/BUG_NonSGDOptimizer_Variable_Estimator\r\npython cifar10_download_and_extract.py\r\npython cifar10_main.py\r\n```\r\n\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI changed 3 lines inside an officially supported model to illustrate my point.\r\nhttps://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py\r\nI added a `z` variable. If I use `tf.Variable` directly, an error happens at training time.\r\n```\r\n      z = tf.Variable(0, dtype=tf.float32, trainable=True, name='tf_Variable_direct') # this fails at optimization time\r\n      # z=tf.get_variable(\"tf_get_variable\", [], dtype=tf.float32) #this works!\r\n      inputs = tf.identity(inputs+z, 'final_reduce_mean')\r\n```\r\n\r\nThe error is:\r\n```\r\n  File \"/home/yiliu/gpuenv/lib/python3.4/site-packages/tensorflow/python/training/momentum.py\", line 98, in _apply_dense\r\n    use_nesterov=self._use_nesterov).op\r\n  File \"/home/yiliu/gpuenv/lib/python3.4/site-packages/tensorflow/python/training/gen_training_ops.py\", line 634, in apply_momentum\r\n    name=name)\r\n  File \"/home/yiliu/gpuenv/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py\", line 641, in _apply_op_helper\r\n    \"(e.g.: a tf.Variable)\") % (op_type_name, input_name))\r\nTypeError: 'ApplyMomentum' Op requires that input 'accum' be a mutable tensor (e.g.: a tf.Variable)\r\n```\r\n\r\nI have the following additional anecdotal information:\r\n* The bug does not manifest when `GradientDescentOptimizer` is used, presumably because it doesn't involve momentum updates.\r\n* The bug does not manifest itself when I declare the variable using `tf.get_variable`\r\n* I did some more digging (work not shown), but it seems to have something to do with the dtype of the object created directly by `tf.Variable` having dtype `float32_ref` after coming through `optimizer.compute_gradients(loss * loss_scale)` inside `resnet_run_loop.py`. This is unlike the other variables, which have `float32` in that context. Maybe there is some kind of confusion in type checking.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "Updated as instructed @tensorflowbutler ", "@mrry does this count as a \"framework puzzle\"? Please feel free to reassign. ", "No, I think it's something to do with how the variable scopes are set up in that model's implementation. Assigning to @robieta, since he added that code in fbb27cf31f09c3b4b10c1e237fd283f06db301d2 (although this issue should probably move to the models repository).", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21530\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21530\">No</a>\n"]}, {"number": 21529, "title": "docs_src: minor spelling tweaks", "body": "", "comments": []}, {"number": 21528, "title": "Optimization flags are ignored in builds", "body": "user defined optimization flags, i.e. bazel build -copt=\"xxx\", regardless whether passed from command-line or are not propagated to compilation steps. This creates a problem especially in cuda builds and same for both clang and nvcc based builds. Here is an example\r\n\r\nBazel command-line\r\n```shell\r\nbazel build -s -c opt --copt=\"-march=sandybridge\" --copt=\"-mtune=broadwell\" --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\none of the compilation commands\r\n```\r\nexternal/local_config_cuda/crosstool/extra_tools/bin/clang -MD -MF bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/host/genfiles -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote external/llvm -iquote bazel-out/host/genfiles/external/llvm -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/llvm/lib/IR -isystem bazel-out/host/genfiles/external/llvm/lib/IR -isystem bazel-out/host/bin/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/host/genfiles/external/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm/include/llvm/IR -isystem external/llvm/include -isystem bazel-out/host/genfiles/external/llvm/include -isystem bazel-out/host/bin/external/llvm/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c tensorflow/compiler/xla/service/gpu/tuple_thunk.cc -o bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.o\r\n```", "comments": ["Hi, I don't work on the TF build system.  Perhaps you mean to assign to someone on the TF build team?", "Oh, I see why you thought this should be sent to me.  :)\r\n\r\nThe first command line has nothing to do with CUDA or GPUs, so I think this is outside my wheelhouse.\r\n\r\nIf it's broken only for CUDA compiles, then that's a different story, but that's not what I'm seeing.", "cc @gunan @ilya-biryukov", "I think you configured for GPU and to build with clang in the configure script.\r\n`--config=cuda` gets added in bazelrc when you choose to build for GPU in our configure script.\r\n\r\n@ilya-biryukov set this option up for us, so Ill let him look.", "@jlebar current cuda toolchain file filters out any parameters it doesn't understand instead of passing them as compiler parameters to nvcc\r\nit looks like similar issue is happening with clang compilation., --config=cuda is in bazelrc file", "Actually one should *not* add --config=cuda to command-line anymore since if you select cuda with clang, it creates a conflicting rule for building the pip package and won't build.", "Two more updates. I instrumented crosstool_wrapper_driver_is_not_gcc to printout compiler flags it get, used for cpu compilation and gpu compilation and did a build with -s option, generating around 140k line log file. Some early observations\r\n- For some reason only part of the files get --copt options\r\n- If these options are passed to cuda compilation, they are ignored, as was expected from the code.\r\nTinkering more.", "Maybe these are excluded in our crosstool intentionally?\r\nIs it possible that the following was  the cause to make such a decision?\r\nhttps://devtalk.nvidia.com/default/topic/780902/nvcc-with-avx-support-cannot-find-gcc-builtin-intrinsics/\r\nDoes the above still apply?", "@gunan,\r\nI believe issue there is different. Reporter there is trying to use gcc avx intrinsics in .cu files. passing -Xcompiler \"-mavx\" should be perfectly fine. Besides, the wrapper actually would pass -mavx (or any -mxxx) flag to compiler but would not pass -march=sandybridge which would also enable same flags.\r\n", "@meteorcloudy could you loop in someone who can help out with our gpu crosstool?\r\n", "From the command line\r\n```\r\nexternal/local_config_cuda/crosstool/extra_tools/bin/clang -MD -MF bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/host/genfiles -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote external/llvm -iquote bazel-out/host/genfiles/external/llvm -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem bazel-out/host/bin/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/llvm/lib/IR -isystem bazel-out/host/genfiles/external/llvm/lib/IR -isystem bazel-out/host/bin/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/host/genfiles/external/llvm/include/llvm/IR -isystem bazel-out/host/bin/external/llvm/include/llvm/IR -isystem external/llvm/include -isystem bazel-out/host/genfiles/external/llvm/include -isystem bazel-out/host/bin/external/llvm/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -c tensorflow/compiler/xla/service/gpu/tuple_thunk.cc -o bazel-out/host/bin/tensorflow/compiler/xla/service/gpu/_objs/gpu_executable/tensorflow/compiler/xla/service/gpu/tuple_thunk.pic.o\r\n```\r\n\r\nLooks like Bazel is compiling for host configuration. You need to use `--host_copt` and `--host_cxxopt` if you want to pass options for those compilations. \r\nAlso /cc @mhlopko the C++ expert.", "@samikama try this: https://github.com/gentoo/gentoo/blob/0d7adf51d80763d2ca8c3e5ffac4dd9b3edbe388/sci-libs/tensorflow/tensorflow-1.11.0_rc0.ebuild#L125-L141\r\nim pretty sure that snippet gets all flags properly. (at least for one machine, I havent worked on cross-compiling yet)", "@perfinion, Sorry I can't see how this helps with cuda compilation.", "Is there anything that needs to be done on the Bazel side? What Yun said in https://github.com/tensorflow/tensorflow/issues/21528#issuecomment-421932118 should fix the issue.", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}, {"number": 21527, "title": "Update install source docs.", "body": "", "comments": ["cc @fchollet ", "cc @MarkDaoust "]}, {"number": 21525, "title": "A little bug from the api tf.layers.dence page.", "body": "There is a small mistake on your page of the description of the api tf.layers.dense.\r\nThe line:\r\n`This layer implements the operation: outputs = activation(inputs.kernel + bias) Where activation is the `\r\nI think the . should be \u00b7 ?", "comments": ["Added PR #21533 for the fix."]}, {"number": 21524, "title": "Nightly Build Binaries", "body": "@av8ramit Are there any updates on ways to obtain nightly builds for the dynamic libraries (i.e., libtensorflow.so and libtensorflow_framework.so)? In the beginning of the summer you had mentioned that you were working on a new way to do that.", "comments": ["Hey @eaplatanios sorry for the delay we are still in the process of doing so.\r\n@asimshankar any thoughts on the best place to store these?", "@av8ramit : Where are the nightly pip wheels stored? Can we use the same location? (Or if not, can we carve out a nightly directory in storage.googleapis.com?)", "@asimshankar nightly pip whls are stored on pypi which I don't think is feasible. storage.googleapis.com works.", "Seems fine to have the nightly builds uploaded to the GCS buckets where we store the release versions?", "I don\u2019t know if you are expecting a reply from me but this solution would be great! One option would be to put them alongside the release versions and change the version number to something along the lines of `1.11-nightly`, or even just `nightly`.", "@eaplatanios I'll try my best to accommodate soon!", "@av8ramit That\u2019s great, thanks!:)", "Status update: Blocked on something @eaplatanios but still working on it.", "@av8ramit Thanks a lot for the update and sorry for the slow response but I was traveling all this time. Do you have an approximate ETA by any chance?", "Nightly LibtensorFlow Ubuntu\r\n[JNI CPU](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow_jni-cpu-linux-x86_64.tar.gz)\r\n[JNI GPU](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow_jni-gpu-linux-x86_64.tar.gz)\r\n[Proto](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow_proto.zip)\r\n[CPU](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-cpu-linux-x86_64.tar.gz)\r\n[GPU](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-gpu-linux-x86_64.tar.gz)\r\n[SRC-jar](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow-src.jar)\r\n[jar](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow.jar)\r\n\r\n\r\nHere are the nightly links for now until I get a cleaner chance to present them.\r\n", "@av8ramit That's great and sufficient for me as I only need links. I notice though that there is no option for mac binaries. Are those available too? I tried changing the platform name to \"darwin\" for the CPU links you posted, but couldn't get a valid link. Thanks a lot! :)", "I've only been able to do linux binaries so far. Mac and windows will come soon.", "How about now for mac? [Link](https://storage.googleapis.com/tensorflow-nightly/github/tensorflow/lib_package/libtensorflow_jni-cpu-darwin-x86_64.tar.gz)", "@asimshankar do you see value in making these links public or adding them to the README?", "@av8ramit Sorry I never responded. Thanks for the Mac binaries too! Regarding value in these links, I believe that other people using the C API may find those useful, same way that I do.", "@av8ramit There seems to be an issue with the Linux GPU binaries, that I can't figure out. I get the following error when using the GPU prebuilt binaries:\r\n\r\n```\r\nundefined symbol: _ZN10tensorflow12OpDefBuilder4AttrESs\r\n```\r\n\r\nI don't get this error with the CPU binaries. Do you know what may be causing this? Are they both compiled using GCC 4?", "@av8ramit I see that the precompiled GPU binary (libtensorflow_framework.so) contains this symbol:\r\n\r\n```\r\n_ZN10tensorflow12OpDefBuilder4AttrEN4absl11string_viewE\r\n```\r\n\r\nwhereas the CPU one contains:\r\n\r\n```\r\n_ZN10tensorflow12OpDefBuilder4AttrESs\r\n```\r\n\r\nShould this be expected? And even so, why does this inconsistency exist?", "Adding @asimshankar in case you have any idea why this is the case. Note that the GPU version compiled locally on my machine also contains `_ZN10tensorflow12OpDefBuilder4AttrESs` rather than the absl version mentioned earlier.", "I suspect what happened is that one of the binaries is built with https://github.com/tensorflow/tensorflow/commit/6fa6bd045c98bdc89424a3425e15b5161586a9a7 and the other with https://github.com/tensorflow/tensorflow/commit/3f23f4ddeabbdc0704444d84c158bd6c348a9f10\r\n\r\n@av8ramit can correct me if I'm wrong, but I believe we don't currently guarantee that all the nightlies are built from the exact same version (as per https://github.com/tensorflow/community/blob/master/rfcs/20181026-tf-nightly.md)", "Yeah that's probably what it is. Nightly binary pushes are best effort. ", "@asimshankar @av8ramit I see that makes sense. So, it used to be that there was a Travis interface where we could go and check when the last successful build was. Does that something like that still exist, so we have some information about when the nightlies are obtained from?", "@asimshankar @av8ramit It seems that the GPU nightly hasn't changed in at least 23 days now. Is there any reason for that and more generally any way to check the status of these nightly builds?", "@asimshankar @av8ramit Sorry for the spamming, but are there any updates on this?", "Hi @eaplatanios Sorry about the delay. I'll look into this this week. Hopefully have it fixed after the holiday. Prioritizing ubuntu GPU for you.", "@av8ramit That's great. Thanks a lot! :)", "@eaplatanios has this been fixed?\r\n", "@eaplatanios is this working for you now?", "@av8ramit Sorry I was away for the break. Yes it is. Thank you very much!\r\n\r\nAre there any plans to gave a build status page on your CI server like you used to so we know what day the current build is from?", "If anyone is looking for Windows nightly's (I ended up in this issue trying to find out what happened to the windows tf-nightly-gpu pypi package) you can find links to them here:\r\n[https://github.com/PlatinumLyfe/tf-windows-gpu/](https://github.com/PlatinumLyfe/tf-windows-gpu/)\r\n\r\nGot sick of waiting since I was using \"tf-nightly-gpu\" pypi package and looking at the history of that package, it seems less platforms are supported.\r\n\r\nTook me ages to get these to compile and reconfigured a bunch of things which you can find mentioned at that repo...\r\n", "@PlatinumLyfe unfortunately we are having some issues on our infrastructure side so we haven't been able to update the tf-nightly-gpu binaries since [12/26](https://pypi.org/manage/project/tf-nightly-gpu/release/1.13.0.dev20181226). Sorry for the inconvenience. We also have plans in the future to support Python3.7 for windows.\r\n\r\n@eaplatanios we don't have any plans for that at the moment unfortunately. Any input @asimshankar ?", "There is no immediate plan to create a better UI or adopt Travis for libtensorflowbuilds."]}, {"number": 21523, "title": "SyncReplicaOptimizer prints global step warning unnecessarily", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**:  cuda-9.0/ cuDNN 7\r\n- **GPU model and memory**: TITAN X 12GB\r\n- **Exact command to reproduce**:  Run the code below to produce the warnings\r\n\r\n### Describe the problem\r\nThis is less a bug and more a suggested improvement. The setup illustrated in the code above is common for reinforcement learning algorithms where the workers are used to collect experience by acting in an environment according to the current policy in a distributed fashion and then synchronizing to update the parameters of the shared networks (stored on the parameter server). There are two ops that are run (multiple times) in a single iteration of the training loop. One is an `act` op and the other is the `update` op. The `act` op has nothing to do with the optimizer while the `update` op does. As such, running the `act` op should not affect the `global_step` while the `update` op should increment `global_step`. The problem, in my estimation, is that tensorflow does not recognize this difference and complains (by printing a warning) that `global_step` is not incremented every time the `act` op is run.\r\n\r\nThis is extremely annoying as thousands of warnings are printed and useful information is lost in the torrent of text. Turning off all warning is the workaround I am using right now, but this is clearly not a long-term solution.\r\n\r\nThe exact warning is:\r\n\r\n``` \r\n\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 835 vs previous value: 835. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\n```\r\n\r\n### Source code / logs\r\n```\r\nimport os\r\nimport multiprocessing as mp\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnum_workers=4\r\n\r\ndef model(images):\r\n    net = tf.layers.dense(images, 500, activation=tf.nn.relu)\r\n    net = tf.layers.dense(net, 500, activation=tf.nn.relu)\r\n    net = tf.layers.dense(net, 10, activation=None)\r\n    return net\r\n\r\ndef train_run_parallel(cluster, job, task, num_workers):\r\n    server = tf.train.Server(cluster, job_name=job, task_index=task)\r\n    if job == 'ps':\r\n        server.join()\r\n    else:\r\n        worker_device = \"/job:worker/task:{}\".format(task)\r\n        with tf.device(tf.train.replica_device_setter(cluster=cluster)):\r\n            def train_next_batch(batchsize):\r\n                imgs = np.random.rand(batchsize,784)\r\n                inds = np.random.randint(0, 10, (batchsize))\r\n                labels = np.eye(10)[inds]\r\n                return imgs,labels\r\n            \r\n            images = tf.placeholder(tf.float32, [None, 784])\r\n            labels = tf.placeholder(tf.int32, [None, 10])    \r\n\r\n            logits = model(images)\r\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\r\n\r\n            hooks = []\r\n            global_step = tf.train.get_or_create_global_step()\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=1e-04)\r\n            optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\r\n                                        total_num_replicas=num_workers, )\r\n            \r\n            hooks.append(optimizer.make_session_run_hook(task==0, num_tokens=0))\r\n            train_op = optimizer.minimize(loss, global_step=global_step,\r\n                                          aggregation_method=tf.AggregationMethod.ADD_N)\r\n            \r\n            \r\n            mon_sess = tf.train.MonitoredTrainingSession(master=server.target,\r\n                                                   is_chief=(task == 0),\r\n                                                   checkpoint_dir=\"./test_checkpoint_dir\",\r\n                                                   hooks=hooks)\r\n\r\n            for e in range(1000):\r\n                # simulate roll-out for reinforcement learning\r\n                for _ in range(100):\r\n                    img_batch, label_batch = train_next_batch(32)\r\n                    pred_logits = mon_sess.run([logits], feed_dict={images:img_batch})\r\n                img_batch, label_batch = train_next_batch(32)\r\n                _, ls, step = mon_sess.run([train_op, loss, global_step],\r\n                                            feed_dict={images: img_batch, labels: label_batch})\r\n                if step % 10 == 0:\r\n                    print(\"Worker %d, Train step %d, loss: %f\" % (task, step, ls))\r\n            server.join()\r\n            \r\ndef main():    \r\n    cluster = tf.train.ClusterSpec({\r\n        'worker': ['localhost:'+str(30352+w) for w in range(num_workers)],\r\n        'ps': [\r\n            'localhost:30351'\r\n        ]\r\n    })\r\n\r\n    job_task_index_map = [('ps', 0)]\r\n    for w in range(num_workers): job_task_index_map.append(('worker', w))\r\n\r\n    procs = []\r\n\r\n    for job, task in job_task_index_map:\r\n        proc = mp.Process(target=train_run_parallel, args=(cluster, job, task, num_workers))\r\n        procs.append(proc)\r\n        proc.start()\r\n    \r\n    for proc in procs:\r\n        proc.join()\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n", "comments": ["Annoying warning, agree", "I create a PR #22968 to fix the issue (log only in the first 5 times), will it solve your problem?", "I think that would be a good workaround."]}, {"number": 21521, "title": "[Docs] Update iOS selective registration docs", "body": "Fixes #20591\r\n@MarkDaoust /cc", "comments": []}, {"number": 21520, "title": "Extension 'tensorflow/tensorflow.bzl' has errors", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution** : Linux Ubuntu 16.04\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.16.0\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: nvidia GTX 860m 16gb\r\n- **TensorFlow installed from**: Haven't install\r\n- **TensorFlow version**: wanted to install 0.12\r\n- **Exact command to reproduce**: NA\r\n- **Mobile device**: NA\r\n\r\n\r\n### Describe the problem\r\nWhen I try to download the Tensorflow-gpu from the source with Bazel, it says:\r\n\r\nERROR: /home/kevin/tensorflow/tensorflow/tensorflow.bzl:582:12: name 'set' is not defined\r\nERROR: error loading package '': Extension 'tensorflow/tensorflow.bzl' has errors\r\n\r\n\r\n### Source code / logs\r\n$ git checkout r0.12\r\n$ ./configure \r\n~/tensorflow ~/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] \r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] No Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] \r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with GPU support? [y/N] y\r\nGPU support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nStarting local Bazel server and connecting to it...\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nStarting local Bazel server and connecting to it...\r\nERROR: /home/kevin/tensorflow/tensorflow/tensorflow.bzl:582:12: name 'set' is not defined\r\nERROR: error loading package '': Extension 'tensorflow/tensorflow.bzl' has errors\r\nBuilding: no action\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nTensorFlow version\nExact command to reproduce\nMobile device", "It has been 22 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is still an issue by using Bazel.\r\nbut i can use this to install v0.12\r\n`pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.0-cp27-none-linux_x86_64.whl`", "0.12 is at this point 2 years old. You have to have the bazel version that is compatible with that version.\r\n\r\n@martinwicke should we start deleting these old branches to avoid user confusion?", "I'm a big fan of historical things being available. I suppose we can remove the branch and leave the tag. But I don't think it'll save much confusion. \r\n\r\nWe are not supporting old releases though. "]}, {"number": 21519, "title": "Fix warning in image_ops_test", "body": "While running test:\r\n```\r\nbazel test -s --verbose_failures --config=opt --cache_test_results=no //tensorflow/python:image_ops_test\r\n```\r\n\r\nThe following warning showed up:\r\n```\r\nWARNING:tensorflow:From /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/image_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/image_ops_impl.py:1170: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the `axis` argument instead\r\n```\r\n\r\nThis fix fixes the above warning.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 21518, "title": "1.10 build fails with \"No module named 'keras_applications'\"", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Fedora 28\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16\r\n- **GCC/Compiler version (if compiling from source)**: 7.3\r\n- **CUDA/cuDNN version**: 9.2\r\n- **GPU model and memory**: Quadro M2200 4G\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nHi, a recent checkout of master (~ 2 hours ago) fails (for me) with\r\n\r\n```\r\nERROR: /home/key/code/tensorflow/tensorflow/BUILD:584:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)\r\n/home/key/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTraceback (most recent call last):\r\n  File \"/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"/home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py\", line 21, in <module>\r\n    import keras_applications\r\nModuleNotFoundError: No module named 'keras_applications'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 3855.754s, Critical Path: 156.93s\r\nINFO: 8932 processes: 8932 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nCould you please advise?", "comments": ["I believe the following will fix this issue:\r\n\r\n```\r\npip install keras_applications==1.0.4 --no-deps\r\npip install keras_preprocessing==1.0.2 --no-deps\r\n```\r\n\r\n@yifeif any additional info?", "You might also need `pip install h5py==2.8.0`", "we should update our installation from sources documentation with this information.", "https://github.com/tensorflow/tensorflow/pull/21527\r\n\r\n@skeydan please let us know if this helps.", "I can confirm that installing the three packages resolves the issue as per [this link](https://stackoverflow.com/questions/51771039/error-compiling-tensorflow-from-source-no-module-named-keras-applications/51774943#51774943)", "works great, thank you!!", "@gunan Are you going to update the Dockerfiles or do you want me to submit a PR for it? All the containerized builds are failing too.", "AH, I completely missed that.\r\n@av8ramit, could you send a PR to update all our dockerfiles to install the 3 pip packages mentioned in this issue?", "Sure, sounds good.", "Sent internally.", "This issue has now been fixed.", "I am still having this issue. I have installed keras(-applications) and h5py according to [this](https://stackoverflow.com/questions/51771039/error-compiling-tensorflow-from-source-no-module-named-keras-applications/51774943#51774943).\r\nRunning a fresh pull of master with most recent commit 19cafed2ae 62191da081 Thu Aug 16 13:04:52 \r\nRunning\r\n`bazel build -c opt --copt=-march=native //tensorflow/tools/pip_package:build_pip_package`\r\nThe error ends with\r\n`ERROR: /Users/owengray/Desktop/git/tensorflow/tensorflow/BUILD:576:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_owengray/8e2e063d880456f5e168abd014557d0b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/private/var/tmp/_bazel_owengray/8e2e063d880456f5e168abd014557d0b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 81, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/private/var/tmp/_bazel_owengray/8e2e063d880456f5e168abd014557d0b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import applications\r\n  File \"/private/var/tmp/_bazel_owengray/8e2e063d880456f5e168abd014557d0b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py\", line 21, in <module>\r\n    import keras_applications\r\nModuleNotFoundError: No module named 'keras_applications'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build`\r\n", "What python version are you running with? What was the output of your pip install commands?", "`Owens-MBP:~ owengray$ pip install keras_applications==1.0.4 --no-deps\r\nRequirement already satisfied: keras_applications==1.0.4 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (1.0.4)\r\nOwens-MBP:~ owengray$ pip install keras_preprocessing==1.0.2 --no-deps\r\nRequirement already satisfied: keras_preprocessing==1.0.2 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (1.0.2)\r\nOwens-MBP:~ owengray$ pip install h5py==2.8.0\r\nRequirement already satisfied: h5py==2.8.0 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (2.8.0)\r\nRequirement already satisfied: numpy>=1.7 in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from h5py==2.8.0) (1.15.0)\r\nRequirement already satisfied: six in /opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from h5py==2.8.0) (1.11.0)`\r\n`Owens-MBP:~ owengray$ /usr/local/bin/python3 --version\r\nPython 3.6.5`", "Also,\r\n`Owens-MBP:tensorflow owengray$ ./configure \r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.16.1 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/local/bin/python3\r\nFound possible Python library paths:\r\n  /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]`\r\n\r\nI get several errors running bazel fetch ... -k, but they don't seem related.\r\n\r\nEDIT: Actually, it looks like I may have answered my own question--not reading library paths clearly enough (/Library vs /opt/local/Library)", "Yup, that looks like it's it. Please let us know if it works now.", "Still didn't work.\r\n`Found possible Python library paths:\r\n  /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages]\r\n/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages`\r\n\r\n\r\n`  File \"/private/var/tmp/_bazel_owengray/8e2e063d880456f5e168abd014557d0b/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/keras/applications/__init__.py\", line 21, in <module>\r\n    import keras_applications\r\nModuleNotFoundError: No module named 'keras_applications'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build`", "My theory is you're not installing keras_applications to the right python path that you're specifying. \r\n", "Using tab autocomplete:\r\n`/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras\r\nkeras/               keras_applications/  keras_preprocessing/`", "What about building tensorflow in a virtual environment?", "You will either need to create the virtualenv with `--system-site-packages`, or run `pip install keras-applications` (and other pip install commands) after you activate the virtualenv.", "I got the same issue while building the latest master. Why is bazel not taking care of it?", "That is an interesting point. I think the answer is we have not looked into making this package available through our bazel workspace.\r\nI would be thrilled to review such a contribution to the repo. Unfortunately, I have a few high priority items on my list that make me unable to work on it.", "I am building TF with debug symbols, the latest greatest commit, and it looks like Tensorflow requires \r\n```\r\n    'keras_applications >= 1.0.5',\r\n    'keras_preprocessing >= 1.0.3',\r\n```\r\nbut keras itself wants different version\r\n```\r\nerror: Keras-Applications 1.0.5 is installed but keras-applications==1.0.4 is required by {'keras'}\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6729cd7d1c07e547298fa2a02d1e36390dc62f0a/tensorflow/tools/pip_package/setup.py\r\n\r\n\r\n\r\n```\r\n% pip install keras_applications==1.0.4  --no-deps\r\nRequirement already satisfied: keras_applications==1.0.4 in ~/.pyenv/versions/venv36/lib/python3.5/site-packages (1.0.4)\r\n                 \r\n% pip install keras_preprocessing==1.0.2  --no-deps\r\nRequirement already satisfied: keras_preprocessing==1.0.2 in ~/.pyenv/versions/env36/lib/python3.5/site-packages (1.0.2)\r\n                          \r\n% python3 setup.py develop\r\nrunning develop\r\nrunning egg_info\r\nwriting entry points to tensorflow.egg-info/entry_points.txt\r\nwriting requirements to tensorflow.egg-info/requires.txt\r\nwriting top-level names to tensorflow.egg-info/top_level.txt\r\nwriting tensorflow.egg-info/PKG-INFO\r\nwriting dependency_links to tensorflow.egg-info/dependency_links.txt\r\nreading manifest file 'tensorflow.egg-info/SOURCES.txt'\r\nreading manifest template 'MANIFEST.in'\r\nwarning: no files found matching '*.py' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.so' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*' under directory 'tensorflow/aux-bin'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/Eigen'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/external'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/google'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\nwriting manifest file 'tensorflow.egg-info/SOURCES.txt'\r\nrunning build_ext\r\nCreating ~/.pyenv/versions/venv36/lib/python3.5/site-packages/tensorflow.egg-link (link to .)\r\ntensorflow 1.10.0 is already the active version in easy-install.pth\r\nInstalling saved_model_cli script to ~/.pyenv/versions/venv36/bin\r\nInstalling tensorboard script to ~/.pyenv/versions/venv36/bin\r\nInstalling tflite_convert script to ~/.pyenv/versions/venv36/bin\r\nInstalling toco_from_protos script to `/.pyenv/versions/venv36/bin\r\nInstalling freeze_graph script to ~/.pyenv/versions/venv36/bin\r\nInstalling toco script to ~/.pyenv/versions/venv36/bin\r\n\r\nInstalled tensorflow/tensorflow/tools/pip_package\r\nProcessing dependencies for tensorflow==1.10.0\r\nerror: Keras-Applications 1.0.5 is installed but keras-applications==1.0.4 is required by {'keras'}\r\n```\r\n\r\nEven if I downgrade keras_applications and keras_processing TF installation overwrites them and complains that {keras} wants different version.", "I finally got it working. I just needed to make sure that the version of python I passed to the first question in ./configure could successfully import keras when run from command line (ended up being in Frameworks). I may have also needed to set PYTHONPATH.\r\nI also found that the option --copt=-march=native did not work, and I needed to manually specify e.g. --copt=-mavx2\r\nI also got the keras version mismatch error, but the compilation succeeded and works fine.", "In my case it was the problem that there are two different versions of packages with `-` and `_` after keras so `keras_applications` and `keras-applications`. I was looking at one and TF was requiring the other one. The same goes for `keras_preprocessing`", "Closing this for now. Please comment if this is still an issue for you in virtualenv.", "I had the same problem during the installation in virtualenv. It worked once I installed \r\n\r\npip install keras_applications==1.0.4 --no-deps\r\npip install keras_preprocessing==1.0.2 --no-deps\r\npip install h5py==2.8.0\r\n\r\n**outside** of the virtual environment.\r\n", "@av8ramit @gunan @yifeif  I alse faced same problem, Environment information: keras==2.2.4, tensorflow-gpu==1.10.0, keras_applications==1.0.7, keras_preprocessing==1.0.9, CentOS 7.0, Python==3.6.5, very strange thing is that the training phase is OK, but while saved the trained model, and then load the trained model in testing phase, and I can import the module in Python3 command line:\r\n[root@192.168.0.128 /home/test]# python3\r\nPython 3.6.5 (default, Mar 14 2019, 12:54:24) \r\n[GCC 4.8.2 20140120 (Red Hat 4.8.2-16)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import keras_applications\r\n>>> \r\n\r\nThe system show the bellow error informatin:\r\n  File \"nasnet_predict.py\", line 278, in <module>\r\n    model = load_model(\"res_models/model.h5\")\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/saving.py\", line 419, in load_model\r\n    model = _deserialize_model(f, custom_objects, compile)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/saving.py\", line 225, in _deserialize_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/saving.py\", line 458, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/layers/__init__.py\", line 55, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\", line 145, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/network.py\", line 1032, in from_config\r\n    process_node(layer, node_data)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/network.py\", line 991, in process_node\r\n    layer(unpack_singleton(input_tensors), **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 457, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/layers/core.py\", line 687, in call\r\n    return self.function(inputs, **arguments)\r\n  File \"/usr/local/lib/python3.6/site-packages/keras/applications/__init__.py\", line 23, in wrapper\r\n    def keras_modules_injection(base_fun):\r\nNameError: name 'keras_applications' is not defined", "I solved this error (on 1.13.1) by running:\r\n\r\npip3 install keras_applications==1.0.6 --no-deps\r\npip3 install keras_preprocessing==1.0.5 --no-deps\r\npip3 install h5py==2.8.0\r\n", "still got the same problem\r\n\r\nwindows 10\r\npython 3.6\r\ntensorflow-gpu 1.12.0\r\nkeras 2.2.4\r\nkeras-applications 1.0.6\r\nkeras-preprocessing 1.0.5\r\nh5py 2.8.0\r\n\r\n\r\n**NameError: name 'keras_applications is not defined**", "> I solved this error (on 1.13.1) by running:\r\n> \r\n> pip3 install keras_applications==1.0.6 --no-deps\r\n> pip3 install keras_preprocessing==1.0.5 --no-deps\r\n> pip3 install h5py==2.8.0\r\n\r\nstill got the same problem\r\n\r\nwindows 10\r\npython 3.6\r\ntensorflow-gpu 1.12.0\r\nkeras 2.2.4\r\nkeras-applications 1.0.6\r\nkeras-preprocessing 1.0.5\r\nh5py 2.8.0\r\n\r\nNameError: name 'keras_applications is not defined", " tensorflow-gpu 1.1.2 ? I use 1.13.1", "> tensorflow-gpu 1.1.2 ? I use 1.13.1\r\n\r\ntensorflow-gpu 1.12.0 \r\nMy mistake\r\n", "I encountered the same problem after 6 hours of compiling.\r\n\r\nIf I run build again, does it start compiling all over again or resume? Because I don't want to compile 6 hours again.\r\n\r\nEdit: Yes, it continues compilation where it left off.", "when you install the [Keras-Preprocessing ](https://pypi.org/project/Keras-Preprocessing), This error is not resolved, you can check if you have multi versions of python. and it not been installed in your compile python version."]}, {"number": 21517, "title": "Fix documentation on softmax_cross_entropy_with_logits_v2", "body": "- Return documentation was incorrectly stating shape of tensor\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nlogits = np.array([\r\n    [5.0,  2.3, 0.0, -3.2, -0.8],\r\n    [2.3,  0.1, 1.8,  1.1,  1.2],\r\n    [-1.2, 1.1, 0.1,  2.5,  0.7]\r\n])\r\n\r\nlabels = np.array([\r\n    [1., 1., 0., 0., 0.],\r\n    [0., 0., 1., 0., 1.],\r\n    [0., 0., 0., 1., 0.]\r\n])\r\n\r\nx_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(tf.shape(labels)))  # [3, 5]\r\n    print(sess.run(tf.shape(x_entropy)))  # [3]\r\n```\r\n\r\nIn this example we see that the `x_entropy` shape does not equal the shape of `labels` as stated in the original documentation, it is of shape `[batch_size]`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@mbrio could you pull rebase and push again? Any time to address changes?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}]