[{"number": 24101, "title": "Choosing the right Bazel version to build TF is confusing", "body": "This bug was closed while the problem still exists: https://github.com/tensorflow/tensorflow/issues/21362", "comments": ["i have the problem while following this : https://www.tensorflow.org/install/source#docker_linux_builds to build the docker image", "I still get the issue with version 1.12 with tensorflow/configure.py", "I have same issue too. exact error shown in pc is \r\nroot@0d56190c7f6e:/tensorflow# ./configure\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!\r\n\r\neven if i have latest bazel  version 0.21.0", "Can be fixed by upgrading bazel (as was stated in `./configure` output)\r\n```bash\r\n$ wget https://github.com/bazelbuild/bazel/releases/download/0.21.0/bazel-0.21.0-installer-linux-x86_64.sh # bazel installer from https://github.com/bazelbuild/bazel/releases\r\n$ chmod +x bazel-0.21.0-installer-linux-x86_64.sh \r\n$ ./bazel-0.21.0-installer-linux-x86_64.sh \r\n...\r\nUncompressing.......\r\n\r\nBazel is now installed!\r\n\r\n$ bazel version\r\nExtracting Bazel installation...\r\nINFO: Invocation ID: f6ac0879-d756-4b94-8818-dcab72c7370c\r\nBuild label: 0.21.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 19 12:58:44 2018 (1545224324)\r\nBuild timestamp: 1545224324\r\nBuild timestamp as int: 1545224324\r\n\r\n$ ./configure \r\nINFO: Invocation ID: a33256ba-1596-40c5-999c-ec304b868c89\r\nYou have bazel 0.21.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n```", "I have the same exact error. The official build guide shows 1.15 as the tested version though. \r\nhttps://www.tensorflow.org/install/source", "The reason I installed 0.15 bazel is because of the above comment.\r\n", "@dveselov It does not work with tf 1.12.0 when upgrade bazel to 0.21.0", "I get the following error, can anyone help me, I believe that it is related to this issue. When configuring bazel I get the following error.\r\n\r\n```\r\nroot@636420111df0:/tensorflow# ./configure\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.23.2 installed.\r\nPlease downgrade your bazel installation to version 0.21.0 or lower to build TensorFlow!\r\n```\r\n\r\nUsing tf 1.13 with bazel 0.23.2. \r\n\r\nAm I better off down grading tf or bazel, I can't seem to find any instructions to install a different version of bazel", "```cmake``` does't seem to have such problems. Why not just use cmake instead?", "@lamberta \"What version of Bazel do I install to build TF\" seems to be a common problem, and a static doc may be unreliable because the recommended version is likely to change as changes are made to master. Our best reference for any branch is always the [configure script](https://github.com/tensorflow/tensorflow/blob/master/configure.py#L1624). Do you think it would be best to link straight to that from the build docs, or something like that?\r\n\r\nUnfortunately, it looks like this issue has migrated from its original intent (to ask about the `--batch` warning). Since that's just a warning and the issue has changed topic, I'll adjust the title. If anyone still experiences issues with `--batch` mode, please create another issue.", "So, there is not solution to this?", "If the issue is that `bazel ...` prints\r\n```\r\nWARNING: --batch-mode is deprecated...\r\n```\r\n\r\nthat's something caused by Bazel, not TensorFlow (also, it's just a warning, nothing breaks, can be safely ignored).\r\n\r\n---\r\n\r\nIf the issue is that every version of TensorFlow can only build with some specific Bazel versions, then running `./configure` before doing any compile will check if you have a Bazel that is supported. Afaik, there is no bug in the logic there, but if there's an issue, please open one and tag me.\r\n\r\n---\r\n\r\nUpgrading/downgrading `bazel` is just a matter of running the following commands\r\n\r\n```bash\r\n$ BAZEL_VERSION=\"0.26.0\"     # insert your desired version here, for example 0.26.0\r\n$ wget https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh   # if not on x86_64, change that too\r\n$ chmod +x bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh   # or the file you just downloaded\r\n$ ./bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\r\n$ bazel version   # this should now print the same as BAZEL_VERSION\r\n```\r\n\r\nI think this issue diverged too much, so I propose we close it and open new issues for individual components, if needed.", "My issue was that the TF guide was telling me to use bazel 0.23.0 or lower but Bazel was telling me to upgrade to 0.24.1 to use it. In the end, I just upgraded and it worked. i believe this should be closed as well.", "Please downgrade your bazel installation to version x.x.x or lower to build TensorFlow! Even if you follow the version compatibility at https://www.tensorflow.org/install/source#cpu-only_2\r\n\r\nPlease downgrade your bazel installation to version 0.0.0 and still it doesn't work."]}, {"number": 24100, "title": "Fix warning caused by keep_dims in sparse_grad.py", "body": "While running tests, noticed the following warning:\r\n```\r\n...\r\ntensorflow/python/ops/sparse_grad.py:281: calling sparse_reduce_sum (from tensorflow.python.ops.sparse_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\n\r\n```\r\nThis fix fixes the warning.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Ping @ymodak to take a look at the PR."]}, {"number": 24099, "title": "NotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT6..............", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: dont think its possible\r\n- TensorFlow installed from (source or binary): preinstalled\r\n- TensorFlow version (use command below): tensorflow               1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version: \r\n- GPU model and memory:NA but  **TPU**\r\n\r\n**A bit of context**  im trying  to train on mnist dataset using TPUs just as an exercise, using colab  \r\n\r\n**Describe the current behavior**\r\nthrowing the error same as the header of this issue\r\n```\r\nNotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()\r\n\t.  Registered:  <no registered kernels>\r\n\r\n\t [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()]]\r\n\t [[node input_pipeline_task0/while/IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:3053)  = IteratorGetNext[_class=[\"loc:@input_pipeline_task0/while/InfeedQueue/split/1\"], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=\"/job:tpu_worker/replica:0/task:0/device:CPU:0\"](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]] \r\n```\r\n\r\n**Describe the expected behavior**\r\nit should start training, or more probably throw some other error \r\nhonestly, everything related to TPUs on Colab is in aplha, so i was kinda expecting bugs after bugs and so it happened, nothing personal  \r\nthis is the final bug i got stuck on \r\n\r\n**Code to reproduce the issue**\r\n**_here's a  jupyter notebook:_  \r\n[ here](https://colab.research.google.com/drive/1J0Ejxn1FLxshahlbgKErLh-1u-7MPb_I)** \r\nbut if youre feeling lazy, here you go\r\n\r\n\r\n```\r\nfrom keras.datasets import mnist\r\nfrom tensorflow import data as Data\r\nimport tensorflow as tf \r\nimport numpy as np \r\n\r\ndata = mnist.load_data()\r\n\r\nx_train, y_train  = data[0][0], data[0][1] \r\nx_test, y_test  = data[1][0], data[1][1] \r\n\r\n# x_train, x_test, y_train, y_test = x_train[0:12000], x_test[0:12000], y_train[0:12000], y_test[0:12000]\r\n\r\ndef gen(x=x_train , y= y_train, batch_size = 32 ):\r\n    \r\n    for i in range(len(x)//batch_size):\r\n        yield (x[i*batch_size : (i+1)*batch_size], y[i*batch_size : (i+1)*batch_size])\r\n\r\ndef cinp(params):\r\n  bs = params['batch_size'] \r\n  train_data = Data.Dataset.from_generator( generator= gen,\r\n                                             output_types= ( tf.float32, tf.int32),\r\n                                             output_shapes= ((bs, 28,28),(bs)) )\r\n    \r\n  return train_data.shuffle(100)\r\ndef test_cinp():\r\n    bs = params['batch_size'] \r\n    train_data = Data.Dataset.from_generator( generator= gen,\r\n                                             output_types= ( tf.float32, tf.int32),\r\n                                             output_shapes= ((bs, 28,28),(bs)), \r\n                                                args = (x_test, y_test))\r\n    return train_data.shuffle(100)\r\n\r\nimg_size = 28\r\nnum_channels = 1 \r\n\r\ndef model_fn(features, labels, mode, params):\r\n   \r\n    \r\n    x = features\r\n    net = tf.reshape(x, [-1, img_size, img_size, num_channels])    \r\n\r\n    # First convolutional layer.\r\n    net = tf.layers.conv2d(inputs=net, name='layer_conv1',\r\n                           filters=16, kernel_size=5,\r\n                           padding='same', activation=tf.nn.relu)\r\n    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\r\n\r\n    # Second convolutional layer.\r\n    net = tf.layers.conv2d(inputs=net, name='layer_conv2',\r\n                           filters=36, kernel_size=5,\r\n                           padding='same', activation=tf.nn.relu)\r\n    net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)    \r\n    # Flatten to a 2-rank tensor.\r\n    net = tf.layers.flatten(net)\r\n    net = tf.layers.dense(inputs=net, name='layer_fc1',\r\n                          units=128, activation=tf.nn.relu)    \r\n    net = tf.layers.dense(inputs=net, name='layer_fc2',\r\n                          units=10)\r\n\r\n    # Logits output of the neural network.\r\n    logits = net\r\n\r\n    # Softmax output of the neural network.\r\n    y_pred = tf.nn.softmax(logits=logits)\r\n    \r\n    # Classification output of the neural network.\r\n    y_pred_cls = tf.argmax(y_pred, axis=1)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n\r\n        spec = tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=y_pred_cls)\r\n    else:\r\n     \r\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\r\n                                                                       logits=logits)\r\n        loss = tf.reduce_mean(cross_entropy)\r\n\r\n        # Define the optimizer for improving the neural network.\r\n\r\n        optimizer = tf.contrib.tpu.CrossShardOptimizer(\r\n            tf.train.AdamOptimizer(learning_rate=0.002, epsilon= 0.001))\r\n\r\n        # Get the TensorFlow op for doing a single optimization step.\r\n        train_op = optimizer.minimize(\r\n            loss=loss, global_step=tf.train.get_global_step())\r\n\r\n        metrics = \\\r\n        {\r\n            \"accuracy\": tf.metrics.accuracy(labels, y_pred_cls)\r\n        }\r\n\r\n        # Wrap all of this in an EstimatorSpec.\r\n        spec = tf.contrib.tpu.TPUEstimatorSpec(\r\n            mode=mode,\r\n            loss=loss,\r\n            train_op=train_op)\r\n        \r\n    return spec\r\n\r\n\r\nrun_config = tf.contrib.tpu.RunConfig( \r\n      master='grpc://'+os.environ['COLAB_TPU_ADDR'], \r\n     \r\n      session_config=tf.ConfigProto(\r\n          allow_soft_placement=True, log_device_placement=True),          \r\n          tpu_config=tf.contrib.tpu.TPUConfig( iterations_per_loop = 100,  )\r\n      )\r\n\r\nrunn_config = run_config.replace(save_checkpoints_secs = None)\r\nmodel = tf.contrib.tpu.TPUEstimator(model_fn= model_fn, \r\n                                    config = runn_config  , train_batch_size= 32 , )\r\n\r\nmodel.train( cinp , steps = 1000, )   #this is where error occurs\r\n \r\n\r\n```\r\n**Other info / logs**\r\nHERE's the complete log of the error \r\n```\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:TPU job name tpu_worker\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Initialized dataset iterators in 0 seconds\r\nINFO:tensorflow:Installing graceful shutdown hook.\r\nINFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\r\nINFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\r\n\r\nINFO:tensorflow:Init TPU system\r\nINFO:tensorflow:Initialized TPU in 4 seconds\r\nINFO:tensorflow:Starting infeed thread controller.\r\nINFO:tensorflow:Starting outfeed thread controller.\r\nINFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\r\nINFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\r\nINFO:tensorflow:Error recorded from infeed: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()\r\n\t.  Registered:  <no registered kernels>\r\n\r\n\t [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()]]\r\n\t [[node input_pipeline_task0/while/IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:3053)  = IteratorGetNext[_class=[\"loc:@input_pipeline_task0/while/InfeedQueue/split/1\"], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=\"/job:tpu_worker/replica:0/task:0/device:CPU:0\"](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]]\r\nINFO:tensorflow:Error recorded from training_loop: Step was cancelled by an explicit call to `Session::Close()`.\r\nINFO:tensorflow:training_loop marked as finished\r\nWARNING:tensorflow:Reraising captured error\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1333     try:\r\n-> 1334       return fn(*args)\r\n   1335     except errors.OpError as e:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1318       return self._call_tf_sessionrun(\r\n-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1320 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)\r\n   1406         self._session, options, feed_dict, fetch_list, target_list,\r\n-> 1407         run_metadata)\r\n   1408 \r\n\r\nNotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()\r\n\t.  Registered:  <no registered kernels>\r\n\r\n\t [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()]]\r\n\t [[{{node input_pipeline_task0/while/IteratorGetNext}} = IteratorGetNext[_class=[\"loc:@input_pipeline_task0/while/InfeedQueue/split/1\"], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=\"/job:tpu_worker/replica:0/task:0/device:CPU:0\"](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-37-ec76835859c0> in <module>()\r\n----> 1 model.train( cinp , steps = 1000, )\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n   2407     finally:\r\n   2408       rendezvous.record_done('training_loop')\r\n-> 2409       rendezvous.raise_errors()\r\n   2410 \r\n   2411   def evaluate(self, input_fn, steps=None, hooks=None, checkpoint_path=None,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py in raise_errors(self, timeout_sec)\r\n    126       else:\r\n    127         logging.warn('Reraising captured error')\r\n--> 128         six.reraise(typ, value, traceback)\r\n    129 \r\n    130     for k, (typ, value, traceback) in kept_errors:\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in reraise(tp, value, tb)\r\n    691             if value.__traceback__ is not tb:\r\n    692                 raise value.with_traceback(tb)\r\n--> 693             raise value\r\n    694         finally:\r\n    695             value = None\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/error_handling.py in catch_errors(self, source, session)\r\n     99     \"\"\"Context manager to report any errors within a block.\"\"\"\r\n    100     try:\r\n--> 101       yield\r\n    102     except Exception:  # pylint: disable=broad-except\r\n    103       self.record_error(source, sys.exc_info(), session)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py in _run_infeed(self, queue_ctx, session)\r\n    440       else:\r\n    441         for _ in queue_ctx.read_iteration_counts():\r\n--> 442           session.run(self._enqueue_ops)\r\n    443       logging.info('Infeed thread finished, shutting down.')\r\n    444 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    927     try:\r\n    928       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 929                          run_metadata_ptr)\r\n    930       if run_metadata:\r\n    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1151       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1152                              feed_dict_tensor, options, run_metadata)\r\n   1153     else:\r\n   1154       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1326     if handle is None:\r\n   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1328                            run_metadata)\r\n   1329     else:\r\n   1330       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1346           pass\r\n   1347       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1348       raise type(e)(node_def, op, message)\r\n   1349 \r\n   1350   def _extend_graph(self):\r\n\r\nNotFoundError: No registered 'PyFunc' OpKernel for CPU devices compatible with node {{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()\r\n\t.  Registered:  <no registered kernels>\r\n\r\n\t [[{{node PyFunc}} = PyFunc[Tin=[], Tout=[DT_INT64], token=\"pyfunc_3\", _device=\"/job:tpu_worker/task:0/device:CPU:0\"]()]]\r\n\t [[node input_pipeline_task0/while/IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py:3053)  = IteratorGetNext[_class=[\"loc:@input_pipeline_task0/while/InfeedQueue/split/1\"], output_shapes=[[32,28,28], [32]], output_types=[DT_FLOAT, DT_INT32], _device=\"/job:tpu_worker/replica:0/task:0/device:CPU:0\"](input_pipeline_task0/while/IteratorGetNext/Enter, ^input_pipeline_task0/while/Identity)]]\r\n`````\r\n\r\n", "comments": ["Hi, I'm trying the same as you at the moment. I'm going through docs and found a mention that a `tf.data.Dataset` is currently required, since the TPUEstimator moves the data enqueue to the remote machines. (https://cloud.google.com/tpu/docs/using-estimator-api).\r\n\r\nI tried wrapping the generator in a py_func, but py_func is one of the unsupported tensorflow ops.\r\n\r\nWhen I have time I'll try an example with a tfrecord dataset for mnist.\r\n\r\nEdit:\r\n\r\nTo summarize the best of my experience, using the free colab TPU's:\r\n- Dataset API works with tfrecords hosted on cloud compute buckets accessible from the remote TPU server. References to a local (on the host machine) tfrecord will throw errors. [REF](https://cloud.google.com/tpu/docs/tutorials/mnist)\r\n- Keras subclassing is incompatible with `keras_to_tpu_model`. Unsure about `TPUEstimator`, although [this post](https://medium.com/tensorflow/code-with-eager-execution-run-with-graphs-optimizing-your-code-with-revnet-as-an-example-6162333f9b08) implies that it should work.\r\n- Keras functional api + numpy data generator works as expected [REF](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/fashion_mnist.ipynb)", "@nathanin thanks for the reply \r\ncan you share any colab notebooks, it'll be really helpful \r\n", "hey, can you let me know what caused the error, and if its possible to use tpus on colab  in this setting,  thanks", "Hey all,\r\n\r\nYou cannot run a `PyFunc` on the `tpu_worker`, as there is no python interpreter on the TPU Machines. The recommendation is to use `tf.data` and TensorFlow ops if possible. (This is often a good idea anyway, as it can be very difficult to ensure Python can keep a Cloud TPU busy with data on many models.)\r\n\r\nHowever, if you'd really like to use a PyFunc as part of your training process, you can use the [`StreamingFilesDataset`](https://github.com/tensorflow/tensorflow/blob/7eda28c730e6602528d7c8bb528918e538ba6d72/tensorflow/python/tpu/datasets.py#L51) which allows you to run a dataset with a pyfunc in your Colab process, and stream the resulting tensors over to the Cloud TPU.\r\n\r\nHope that helps!\r\n-Brennan\r\n", "so, if is use tfrecords with data api and only tf ops, would it work? ", "Below is a script that writes a tf.data.Dataset at the end of an input pipeline to TFRecords. Those TFRecords can then be read in the input function to a TPU using tf.data.TFRecordDataset (with `tf.FixedLenFeature([], tf.string)` features passed to `tf.parse_single_example`, then `tf.parse_tensor` and `.set_shape` to read the serialized TensorProto back into a Tensor).\r\n\r\nThis way, the TPU just reads TFRecord files, which will be more performant anyway, plus, there's no need to do any data preprocessing (no PyFunc kernel needed) on the TPU.\r\n\r\n_Note._ At first, I tried to use [tf.data.experimental.TFRecordWriter](https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter), but couldn't make it work. So I ended up looping over the dataset in Eager mode.\r\n\r\n## Code\r\n```\r\n# Enable Eager to easily loop over the dataset, would be nice to not depend on Eager though.\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n```\r\n\r\n1. Define input pipeline returning a `tf.data.Dataset`. Logic in `to_tfrecord` (below) assumes it's a dataset yielding `dict`s, but can be modified to suit tuples as well.\r\n```python\r\ndef get_dataset(...):\r\n  \"\"\"Function returning a tf.data.Dataset. \r\n     Logic in `to_tfrecord` assumes it's a dataset yielding `dict`s.\"\"\"\r\n\r\n  ### Input pipeline defined here\r\n  return d\r\n```\r\n\r\n2. Define helpers to transform tensors -> bytestrings -> `BytesList`s -> `Feature`s -> `Example`.\r\n```python\r\ndef _tensor_to_feature(tensor):\r\n  \"\"\"Transforms (non-)scalar tensor to bytestring and \r\n     stores it as BytesList in a Feature.\"\"\"\r\n\r\n  serialized_tensor = tf.serialize_tensor(tensor)\r\n  bytes_string = serialized_tensor.numpy()\r\n  bytes_list = tf.train.BytesList(value=[bytes_string])\r\n\r\n  return tf.train.Feature(bytes_list=bytes_list)\r\n\r\n\r\ndef to_tfrecord(ds, filepath):\r\n  with tf.python_io.TFRecordWriter(filepath) as writer:\r\n    for feat_dict in ds:\r\n      features = {\r\n        'PUT_FEATURE_NAME_HERE': _tensor_to_feature(feat_dict['PUT_FEATURE_NAME_HERE']),\r\n        ...\r\n      }\r\n\r\n      # Create a Features message using tf.train.Example.\r\n      example_proto = tf.train.Example(features=tf.train.Features(feature=features))\r\n      example_string = example_proto.SerializeToString()\r\n\r\n      # Write to TFRecord\r\n      writer.write(example_string)\r\n```\r\n\r\n3. Write dataset to disk\r\n```python\r\nif __name__ == '__main__':\r\n  # Construct `tf.data.Dataset`\r\n  d = get_dataset(...)\r\n\r\n  # Write dataset to disk\r\n  out_file = ...\r\n  to_tfrecord(d, out_file)\r\n```", "@jppgks thanks for writing the snippet, ", "correct me if im wrong, answering my own question \r\n> so, if is use tfrecords with data api and only tf ops, would it work?\r\n\r\nit works if youre using a gcs bucket, but not if youre using local colab paths to store checkpoints and load data from", "@deepakmeena635 I believe so although I\u2019m not sure about reading in data from Colab. Anyway, you want to stream data from storage as close as possible to the TPU machine, so you would use GCS buckets in the same region and zone as the TPU is located in.", "Does any one have any solution of :\r\n\r\nNotFoundError: No registered 'Identity' OpKernel for 'TPU' devices compatible with node #37426\r\n", "TPU support only Tensorflow operations make sure there's no other\noperations/calculations thats what this thread is about though\n\nOn Tue, Sep 1, 2020 at 6:03 PM sachinsaxena021988 <notifications@github.com>\nwrote:\n\n> Does any one have any solution of :\n>\n> NotFoundError: No registered 'Identity' OpKernel for 'TPU' devices\n> compatible with node #37426\n> <https://github.com/tensorflow/tensorflow/issues/37426>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24099#issuecomment-684819022>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AI32SRK5GVUZ5RLTIT2QMN3SDTS2TANCNFSM4GHUOPFQ>\n> .\n>\n"]}, {"number": 24098, "title": "Fix SparseDenseCwise's broadcasting issue", "body": "This fix tries to address the issue raised in #24072. In `sparse_dense_cwise_mul/add` operations the broadcasting only support dense to sparse, though the validation was not captured.\r\n\r\nThis fix fixes the validation in SparseDenseBinaryOpShared so that error could be thrown correctly.\r\n\r\nThis fix fixes #24072.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Also ping @rmlarsen to take a look. Thanks for all the help so far!", "@yongtang Thanks for the contribution. I will review this tomorrow."]}, {"number": 24097, "title": "Fix deprecated warning of sparse_merge", "body": "While playing with tf.sparse.to_indicator, the following warning surfaces:\r\n```\r\nroot@ubuntu:/v# python\r\nPython 2.7.15rc1 (default, Nov 12 2018, 14:31:15)\r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> v = tf.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20], [1, 1, 4, 2])\r\n>>> tf.sparse.to_indicator(v, 2)\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/sparse_ops.py:1491: sparse_merge (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nNo similar op available at this time.\r\n```\r\n\r\nThe reason is that `tf.sparse.to_indicator` calls `sparse_merge` directly.\r\n\r\nThis fix adds `sparse_merge_impl` so that the above warning could be avoided.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Ping @ymodak to take a look.", "Nagging Reviewer @annarev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 104 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24096, "title": "Build failure with Ubuntu 18.04", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: f1edf205b290c7fdeefe0b73d27c50a3cab67dcb\r\n- Python version: Python 2.7.15rc1\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): Build label: 0.19.1\r\n- GCC/Compiler version (if compiling from source): gcc version 7.3.0 (Ubuntu 7.3.0-27ubuntu1~18.04) \r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory:n/a\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhile trying to build tf from source (master branch) on Ubuntu 18.04, the following error surfaces:\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:2211:1: C++ compilation of rule '//tensorflow/core:lib_internal_impl' failed (Exit 1)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:150:0,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/lib/random/random_distributions.h:28,\r\n                 from ./tensorflow/core/lib/random/simple_philox.h:24,\r\n                 from ./tensorflow/core/lib/random/distribution_sampler.h:38,\r\n                 from tensorflow/core/lib/random/random_distributions.cc:16:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(2) long long int; TgtPacket = __vector(4) float]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet Eigen::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(4) float]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/PacketMath.h:574:33:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(2) long long int' to type '__vector(4) float'\r\n   return static_cast<TgtPacket>(a);\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) float; TgtPacket = __vector(2) long long int]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet Eigen::internal::pldexp_float(Packet, Packet) [with Packet = __vector(4) float]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/PacketMath.h:578:33:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(4) float' to type '__vector(2) long long int'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) long long int; TgtPacket = __vector(8) float]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet Eigen::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(8) float]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX/PacketMath.h:423:33:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(4) long long int' to type '__vector(8) float'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(8) float; TgtPacket = __vector(4) long long int]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet Eigen::internal::pldexp_float(Packet, Packet) [with Packet = __vector(8) float]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/arch/AVX/PacketMath.h:427:33:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:140:10: error: invalid static_cast from type 'const __vector(8) float' to type '__vector(4) long long int'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 173.313s, Critical Path: 20.46s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 250 processes: 250 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n$ ./configure # with default options\r\n$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```", "comments": ["TensorFlow is tested against GCC 4 versions, that can be a possible reason for above bazel build failure. Please take a look at [bazel build option](https://www.tensorflow.org/install/source#bazel_build) from this link to know more about GCC compatibility.", "Hi, same Problem here with comparable setup. Trying to compile for AVX512F.\r\n\r\nUsing the provided build option \r\n\r\n> --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"\r\n\r\ndoes not change the error. I will try with gcc 4 now and report back.", "I was able to build successfully with gcc-4. \r\nEdit: False, i missed the avx512f option.\r\n\r\nI used this option instead which fixed my problem in any newer gcc version:\r\nhttps://github.com/tensorflow/tensorflow/issues/4775#issuecomment-282522157\r\n(i am building for skylake)\r\n\r\nEdit2: It only works if i don't use the ABI compatibility option. The resulting whl installation still has to be tested, i will keep updating on this. Building on Gcc/G++-6 currently with Bazel 0.19.2 on Ubuntu 18.04\r\n\r\nThe original Issue was in my case definitely caused by enabling avx512f\r\n\r\nEdit3: It all worked out fine by using -march=skylake-avx512 for building in the end (core i9). \r\n\r\nOP: can you relate to any of this?", "Confirmed with the following environment.\r\n\r\nUbuntu 16.04 hwe (hardware enhancement): kernel 4.15.0-39-generic\r\nBazel: 0.19.2\r\ngcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nTF version: bb058ae291d565aa985e3fa8437ee026ad5c10f6\r\nPython 2.7.12\r\n\r\n```\r\nERROR: /home/byronyi/tensorflow/tensorflow/lite/kernels/BUILD:57:1: C++ compilation of rule '//tensorflow/lite/kernels:eigen_support' failed (Exit 1)\r\nIn file included from external/eigen_archive/Eigen/Core:150:0,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/eigen_tensor_reduced_instantiations_oss.h:26,\r\n                 from ./tensorflow/lite/kernels/internal/optimized/eigen_spatial_convolutions.h:37,\r\n                 from tensorflow/lite/kernels/eigen_support.cc:20:\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(2) long long int; TgtPacket = __vector(4) float]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet EigenForTFLite::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(4) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/PacketMath.h:574:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(2) long long int' to type '__vector(4) float'\r\n   return static_cast<TgtPacket>(a);\r\n                                  ^\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) float; TgtPacket = __vector(2) long long int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet EigenForTFLite::internal::pldexp_float(Packet, Packet) [with Packet = __vector(4) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/PacketMath.h:578:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(4) float' to type '__vector(2) long long int'\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(4) long long int; TgtPacket = __vector(8) float]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:25:40:   required from 'Packet EigenForTFLite::internal::pfrexp_float(const Packet&, Packet&) [with Packet = __vector(8) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/AVX/PacketMath.h:423:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(4) long long int' to type '__vector(8) float'\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h: In instantiation of 'TgtPacket EigenForTFLite::internal::pcast(const SrcPacket&) [with SrcPacket = __vector(8) float; TgtPacket = __vector(4) long long int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/SSE/../Default/GenericPacketMathFunctions.h:35:37:   required from 'Packet EigenForTFLite::internal::pldexp_float(Packet, Packet) [with Packet = __vector(8) float]'\r\nexternal/eigen_archive/Eigen/src/Core/arch/AVX/PacketMath.h:427:33:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:140:34: error: invalid static_cast from type 'const __vector(8) float' to type '__vector(4) long long int'\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(2) long long int; Packet = __vector(4) float]' used but never defined\r\n preinterpret(const Packet& a); /* { return reinterpret_cast<const Target&>(a); } */\r\n ^\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(4) float; Packet = __vector(2) long long int]' used but never defined\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(4) long long int; Packet = __vector(8) float]' used but never defined\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:157:1: warning: inline function 'Target EigenForTFLite::internal::preinterpret(const Packet&) [with Target = __vector(8) float; Packet = __vector(4) long long int]' used but never defined\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 161.557s, Critical Path: 16.13s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]\r\nINFO: 1142 processes: 1142 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@benoitsteiner Mind to take a look here?", "@yongtang @octogondude @ymodak Reverting 029389e0c69919392bd9a0d070d0d58498b0fb1d seems to work.\r\n\r\nPing @ezhulenev", "I have a PR with Eigen update ready, didn't want to submit it on Friday, will do first thing tomorrow, it should fix this issue.", "I have the same issue.", "I'd warn you that AVX512 may not be properly supported.  I've had intermittent system lockups with TF 1.12 and an earlier version when compiling using -march=native on an i9-7940x which enables AVX512 support.  In my case, the code compiled and ran fine but every so often, when running TF, UBT 18.04 would crash with no outward sign what was going on (no overtemp, or kernel log entry,...) \r\n\r\nThere's a few issues open related to AVX512.  The suggested answer seems to be to try using MKL instead.  In my case, I'm using a GPU so simply disabling AVX512 was an acceptable option.", "Is it fixed after https://github.com/tensorflow/tensorflow/commit/43344c5014b2d33074feb6ff537261f9827453a6 ?", "Thanks @ezhulenev. I have tried with the latest master and the issue has been resolved \ud83d\udc4d ", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24095, "title": "Missing memmapped_file_system for Windows BUILD", "body": " \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.17.1\r\n- GCC/Compiler version (if compiling from source): msvc \r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\nI'm building DeepSpeech for Windows that is using tensorflow r1.12, it uses memmapped code that is causing undeclared errors due to the exclusion of memmapped_file_system.h for Windows [Here](https://github.com/mozilla/tensorflow/blob/bea86c1e884730cf7f8615eb24d31872c198c766/tensorflow/core/BUILD#L2482), if I remove the empty select for Windows it compiles.\r\nThis a possible bug or there is a reason for excluding those files for Windows BUILD?\r\n\r\nI posted on StackOverflow but no one answered.\r\n\r\n\r\n ", "comments": ["This would be useful for the DeepSpeech project progress, so any chance you could take a quick look at this @mrry or point us in the direction of the right person to talk to? Thanks!", "Hello, @mrry can we get some feedback on this patch ? Should this be upstreamed ? Thanks !", "Hello, can we get some update here ? @mrry @petewarden ", "Still no activity on that ? @petewarden @mrry ", "Or maybe @hgadig ?", "As far as I can tell, it should be fine to build that code on Windows. The version history is fuzzy, but I think @meteorcloudy added the exception originally [in October 2016](https://github.com/tensorflow/tensorflow/commit/c5ab3dd177dc16bb211821e38219f350a613b5e8) when it didn't work on Windows, but support for `WinReadOnlyMemoryRegion` was contributed [shortly afterwards](https://github.com/tensorflow/tensorflow/commit/e2d51a87f0727f8537b46048d8241aeebb6e48d6).", "Thanks @mrry ."]}, {"number": 24094, "title": "Profiling is not working when two machines run with FULL_TRACE", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNone\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nNone\r\n- TensorFlow version (use command below):\r\n1.11\r\n- Python version:\r\n2.7.12\r\n- Bazel version (if compiling from source):\r\n0.15.0\r\n- GCC/Compiler version (if compiling from source):\r\n5.4.0 \r\n- CUDA/cuDNN version:\r\n9.0, 7.0.5\r\n- GPU model and memory:\r\nTitanXP, 16GB\r\n\r\n**Describe the current behavior**\r\nI've added GPU tracing for distributed training to get more accurate GPU trace.\r\nProfiling is not working when two machines(workers) run with FULL_TRACE in a distributed mode(PS). However, if only one machine tries it then it works.  I've seen this line (https://github.com/tensorflow/tensorflow/blob/284b2783c4676c9d00199a585db9d6d07d9e68be/tensorflow/core/platform/default/device_tracer.cc#L452). Is it related to this issue? Only one process can profile for a distributed job?\r\n\r\n**Describe the expected behavior**\r\nProfiling works for both machines.\r\n\r\n**Code to reproduce the issue**\r\nThis is the TF code that I modified for GPU tracing in a distributed mode.\r\nhttps://github.com/snuspl/tensorflow/tree/r1.11\r\n\r\n**Other info / logs**\r\nProcesses are hanged after the log of \"successfully opened CUDA library libcupti.so.9.0 locally\".\r\n\r\n```\r\nINFO:139724566263552:PARALLAX:global step: 300, loss: 8.826963, throughput: 2.748203 steps/sec\r\nINFO:139838103807744:PARALLAX:global step: 300, loss: 8.755314, throughput: 2.748064 steps/sec\r\nINFO:139724566263552:PARALLAX:global step: 350, loss: 8.723333, throughput: 2.733742 steps/sec\r\nINFO:139838103807744:PARALLAX:global step: 350, loss: 8.800816, throughput: 2.733653 steps/sec\r\nINFO:139724566263552:PARALLAX:global step: 400, loss: 8.311481, throughput: 2.760322 steps/sec\r\nINFO:139838103807744:PARALLAX:global step: 400, loss: 8.774604, throughput: 2.757400 steps/sec\r\n2018-12-01 23:56:29.950572: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally\r\n2018-12-01 23:56:29.952852: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally\r\n2018-12-01 23:56:33.335883: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally\r\n2018-12-01 23:56:33.338704: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.0 locally\r\n\r\n```\r\n\r\n", "comments": []}, {"number": 24093, "title": "quant_graph caonvert to .tflite:  Converting unsupported operation: Dequantize", "body": "I want to know where is wrong, please someone help me.\r\n\r\nversion:\r\nPython 2.7.12 (default, Nov 12 2018, 14:36:49)\r\n\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'1.13.0-dev20181201'\r\n>>> \r\n\r\nlog:\r\nsuper@super-virtual-machine:~/Downloads$ tflite_convert \\\r\n>   --output_file=/home/super/Downloads/mobilenet_v1_0.50_128/mobilenet_v1.tflite \\\r\n>   --graph_def_file=/home/super/Downloads/mobilenet_v1_0.50_128/quantized_graph.pb \\\r\n>   --inference_type=QUANTIZED_UINT8 \\\r\n>   --input_arrays=input \\\r\n>   --output_arrays=MobilenetV1/Predictions/Reshape_1 \\\r\n>   --mean_values=128 \\\r\n>   --std_dev_values=127\r\n2018-12-01 22:45:07.489914: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/tflite_convert\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 442, in main\r\n    app.run(main=run_main, argv=sys.argv[:1])\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 438, in run_main\r\n    _convert_model(tflite_flags)\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/tflite_convert.py\", line 191, in _convert_model\r\n    output_data = converter.convert()\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/lite.py\", line 455, in convert\r\n    **converter_kwargs)\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 430, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"/home/super/.local/lib/python2.7/site-packages/tensorflow/lite/python/convert.py\", line 204, in toco_convert_protos\r\n    \"TOCO failed. See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.\r\n2018-12-01 22:45:08.455379: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.471418: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.476136: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.476205: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.476247: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.476287: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.476516: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.477495: I tensorflow/lite/toco/import_tensorflow.cc:1299] Converting unsupported operation: Dequantize\r\n2018-12-01 22:45:08.481725: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 138 operators, 226 arrays (0 quantized)\r\n2018-12-01 22:45:08.497846: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 138 operators, 226 arrays (0 quantized)\r\n2018-12-01 22:45:08.499427: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 57 operators, 132 arrays (1 quantized)\r\n2018-12-01 22:45:08.506318: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 57 operators, 132 arrays (1 quantized)\r\n2018-12-01 22:45:08.506970: F tensorflow/lite/toco/tooling_util.cc:1698] Array MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array MobilenetV1/MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nAborted (core dumped)\r\n\r\n", "comments": ["It looks like you have some tflite unsupported ops in your model. Please refer [TensorFlow Lite & TensorFlow Compatibility Guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tf_ops_compatibility.md#tensorflow-lite--tensorflow-compatibility-guide).", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24092, "title": "Update docs for tf.contrib.layers.optimize_loss  \u2026", "body": "Easier to read and corrects name of example callable", "comments": ["@fchollet Thank you for the PR review.  I made changes according to your comments about 20 days ago and thought I left a comment that would alert you, but I am \"@-mentioning\" you again just in case."]}, {"number": 24091, "title": "how to open model.cpkt in ubuntu", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["You can try this:\r\nwith tf.Session() as sess:\r\n    saver = tf.train.import_meta_graph('/tmp/model.ckpt.meta')\r\n    saver.restore(sess, \"/tmp/model.ckpt\")\r\n\r\nEven though there is not a physical file called /tmp/model.ckpt. It is the prefix of filenames created for the checkpoint. Users only interact with the prefix instead of physical checkpoint files. This [example](https://github.com/tensorflow/tensorflow/blob/f1edf205b290c7fdeefe0b73d27c50a3cab67dcb/tensorflow/python/training/saver.py#L1660) from saver.py should help.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 24089, "title": "Enable contrib/opt/external_optimizer in eager mode", "body": "Add a new interface that is eager compatible.  \r\n\r\nWhich requires both loss and equalities/inequalities passed in as a function\r\n\r\nAddresses https://github.com/tensorflow/tensorflow/issues/23973\r\n\r\n", "comments": ["It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 24088, "title": "Configurable AWS logging for S3 filesystem", "body": "AWS logging for S3 is too verbose and should be configurable.\r\n\r\n```\r\nexport AWS_LOG_LEVEL=3\r\n```\r\n\r\nUsing TF_CPP_MIN_LOG_LEVEL for the AWS log handler can lead to a high\r\nrate of aws logging which drowns out less frequent  logs\r\n\r\nRelated commits 7bb0592ef2f5ee4ac9261448daf51446cfc19941, f67a7a4af7602b383a914340bc5b50ca099f30cb\r\nIssue #21898", "comments": [" @yongtang @qlzh727 @ymodak any chance to look at these changes?", "\ud83d\ude4f ", "@jquadrino Sorry for the late reply. The email slip through my email. Leaved one question otherwise it looks fine to me.", "@yongtang It now defaults to `TF_CPP_MIN_LOG_LEVEL` if `AWS_LOG_LEVEL` is not found.", "@xiejw  Could you please take a look and approve. ", "Please reassign the owner for this code path. ", "As mentioned, I am not the owner for this part. Please find another owner or seek admin's help. ", "@ mrry can you please review this PR", "@rthadur could we rerun the checks?", "Just to be clear, in order to use this you must set `AWS_LOG_LEVEL` to an integer defined in `core/platform/default/logging.h`. For example, to set to to FATAL:\r\n\r\n```\r\nexport AWS_LOG_LEVEL=3\r\n```\r\n\r\nAnd now the s3 plugin won't spam your logs!!!", "I am trying to undertand how to use this as for us this is quite a critically important bug to have fixed. @eggie5 you mention that we should set `export AWS_LOG_LEVEL=3` to use the fix. As defined in `core/platform/default/logging.h` we have:\r\n\r\n```\r\nconst int INFO = 0;            // base_logging::INFO;\r\nconst int WARNING = 1;         // base_logging::WARNING;\r\nconst int ERROR = 2;           // base_logging::ERROR;\r\nconst int FATAL = 3;           // base_logging::FATAL;\r\nconst int NUM_SEVERITIES = 4;  // base_logging::NUM_SEVERITIES;\r\n```\r\n\r\nSo this would set the logging level to `FATAL`. But, where do I actually set this? Can this be passed as a parameter when starting for example `tensorflow/serving:latest-gpu`?\r\n\r\nRunning as `docker run -e  AWS_LOG_LEVEL=3  tensorflow/serving:1.13.0 ` doesn't appear to have an effect\r\n\r\n"]}, {"number": 24087, "title": "Add benchmarks for list_files dataset", "body": "This PR adds a benchmark test for `tf.data.Dataset.list_files()`.", "comments": ["@jsimsa Thanks for your review! The issues are fixed now."]}, {"number": 24086, "title": "[Intel MKL] Adding support to handle FusedConv2D", "body": "This commit adds support to handle Grappler-fused Conv2D operators\r\nin MKL layout pass.\r\n\r\nSome changes are from clang format check, and not related to handling\r\nof fusion.", "comments": ["I am closing this PR temporarily until we figure out internal logistics. Sorry for trouble.", "Opening up again..", "@penpornk Thanks for comments. I have addressed all of them. Pls take a look.", "@penpornk Thanks. Pls take a look.", "@penpornk Thanks for feedback. Pls take a look.\r\n", "Hi @penpornk thanks for approving. I am not able to access the log for Windows bagel build failure - not sure if it is related to my PR also.", "@nhasabni No worries! Those are existing failures. This PR has already been pulled in and is getting merged."]}, {"number": 24085, "title": "Update the variable names in list_files_test to be more readable", "body": "This PR updates some variable names in list_files_test to be more readable. ", "comments": ["@jsimsa Could you help review this PR when you have time?", "@jsimsa Sorry that I missed an indentation issue. Just submit a commit to fix it. Could you trigger the test again? Thanks!"]}, {"number": 24084, "title": "Fix the unexpected behavior of RandomUniform in the eager mode", "body": "This PR fixes #23882. It may also be related to #23407. cc: @alextp  \r\n\r\nIn the eager mode, the kernels are cached to improve the performance, where the `cache_key` is generated based on the Op's attributes. That means, two Ops with the same attributes will use the same cached kernel. However, this will cause some unexpected behaviors for RandomUniform as described in #23882. \r\n\r\nThis PR adds an `op_id` attribute to `RandomUniform` Op to enable users to decide whether to reuse the cached RandomUniform kernel or create a new RandomUniform kernel.", "comments": ["@josh11b @asimshankar we have a longer-term plan to improve the random state situation but this is an acceptable short-term fix (in principle; I haven't looked at the details carefully). What should we do?", "Thank you, @alextp! @josh11b @asimshankar Do you have any suggestion/comments on this PR? ", "A kindly reminder that this PR has been a while. @alextp @josh11b Do you get a chance to review this PR?", "Nagging Reviewer @alextp, @josh11b: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "I'm closing this as we'd like to address these problems with the more comprehensive solution described in https://github.com/tensorflow/community/pull/38 . Can you comment on that RFC instead?\r\n\r\nThanks!"]}, {"number": 24083, "title": "Constant folding doesn't fold weights that are using weight normalization ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.15.0?\r\n- GCC/Compiler version (if compiling from source):  5.4.0\r\n- CUDA/cuDNN version: CUDA 10.0.130, CUDNN 7.4.1.5\r\n- GPU model and memory: TITAN V 10956 MB\r\n\r\n**Describe the current behavior**\r\nI have a 1D convolution layer whose weights are using weight normalization (Salimans & Kingma, 2016). The weights to the convolution are processed using the following equation: `w = g * v/2-norm(v)`. After running the constfold optimizer, these operations on the weights are still present, even though they should be folded.\r\n\r\n**Describe the expected behavior**\r\nI would expect the normalization ops to be folding into the weights, so that only one Const/read node remains.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport math\r\nfrom tensorflow.core.protobuf import meta_graph_pb2\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\nfrom tensorflow.python.grappler import tf_optimizer\r\n\r\ndef conv_graph(x, output_name='output', kernel_width=3, in_dim=1024, out_dim=1024):\r\n  \"\"\"Applies convolution with gated linear units on x.\r\n  https://github.com/NVIDIA/OpenSeq2Seq/blob/master/open_seq2seq/parts/convs2s/conv_wn_layer.py\r\n    Args:\r\n      x: A float32 tensor with shape [batch_size, length, in_dim]\r\n    Returns:\r\n      float32 tensor with shape [batch_size, length, out_dim].\r\n  \"\"\"\r\n  # Define Variables\r\n  conv_out_size = 2 * out_dim\r\n  V_std = math.sqrt(4.0 * 0.8 / (kernel_width * in_dim))\r\n  V = tf.get_variable('V', shape=[kernel_width, in_dim, conv_out_size],\r\n                      initializer=tf.random_normal_initializer(mean=0, stddev=V_std),\r\n                      trainable=True)\r\n  V_norm = tf.norm(V.initialized_value(), axis=[0, 1])\r\n  g = tf.get_variable('g', initializer=V_norm, trainable=True)\r\n  W = tf.reshape(g, [1, 1, conv_out_size]) * tf.nn.l2_normalize(V, [0, 1])\r\n\r\n  output = tf.nn.conv1d(value=x, filters=W, stride=1, padding=\"VALID\")\r\n  output = tf.identity(output, name=output_name)\r\n  return output\r\n\r\ndef apply_constfold(frozen_graph, output_nodes):\r\n  graph = tf.Graph()\r\n  with graph.as_default():\r\n    tf.import_graph_def(frozen_graph, name=\"\")\r\n  grappler_meta_graph_def = tf.train.export_meta_graph(graph_def=graph.as_graph_def(add_shapes=True), graph=graph)\r\n\r\n  _to_bytes = lambda s: s.encode(\"utf-8\", errors=\"surrogateescape\")\r\n  output_collection = meta_graph_pb2.CollectionDef()\r\n  output_list = output_collection.node_list.value\r\n  for i in output_nodes:\r\n    if isinstance(i, tf.Tensor):\r\n      output_list.append(_to_bytes(i.name))\r\n    else:\r\n      output_list.append(_to_bytes(i))\r\n  # TODO(laigd): use another key as the outputs are really not train_op.\r\n  grappler_meta_graph_def.collection_def[\"train_op\"].CopyFrom(output_collection)\r\n  rewriter_config = rewriter_config_pb2.RewriterConfig()\r\n  rewriter_config.optimizers.extend([\"constfold\"])\r\n\r\n  session_config_with_trt = tf.ConfigProto()\r\n  session_config_with_trt.graph_options.rewrite_options.CopyFrom(\r\n      rewriter_config)\r\n  frozen_graph = tf_optimizer.OptimizeGraph(session_config_with_trt, grappler_meta_graph_def, graph_id=b\"tf_graph\")\r\n  return frozen_graph\r\n\r\nif __name__ == '__main__':\r\n  with tf.Graph().as_default():\r\n    # Create graph\r\n    x = tf.placeholder(dtype=tf.float32, shape=(None, None, 1024), name='input')\r\n    y = conv_graph(x)\r\n    # Initialize\r\n    with tf.Session() as sess:\r\n      sess.run(tf.global_variables_initializer())\r\n      # Freeze graph\r\n      frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n          sess,\r\n          sess.graph_def,\r\n          output_node_names=['output'])\r\n\r\n  print('Nodes before:')\r\n  [print(n.name, n.op) for n in frozen_graph.node]\r\n\r\n  # const folding\r\n  frozen_graph = apply_constfold(frozen_graph, output_nodes=['output'])\r\n\r\n  print('----------------------------------------')\r\n  print('Nodes after:')\r\n  [print(n.name, n.op) for n in frozen_graph.node]\r\n```\r\n**Output of script**\r\n```\r\nNodes before:\r\ninput\r\nV\r\nV/read\r\ng\r\ng/read\r\nReshape/shape\r\nReshape\r\nl2_normalize/Square\r\nl2_normalize/Sum/reduction_indices\r\nl2_normalize/Sum\r\nl2_normalize/Maximum/y\r\nl2_normalize/Maximum\r\nl2_normalize/Rsqrt\r\nl2_normalize\r\nmul\r\nconv1d/ExpandDims/dim\r\nconv1d/ExpandDims\r\nconv1d/ExpandDims_1/dim\r\nconv1d/ExpandDims_1\r\nconv1d/Conv2D\r\nconv1d/Squeeze\r\noutput\r\n----------------------------------------\r\nNodes after:\r\ninput\r\nV\r\nReshape\r\nl2_normalize/Sum/reduction_indices\r\nl2_normalize/Maximum/y\r\nconv1d/ExpandDims/dim\r\nconv1d/ExpandDims_1/dim\r\nV/read\r\nconv1d/ExpandDims\r\nl2_normalize/Square\r\nl2_normalize/Sum\r\nl2_normalize/Maximum\r\nl2_normalize/Rsqrt\r\nl2_normalize\r\nmul\r\nconv1d/ExpandDims_1\r\nconv1d/Conv2D\r\nconv1d/Squeeze\r\noutput\r\n```\r\n\r\nEdit: updated repro for 1.13", "comments": ["@rmlarsen Have you had any chance to look at this?", "Running with TF_CPP_MIN_VLOG_LEVEL=1, I get the following output from constant_folding.cc. Looks like the size would be too large. The resulting weights are [1, 3, 1024, 2048] (around 20mb). This is greater than the hardcoded limit in constant_folding.cc (10mb). \r\n\r\nTF-TRT heavily relies on constant folding, is there a way we can override this size limit?\r\n\r\nAlso, the resulting weight after constant folding is the same size as the input (it doesn't change the shape, it just gets scaled). So this logic of having a size limit doesn't quite make sense in this case.\r\n\r\n```\r\n2018-12-13 20:49:37.173127: I tensorflow/core/grappler/optimizers/constant_folding.cc:1236] Failed to fold node name: \"V/read\"\r\nop: \"Identity\"\r\ninput: \"V\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\n\r\nError message: Invalid argument: Can't fold ConstantFolding/V/read-folded, its size would be too large\r\n```"]}, {"number": 24082, "title": "proto", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing this issue due to lack of information to work with. Please post a new issue by providing all the information asked by the template. Thanks!"]}, {"number": 24081, "title": "Split convolution invocation into preparation and actual invocation", "body": "- split DoConvolve into:\r\n\r\nPrepareForConvolution\r\nDoConvolve\r\n\r\n- split DoConvolveBackwardData into:\r\n\r\nPrepareForConvolutionBackwardData\r\nDoConvolveBackwardData\r\n\r\n- split DoConvolveBackwardFilter into:\r\n\r\nPrepareForConvolutionBackwardFilter\r\nDoConvolveBackwardFilter\r\n\r\nPrepareForConvolutionXXX would allocate scratch memory.\r\nDoConolveXXX would invoke actual convolution algorithms.\r\n\r\nImplement forward convoution, backward input convolution, backward filter\r\nconvolution on CUDA path.", "comments": ["@timshen91 per discussion please find my PR to separate convolution invocation into 2 parts here. This PR only covers logic for CUDA path.\r\n\r\nImplementation for ROCm shall be submitted after other PRs from @deven-amd for StreamExecutor.", "@timshen91 I've addressed all the issues on CI other than 2 test targets: Windows Bazel, Windows Bazel GPU. From the logs I don't think they are relevant to this PR.", "@timshen91 Just want to understand how to proceed on this PR. I applied `kokoro:force-run` label again and we are stuck on MacOS/Windows builds, but from the logs they have nothing to do with this PR.\r\n\r\nBTW, we are about to submit a couple more PRs which includes more refactoring to StreamExecutor which makes larger part it shared between CUDA and ROCm paths.", "@jlebar do you know how to get this patch merged? I don't seem to have the option.", "> @jlebar do you know how to get this patch merged? I don't seem to have the option.\r\n\r\nForwarded instructions offline, since they're not public."]}, {"number": 24080, "title": "Add link to CPU Artifacts to README.md for ppc64le", "body": "adds links to cpu artifiacts for nightly and release builds\r\nReplaces \"IBM ppc64le\" with \"Linux ppc64le\"\r\nUses the build of every commit for build status and not the nightly artifact build.\r\n  - This last change is also made to the GPU build in this commit", "comments": ["@qlzh727, can you help move this along the merge process. ", "Sure, kicking off the tests now. Sorry for the delay."]}, {"number": 24079, "title": "Inconsistent keras API regarding variable_scope", "body": "I am trying to reuse some layers (share the weights), so I've used some variable_scope and inside I created some keras layers. For some type of layers like tf.keras.layers.Dense, the weights are shared, for others like tf.keras.layers.Embedding for example new weights are created instead of being reused.\r\n\r\nProbably similar to https://github.com/tensorflow/tensorflow/issues/20426 and https://github.com/tensorflow/tensorflow/issues/14703 \r\n\r\nCan you advise a fix as it seems variable_scope are being deprecated in TF2.0 ?\r\n\r\nThanks\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef mlp( s ):\r\n    word_embeddings = tf.get_variable(\"we\",[3, 10])\r\n    emb = tf.nn.embedding_lookup(word_embeddings, s)\r\n    return emb\r\n\r\ndef mlpkeras( s ):\r\n    emb = tf.keras.layers.Embedding( 3, 10)(s)\r\n    return emb\r\n\r\ndef mlp2( s ):\r\n    emb = tf.keras.layers.Dense(10)(s)\r\n    return emb\r\n\r\ndef testEmbeddingTFLayer():\r\n    sess = tf.Session()\r\n    input = tf.placeholder(tf.int32, ((None,20)))\r\n\r\n    with (tf.variable_scope(\"main\", reuse=False) ):\r\n        res1 = mlp(input)\r\n\r\n    with (tf.variable_scope(\"main\", reuse=True) ):\r\n        res2 = mlp(input)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    print(\"TestEmbeddingTFLayer :\")\r\n    print( sess.run( [tf.reduce_sum( (res1-res2)**2)] , feed_dict= {input:np.zeros((1,20)) } ) ) \r\n\r\ndef testDenseLayer():\r\n    sess = tf.Session()\r\n    input = tf.placeholder(tf.float32, ((None,20)))\r\n\r\n    with (tf.variable_scope(\"main\", reuse=False) ):\r\n        res1 = mlp2(input)\r\n\r\n    with (tf.variable_scope(\"main\", reuse=True) ):\r\n        res2 = mlp2(input)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    print(\"TestDenseLayer : \")\r\n    print( sess.run( [tf.reduce_sum( (res1-res2)**2)] , feed_dict= {input:np.zeros((1,20)) } ) ) \r\n\r\n\r\ndef testEmbeddingKerasLayer():\r\n    sess = tf.Session()\r\n    input = tf.placeholder(tf.int32, ((None,20)))\r\n\r\n    with (tf.variable_scope(\"main\", reuse=False) ):\r\n        res1 = mlpkeras(input)\r\n\r\n    with (tf.variable_scope(\"main\", reuse=True) ):\r\n        res2 = mlpkeras(input)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    print(\"TestEmbeddingKerasLayer :\")\r\n    print( sess.run( [tf.reduce_sum( (res1-res2)**2)] , feed_dict= {input:np.zeros((1,20)) } ) ) \r\n\r\n\r\n\r\ntestEmbeddingTFLayer() \r\ntestDenseLayer()\r\ntestEmbeddingKerasLayer()\r\n```\r\nPrints 0 , 0 , and not 0 (expecting 0)\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: CUDA Driver Version / Runtime Version          9.2 / 8.0\r\n- GPU model and memory: 1080Ti\r\n\r\n\r\n", "comments": ["The plan in 2.x is that variable sharing happens by sharing objects. So instead of being a function, `mlp` could be a `tf.keras.Model` subclass for example, creating its sub-Layers and variables only once and saving them in attributes. Then it would be called twice instead of created twice. This is much more efficient when executing eagerly, and seems to produce fewer hard-to-debug errors.\r\n\r\nDoes that work for you?", "Not really. It seems to work for the simple cases but still miss some important features.\r\nFor example I can't figure how I would do some polyak averaging using the tf.keras.Model API in a generic way.\r\n\r\n```\r\nnetwork1 = MyModel1()\r\nnetwork2 = tf.keras.models.clone_model(network1)\r\nvarnetwork1 = getVariables( network1 )  \r\nvarnetwork2 = getVariables( network2 )\r\nupdates = tf.group( [ tf.assign(v2, v2 *polyak + (1-polyak)*v1 )  for (v1,v2) in zip(varnetwork1,varnetwork2) ] )\r\n```\r\nIt seems model.trainable_variables behavior is not defined by the API\r\n\r\nKeras API is nice for beginners, but it is also quite verbose, (you must declare your layer first in init or build before being able to use them inside call)\r\n\r\nWouldn't this also mean tf.nn and tf.nn.layers shouldn't be use them inside model.__call__ ?\r\n\r\nSerialization of Keras Models is also non trivial : \r\nOnce you start sharing weights between two networks. \r\n```\r\nsm = MySharedModel()\r\nmodel1 = MyModel( sm )\r\nmodel2 = MyModel( sm )\r\nsavedModel1 = serialize(model1)\r\nsavedModel2 = serialize(model2)\r\nloadModel1 = load(savedModel1)\r\nloadModel2 = load(savedModel2)\r\n```\r\nNow loadModel1 and loadModel2 don't share weights.", "@unrealwill,\r\nSorry for the delayed response. Can you please confirm if this issue still persists? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "out of date", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24079\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24079\">No</a>\n"]}, {"number": 24078, "title": "TFTRT: Support StridedSlice Op for most cases + unit tests", "body": "We have found that StridedSlice is used throughout many models that we are interested in converting, such as NASNet and ConvSeq2Seq. This PR provides support for the most common usages of StridedSlice. The NodeValidator will prevent attempted conversion for the usages that we do not yet support.\r\n\r\nSince TRT does not have a StridedSlice or Slice layer, we instead create an IPaddingLayer with negative padding. Since IPaddingLayer requires NCHW inputs and only allows modification of HW dimensions, we sometimes have to reshape or transpose the input to conform to these restrictions.", "comments": ["@azaks2 @smit-hinsu\r\nCould you please review.", "I am on vacation this week and will review this PR next week. Let me know in case you need to get this reviewed sooner.", "> I am on vacation this week and will review this PR next week. Let me know in case you need to get this reviewed sooner.\r\n\r\n@smit-hinsu Next week sounds good. The ideal would be to get it in TF1.13. Thank you. ", "Let's rebase against https://github.com/tensorflow/tensorflow/pull/24275 to remove the conflicts. Thanks!", "Fixed the conflicts."]}, {"number": 24077, "title": "Tensorflow on AMD G.P.U when will you support??", "body": "I love tensorflow but i have an amd GPU and training my models on my cpu is tedious \r\nwhen will you support amd or will it ever happen", "comments": ["They are working on [upstreaming](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream) their port and it's only going to work on Linux. Don't know when they'll be completed since this could take until the next quarter but I'm not an AMD employee so I can not specifically say ... \r\n\r\nAs in the situation of other OS's, for Windows they are looking into porting [HIP](https://github.com/ROCm-Developer-Tools/HIP) to their [PAL](https://github.com/GPUOpen-Drivers/pal) and for macOS try contacting Apple to see if they'll attempt to port Tensorflow kernels to their Metal API ... ", "I think it was resolved. Thanks @Degerz. \r\nIn future, if you have support questions, please post them on [Stackoverflow]( https://stackoverflow.com/) Thanks!"]}, {"number": 24076, "title": "CMake build Tensorflow C++ on Windows 10 Error : Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: the latest version\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): I am using CMake\r\n- GCC/Compiler version (if compiling from source): gcc 6.30\r\n- CUDA/cuDNN version: Installing only the CPU version\r\n- I am also using Microsoft Visual Studio Community 2017 Version 15.9.3, btw\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am following the \"Step by step Windows Build\" from https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake. However, at step 3, I can't seem to pass `Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED`, which resulting failed building process. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nPS C:\\Users\\bw\\tensorflow\\tensorflow\\contrib\\cmake\\build> cmake .. -A x64 -Thost=x64 -DCMAKE_BUILD_TYPE=Release `\r\n>> -DSWIG_EXECUTABLE='C:\\Program Files\\swigwin-3.0.12\\swig.exe' `\r\n>> -DPYTHON_EXECUTABLE='C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\python.exe' `\r\n>> -DPYTHON_LIBRARIES='C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\libs\\python36.lib'\r\n-- Building for: Visual Studio 15 2017\r\nCMake Warning at CMakeLists.txt:9 (message):\r\n  Your current cmake generator is set to use 32 bit toolset architecture.\r\n  This may cause \"compiler out of heap space\" errors when building.  Consider\r\n  using the flag -Thost=x64 when running cmake.\r\n\r\n\r\n-- The C compiler identification is MSVC 19.16.27024.1\r\n-- The CXX compiler identification is MSVC 19.16.27024.1\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test MSVC_OPENMP_SUPPORT\r\n-- Performing Test MSVC_OPENMP_SUPPORT - Success\r\n-- Found PythonInterp: C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/python.exe (found version \"3.6.7\")\r\n-- Found PythonLibs: optimized;C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/libs/python36.lib;debug;C:/Program Files (x86)/Microsoft Visual Studio/Shared/Python36_64/libs/python36_d.lib (found version \"3.6.7\")\r\n-- Found SWIG: C:/Program Files/swigwin-3.0.12/swig.exe (found version \"3.0.12\")\r\nCMake Error at tf_python.cmake:811 (string):\r\n  string sub-command REPLACE requires at least four arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:583 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:812 (string):\r\n  string sub-command REPLACE requires at least four arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:583 (include)\r\n\r\n\r\nCMake Error at tf_python.cmake:813 (string):\r\n  string sub-command REPLACE requires at least four arguments.\r\nCall Stack (most recent call first):\r\n  CMakeLists.txt:583 (include)\r\n\r\n\r\n-- Configuring incomplete, errors occurred!\r\nSee also \"C:/Users/bw/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\r\nSee also \"C:/Users/bw/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".\r\n```\r\n\r\n**Any other info / logs**\r\nCMakeOutput.log: https://www.dropbox.com/s/7fweyunxdbmxa1k/CMakeOutput.log?dl=0\r\nCMakeError.log: https://www.dropbox.com/s/tucx0tl6346kdpd/CMakeError.log?dl=0\r\n\r\n", "comments": ["Can you please switch to following known good configurations and build again:\r\n[Microsoft  Visual Studio 2015](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2015)\r\nPython 3.5", "Thanks for your reply @ymodak ! I have indeed tried building it with vs 2015 and Python 3.5, and got same error:\r\n\r\n>  cmake .. -G 'Visual Studio 14 2015 Win64' -Thost=x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE='C:\\Program Files\\swigwin-3.\r\n> 0.12\\swig.exe'  -DPYTHON_EXECUTABLE='C:\\Users\\user_1\\AppData\\Local\\Programs\\Python\\Python35\\python.exe'  -DPYTHON_LIBRARIES='C:\\Users\\user_1\\AppData\\Local\\Programs\\Python\\Python35\\libs\\py\r\n> thon35.lib'\r\n> CMake Warning at CMakeLists.txt:9 (message):\r\n>   Your current cmake generator is set to use 32 bit toolset architecture.\r\n>   This may cause \"compiler out of heap space\" errors when building.  Consider\r\n>   using the flag -Thost=x64 when running cmake.\r\n> \r\n> \r\n> -- Selecting Windows SDK version  to target Windows 10.0.17134.\r\n> -- The C compiler identification is MSVC 19.0.24234.1\r\n> -- The CXX compiler identification is MSVC 19.0.24234.1\r\n> -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe\r\n> -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe -- works\r\n> -- Detecting C compiler ABI info\r\n> -- Detecting C compiler ABI info - done\r\n> -- Detecting C compile features\r\n> -- Detecting C compile features - done\r\n> -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe\r\n> -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe -- works\r\n> -- Detecting CXX compiler ABI info\r\n> -- Detecting CXX compiler ABI info - done\r\n> -- Detecting CXX compile features\r\n> -- Detecting CXX compile features - done\r\n> -- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n> -- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n> -- Performing Test MSVC_OPENMP_SUPPORT\r\n> -- Performing Test MSVC_OPENMP_SUPPORT - Success\r\n> -- Found PythonInterp: C:/Users/user_1/AppData/Local/Programs/Python/Python35/python.exe (found version \"3.5.4\")\r\n> -- Found PythonLibs: C:/Users/user_1/AppData/Local/Programs/Python/Python35/libs/python35.lib (found version \"3.5.4\")\r\n> -- Found SWIG: C:/Program Files/swigwin-3.0.12/swig.exe (found version \"3.0.12\")\r\n> CMake Error at tf_python.cmake:811 (string):\r\n>   string sub-command REPLACE requires at least four arguments.\r\n> Call Stack (most recent call first):\r\n>   CMakeLists.txt:583 (include)\r\n> \r\n> \r\n> CMake Error at tf_python.cmake:812 (string):\r\n>   string sub-command REPLACE requires at least four arguments.\r\n> Call Stack (most recent call first):\r\n>   CMakeLists.txt:583 (include)\r\n> \r\n> \r\n> CMake Error at tf_python.cmake:813 (string):\r\n>   string sub-command REPLACE requires at least four arguments.\r\n> Call Stack (most recent call first):\r\n>   CMakeLists.txt:583 (include)\r\n> \r\n> \r\n> -- Configuring incomplete, errors occurred!\r\n> See also \"C:/Users/user_1/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeOutput.log\".\r\n> See also \"C:/Users/user_1/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/CMakeError.log\".", "(The CMake build is no longer supported by the core TensorFlow team, so I'm marking this issue as \"community support\".)", "What do I have to edit in the CMakeLists.txt file to make it work? Does anyone have any suggestions? Thanks.", "@bwang482 We are checking to see if you still need help on this issue? We recommend that you upgrade to 2.7 which is latest stable version of TF . Also please have a look on the [link](https://stackoverflow.com/questions/53585905/cmake-build-tensorflow-c-on-windows-10-error-test-compiler-opt-arch-native-s) and let us know if it helps? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24076\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24076\">No</a>\n"]}, {"number": 24075, "title": "steps_per_epoch not honored in tf.keras.fit", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: libcudnn7_7.4.1.5-1+cuda9.0\r\n- GPU model and memory:\r\nTesla P100-PCIE-12GB\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nmnist_model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(filters=35,kernel_size=(3,3), strides=(1,1), padding='same', \r\n                           activation='relu', input_shape = (1, 28, 28), data_format=\"channels_first\",\r\n                           use_bias=True, bias_initializer=tf.keras.initializers.constant(0.01), \r\n                           kernel_initializer='glorot_normal'),\r\n#     tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same', data_format='channels_first'),\r\n    tf.keras.layers.Conv2D(filters=36,kernel_size=(3,3), strides=(1,1), padding='same', \r\n                           activation='relu', data_format=\"channels_first\", use_bias=True,\r\n                           bias_initializer=tf.keras.initializers.constant(0.01), kernel_initializer='glorot_normal'),\r\n#     tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same', data_format='channels_first'),\r\n    tf.keras.layers.Conv2D(filters=36,kernel_size=(3,3), strides=(1,1), padding='same',\r\n                           activation='relu', data_format=\"channels_first\", use_bias=True,\r\n                           bias_initializer=tf.keras.initializers.constant(0.01), kernel_initializer='glorot_normal'),\r\n#     tf.keras.layers.BatchNormalization(),\r\n    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding='same', data_format='channels_first'),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(576, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='relu')\r\n]) \r\n```\r\n\r\nthis model, when fitted with\r\n```\r\n(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\r\ndataset = tf.data.Dataset.from_tensor_slices(\r\n  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float16),\r\n   tf.cast(mnist_labels,tf.int8)))\r\ndataset = dataset.shuffle(1000)\r\nmnist_images = tf.convert_to_tensor(np.expand_dims(mnist_images, axis = 1))\r\nmnist_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"categorical_crossentropy\", metrics=['accuracy'])\r\nmnist_model.fit(mnist_images, tf.one_hot(mnist_labels, depth=10), epochs=2, steps_per_epoch=100)\r\n```\r\n\r\ngenerates error:\r\n```\r\nResourceExhaustedError: OOM when allocating tensor with shape[60000,35,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node conv2d_19/Conv2D}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training_6/Adam/gradients/conv2d_19/Conv2D_grad/Conv2DBackpropFilter\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_identity_conv2d_19_input_0, conv2d_19/Conv2D/ReadVariableOp)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[{{node ConstantFoldingCtrl/loss_6/dense_13_loss/broadcast_weights/assert_broadcastable/AssertGuard/Switch_0/_912}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_324_C...d/Switch_0\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n```\r\n\r\n**Describe the expected behavior**\r\nBatch size is 600000 / 100 = 6000, i.e., Keras should not be allocating tensors with shape [60000,35,28,28]. The steps_per_epoch param is not honored.\r\n\r\n**Code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs**\r\nNA", "comments": ["This issue is more suitable on TensorFlow models repo. Please post it on TF models repo from [here](https://github.com/tensorflow/models/issues/new). Thanks!", "> This issue is more suitable on TensorFlow models repo. Please post it on TF models repo from [here](https://github.com/tensorflow/models/issues/new). Thanks!\r\n\r\nHi, ymodak, I'm not sure why this is more appropriate on the models repo. I'm only using existing high-level Keras features here, and the model is in no way experimental."]}, {"number": 24074, "title": "\"Keyword argument not understood\" - Raspberry Pi", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 3B+ (latest dist)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 1.11.0 (latest from pip)\r\n- Keras version: 2.2.4 (latest from pip)\r\n\r\nHello,\r\nI am trying to accelerate a CNN model in order to run it in python and on my raspberry pi. I've tried using tensorflow lite as a solution to this but I still have no success in converting my keras model to a lite one and use it afterwards. I've tried tflite_convert on the command line but I get the error quoted below. I've also tried using the TocoConverter inside python but I get the same error. Could you help me?\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nraise TypeError('Keyword argument not understood:' , kwarg)\r\nTypeError: ('Keyword argument not understood:' , u'output_padding')\r\n```\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 24073, "title": "TPU runs as slow as CPU when using keras_to_tpu_model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): [run in colab](https://colab.research.google.com)\r\n- TensorFlow installed from (source or binary):[Pre-installed in colab](https://colab.research.google.com)\r\n- TensorFlow version (use command below):[1.12.0 Pre-installed in colab](https://colab.research.google.com)\r\n- Python version:3.6.7 (default, Oct 22 2018, 11:32:17) [GCC 8.2.0]\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: TPU\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n`v1.12.0-0-ga6d8ffae09 1.12.0`\r\n**Describe the current behavior**\r\nI use tf.contrib.tpu.keras_to_tpu_model to make my code be able to run on TPU,but it took 170 hours to finish an epoch while CPU took the same time and GPU took only 40 hours per epoch.I tried to adjust batch size but nothing changed.And I've tested the input function may take up 20% of the run time when running on GPU, so I think it's maybe not the main reason.\r\n**Describe the expected behavior**\r\nRun faster than GPU\r\n\r\n**Code to reproduce the issue**\r\nHere is my code:[https://github.com/WangHexie/DHNE/blob/master/src/hypergraph_embedding.py](https://github.com/WangHexie/DHNE/blob/master/src/hypergraph_embedding.py)\r\nRun on colab:         \r\n 1. TPU:[https://colab.research.google.com/gist/WangHexie/30c385509f9cd93be747f04c39f039a4/tpu-error.ipynb](https://colab.research.google.com/gist/WangHexie/30c385509f9cd93be747f04c39f039a4/tpu-error.ipynb)       \r\n 2. GPU\uff1a[https://colab.research.google.com/gist/WangHexie/5bfac53bf92ef0ad527f15ddbf8705e1/-gpu-ipynb.ipynb](https://colab.research.google.com/gist/WangHexie/5bfac53bf92ef0ad527f15ddbf8705e1/-gpu-ipynb.ipynb)\r\n\r\n", "comments": ["Same problem here... and strangely, sometimes the TPU model works, but sometime it runs as slow as a CPU model.", "@WangHexie This is an old issue. Is this still an issue for you. We are not supporting Python2.x and TF1.x any more. I tried running your code with recent `TF1.15.5` and faced different error as shown below. [Here](https://colab.research.google.com/gist/jvishnuvardhan/a57dd3917c7c4d53e43b42430bd31bbb/tpu-error.ipynb) is a gist for reference.\r\n\r\n`AttributeError: 'Model' object has no attribute 'target_tensors'`\r\n\r\nIf this is still an issue, test it with recent `TF2.x`. If you notice any issues with recent TF versions, please provide a standalone code to reproduce the issue. Thanks!\r\n\r\nPlease close the issue if this was already resolved for you. Thanks!"]}, {"number": 24072, "title": "Bad broadcasting when multiplying sparse and dense tensors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0\r\n- Python version: 3.6.7 Anaconda 64bit\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nWhen multiplying (`*` operator) a sparse tensor `S` with rank `N` with a dense tensor `D` with rank `M`, where `N > M` and `S` and `D` have different but broadcast-compatible shapes, then TensorFlow attempts to perform the multiplication producing and incorrect sparse tensor as a result with the shape of `S`.\r\n\r\n**Describe the expected behavior**\r\n\r\nTensorFlow should refuse to perform a sparse-dense multiplication that requires broadcasting. This is the case when both tensors have the same rank or the rank of the sparse tensor is less than the rank of the dense tensor, but not when the rank of the sparse tensor is greater.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```py\r\nimport tensorflow as tf\r\n\r\nwith tf.Graph().as_default(), tf.Session() as sess:\r\n    a = tf.reshape(tf.range(12), [3, 4, 1])\r\n    b = tf.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20], [1, 1, 4, 2])\r\n    c = a * b\r\n    print(c.shape)\r\n    # (1, 1, 4, 2)\r\n    print(sess.run(tf.sparse.to_dense(c)))\r\n    # [[[[ 0  0]\r\n    #    [10  0]\r\n    #    [ 0  0]\r\n    #    [60  0]]]]\r\n```\r\n\r\n**Other info / logs**\r\nNA", "comments": ["@javidcf I added a PR #24098 for the fix. Please take a look.", "@yongtang Thanks, that seems to do a better job at validating the input, although to be honest I'm not familiar enough with the source to know if any other changes are needed. Only one thing, where the comment says `all dims in lhs is smaller or equal`, I think it should be `all dims in lhs is greater or equal`? Or maybe I'm misunderstanding something...\r\n\r\nBesides that, I just noticed that these errors are only raised on evaluation. I suppose it would be convenient to raise an error on graph construction when detectable too... but that is a different issue really, for binary sparse ops in general - and also, if sparse broadcasting is planned to be supported at some point in the future, maybe it's not worth the effort.", "@javidcf Thanks. You are right and I have updated the PR for the comment. To raise the error on graph construction might requires more and sometimes the dims are not known at the construction time. I will take a look and create a separate PR if possible."]}, {"number": 24071, "title": " Add drop_remainder argument to bucket_by_sequence_length", "body": "`tf.data.experimental.bucket_by_sequence_length` does not allow to drop\r\nthe last batch in case it has fewer than `batch_size` elements.\r\nThis patch does implement `drop_remainder` for `bucket_by_sequence_length`\r\nto enable thhis behaviour.\r\n\r\n`drop_remainder` is optinal and set to `False` by default to maintain\r\ncompatibility.\r\n\r\nI will add a PR to update the documentation of bucket_by_sequence_length once this is merged.", "comments": ["@mrry , I added `drop_remainder` tests to the existing test cases.\r\nAre these sufficient?\r\n\r\nI run the tests using the nightly-docker file (CPU only).", "Thanks for the submission... this looks good to me!", "@mrry  The `bucket_by_sequence_length` test passes with the latest commit 4ddd2ab.\r\n\r\nThe only other test that is failing is the `api_compatibility_test`.\r\nHowever, I am not sure how these work. Am I responsible for taking action when they fail?", "@yweweler I think you can update the necessary API files by running the following two commands:\r\n\r\n```\r\nbazel build //tensorflow/tools/api/tests:api_compatibility_test\r\nbazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens=True\r\n```", "> @yweweler I think you can update the necessary API files by running the following two commands:\r\n> \r\n> ```\r\n> bazel build //tensorflow/tools/api/tests:api_compatibility_test\r\n> bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens=True\r\n> ```\r\n\r\n@mrry I have to admit that I am a bit stuck.\r\nI am using the two lines from [tensorflow/tools/api/tests/README.txt](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/README.txt) to build and update the goldens.\r\n```\r\n$ bazel build //tensorflow/tools/api/tests:api_compatibility_test\r\n$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\n```\r\nThe build part works flawlessly.\r\nI assured that all dependencies are installed properly and that building tensorflow as a whole also works.\r\nHowever, updating the goldens using `api_compatibility_test` crashes.\r\n\r\n```\r\n(tensorflow_27_venv) user:~/downloads/tensorflow$ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\nTraceback (most recent call last):\r\n  File \"/home/user/downloads/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/tools/api/tests/api_compatibility_test.py\", line 36, in <module>\r\n    from tensorflow._api.v2 import v2 as tf_v2\r\n  File \"/home/user/downloads/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/_api/v2/v2.py\", line 325, in <module>\r\n    if _tf_api_dir not in __path__:\r\nNameError: name '__path__' is not defined\r\n```\r\n\r\nThere are some issues in the repo suggesting that the procedure can only be executed using python2.7.\r\nHowever, I tried doing the whole procedure both using python2.7 and python3.5 and ran into the same error every time.\r\n\r\nI also tried the same thing on older commits from the `master` branch. Sadly with the same outcome.", "> NameError: name '\\_\\_path__' is not defined\r\n\r\nI already fiddled around with the \"\\_\\_path__\"  variable by manually initializing it inside `/home/user/downloads/tensorflow/bazel-bin/tensorflow/tools/api/tests/api_compatibility_test.runfiles/org_tensorflow/tensorflow/_api/v2/__init__.py`\r\n\r\nHowever, this just leads to more errors, like:\r\n```\r\nImportError: cannot import name v2\r\n```\r\n", "@annarev Is thie `api_compatibility_test` currently expected to work in open source?", "> @annarev Is thie `api_compatibility_test` currently expected to work in open source?\r\n\r\nI would like to prevent this to fall into decline.\r\n\r\nGuessing from the `api_compatibility_test` code I think that the changes to the goldens are minimal.\r\nTherefore I could patch the goldens manually and see if the test passes. I don't really like working around this but currently I see no other option.\r\n\r\nEither way it would be great to have some clarification on the state of `api_compatibility_test`.", "@mrry I patched the goldens for both v1 and v2 (Not sure if this is desired).\r\n\r\nI also retried executing the test with different docker images (`devel-py3` and `nightly-devel`), sadly with the  same outcome as described above.\r\n\r\nAs I can't test the changes using `api_compatibility_test`, it would be great if we could run the through kokoro.", "@jsimsa -  Could you PTAL at this ?", "@jsimsa I made the changes you requested.", "@shivaniag could you please review this PR, the newly introduced test still uses session and I believe it should be switched to the new style which you introduced", "> @shivaniag could you please review this PR, the newly introduced test still uses session and I believe it should be switched to the new style which you introduced\r\n\r\nThats a good point. I did not notice that @shivaniag updated the tests in the meantime.\r\n\r\nI was going through his commit 4caa17185fb39420c87373af77b538e9016be46d trying to understand the changes he made to the other tests.\r\nBefore updating the PR to match his changes I need to know why the following change is needed.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/4caa17185fb39420c87373af77b538e9016be46d/tensorflow/python/data/experimental/kernel_tests/bucket_by_sequence_length_test.py#L128-L130\r\n\r\n@shivaniag Why do we need to disable the sum_check when executing eagerly?", "@yweweler: No, we don't need to disable sum_check when executing eagerly, there is a need to add eager coverage for this test which has been left as a TODO in the last commit.", "@shivaniag  This should implement the requested changes.", "Not sure why I can't see the test results from the kokoro run anymore.\r\nHowever, apart from me having forgot to run pylint on the last commit, the other errors were not related to this PR (as far as I can tell.)"]}]