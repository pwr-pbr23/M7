[{"number": 50202, "title": "How to \"tf.data.experimental.AutoShardPolicy.OFF\" if tf.data is not used? But gets this message while using tf.keras.model.fit.", "body": "**System information**\r\n- TensorFlow version (you are using):2.5\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nHow to \"tf.data.experimental.AutoShardPolicy.OFF\" if tf.data is not used? But gets this message while using tf.keras.model.fit.\r\n\r\n", "comments": ["@rrklearn2020 ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "Its difficult to use TF.data.Dataset for relational database with Camera, PointCloud, Map and other data/information.\r\nOn single system with multiple GPU's, its better to use distributed training with MirroredStrategy (Tensorflow).\r\nIt would be great if the 'AutoShardpolicy' can be set to OFF in such use cases, either in the model.fit (TF.KERAS) statement or any other ways.\r\n\r\nPlease inform the effect of having 'AutoShardpolicy' set to OFF on single machine application, since 'Sharding is a method for distributing data across multiple machines'.", "Keras may want to make autosharding configurable for non-dataset inputs. Assigning to @fchollet for triage", "@fchollet and @aaudiber , It would be a great help, if you could help to address this concern of 'AutoShardPolicy policy' .\r\n\r\nBy having the 'AutoShardPolicy policy' training takes long time in searching for tf.data.Dataset, for the training where tf.data.Dataset is not used. This consumes lots of time during each epoch.\r\n\r\nBy setting 'AutoShardPolicy policy' as OFF,  reduces the time taken for training, with single machine with multiple GPU's."]}, {"number": 50198, "title": "Massive memory allocations and reduced performance in GradientTape for loop and Optimizer when using jit_compile", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 34 Workstation Edition\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.5.0-rc2-14-gfcdf6593470 2.5.0-rc3\r\n- Python version: 3.9.5\r\n- GPU model and memory: Running on CPU\r\n(If it is important the CPU is a 3900X and has 32GB of RAM)\r\n\r\nI recently ran into issues with a custom training loop and `jit_compile` so i ran some small tests and found massive performance issues when enabling `jit_compile`. \r\n\r\nThe code is a simple for loop inside a `tf.GradientTape` context (see below) where i accumulate a loss value. (Yes in this case i could batch the calls but that sadly isn't possible in the original issue i was talking about earlier)\r\n\r\nIn my test i ran the exact same training code once with `jit_compile` enabled and once with just using `tf.function`. I found that when enabling `jit_compile` the code runs approximately 1.7 times slower while consuming 12GB of memory as opposed to 1.4GB without `jit_compile`. I do not know if the slowdown is a direct result of the memory usage or due to other issues.\r\nIf i remove the `tf.GradientTape` the memory issue vanishes however the speed issue still persists. I also tried specifically setting the watched variables with no change.\r\nIf i leave the gradient tape in and just remove the call to the optimizer the memory usage drops to 8.8GB however performance is now over 25 times worse (16.5s instead of 0.6s). Which seems to suggest that both the optimizer and the GradientTape have problems.\r\nLastly i also tried setting a constant value directly in the range inside the for loop which also did not change the memory or performance issues.\r\n\r\nI would expect the jit compiled code to at least have the same memory footprint as the not compiled version.\r\n\r\n**The code:**\r\n```\r\nimport tensorflow as tf\r\nimport timeit\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.InputLayer((2, )),\r\n    tf.keras.layers.Dense(2048, activation='elu'),\r\n    tf.keras.layers.Dense(2048, activation='elu'),\r\n    tf.keras.layers.Dense(1)\r\n])\r\n\r\nopt = tf.keras.optimizers.Adam()\r\n\r\n\r\n@tf.function\r\ndef normal_func(x, y, k):\r\n    print(\"Tracing normal\")\r\n    with tf.GradientTape() as tape:\r\n        loss = tf.constant((0, ), dtype=tf.float32)\r\n        for i in tf.range(k):\r\n            geny = model(tf.expand_dims(x[i], axis=0), training=True)[0]\r\n            loss = loss + tf.square(y[i] - geny)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    opt.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss\r\n\r\n\r\n@tf.function(jit_compile=True)\r\ndef jit_func(x, y, k):\r\n    print(\"Tracing jit\")\r\n    with tf.GradientTape() as tape:\r\n        loss = tf.constant((0, ), dtype=tf.float32)\r\n        for i in tf.range(k):\r\n            geny = model(tf.expand_dims(x[i], axis=0), training=True)[0]\r\n            loss = loss + tf.square(y[i] - geny)\r\n\r\n    grads = tape.gradient(loss, model.trainable_variables)\r\n    opt.apply_gradients(zip(grads, model.trainable_variables))\r\n    return loss\r\n\r\n\r\nx_ = tf.reshape(tf.range(0, 1024, dtype=tf.float32), shape=(512, 2))\r\ny_ = tf.reshape(tf.range(0, 512, dtype=tf.float32), shape=(512, ))\r\n\r\nk_ = tf.constant(8)\r\nk2_ = tf.constant(256)\r\nk3_ = tf.constant(512)\r\n\r\n# Sanity tests for retracing\r\nnormal_func(x_, y_, k_)\r\nnormal_func(x_, y_, k2_)\r\nnormal_func(x_, y_, k3_)\r\n\r\njit_func(x_, y_, k_)\r\njit_func(x_, y_, k2_)\r\njit_func(x_, y_, k3_)\r\n\r\n# Actual performance tests\r\nprint('Runtime without jit_compile:')\r\nprint(timeit.timeit(lambda: normal_func(x_, y_, k_), number=10))\r\nprint(timeit.timeit(lambda: normal_func(x_, y_, k2_), number=10))\r\nprint(timeit.timeit(lambda: normal_func(x_, y_, k3_), number=10))\r\n\r\nprint('\\nRuntime with jit_compile:')\r\nprint(timeit.timeit(lambda: jit_func(x_, y_, k_), number=10))\r\nprint(timeit.timeit(lambda: jit_func(x_, y_, k2_), number=10))\r\nprint(timeit.timeit(lambda: jit_func(x_, y_, k3_), number=10))\r\n```\r\n\r\nThe output when running on my machine:\r\n[output.log](https://github.com/tensorflow/tensorflow/files/6631656/output.log)", "comments": ["@rmothukuru ,\r\nI was able to reproduce the issue in v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/88c56e984d474e69a27518fd1593ee60/50198.ipynb).", "@CodingRays,\r\nI suspect the line `loss = loss + tf.square(y[i] - geny)` is the culprit. \r\n\r\nCan you please explain why do you want to add the **`Loss`** that way as it is not mentioned to do so in the [documentation of Custom Training Loop](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#using_the_gradienttape_a_first_end-to-end_example)?\r\n ", "Im not sure what you mean with not mentioned in the documentation. I dont see this situation mentioned in the docs. I just need some way to accumulate the losses over the iterations of the for loop while still being inside the gradient tape. In the docs the gradient tape is always inside the for loop.\r\nIf it helps for context the original problem i had was with updating a lstm based ddpg model and there was some processing i had to do in between the timesteps which is why i wasnt able to batch it.", "Note that on GPU performance numbers are as expected:\r\n\r\n```\r\nRuntime without jit_compile:\r\n0.053597197867929935\r\n1.1324485160876065\r\n2.3468134391587228\r\n\r\nRuntime with jit_compile:\r\n0.039613737957552075\r\n0.7868384378962219\r\n1.581948912004009\r\n```"]}, {"number": 50189, "title": "Does tf.math.equal supports tf.sparse.SparseTensor?", "body": "## URL with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/math/equal\r\n\r\n## Description of issue (what needs changing):\r\nThe doc says `tf.math.equal` supports sparse tensor as inputs, but it is actually not supported at the moment. See the example below. I haven't checked all math operations that claims to support sparse tensors, maybe there are other similar doc errors like this.\r\n\r\nI'm using tf `2.6.0-dev20210601`.\r\n\r\n### Usage example\r\n```python\r\nimport tensorflow as tf\r\n\r\na = tf.sparse.SparseTensor(\r\n    indices=[[0, 0], [0, 1], [1, 2]],\r\n    values=[1, 1, 1],\r\n    dense_shape=[2, 3]\r\n)\r\n\r\nb = tf.sparse.SparseTensor(\r\n    indices=[[0, 0], [0, 1], [1, 2]],\r\n    values=[1, 1, 1],\r\n    dense_shape=[2, 3]\r\n)\r\n\r\ntf.math.equal(a, b)  # raises ValueError\r\n```\r\n\r\nI think making `equal` operation supports sparse tensor is needed.\r\n\r\n", "comments": ["@Saduf2019 \r\nWas able to reproduce the issue with TF v2.4,v2.5 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bc22e4051e41ba09b68ec8c9352e4a51/50189.ipynb). Thanks!", "Hey I'm new to contributing to open source, can I work on this issue? ", "Thanks for working on the issue. I'm using this temporary hack as a workaround atm.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.sparse import SparseTensor\r\n\r\n\r\ndef sparse_tensor_not_equal(a: SparseTensor, b: SparseTensor) -> SparseTensor:\r\n    diff = tf.math.abs(tf.sparse.add(tf.math.negative(a), b))\r\n    diff = tf.cast(diff, bool)\r\n\r\n    return diff\r\n\r\ndef sparse_tensor_equal(a: SparseTensor, b: SparseTensor) -> SparseTensor:\r\n    diff = sparse_tensor_not_equal(a, b)\r\n    is_equal = tf.sparse.map_values(tf.math.logical_not, diff)\r\n\r\n    return is_equal\r\n\r\n\r\na = tf.sparse.SparseTensor(indices=[[0, 0], [0, 1], [1, 2]], values=[1, 1, 1], dense_shape=[2, 3])\r\nb = tf.sparse.SparseTensor(indices=[[0, 1], [0, 2], [1, 1], [1, 2]], values=[1, 1, 1, 1], dense_shape=[2, 3])\r\nprint(sparse_tensor_equal(a, b))\r\n```", "Looking at the code, it looks to me like `tf.math.equal` and `tf.math.not_equal` do not, and have never, supported sparse tensors. (And they only support IndexedSlices in so far as IndexedSlices can be converted to tensors, so it seems odd to call out support for IndexedSlices.)  It looks like this erroneous documentation was added in e0e1efbe0811aa0913ad8400c532b33c76425427.\r\n\r\nIt would certainly be possible to extend `tf.math.equal` to support sparse tensors.  If this is done, we should be sure to document the semantics -- namely that `tf.equal(x, y)` iff `tf.equal(tf.sparse.to_dense(x), tf.sparse.to_dense(y))` (so two sparse tensors are considered equal even if one has an explict nonzero where another has an implicit nonzero).  Also, we should document that `x` and `y` are assumed to be in standard lexicographic order (and tf.sparse.reorder needs to be used if they are not).\r\n\r\nAs for other ops in `math_ops.py` that claim to support `SparseTensors`, I didn't see any similar documentation errors.  In particular, the following claim to work and do work: `tf.math.abs`, `tf.math.negative`, `tf.math.sign`, `tf.cast`.  Also, `my_dense * my_sparse` works (using the multiplication operator), though I don't see that fact documented anywhere.  (In contrast, `tf.math.multiply(my_dense, my_sparse)` does not work).", "@HongtaoYang \r\nIs this still an issue ?\r\nCould you please refer to the [comment](https://github.com/tensorflow/tensorflow/issues/50189#issuecomment-862413890) above and let us know if it helps?\r\nThanks!"]}, {"number": 50188, "title": "How to use TF.data.Dataset for relational database like nuScenes", "body": "**System information**\r\n- TensorFlow version (you are using): tensorflow==2.5.0 (pip installed, Ubuntu 20.04, CUDA 11.)\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.data.Dataset is excellent while implementing the data-pipeline, with map, shuffle, batch and prefetch (inducing AUTOTUNE features) . But currently the relational database like nuScenes (https://www.nuscenes.org/nuscenes#data-format). has difficulty to implement with multiple sensor data like camera images and pointCloud's. Please guide or inform the steps to be taken, with examples.\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nAll relational database like nuScenes can use the tf.data.Dataset and also perform better while using the distributed training. \r\n", "comments": []}, {"number": 50178, "title": "What could explain that MultiWorkerMirroredStrategy is very slow ?", "body": "We are trying to use a multi-worker strategy with Keras and tensorflow 2.5. We have 2 hosts with 8 A100 GPU on each host.\r\n\r\nOur code looks like the following\r\n\r\n```\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n\r\nwith strategy.scope():\r\n    model = build_model()\r\n    model.compile(...)\r\n\r\ntrain_data = build_train_data()\r\ntrain_data = train_data.take(1).cache().repeat()\r\n\r\nmodel.fit(train_data)\r\n```\r\n\r\nAs you notice in this pseudo code, we cache one batch of our dataset to reduce the I/O bound and to measure precisely the training time.\r\n\r\nThe code where run on two nodes is very slow. All GPUs are almost idle every time.\r\n\r\nWhen we run the same code on a single node (8 GPUs) with MirroredStrategy we get a very high speed.\r\n\r\nSo, what can be the bottleneck here ?", "comments": ["Another issue related to this one is that when we force the strategy to use NCCL with\r\n```\r\ncommunication_options = tf.distribute.experimental.CommunicationOptions(\r\n        implementation=tf.distribute.experimental.CommunicationImplementation.NCCL\r\n)\r\n\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(\r\n    communication_options=communication_options\r\n)\r\n```\r\n\r\nThe code fails with segmentation fault.\r\n\r\nWe don't know whether it is related to Tensorflow or NCCL. ", "Inter-node GPU communication is usually much slower than intra-node, so the bottleneck might be on network bandwidth, especially if your model's computation is very cheap.\r\n\r\nPerhaps you can set environment NCCL_DEBUG=INFO to verify if NCCL is used (in theory strategy should automatically choose NCCL if there are GPUs). It would also help debug the seg fault when you force the strategy to use NCCL.\r\n\r\n"]}, {"number": 50172, "title": "batch normalization folding issue in Conv2d with dilation>1", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow version (use command below):\r\n- Python version: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n\r\n**Describe the current behavior**\r\nI'm converting a simple Conv2d->BatchNorm->ReLU network to TFLite. In previous versions, the BatchNorm would be folded into the Conv2D weights to avoid the extra quantization. In TF 2.5.0, Conv2D with dilation>1 is applied with space2batch - batch2space nodes, which seems to confuse the converter.\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour should be to fold the BatchNorm weights into the Conv2D weights (The same way as in TF 2.4.1)\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef representative_dataset():\r\n    for _ in range(10):\r\n        data = np.random.rand(1, 244, 244, 3)\r\n        yield [data.astype(np.float32)]\r\n\r\n\r\ndil = 2\r\nfor ptq in [False, True]:\r\n    _in = tf.keras.Input((244, 244, 3))\r\n    if ptq:\r\n        x = _in\r\n    else:\r\n        x = tf.quantization.fake_quant_with_min_max_args(_in)\r\n\r\n    x = tf.keras.layers.Conv2D(16, 3, padding='same', dilation_rate=(dil, dil), use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n    x = tf.keras.layers.ReLU()(x)\r\n    if not ptq:\r\n        x = tf.quantization.fake_quant_with_min_max_args(x)\r\n    model = tf.keras.Model(_in, x)\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    if ptq:\r\n        converter.representative_dataset = representative_dataset\r\n    tflite_quant_model = converter.convert()\r\n\r\n    s = '_ptq' if ptq else ''\r\n    tflite_file = f'/tmp/dilated_conv2d/dil{dil}{s}{c}.tflite'\r\n    with open(tflite_file, 'wb') as f:\r\n        f.write(tflite_quant_model)\r\n\r\n    print(f'Generated {tflite_file} with TF {tf.__version__}')\r\n\r\n```\r\nTF 2.5.0: Quantizting with fake_quant nodes\r\n![dil2](https://user-images.githubusercontent.com/78862769/121343752-b1e0ab80-c92b-11eb-88a7-5a53e5583b89.png)\r\n\r\nTF 2.5.0: Quantizting without fake_quant nodes\r\n![dil2_ptq](https://user-images.githubusercontent.com/78862769/121343757-b311d880-c92b-11eb-94e6-408be513cc00.png)\r\n\r\nTF 2.4.1: Quantizting without fake_quant nodes\r\n![dil2_ptq_TF241](https://user-images.githubusercontent.com/78862769/121343755-b2794200-c92b-11eb-83c9-b86e0e506d8b.png)\r\n\r\n(Images generated with Netron: https://netron.app/)\r\n", "comments": ["cc: @abattery @thaink @rino20 @ethkim\r\nCould any of you triage this further? I'm not sure whether this is a general conversion issue or model optimization specific issue.", "Looks like this is a newly introduced behavior by the MLIR quantizer in TF 2.5. @teijeong @liufengdb could you take a look at this?", "Thanks for spotting this. Meanwhile, you can specify `converter.experimental_new_quantizer = False` to fall back to old quantizer.", "Seems like regression in the converter. Even a float model generated from TF 2.5 has SpaceToBatchNd ops (and so). @renjie-liu, can you take a look?\r\n\r\ncolab to reproduce: https://colab.research.google.com/drive/1c-TLMmQ_ylcEbB1Q8fIveweG__TzMDyb#scrollTo=swVmTdSxuWkN\r\n\r\n![image](https://user-images.githubusercontent.com/4837376/122018099-82371500-cdfd-11eb-8ef2-0e444243ac99.png)\r\n", "Hi,\r\n\r\nSorry for late reply. I've verified this is causes by the dynamic batch size dimension in your model. Basically the dilated conv will fail to match at this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/7d94fc2b30f66cf2ee269b6cf42bdeef8f40a288/tensorflow/compiler/mlir/lite/transforms/dilated_conv.h#L109\r\n\r\nThe keras input layer will create an input with dynamic batch size, could you specify a fix batch size when building the input layer? Something like:\r\n  inp = tf.keras.layers.Input(batch_size=1, shape=(224, 224, 3))\r\n\r\nI've verified the dilated conv will be fused once changing to a fixed batch size."]}, {"number": 50170, "title": "tf.Variable cannot be used inside the gradient function of a tf.custom_gradient(), when tf.cond() and graph mode are used", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Red Hat Enterprise Linux release 8.2 (Ootpa)\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n`tf.Variable` cannot be used inside the gradient function of a `tf.custom_gradient()`, when `tf.cond()` and graph mode are used. Trying to do so results in a crash.\r\n\r\n**Describe the expected behavior**\r\nIt should be possible to use such objects in this specific case, and there should be not crash.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\nw = tf.Variable(2.0, trainable=False)\r\n\r\n@tf.function\r\ndef test(x):\r\n    @tf.custom_gradient\r\n    def multiply_by_w(x):\r\n        y = w * x\r\n        def grad_fn(grad_y):\r\n            return grad_y * w\r\n        return (y, grad_fn)\r\n\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(x)\r\n        y = tf.cond(tf.constant(True), lambda: multiply_by_w(x), lambda: x)\r\n\r\n    return tape.gradient(y, x)\r\n\r\ntest(tf.constant(5.0))\r\n```\r\n**Other info / logs**\r\n\r\nRunning the code above gives the following traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-a323f51f0fc4> in <module>\r\n     18     return tape.gradient(y, x)\r\n     19 \r\n---> 20 test(tf.constant(5.0))\r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    887 \r\n    888       with OptionalXlaContext(self._jit_compile):\r\n--> 889         result = self._call(*args, **kwds)\r\n    890 \r\n    891       new_tracing_count = self.experimental_get_tracing_count()\r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    931       # This is the first call of __call__, so we have to initialize.\r\n    932       initializers = []\r\n--> 933       self._initialize(args, kwds, add_initializers_to=initializers)\r\n    934     finally:\r\n    935       # At this point we know that the initialization is complete (or less\r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    762     self._concrete_stateful_fn = (\r\n    763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 764             *args, **kwds))\r\n    765 \r\n    766     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   3048       args, kwargs = None, None\r\n   3049     with self._lock:\r\n-> 3050       graph_function, _ = self._maybe_define_function(args, kwargs)\r\n   3051     return graph_function\r\n   3052 \r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3442 \r\n   3443           self._function_cache.missed.add(call_context_key)\r\n-> 3444           graph_function = self._create_graph_function(args, kwargs)\r\n   3445           self._function_cache.primary[cache_key] = graph_function\r\n   3446 \r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3287             arg_names=arg_names,\r\n   3288             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3289             capture_by_value=self._capture_by_value),\r\n   3290         self._function_attributes,\r\n   3291         function_spec=self.function_spec,\r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    997         _, original_func = tf_decorator.unwrap(python_func)\r\n    998 \r\n--> 999       func_outputs = python_func(*func_args, **func_kwargs)\r\n   1000 \r\n   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    670         # the function a weak reference to itself to avoid a reference cycle.\r\n    671         with OptionalXlaContext(compile_with_xla):\r\n--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    673         return out\r\n    674 \r\n\r\n~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    984           except Exception as e:  # pylint:disable=broad-except\r\n    985             if hasattr(e, \"ag_error_metadata\"):\r\n--> 986               raise e.ag_error_metadata.to_exception(e)\r\n    987             else:\r\n    988               raise\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-1-a323f51f0fc4>:11 grad_fn  *\r\n        return grad_y * w\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1250 binary_op_wrapper\r\n        raise e\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1234 binary_op_wrapper\r\n        return func(x, y, name=name)\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1575 _mul_dispatch\r\n        return multiply(x, y, name=name)\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\r\n        return target(*args, **kwargs)\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:530 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6250 mul\r\n        \"Mul\", x=x, y=y, name=name)\r\n    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:531 _apply_op_helper\r\n        repr(values), type(values).__name__, err))\r\n\r\n    TypeError: Expected float32 passed to parameter 'y' of op 'Mul', got <tf.Variable 'Variable:0' shape=() dtype=float32> of type 'ResourceVariable' instead. Error: Expected resource passed to parameter 'resource' of op 'ReadVariableOp', got <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>> of type 'EagerTensor' instead. Error: _capture_helper() takes 3 positional arguments but 4 were given\r\n```\r\nSome observations:\r\n- The code crashes inside `grad_fn()` in operation `grad_y * w` because `w` is of unexpected type.\r\n- No crash when not using `tf.custom_gradient`, i.e. by using `multiply_by_w = lambda x: w * x` instead.\r\n- No crash when removing `tf.cond()`, i.e., by replacing `y = tf.cond(tf.constant(True), lambda: multiply_by_w(x), lambda: x)` by `y = multiply_by_w(x)`.\r\n- No crash in eager mode (i.e. by commenting out the `tf.function` decorator).\r\n", "comments": ["@sachinprasadhs \r\n\r\nI was able to replicate the issue reported here.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/c57f6a3872b9341abd2d55e2ff3251c2/untitled83.ipynb). Thanks\r\n", "Hi,\r\n\r\nDoes someone have any idea what could be going on here?\r\n\r\nThanks,", "Hi @loic-ehrhardt, thanks for raising the issue. This is probably caused by some bug when a tf.custom_gradient is used inside a tf.cond. In addition to the observations you found, one workaround is adding two lines to grad_fn: \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nw = tf.Variable(2.0, trainable=False)\r\n\r\n@tf.function\r\ndef test(x):\r\n  @tf.custom_gradient\r\n  def multiply_by_w(x):\r\n    y = w * x\r\n    def grad_fn(grad_y):\r\n      handle = tf.compat.v1.get_default_graph().outer_graph.capture(w.handle)\r\n      read_value = tf.raw_ops.ReadVariableOp(resource=handle, dtype=tf.float32)\r\n      return grad_y * read_value\r\n    return y, grad_fn\r\n  \r\n  with tf.GradientTape() as tape:\r\n    tape.watch(x)\r\n    y = tf.cond(tf.constant(True), lambda: multiply_by_w(x), lambda: x)\r\n  return tape.gradient(y, x)\r\n\r\ntest(tf.constant(5.0))\r\n```\r\nWhile we are working to fix bug, would you try this workaround and let us know if your work is unblocked? Thank you.\r\n", "Hi @JXRiver,\r\n\r\nNice, this works indeed for me.\r\nLooking forward for the bug fix.\r\n\r\nThanks a lot.", "Awesome!"]}, {"number": 50154, "title": "MetricsContainer.update_state passes mask as sample_weight to metric_obj.update_state", "body": "**System information**\r\n- Have I written custom code YES\r\n- OS Platform and Distribution OS X 10.15.6\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.5.0\r\n- Python version 3.8\r\n\r\n\r\n**Describe the current behavior**\r\nAfter upgrading from tensorflow 2.2.1 to 2.5.0, I get an error while training an LSTM model with F1Score for the metrics (issue does not exist if using CategoricalAccuracy for the metrics). After comparing the source code for both versions I found a bug (I think).\r\nIn version 2.2.1, in tensorflow/python/keras/engine/compile_utils.py, line 411 (inside MetricsContainer.update_state), the metric_obj is updated by passing y_t and y_p, the optional argument sample_weight is not passed. In version 2.5.0, same method, line 460, something is passed for the optional argument sample_weight, but it is the mask obtained from y_p, hence of dtype tf.bool instead of tf.float32, which gives a TypeError when updating the state of F1Score, because it tries to multiply the boolean mask with tf.float32 tensors. I think tis is incorrect.\r\n\r\nFull error:\r\n```\r\nin user code:\r\n\r\n    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow_addons/metrics/f_scores.py:157 _weighted_sum  *\r\n        val = tf.math.multiply(val, tf.expand_dims(sample_weight, 1))\r\n    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:530 multiply\r\n        return gen_math_ops.mul(x, y, name)\r\n    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6249 mul\r\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper\r\n        raise TypeError(\r\n\r\n    TypeError: Input 'y' of 'Mul' Op has type bool that does not match type float32 of argument 'x'.\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\nNo TypeError when updating metrics during training.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? no\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.layers import Masking, Bidirectional, LSTM, Dropout, TimeDistributed, Dense, Lambda\r\nfrom tensorflow_addons.metrics import F1Score\r\nimport numpy as np\r\n\r\nwindow_length = 50\r\nembedding_dimension = 100\r\nnum_classes = 5\r\n\r\ntf.random.set_seed(42)\r\nembeddings = tf.keras.Input(shape=(window_length, embedding_dimension), dtype=tf.float32, name=\"embedding_sequence\")\r\nnwords = tf.keras.Input(shape=(), dtype=tf.int32, name=\"nwords\")\r\n\r\nmasked_embedding = Masking()(embeddings)\r\n\r\nbilstm = Bidirectional(\r\n    LSTM(\r\n        units=16,\r\n        return_sequences=True,\r\n        dropout=0.0,\r\n        recurrent_dropout=0.0,\r\n    )\r\n)(masked_embedding)\r\n\r\nbilstm = Dropout(rate=0.5, seed=42)(bilstm)\r\n\r\nlogits = TimeDistributed(Dense(num_classes, activation=\"softmax\"), name=\"logits\")(bilstm)\r\n\r\npred_ids = tf.argmax(logits, axis=2, output_type=tf.int32)\r\n\r\nnaming_layer = Lambda(lambda x: x, name=\"pred_ids\")\r\npred_ids = naming_layer(pred_ids)\r\n\r\nloss = {\"logits\": \"categorical_crossentropy\"}\r\n\r\nmodel = tf.keras.Model(inputs=[embeddings, nwords], outputs=[logits, pred_ids], name=\"ner_bilstm\")\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(\r\n        learning_rate= 0.001),\r\n    loss=loss,\r\n    metrics={\"logits\": [F1Score(num_classes=num_classes, average=\"micro\")]},\r\n)\r\n\r\ndummy_pred_ids = np.array([\"dummy\"] * 7)\r\n\r\n\r\n\r\ndata_x = {\"embedding_sequence\": np.random.rand(7,window_length,embedding_dimension), \"nwords\": np.array([window_length]*7)}\r\ndata_y = {\r\n    \"logits\": np.zeros((7, window_length, num_classes), dtype=int),\r\n    \"pred_ids\": dummy_pred_ids,\r\n}\r\nmodel.fit(x=data_x, y=data_y, validation_data=(data_x, data_y))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue in TF 2.5 version. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/c598195a4804e3a939de844c14fb9200/untitled.ipynb).Thanks!", "Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!"]}, {"number": 50147, "title": "systemlib com_github_grpc_grpc fails", "body": "**System information**\r\n- OS Platform and Distribution (e.g., openSUSE Tumbleweed):\r\n\r\n- TensorFlow installed from source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8.10\r\n- Bazel version: 3.7\r\n- GCC/Compiler version: 11.1.1\r\n\r\n**Describe the problem**\r\nWhen using the systemlib `com_github_grpc_grpc` the compilation fails with following error\r\n```\r\n  Repository rule _tf_http_archive defined at:\r\n  /home/abuild/rpmbuild/BUILD/tensorflow2-2.5.0/third_party/repo.bzl:65:35: in <toplevel>\r\n ERROR: /home/abuild/rpmbuild/BUILD/tensorflow2-2.5.0/tensorflow/core/data/service/BUILD:536:16: no such package '@com_github_grpc_grpc//src/compiler': BUILD file not found in directory 'src/compiler' of external repository @com_github_grpc_grpc. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/core/data/service:_worker_cc_grpc_proto_grpc_codegen'\r\n ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\n```\r\nI have checked the file `tensorflow/workspace2.bzl` where following definitions can be found:\r\n```\r\n    tf_http_archive(\r\n        name = \"com_github_grpc_grpc\",\r\n        sha256 = \"b956598d8cbe168b5ee717b5dafa56563eb5201a947856a6688bbeac9cac4e1f\",\r\n        strip_prefix = \"grpc-b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd\",\r\n        system_build_file = \"//third_party/systemlibs:grpc.BUILD\",\r\n        patch_file = \"//third_party/grpc:generate_cc_env_fix.patch\",\r\n        system_link_files = {\r\n            \"//third_party/systemlibs:BUILD\": \"bazel/BUILD\",\r\n            \"//third_party/systemlibs:grpc.BUILD\": \"src/compiler/BUILD\",\r\n            \"//third_party/systemlibs:grpc.bazel.grpc_deps.bzl\": \"bazel/grpc_deps.bzl\",\r\n            \"//third_party/systemlibs:grpc.bazel.grpc_extra_deps.bzl\": \"bazel/grpc_extra_deps.bzl\",\r\n            \"//third_party/systemlibs:grpc.bazel.cc_grpc_library.bzl\": \"bazel/cc_grpc_library.bzl\",\r\n            \"//third_party/systemlibs:grpc.bazel.generate_cc.bzl\": \"bazel/generate_cc.bzl\",\r\n            \"//third_party/systemlibs:grpc.bazel.protobuf.bzl\": \"bazel/protobuf.bzl\",\r\n        },\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz\",\r\n            \"https://github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz\",\r\n        ],\r\n    )\r\n```\r\nwhich means that instead of `src/compiler/BUILD`  the file `//third_party/systemlibs:grpc.BUILD` is used. This file is available, so I do not understand why bazel is complaing. \r\n\r\nIs this a problem with the systemlbs in 3.5.0 or a problem with `com_github_grpc_grpc`?", "comments": ["Hi @mihaimaruseac and @perfinion , any news? I am also blocked by the same issue.\r\n\r\n@mslacken Any work-around you've found?", "I'm currently trying to package TF 2.5.1 for Debian as well, but this one is a showstopper.", "The underlying issue is that the entry with `src/compiler/BUILD` is overwritten by the path in `system_build_file` .\r\nWe would need to point `//third_party/systemlibs:grpc.BUILD` to both `src/compiler/BUILD` and `bazel/BUILD`. Technically, the `system_link_files` are just symlinks and by that, we cannot point a single symlink to two different files.\r\n\r\nInstead, we have to split the `grpc.BUILD` and point `src/compiler/BUILD` to the corresponding file. This is implemented in the following patch (As TF requires a CLA, I will not create an MR).\r\n\r\n```patch\r\n--- /dev/null\r\n+++ b/third_party/systemlibs/grpc.bazel.grpc.src.compiler.bzl\r\n@@ -0,0 +1,23 @@\r\n+genrule(\r\n+    name = \"ln_grpc_cpp_plugin\",\r\n+    outs = [\"grpc_cpp_plugin.bin\"],\r\n+    cmd = \"ln -s $$(which grpc_cpp_plugin) $@\",\r\n+)\r\n+\r\n+sh_binary(\r\n+    name = \"grpc_cpp_plugin\",\r\n+    srcs = [\"grpc_cpp_plugin.bin\"],\r\n+    visibility = [\"//visibility:public\"],\r\n+)\r\n+\r\n+genrule(\r\n+    name = \"ln_grpc_python_plugin\",\r\n+    outs = [\"grpc_python_plugin.bin\"],\r\n+    cmd = \"ln -s $$(which grpc_python_plugin) $@\",\r\n+)\r\n+\r\n+sh_binary(\r\n+    name = \"grpc_python_plugin\",\r\n+    srcs = [\"grpc_python_plugin.bin\"],\r\n+    visibility = [\"//visibility:public\"],\r\n+)\r\n--- a/tensorflow/workspace2.bzl\r\n+++ b/tensorflow/workspace2.bzl\r\n@@ -640,7 +640,7 @@\r\n         patch_file = \"//third_party/grpc:generate_cc_env_fix.patch\",\r\n         system_link_files = {\r\n             \"//third_party/systemlibs:BUILD\": \"bazel/BUILD\",\r\n-            \"//third_party/systemlibs:grpc.BUILD\": \"src/compiler/BUILD\",\r\n+            \"//third_party/systemlibs:grpc.bazel.grpc.src.compiler.bzl\": \"src/compiler/BUILD\",\r\n             \"//third_party/systemlibs:grpc.bazel.grpc_deps.bzl\": \"bazel/grpc_deps.bzl\",\r\n             \"//third_party/systemlibs:grpc.bazel.grpc_extra_deps.bzl\": \"bazel/grpc_extra_deps.bzl\",\r\n             \"//third_party/systemlibs:grpc.bazel.cc_grpc_library.bzl\": \"bazel/cc_grpc_library.bzl\",\r\n--- a/third_party/systemlibs/grpc.BUILD\r\n+++ b/third_party/systemlibs/grpc.BUILD\r\n@@ -50,27 +50,3 @@\r\n     ],\r\n     visibility = [\"//visibility:public\"],\r\n )\r\n-\r\n-genrule(\r\n-    name = \"ln_grpc_cpp_plugin\",\r\n-    outs = [\"grpc_cpp_plugin.bin\"],\r\n-    cmd = \"ln -s $$(which grpc_cpp_plugin) $@\",\r\n-)\r\n-\r\n-sh_binary(\r\n-    name = \"grpc_cpp_plugin\",\r\n-    srcs = [\"grpc_cpp_plugin.bin\"],\r\n-    visibility = [\"//visibility:public\"],\r\n-)\r\n-\r\n-genrule(\r\n-    name = \"ln_grpc_python_plugin\",\r\n-    outs = [\"grpc_python_plugin.bin\"],\r\n-    cmd = \"ln -s $$(which grpc_python_plugin) $@\",\r\n-)\r\n-\r\n-sh_binary(\r\n-    name = \"grpc_python_plugin\",\r\n-    srcs = [\"grpc_python_plugin.bin\"],\r\n-    visibility = [\"//visibility:public\"],\r\n-)\r\n\r\n```"]}, {"number": 50132, "title": "CMake Error Building Tensorflow Lite", "body": "**System information**\r\n- OS Platform: Windows 10 Professional\r\n- TensorFlow installed from source\r\n- TensorFlow version: Latest\r\n- Installed using Git/CMake Version 3.20.0\r\n- Visual Studio Community 2019 16.9.31112.23\r\n\r\n\r\n**Describe the problem**\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI am attempting to build TFLM in VS2019 with the Eigen library, as reported in Issue #48255: https://github.com/tensorflow/tensorflow/issues/48255 \r\n\r\nAs part of troubleshooting that issue, I am trying to follow the basic instructions for building Tensorflow Lite using CMake, and the first basic build attempt fails. Following the instructions as per this page:\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library\r\n\r\nMy entire command prompt history for following the above instructions are below.\r\nC:\\Users\\jtork>mkdir cmake_fflite\r\nC:\\Users\\jtork>cd cmake_tflite\r\nC:\\Users\\jtork\\cmake_tflite>git clone https://github.com/tensorflow/tensorflow.git tensorflow_src\r\nC:\\Users\\jtork\\cmake_tflite>mkdir tflite_build\r\nC:\\Users\\jtork\\cmake_tflite>cd tflite_build\r\nC:\\Users\\jtork\\cmake_tflite\\tflite_build>cmake ../tensorflow_src/tensorflow/lite\r\n\r\nThe above lines complete successfully. The output logs from the first cmake are included below in \"Output Log 1\".\r\n\r\nThen the command below is entered to build the project:\r\nC:\\Users\\jtork\\cmake_tflite\\tflite_build>cmake --build . -j\r\n\r\nThe above line fails. The output logs from this command are included below in \"Output Log 2\". The failure occurs in building the project tflite_build\\flatbuffers-flatc.vcxproj, which throws an error [C2220](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2220?f1url=%3FappId%3DDev16IDEF1%26l%3DEN-US%26k%3Dk(C2220)%26rd%3Dtrue&view=msvc-160) for warning treated as an error, for warning [C5430](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4530?f1url=%3FappId%3DDev16IDEF1%26l%3DEN-US%26k%3Dk(C4530)%26rd%3Dtrue&view=msvc-160). \r\n\r\nThis warning is generated from the tflite_build\\flatbuffers-flatc\\src\\flatbuffers\\flatc-build\\flatc.vcxproj project, which references a \"vector\" (no extension) file located in C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include. The error is line 847 of this file, which uses a _TRY_BEGIN macro that apparently doesn't use unwind semantics for a try handler (the project is not configured to use unwind semantics). I was not able to identify the compiler option for it to use unwind semantics, which is the /EHsc compiler option.\r\n\r\nI believe this will be a multi-layered onion in getting to a point where I can successfully build a TFLM example project that uses and pulls in the Eigen library for LSTM models. But this is the first step, and as far as I can tell the basic CMake Tensorflow Lite build is broken.\r\n\r\n*****************************************************************\r\n************************* Output Log 1 ************************\r\n*****************************************************************\r\nC:\\Users\\jtork\\cmake_tflite\\tflite_build>cmake ../tensorflow_src/tensorflow/lite\r\n-- Building for: Visual Studio 16 2019\r\n-- Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- The C compiler identification is MSVC 19.28.29913.0\r\n-- The CXX compiler identification is MSVC 19.28.29913.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Looking for pthread.h\r\n-- Looking for pthread.h - not found\r\n-- Found Threads: TRUE\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11\r\n-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11 - Failed\r\n-- Performing Test COMPILER_SUPPORT_std=cpp03\r\n-- Performing Test COMPILER_SUPPORT_std=cpp03 - Failed\r\n-- Performing Test standard_math_library_linked_to_automatically\r\n-- Performing Test standard_math_library_linked_to_automatically - Success\r\n-- Standard libraries to link to explicitly: none\r\n-- Performing Test COMPILER_SUPPORT_OPENMP\r\n-- Performing Test COMPILER_SUPPORT_OPENMP - Success\r\n-- Looking for a Fortran compiler\r\n-- Looking for a Fortran compiler - NOTFOUND\r\n--\r\n-- Configured Eigen 3.4.99\r\n--\r\n-- Available targets (use: cmake --build . --target TARGET):\r\n-- ---------+--------------------------------------------------------------\r\n-- Target   |   Description\r\n-- ---------+--------------------------------------------------------------\r\n-- install  | Install Eigen. Headers will be installed to:\r\n--          |     <CMAKE_INSTALL_PREFIX>/<INCLUDE_INSTALL_DIR>\r\n--          |   Using the following values:\r\n--          |     CMAKE_INSTALL_PREFIX: C:/Program Files (x86)/tensorflow-lite\r\n--          |     INCLUDE_INSTALL_DIR:  include/eigen3\r\n--          |   Change the install location of Eigen headers using:\r\n--          |     cmake . -DCMAKE_INSTALL_PREFIX=yourprefix\r\n--          |   Or:\r\n--          |     cmake . -DINCLUDE_INSTALL_DIR=yourdir\r\n-- doc      | Generate the API documentation, requires Doxygen & LaTeX\r\n-- blas     | Build BLAS library (not the same thing as Eigen)\r\n-- uninstall| Remove files installed by the install target\r\n-- ---------+--------------------------------------------------------------\r\n--\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT\r\n-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Failed\r\n-- Looking for _strtof_l\r\n-- Looking for _strtof_l - found\r\n-- Looking for _strtoui64_l\r\n-- Looking for _strtoui64_l - found\r\n-- The ASM compiler identification is MSVC\r\n-- Found assembler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe\r\n-- Downloading clog to C:/Users/jtork/cmake_tflite/tflite_build/clog-source (define CLOG_SOURCE_DIR to avoid it)\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/clog-download\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'clog'\r\n  Performing download step (download, verify and extract) for 'clog'\r\n  -- Downloading...\r\n     dst='C:/Users/jtork/cmake_tflite/tflite_build/clog-download/clog-prefix/src/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'\r\n     timeout='none'\r\n     inactivity timeout='none'\r\n  -- Using src='https://github.com/pytorch/cpuinfo/archive/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'\r\n  -- [download 100% complete]\r\n  -- verifying file...\r\n         file='C:/Users/jtork/cmake_tflite/tflite_build/clog-download/clog-prefix/src/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='C:/Users/jtork/cmake_tflite/tflite_build/clog-download/clog-prefix/src/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'\r\n       dst='C:/Users/jtork/cmake_tflite/tflite_build/clog-source'\r\n  -- extracting... [tar xfz]\r\n  -- extracting... [analysis]\r\n  -- extracting... [rename]\r\n  -- extracting... [clean up]\r\n  -- extracting... done\r\n  Generating clog-prefix/src/clog-stamp/Debug/clog-update\r\n  Skipping patch step (no custom command) for 'clog'\r\n  No configure step for 'clog'\r\n  No build step for 'clog'\r\n  No install step for 'clog'\r\n  No test step for 'clog'\r\n  Completed 'clog'\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/clog-download/CMakeLists.txt\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/clog-download/CMakeLists.txt\r\n-- Downloading cpuinfo to C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-source (define CPUINFO_SOURCE_DIR to avoid it)\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'cpuinfo'\r\n  Performing download step (download, verify and extract) for 'cpuinfo'\r\n  -- Downloading...\r\n     dst='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/cpuinfo-prefix/src/5916273f79a21551890fd3d56fc5375a78d1598d.zip'\r\n     timeout='none'\r\n     inactivity timeout='none'\r\n  -- Using src='https://github.com/pytorch/cpuinfo/archive/5916273f79a21551890fd3d56fc5375a78d1598d.zip'\r\n  -- [download 100% complete]\r\n  -- verifying file...\r\n         file='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/cpuinfo-prefix/src/5916273f79a21551890fd3d56fc5375a78d1598d.zip'\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/cpuinfo-prefix/src/5916273f79a21551890fd3d56fc5375a78d1598d.zip'\r\n       dst='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-source'\r\n  -- extracting... [tar xfz]\r\n  -- extracting... [analysis]\r\n  -- extracting... [rename]\r\n  -- extracting... [clean up]\r\n  -- extracting... done\r\n  Generating cpuinfo-prefix/src/cpuinfo-stamp/Debug/cpuinfo-update\r\n  Performing patch step (custom command) for 'cpuinfo'\r\n  No configure step for 'cpuinfo'\r\n  No build step for 'cpuinfo'\r\n  No install step for 'cpuinfo'\r\n  No test step for 'cpuinfo'\r\n  Completed 'cpuinfo'\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/CMakeLists.txt\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/CMakeLists.txt\r\n-- Downloading FP16 to C:/Users/jtork/cmake_tflite/tflite_build/FP16-source (define FP16_SOURCE_DIR to avoid it)\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/FP16-download\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'fp16'\r\n  Performing download step (download, verify and extract) for 'fp16'\r\n  -- Downloading...\r\n     dst='C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/fp16-prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'\r\n     timeout='none'\r\n     inactivity timeout='none'\r\n  -- Using src='https://github.com/Maratyszcza/FP16/archive/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'\r\n  -- [download 100% complete]\r\n  -- verifying file...\r\n         file='C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/fp16-prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/fp16-prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'\r\n       dst='C:/Users/jtork/cmake_tflite/tflite_build/FP16-source'\r\n  -- extracting... [tar xfz]\r\n  -- extracting... [analysis]\r\n  -- extracting... [rename]\r\n  -- extracting... [clean up]\r\n  -- extracting... done\r\n  Generating fp16-prefix/src/fp16-stamp/Debug/fp16-update\r\n  Skipping patch step (no custom command) for 'fp16'\r\n  No configure step for 'fp16'\r\n  No build step for 'fp16'\r\n  No install step for 'fp16'\r\n  No test step for 'fp16'\r\n  Completed 'fp16'\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/CMakeLists.txt\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/CMakeLists.txt\r\n-- Downloading FXdiv to C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-source (define FXDIV_SOURCE_DIR to avoid it)\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'fxdiv'\r\n  Performing download step (download, verify and extract) for 'fxdiv'\r\n  -- Downloading...\r\n     dst='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/fxdiv-prefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'\r\n     timeout='none'\r\n     inactivity timeout='none'\r\n  -- Using src='https://github.com/Maratyszcza/FXdiv/archive/b408327ac2a15ec3e43352421954f5b1967701d1.zip'\r\n  -- [download 100% complete]\r\n  -- verifying file...\r\n         file='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/fxdiv-prefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/fxdiv-prefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'\r\n       dst='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-source'\r\n  -- extracting... [tar xfz]\r\n  -- extracting... [analysis]\r\n  -- extracting... [rename]\r\n  -- extracting... [clean up]\r\n  -- extracting... done\r\n  Generating fxdiv-prefix/src/fxdiv-stamp/Debug/fxdiv-update\r\n  Skipping patch step (no custom command) for 'fxdiv'\r\n  No configure step for 'fxdiv'\r\n  No build step for 'fxdiv'\r\n  No install step for 'fxdiv'\r\n  No test step for 'fxdiv'\r\n  Completed 'fxdiv'\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/CMakeLists.txt\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/CMakeLists.txt\r\n-- Downloading pthreadpool to C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-source (define PTHREADPOOL_SOURCE_DIR to avoid it)\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'pthreadpool'\r\n  Performing download step (download, verify and extract) for 'pthreadpool'\r\n  -- Downloading...\r\n     dst='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/pthreadpool-prefix/src/545ebe9f225aec6dca49109516fac02e973a3de2.zip'\r\n     timeout='none'\r\n     inactivity timeout='none'\r\n  -- Using src='https://github.com/Maratyszcza/pthreadpool/archive/545ebe9f225aec6dca49109516fac02e973a3de2.zip'\r\n  -- [download 100% complete]\r\n  -- verifying file...\r\n         file='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/pthreadpool-prefix/src/545ebe9f225aec6dca49109516fac02e973a3de2.zip'\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/pthreadpool-prefix/src/545ebe9f225aec6dca49109516fac02e973a3de2.zip'\r\n       dst='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-source'\r\n  -- extracting... [tar xfz]\r\n  -- extracting... [analysis]\r\n  -- extracting... [rename]\r\n  -- extracting... [clean up]\r\n  -- extracting... done\r\n  Generating pthreadpool-prefix/src/pthreadpool-stamp/Debug/pthreadpool-update\r\n  Skipping patch step (no custom command) for 'pthreadpool'\r\n  No configure step for 'pthreadpool'\r\n  No build step for 'pthreadpool'\r\n  No install step for 'pthreadpool'\r\n  No test step for 'pthreadpool'\r\n  Completed 'pthreadpool'\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/CMakeLists.txt\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/CMakeLists.txt\r\n-- Downloading PSimd to C:/Users/jtork/cmake_tflite/tflite_build/psimd-source (define PSIMD_SOURCE_DIR to avoid it)\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/psimd-download\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'psimd'\r\n  Performing download step (git clone) for 'psimd'\r\n  Cloning into 'psimd-source'...\r\n  Already on 'master'\r\n  Your branch is up to date with 'origin/master'.\r\n  Performing update step (git update) for 'psimd'\r\n  Skipping patch step (no custom command) for 'psimd'\r\n  No configure step for 'psimd'\r\n  No build step for 'psimd'\r\n  No install step for 'psimd'\r\n  No test step for 'psimd'\r\n  Completed 'psimd'\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/psimd-download/CMakeLists.txt\r\n  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/psimd-download/CMakeLists.txt\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build\r\n\r\nC:\\Users\\jtork\\cmake_tflite\\tflite_build>\r\n*****************************************************************\r\n************************* Output Log 2 ************************\r\n*****************************************************************\r\n\r\nC:\\Users\\jtork\\cmake_tflite\\tflite_build>cmake --build . -j\r\nMicrosoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Creating directories for 'flatbuffers-flatc'\r\n  Skipping download step (SOURCE_DIR given) for 'flatbuffers-flatc'\r\n  Generating flatbuffers-flatc/src/flatbuffers-flatc-stamp/Debug/flatbuffers-flatc-update\r\n  Skipping patch step (no custom command) for 'flatbuffers-flatc'\r\n  Performing configure step for 'flatbuffers-flatc'\r\n  -- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.\r\n  -- The C compiler identification is MSVC 19.28.29913.0\r\n  -- The CXX compiler identification is MSVC 19.28.29913.0\r\n  -- Detecting C compiler ABI info\r\n  -- Detecting C compiler ABI info - done\r\n  -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped\r\n  -- Detecting C compile features\r\n  -- Detecting C compile features - done\r\n  -- Detecting CXX compiler ABI info\r\n  -- Detecting CXX compiler ABI info - done\r\n  -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped\r\n  -- Detecting CXX compile features\r\n  -- Detecting CXX compile features - done\r\n  -- Looking for _strtof_l\r\n  -- Looking for _strtof_l - found\r\n  -- Looking for _strtoui64_l\r\n  -- Looking for _strtoui64_l - found\r\n  -- Configuring done\r\n  -- Generating done\r\n  -- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/flatbuffers-flatc/src/flatbuffers-flatc-build\r\n  Performing build step for 'flatbuffers-flatc'\r\n  Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework\r\n  Copyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n    Checking Build System\r\n    Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/flatbuffers/CMakeLists.txt\r\n    idl_parser.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_text.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    reflection.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\src\\reflection.cpp(196): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::_Simple_types<_T\r\n  y>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\src\\reflection.cpp(306): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    util.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\istream(519,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\f\r\nlatbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\istream(513): message : while compiling class template member function 'std::basic_istream<char,std::char_traits<char>> &std::basic_istream<char,std::char_traits<char>>::read(_Elem *,std::streamsize)' [C:\\Users\\jtork\\c\r\n  make_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Elem=char\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\src\\util.cpp(64): message : see reference to function template instantiation 'std::basic_istream<char,std::char_traits<char>> &std::basic_istream<char,std::char_traits<char>>::read(_Elem *,std::streamsize)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_bui\r\n  ld\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Elem=char\r\n            ]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\istream(699): message : see reference to class template instantiation 'std::basic_istream<char,std::char_traits<char>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-b\r\n  uild\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\istream(519,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Us\r\ners\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_cpp.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_csharp.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_dart.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_kotlin.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_go.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_java.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_js_ts.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_php.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_python.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_lobster.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_lua.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_rust.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_fbs.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_grpc.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_json_schema.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    idl_gen_swift.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    Generating Code...\r\n    Compiling...\r\n    flatc.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    flatc_main.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    code_generators.cpp\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\fl\r\natbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec\r\n  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::\r\n  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n            with\r\n            [\r\n                _Ty=uint8_t\r\n            ]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Use\r\nrs\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    cpp_generator.cc\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\f\r\nlatbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\\Users\\jtork\\cmake_tflite\\t\r\n  flite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_b\r\n  uild\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-b\r\n  uild\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Us\r\ners\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    go_generator.cc\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\f\r\nlatbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\\Users\\jtork\\cmake_tflite\\t\r\n  flite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\grpc\\src\\compiler\\go_generator.cc(43): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tf\r\n  lite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-b\r\n  uild\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Us\r\ners\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    java_generator.cc\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\f\r\nlatbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\\Users\\jtork\\cmake_tflite\\t\r\n  flite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_b\r\n  uild\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-b\r\n  uild\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Us\r\ners\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    python_generator.cc\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\f\r\nlatbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\\Users\\jtork\\cmake_tflite\\t\r\n  flite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_b\r\n  uild\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-b\r\n  uild\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Us\r\ners\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    swift_generator.cc\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): error C2220: the following warning is treated as an error [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\f\r\nlatbuffers-flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\\Users\\jtork\\cmake_tflite\\t\r\n  flite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\\include\\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_b\r\n  uild\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj]\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-b\r\n  uild\\flatc.vcxproj]\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.28.29910\\include\\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc\\src\\flatbuffers-flatc-build\\flatc.vcxproj] [C:\\Us\r\ners\\jtork\\cmake_tflite\\tflite_build\\flatbuffers-flatc.vcxproj]\r\n    Generating Code...\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Microsoft\\VC\\v160\\Microsoft.CppCommon.targets(240,5): error MSB8066: Custom build for 'C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-flatc-mkdir.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMak\r\neFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-flatc-download.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-flatc-update.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-flatc-patch.rule;C:\\Users\\jtork\\\r\ncmake_tflite\\tflite_build\\CMakeFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-flatc-configure.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-flatc-build.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\677b4dc9deae3119b927753ee87e3f27\\flatbuffers-fl\r\natc-install.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\079e40c0cfd707bfff9f1d804fd56055\\flatbuffers-flatc-complete.rule;C:\\Users\\jtork\\cmake_tflite\\tflite_build\\CMakeFiles\\f737f6c2b0c30e020289204dac9114d1\\flatbuffers-flatc.rule' exited with code 1. [C:\\Users\\jtork\\cmake_tflite\\tflite_build\\flatbuffers\r\n-flatc.vcxproj]\r\n\r\nC:\\Users\\jtork\\cmake_tflite\\tflite_build>\r\n\r\n\r\n\r\n\r\n", "comments": ["@terryheo could you take a look?"]}, {"number": 50130, "title": "TensorFlow renames inputs when restoring/resaving SavedModel", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tf-nightly\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: CPU\r\n- GPU model and memory: CPU\r\n\r\n**Describe the current behavior**\r\nWhen a user restores a SavedModel and resaves it, TensorFlow renames the model inputs in the signature, prefixing them with `\"inputs/\"`. I'm not sure when this behavior was introduced (it happened non-deterministically in `tensorflow==2.4.1`, but is deterministic in `tensorflow==2.5.0` and `tf-nightly`) or expected. I have a use case where I am constrained in my input names in the signature, although I can work around it if renaming is expected (and deterministic) so this issue is mainly to raise awareness if the renaming is unexpected \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport subprocess\r\nimport tensorflow as tf\r\n\r\ninputs = {\r\n    \"first_feature\": tf.constant([1, 2, 3]),\r\n    \"second_feature\": tf.constant([4, 5, 6]),\r\n}\r\n\r\nclass MyModel(tf.keras.Model):\r\n    def call(self, inputs, training=None):\r\n        return tf.random.uniform([3, 1])\r\n\r\nmodel = MyModel()\r\nmodel.compile(optimizer=\"sgd\", loss=\"mse\")\r\nmodel.fit(inputs, tf.constant([7, 8, 9]))\r\nmodel.save(\"model\")\r\ndict_model_def = subprocess.check_output(\"saved_model_cli show --dir model --tag_set serve --signature_def serving_default\".split())\r\nprint(\"MODEL\")\r\nprint(dict_model_def.decode(\"utf-8\"))\r\n\r\nprint(\"RESTORED MODEL\")\r\nrestored_model = tf.keras.models.load_model(\"model\")\r\nrestored_model.save(\"restored_model\")\r\nrestored_model_def = subprocess.check_output(\"saved_model_cli show --dir restored_model --tag_set serve --signature_def serving_default\".split())\r\nprint(restored_model_def.decode(\"utf-8\"))\r\n```\r\n\r\nOutput:\r\n```\r\n1/1 [==============================] - 0s 117ms/step - loss: 56.5648\r\nMODEL\r\nThe given SavedModel SignatureDef contains the following input(s):\r\n  inputs['first_feature'] tensor_info:\r\n      dtype: DT_INT32\r\n      shape: (-1, 1)\r\n      name: serving_default_first_feature:0\r\n  inputs['second_feature'] tensor_info:\r\n      dtype: DT_INT32\r\n      shape: (-1, 1)\r\n      name: serving_default_second_feature:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\n  outputs['output_1'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (3, 1)\r\n      name: StatefulPartitionedCall:0\r\nMethod name is: tensorflow/serving/predict\r\n\r\nRESTORED MODEL\r\nThe given SavedModel SignatureDef contains the following input(s):\r\n  inputs['inputs/first_feature'] tensor_info:\r\n      dtype: DT_INT32\r\n      shape: (-1, 1)\r\n      name: serving_default_inputs/first_feature:0\r\n  inputs['inputs/second_feature'] tensor_info:\r\n      dtype: DT_INT32\r\n      shape: (-1, 1)\r\n      name: serving_default_inputs/second_feature:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\n  outputs['output_1'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (3, 1)\r\n      name: StatefulPartitionedCall:0\r\nMethod name is: tensorflow/serving/predict\r\n```", "comments": ["Was able to reproduce the issue in [TF 2.5](https://colab.research.google.com/gist/saikumarchalla/74ba0e9d6ee7c70332e9fa9e054a8be4/untitled.ipynb) but in [TF 2.4](https://colab.research.google.com/gist/saikumarchalla/3a336fa0e3587807e4f2696b09cec5e3/untitled97.ipynb) model inputs are same. Please find the attached gists. Thanks!", "@saikumarchalla if I re-run your TF 2.4 gist a few times I do get the inputs modified intermittently about half the time.", "Bump, as I dug further into this. If I change the code above to simply decorate `MyModel.call` with `@tf.function` and do `model.call.get_concrete_function(inputs).structured_input_signature`, I get\r\n\r\n```\r\n(({'first_feature': TensorSpec(shape=(3,), dtype=tf.int32, name='inputs/first_feature'),\r\n   'second_feature': TensorSpec(shape=(3,), dtype=tf.int32, name='inputs/second_feature')},\r\n  None),\r\n {})\r\n```\r\n\r\nThis information is used when restoring the model in `infer_inputs_from_restored_function_call`\r\nhttps://github.com/tensorflow/tensorflow/blob/0733c41e5cfe09362b6ec1559b1b02bd1c19fff2/tensorflow/python/keras/saving/saved_model/load.py#L1133\r\n\r\nwhere the docstring notes:\r\n\r\n```\r\nRestored layer call function. It is assumed that `fn` has at least\r\n        one concrete function and that the inputs are in the first argument.\r\n```\r\n\r\nWhen restoring the model above, `infer_inputs_from_restored_function_call` does have multiple concrete functions, but the input names differ across them (in tensorflow<2.5 perhaps there was some randomness in the ordering of these concrete functions).\r\n\r\nGiven that `get_concrete_function` joins the Python variable name (`inputs` in this case) with the name of the input (`first_feature` or `second_feature`), I wonder if there is a way to mark the correct concrete function from which to infer the inputs when saving the Keras model instead of just assuming the first one or otherwise remove the Python variable name."]}, {"number": 50124, "title": "Keras `image_dataset_from_directory` shuffles labels", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- CUDA version: \r\n```\r\nnvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2021 NVIDIA Corporation\r\nBuilt on Sun_Feb_14_22:08:44_Pacific_Standard_Time_2021\r\nCuda compilation tools, release 11.2, V11.2.152\r\nBuild cuda_11.2.r11.2/compiler.29618528_0\r\n```\r\n- cuDNN version: `cudnn-11.2-windows-x64-v8.1.1.33`\r\n- GPU model and memory: RTX 2070 Super\r\n\r\n**Describe the current behavior**\r\nI have a folder structure with ~6500 different classes.\r\nThe structure is the following:\r\n```\r\netlcdb\r\n|->  0000\r\n|       -> 0.jpg\r\n|       -> 1.jpg\r\n|       ...\r\n|-> 0001\r\n|       -> 0.jpg\r\n|       -> 1.jpg\r\n|       ...\r\n|   .\r\n|   .\r\n|   .\r\n|-> 6542\r\n|       -> 0.jpg\r\n|       -> 1.jpg\r\n|       ...\r\n```\r\nI am using `tf.keras.preprocessing.image_dataset_from_directory` to create a `tf.data.dataset` from this folder structure.\r\nLike this:\r\n```\r\n#batch size\r\nbs=512\r\n# class names\r\nclasses = [\"%04d\" % i for i in range(len(labels))]\r\n\r\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(\r\n    directory=r'F:\\data_sets\\etlcdb',\r\n    labels=\"inferred\",\r\n    label_mode=\"categorical\",\r\n    class_names=classes,\r\n    color_mode=\"grayscale\",\r\n    batch_size=bs,\r\n    image_size=(64, 64),\r\n    validation_split=0.15,\r\n    subset=\"training\",\r\n    seed=123\r\n)\r\n```\r\nOutput:\r\n```\r\nFound 6731099 files belonging to 6543 classes.\r\nUsing 5721435 files for training.\r\n```\r\nAfterwards I define some processing:\r\n```\r\ntrain = train_dataset.map(\r\n    lambda x, y : (tf.cast(x, tf.float16), (tf.cast(y, tf.float16))),\r\n    num_parallel_calls=tf.data.AUTOTUNE\r\n)\r\ntrain = train.cache(r\"F:\\data_sets\\etlcdb_cache\\cache_train\")\r\n#train = train.shuffle(buffer_size=bs*3)\r\ntrain = train.prefetch(buffer_size=tf.data.AUTOTUNE)\r\n```\r\nFinally I train the network with `fit()`.<br/><br/>\r\nWhen than predictions are made the labels do not match anymore.\r\nWith a variable `labels` which contains the class labels matching the folder structure (attached as a .txt file) I execute following code:\r\n```\r\nsample = tf.keras.preprocessing.image.load_img(\r\n    path= r\"F:\\data_sets\\etlcdb\\0002\\3.jpg\",\r\n    color_mode=\"grayscale\"\r\n)\r\nsample = tf.keras.preprocessing.image.img_to_array(sample)\r\nsample = sample.reshape((1, 64, 64, 1))\r\n\r\nprediction = f16_model.predict(sample)\r\n```\r\nResult (the text on top of the image is the prediction and its \"accuracy\"):\r\n![image](https://user-images.githubusercontent.com/51273483/121015823-b70ef080-c79b-11eb-984f-4d61e49e5e7f.png)\r\nBecause the CNN does detect all 2's as \u60f9 I am certain that the labels somehow get mixed up.\r\n\r\n**Describe the expected behavior**\r\nThe labels inferred by the folder structure should line-up with the output tensor \r\nfrom the last layer of the CNN.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe notebook where this occurred can be found [here](https://github.com/CaptainDario/DaKanji-ML/blob/feature-recognize_more_characters/single_kanji_cnn/single_kanji_cnn_training.ipynb).\r\nBut the appropriate folder structure needs to be created.\r\n\r\n\r\n[labels.txt](https://github.com/tensorflow/tensorflow/files/6608772/labels.txt)", "comments": ["Is the assumption that the inferred labels should have the following structure wrong?\r\n\r\n| folder name | one hot vector|\r\n| :-----------: | :---: |\r\n| 0000 | [1, 0, 0, ..., 0] |\r\n| 0001 | [0, 1, 0, ..., 0] |\r\n| 0002 | [0, 0, 1, ..., 0] |\r\n| ... | ... |\r\n| 6542 | [0, 0, 0, ..., 1] |\r\n", "After further investigation it seems that `class_names=classes` is being ignored.\r\nKeras infers the label order like the following:\r\n\r\n| folder name | inferred one hot vector |\r\n| :---: | :---: |\r\n| 0000 | [1, 0, 0, 0, 0, ..., 0] |\r\n| 0001 | [0, 1, 0, 0, 0, ..., 0] |\r\n| 0010 | [0, 0, 1, 0, 0, ..., 0] |\r\n| 0100 | [0, 0, 0, 1, 0, ..., 0] |\r\n| 1000 | [0, 0, 0, 0, 1, ..., 0] |\r\n\r\n@UsharaniPagadala should setting `class_names=[\"0000\", \"0001\", \"0002\", \"0003\", \"0004\", ....]` not force the order I want?\r\n\r\n\r\n**Desired order:** \r\n| folder name | one hot vector|\r\n| :-----------: | :---: |\r\n| 0000 | [1, 0, 0, ..., 0] |\r\n| 0001 | [0, 1, 0, ..., 0] |\r\n| 0002 | [0, 0, 1, ..., 0] |\r\n| ... | ... |\r\n| 6542 | [0, 0, 0, ..., 1] |", "If I order the labels like this to match keras inferred labels everything works fine.\r\n\r\n``` python\r\nls = labels_1 + labels_2\r\n# order the labels\r\nindexs = sorted([str(i) for i in range(0, len(ls))])\r\nordered_labels = [ls[int(i)] for i in indexs]\r\n```\r\n\r\nDo I not understand the `class_names` argument correctly or is this a bug?\r\n\r\nBecause the docs say this:\r\n> Only valid if \"labels\" is \"inferred\". This is the explict list of class names (must match names of subdirectories). **Used to control the order of the classes (otherwise alphanumerical order is used).**\r\n\r\nI think the one-hot encoding should be different.", "@UsharaniPagadala it seems to be `tf 2.5.0` related. I ran the same notebook on `tf 2.4.0` and there the ordering is correct.", "@CaptainDario Can you please check with `tf-nightly` and let us know whether the issue persists. Thanks!", "I will try with `tf-nightly` in the coming days and report back here.", "Tried it an the problem persists with `tf-nightly==2.6.0.dev20210616`.", "@CaptainDario If you can create a simple standalone code to reproduce the issue, then I can test it myself. Currently I am not sure what is the root-cause. Can you please create a colab gist or jupyter notebook that we can use to reproduce? Thanks!", "@jvishnuvardhan I will try to create a standalone notebook in the coming days.", "@jvishnuvardhan is there away to give you my dataset directly?\r\nI cannot reproduce the issue with anythin else and caching the dataset takes to much space on colab.", "@CaptainDario If the issue is with your data (as you can not reproduce with other data), then this is not a TF issue. TF repository focus on bugs/performance related issues. We cannot debug your data. Thanks!", "How can wrongly inferred labels be caused by my data?\r\nMy dataset has 6000+ classes and is highly imbalanced and something of that causes the labels to be mixed up.\r\n\r\nHowever if this is too much of an edge case feel free to close this.", "I seem to be having the same issue. I have a model that uses inferred labels for a large image dataset, and I mapped them to numeric ids using the class_names argument. It was all working properly until a few days ago.", "@teeters could you try removing a part of the samples and see if the issue persists? For me it was solved when I removed ~50% of the samples of every class.", "So, I haven't tried removing data, but I did try visualizing some of the images using this code:\r\n\r\n```\r\nimport matplotlib.pyplot as plt \r\n\r\nplt.figure(figsize=(10, 10))\r\nfor images, labels in train_dataset.take(1):\r\n  for i in range(9):\r\n    ax = plt.subplot(3, 3, i + 1)\r\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\r\n    label = tf.argmax(labels[i]).numpy()\r\n    plt.title(str(label)+' '+class_names[label])\r\n    plt.axis(\"off\")\r\n```\r\n\r\n(`class_names` is a dict that maps the string class names to their desired numeric ids). The images show up with the correct ids and string names. So maybe the shuffling is happening somewhere else in the training process.", "> After further investigation it seems that `class_names=classes` is being ignored.\r\n> Keras infers the label order like the following:\r\n> \r\n> folder name\tinferred one hot vector\r\n> 0000\t[1, 0, 0, 0, 0, ..., 0]\r\n> 0001\t[0, 1, 0, 0, 0, ..., 0]\r\n> 0010\t[0, 0, 1, 0, 0, ..., 0]\r\n> 0100\t[0, 0, 0, 1, 0, ..., 0]\r\n> 1000\t[0, 0, 0, 0, 1, ..., 0]\r\n> @UsharaniPagadala should setting `class_names=[\"0000\", \"0001\", \"0002\", \"0003\", \"0004\", ....]` not force the order I want?\r\n> \r\n> **Desired order:**\r\n> \r\n> folder name\tone hot vector\r\n> 0000\t[1, 0, 0, ..., 0]\r\n> 0001\t[0, 1, 0, ..., 0]\r\n> 0002\t[0, 0, 1, ..., 0]\r\n> ...\t...\r\n> 6542\t[0, 0, 0, ..., 1]\r\n\r\n@CaptainDario how did you view the one-hot vectors in this post?", "> \r\n> \r\n> > After further investigation it seems that `class_names=classes` is being ignored.\r\n> > Keras infers the label order like the following:\r\n> > folder name\tinferred one hot vector\r\n> > 0000\t[1, 0, 0, 0, 0, ..., 0]\r\n> > 0001\t[0, 1, 0, 0, 0, ..., 0]\r\n> > 0010\t[0, 0, 1, 0, 0, ..., 0]\r\n> > 0100\t[0, 0, 0, 1, 0, ..., 0]\r\n> > 1000\t[0, 0, 0, 0, 1, ..., 0]\r\n> > @UsharaniPagadala should setting `class_names=[\"0000\", \"0001\", \"0002\", \"0003\", \"0004\", ....]` not force the order I want?\r\n> > **Desired order:**\r\n> > folder name\tone hot vector\r\n> > 0000\t[1, 0, 0, ..., 0]\r\n> > 0001\t[0, 1, 0, ..., 0]\r\n> > 0002\t[0, 0, 1, ..., 0]\r\n> > ...\t...\r\n> > 6542\t[0, 0, 0, ..., 1]\r\n> \r\n> @CaptainDario how did you view the one-hot vectors in this post?\r\n\r\n@teeters  I trained a model on my data and gave it examples with the specific label.\r\nAnd allways when I gave a sample from class \"0010\" it returned `[0, 0, 1, ...]`. Therefore I am sure that it actually learned the right thing just with the wrong label...", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 50108, "title": "ERROR: tensorflow-2.6.0-cp38-cp38-macosx_11_0_x86_64.whl is not a supported wheel on this platform.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Big Sur 11.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n-Source\r\n- TensorFlow version:\r\n- Tensorflow 2.6\r\n- Python version:\r\n- Python 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Conda\r\n- Bazel version (if compiling from source):\r\n- basilisk\r\n- GCC/Compiler version (if compiling from source):\r\n- Apple clang version 12.0.5 (clang-1205.0.22.9)\r\nTarget: x86_64-apple-darwin20.5.0\r\n\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n- AMD Radeon\r\n\r\n\r\n\r\n**Describe the problem**\r\nERROR: tensorflow-2.6.0-cp38-cp38-macosx_11_0_x86_64.whl is not a supported wheel on this platform.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n pip install /tmp/tensorflow_pkg/tensorflow-2.6.0-cp38-cp38-macosx_11_0_x86_64.whl\r\n\r\n\r\n**Any other info / logs**\r\nHere are the currently supported wheels.  Big Sur is OS X 11, prior wheels are OS X 10...\r\n\r\nhttps://pypi.org/project/tensorflow/2.5.0/#files\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@dbl001 Can you please list what you steps you followed before encountering this error? Where did you find that .whl? Also, share any other details to resolve this issue faster. Thanks!", "On Big Sur 11.4\r\n```\r\n$ bazelisk build [--config=option] //tensorflow/tools/pip_package:build_pip_package\r\n...\r\n$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\ns/pip_package/build_pip_package /tmp/tensorflow_pkg\r\nMon Jun 21 20:01:41 PDT 2021 : === Preparing sources in dir: /var/folders/3n/56fpv14n4wj0c1l1sb106pzw0000gn/T/tmp.XXXXXXXXXX.g9mTJx0U\r\n~/tensorflow ~/tensorflow\r\n~/tensorflow\r\n~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/tensorflow\r\n~/tensorflow\r\n/var/folders/3n/56fpv14n4wj0c1l1sb106pzw0000gn/T/tmp.XXXXXXXXXX.g9mTJx0U/tensorflow/include ~/tensorflow\r\n~/tensorflow\r\nMon Jun 21 20:02:06 PDT 2021 : === Building wheel\r\nwarning: no files found matching 'README'\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pyi' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.so.[0-9]' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.csv' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*.proto' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\n[WARNING] This wheel needs a higher macOS version than the version your Python interpreter is compiled against.  To silence this warning, set MACOSX_DEPLOYMENT_TARGET to at least 11_0 or recreate these files with lower MACOSX_DEPLOYMENT_TARGET:  \r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/libtensorflow_framework.2.dylib\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/libtensorflow_framework.2.6.0.dylib\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/libtensorflow_framework.dylib\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_tfe.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_device_lib.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_quantize_training.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_tfcompile.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_toco_api.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_mlir.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_parallel_device.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_tf_session.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_py_exception_registry.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_tensorflow_internal.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_debug_events_writer.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_sanitizers.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/_pywrap_events_writer.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/autograph/impl/testing/pybind_for_testing.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/saved_model/experimental/pywrap_libexport.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_transform_graph.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_kernel_registry.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_stat_summarizer.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_tf_stack.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_tfprof.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_nest.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_util_port.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/fast_module_type.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_checkpoint_reader.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_tensor_float_32_execution.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/util/_pywrap_utils.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_dtypes.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_op_def_registry.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_pywrap_python_api_info.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/fast_tensor_util.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_errors_test_helper.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_python_memory_checker_helper.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_pywrap_python_op_gen.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_pywrap_python_api_dispatcher.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_pywrap_python_api_parameter_converter.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_op_def_util.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/framework/_proto_comparators.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/platform/_pywrap_tf2.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/platform/_pywrap_stacktrace_handler.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/grappler/_pywrap_tf_optimizer.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/grappler/_pywrap_tf_cluster.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/grappler/_pywrap_tf_item.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/lib/core/_pywrap_bfloat16.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/lib/core/_pywrap_py_func.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/lib/io/_pywrap_record_io.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/lib/io/_pywrap_file_io.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/profiler/internal/_pywrap_profiler.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/profiler/internal/_pywrap_traceme.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/data/experimental/service/_pywrap_server_lib.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/python/data/experimental/service/_pywrap_utils.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/lite/python/optimize/_pywrap_tensorflow_lite_calibration_wrapper.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/compiler/tf2tensorrt/_pywrap_py_utils.so\r\nbuild/bdist.macosx-10.9-x86_64/wheel/tensorflow/compiler/tf2xla/ops/_xla_ops.soTraceback (most recent call last):\r\n  File \"setup.py\", line 304, in <module>\r\n    setup(\r\n  File \"/Users/davidlaxer/anaconda3/lib/python3.8/site-packages/setuptools/__init__.py\", line 153, in setup\r\n    return distutils.core.setup(**attrs)\r\n  File \"/Users/davidlaxer/anaconda3/lib/python3.8/distutils/core.py\", line 148, in setup\r\n    dist.run_commands()\r\n  File \"/Users/davidlaxer/anaconda3/lib/python3.8/distutils/dist.py\", line 966, in run_commands\r\n    self.run_command(cmd)\r\n  File \"/Users/davidlaxer/anaconda3/lib/python3.8/distutils/dist.py\", line 985, in run_command\r\n    cmd_obj.run()\r\n  File \"/Users/davidlaxer/anaconda3/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 328, in run\r\n    impl_tag, abi_tag, plat_tag = self.get_tag()\r\n  File \"/Users/davidlaxer/anaconda3/lib/python3.8/site-packages/wheel/bdist_wheel.py\", line 278, in get_tag\r\n    assert tag in supported_tags, \"would build wheel with unsupported tag {}\".format(tag)\r\nAssertionError: would build wheel with unsupported tag ('cp38', 'cp38', 'macosx_11_0_x86_64')\r\n```", "I tried download a wheel from \r\nhttps://pypi.org/project/tensorflow-macos/#files.\r\n\r\nTo get it to install, I tried renaming it from:\r\ntensorflow_macos-2.5.0-cp38-cp38-macosx_11_0_x86_64.whl\r\nto\r\ntensorflow_macos-2.5.0-cp38-cp38-macosx_10_15_x86_64.whl\r\n\r\nAfter the .whl installed, I tried importing tensorflow in python:\r\n\r\n```\r\nIn [2]: import tensorflow\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-2-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\n~/tensorflow-metal/lib/python3.8/site-packages/tensorflow/__init__.py in <module>\r\n    447     _plugin_dir = _os.path.join(_s, 'tensorflow-plugins')\r\n    448     if _os.path.exists(_plugin_dir):\r\n--> 449       _ll.load_library(_plugin_dir)\r\n    450       # Load Pluggable Device Library\r\n    451       _ll.load_pluggable_device_library(_plugin_dir)\r\n\r\n~/tensorflow-metal/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)\r\n    152 \r\n    153     for lib in kernel_libraries:\r\n--> 154       py_tf.TF_LoadLibrary(lib)\r\n    155 \r\n    156   else:\r\n\r\nNotFoundError: dlopen(/Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: _TF_AssignUpdateVariable\r\n  Referenced from: /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib\r\n  Expected in: flat namespace\r\n\r\nIn [3]: quit()\r\n(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 notebooks % nm -n /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib | grep  _TF_AssignUpdateVariable\r\n\r\n                 U _TF_AssignUpdateVariable\r\n\r\n```", "When I followed the instructions for installing tensorflow-macos and tensorflow-metal from:\r\nhttps://developer.apple.com/metal/tensorflow-plugin/\r\n\r\nI got:\r\n```\r\npython -m pip install tensorflow-macos\r\nRequirement already satisfied: tensorflow-macos in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (2.5.0)\r\nRequirement already satisfied: six~=1.15.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.15.0)\r\nRequirement already satisfied: wheel~=0.35 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (0.36.2)\r\nRequirement already satisfied: numpy~=1.19.2 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.19.5)\r\nRequirement already satisfied: tensorboard~=2.5 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (2.5.0)\r\nRequirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (2.5.0)\r\nRequirement already satisfied: google-pasta~=0.2 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (0.2.0)\r\nRequirement already satisfied: astunparse~=1.6.3 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.6.3)\r\nRequirement already satisfied: typing-extensions~=3.7.4 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (3.7.4.3)\r\nRequirement already satisfied: grpcio~=1.34.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.34.1)\r\nRequirement already satisfied: h5py~=3.1.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (3.1.0)\r\nRequirement already satisfied: keras-nightly~=2.5.0.dev in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (2.5.0.dev2021032900)\r\nRequirement already satisfied: gast==0.4.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (0.4.0)\r\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.1.2)\r\nRequirement already satisfied: protobuf>=3.9.2 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (3.17.3)\r\nRequirement already satisfied: flatbuffers~=1.12.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.12)\r\nRequirement already satisfied: wrapt~=1.12.1 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.12.1)\r\nRequirement already satisfied: absl-py~=0.10 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (0.13.0)\r\nRequirement already satisfied: opt-einsum~=3.3.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (3.3.0)\r\nRequirement already satisfied: termcolor~=1.1.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-macos) (1.1.0)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (2.0.1)\r\nRequirement already satisfied: google-auth<2,>=1.6.3 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (1.32.0)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (2.25.1)\r\nRequirement already satisfied: markdown>=2.6.8 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (3.3.4)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (0.4.4)\r\nRequirement already satisfied: setuptools>=41.0.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (47.1.0)\r\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (0.6.1)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow-macos) (1.8.0)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-macos) (0.2.8)\r\nRequirement already satisfied: rsa<5,>=3.1.4 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-macos) (4.7.2)\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-macos) (4.2.2)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-macos) (1.3.0)\r\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-macos) (0.4.8)\r\nRequirement already satisfied: certifi>=2017.4.17 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-macos) (2021.5.30)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-macos) (4.0.0)\r\nRequirement already satisfied: idna<3,>=2.5 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-macos) (2.10)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-macos) (1.26.5)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-macos) (3.1.1)\r\n(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 notebooks % python -m pip install tensorflow-metal\r\nRequirement already satisfied: tensorflow-metal in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (0.1.1)\r\nRequirement already satisfied: wheel~=0.35 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-metal) (0.36.2)\r\nRequirement already satisfied: six~=1.15.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-metal) (1.15.0)\r\nRequirement already satisfied: grpcio~=1.34.0 in /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages (from tensorflow-metal) (1.34.1)\r\n(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 notebooks % ipython\r\nPython 3.8.5 (default, Sep  4 2020, 02:22:02) \r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.24.1 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\n~/tensorflow-metal/lib/python3.8/site-packages/tensorflow/__init__.py in <module>\r\n    447     _plugin_dir = _os.path.join(_s, 'tensorflow-plugins')\r\n    448     if _os.path.exists(_plugin_dir):\r\n--> 449       _ll.load_library(_plugin_dir)\r\n    450       # Load Pluggable Device Library\r\n    451       _ll.load_pluggable_device_library(_plugin_dir)\r\n\r\n~/tensorflow-metal/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py in load_library(library_location)\r\n    152 \r\n    153     for lib in kernel_libraries:\r\n--> 154       py_tf.TF_LoadLibrary(lib)\r\n    155 \r\n    156   else:\r\n\r\nNotFoundError: dlopen(/Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 6): Symbol not found: _TF_AssignUpdateVariable\r\n  Referenced from: /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib\r\n  Expected in: flat namespace\r\n\r\n```"]}, {"number": 50086, "title": "argmax for tf.sparse", "body": "**Describe the feature and the current behavior/state.**\r\nSimilar to tf.sparse.reduce_max, define an argmax operation. I think, computing an argmax over a dimension is not possible with sparse tensors otherwise. If it is, a hint would be appreciated\r\n\r\n**Will this change the current api? How?**\r\nAdd function tf.sparse.argmax\r\n\r\n**Who will benefit with this feature?**\r\nPeople using argmax / sparse tensors\r\n\r\n**Any Other info.**\r\n\r\nThanks!", "comments": ["I would also be happy to implement this, if you could give me some hints. ", "Any help from my side required? Any timeline on this?"]}, {"number": 50080, "title": "XLA AOT compile failed for certain models on s390x, `aot_compiled_test` fail to build", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.5.0-0-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen running test case `//tensorflow/python/tools:aot_compiled_test`, it will fail to build on s390x machine.\r\n\r\n**Describe the expected behavior**\r\nThe test case should build and pass\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** \r\nThe direct cause of the issue is `vector_register_num_elements` function in [target_machine_features.h](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/compiler/xla/service/cpu/target_machine_features.h#L87) returning 0 when aot compiler calls it. And this only happens for model [MatMulSmall](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/tools/aot_compiled_test.cc#L102). This issue is model-specific because if the output matrix (for matmul) has both dimensions greater than 32, then there will be no issue; the error only happens when either dimension of the output matrix is equal to or less than 32.\r\n\r\nI am not entirely sure why `vector_register_num_elements` is returning 0 in certain cases, but I do have the following observation. In this test case, it calls the `saved_model_cli` command-line interface for compiling. The full command is as follow:\r\n```\r\nbazel-out/host/bin/tensorflow/python/tools/saved_model_cli aot_compile_cpu --dir \"$(dirname bazel-out/s390x-opt/bin/tensorflow/python/tools/x_matmul_y_small/saved_model.pb)\" --output_prefix bazel-out/s390x-opt/bin/tensorflow/python/tools/aot_compiled_x_matmul_y_small --cpp_class XMatmulYSmall --variables_to_feed '' --signature_def_key serving_default --multithreading False --target_triple systemz-none-linux-gnu --tag_set serve\r\n```\r\nI noticed that if we add `--target_cpu z14` flag here, the test case will pass. This is interesting because the `target_cpu` flag was added in this [commit](https://github.com/tensorflow/tensorflow/commit/e25d9862ca5c42997112c564f1253fd001bc4a15). I believe it should be left empty unless we are cross-compiling. In other words, if it is empty, LLVM compiler should use the host cpu by default. \r\n\r\nDue to the fact that once we explicitly specify the host cpu the test will pass, I think the issue here is that LLVM compiler could not detect the host cpu in default case. I have a temporary fix that could feed the `target_cpu` flag with host cpu to LLVM when it is absent:\r\n```diff\r\ndiff --git a/tensorflow/python/BUILD b/tensorflow/python/BUILD\r\nindex 4cfc389eac6..c97b8c845aa 100644\r\n--- a/tensorflow/python/BUILD\r\n+++ b/tensorflow/python/BUILD\r\n@@ -394,6 +394,7 @@ tf_python_pybind_extension(\r\n     module_name = \"_pywrap_tfcompile\",\r\n     deps = [\r\n         \":tfcompile_headers_lib\",\r\n+        \"@llvm-project//llvm:Support\",\r\n         \"@pybind11\",\r\n         \"//third_party/python_runtime:headers\",\r\n         \"//tensorflow/python/lib/core:pybind11_lib\",\r\ndiff --git a/tensorflow/python/tfcompile_wrapper.cc b/tensorflow/python/tfcompile_wrapper.cc\r\nindex c8818309919..c24fbfbcb8d 100644\r\n--- a/tensorflow/python/tfcompile_wrapper.cc\r\n+++ b/tensorflow/python/tfcompile_wrapper.cc\r\n@@ -15,6 +15,7 @@ limitations under the License.\r\n\r\n #include <string>\r\n\r\n+#include \"llvm/Support/Host.h\"\r\n #include \"pybind11/cast.h\"\r\n #include \"pybind11/pybind11.h\"\r\n #include \"pybind11/pytypes.h\"\r\n@@ -45,7 +46,8 @@ PYBIND11_MODULE(_pywrap_tfcompile, m) {\r\n         flags.graph = std::move(graph);\r\n         flags.config = std::move(config);\r\n         flags.target_triple = std::move(target_triple);\r\n-        flags.target_cpu = std::move(target_cpu);\r\n+        flags.target_cpu = std::move(target_cpu.empty() ?\r\n+                       llvm::sys::getHostCPUName().str() : target_cpu);\r\n         flags.target_features = std::move(target_features);\r\n         flags.entry_point = std::move(entry_point);\r\n         flags.cpp_class = std::move(cpp_class);\r\n```\r\nI understand this might not be an ideal solution for the issue, so I wonder if there is another way we could feed such info to LLVM compiler? It will also be very helpful if anyone could help me identify the core issue here (why LLVM compiler cannot detect the host cpu in the default case?)\r\n\r\n**Standalone code to reproduce the issue**\r\nRun `bazel test --cache_test_results=no --build_tests_only --test_output=errors --verbose_failures -- //tensorflow/python/tools:aot_compiled_test` on s390x machine\r\n\r\n**Other info / logs** \r\nAttaching the test log here: [aot_compiled_test.log](https://github.com/tensorflow/tensorflow/files/6599110/aot_compiled_test.log)\r\nPlease also note that this test case was passing for TensorFlow 2.4.0 and 2.4.1 release\r\n\r\n", "comments": []}, {"number": 50079, "title": " TypeError: tf__call() got an unexpected keyword argument 'y'", "body": "**System information**\r\nI am using TensorFlow 2.5 with Python 3.6 I have pip-installed TF2.4 within an anaconda environment.\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n**Standalone code to reproduce the issue** \r\nimport tensorflow as tf\r\nassert float(tf.__version__[:3]) >= 2.3\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = tf.compat.v1.InteractiveSession(config=config)\r\nimport tensorflow.keras as keras\r\nimport pathlib\r\nimport numpy as np\r\nimport tensorflow_model_optimization as tfmot\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\norigin_model = tf.keras.applications.MobileNetV3Small(\r\n    input_shape=(224, 224, 3), alpha=1.0, include_top=True, weights='imagenet',\r\n    input_tensor=None, pooling=None, classes=1000,\r\n    classifier_activation='softmax'\r\n)\r\nquant_aware_model = tfmot.quantization.keras.quantize_model(origin_model)\r\nquant_aware_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.00001),  # 0.045, momentum=0.9, decay=0.98),\r\n                              loss='sparse_categorical_crossentropy',\r\n                              metrics=['accuracy'])\r\n\r\n**Any other info / logs**\r\n\r\n![image](https://user-images.githubusercontent.com/32632952/120812356-7d619e00-c57f-11eb-84fb-d11cc3389b03.png)\r\n\r\nWhen I use the API \"tfmot.quantization.keras.quantize_model\", there is a bug.I guess that some layers in mobilenetV3 do not support quantization operations, such as the lamba layer. I wonder if you have encountered it?\r\n\r\nthanks in advance.", "comments": ["@rino20 @ethkim Could you take a look?", "Hi, thanks for reporting the issue. \r\n\r\nYou are right, the current QAT is not supporting the part of layers in MobileNetV3. We are working to expand the support. \r\n\r\nI will update here when the support is ready. ", "@qiyangzhang0329 I can reproduce the issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/626c3431377c67070094121d3e6a7be9/untitled.ipynb) is a gist for reference. Thanks!", "> @qiyangzhang0329 I can reproduce the issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/626c3431377c67070094121d3e6a7be9/untitled.ipynb) is a gist for reference. Thanks!\r\n\r\n@jvishnuvardhan Thanks. But the bug has not been solved.", "@rino20 How soon can this issue be expected to be supported? It means a lot to me. Thanks.\r\n", "@rino20 , would the  vision of tf 2.6 support QAT ?", "QAT is included in TFMOT's release. I thought this is fixed by recent updates, but assign @xhark for better information. \r\n\r\n@qiyangzhang0329  could you check this is still happening in the HEAD of the TF model optimization toolkit? "]}, {"number": 50069, "title": "Error while quantizing a tensorflow bert model: ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT32 for input 0, name: input_ids ", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installation pip package\r\n- TensorFlow library 2.4\r\n\r\nI created a Bert Model for text Classification and then trying to convert to fully integer quantization where I am developing the representative_data_set as following. but on converter.convert, it gives me the following error.\r\n\r\n### 2. Code\r\nnum_calibration_steps = 100\r\n\r\ndef representative_dataset_gen():\r\n    for i in range(num_calibration_steps):\r\n        value = np.expand_dims(input_text_ids[i], axis=0).astype('float32')\r\n        yield [value]\r\n\r\n\r\nand converts the model as \r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(themodel)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.allow_custom_ops = True\r\nprint('Converting')\r\n\r\ntflite_model = converter.convert()\r\n\r\n------------------------------------------------------------------------------------\r\n\r\n\r\n\r\n\r\n### 2. Error : \r\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT32 for input 0, name: input_ids \r\n", "comments": ["I had similar issue while converting the BERT model to TFLite.\r\nCheck this issue if it helps [Issue converting full scale BERT Model to TFLite Model.  #50026](https://github.com/tensorflow/tensorflow/issues/50026)\r\nIt seems some of the ops are not yet fully supported by the TFLite Converter.", "input_ids seems int32 not float32.\r\nWould you please try to change the data type of representative data?\r\n(e.g. .astype('float32') -> .astype('int32'))", "@CyberCrack Yes you are right because I have been stuck on this issue for quite some time now since adding on to what @Xhark said, I did also change it to astype('int32') that I had tried already and it gave me this error: \r\n\r\nRuntimeError: Failed to initialize op resolver for calibration:\r\nThere are unresolved custom ops: []Encountered unresolved custom op: Erf.Node number 95 (Erf) failed to prepare.\r\n", "@asimsultan Glad I could help.\r\n\r\nIf you are after reducing the size of the model you can try using the [ALBERT](https://tfhub.dev/tensorflow/albert_en_base/3) or [TF Lite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification). Which can help you reduce size drastically.\r\nAlthough it is possible that you might now achieve good enough metrics for your model and if this happens you can use the smaller versions of BERT, by reducing the number of Transformer Layers (this is what I ended up doing to solve my problem). TFHub provides many options you can check this [TFHub link](https://tfhub.dev/s?q=small_bert). I suggest starting with 4 Transformer Layers and choose the maximum hidden layers i.e. 768 [Direct Link](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/2) and move your way up as per the difficulty of your problem you are trying to solve.\r\n", "So here is the use case. \r\nI did perform Dynamic Range Quantization and it did give me smaller model from 1.3 GB (Actual Bert model) to 122 mbs(quantized bert model) but the problem arises at the inference while prediction And for that I read that someone referred using Full integer floatfallback quantization so I just tried that where the only thing i had to pass extra was representative_data to calibrate the variable tensor inputs, and over there i have this data coming like .astype(float32) but thats not taken by the already existing model and if i enter astype('int32') etc, it gives me custom ops not supported.\r\n\r\n\r\nI think I will have to do some stuff like from scratch of the model development and for that currently I will have to look for it. \r\n\r\nBut I just wanted to ask one simple question from every one.\r\n\r\nFor Bert Model, Can we get a small version of it that should be small in size and that should be quick in inference without any errors?", "Hi @asimsultan ! Could you please share the BERT model you are trying to convert in TFLite model? I converted a BERT model to Lite for testing . Attaching [gist](https://colab.sandbox.google.com/gist/mohantym/971786f2350bdf11e1dd1b50d8eebf26/git_50069.ipynb) for reference . Thank you!"]}, {"number": 50025, "title": "Comparison of conversion and int8 conversion for TFLite ", "body": "Hi, I'm working on converting trained tensorflow model to uint8 and int8. But I found that the results between the two models are different, the followings are settings of conversion:\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 1.15.0\r\n\r\n### 2. Code\r\n[int8 conversion]\r\n```\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT] \r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8 \r\n    converter.inference_output_type = tf.int8 \r\n    converter.representative_dataset = representative_data_gen\r\n    tflite_int8_model = converter.convert()\r\n```\r\n\r\n[uint8 conversion]\r\n```\r\n    converter.target_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS, tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.inference_type = tf.uint8\r\n\r\n    input_arrays = converter.get_input_arrays()\r\n    converter.quantized_input_stats = {input_arrays[0]: (127, 127)}    \r\n    converter.default_ranges_stats = (-128, 127) \r\n    tflite_uint8_model = converter.convert()\r\n```\r\n\r\n\r\n### 3. Failure after conversion\r\nThe results between the int8 and uint8 models are different\r\n(int 8 model with representative dataset)\r\n![int8_with_representative_dataset](https://user-images.githubusercontent.com/8951991/120624329-2bdbe500-c493-11eb-94a8-4f24ffccad42.png)\r\n\r\n(uint8 model with input_stats and ranges_stats)\r\n![uint8_with_range](https://user-images.githubusercontent.com/8951991/120624431-47df8680-c493-11eb-9cbe-cdef9949518e.png)\r\n\r\n ### 5. (optional) Any other info\r\nI also tried to applying `converter.optimizations = [tf.lite.Optimize.DEFAULT], converter.representative_dataset ` and `converter.quantized_input_stats, converter.default_ranges_stats` simultaneously, but the generated model contains two quantize layers, which generates wrong results.\r\n![wrong_conversion](https://user-images.githubusercontent.com/8951991/120625297-24690b80-c494-11eb-86b0-8b3bdc16fa97.png)\r\n\r\nIs it possible to generate uint8 models with `converter.representative_dataset ` with all layers are quantized as uint8 instead of int8?\r\n\r\nBest regards", "comments": ["@daverim Hi david, you may have know more details about this. Would you please take a look?"]}, {"number": 50002, "title": "Kernel dies on interpreter.invoke()", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version 2.5\r\n\r\n\r\n**So here is the attached code in which I am loading a tflite model and I want to perform inference on that, for 1 input it works fine but when I changed the input_tensor to 1000, it kills the kernel. Any solution for this? Thanks!**\r\n\r\n```\r\nfrom transformers import MobileBertModel, MobileBertConfig, MobileBertTokenizer, MobileBertForSequenceClassification, MobileBertTokenizerFast, TFMobileBertModel\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport pandas as pd\r\npath = 'mobilequantmodel.tflite'\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=path)\r\ninterpreter.allocate_tensors()\r\n\r\n# model_name = 'bert-base-uncased'\r\nmodel_name = \"google/mobilebert-uncased\"\r\nprint(model_name)\r\n\r\n# Max length of tokens\r\nmax_length = 100\r\n\r\nconfig = MobileBertConfig.from_pretrained(model_name)\r\nconfig.output_hidden_states = False\r\n\r\n# Load BERT tokenizer\r\ntokenizer = MobileBertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\r\ndf = pd.read_csv('file_sentences.csv')\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\n\r\nddf = df[(df['useful'] == 1) & (df['language']=='en')]\r\n\r\nx_test = tokenizer(\r\n  text= ddf['paragraph'].to_list(),\r\n  add_special_tokens=True,\r\n  max_length=max_length,\r\n  truncation=True,\r\n  return_tensors='tf',\r\n  padding='max_length', \r\n  return_token_type_ids = False,\r\n  return_attention_mask = False,\r\n  verbose = True)\r\n\r\ninput_text = x_test['input_ids']\r\n\r\ninterpreter.resize_tensor_input(input_details[0]['index'], [1000, 100])\r\ninterpreter.allocate_tensors()\r\n\r\nipp = input_text[:1000]\r\ninterpreter.get_input_details()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], ipp)\r\n\r\ninterpreter.invoke()\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n", "comments": ["It is really hard to reproduce your issue at our end. Please consider providing a minimal, reproducible step as a gist or provide the model file if possible.", "Sure, let me create a reproducible complete code snippet with the model.\r\n\r\n", "So I am going to explain my model working from scratch along with that, I am going to share my tflite model files so that would be easy for you to interpret the error.\r\n\r\nI have a dataset of sentiment analysis and I am using BertModel for its classification. I developed the bert model and I named it as SentimentBertModel.h5 and then using tflite, I changed the model to a tflite model to make it small in size and to do quick inference on that. The model gets converted and I named it as TheFinalQuantizedModel.tflite. Now when I perform the inference on that, on interpreter.invoke(), either the kernel dies or I get the following error which I am going to mentione now.\r\n\r\n\r\nCode for the model development, conversion to tflite(quantization) and inference are all present in the given link along with the dataset used and the tflite model developed too. \r\n\r\nLink to all the files: https://drive.google.com/file/d/1cqq0OKVr3rKlH6yt-CLVskcnms7z6bte/view?usp=sharing\r\n\r\nThe error I get is this one.\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-11-df579c7368bc> in <module>\r\n----> 1 interpreter.invoke()\r\n      2 output_data = interpreter.get_tensor(output_details[0]['index'])\r\n      3 print(output_data)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n    538     \"\"\"\r\n    539     self._ensure_safe()\r\n--> 540     self._interpreter.Invoke()\r\n    541 \r\n    542   def reset_all_variables(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: Erf.Node number 97 (Erf) failed to prepare.\r\n\r\n\r\nNOTE: I am new to Quantization and if there is something that I am not doing right, kindly let me know from the scratch so I could just fix it soon.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"]}, {"number": 49993, "title": "SparseTensor indices stored as tf.int32 instead of tf.int64", "body": "`tf.SparseTensor ` currently only accepts indices as `tf.int64`. Which mean memory of indices > values (`tf.float32`) in general. Would it make sense to have an option to store indices as `tf.int32`? This would be useful for storing large sparse matrices where the index values are < 2^32 as it would free up some RAM", "comments": []}, {"number": 49972, "title": "Running Hello World example on STM32F746G-DISCO results in HardFault", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Source\r\n- Tensorflow version (commit SHA if source): Release 2.6.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F746G-DISCO\r\n- MBed version: 1.10.5\r\n\r\n**Describe the problem**\r\nAfter compiling and copying the mbed.bin to the STM32F746G-DISCO, the program runs for a short bit and then experiences a Hard Fault.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nInstructions followed:\r\n[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world](url)\r\n\r\ncp ./BUILD/DISCO_F746NG/GCC_ARM/mbed.bin /media/jomodev/DIS_F746NG/\r\n\r\nError seen from the STM32F7 DISCO Term output:\r\n```\r\nScript started on 2021-06-01 20:31:25-0700\r\n$ screen /dev/ttyACM0 9600\r\nx_value: 1.4361557*2^-4, y_value: 1.7621826*2^-4\r\nx_value: 1.4361557*2^-3, y_value: 1.8977352*2^-3\r\nx_value: 1.0771168*2^-2, y_value: 1.389413*2^-2\r\nx_value: 1.4361557*2^-2, y_value: 1.5588537*2^-2\r\nx_value: 1.7951952*2^-2, y_value: 1.7960706*2^-2\r\nx_value: 1.0771168*2^-1, y_value: 1.965511*2^-2\r\nx_value: 1.2566366*2^-1, y_value: 1.1183078*2^-1\r\nx_value: 1.4361557*2^-1, y_value: 1.3724688*2^-1\r\nx_value: 1.6156756*2^-1, y_value: 1.4910772*2^-1\r\nx_value: 1.7951952*2^-1, y_value: 1.6605182*2^-1\r\nx_value: 1.9747147*2^-1, y_value: 1.7791265*2^-1\r\nx_value: 1.0771168*2^0, y_value: 1.7791265*2^-1\r\nx_value: 1.1668766*2^0, y_value: 1.8977352*2^-1\r\nx_value: 1.2566366*2^0, y_value: 1.9316229*2^-1\r\nx_value: 1.3463962*2^0, y_value: 1.965511*2^-1\r\nx_value: 1.4361557*2^0, y_value: 1.0166438*2^0\r\n[46C\r\n++ MbedOS Fault Handler ++\r\n[26C\r\n[26CFaultType: HardFault\r\n[46C\r\n[46CContext:\r\n[54CR0: 72\r\n[60CR1: FFFE\r\n[68CR2: FFDB4437\r\n[80CR3: 72\r\n[86CR4: FFFFFFEF\r\n[98CR5: A\r\n[103CR6: A\r\n[108CR7: FFDB4437\r\n[8CR8: 0\r\n[13CR9: 72\r\n[19CR10: 8\r\n[25CR11: 68\r\n[32CR12: 38F\r\n[40CSP   : 2004FF98\r\n[55CLR   : 800051B\r\n[69CPC   : 8000488\r\n[83CxPSR : 810B0000\r\n[98CPSP  : 0\r\n[106CMSP  : 2004FF30\r\n[9CCPUID: 410FC271\r\n[24CHFSR : 40000000\r\n[39CMMFSR: 0\r\n[47CBFSR : 4\r\n[55CUFSR : 0\r\n[63CDFSR : 9\r\n[71CAFSR : 0\r\n[79CMode : Thread\r\n[92CPriv : Privileged\r\n[109CStack: MSP\r\n[7C-- MbedOS Fault Handler --\r\n[33C\r\n[33C\r\n[33C\r\n[33C++ MbedOS Error Info ++\r\n[56CError Status: 0x80FF013D Code: 317 Module: 255\r\n[102CError Message: Fault exception\r\n[20CLocation: 0x8000488\r\n[39CError Value: 0x20002DEC\r\n[62CFor more info, visit: https://mbed.com/s/error?error=0x80FF013D&tgt=\r\n\r\n```\r\n\r\n", "comments": []}, {"number": 49961, "title": "tflite esrgan super image resolution limited 50x50", "body": "how to train super image resolution, that accept any input image", "comments": ["@lintian06 could you take a look?"]}, {"number": 49956, "title": "Wrong hostnames in SlurmClusterResolver from expand_range_expression", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.8.10\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: Tesla V100 and 32GB\r\n\r\n\r\n**Describe the current behavior**\r\nWe have a Slurm cluster and I wanted to train with a multi-node setup. For the cluster_resolver I used tf.distribute.cluster_resolver.SlurmClusterResolver (without any configuration) and let this class fetch the information from the Slurm environment variables. Now \"expand_hostlist(hostlist)\" tries to get a list of hostnames, but depending on the naming convention of the hosts this leads to the following problem:\r\nIf the input names have a leading '0' like 'n[009-011]' in the string, it will be converted to ['n9', 'n10', 'n11'].\r\n\r\nThe ultimate conversion problem occurs in expand_range_expression(range_exp), where the range string is converted to an int():\r\n```\r\nfor i in range(int(sub_range[0]), int(sub_range[1]) + 1):\r\n        yield i\r\n```\r\nThis will remove the leading 0\u2019s.\r\n\r\n**Describe the expected behavior**\r\nThe correct conversion would be [\u2018n009\u2019, \u2018n010\u2019, \u2018n011\u2019]\r\n\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** \r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution (if contributing):\r\n```\r\nstring_length = len(sub_range[0])\r\nfor i in range(int(sub_range[0]), int(sub_range[1]) + 1):\r\n        yield str(i).zfill(string_length)\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nI remove all the slurm stuff, because that is not necessary to show the problem. \r\n[colab example](https://colab.research.google.com/drive/1cH6UKVs-fS3QlRWONjItF432f_CO4uM-?usp=sharing)\r\n\r\n", "comments": ["@ymodak ,\r\nI was able to reproduce the issue in TF v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/3b947d144b836f7b8aec88110bc5172e/49956.ipynb).", "Hi Ivo, thanks for reporting the issue. Would you like to send a PR since you already have a fix?", "Hi @crccw, sure I can prepare a PR. I will also include such a case in the unit test.", "Hi,\r\nany update about this issue?\r\nThe problem still happens with tensorflow 2.7.0. Even on the master branch, the integer conversion is still there: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/distribute/cluster_resolver/slurm_cluster_resolver.py#L63-L64.\r\nThanks"]}, {"number": 49947, "title": "Try to use TensorflowLite (API C++) NNAPI acceleration, meet errors.", "body": "**BUILD .SO System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 9 (android 9, Snapdragon855)\r\n- TensorFlow installed from (source or binary):   source  (branch in * tf25   15d5b93 [origin/r2.5] Merge pull request #49270 from angerson/r2.5)\r\n- TensorFlow version: 2.5\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?: conda \r\n- Bazel version (if compiling from source):  3.7.2\r\n- GCC/Compiler version (if compiling from source):  7.3.0\r\n- CUDA/cuDNN version: NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0  CUDANN Version: 7.3.1\r\n- GPU model and memory:  Tesla K40m 11441MiB\r\n- cmake version: 3.20.0\r\n- NDK version : 21b (NDK API level us 21)\r\n- SDK API level  use version 28 \r\n**BUILD .apk System information**\r\n- Windows 10 10.0\r\n- Android build tools use versions: 28.0.3\r\n- Android Studio 4.2.1\r\n- compile sdk version: 28\r\n- NDK version\uff1a 21.1.6352462\r\n\r\n \r\n**Describe the problem**\r\nBriefly\uff1a\r\n ubuntu build libnnapi_delegate.so ->  build .apk in windows10 Android Studio ->meet linkerror\uff1ajava.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol referenced by so.\r\nverbosely\uff1a \r\nI refer to this super_resolution [example ](https://github.com/tensorflow/examples/tree/master/lite/examples/super_resolution/android) try to run the AI model on the mobile DSP\uff0cI build nnapi delegate  .so like this \uff1a\r\n####\r\nbazel build -c opt --repository_cache=/home/wjl/dockerworkspace/tensorflow_16ubuntu/2cache --distdir=/home/wjl/dockerworkspace/tensorflow_16ubuntu/2cache --config=android_arm64  --cxxopt=\"-std=c++14\"       //tensorflow/lite/delegates/nnapi:nnapi_delegate\r\n####\r\nthen I get \uff1a\r\nbazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/nnapi/libnnapi_delegate.so (about  1.6M).\r\nI add so and header files to the super_resolution demo.\r\nNo errors or warnings were reported when compiling the apk. But at runtime, logcat reports an error: jProcess: org.tensorflow.lite.examples.superresolution, PID: 15939\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite16logging_internal13MinimalLogger3LogENS_11LogSeverityEPKcz\" referenced by \"/data/app/org.tensorflow.lite.examples.superresolution-50dhWLUyWoZrFw8sIcvg5w==/lib/arm64/libnnapi_delegate_strip.so\"...\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1016)\r\n        at java.lang.System.loadLibrary(System.java:1669)\r\n        at org.tensorflow.lite.examples.superresolution.MainActivity.<clinit>(MainActivity.java:47)\r\n        at java.lang.Class.newInstance(Native Method)  ...\r\n\r\n1. How to solve the missing symbol of so\uff1f\r\n2. I add the following code to the SR demo to call nnapi,  I have to use c++ interface and NDK .Is it correct to call nnapi like this? My apk hasn't run yet.\r\n\"\"\"\"\"\"\"add this \"\"\"\"\"\"\r\n       #include \"tensorflow/lite/delegates/nnapi/nnapi_delegate.h\" \r\n       ...\r\n      // reference link: \u200bhttps://github.com/lackhole/CuteModel/blob/master/CuteModel.cpp\r\n     nnapiOptions = tflite::StatefulNnApiDelegate::Options();  // nnapi\r\n      nnApiDelegate_ =  new tflite::StatefulNnApiDelegate(nnapiOptions);\r\n      TfLiteInterpreterOptionsAddDelegate(options_, nnApiDelegate_);\r\n\r\n", "comments": ["./readelf   -d  libnnapi_delegate.so\r\n>>\r\nDynamic section at offset 0x20a60 contains 26 entries:\r\n  Tag        Type                         Name/Value\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc++_shared.so]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libdl.so]\r\n 0x0000000000000001 (NEEDED)             Shared library: [libc.so]\r\n 0x000000000000001d (RUNPATH)            Library runpath: [$ORIGIN/../../../../_solib___Caarch64-linux-android-clang9.0.8-libcpp/:$ORIGIN/_solib___Caarch64-linux-android-clang9.0.8-libcpp/]\r\n 0x000000000000001a (FINI_ARRAY)         0x21a48\r\n 0x000000000000001c (FINI_ARRAYSZ)       16 (bytes)\r\n 0x0000000000000004 (HASH)               0x1c8\r\n 0x000000006ffffef5 (GNU_HASH)           0x6a0\r\n 0x0000000000000005 (STRTAB)             0x1af8\r\n 0x0000000000000006 (SYMTAB)             0xa78\r\n 0x000000000000000a (STRSZ)              10592 (bytes)\r\n 0x000000000000000b (SYMENT)             24 (bytes)\r\n 0x0000000000000003 (PLTGOT)             0x21c40\r\n 0x0000000000000002 (PLTRELSZ)           2520 (bytes)\r\n 0x0000000000000014 (PLTREL)             RELA\r\n 0x0000000000000017 (JMPREL)             0x4760\r\n 0x0000000000000007 (RELA)               0x45f8\r\n 0x0000000000000008 (RELASZ)             360 (bytes)\r\n 0x0000000000000009 (RELAENT)            24 (bytes)\r\n 0x000000000000001e (FLAGS)              BIND_NOW\r\n 0x000000006ffffffb (FLAGS_1)            Flags: NOW\r\n 0x000000006ffffffe (VERNEED)            0x45b8\r\n 0x000000006fffffff (VERNEEDNUM)         2\r\n 0x000000006ffffff0 (VERSYM)             0x4458\r\n 0x000000006ffffff9 (RELACOUNT)          3\r\n", "@miaowang14 could you take a look?", "libtensorflowlite_jni.so\r\nlibtensorflowlite_gpu_jni.so\r\nand\r\nlibnnapi_delegate.so \r\nlibnnapi_implementation.so\r\nlibnnapi_util.so\r\nin my project/libraries.\r\n\r\n", "The error is coming from libnnapi_delegate_strip.so:\r\n`java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol \"_ZN6tflite16logging_internal13MinimalLogger3LogENS_11LogSeverityEPKcz\" referenced by \"/data/app/org.tensorflow.lite.examples.superresolution-50dhWLUyWoZrFw8sIcvg5w==/lib/arm64/libnnapi_delegate_strip.so\"...\r\n`\r\n\r\nI wonder how is libnnapi_delegate_strip.so generated?\r\n", "I use this command first\uff1a\r\nbazel build -c opt --repository_cache=/2cache --distdir=/2cache --config=android_arm64  --cxxopt=\"-std=c++14\"  \r\n //tensorflow/lite/delegates/nnapi:nnapi_delegate \r\nthen stip by:\r\naarch64-linux-android-strip\r\nIf I do not strip this shared library, will encounter the same errors.", "I see. Do you see similar error with the unstriped version?", "I have solved the problem of dynamic libraries, just replace libtensorflowlite_gpu_jni.so with libtensorflowlite_gpu_delegate.so.\r\n Can you teach me how to combine those dynamic libraries?", "I am not sure what is going on here with libtensorflowlite_gpu_jni, @abattery do you know?"]}, {"number": 49946, "title": "Wrong gradient from complex determinant", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur, Version: 11.2.3 (20D91)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nI implemented a straightforward example which illustrates the issue:\r\nI have a 2-dimensional tensor which is mapped to a complex tensor as:\r\n\r\nx_i -> z_i = (x_i, lamb_i * x_i)\r\n\r\nwith some real values for lambda = [lambda_1, lambda_2].\r\nAfterwards I calculate the Jacobian (with respect to x) and its determinant, which is\r\nalso easy to do as it is just a 2x2 matrix.\r\nIn the end, I take |det|^2 as final output L.\r\n\r\nAnalytically, you would now get for the gradient:\r\ngrad L = [ 2 * lambda_1 * (1+lambda_2^2), 2 * lambda_2 * (1+lambda_1^2)\r\n\r\nSo if you insert lambda_test = [ 1.0 , 2.0]\r\nYou should obtain: grad L(lambda_test) = [ 10, 8]\r\n\r\nHowever, Tensorflow yields: TF-Grad = [-14, -8.8],\r\nwhich obviously is completely off. \r\n\r\nAdditionally, we also implemented a simple numerical derivative ourselves (just the basic definition of the gradient in terms of difference quotient). This calculation does yield the correct gradient (within numerical uncertainties)\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue can be reproduced in this [gist](https://colab.research.google.com/drive/1C8PQKBWS-ykraftMqVq6MWmOjcmdQbhH?usp=sharing).\r\n\r\n", "comments": ["@jvishnuvardhan ,\r\nI was able to reproduce the issue in tf v2.4,v2.5 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/bbdf96c1d35d60d54d79f978596ce6ba/49946.ipynb).", "Is there any new insight in what is going wrong?"]}, {"number": 49944, "title": "QuantizedOpsTest.testAxis fails on cascade lake CPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 8.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): GCC 10.2.0\r\n- CUDA/cuDNN version: None\r\n\r\n**Describe the current behavior**\r\n\r\nQuantizedOpsTest.testAxis fails on cascade lake systems when native optimizations are enabled\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nRun the TF test `//tensorflow/python:quantized_ops_test` through bazel\r\n\r\n**Other info / logs**\r\n```\r\nFAIL: testAxis (__main__.QuantizedOpsTest)\r\nQuantizedOpsTest.testAxis\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/quantized_ops_test.py\", line 94, in testAxis\r\n    self.assertAllEqual(quantized, expected_quantized)\r\n  File \"/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1253, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2881, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b, err_msg=\"\\n\".join(msgs))\r\n  File \"/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/SciPy-bundle/2020.11-foss-2020b/lib/python3.8/site-packages/numpy/testing/_private/utils.py\", line 930, in assert_array_equal\r\n    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\r\n  File \"/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/SciPy-bundle/2020.11-foss-2020b/lib/python3.8/site-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\nnot equal where = (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\r\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\r\n       2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1,\r\n       1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1,\r\n       1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0]), array([0, 1, 2, 3, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\r\n       1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\r\n       3, 4, 0, 1, 2, 3, 4, 0, 0, 1, 2, 3]))\r\nnot equal lhs = array([ -64,    0,   38,  102,   38,  102,   71,   64,   64, -128,  -64,\r\n          0,  102,   71,   64, -128, -128,  -64,    0,   38,   64, -128,\r\n        -64,    0,    0,   38,  102,   71,    0,   38,  102,   71,   71,\r\n         64, -128,  -64,  102,   71,   64, -128, -128,  -64,    0,   38,\r\n         71,   64, -128,  -64,  -64,    0,   38,  102,   38,  102,   71,\r\n         64], dtype=int8)\r\nnot equal rhs = array([-128,  -64,    0,   38,  -64,    0,   38,  102,   71,   64, -128,\r\n        -64,    0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,\r\n         71,   64, -128,  -64,    0,   38,  102,   71,   64, -128,  -64,\r\n          0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,   71,\r\n         64, -128,  -64,    0,   38,  102,   71,   64,  102,   71,   64,\r\n       -128], dtype=int32)\r\nMismatched elements: 56 / 120 (46.7%)\r\nMax absolute difference: 230\r\nMax relative difference: 4.36842105\r\n x: array([[[[ -64,    0,   38,  102,  102],\r\n         [  71,   64, -128,   38,  102],\r\n         [  71,   64,   64, -128,  -64],...\r\n y: array([[[[-128,  -64,    0,   38,  102],\r\n         [  71,   64, -128,  -64,    0],\r\n         [  38,  102,   71,   64, -128],...\r\n```\r\n\r\nThis seems to be related to https://github.com/tensorflow/tensorflow/issues/47179 which can also only be observed on cascade lake systems.", "comments": ["We found that this failure can be avoided by compiling with `-mno-avx512f`\r\n\r\nNote that this issue applies for compiling for Intel Skylake and Cascade Lake and likely Icelake", "@Flamefire \r\nWe see that there is an associated pr with the issue and this issue will be closed once the pr is merged.", "@Saduf2019 Which PR is that? I can't see any reference to a PR popping up here...", "> @Saduf2019 Which PR is that? I can't see any reference to a PR popping up here...\r\n\r\n[link](https://github.com/easybuilders/easybuild-easyconfigs/pull/12906), it is associated with issues raised by flamefire.", "That PR is in another repository which makes building TF 2.5 possible but where we noticed that it fails on certain architectures as reported here and hence mentions this issue. It is not associated to this issue in a way that closes it as it does not fix the issue.\r\n\r\nWe do however have found a workaround by disabling certain compiler optimizations. But of course that is not a real fix.", "+1, we found a *workaround*, but it suggests a problem elsewhere (perhaps even a compiler bug, that's unclear)", "@Saduf2019  I spent a considerable amount of time tracing this down. In conclusion: The vectorized caster function at https://github.com/tensorflow/tensorflow/blob/00f71cecff49dfa1c4f2c2210dc833fb9daa5d87/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/TypeCastingAVX512.h#L62 is fundamentally broken.\r\n\r\nFirst the EIGEN_VECTORIZE_AVX512BW define that is checked for is never defined. So this part is actually dead code.\r\nEven if it was defined (and it should for icelake/skylake) then the usage is wrong:\r\n- `_mm512_packs_epi32` alternates between the 2 arguments every 4 values, i.e. 4 ints from a, then 4 from b, then 4 from a, ...\r\n- similar for `_mm512_packs_epi16` but there it alternates every 8  values.\r\n\r\nAs a result the final value is a shuffled version of what it should be. Testing this with 64 values I get (top actual, bottom expected values):\r\n```\r\nnot equal lhs = array([   0,   38,  102,   71,  102,   71,   64, -128,   64, -128,  -64,\r\n          0,  102,   71,   64, -128,  -64,    0,   38,  102,   38,  102,\r\n         71,   64,  -64,    0,   38,  102,   38,  102,   71,   64, -128,\r\n        -64,    0,   38,   71,   64, -128,  -64, -128,  -64,    0,   38,\r\n          0,   38,  102,   71], dtype=int8)\r\nnot equal rhs = array([ 102,   71,   64, -128,  -64,    0,   38,  102,   71,   64, -128,\r\n        -64,    0,   38,  102,   71,   38,  102,   71,   64, -128,  -64,\r\n          0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,    0,\r\n         38,  102,   71,   64, -128,  -64,    0,   38,  102,   71,   64,\r\n       -128,  -64,    0,   38], dtype=int32)\r\n```\r\n\r\nThe other (actually used) code looks like it was attempted to take that into account but failed:\r\n```\r\nnot equal lhs = array([ -64,    0,   38,  102,   38,  102,   71,   64,   64, -128,  -64,\r\n          0,  102,   71,   64, -128, -128,  -64,    0,   38,   64, -128,\r\n        -64,    0,    0,   38,  102,   71,    0,   38,  102,   71,   71,\r\n         64, -128,  -64,  102,   71,   64, -128, -128,  -64,    0,   38,\r\n         71,   64, -128,  -64,  -64,    0,   38,  102,   38,  102,   71,\r\n         64], dtype=int8)\r\nnot equal rhs = array([-128,  -64,    0,   38,  -64,    0,   38,  102,   71,   64, -128,\r\n        -64,    0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,\r\n         71,   64, -128,  -64,    0,   38,  102,   71,   64, -128,  -64,\r\n          0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,   71,\r\n         64, -128,  -64,    0,   38,  102,   71,   64,  102,   71,   64,\r\n       -128], dtype=int32)\r\n```\r\n\r\nI'm wondering how this went undetected for so long. That file is quite old and the code basically unchanged. Isn't there any CI for AVX512 machines?\r\n\r\nCan you redirect this to the person in charge for the AVX512 part? There might be other issues in those parts but I see that e.g. for the Quint8 case this seems to have been fixed/workarounded with the following commit (see especially the comment) https://github.com/tensorflow/tensorflow/commit/a9911cb06b931be7207ac2938dfffe9db3313e3c#diff-01340595b74ffbb772190262a25ca85a32072c2dfd331f0501d50068cee01a39R167-R168\r\n\r\nEdit: One of the issues in the other code I found is that it got the mask backwards at https://github.com/tensorflow/tensorflow/blob/00f71cecff49dfa1c4f2c2210dc833fb9daa5d87/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/TypeCastingAVX512.h#L77\r\n\r\nReversing the order now gives 8 consecutive correct results instead of only 4. As expected the output is then basically `a[0:8]+b[0:8]+c[0:8]+d[0:8]+a[8:16]+b[8:16]+c[8:16]+d[8:16]` i.e. exactly what is done at https://github.com/tensorflow/tensorflow/blob/00f71cecff49dfa1c4f2c2210dc833fb9daa5d87/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/TypeCastingAVX512.h#L95-L96 which appends the high part of abcd to the low part of abcd, which is of course wrong but at least matches the naming"]}, {"number": 49943, "title": "No tensorflow.lite.tools.evaluation.proto files inside tf 2.5.0 after bazel build", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@anshudaur ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n", "Hello, \r\n\r\nPython version is 3.8\r\ninstalled using conda\r\nTensorflow version 2.5, built using bazel(3.7.2 and GCC 7.3.1) on ubuntu 18\r\ncudnn 8.2 and cuda 11.2\r\n\r\nDescript about the problem: \r\nI want to test the performance of my tflite model on custom test records. i am trying to generate .pb file for the annotations to test it directly using tflite model (on desktop).\r\n\r\nI am following the description mentioned in : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection\r\n\r\nand there are files missing in tensorflow built with bazel: tensorflow.lite.tools.evaluation.proto\r\n\r\nThank you \r\nAnshu\r\n\r\n", "Did you try running the binary on android or desktop? Also can you please briefly describe the steps you followed/share the stacktrace? Thanks!", "My steps for setting up bazel : \r\n1. conda install -c conda-forge bazel\r\n2. ./configure\r\n3. `bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n4. `./bazel-bin/tensorflow/tools/pip_package/build_pip_package tensorflow_pkg`\r\n5. `pip install tensorflow_pkg/tensorflow_gpu-2.5.0-cp39-cp39-manylinux2010_x86_64.whl'\r\n\r\n\r\nI am trying it on desktop , right now i am getting error while importing library \ud83d\udc4d \r\nStacktrace is : \r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-1-5a6749b88800> in <module>\r\n     23 \r\n     24 from absl import logging\r\n---> 25 from tensorflow.lite.tools.evaluation.proto import evaluation_stages_pb2\r\n\r\nModuleNotFoundError: No module named 'tensorflow.lite.tools.evaluation'\r\n\r\n----------------------------------\r\nAnd when i try to print the directories inside dir(tf.lite.tools), i get following error : \r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-8-b8e2a98a4607> in <module>\r\n----> 1 dir(tf.lite.tools)\r\n\r\nAttributeError: module 'tensorflow._api.v2.lite' has no attribute 'tools'\r\n-------------------------------\r\nI have tried both tf 2.5 and tf nightly installations, and i am still getting the same errors\r\n\r\nThank you\r\nAnshu", "Hi Sachin, could you help take a look? Thanks!", "Hey @anshudaur , the TFLite coco detection tool doesn't accept .pb files. How do you plan to run the TFLite model with that data? Also the [documentation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection) suggests using `bazel run` on desktop, so I didn't understand what BUILD failed. What command did you run exactly?", "HI @srjoglekar246 , In short, i want to evaluate my tflite model on test tf records. since using tf records is not possible, I want to run the tflite model on .pb files of the dataset \r\n\r\nI am following the instructions : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection\r\n\r\n\r\nAfter building tensorflow with bazel , \r\n1. i am not able to run tensorflow\r\n2. I am also not able to generate .pb files of dataset as shown on the \"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\" , this is required in order to directly run tflite model directly on the .pb file of the dataset\r\n\r\nHope this helps.\r\n\r\nBest,\r\nAnshu Daur\r\n", "Aah I see. I think the issue is with your installation of tensorflow, since you are not able to run tensorflow itself. The `ModuleNotFoundError` shows that there is some issue with the TF package. Can you try using a [virtual env](https://www.tensorflow.org/install/pip#2.-create-a-virtual-environment-recommended) to cleanly install again?", "Can you suggest a stable build configuration? I have tried with TF 2.5, 2.5, 2.3 (build fails).. and 2.2 (with this i get old bazel error )\r\n\r\nI am following the configurations as mentioned : https://www.tensorflow.org/install/source#linux", "You need to have TF 2.3 and above. So 2.5 should work.", "So, afterr the build was successful bazel build, but I am still getting the error as for r2.4 , bazel 3.1.0 \r\nAlso, the kernel keeps restarting for import tensorflow as tf and gives this error ->\r\n\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nRuntimeError: module compiled against API version 0xe but this version of numpy is 0xd\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nImportError: numpy.core._multiarray_umath failed to import\r\n\r\nThe numpy version is : 1.19.2  for tf 2.4\r\n\r\nThank you\r\nAnshu", "See if any of the solutions [here](https://stackoverflow.com/questions/67898228/error-runtimeerror-module-compiled-against-api-version-0xe-but-this-version-o) work for you?", "The page does not exist, but i am able to build tensorflow with bazel. \r\nSo now i am trying bazel command on my custom dataset which have parsed to coco annotation files 2017\r\nBut now the file generated using bazel is 0 bytes \r\n\r\nbazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival --   --images_folder=/data/images/   --instances_file=/../../data.json   --output_folder=/../tflite_pb_data\r\n\r\nNext, i also tried running the preprocessing script within the same environment where i build tensorflow with bazel . \r\nAnd I am still not able to run the preprocessing script because of the same issue. (ModuleNotFoundError: No module named 'tensorflow.lite.tools.evaluation')\r\n\r\nDo you have any other suggestion?\r\n\r\nThank you\r\nAnshu", "Not sure if this is an issue, but you are using a tensorflow_gpu package but not building it with the [correct `--config`](https://www.tensorflow.org/install/source#gpu_support_2)? I would suggest using some [CPU configs](https://www.tensorflow.org/install/source#linux) first, to ensure that the issue isn't with the tensorflow build. \r\n\r\nDoes your environment successfully execute `import tensorflow`?", "Actually, I am indeed using tensorflow default (cpu) package. And yes i can successfully import tensorflow. \r\n\r\nCurrent config that i am working with is : tensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0\r\n\r\n"]}, {"number": 49940, "title": "tf.keras.models.load_model does not load optimizer's weigths", "body": "**System information**\r\nTensorflow: 2.5.0\r\n\r\n**Describe current behaviour**\r\nWhen using `tf.keras.models.load_model()` on a savedmodel containing a keras model saved using `tf.keras.Model.save(include_optimizer=True)`, the optimizer's weights are NOT loaded\r\n\r\n**Describe expected behaviour**\r\nThe reconstructed model should contain the optimizer's weights to be able to resume training.\r\n\r\n**Standalone code to reproduce issue**\r\nhttps://colab.research.google.com/drive/1Y0y-pvE2QDw1Wj68XnbpLaUggXAXyzcx?usp=sharing\r\n", "comments": ["@alvaro-garcia-carrasco \r\n\r\nI saved model in hdf5 format and could be able to see the optimizer's weights loaded in the reconstructed model.Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/c270c6a1bc96365ac18ad4522e1b827b/load_savedmodel.ipynb).Thanks", "Hi @UsharaniPagadala! Thanks for the response. Saving/loading the model as hdf5 format works fine and it is a potential workaround, but one should still be able to save/load it using the savedmodel, which is the recommended format. \r\n\r\nIn the documentation of how to save/load keras models they provide a savedmodel example that does not work (it does not reconstruct the optimizer) https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format ", "It seems the reconstructed_model restored the optimizers states.\r\n\r\n<img width=\"800\" alt=\"\u622a\u5c4f2021-06-02 \u4e0b\u53484 51 02\" src=\"https://user-images.githubusercontent.com/34032031/120450162-c1f40a80-c3c2-11eb-90d8-930fbd635b30.png\">\r\n", "It keeps the parameters of the optimizer such as learning rate and decay but I think that it does not restore the weights", "@alvaro-garcia-carrasco \r\n\r\nThe SavedModel contains `assets  saved_model.pb  variables\r\n`\r\n\r\nThe model architecture, and training configuration (including the optimizer, losses, and metrics) are stored in saved_model.pb. The weights are saved in the variables/ directory.For more info refer [this](https://www.tensorflow.org/guide/keras/save_and_serialize#what_the_savedmodel_contains).Thanks \r\n\r\n", "@UsharaniPagadala \r\n\r\nThe internals of how the graph and the variables are stored is not relevant. I am trying to load a SavedModel following the instructions in the documentation https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format and the optimizer's weights are not being loaded, as shown in the standalone code I provided to reproduce the issue. ", "@alvaro-garcia-carrasco \r\n\r\n`model.save_weights` method will help you to extract weights from saved model format .Please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/d595f97fad666b3e7d6f96c6ed9e3c96/copy-of-load_savedmodel.ipynb).Thanks", "@UsharaniPagadala This issue is not related on how to save the weights of a keras model. \r\n\r\nI refer again to mi initial bug description. The problem is that tf.keras.models.load_model does not load optimizer's weigths, as shown by the standalone code I provided.", "@alvaro-garcia-carrasco By default, the `compile` argument under `load_model` is set to `True`. If you replace that with `False`, then you can see optimizer weights as you are expecting. Please see below for the change\r\n\r\nIn your code\r\n`reconstructed_model = tf.keras.models.load_model(\"my_model\",compile=True)`\r\n\r\nChange the above line to \r\n`reconstructed_model = tf.keras.models.load_model(\"my_model\",compile=False)\r\n`\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/cff2f8fe46049754b24f36ea43fb1cbb/load_savedmodel.ipynb) for reference. In the gist, I have shown that the variables before saving the model and after loading the model are same. Thanks!\r\n", "@jvishnuvardhan Thanks for the reply. The reason to restate the optimizer is to continue training. The keras model needs to be compiled after loading to continue training, which erases the optimizer weights again :-(. I feel we are getting closer though. At this step, I could hack it around and extract the optimizer's weights before compiling the model again, but it would be a hack, and it is not how the documentation says that it should work.", "@jvishnuvardhan In your gist, the optimizer weights aren't the same - `model.optimizer.weights` has a length of `5`, while `reconstructed_model.optimizer.weights` has a length of `4`. In any case, I'm not sure that the bug with retraining models is because of the optimizer weights. I have a [gist](https://colab.research.google.com/drive/1MarUl9Q-Ny6BI5e7kRQLsJoxuqxv4GEr?usp=sharing) which shows that even if I overwrite the saved model weights with the original optimizer weights, the model doesn't retrain in the same way.\r\n\r\n@alvaro-garcia-carrasco Were you able to get the training resumption to work? I've tried to hack this as you mentioned, but I wasn't able to figure out how to get this working correctly.", "@alvaro-garcia-carrasco and @saumikn Thanks. Agree that this is a bug. We will work on it. If you want to contribute, please feel free to raise a PR to update the source code. Thanks!", "Duplicate of #https://github.com/tensorflow/tensorflow/issues/44670\r\n\r\nLet's track the progress in that issue. Thanks!"]}, {"number": 49935, "title": "Pass epoch number as arg to `tf.data.Dataset.from_generator`", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**: I am trying to implement a tensorflow dataset which uses a generator underneath yielding batches following a linearly increasing batch size scheme. So, the first epoch it could yield batches of size `x`, the next epoch it would yield batches of size `x+c`. `c` is dependent on the current epoch number. I am defining my tensorflow dataset as:\r\n\r\n```\r\ndef gen(epoch_number):\r\n    yield epoch_number # Just representative. In reality this will be some function of epoch_number and data.\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, output_types=(tf.int32), args=[x])\r\n\r\n# A custom keras model derived from tf.keras.Model\r\nmodel = SomeModel()\r\n\r\nmodel.fit(dataset)\r\n```\r\n\r\nI am trying to figure out what to pass as `x` in the above code so that the generator has information about the current epoch number when the generator is called.\r\nAs far as I know, there is no instance attribute of `tf.keras.Model` which could be used as `x` so that it's evaluated at runtime and passed to the generator.\r\n\r\nIs it possible to do so?\r\nI'd like to avoid overriding the `fit` or `train_step` method of the model.\r\n\r\n**Will this change the current api? How?**:  I don't think so.\r\n\r\n**Who will benefit with this feature?**: Batch yielding from dataset can use training stages and depend on them, for example number of epochs passed in the above example.\r\n\r\n", "comments": []}, {"number": 49931, "title": "Model converted to TFlite performs slow during inference", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorflow saved model converted to TFlite model gives poor performance during inference. The inference time is same as (or greater than in some cases) tensorflow model\r\n\r\n**Describe the expected behavior**\r\nInference time should be low according to the documentation\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing): Yes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/16IAyeNAIdfgRrS1CLhOqBrAc0bvFNt5A?usp=sharing", "comments": ["@rohanshingade ,\r\n\r\nI do not have access to the link you have provided. Could you please provide the required permissions to view the files.", "@tilakrayal updated.\r\nhttps://colab.research.google.com/drive/16IAyeNAIdfgRrS1CLhOqBrAc0bvFNt5A?usp=sharing", "Hi, it seems you're benchmarking on workstations, TFLite is not well optimized for x86 comparing to Tensorflow.", "> Hi, it seems you're benchmarking on workstations, TFLite is not well optimized for x86 comparing to Tensorflow.\r\n\r\nWe have done optimizations for x86, but they are not likely enabled (probably due to the OS+compiler used when building the tflite distribution). I guess this is why you see slow inference in the colab environment.\r\n\r\nCould you build your own tflite distribution on your workstation and benchmark the model performance? You could use either bazel or cmake ([guide](https://www.tensorflow.org/lite/guide/build_cmake)) to achieve this.\r\n\r\n* when using bazel, pls explicitly add \"-c opt --define=tflite_with_ruy=true --define=tflite_with_xnnpack=true\"\r\n* when using cmake, pls turn on TFLITE_ENABLE_RUY option as described [here](https://www.tensorflow.org/lite/guide/build_cmake#available_options_to_build_tensorflow_lite)", "@multiverse-tf i tried what you suggested but there was no improvement in performance\r\nInference time for\r\nOut of the box model: 59 seconds\r\nTF model convert to Tflite model: 21 mins\r\n\r\nCPU model name\t: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz", "Was able to replicate the issue with TF 2.6.0-dev20210606,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/12dd5455a7ed54d88f26dffac540d649/untitled233.ipynb) ..Thanks !"]}]