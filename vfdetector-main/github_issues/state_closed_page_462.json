[{"number": 39957, "title": "TF Lite wheel is not supported on platform including ARMv7 and python 3.6.5", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Hardware : Freescale i.MX6 Quad/DualLite\r\n- Processor: ARMv7 Processor rev 10 (v71)\r\n- OS Platform and Distribution: Yocto built Linux distribution (kernel 4.9.4+)\r\n- TensorFlow Lite interpreter installed: wheel https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp36-cp36m-linux_armv7l.whl (https://www.tensorflow.org/lite/guide/python#learn_more) \r\n- Installed python version: 3.6.5\r\n- Installed pip3 version: 9.0.3\r\n\r\nI downloaded the wheel described above from Tensorflow's website and followed the guidelines doing so because the processor is of the ARMv7 type and because the installed python version is 3.6.5. \r\nI transferred the wheel from my local computer to the hardware over SSH. Now running `pip3 install tflite_runtime-2.1.0.post1-cp36-cp36m-linux_armv71.whl` gives me the following error message: `tflite_runtime-2.1.0.post1-cp36-cp36m-linux_armv71.wh is not a supported wheel on this platform`.\r\n\r\nWhy is that? As far as I know, the hardware and software answers to all requirements set out to be able to install this particular wheel.\r\n\r\n\r\n", "comments": ["I also tried the same exact thing but with python 3.7.4 (and pip 19.3.1) and the corresponding ARMv7 wheel and had the same exact outcome.", "I also updated pip to the latest current version (20.1.1) and the issue was not resolved", "What's the result of the following commnad?\r\n```\r\npython3 -c \"import wheel.pep425tags as w; print('\\n'.join(['-'.join(p) for p in w.get_supported()]))\"\r\n```", "`TypeError: get_supported() missing 1 positional argument: 'archive_root`", "wheel version is 0.34.2", "setuptools is version 47.1.1", "My wheel.get_supported() doesn't require any parameter. Could you check yours? You can find the library location with \"pip3 show wheel\"", "Which version are you using? \r\nFor me it does : \r\n![image](https://user-images.githubusercontent.com/29673343/83499350-8841a880-a4bd-11ea-8a73-e98c7ae2ecf5.png)\r\nI installed wheel from https://pypi.org/project/wheel/#files : (https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl)", "Then why don't you try the following command?\r\n```\r\npython3 -c \"import wheel.pep425tags as w; print('\\n'.join(['-'.join(p) for p in w.get_supported('')]))\"\r\n```", "You are right:\r\nOutput : \r\n![image](https://user-images.githubusercontent.com/29673343/83503791-eb363e00-a4c3-11ea-8d4d-fd08c65e36d9.png)\r\n\r\nit must be noted that for this screenshot, I was using python 3.7.4 (not 3.6.5) and so I tried installing the cp37 wheel, which failed", "Another question: I had a similar issue with numpy and fixed it by installing from source (.zip from PyPi's latest version) so I was wondering if there is a similar strategy I could use for tf lite interpreter only (so not the complete tf), in case installing the wheel fails", "You can use tflite literpreter with https://www.tensorflow.org/lite/guide/python which install tflite_runtime package.\r\nI think you're using multiple Python versions. Two things worths to try.\r\n1. Please don't use root account for your development. You'd better have a dev account.\r\n2. In your dev account, try to use Python virtual environment to use a specific Python version. \r\nhttps://www.tensorflow.org/install/pip#2.-create-a-virtual-environment-recommended", "I agree on the root account, though I have no other option. Except from it being bad practice, this should not be the reason that installing the wheel fails right?\r\nPlease refer to the original question. Trying to install the wheel from the first link in your comment fails:\r\n![image](https://user-images.githubusercontent.com/29673343/83517679-90a7dc80-a4d9-11ea-8a72-293b9d8cfb74.png)\r\n", "Could it have anything to do with the `Tag_FP_arch` in the `tflite_runtime/_interpreter_wrapper.so` of the wheel while the CPU presents as `model name      : ARMv7 Processor rev 10 (v7l)` and `Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpd32`?\r\nSee the following screenshot when running the command `readelf -a _interpreter_wrapper.so | grep FP`:\r\n![image](https://user-images.githubusercontent.com/29673343/83530234-bb9b2c00-a4eb-11ea-85f1-3c3f39def604.png)\r\n", "I also tried other TF Lite interpreter wheels, which I found in this interesting repo: https://github.com/PINTO0309/TensorflowLite-bin, to no avail", "If using a cross-compiled installer does not solve the problem, you may have success with a native build on the Yocto environment. I've only used Yocto a couple of times to run Yocto on the RaspberryPi, so I don't know much about it.\r\n\r\n**https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/make**\r\n\r\n**https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile**", "@FlorentijnD could you share your ABI of target platform?\r\nMaybe this will help.\r\n```\r\nreadelf -A /bin/ls\r\n```", "It could be better to have ELF header as well.\r\n\r\n```\r\nreadelf -Ah /bin/ls\r\n```", "Thank you for your response @PINTO0309 . Making a Yocto recipe and deploying it that way is the end goal indeed! \r\n@terryheo This is the output of `readelf -Ah /bin/ls`: \r\n![image](https://user-images.githubusercontent.com/29673343/83609972-3d886500-a57f-11ea-9866-dfbef75dad6d.png)\r\nThank you as well for looking into this\r\n", "As you mentioned earlier, the VFP version looks suspicious. Current TFLite PIP wheel is built with VFPv4 while your target device uses VFPv3.\r\nActually, I'm developing a new PIP build method with Bazel. Could you try it?\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package#alternative-build-with-bazel-experimental\r\n\r\nThe following command builds new tflite wheel for VFPv3 under x86_64 workstation.\r\n```\r\nCI_DOCKER_EXTRA_PARAMS=\"-e CUSTOM_BAZEL_FLAGS=--copt=-mfpu=neon-vfpv3 -e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.6\" \\\r\ntensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf\r\n```\r\n\r\nYou can find a generated ARM wheel under tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.2.0-py3-none-linux_armv7l.whl", "I will give it a try! Will get back to you asap. Do you know of TF Lite wheels that were built in the past that support VFPv3?", "I don't think so. The option has been used for long time.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/targets/rpi_makefile.inc#L11", "Building the wheel seemed to work without issues. Installing the wheel didn't cause any trouble too. However, when importing the `tflite_runtime`, something goes wrong. See screenshot for the error.\r\n![image](https://user-images.githubusercontent.com/29673343/83639462-05971700-a5ab-11ea-91cf-b6a312d237e2.png)\r\n", "If this problem persists or is not easy to overcome, would you know if the CPP TF Lite API would run into similar issues because of the mismatch of VFP versions?", "Also, I was having the same issue with numpy, and was able to install numpy from source (https://files.pythonhosted.org/packages/2d/f3/795e50e3ea2dc7bc9d1a2eeea9997d5dce63b801e08dfc37c2efce341977/numpy-1.18.4.zip from https://pypi.org/project/numpy/#files). Would there be a similar strategy possible for tf lite runtime? (for example by removing `mfpu=neon-vfpv4`, or replacing it with `neon-vfpv3` in the `rpi_makefile.inc`) If so, how does one go about doing so?", "Yes, there might be a binary compatibility issue. Let's try it out. (I've searched flags for iMX6)\r\nHere are some commands to try. Could you copy the output file (benchmark_model) to your target and check if it runs well or not?\r\n\r\n```\r\n$ bazel build --config=elinux_armhf tensorflow/lite/tools/benchmark:benchmark_model\r\n$ bazel build --config=elinux_armhf --copt=-march=armv7-a --copt=-mfpu=vfpv3 tensorflow/lite/tools/benchmark:benchmark_model\r\n$ bazel build --config=elinux_armhf --copt=-march=armv7-a --copt=-mfpu=vfpv3 --copt=-mfloat-abi=softfp tensorflow/lite/tools/benchmark:benchmark_model\r\n$ bazel build --config=elinux_armhf --copt=-march=armv7-a --copt=-mcpu=cortex-a9 --copt=-mtune=cortex-a9 --copt=-mfpu=vfpv3 --copt=-mfloat-abi=softfp tensorflow/lite/tools/benchmark:benchmark_model\r\n```\r\nOr if you have CFLAGS for your Yocto project, you can also try it with \"--copt\" option and share the result.", "After running `bazel build --config=elinux_armhf tensorflow/lite/tools/benchmark:benchmark_model` successfully I don't see any binary in the directory where I understand it should be in. ![image](https://user-images.githubusercontent.com/29673343/83752772-5bca8f80-a669-11ea-9f7a-9e31e4beac1c.png) What am I missing?\r\n", "![image](https://user-images.githubusercontent.com/29673343/83753129-02af2b80-a66a-11ea-8802-e3fc4f704b19.png)\r\nThis is the output for the binary generated by `bazel build --config=elinux_armhf tensorflow/lite/tools/benchmark:benchmark_model`", "Can you share which glibc version do you available with the target?\r\n\r\n```\r\nls -l /lib/arm-linux-gnueabihf/libc.so.6\r\n```\r\n\r\nThe elinux_armhf is using this toolchain. https://github.com/tensorflow/tensorflow/tree/master/third_party/toolchains/embedded/arm-linux\r\nI'm wondering if you can update your target glibc version.", "![image](https://user-images.githubusercontent.com/29673343/83757835-61c46e80-a671-11ea-988b-d26d5cfd713e.png)\r\n", "https://bin.entware.net/armv7sf-k3.2/ I found libc_2.27 here. I could try that? though I'm a little hesitant because I'm not sure if that would break anything wrt my hardware", "Where did you get Yocto Linux for your i.MX6 Quad/DualLite? Doesn't NXP provide updated Linux release for your device?", "By using that specific Yocto Linux, we managed to install the most lightweight option possible. To be fair, I was not involved in creating this particular Yocto image.", "I think you might be able to get updated Yocto release. In the meantime, I'll check if it's feasible to support libc-2.22.\r\nBTW, if you can share where did you get your Yocto image, it'll be helpful for us to understand user's development environment. ", "I asked around. Your question is a little bit difficult to answer. `The hardware we're using is part of a bigger hardware component which acts as a custom board. That custom board is not really supported by NXP, and the current image is created using customized yocto layers, there are bits and pieces which are based on other similar boards and reused here.` Hope this helps in some way", "The filename in the original report is tagged `linux_armv71`. That's the number 1, not the letter L as it should be. If that really is the filename, then that could explain the error.", "Hi @FlorentijnD ! \r\nWe are checking to see whether you still need help in this issue . Have you tried with root access , latest pip  and Tensorflow version after installing the dependencies ? Attaching relevant threads for reference.[1](https://www.tensorflow.org/lite/guide/build_cmake_arm), [2 ](https://github.com/PINTO0309/Tensorflow-bin)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @mohantym. Thanks for following up on this. I don't neet help anymore with this issue as I don't have access to the hardware that originally caused the issue to arise. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39957\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39957\">No</a>\n"]}, {"number": 39956, "title": "Tensorflow GPU on cpu machine/container", "body": "Try to fix: https://github.com/tensorflow/tensorflow/issues/37689\r\n\r\nPlease note that this a blackbox patch cause I am still waiting for a sustainable solution for sparse code contribution at https://github.com/tensorflow/build/issues/5\r\n\r\n", "comments": ["/cc @gbaned Can you triage this?", "@chsigg What do you think?", "@gbaned If @chsigg is busy in these days let me know if you can re-route the review cause I don't know if I will have a free slot in the next days.", "@chsigg thanks"]}, {"number": 39955, "title": "AttributeError: module 'tensorflow.python.keras.api._v2.keras.preprocessing.text' has no attribute 'text_dataset_from_directory'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using example code from tensorflow\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: On computer\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version:3.8.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:Not installed\r\n- GPU model and memory:No GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nGIT Version:\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\nUnable Unable to load the function text_dataset_from_directory. I have installed tensorflow 2.2 and keras. I am not sure if any api is called from the installed package.\r\nI installed tf - nightly as well, but still unable to resolve the issue. I am not sure if am missing anything. Please help me on this.\r\n\r\n**Describe the expected behavior**\r\nAll functions has to work.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![image](https://user-images.githubusercontent.com/7239271/83170984-00614480-a133-11ea-964e-021faa011e9d.png)\r\n", "comments": ["@Harish2104 \r\n\r\nCan you please go through the [link](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory) and see if it helps you.Request you to share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39955\">No</a>\n", "Hi There,\r\n\r\nI also meet this problem the same as @Harish2104.  And my Python version is 3.7,7, Tensorfow is 2.2.0\r\n\r\n**AttributeError: module 'tensorflow.keras.preprocessing' has no attribute 'text_dataset_from_directory'**\r\n\r\nPlease give help and give advice if you know how to resolve it.\r\nMany thanks.\r\n\r\n![image](https://user-images.githubusercontent.com/329152/85943225-46433f80-b961-11ea-9b60-6eef8ca5e529.png)\r\n![image](https://user-images.githubusercontent.com/329152/85943253-8a364480-b961-11ea-81ab-5d165c1ee0a1.png)\r\n\r\n", "My problem is also not resolved yet. I am also not able get these functions", "Facing exactly same problem", "@Harish2104 @RutujaWanjari @mujiatong This api is only available with tf-nightly. Please install tf-nightly before using this api. Read the doc [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory)", "> @Harish2104 @RutujaWanjari @mujiatong This api is only available with tf-nightly. Please install tf-nightly before using this api. Read the doc [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory)\r\n\r\nYou tell me only available with tf-nightly?\r\n\r\n![image](https://user-images.githubusercontent.com/4510984/98586322-84833f00-2303-11eb-8c21-afe3e24fe1df.png)\r\n\r\n"]}, {"number": 39954, "title": "Convert keras model to quantized tflite lost precision", "body": "I'm trying to convert my keras model into tflite quantized model so that I can run my model on coral TPU, but the output of my keras model and tflite model are significantly different.\r\n\r\nThe red points are quantized tflite model output, and blue points are original keras model output.\r\n\r\n[img is here](https://i.stack.imgur.com/AdFyR.png)\r\n\r\nHere is my code to convert keras model to quantized tflite model :\r\n\r\n```\r\nquant = True\r\ngc.collect()\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pathlib\r\nprint(tf.__version__)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nif quant:\r\n    print(\"Converting quant....\")\r\n    sample_size = 200\r\n    rdm_idx = np.random.choice(range(len(X_test)),sample_size)\r\n    rep_data = tf.cast(X_train[rdm_idx], tf.float32) / 255.0\r\n    dataset = tf.data.Dataset.from_tensor_slices(rep_data).batch(1)\r\n\r\n    def representative_data_gen():\r\n        for input_value in dataset.take(sample_size):\r\n            yield [input_value]\r\n\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    converter.representative_dataset = representative_data_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_model_quant = converter.convert()\r\n    open(\"MaskedLandMarkDetction_MobileNetV2_quant_fromKeras_v5.tflite\", \"wb\").write(tflite_model_quant)\r\n\r\n    print(\"Write quantization tflite done.\")\r\nelse:\r\n    print(\"Converting normal....\")\r\n    tflite_model = converter.convert()\r\n    open(\"MaskedLandMarkDetction_MobileNetV2_fromKeras.tflite\", \"wb\").write(tflite_model)\r\n    print(\"Write tflite done.\") \r\n```\r\n\r\n`X_train `is my training data, and I will scale input images value from 0 to 1 by divided `255.`, so I do the same in `representative_data_gen `functions.\r\n\r\nAny assistance you can provide would be greatly appreciated.", "comments": ["I use tensorflow-gpu==2.2.0 BTW.\r\n\r\nI already tried increase my representative_data_gen to about 20000 images in my training data, but results are still the same.", "In order to expedite the trouble-shooting process, could you please provide the keras model file you are using in the code. Thanks!", "> In order to expedite the trouble-shooting process, could you please provide the keras model file you are using in the code. Thanks!\r\n\r\nSure, you can download my weights file from my [google drive](https://drive.google.com/file/d/1_9Nl5dT17HLap0H_kNZTGqUAYh3XMsnQ/view?usp=sharing), and the following code is my model structure :\r\n```\r\n\r\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\r\nfrom tensorflow.keras.layers import *\r\nfrom tensorflow.keras.models import Model\r\n\r\ninput_shape = (96,96,3)\r\nmodel = MobileNetV2(input_shape = input_shape, weights=\"imagenet\",alpha=1.0, include_top=False)\r\ninp = model.input\r\nout = model.output\r\nx = GlobalAveragePooling2D()(out)\r\nx = Dropout(0.2)(x)\r\nmasked_output = Dense(64, activation = \"relu\")(x)\r\nmasked_output = Dense(16, activation = \"relu\")(masked_output)\r\nmasked_output = Dense(8, activation = \"relu\")(masked_output)\r\nmasked_output = Dense(2, name='masked', activation = \"softmax\")(masked_output)\r\n\r\nLandMark_output = Dense(64,activation=\"relu\")(x)\r\nLandMark_output = Dense(32,activation=\"relu\")(LandMark_output)\r\nLandMark_output = Dense(10,name=\"landmark\",activation = \"sigmoid\")(LandMark_output)\r\n\r\nmodel = Model(inp,outputs = [masked_output,LandMark_output])\r\n\r\nmodel.load_weights(\"best_weights_v5.h5\")\r\n```\r\n\r\nAs you can see from file name of my weights file, this is my 5th version model, and this problem only occur on this version, the same code works well on the previous version of my model weights.", "If you need my training data to run the representative data function, there is a download link in my [github repository](https://github.com/ChiHangChen/Masked_Classification_with_Facial_Landmark)", "@ChiHangChen,\r\nWhile running the code, I am facing an error stating `NameError: name 'X_test' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/184323bc4c5a9cfd16547774111b65da/39954.ipynb#scrollTo=WqwfPdl7R4YG).\r\n\r\nI was unable find the data in your repo. Could you please provide a sample of the test and training data you are using in the code. Thanks!", "> @ChiHangChen,\r\n> While running the code, I am facing an error stating `NameError: name 'X_test' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/184323bc4c5a9cfd16547774111b65da/39954.ipynb#scrollTo=WqwfPdl7R4YG).\r\n> \r\n> I was unable find the data in your repo. Could you please provide a sample of the test and training data you are using in the code. Thanks!\r\n\r\n@amahendrakar Thanks for your kindly help, I upload my `X_test` to google drive as a npy file, you can download it directly from [here]([url](https://drive.google.com/file/d/1HqjHhWZIg9kyrChAm4bXZe8sZnPCIK4L/view?usp=sharing)).\r\n\r\nAnd I also modified the code :\r\n\r\n```\r\nimport gc\r\nquant = True\r\ngc.collect()\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pathlib\r\nprint(tf.__version__)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nif quant:\r\n    print(\"Converting quant....\")\r\n    sample_size = 200\r\n    rdm_idx = np.random.choice(range(len(X_test)),sample_size)\r\n    rep_data = tf.cast(X_test[rdm_idx], tf.float32)\r\n    dataset = tf.data.Dataset.from_tensor_slices(rep_data).batch(1)\r\n\r\n    def representative_data_gen():\r\n        for input_value in dataset.take(sample_size):\r\n            yield [input_value]\r\n\r\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n    converter.representative_dataset = representative_data_gen\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.uint8\r\n    converter.inference_output_type = tf.uint8\r\n    tflite_model_quant = converter.convert()\r\n    open(\"MaskedLandMarkDetction_MobileNetV2_quant_fromKeras_v5.tflite\", \"wb\").write(tflite_model_quant)\r\n\r\n    print(\"Write quantization tflite done.\")\r\nelse:\r\n    print(\"Converting normal....\")\r\n    tflite_model = converter.convert()\r\n    open(\"MaskedLandMarkDetction_MobileNetV2_fromKeras.tflite\", \"wb\").write(tflite_model)\r\n    print(\"Write tflite done.\") \r\n```\r\n\r\nSince the `X_test` array I uploaded is already divided by 255., so I remove the part that divided X_test by 255.", "> @amahendrakar Thanks for your kindly help, I upload my `X_test` to google drive as a npy file, you can download it directly from here.\r\n\r\n@ChiHangChen,\r\nCould you please add the link to the file on your Google Drive. Thanks!", "> > @amahendrakar Thanks for your kindly help, I upload my `X_test` to google drive as a npy file, you can download it directly from here.\r\n> \r\n> @ChiHangChen,\r\n> Could you please add the link to the file on your Google Drive. Thanks!\r\n\r\nI can't believe I forgot it.\r\nhttps://drive.google.com/file/d/1HqjHhWZIg9kyrChAm4bXZe8sZnPCIK4L/view?usp=sharing\r\nSorry for wasting your time.", "@suharshs , could you take a look at this? It looks like this is a duplicate one in StackOverflow: https://stackoverflow.com/questions/62059340/convert-keras-model-to-quantized-tflite-lost-precision\r\n\r\nNote, the author had some issues w/ QAT after you suggested it as mentioned in the stackoverflow thread.", "@ChiHangChen This is a stale issue. \r\n\r\nIs this still an issue for you? Can you please check with recent TF versions as well as try Quantization aware training https://github.com/tensorflow/model-optimization and let us know whether the issue persists or not. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39954\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39954\">No</a>\n"]}, {"number": 39953, "title": "The problem of using the function \u201cSetDefaultDevice\u201d to set the gpu", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):source C++\r\n- TensorFlow version (use command below):2.1\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0 , 7.6.5\r\n- GPU model and memory:4G\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUse the following code to set the gpu, but it has no effect\uff1a\r\n```\r\nstatus = LoadSavedModel(sessionOptions, runOptions, model_path, { \"serve\" }, &bundle);\r\nstd::unique_ptr<tensorflow::Session>& session = bundle.session;\r\nGraphDef graphdef;\r\nsession->Create(graphdef);\r\ngraph::SetDefaultDevice(\"/gpu:2\", &graphdef);\r\n```\r\n\r\nNo matter how many parameters I set, it shows that gpu is found: 0", "comments": ["By using a python session can you please print [list of visible devices](https://www.tensorflow.org/api_docs/python/tf/config/get_visible_devices) in your session.\r\nYou can use:\r\n```python\r\ntf.config.get_visible_devices(device_type=None)\r\n```\r\nThis can help us determine if other devices are recognized or not. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39953\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39953\">No</a>\n"]}, {"number": 39952, "title": "Registration for GCS C API Filesystem", "body": "@mihaimaruseac \r\nThis is the code for registration of GCS to the C API Filesystem", "comments": []}, {"number": 39951, "title": "Fix #39509: Invalid computations on ARMv7 running under NodeJS", "body": "", "comments": ["Please make against master. We don't accept PRs against release branches (`r$x.$y`) after the final release ended, except if we are doing a patch release. We do patch releases mostly for security issues and only taking in stuff that is present on `master` branch.\r\n\r\nOnce this is in master, we can evaluate if it is worth creating a cherry-pick against `r2.2`. TF2.2 is not a long term support release so if we don't need to issue a patch release for it we won't merge anything else on `r2.2`.\r\n\r\nIs the bug you're seeing also affecting 1.15 and 2.1 (the LTS versions)? How about 2.0?", "Well r2.2 is the first one where our model can trigger, so I can't ensure the issue is present on 2.1 or 2.0. Looks like the fix was accepted on ruy side, so what would be the best way to fix on master? update tf master with ruy master or just add the fix as a patch applied in tensorflow? ", "Looks like it will be taken care of by ruy's mainteneur : https://github.com/tensorflow/tensorflow/issues/39509#issuecomment-635434843, so I'm closing this PR. ", "I'm not sure what would be the best fix as I'm not an owner of tflie/ruy code.", "FYI: the TF->Ruy repository reference update bringing this fix in, had been through 3 successive rollbacks, but now it looks like it's finally sticking:\r\nhttps://github.com/tensorflow/tensorflow/commit/c2548337173ca6d4c1949225bc3dee2dc3f1a7dc#diff-2c39bc19b761ac36dc046245d1d47fe6"]}, {"number": 39950, "title": "Fix #39509: Invalid computations on ARMv7 running under NodeJS", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39950) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 39949, "title": "Tflite convertor generates 2 graphs ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source):\r\n- TensorFlow version (tf-nightly.2.3.0-dev20200527):\r\n\r\nI have used the latest version of TF-nightly and used it for the quantization of a model.\r\nThe model consists of LSTM layers, when I tried to convert to tflite version it generates main and subgraph.  \r\n\r\nThe subgraph consists of LSTM while loop. How can I combine this multiple graph into a single graph.\r\n\r\nFollowing is the code used for conversion \r\n\r\n`converter = tf.lite.TFLiteConverter.from_keras_model(model)`\r\n`converter.optimizations = \\[tf.lite.Optimize.DEFAULT\\]`\r\n`converter.experimental_new_converter = True`\r\n`tflite_model = converter.convert()`\r\n`open(\"converted_model.tflite\", \"wb\").write(tflite_model)`", "comments": ["@OriAlpha Can you please share a standalone code to reproduce the issue? Model code or saved models would be better. Thanks!", "```\r\nmodel.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2),activation='relu', padding='same'), input_shape=(60,15,80,80,3)))\r\nmodel.add(TimeDistributed(Conv2D(32, (3,3),kernel_initializer=\"he_normal\", activation='relu')))\r\nmodel.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\r\nmodel.add(TimeDistributed(Flatten()))\r\nmodel.add(LSTM(256, return_sequences=False, dropout=0.5))\r\nmodel.add(Dense(4, activation='softmax'))\r\n```\r\n\r\nYou could use this code to generate the model and use the above code to convert to TFLITE, which gives out 2 graphs", "@OriAlpha Can you please check [this issue](https://github.com/tensorflow/tensorflow/issues/39564#issuecomment-628950492) which is i guess similar. Please let us know whether it helped or not. If that didn't help, then please provide entire standalone code in one colab or jupyter notebook. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "the page is giving errors as below \r\nSorry, the file you have requested does not exist.", "Can you please check this https://github.com/tensorflow/tensorflow/issues/39564. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39948, "title": "ERROR: Unrecognized option: --experimental_repo_remote_exec", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.8.0\r\n- Python version: 3.5.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):0.10.0\r\n- GCC/Compiler version (if compiling from source):6.5.0\r\n- CUDA/cuDNN version:9.0/7.1\r\n- GPU model and memory:1050 , 2G\r\n\r\n\r\n\r\n**Describe the problem**\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=114\r\nINFO: Reading rc options for 'build' from /home/why/Downloads/NVIDIA_driver&CUDA&Tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nWARNING: while reading option defaults file '/home/why/Downloads/NVIDIA_driver&CUDA&Tensorflow/tensorflow/.bazelrc':\r\n  invalid command name 'try-import'.\r\nWARNING: while reading option defaults file '/home/why/Downloads/NVIDIA_driver&CUDA&Tensorflow/tensorflow/.bazelrc':\r\n  invalid command name 'try-import'.\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@why-freedom,\r\nIs there any specific reason you are using TensorFlow 1.8? \r\n\r\nCould you please upgrade to TensorFlow 2.x and let us know if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39948\">No</a>\n", "similar problem. \r\nI already got tensorflow(1.15.0) installed using conda install.\r\nThen I read that to use the tools I have to install it from source. So I downloaded this repo and tried\r\n` bazel build /tensorflow/tools/graph_transforms:summarize_graph`\r\nthen I got `Unrecognized option: --experimental_repo_remote_exec` error.\r\nBy the way, at this time, I couldn't import tensorflow. it throwed \r\n`ImportError: cannot import name 'function_pb2` error.  I thought this might because the downloaded source code kinda conflicts with the installed tf package.  \r\nAny suggestion on how do I build and use tools?\r\nDo I have to remove the installed tensorflow package and only use the source code?\r\n", "@runfengxu,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template so that we can track the issue there. Thanks!", "Maybe, the `bazel` version is not matched. so you can find the \"_TF_MIN_BAZEL_VERSION\" and \"_TF_MAX_BAZEL_VERSION\" in a file of  \"tensorflow/configure.py\"."]}, {"number": 39947, "title": "Can't build tensorflow from source with docker image", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Mint 19.3\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 2.2.0\r\n- **Python version**: 3.6.9\r\n- **Bazel version (if compiling from source)**: 3.0.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.5.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: Nvidia RTX 2060 (Laptop), 6GB\r\n-**Driver Version**: 440.59\r\n- **Exact command to reproduce**: \r\nDocker version: 19.03.09\r\nNvidia Container Toolkit: installed and tested\r\nCommands:\r\ndocker pull tensorflow/tensorflow:devel-py3\r\ndocker run -it -w /tensorflow_src -v $PWD:/mnt -e HOST_PERMS=\"$(id -u):$(id -g)\" \\\r\n    tensorflow/tensorflow:devel-py3 bash\r\nWithin the container: \r\ngit pull\r\n./configure \r\nbazel build --config=opt --verbose_failures //tensorflow/tools pip_package:build_pip_package\r\n\r\n\r\n\r\n### Describe the problem\r\nI tried to build tensorflow from source with docker. \r\nI strictly followed the instructions from the tensorflow website, but the build is never successfull and i get an internal compiler error.\r\nI tried it with and without Cuda support, always fails.\r\n\r\n### Source code / logs\r\n[tf_build_errors.txt](https://github.com/tensorflow/tensorflow/files/4695738/tf_build_errors.txt)\r\n\r\n", "comments": ["@robkuehl  \r\nPlease refer to these issues with similar error message.\r\n#23401 #23765 [link](https://csharp.developreference.com/article/13574855/Tensorflow+Custom+Compile+on+Windows)", "> \r\n> \r\n> @robkuehl\r\n> Please refer to these issues with similar error message.\r\n> #23401 #23765 [link](https://csharp.developreference.com/article/13574855/Tensorflow+Custom+Compile+on+Windows)\r\n\r\nIssues don't seem related as neither of the others appeared using the docker image or am I wrong?", "`gcc: internal compiler error: Killed (program cc1plus)`\r\nIt's very likely that the docker container ran out of RAM during compilation. [Limiting Bazel's memory usage ](https://www.tensorflow.org/install/source#bazel_build_options) might be able to help, e.g. `--local_ram_resources=2048`.", "> `gcc: internal compiler error: Killed (program cc1plus)`\r\n> It's very likely that the docker container ran out of RAM during compilation. [Limiting Bazel's memory usage ]https://www.tensorflow.org/install/source#bazel_build_options) might be able to help, e.g. `--local_ram_resources=2048`.\r\n\r\nTried it but still fails. I tried to include the whole output of bazel but the filesize was about 50MB. I hope the following reduced version is enough.\r\n\r\n\r\n[bazel_build_bug_report.txt](https://github.com/tensorflow/tensorflow/files/4711001/bazel_build_bug_report.txt)\r\n", "Any chance I get further help on this issue?", "@angerson \r\nTried to build using docker using a quarter of the total resources. Build failed with the following error:\r\n```\r\nERROR: /repo/tensorflow/compiler/mlir/tensorflow/BUILD:411:15: C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:tensorflow_ops_n_z' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 111 argument(s) skipped)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /repo/tensorflow/lite/python/BUILD:55:10 C++ compilation of rule '//tensorflow/compiler/mlir/tensorflow:tensorflow_ops_n_z' failed (Exit 4): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 111 argument(s) skipped)\r\nINFO: Elapsed time: 2011.170s, Critical Path: 631.06s\r\nINFO: 1083 processes: 155 internal, 928 local.\r\n```\r\n", "@robkuehl ,\r\nCan you please try to install the latest tf v2.5 or v2.6 and let us know if the issues still persists.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39947\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39947\">No</a>\n", "--jobs 1 --local_ram_resources=2048 maximizing available memory during compilation helped in my case. "]}, {"number": 39946, "title": "Support reduce_max for float32 and int8 in TFL micro", "body": "Implements float32 and int8 support for the REDUCE_MAX kernel in TFLu.", "comments": ["@patriklaurell Can you please resolve conflicts? Thanks!", "conflict resolved! thanks for the reminder", "@patriklaurell Can you please resolve conflicts? Thanks!", "@patriklaurell Can you please resolve conflicts? Thanks!", "@patriklaurell Can you please check @advaitjain's comments and resolve conflicts?. Thanks!", "@patriklaurell, Any update on this PR? Please. Thanks!", "Sorry for my slow response. I have been on my summer vacation the last few weeks. I now resolved conflicts and addressed @advaitjain 's comments.", "@patriklaurell  Can you please check @advaitjain's comments and keep us posted ? Thanks!", "@patriklaurell  Can you please check @advaitjain's comments and resolve conflicts?. Thanks!", "I have pushed a commit that fixes one remaining use of variable length arrays with abf7e0baa86a96a4c56e07198f753f69cb4982b2\r\n\r\nAnd #43123 will make it so that @patriklaurell would have caught this earlier."]}, {"number": 39945, "title": "[MLIR] NFC remove dangling braces in comments", "body": "Remove incorrect braces in example comments in hlo/lhlo/linalg\r\nconversion passes - these mess up brace matching.", "comments": ["Gentle ping reviewers @joker-eph "]}, {"number": 39943, "title": "EagerTensor without building a function", "body": "Hi there,\r\n\r\nI want to create a simple pipeline to predict Survival in the Titanic dataset.\r\nMy goal to load data butch by butch from disk (I don't want to load all CSV into the memory) and made a prediction with DNNLinearCombinedClassifier.\r\n\r\nDuring the testing I got  #RuntimeError: Attempting to capture an EagerTensor without building a function.#  error message. \r\n\r\nThe script work well with this estimator:\r\n```\r\npreprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)\r\n\r\nmodel = tf.keras.Sequential([\r\n  preprocessing_layer,\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dense(1),\r\n])\r\n\r\n# compile the model\r\nmodel.compile(\r\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n    optimizer='adam',\r\n    metrics=['accuracy'])\r\n\r\nmodel.fit(train_data, epochs=20)\r\n```\r\n\r\nHere is my code:\r\n\r\n```\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n####################################################################################################\r\n####################### helper functions ###########################################################\r\n####################################################################################################\r\n\r\n# load dataset from disk in to the memory batch by batch\r\ndef get_dataset(params, file_path):\r\n  dataset = tf.data.experimental.make_csv_dataset(\r\n      file_path,\r\n      batch_size=1, # Artificially small to make examples easier to show.\r\n      label_name=params['label'],\r\n      na_value=\"?\",\r\n      num_epochs=1,\r\n      ignore_errors=True)\r\n  return dataset\r\n\r\n# create feature columns for model\r\ndef generat_feature_coulumns(NUMERIC_FEATURES,CATEGORIES):\r\n    numeric_columns = []\r\n    for feature in NUMERIC_FEATURES:\r\n      num_col = tf.feature_column.numeric_column(key=feature)\r\n      numeric_columns.append(num_col)\r\n    \r\n    categorical_columns = []\r\n    for feature, vocab in CATEGORIES.items():\r\n      cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\r\n            key=feature, vocabulary_list=vocab)\r\n      categorical_columns.append(tf.feature_column.indicator_column(cat_col))\r\n    return   numeric_columns,categorical_columns\r\n\r\n# helper function\r\ndef pack_row(batch, label):\r\n    return batch, label\r\n\r\n####################################################################################################\r\n####################### set parameters   ###########################################################\r\n####################################################################################################\r\n    \r\n# set params\r\nparams = {\r\n        'label' : 'survived',\r\n        'labels': [0,1],\r\n        'train' : 'dataset/train.csv',\r\n        'test' : 'dataset/eval.csv'  ,\r\n        'model_dir' : 'results/'\r\n        }\r\n\r\n# set the numeric and categorical feature,values\r\nNUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']\r\nCATEGORIES = {\r\n    'sex': ['male', 'female'],\r\n    'class' : ['First', 'Second', 'Third'],\r\n    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\r\n    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\r\n    'alone' : ['y', 'n']\r\n    }\r\n\r\n\r\n####################################################################################################\r\n##################################### create model #################################################\r\n####################################################################################################\r\n# generate numeric and categorical feature columns\r\nnumeric_columns,categorical_columns =  generat_feature_coulumns(NUMERIC_FEATURES,CATEGORIES)\r\n# make mixed feature layer\r\n\r\nmodel = tf.estimator.DNNLinearCombinedClassifier(\r\n    model_dir=params['model_dir'],\r\n    linear_feature_columns=[categorical_columns],\r\n    dnn_feature_columns=[numeric_columns],\r\n    dnn_hidden_units=[100, 50],\r\n    n_classes = 2)\r\n\r\n####################################################################################################\r\n####################### train the model   ##########################################################\r\n####################################################################################################\r\n\r\ntest_dataset = get_dataset(params,params['test'])\r\ntrain_dataset = get_dataset(params,params['train'])\r\n\r\ntrain_data =  train_dataset.map(pack_row).shuffle(500)\r\ntest_data = test_dataset.map(pack_row)\r\n  \r\nmodel.train(lambda: test_data, steps=100)\r\n```", "comments": ["@korosig,\r\nIn order to expedite the trouble-shooting process, could you please share the `train.csv` and `eval.csv` files you are using in the code. Thanks!", "> @korosig,\r\n> In order to expedite the trouble-shooting process, could you please share the `train.csv` and `eval.csv` files you are using in the code. Thanks!\r\n\r\nYes ... here is it\r\n[eval.txt](https://github.com/tensorflow/tensorflow/files/4709672/eval.txt)\r\n[train.txt](https://github.com/tensorflow/tensorflow/files/4709673/train.txt)\r\n", "@korosig,\r\nCould you please provide the TensorFlow version you are using?\r\n\r\nOn running the code with TF v2.2, I am facing an error stating \r\n```\r\nValueError: The graph (<tensorflow.python.framework.ops.Graph object at 0x7fe6da268a90>) of the iterator is different from the graph (<tensorflow.python.framework.ops.Graph object at 0x7fe6e052e5f8>) the dataset: tf.Tensor(<unprintable>, shape=(), dtype=variant) was  created in. \r\nIf you are using the Estimator API, make sure that no part of the dataset returned by the `input_fn` function is defined outside the `input_fn` function. \r\nPlease ensure that all datasets in the pipeline are created in the same graph as the iterator.\r\n```\r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8865da4070263c96f2907014a8870a2e/39943.ipynb#scrollTo=clE7dqdpx-Ov). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39942, "title": "Incorrect Layer Normalization description", "body": "Hi,\r\nI see a [description of LayerNormalization](https://www.tensorflow.org/addons/tutorials/layers_normalizations).\r\nAccording to this image:\r\n![image](https://user-images.githubusercontent.com/1153259/83129373-64f5b280-a0e5-11ea-9f66-8f4ae4e3fbb2.png)\r\nLayerNorm should compute mean and var across dimensions (C, H, W), thus I expect them to be 1D tensors of length N.\r\nBut the code of [tf.keras.layers.LayerNormalization](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/normalization.py#L1131) computes mean and var on 3rd dimension (which is C) and generates tensors of shape (N,H,W,1)\r\n\r\nThat seems incorrect, am I missing something? ", "comments": ["I believe this is a bug with the implementation of LayerNormalization! \r\nA brief sanity check shows that the current implementation of tf.keras.layers.LayerNormalization() is actually an implementation of the instance normalization layer (e.g. tfa.layers.InstanceNormalization)\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\nx = tf.zeros((4, 16, 16, 3))\r\nlayer_norm = tf.keras.layers.LayerNormalization()\r\ninst_norm = tfa.layers.InstanceNormalization()\r\ny1 = layer_norm(x)\r\ny2 = inst_norm(x)\r\nprint(layer_norm.trainable_variables)  # expects gamma and beta variables to be scalars\r\nprint(inst_norm.trainable_variables)   # expects gamma and beta variables to be vectors of length 3 (i.e number of channels)\r\n```\r\n\r\nThe output shows that both layers have the same number of trainable weights, which is incorrect for the Layer normalization.", "Seems that tf.keras.layers.LayerNormalization corresponds with the InstanceNormalization image as well. So I think @MoustafaMeshry is right. There is a bug in tf.keras.layers.LayerNormalization.\r\n@yhliang2018 what do you think?", "This was also reported in the tensorflow_addons repository. I am also convinced that there is a problem with Tensorflow' s `LayerNormalization()` and the correct behaviour is `GroupNormalization(groups=1)`\r\n\r\nhttps://github.com/tensorflow/addons/issues/1720\r\n\r\nCould this be an opportunity to merge GroupNormalization if it has the expected behaviour while LayerNormalization doesn't?", "I suppose the default \"axis\" param in LayerNormalization should be [1:] rather than -1 which confuse me a lote\r\n", "On master there is now a comment explaining this behaviour:\r\n\r\n```\r\n  Note that other implementations of layer normalization may choose to define\r\n  `gamma` and `beta` over a separate set of axes from the axes being\r\n  normalized across. For example, Group Normalization\r\n  ([Wu et al. 2018](https://arxiv.org/abs/1803.08494)) with group size of 1\r\n  corresponds to a Layer Normalization that normalizes across height, width,\r\n  and channel and has `gamma` and `beta` span only the channel dimension.\r\n  So, this Layer Normalization implementation will not match a Group\r\n  Normalization layer with group size set to 1.\r\n```", "@dsalnikov \r\nCould you please let us know if this is still an issue, can you verify with later versions of tf and let us know.", "@Saduf2019 Yes, this is still an issue. As @lingyixia mentioned, the default value for the 'axis' argument should correspond to [1:len(input_shape)] (i.e all channel dimensions except for the batch dimension). Since the init() function does not have access to the input_shape, the fix probably requires a few more than a single line change.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019, yes this is still an issue. Implementation doesn't match the document. ", "This is still an issue", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39942\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39942\">No</a>\n"]}, {"number": 39941, "title": "Building libtensorflow.so 1.10.1 on Linux with GCC 4.8 and -std=c++11 doesn't export CXX11 symbols", "body": "**System information**\r\n- OS: CentOS 7.6\r\n- TensorFlow installed from: source\r\n- TensorFlow version: 1.10.1\r\n- Python version: 2.7\r\n- Bazel version: 0.15.0\r\n- GCC version: 4.8.5\r\n\r\n**Describe the problem**\r\nWhen I try to build a shared TensorFlow library on CentOS 7 with the GCC 4.8 compiler the compiled library doesn't used C++11 for their exported symbols. However, when I try to use the library in a standalone program and include the TensorFlow headers it fails during linking because it expects C++11 symbols whereas they are not present in the libtensorflow.so shared library, e.g.\r\n```undefined reference to `tensorflow::Status::ToString[abi:cxx11]() const```\r\nwhere as libtensorflow_framework.so has\r\n```T tensorflow::Status::ToString() const```.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nPriory to building I have altered `.tf_configuration.bazelrc` (imported in .bazlerc) by including\r\n`build --cxxopt='-std=c++11'`.\r\nFurthermore, I have added to the tensorflow/BUILD file\r\n`linkopts = [\"-std=c++11\"]`\r\n`copts = [\"-std=c++11\"]`\r\nto the libtensorflow_framework.so and libtensorflow.so targets.\r\nFinally I invoke the build with\r\n`bazel build --cxxopt='-std=c++11' tensorflow::libtensorflow.so`\r\nbut the resulting libtensorflow.so doesn't export C++11 symbols.\r\n\r\nWhen I tried building libtensorflow.so with GCC 5 on a Linux Mint machine it exported the symbols as C++11; furthermore I can compile tensorflow 2 on my Debian machine and the symbols also use C++11. For my particular purpose, though, I need to use GCC 4.8 and that's where it fails.\r\n\r\nIs it, bychance, possible to statically link the standard C++ libraries in the tensorflow library?", "comments": ["Have you tried compiling with a newer devtoolset? GCC 4.8 doesn't support the C++11 flags.\r\n\r\n```\r\nsudo yum install centos-release-scl\r\nsudo yum install devtoolset-8-gcc devtoolset-8-gcc-c++\r\nscl enable devtoolset-8 -- bash #sets gcc8 as the default compiler for a session within your current session\r\n```\r\n", "The target system this is being built for only has GCC 4.8 that is why I need to get it working in a GCC 4.8 environment.\r\nWhat do you mean GCC 4.8 doesn't support C++11 flags? I thought -std=c++11 works with GCC 4.8?", "Can you please try to build from one of the supported versions (1.15, 2.0, 2.1, 2.2, master)?\r\nTF 1.10.1 is too old for support.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39941\">No</a>\n"]}, {"number": 39940, "title": "Segmentation fault (core dumped)", "body": "tensorflow2.1.0 \r\nLinux mctech 2.6.32-431.el6.x86_64\r\ncentos7\r\n", "comments": ["@git-chenjiqing,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "Here's what I get when I execute it with gdb:\r\n`Program received signal SIGSEGV, Segmentation fault.\r\nupdate_refs (generation=<value optimized out>, n_collected=0x7fffffffaad8, n_uncollectable=0x7fffffffaae0, nofail=0)\r\n    at /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c:243\r\n243     /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c: No such file or directory.\r\n        in /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.12-1.132.el6.x86_64 libICE-1.0.6-1.el6.x86_64 libSM-1.2.1-2.el6.x86_64 libX11-1.5.0-4.el6.x86_64 libXau-1.0.6-4.el6.x86_64 libXext-1.3.1-2.el6.x86_64 libXrender-0.9.7-2.el6.x86_64`", "### CentOS release 6.5 (Final)\r\n### tensorflow.__version__==:2.1.0-rc1\r\nAn error occurred at tf.keras.Model (inputs, (x_ 8, x), name=name)\r\nThis is my output through DGB\r\n```bash\r\ngdb python3\r\nGNU gdb (GDB) Red Hat Enterprise Linux (7.2-60.el6_4.1)\r\nCopyright (C) 2010 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>;\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-redhat-linux-gnu\".\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>;...\r\nReading symbols from /root/anaconda3/bin/python3...done.\r\n(gdb)\r\n(gdb) run yolo_test.py\r\nStarting program: /root/anaconda3/bin/python3 yolo_test.py\r\n[Thread debugging using libthread_db enabled]\r\nMissing separate debuginfo for /root/anaconda3/lib/python3.7/site-packages/numpy/core/../../numpy.libs/libgfortran-ed201abd.so.3.0.0\r\n[New Thread 0x7fffee5f3700 (LWP 26448)]\r\n[New Thread 0x7fffedbf2700 (LWP 26449)]\r\n[New Thread 0x7fffeb1f1700 (LWP 26450)]\r\n[New Thread 0x7fffe87f0700 (LWP 26451)]\r\n[New Thread 0x7fffe5def700 (LWP 26452)]\r\n[New Thread 0x7fffe33ee700 (LWP 26453)]\r\n[New Thread 0x7fffe09ed700 (LWP 26454)]\r\n[Thread 0x7fffee5f3700 (LWP 26448) exited]\r\n[Thread 0x7fffe09ed700 (LWP 26454) exited]\r\n[Thread 0x7fffe5def700 (LWP 26452) exited]\r\n[Thread 0x7fffedbf2700 (LWP 26449) exited]\r\n[Thread 0x7fffeb1f1700 (LWP 26450) exited]\r\n[Thread 0x7fffe33ee700 (LWP 26453) exited]\r\n[Thread 0x7fffe87f0700 (LWP 26451) exited]\r\nDetaching after fork from child process 26467.\r\n2020-05-28 19:56:26.580410: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-05-28 19:56:26.580470: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-05-28 19:56:26.580478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nDetaching after fork from child process 26469.\r\nMissing separate debuginfo for /root/anaconda3/lib/python3.7/site-packages/cv2/.libs/libz-a147dcb0.so.1.2.3\r\n__version__==:2.1.0-rc1\r\n[New Thread 0x7fffe09ed700 (LWP 26470)]\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\nupdate_refs (generation=<value optimized out>, n_collected=0x7fffffffaad8, n_uncollectable=0x7fffffffaae0, nofail=0)\r\n    at /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c:243\r\n243     /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c: No such file or directory.\r\n        in /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.12-1.132.el6.x86_64 libICE-1.0.6-1.el6.x86_64 libSM-1.2.1-2.el6.x86_64 libX11-1.5.0-4.el6.x86_64 libXau-1.0.6-4.el6.x86_64 libXext-1.3.1-2.el6.x86_64 libXrender-0.9.7-2.el6.x86_64\r\n```", "@git-chenjiqing,\r\nCould you please provide the Python script or the Python notebook you are using, so that we can reproduce the issue on our end. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Do you have a minimized script that we can use?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39940\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39940\">No</a>\n"]}, {"number": 39939, "title": "Segmentation fault (core dumped)", "body": "", "comments": ["@git-chenjiqing \r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "###&nbsp;CentOS&nbsp;release 6.5 (Final)\r\n### tensorflow.__version__==:2.1.0-rc1\r\nAn error occurred at tf.keras.Model (inputs, (x_ 8, x), name=name)\r\nThis is my output through DGB\r\n```bash\r\ngdb python3\r\nGNU gdb (GDB) Red Hat Enterprise Linux (7.2-60.el6_4.1)\r\nCopyright (C) 2010 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html&gt;\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.&nbsp; Type \"show copying\"\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-redhat-linux-gnu\".\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/&gt;...\r\nReading symbols from /root/anaconda3/bin/python3...done.\r\n(gdb)&nbsp;\r\n(gdb) run yolo_test.py\r\nStarting program: /root/anaconda3/bin/python3 yolo_test.py\r\n[Thread debugging using libthread_db enabled]\r\nMissing separate debuginfo for /root/anaconda3/lib/python3.7/site-packages/numpy/core/../../numpy.libs/libgfortran-ed201abd.so.3.0.0\r\n[New Thread 0x7fffee5f3700 (LWP 26448)]\r\n[New Thread 0x7fffedbf2700 (LWP 26449)]\r\n[New Thread 0x7fffeb1f1700 (LWP 26450)]\r\n[New Thread 0x7fffe87f0700 (LWP 26451)]\r\n[New Thread 0x7fffe5def700 (LWP 26452)]\r\n[New Thread 0x7fffe33ee700 (LWP 26453)]\r\n[New Thread 0x7fffe09ed700 (LWP 26454)]\r\n[Thread 0x7fffee5f3700 (LWP 26448) exited]\r\n[Thread 0x7fffe09ed700 (LWP 26454) exited]\r\n[Thread 0x7fffe5def700 (LWP 26452) exited]\r\n[Thread 0x7fffedbf2700 (LWP 26449) exited]\r\n[Thread 0x7fffeb1f1700 (LWP 26450) exited]\r\n[Thread 0x7fffe33ee700 (LWP 26453) exited]\r\n[Thread 0x7fffe87f0700 (LWP 26451) exited]\r\nDetaching after fork from child process 26467.\r\n2020-05-28 19:56:26.580410: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-05-28 19:56:26.580470: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-05-28 19:56:26.580478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nDetaching after fork from child process 26469.\r\nMissing separate debuginfo for /root/anaconda3/lib/python3.7/site-packages/cv2/.libs/libz-a147dcb0.so.1.2.3\r\n__version__==:2.1.0-rc1\r\n[New Thread 0x7fffe09ed700 (LWP 26470)]\r\n\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\nupdate_refs (generation=<value optimized out&gt;, n_collected=0x7fffffffaad8, n_uncollectable=0x7fffffffaae0, nofail=0)\r\n&nbsp; &nbsp; at /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c:243\r\n243&nbsp; &nbsp; &nbsp;/tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c: No such file or directory.\r\n&nbsp; &nbsp; &nbsp; &nbsp; in /tmp/build/80754af9/python_1546061345851/work/Modules/gcmodule.c\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.12-1.132.el6.x86_64 libICE-1.0.6-1.el6.x86_64 libSM-1.2.1-2.el6.x86_64 libX11-1.5.0-4.el6.x86_64 libXau-1.0.6-4.el6.x86_64 libXext-1.3.1-2.el6.x86_64 libXrender-0.9.7-2.el6.x86_64\r\n\r\n```\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: \"Saduf2019\"<notifications@github.com&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2020\u5e745\u670828\u65e5(\u661f\u671f\u56db) \u665a\u4e0a8:13\r\n\u6536\u4ef6\u4eba: \"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;; \r\n\u6284\u9001: \"\u957f\u591c\"<1013692111@qq.com&gt;; \"Mention\"<mention@noreply.github.com&gt;; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] Segmentation fault (core dumped) (#39939)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@git-chenjiqing\r\n Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n \r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue template.\r\n \r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "@git-chenjiqing \r\nPlease refer to these links and let us know if it helps:\r\n#36841 #37689 #35968 #36201 [link](https://stackoverflow.com/questions/60368298/could-not-load-dynamic-library-libnvinfer-so-6) [link1](https://community.rstudio.com/t/tensorflow-error-after-deploy-at-shinyapp-io/56792/3)", "thank you #36841 #37689 #35968 #36201 link link1 It didn't help me\r\n\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48,\u6211\u5b89\u88c5\u4e86debuginfo-install glibc-2.12-1.132.el6.x86_64\u540e\u597d\u50cf\u6ca1\u6709\u51fa\u73b0Segmentation fault (core dumped)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39939\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39939\">No</a>\n"]}, {"number": 39938, "title": "micro_speech example breaks when feeding it trained model on Arduino", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- TensorFlow version (use command below):\r\n1.15\r\n- Microcontroller used:\r\nArduino Nano 33 BLE Sense (Arduino 1.8.12 IDE)\r\n\r\n**Describe the current behavior**\r\nmicro_speech example works on pretrained model, but whenever I try to train a model and run it on the device it breaks (even with \"yes\" and \"no\" words). The serial port receives the following message:\r\n\r\nFeature generation failed\r\nRequested feature_data_ size 536907080 doesn't match 1960.\r\n\r\nWhen running test_micro_speech_test with the same model it also crashes the program (it never stops running).\r\n\r\nThe model trains fine and achieves a good accuracy on colab and it doesn't give any errors when converting either.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should run inference just like the pretrained model.\r\n\r\n**Standalone code to reproduce the issue**\r\nTrained with the following notebook (didn't alter anything):\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/micro_speech/train/train_micro_speech_model.ipynb\r\n\r\nUsing the Arduino TFLite library version 1.15\r\n", "comments": ["It seems the newest version of the micro_speech training script creates a model that's incompatible with the TF Lite library downloadable in the Arduino IDE. Issue was fixed by building a .zip library with the latest version of TF Lite instead.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39938\">No</a>\n", "Hi do you mind telling me how you built the .zip library? I am a little new to Arduino and cloning the tensorflow source code into a zip file is not working for me. ", "Hi. You have to build it with the following script (inside the TF GitHub\nrepo):\n\ntensorflow/lite/micro/tools/ci_build/test_arduino.sh\n\n\nIt takes a few minutes but it will create the zip file at:\n\ntensorflow/lite/micro/tools/make/gen/arduino_x86_64/prj/micro_speech/tensorflow_lite.zip\n\nHope that helps!\n\nOn Tue, 2 Jun 2020 at 23:58, Jared Bell <notifications@github.com> wrote:\n\n> Hi do you mind telling me how you built the .zip library? I am a little\n> new to Arduino and cloning the tensorflow source code into a zip file is\n> not working for me.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39938#issuecomment-637828042>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIINATIK7Z74D3H26AG6N5DRUVYXDANCNFSM4NM4HK5A>\n> .\n>\n", "When I try running the test_arduino.sh script I get an error saying there is something wrong with the readable_run function in helper_functions.sh. Did you you get this error and find a way to fix it?", "I just use WSL. The Linux line endings and the windows ones are different\nso it's causing you problems. This is the fix I used.\n\nhttps://askubuntu.com/questions/966488/how-do-i-fix-r-command-not-found-errors-running-bash-scripts-in-wsl\n\n\nJust apply dos2unix to the entire micro/tools folder and it will change the\nline endings.\n\nOn Wed, 3 Jun 2020 at 22:05, Jared Bell <notifications@github.com> wrote:\n\n> What's the simplest way to run shell commands on windows? I tried running\n> it from powershell using sh test_arduino.sh but get a make error. I also\n> tried to run the script on WSL but get a set illegal option error.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/39938#issuecomment-638432690>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AIINATLGJPHYMSCGSLLH62LRU2UIDANCNFSM4NM4HK5A>\n> .\n>\n", "Hi leocorne, it looks like I still have one error getting in the way. When I am generating my zip I get a \"No rule to make target 'tensorflow/lite/micro.../fixedpoint.h' needed by generate_hello_world_test_make_project'. Stop. \" Any advice on what could be causing this?", "@leocorne  Hi, I upload the example in zip library. But it does't work.\r\nI only delete two sparkfun_edge_* files. And I didn't train. Only  used example", "I'm also still getting this error, using the latest Arduino Web Editor, but with my own model.  The model was created yesterday using TensorFlow 2.2.0 and the TFLite conversion done online using Google Colab.\r\n\r\nI first received this error after using 'micro_speech' from the latest TensorFlow Arduino Library Zip (2.1.0 alpha), via a new clone of Tensorflow.\r\n\r\nI appreciate that Arduino Web Editor is using TensorFlow Lite 1.15.0, however as I'm also getting this error using the latest source from the TensorFlow master branch, I don't think its related to the compile toolchain.\r\n\r\nI performed the following steps on Arduino Web Editor, using my model:\r\n\r\n- The model variable names were modified to use the original 'g_model' and 'g_model_len' variable names.\r\n- Uploaded my model file, and accompanying .h file.\r\n- The speech command word list was increased to include the new trained words and the CommandCount increased from 4 to 13\r\n- micro_speech.ino file was modified to use the new model, replacing the old model.\r\n![Screenshot from 2020-06-19 10-47-40](https://user-images.githubusercontent.com/3838993/85085508-6cf3ce80-b21b-11ea-96cf-77eb611c846e.png)\r\n", "```\r\nFeature generation failed\r\nRequested feature_data_ size 536907080 doesn't match 1960\r\n```\r\n\r\nI have determined that the error message directly above is the typical behaviour exhibited by the loop() method, when the setup() method has failed to correctly run to completion.  In my case, as the pre-allocation of memory is insufficient in size for the Tensor Arena, or the model input data type is not correct.  There may other reasons also why this has failed.  As the serial monitor may not be connected during the setup() method being evaluated, the associated error messages may not be observed by the developer.\r\n\r\nI found these faults in my application by converting the Arduino Web Editor sketch to a Platform IO project, then debugging on the device using a Segger JLink.  I am unaware of any other way to catch these errors at this time.", "Hello, \r\n\r\nI have the same problem and I can't solve it, could any one please explain me how you do it?\r\nFeature generation failed\r\nRequested feature_data_ size 536907080 doesn't match 1960.", "@ErickGQ1423 I think you need to increase the size of the tensor arena.", "Thank you @victorromeo, did you solve the problem with this?", "Yes. Recently, TFLite Arena Sizes have received additional support, by reporting the size of the actual tensor arena used.\r\n```c++\r\n// Guess a larger value of tensor arena 'arena_size' than is actually required.\r\n...\r\n// Build an interpreter to run the model with.\r\n tflite::MicroInterpreter interpreter(model, resolver, tensor_arena, arena_size, error_reporter);\r\n...\r\n // This obtains the number of bytes that are actually in use\r\n    return interpreter.arena_used_bytes();\r\n```\r\n\r\nThen change your code, to use the reported value + 100, to allow for future changes to the TFLite Framework implementation. ", "Hello, @victorromeo could you please help me again?\r\nI have been working for a week with the same problem, but I still have the same. I thing that I am probably not using the correct library in Arduino or probably another code on Colab, I have been following step by step that is in the book TinyML and it looks really easy, but I do not know why my code is not running\r\n\r\nI hope you can help me. "]}, {"number": 39937, "title": "No module named 'tensorflow.contrib'", "body": "I'm using tensorflow with python3.8, but when I run the code, the output shows: `No module named 'tensorflow.contrib'`\r\nDetailed:\r\n`Traceback (most recent call last):\r\n  File \"/Users/brandonli/PycharmProjects/untitled/main.py\", line 7, in <module>\r\n    import tflearn\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tflearn/__init__.py\", line 4, in <module>\r\n    from . import config\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tflearn/config.py\", line 5, in <module>\r\n    from .variables import variable\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tflearn/variables.py\", line 7, in <module>\r\n    from tensorflow.contrib.framework.python.ops import add_arg_scope as contrib_add_arg_scope\r\nModuleNotFoundError: No module named 'tensorflow.contrib'`\r\nI'm running tensorflow 2.2.0\r\nSome people say that running 1.14 could solve the problem, so I run \r\n`sudo pip3 install tensorflow==1.14.0` in the Terminal\r\nbut I found out that I could only install 2.2.0\r\nAny way to solve this problem?\r\nRunning macOS Catalina\r\nPython3.8\r\nalso working with tflearn in my code", "comments": ["@Brandonli-13,\r\n- `tensorflow.contrib` does not exist in TF 2.x. You'll have to use TF 1.x to use `tensorflow.contrib`\r\n\r\n- The reason you are unable to install TF 1.x is due to the Python version incompatibility. Please check the tested build configurations [here](https://www.tensorflow.org/install/source#linux).\r\n\r\n- In this case, I'd suggest you to migrate your code to TF 2.x. You can also switch to Python 3.7 with TF 1.x, but it is not recommended as TF 1.x is not actively supported.\r\n\r\nThanks!"]}, {"number": 39936, "title": "lstm_cell.get_initial_state(batch_size, dtype) caught 'Duplicate node name' error if batch_size is unknown", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n<ins>Yes</ins>\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n<ins>macOS Catalina version 10.15.5</ins>\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n<ins>No</ins>\r\n- TensorFlow installed from (source or binary):\r\n<ins>binary</ins>\r\n- TensorFlow version (use command below):\r\n<ins>2.1.0</ins>\r\n- Python version:\r\n<ins>3.7.7</ins>\r\n- Bazel version (if compiling from source):\r\n<ins>N/A</ins>\r\n- GCC/Compiler version (if compiling from source):\r\n<ins>N/A</ins>\r\n- CUDA/cuDNN version:\r\n<ins>N/A</ins>\r\n- GPU model and memory:\r\n<ins>N/A</ins>\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nCalling `get_initial_state(batch_size, dtype)` from LSTMCell caught `ValueError: Duplicate node name in graph: 'zeros/packed'` error if batch_size is unknown or inferred from `tf.shape(input)[0]`\r\n\r\n**Describe the expected behavior**\r\n`get_initial_state(batch_size, dtype)` returns the expected states even if batch_size is a Tensor, because often times the batch_size is unknown beforehand and could be varied from each run\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nThis code works:\r\n```\r\nimport tensorflow as tf\r\ncell = tf.keras.layers.LSTMCell(units=32, name=\"controller\")\r\nstate = cell.get_initial_state(batch_size=2, dtype=tf.float32)\r\nprint(state)\r\n```\r\n\r\nThe following doesn't work:\r\n```\r\nimport tensorflow as tf\r\ncell = tf.keras.layers.LSTMCell(units=32, name=\"controller\")\r\ni = tf.keras.Input(shape=[2, 3])\r\nstate = cell.get_initial_state(batch_size=tf.shape(i)[0], dtype=tf.float32)\r\nprint(state)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe functionality is required in a [DNC model](https://github.com/carusyte/tf-DNC/blob/2ca0f2bf7e9b2dcf45e776db0f733d434246628f/dnc/dnc.py#L148) I'm experimenting with recently.", "comments": ["@carusyte \r\nI ran the code shared on tf 2.2 and do not face the error as faced on 2.1, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/99e8008fde16d35e7d34a86f1ef85d11/untitled198.ipynb)", "> @carusyte\r\n> I ran the code shared on tf 2.2 and do not face the error as faced on 2.1, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/99e8008fde16d35e7d34a86f1ef85d11/untitled198.ipynb)\r\n\r\nI tried the gist provided and seems they're working on 2.2 version. \r\nGuess I'll have to upgrade TF and keep up with the pace. Thanks so much, @Saduf2019 !", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39936\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39936\">No</a>\n"]}, {"number": 39935, "title": "[TF 2.3.0] [Intel MKL] Eigen Threadpool Support with Intel oneDNN", "body": "> <em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**PR to be submitted to replace OpenMP with Eigen Threadpool.**\r\nAttn: @penpornk \r\n\r\n1. https://github.com/tensorflow/tensorflow/pull/39893 **( Merged)**\r\n2. https://github.com/tensorflow/tensorflow/pull/39915 **(Merged)**\r\n3. #40128 **(Merged)**\r\n4. #40254 **(Merged)**\r\n5.  #40488 **(Merged)**\r\n6.  #40489 **(Merged)**\r\n\r\nEdited by penpornk@ to update the PR list.", "comments": ["@nammbash Thank you for creating the tracking issue!", "@penpornk As mentioned in the description previously, #39877 has been replaced by two smaller PR's. \r\nhttps://github.com/tensorflow/tensorflow/pull/40128\r\nhttps://github.com/tensorflow/tensorflow/pull/40254", "@Srini511 Thank you for the heads up. I've updated the first post.", "@nammbash Is custom thread pool still on track for 2.3? Are there more PRs for 2.3 coming?", "> @nammbash Is custom thread pool still on track for 2.3? Are there more PRs for 2.3 coming?\r\n\r\n@penpornk No more PRs from our side and yes it is  tracker for T2.3 functionality wise(not performant).(Hence the late reply sorry about that) Just that the Eigen PR and the Random Uniform Vectorize is in the works pending merge. FYI. Thank you for all the help and hard work on this.", "@nammbash Thank you for the clarification and for all the hard work from your side as well! :)", "Closing this issue since all the tracked PRs are merged."]}, {"number": 39934, "title": "[TF 2.3.0] [Intel MKL] BFloat16 Support for Intel CPUs", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**Adding support for Bfloat16 datatype for running models on Intel CPUs.**\r\nAttn: @penpornk \r\n**The following PRs have been submitted for inclusion in TF2.3.0**\r\n1. #34504 **(Closed)**; Replaced by #40596 **(Merged)**\r\n2. #39597 **(Merged)**\r\n3. #40212 **(Merged)**\r\n4. #40211 **(Merged)**\r\n5. #40455 **(Merged)** \r\n6. #40330 **(Merged)**\r\n7. #40598 **(Merged)**\r\n\r\nTF2.3.0 performance of Resnet-50-v1.5 depends on Eigen PR: \r\n\r\n8. [Eigen MR #84](https://gitlab.com/libeigen/eigen/-/merge_requests/84) **(Merged)**\r\n9. #40684 **(Merged as https://github.com/tensorflow/tensorflow/commit/8cf97846290cf7d8b95256fe3123abaaa8c8e553)**\r\n\r\n**Postponed to 2.4**\r\n* [Change the type `tensorflow::bfloat16` to `Eigen::bfloat16`](https://gitlab.com/libeigen/eigen/-/merge_requests/84#note_365322477).\r\n* Vectorize RandomUniform: https://github.com/tensorflow/tensorflow/pull/39747 **(Test failure)** ", "comments": ["@nammbash Thank you for creating the tracking issue!", "@penpornk  Friendly reminder\r\n#40598 is a bug fix for TF2.3 pending review.\r\n#34504 is still pending Google internal merge for TF2.3. \r\n", "@nammbash Thank you for the reminder!\r\nApproved #40598 .\r\n#34504 doesn't look like it's approved yet. I'll follow up on it.", "By the way, #39747 has a new test failure (posted on the PR) and can't be merged until it's fixed.", "#34504 has been replaced by #40596 which was merged yesterday. I'll update the first post.", "#40598 is merged.", "@penpornk  Looks like we were all updating together. \r\nPlease take a look at it now. Eigen Third party PR upgraded submitted and pending Review.\r\n@agramesh1 will update on any other Pending PR decision. ", "The branch has been cut. I'm closing the issue now. Thank you all again for your hard work!", "@penpornk thank you too!"]}, {"number": 39933, "title": "Fix an exception on using tf.keras.utils.Sequence with tf.distribute.MirroredStrategy under multi-GPU environment", "body": "There is an exception issue on using data provider which is subclass of tf.keras.utils.Sequence with tf.distribute.MirroredStrategy in multi-GPU environment.\r\n\r\nThere can be get() function calls from other GPUs' training context after an epoch ending have been occured in a GPU's context.\r\nThe existing implementation of OrderedEnqueuer class(in tensorflow/python/keras/utils/data_utils.py) access directly inner data of Python's queue object, thus there can be data race issues.\r\n\r\nIn OrderedEnqueuer.get() function, \r\n`self.queue.task_done()`\r\ncause 'task_done() called too many times' exception, when it called after another context's OrderedEnqueuer.stop call during waiting for\r\n`inputs = self.queue.get(block=True).get()`.\r\n\r\nTo prevent this exception, we can check OrderedEnqueuer.is_running() before call self.queue.task_done().\r\n\r\n\r\nAlso, after fix the exception issue, infinity waiting can occured in \r\n`self.queue.get(block=True)`\r\nafter some GPUs' context wait for it after another thread's end of data.\r\nIt can be walk arounded by introducing timeout for the queue waiting.\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39933) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39933) for more info**.\n\n<!-- need_author_cla -->", "@googlebot I fixed it.", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39933) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39933) for more info**.\n\n<!-- ok -->", "Could u add a test case for that to showcase the failure in the multi-gpu's case?", "This is a test code for the issue what I mentioned:\r\n[tf_test_multi_gpu.py.txt](https://github.com/tensorflow/tensorflow/files/4699458/tf_test_multi_gpu.py.txt)\r\n(Changed extention to .txt due to limitation of attachable file types for GitHub's comment.)\r\n\r\nI wrote the code as compatible with existing unit tests, but I didn't commit it as an unit test of TF code because the function consumes a lot of resources which can cause spurious test fail due to OOM.\r\n\r\n\r\nThe problem is occured under both heavy data loading and heavy training situation, so I added fake slow data loader Sequence object and used big model to fit.\r\nMy test environment was 16-cores Ubuntu 18.04 machine has 2 NVIDIA TITAN RTX GPUs connected with NVLink.\r\n\r\nIt's behavior is vary for values of _sequence_length_ and _queue_size_:\r\n\r\n1.\r\nThe setting that I attached(seq.len=7, queue.len=10) usually caused the exception that I mentioned.\r\nStack trace was :\r\n```\r\n2020-05-29 11:34:35.128820: W tensorflow/core/framework/op_kernel.cc:1741] Invalid argument: ValueError: task_done() called too many times\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 243, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 785, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\", line 801, in wrapped_generator\r\n    for data in generator_fn():\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 880, in get\r\n    six.reraise(*sys.exc_info())\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n\r\n  File \"/home/dev/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 875, in get\r\n    self.queue.task_done()\r\n\r\n  File \"/usr/lib/python3.7/queue.py\", line 74, in task_done\r\n    raise ValueError('task_done() called too many times')\r\n\r\nValueError: task_done() called too many times\r\n```\r\n\r\n\r\n2.\r\nThe setting (seq.len=7, queue.len=4) occasionally caused the infinity waiting I fixed.\r\n\r\n3.\r\nWhen I found the issue with our real code, the length(# of minibatches) of my data sequence was about 58, but I cannot find exact problematic setting for simple test code with same sequence length right now.\r\n\r\n\r\nI checked that those problems are not occuring after fix the code as I commited.\r\nI also checked the test code in my desktop which has only one NVIDIA GeForce RTX 2080 Ti GPU, those issues had never seen in the single GPU environment.", "Being merged, does this mean this will this be included in TF 2.3.0?"]}, {"number": 39932, "title": "RFC: use atomic instructions for propagating simple node outputs", "body": "This follows a similar approach to the earlier work by Derek Murray\r\n(change ID I9058c55898079cb3ab6011513b4468ddb293f59f) and allows using\r\natomic operations on the pending counts. It extends that work to also\r\napply to graphs that include control/merge nodes (eg the 'wide_deep'\r\nmodel in the tf models repo)\r\n\r\nThe approach here is to note that even in a graph that has control/merge\r\nnodes, it's likely that many nodes do not have any merge/control\r\noutputs. There's already a fast path for this case with simpler logic,\r\nand that simple logic has the advantage of only accessing the pending\r\ncount in one spot where it decrements the incoming pending values. Only\r\nthe last such decrementer propagates the output and activates the node.\r\n\r\nGiven this, we can use atomic operations for all such \"fast path\" nodes\r\nand avoid holding an exclusive lock. We fall back to the exclusive lock\r\nfor any nodes that have slow-path outputs. As a conservative measure,\r\nthis patch acquires the shared lock for the fast path, though it may\r\nnot be necessary.\r\n\r\nThe other bit in this patch is the management of the 'outstanding_ops'\r\nmember. In order to allow concurrent completion of ops without the\r\nexclusive lock, this also becomes atomic. Simply making it atomic is\r\nsufficient but leaves a lot of performance on the table. This patch\r\nbatches the updates of this variable so that each op completion only\r\ntouches it at most once, and no-ops out in the case that the pending op\r\ncount doesn't need to be modified (eg when an op completion activates\r\nexactly one downstream node).\r\n\r\nI benchmarked this change using tensorflow-models/official/r1/wide_deep\r\nand collecting the distribution of \"steps/sec\" reported using commands\r\nlike:\r\n```\r\n  # switch to new code\r\n  $ python -u  census_main.py -ebe=20  -te 20 -mt wide 2>&1 | tee /tmp/before/wide.txt\r\n  # switch to new code\r\n  $ python -u  census_main.py -ebe=20  -te 20 -mt wide 2>&1 | tee /tmp/after/wide.txt\r\n```\r\n... for the 'wide', 'wide_deep', and 'deep' models. I then exported the\r\n'steps/second' metrics to TSV with:\r\n```\r\n  $ grep 'tensorflow:global_step/sec' \\\r\n    /tmp/{before,after}/*.txt | perl -p  -e \\\r\n    's,/tmp/(.+)/(.+).txt:.*?([\\d\\.]+)$,$1 $2 $3,g' > /tmp/data.tsv\r\n```\r\nMean steps/sec are as follows on my AMD Ryzen 9 3900X desktop (12\r\nphysical cores, 'performance' CPU governor enabled):\r\n```\r\nBefore:\r\n    model   steps/sec\r\n----------------------\r\n      deep  888.2449\r\n      wide  778.8037\r\n wide_deep  635.0198\r\n\r\nAfter:\r\n    model   steps/sec   improvement\r\n-----------------------------------\r\n      deep  929.8955  (+4%)\r\n      wide  989.9900  (+27%)\r\n wide_deep  780.8842  (+22%)\r\n```\r\nA few notes worth considering:\r\n\r\nUsing atomic operations has some fixed overhead compared to non-atomics,\r\nand particularly when the cache line being operated on is in M\r\n\"modified\" state in another core. This increased latency of operating on\r\na cache-line in remote M state (aka \"HitM\" access) is also true with\r\nnon-atomic operations, but non-atomics can potentially allow for\r\ninstruction level parallelism and pipeline multiple such accesses\r\ntogether, whereas atomics do not on current x86 architectures[1]. So,\r\nthere is possibly a cross-over point on some graph shapes where the\r\nmutex-based synchronization with non-atomic operations actually beats\r\nthe atomic-based implementation here.\r\n\r\nThat said, the current code path for the non-atomic (mutex-protected)\r\ncase is complex/branchy enough that it's not clear that any significant\r\npipelining of reads can actually occur without some more explicit\r\nunrolling, etc.\r\n\r\nIt's also worth noting that, in uncontended cases, the atomics will be\r\nslower than the non-atomics. However, this is unlikely an issue in real\r\nworkloads, considering that these code paths are only uncontended when\r\nthe actual work of op execution dominates the runtime. In other words,\r\nthis is only a perf bottleneck when it's under contention, and a small\r\nregression for uncontended cases is likely in the noise.\r\n\r\n[1] https://spcl.inf.ethz.ch/Publications/.pdf/atomic-bench.pdf", "comments": ["Mingsheng, I believe you're the right person to be reviewing or finding a reviewer for current runtime changes too? Looks like an interesting optimization but definitely not my area of expertise.", "@rmlarsen Can you please review this PR ? Thanks!", "@rmlarsen Can you please review this PR ? Thanks!", "@toddlipcon This PR failed on some of our tests ( see one of the stack traces below). It is unclear to me what the cause may be. Could you please look into this?\r\n\r\n```\r\nF0819 20:53:58.300506    1035 propagator_state.cc:755] Check failed: iter_state->outstanding_ops >= -delta (1 vs. -7) \r\n*** Check failure stack trace: ***\r\n    @     0x55f91b196c7c  absl::logging_internal::LogMessage::DieIfFatal()\r\n    @     0x55f91b1956e2  absl::logging_internal::LogMessage::SendToLog()\r\n    @     0x55f91b194c43  absl::logging_internal::LogMessage::Flush()\r\n    @     0x55f91b197869  absl::logging_internal::LogMessageFatal::~LogMessageFatal()\r\n    @     0x55f9176c6bd5  tensorflow::PropagatorState::FrameState::AdjustOutstandingOps()\r\nI0819 20:53:58.499822    1093 publisher_client.cc:1105] Publisher client forwarder group(cell:service topic:tensorflow-logging id:applications:local priority:1) connected\r\nI0819 20:53:58.499984    1093 publisher_client.cc:2190] Publisher(cell:service topic:tensorflow-logging id:applications:local) connected\r\n    @     0x55f9176c628c  tensorflow::PropagatorState::PropagateOutputs()\r\n    @     0x55f9176b6c2d  tensorflow::(anonymous namespace)::ExecutorState<>::Process()\r\n    @     0x55f9176b9008  std::__u::__function::__policy_invoker<>::__call_impl<>()\r\n    @     0x55f91820c6c9  Eigen::ThreadPoolTempl<>::WorkerLoop()\r\n    @     0x55f91820beda  std::__u::__function::__policy_invoker<>::__call_impl<>()\r\n    @     0x55f91aa04da0  Thread::ThreadBody()\r\n    @     0x7fa1720a64e8  start_thread\r\n    @     0x7fa171edd22d  clone\r\n```", "Interesting. I'm guessing from the stack that this is a Google internal test? Any way to reproduce externally? Otherwise I'll just look through and see if I can spot an error visually.", "Never mind, I see the test results showed up on the PR and can repro a failure locally. Will update the PR once fixed.", "@toddlipcon  Can you please check @rmlarsen's comments and keep us posted ? Thanks!", "Spent a little time digging into the issue the afternoon. The problem seems to be a race when decrementing the num_outstanding_ops in `FrameState::IterationState`. Specifically, you can get a race like:\r\n\r\nInitially: IterationState::num_outstanding_ops == 1, FrameState::num_pending_inputs == 1\r\n*T1*: decrements num_outstanding_ops from 1->0, but hasn't yet acquired the FrameState::mu_ mutex.\r\n*T2*: one of the pending inputs is completed, which increments num_outstanding_ops to 1 and decrements num_pending_inputs to 0.\r\n*T2*: the newly scheduled op is completed, and now we decrement num_outstanding_ops from 1->0\r\n\r\nNow both threads proceed to acquire `FrameState::mu_` and see that the Iteration is complete (and the frame is complete too). They both think they were \"responsible\" for completing the frame because they were the thread to decrement num_outstanding_ops. But, really, we need to make the check for the other conditions of IsIterationDone() atomically with the decrement.\r\n\r\nFixing this by putting all the AdjustOutstandingOps logic under a mutex again makes it correct but probably would cancel out a lot of the performance gain. I'm thinking through a few other options and will try to circle back on this patch at some point, but not sure exactly when. I'm happy to prioritize it more highly if you think the performance benefits are substantial to an important class of workloads, but otherwise might be more of a back-burner task for me.\r\n\r\n", "Actually had an idea to try out and it seems to have fixed the problem locally. The num_outstanding_ops structure is now adjusted while holding the shared_mutex on the hot path (which was already held, so there isn't any new lock acquisition in the common case). I check the `IsIterationDone()` while holding the shared lock. Since any enter/exit nodes would acquire the exclusive lock while modifying the other conditions for `IsIterationDone()` I think the shared lock is sufficient.\r\n\r\nWould it be possible to re-run the test suite? I don't have access to RBE or anything and I think it would take a couple days to run it all on my desktop. Otherwise, I'll be a Googler in a couple weeks and can probably figure out how to run them.", "BTW I was able to re-run the benchmarks and still seeing similar improvements of the branch relative to master.\r\n\r\nHere \"after-ub\" is with the \"undefined behavior\" version of the code that reinterpret_casts to atomic when atomics are necessary, and \"after\" is the current version of this PR that uses atomic all the time. There's a slight loss in performance by not using the reinterpret_cast trick, but the current PR is still a good improvement relative to master on my machine.\r\n\r\nmodel|run|throughput|vs baseline\r\n------|---|-----------|----------\r\ndeep|before|875.9\r\ndeep|after-ub|909.0 | +3.9%\r\ndeep|after|897.2 | +2.5%\r\nwide|before|776.5 \r\nwide|after-ub|975.8 | +25.7%\r\nwide|after|927.6 | +19.5%\r\nwide_deep|before|625.5\r\nwide_deep|after-ub|768.6 | +22.9%\r\nwide_deep|after|760.4 | +21.6%\r\n\r\n", "@toddlipcon thanks a lot for your patience. This is a very exciting improvement to the core of TensorFlow. Much appreciated!", "I'm submitting this via the Google-internal submission flow instead of github PR. Closing here."]}, {"number": 39931, "title": "model = keras.models.Sequential() # I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 72.19M (75694080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1.0\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n# building an image classifier\r\nn_rows = 4\r\nn_cols = 10\r\nplt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\r\nfor row in range(n_rows):\r\n    for col in range(n_cols):\r\n        index = n_cols * row + col\r\n        plt.subplot(n_rows, n_cols, index + 1)\r\n        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\r\n        plt.axis('off')\r\n        plt.title(class_names[y_train[index]], fontsize=12)\r\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\r\nsave_fig('fashion_mnist_plot', tight_layout=False)\r\nplt.show()\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\r\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\r\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\r\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\r\n\r\n**Describe the expected behavior**\r\nmodel.layers\r\n\r\n<tensorflow.python.keras.layers.core.Flatten at 0x7ff370af5780>,\r\n <tensorflow.python.keras.layers.core.Dense at 0x7ff370af5c88>,\r\n <tensorflow.python.keras.layers.core.Dense at 0x7ff330ab36d8>,\r\n <tensorflow.python.keras.layers.core.Dense at 0x7ff330ab3828>]\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. \r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/github/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb#scrollTo=KQiHD3LwTRDF\r\n\r\n**Other info / logs** \r\n\r\n2020-05-28 13:40:44.785958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 72 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:29:00.0, compute capability: 7.5)\r\n2020-05-28 13:40:44.787561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed4ff1eab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-05-28 13:40:44.787571: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5\r\n2020-05-28 13:40:44.789256: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 72.19M (75694080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-05-28 13:40:44.796521: F ./tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch<Distribution>, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: out of memory\r\nAborted (core dumped)\r\n", "comments": ["@SarahLynnePu \r\nPlease share simple standalone code to replicate the issue faced or share a colab gist for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39931\">No</a>\n"]}, {"number": 39930, "title": "in resolution of [Wsign-compare] warning id 5", "body": "Unsigned nature of 'output_stream.ByteCount()' implied by context ( the literal word count \ud83d\ude05 : )  ). \r\n\r\nIf being more strict is prudent ( I don't think it is in this case ), the return type of 'output_stream.ByteCount()' could be explicitly set to an unsigned integer type.", "comments": ["What warning are you seeing on which compiler? I don't think we see that internally. Also, C-style casts are discouraged.", "> What warning are you seeing on which compiler? I don't think we see that internally. Also, C-style casts are discouraged.\r\n\r\n`\"INFO: From Compiling tensorflow/core/lib/strings/proto_serialization.cc [for host]:\r\ntensorflow/core/lib/strings/proto_serialization.cc: In function 'bool tensorflow::SerializeToBufferDeterministic(const google::protobuf::MessageLite&, char*, size_t)':\r\ntensorflow/core/lib/strings/proto_serialization.cc:75:44: warning: comparison of integer expressions of different signedness: 'size_t' {aka 'long unsigned int'} and 'int' [-Wsign-compare]\r\n   75 |   return !output_stream.HadError() && size == output_stream.ByteCount();\r\n      |                                       ~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\"`\r\n\r\nCan you recommend an appropriate change?", "@mihaimaruseac "]}, {"number": 39929, "title": "[tf.data] skip concat when origin ds has infinity cardinality", "body": "This pr will skip `a.concatenate(b)` when `a` already has infinity cardinality.\r\n\r\nThis may also be achieved by removing the `ConcatenateDatasetOp` from the graph when `to_concatenate` has infinity cardinality with grappler, in that way `to_concatenate` and its prior ops may be pruned in graph optimization. If you prefer this way, I'd love to contribute.\r\n\r\nThank you for your time on reviewing this pr.", "comments": ["sorry, this may have some memory problem when using session mode. I would probably go and search for substituting concatenate dataset with infinity size with IdentityOp in grappler."]}, {"number": 39928, "title": "in resolution of [Wsign-compare] warning id 3", "body": "It is explicit that 'counter_value' is of unsigned type. It is implied that 'n' is a quantity/count. Given that 'n' is a signed int however, the apparent intended behavior of logging the first N state transistions is not ensured.\r\n\r\nIn resolution of the warning I cast 'n' to size_t. \r\nIt is suggested however that the type n be changed from an int to an appropriate unsigned type in order to disqualify the possibility of the passed 'n' being negative; unintentional behavior.", "comments": ["@mihaimaruseac "]}, {"number": 39927, "title": "in resolution of [Wsign-compare] warning id 0", "body": "I looked a little into the context of how the variables 'last_node' and 'graph_info_->num_nodes()' are used. num_nodes() implies that the quantity returned is unsigned/non-negative, although the same assumption cannot be clearly made about last_node. \r\n\r\nI cast last_node to size_t under the assumption that it is being utilized as a size/non-negative quantity.", "comments": ["@terryheo Could you please take a look?", "@mihaimaruseac "]}]