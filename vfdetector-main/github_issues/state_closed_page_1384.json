[{"number": 11544, "title": "PyImport_Import crash while using bazel to build the project", "body": "I recently ran into a problem, while using bazel to build a project.  This project is compiled as a dynamic linking library, using PyImport_Import to import python module. when there is \"import tensorflow as tf\" in the python file , application who calls the dynamic linking library crashed everytime, but when it\u2018s not there ,everything works just fine. where is the problem?\r\nmy tensorflow version is 1.0.0,python 2.7.0,bazel 0.4.3\r\n\r\n\r\nhere is the console information when the application crashes:\r\n\r\n> F tensorflow/core/framework/function.cc:1015] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for Softmax\r\n\r\n\r\nhere is the test python file looks like: \r\n`from __future__ import print_function\r\nimport tensorflow as tf\r\nimport os\r\nimport time\r\nfrom itertools import izip\r\nimport numpy as np\r\nimport wrapt\r\nimport cv2\r\ndef get_int( ):\r\n    a = 10\r\n    b = 20\r\n    return a + b\r\n\r\ndef get_str( s1, s2 ):\r\n    #return s1 + s2  \r\n    #return 'Hello , TY'  \r\n    return ('Hello, World', 10, 20)`\r\n\r\nhere is the source code of .so file:\r\n`   Py_Initialize();\r\n    if ( !Py_IsInitialized() ) {\r\n        return -1;\r\n    }\r\n    PyEval_InitThreads();\r\n    PyThreadState *mainThreadState = NULL;\r\n    //  save a pointer to the main PyThreadState object \r\n    mainThreadState = PyThreadState_Get();\r\n    //  release the lock \r\n    PyEval_ReleaseLock();\r\n    char* mockargv[1]={(char*)\"\"};\r\n    PySys_SetArgv(1,mockargv);\r\n    PyRun_SimpleString(\"import sys\");\r\n    PyRun_SimpleString(\"sys.path.append('./')\");\r\n    pName_ = PyString_FromString(\"test_py\");\r\n    displayPyObject(pName_);\r\n    if(pName_ == NULL){\r\n        return -1;\r\n    }\r\n    pModule_ = PyImport_Import(pName_);\r\n    displayPyObject(pModule_);`\r\n\r\nand here is the dynamic linking library part of my BUILD file\r\n`cc_binary(\r\n    name = \"test.so\",\r\n    linkshared = 1,\r\n    deps = [\r\n        \":test_lib\",\r\n    ],\r\n)\r\ncc_library(\r\n    name = \"test_lib\",\r\n    visibility = [\"//visibility:__subpackages__\"],\r\n    srcs = glob([\"test.cpp\"\r\n             ],\r\n        ),\r\n    includes=[\"test.h\"],\r\n    linkopts = [\r\n\"-lm -lpthread -L/usr/lib/python2.7 -lpython2.7 -lopencv_core -lopencv_imgproc -lopencv_highgui -lopencv_ml -lfreeimage\"\r\n    ],\r\n    deps = [\r\n        \"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/core:framework\",\r\n        \"//tensorflow/core:framework_internal\",\r\n        \"//tensorflow/core:tensorflow\",\r\n    ],\r\n)\r\n`\r\n\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 11543, "title": "Add `required` argument for `DEFINE_*` APIs and warning information f\u2026", "body": "Add `required` argument for `DEFINE_*` APIs and warning information for unparsed args. #11195 ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "@nolanliou   https://github.com/tensorflow/tensorflow/pull/11532", "Closing for the same reason as #11532: we need this API to match our internal ones, I'm following up with folks internally and then we'll revisit #11195.  Sorry about this, and I want to emphasize that I appreciate your effort to contribute!", "ok, i got it"]}, {"number": 11542, "title": "Tensorflow feed_dict issue", "body": "I am new to tensorflow. I understand that we need to create the tensorflow graph and then call sess.run() to get the values I want.\r\n\r\nHowever, I am confused by how the feed_dict works. \r\nFor example:\r\nWill 1 be the same as 2?\r\n```\r\n1. accuracy, cost = sess.run([accuracy, cost], feed_dict = {X:X_batch, Y:Y_batch})\r\n\r\n2. accuracy  = sess.run(accuracy, feed_dict = {X:X_batch, Y: Y_batch})\r\n   cost = sess.run(cost, feed_dict = {X:X_batch, Y:Y_batch})\r\n```\r\nI don't know that if tensorflow receives the same feed_dict in cost and in tensorflow graph computing accuracy already computes cost, do it go over the neural net again to evaluate the value, or it will return the value computed without going through the net again?\r\n\r\nAlso, from [Hvass-Labs/TensorFlow-Tutorials/TensorFlow Tutorial #02 Convolutional Neural Network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb), in function plot_conv_weights(weights, input_channel=0)\r\n```\r\nweights = sess.run(conv_weigh)\r\n```\r\nSince training weights require we fill the placeholder X and Y with values, but here I saw no feed_dict.\r\n\r\nSo how exactly feed_dict works?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, the short answer is that the two should be the same, assuming that in the course of computing `accuracy` and/or `cost`, the model weights are not modified. In general, each call to `Session.run` will execute all the computations required to compute the requested tensors to fetch. See https://www.tensorflow.org/api_docs/python/tf/Session#run\r\n"]}, {"number": 11540, "title": "Unclear about how to make AttentionWrapper work", "body": "Hello, I am using the TensorFlow 1.2.1 and I want to implement a Seq2Seq model with Attention through tf.contrib.seq2seq. I didn't find enough information of how to use ```tf.contrib.seq2seq.AttentionWrapper``` on documentation, however I successfully find how to use that in the [NMT tutorial](https://github.com/tensorflow/nmt#attention-wrapper-api).\r\n\r\nBefore adding Attention, I already make the Seq2Seq model work, however, it seems very difficult to insert the AttentionWrapper by simply wrapping the decoder cell. It is not fair to ask anyone to read my own code, which is quite long. So here I provide the minimal code to demonstrate the problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n# INPUT\r\nX = tf.placeholder(tf.int32, [None, None])\r\nY = tf.placeholder(tf.int32, [None, None])\r\nX_seq_len = tf.placeholder(tf.int32, [None])\r\nY_seq_len = tf.placeholder(tf.int32, [None])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# ATTENTION\r\nattention_mechanism = tf.contrib.seq2seq.LuongAttention(\r\n    num_units = 128, \r\n    memory = encoder_out,\r\n    memory_sequence_length = X_seq_len)\r\n\r\ndecoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128),\r\n    attention_mechanism = attention_mechanism,\r\n    attention_layer_size = 128)\r\n\r\n# DECODER\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\n\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = encoder_state,\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# LOSS\r\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\r\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\r\n\r\n# BACKWARD\r\nparams = tf.trainable_variables()\r\ngradients = tf.gradients(loss, params)\r\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\r\n```\r\nThe graph cannot be built, due to error message\r\n```\r\nTraceback (most recent call last):\r\n  File \"seq2seq_attn.py\", line 44, in <module>\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2766, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2595, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2545, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 234, in body\r\n    decoder_finished) = decoder.step(time, inputs, state)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py\", line 139, in step\r\n    cell_outputs, cell_state = self._cell(inputs, state)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 180, in __call__\r\n    return super(RNNCell, self).__call__(inputs, state)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/layers/base.py\", line 439, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 677, in call\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\nAttributeError: 'LSTMStateTuple' object has no attribute 'attention'\r\n```\r\nI have also tried using GRU instead of LSTM, the error still exists", "comments": ["@ebrevdo Do you mind having a look, or do you know any colleague responsible for this part? Thanks.", "@zhedongzheng encoder_state, which is the final state of LSTM or GRU in your case, can not be directly used as initial state of an attention cell. Please go to check https://github.com/tensorflow/tensorflow/issues/11077 if you need wrap it with AttentionWrapperState.\r\n", "Thanks @Songweiping for the help, after changing the initial state of decoder, it works!!!\r\n```\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state),\r\n    output_layer = projection_layer)\r\n```", "Adding improved error messages; updates should show up within a day or two in master."]}, {"number": 11539, "title": "Update get_started.md", "body": "Fix various errors in print out statements and unnecessary lines of code", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Can you modify the pull request to merge into `master` rather than `r:1.2`?", "@frankchn I already made the changes to master. Am I not supposed update the 1.2 documentation?", "No, the v1.2 branch is already frozen and released and v1.3 is coming soon, so we\u2019ll just fix forward at this point. \n\n> On Jul 17, 2017, at 12:52 AM, Alan Yee <notifications@github.com> wrote:\n> \n> @frankchn I already made the changes to master. Am I not supposed update the 1.2 documentation?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n> \n", "Would I be allowed to push some changes into v1.3 branch?", "v1.3 will be cut from master when we decide to release it, so whatever that is in master when we create v1.3-rc0 will automatically be included.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@frankchn does this look good, or do I need to do anything else?", "@alanyee We should be good.\r\n\r\nJenkins, test this please.", "Actually @alanyee looking at it now, can you rebase to master and push again? You have some random commits that are not relevant in this PR right now.", "@frankchn I think rebasing to master is what caused the random commits to become involved in this PR. Am I unsure how to do this besides open a new PR?", "I squashed the commits so I think this won't end up polluting `master`. Thanks!"]}, {"number": 11538, "title": "MultiRNNCell fails _like_rnncell check.", "body": "- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra 10.12.5\r\n- **TensorFlow installed from (source or binary)**: binary, pip3 install\r\n- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1\r\n- **Python version**: 3.5.2\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n\r\n### Describe the problem\r\nI believe this is a bug. When initializing a MultiRNNCell like MultiRNNCell([lstm]*3) this subsequently may be passed into something like tf.nn.bidirectional_dynamic_rnn and it will pass the _like_rnncell check and everything proceeds normally. When initializing a MultiRNNCell like MultiRNNCell([lstm_factory() for _ in range num_layers]), such as in [this code](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms) the _like_rnncell check on line 393 of [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py) will fail. If you trace it back to where that function is defined there are 4 qualifiers, the qualifiers that fail are hasattr(cell, 'output_size') and hasattr(cell, 'state_size'). The preferred way of initializing multi cell rnns that do not share input size which lead to later dimension mismatch seem to be this way. Initially I wrote my code with the former implementation but the former initialization leads to input dimension sharing and then mismatches later on which is another issue and why I can't use that one. (Also this probably fails anywhere a _like_rnncell check is used, I know for a fact my AttentionWrapper in my decoder fails for the same reason not just in the bidirectional_dynamic_rnn)\r\n\r\n\r\n### Source code / logs\r\nThis is my encoder copied from my seq2seq model class. \r\n```\r\n    LSTMCell = tf.nn.rnn_cell.LSTMCell\r\n    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\r\n    DropoutWrapper = tf.nn.rnn_cell.DropoutWrapper\r\n\r\n    def encode(\r\n        self,\r\n        num_units, peepholes, inputs,\r\n        num_layers, seq_len, time_major,\r\n        keep_prob=0.5\r\n    ):\r\n        multi_cell = MultiRNNCell(\r\n            [\r\n                (self._cell_factory(num_units, peepholes, keep_prob)\r\n                    for x in range(num_layers))\r\n            ]\r\n        )\r\n        enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(\r\n            cell_fw=multi_cell,\r\n            cell_bw=multi_cell,\r\n            inputs=inputs,\r\n            sequence_length=seq_len,\r\n            dtype=tf.float32,\r\n            time_major=time_major\r\n        )\r\n        return enc_outputs, enc_state\r\n\r\n    def _cell_factory(self, num_units, peepholes, keep_prob):\r\n        lstm = LSTMCell(num_units=num_units, use_peepholes=peepholes)\r\n        dropout = DropoutWrapper(lstm, input_keep_prob=keep_prob)\r\n        return dropout\r\n```\r\n\r\nTrace:\r\n`  File \"execute.py\", line 168, in <module>\r\n    main()\r\n  File \"execute.py\", line 91, in main\r\n    steps=10000\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 241, in train\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 560, in _train_model\r\n    model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py\", line 545, in _call_model_fn\r\n    features=features, labels=labels, **kwargs)\r\n  File \"execute.py\", line 134, in model_wrapper\r\n    keep_prob=params['encode']['keep_probability']\r\n  File \"/Users/panda/Desktop/aura_ml/model_1/model.py\", line 42, in encode\r\n    time_major=time_major\r\n  File \"/Users/panda/Desktop/aura_ml/aura_model/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py\", line 364, in bidirectional_dynamic_rnn\r\n    raise TypeError(\"cell_fw must be an instance of RNNCell\")\r\nTypeError: cell_fw must be an instance of RNNCell`", "comments": ["LOL. Fixed it. I forgot you can linebreak in list comprehensions so it was treating the parens inside the multicell initializer as a generator. Jesus. "]}, {"number": 11537, "title": "Added missing `marital_status` assignment", "body": "", "comments": []}, {"number": 11536, "title": "[Feature Request] Feed tensors to feed_dict", "body": "I think that a nice step towards \"dynamic computation graphs\" is the possibility to feed tensors to placeholders.\r\n\r\nI am building an application which creates a network.\r\nI create my training and testing batches with tf.train.batch. \r\nHowever, the training batches are unlimited (so no epoch limit to the tf.train.slice_input_producer) but I would like to also test during training so to only test one epoch on the testing data.\r\nFor both the training and testing images, I have a preprocessing pipeline which also uses a computation graph, but is different for testing and for training.\r\n\r\nNow the problem is that I have to create the testing batch every time I want to test (because I want to test for 1 epoch, and I cannot otherwise limit the input producing...).\r\nSo it would definitely help, to be able to feed the training and testing batch respectively to the network input.\r\nMy current solution is to create the network every time I make the switch from training to testing with the new network inputs. But this takes quite an (unnecessary) long amount of time...\r\n\r\nIs there perhaps another way to \"link\" two graphs in tensorflow?", "comments": ["It doesn't really make sense to feed a `Tensor` object. Each tensor represents output of a an op and can correspond to different values, whereas numpy array is a specific value. Instead of `feed_dict` you can use standard Python API during graph construction to indicate that you want tensor to be an input to another node.  A common scenario is to only use one graph, and have different parts of the graph correspond to training vs testing parts of the network. "]}, {"number": 11535, "title": "Java API Generics Phase 2", "body": "This is the big change for generics. The main classes like Tensor and Output acquire type parameters. The test programs have been updated accordingly. Existing code written against the non-generic API should still mostly work but will generate \"raw types\" warnings. The setup of the tables in the Types class from phase 1 has been centralized into Types itself to ensure it happens early enough.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The strategy of providing backward compatibility through raw types runs into one snag: Java is not willing to let a `List<Tensor<?>>` be used where a `List<Tensor>` is permitted. It's <5-line fix to the Android demo program to make this work, and I expect a similarly small change to other existing code.\r\n\r\nThe type `Tensor<?>` is admittedly slightly annoying, though accurate. It is possible to avoid having to write it, at the price of adding another class, but backward compatibility would not be regained in any case.", "@tensorflow-jenkins test this please", "@andrewcmyers it looks like there are some bugs in the GPU build, e.g.\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/6226/consoleFull\r\n```\r\nERROR: /workspace/tensorflow/java/BUILD:194:1: Building tensorflow/java/ConstantTest.jar (1 source file) failed: Worker process sent response with exit code: 1.\r\ntensorflow/java/src/test/java/org/tensorflow/op/core/ConstantTest.java:51: warning: [rawtypes] found raw type: Tensor\r\n      Tensor result = sess.runner().fetch(op.asOutput()).run().get(0);\r\n      ^\r\n  missing type arguments for generic class Tensor<T>\r\n  where T is a type-variable:\r\n    T extends Object declared in class Tensor\r\ntensorflow/java/src/test/java/org/tensorflow/op/core/ConstantTest.java:53: warning: [unchecked] unchecked call to <U>copyTo(U) as a member of the raw type Tensor\r\n      assertArrayEquals(ints, result.copyTo(actual));\r\n                                           ^\r\n...\r\n```", "Thanks for the diagnosis, this looks like a (semantic) conflict with other recent commits that have happened in parallel. I'll resolve it.", "@rmlarsen This should build and test successfully now.", "@andrewcmyers Thanks!\r\n@tensorflow-jenkins test this please", "@asimshankar could you please review/approve this?", "Sorry for the delay, between vacation and some other things this dropped out of my view.\r\nI will plan on taking a look at this next week. Thanks for your patience Andrew!", "@quaeler can you resolve the conflicts? Sorry, it's our bad for leaving this simmer so long.\r\n\r\n@asimshankar ping", "I should probably do this merge; there are actually some small design decisions.", "Jenkins test this please.", "Cloning `phase2` locally and running the test which failed also fails for me.\r\n```\r\nJUnit4 Test Runner\r\n...............E\r\nTime: 0.083\r\nThere was 1 failure:\r\n1) testUInt8Tensor(org.tensorflow.TensorTest)\r\njava.lang.IllegalArgumentException: class org.tensorflow.types.TFUInt8 is not a TensorFlow type.\r\n        at org.tensorflow.types.Types.dataType(Types.java:40)\r\n        at org.tensorflow.Tensor.create(Tensor.java:117)\r\n        at org.tensorflow.TensorTest.testUInt8Tensor(TensorTest.java:423)\r\n...\r\n```\r\nIt ain't lying... the static block in `org.tensorflow.types.Types` does not register the `org.tensorflow.types.TFUInt8` class in its `zeroes` and `typeCodes` maps.\r\n", "@tensorflow-jenkins test this please.", "Can someone with the privileges kick off Jenkins to test this, please?\r\n\r\nThere is a lot of history accumulating on this PR. I am happy to create a \"fresh\" PR with the same changes if that helps move things along.", "Jenkins, test this please. ", "@martinwicke Is there potentially something wrong with Jenkins - XLA failure cites docker device space issues:\r\n```\r\nSuccessfully built e0b6e2dbd179\r\nRunning 'tensorflow/tools/ci_build/xla/linux/gpu/run_py3.sh' inside jenkins-tensorflow-pull-requests-xla-2627.gpu...\r\ndocker: Error response from daemon: devmapper: Thin Pool has 977073 free data blocks which is less than minimum required 983040 free data blocks. Create more free space in thin pool or use dm.min_free_space option to change behavior.\r\nSee 'docker run --help'.\r\nBuild step 'Execute shell' marked build as failure\r\n```\r\nand the mac failure is what looks like all of the Python-based tests; WRT the latter, i suppose @andrewcmyers could have merged down from master when master was broken, but given that so many tests are failing, it seems like something more Jenkins-systemic is afoot.\r\n", "Jenkins, test this please.\r\n\r\nBoth issues ought to be resolved now. Let's hope for the best.", "Hi @asimshankar, I created a smaller PR (#12928) that factors out some of this from the addition of generics.", "Jenkins, test this please.", "Ready for review.", "Jenkins, test this please", "Thanks @andrewcmyers !", "Jenkins, test this please"]}, {"number": 11534, "title": "[FeatureRequest] make categorical_x_entropy broadcastable", "body": "For example:\r\n\r\n```\r\n>>> x = tf.placeholder('float32', [4,5,6])\r\n>>> y = tf.placeholder('float32', [1,5,6])\r\n>>> z = tf.nn.softmax_cross_entropy_with_logits(labels=x, logits=y)\r\nTraceback (most recent call last):\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 671, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 5 and 20 for 'SoftmaxCrossEntropyWithLogits' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [5,6], [20,6].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1594, in softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2380, in _softmax_cross_entropy_with_logits\r\n    features=features, labels=labels, name=name)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2508, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1873, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1823, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/home/khaotik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\", line 676, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Dimension 0 in both shapes must be equal, but are 5 and 20 for 'SoftmaxCrossEntropyWithLogits' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [5,6], [20,6].\r\n```\r\n\r\nBut, intuitively, I'd like to broadcast `y` into shape `[4,5,6]` in the above code.\r\n\r\nIt's not too hard to get a workaround such as using `tf.tile`. However it would consume more precious GPU memory.", "comments": ["I'm trying to understand why your labels and logits wouldn't be the same shape. Usually they naturally are the same in most computations I've seen. Can you give a concrete example rather than just a placeholder where you've forced it.", "@aselle Yes, I encountered this during GAN training.\r\n\r\nBasically, it looks like:\r\n\r\n```\r\nnum_class = ...\r\nnum_sample = ...\r\nbatch_size = ...\r\nlabels = tf.placeholder('int32', [1, num_sample])\r\nlogits = tf.placeholder('float32', [batch_size, num_sample, num_class])\r\n```\r\n\r\nIn my use case, the model is about multi-speaker audio separation, each data point comes with multiple single-speaker samples. So `num_samples` can't be merged with `batch_size`.\r\n\r\nWe wanted GAN discriminator to do multi-class classification besides true/false. So we use softmax rather than sigmoid.\r\nHere `labels` is a constant `int` tensor across batches. The broadcast situation arises here.\r\n\r\nIt's really not a bottleneck in our model, but I'm probably just getting OCD about optimization. This situation may also arise in other separation / segmentation tasks, I think.", "@fchollet , @martinwicke ,  thoughts on this feature request?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@ebrevdo isn't this the issue we made `softmax_cross_entropy_with_logits_v2` for?", "No, that was to allow backprop.  This is a request for broadcasting in the c++ kernel, which is a reasonable request.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Added a PR #16784 for the support."]}, {"number": 11533, "title": "add recovery_wait_secs option for MonitoredTrainingSession", "body": "In order to reduce the sleep time by worker to wait for a model to be initialized or restored , \r\n\uff0cwe add  recovery_wait_secs option for MonitoredTrainingSession\uff0cand we are able to start distributed training faster.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Let's wait for @ispirmustafa to return to review, since he owns this class. Might take a few weeks.", "@ispirmustafa could you take a look, please?", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@martinwicke looks like this is good to go, pending API review.", "LG for API once docstring is fixed.", "@rmlarsen I have fixed it for comment.", "Jenkins, test this please", "Jenkins, test this please", "Mind updating the goldens? See the failing test. Thanks!", "I run the test as follows to update test goldens, but nothing changed.\r\n\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\n\r\n![image](https://user-images.githubusercontent.com/13819195/29695085-27a0b376-8973-11e7-94e3-3c3266487e09.png)\r\n\r\nWhat should I do next?  Thanks", "@gunan Can you see what went wrong here?", "You need to run the api compatibility test with python 2. Are you using python3, by any chance?", "We should probably point that out explicitly in the failure message if it\ndoesn't already.\n", "FTR, the error is:\r\n```\r\nERROR:tensorflow:2 differences found between API and golden.\r\nERROR:tensorflow:Issue 1\t: Change detected in python object: tensorflow.train.WorkerSessionCreator.\r\nERROR:tensorflow:Issue 2\t: Change detected in python object: tensorflow.train.\r\nF.\r\n```\r\n\r\n@horance-liu did you run the golden generator? Did you run it in python2 or python3?", "@horance-liu ping", "I have run the golden generator in python2, but nothing changed. See https://user-images.githubusercontent.com/13819195/29695085-27a0b376-8973-11e7-94e3-3c3266487e09.png\r\n", "Sorry for incomplete instructions. You need to be on Linux, and python2 to regenerate goldens. I will look into what we need to also include windows and macos in the OS side.\r\nIf you are working on a macos system, you may use docker to do the golden update.", "@asimshankar @josh11b This is the same change we just reviewed... we said back then it was ok. Didn't make the connection until now. Same question applies -- why is there a timeout at all?", "I think this has since been independently added (albeit with a radically different default). I'll close this PR."]}, {"number": 11532, "title": "support \"required\" for tf.app.flags", "body": "support \"required\" for tf.app.flags", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Thanks for the contribution @ericyue, this is indeed a useful feature for a flags library.\r\n\r\nSome background is probably warranted though: TF's flags library is an API port of an internal flags library at Google that I believe predated argparse.  The flags library's existence is solely to make it easier to opensource internal code, since flags.py's API is used throughout Google internally.  But argparse is more than sufficient for most opensource use cases, so I would like to avoid changing the existing flags API, even to enhance it.\r\n\r\nFurthermore, I believe there are efforts to opensource the internal flags library, which already has this feature of marking flags as required, and rather than introduce a new API that is incompatible with the internal one, I think it would be a good idea to wait for that library to be released.  If it's the case that several months go by and that library is not released, let's revisit this PR and perhaps plan to make it API compatible with that library.\r\n\r\nLet me know if you feel strongly about this right now though, and we can find a way to get this feature in.  In the meantime, I strongly encourage you to use argparse directly.\r\n\r\n\r\n\r\n", "@vrv  ok, I think you means the  google's python-gflag?   in python-gflag make a flag required by using `gflags.MarkFlagAsRequired` . but tensorflow's flag not implement this api.  is there a chance to replace `require` parameter with `MarkFlagAsRequired` ? ", "@ericyue It's similar to python-gflags, but the implementation is different.  I'll have to double check that the API is the same... give me some time to contact the authors internally and see what the plans are."]}, {"number": 11531, "title": "tf.app.flags support \"required\"", "body": "add required support for tf.app.flags\r\n\r\n``flags.DEFINE_bool(\"bool_required\", None, \"HelpString\", required = True)``", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11530, "title": "Add SquaredDifference gradient", "body": "I added gradient function for the [SquaredDifference](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/squared-difference) and test case.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "/CC @asimshankar ", "I noticed that they in [python](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L826) have over a dozen(!) of instances of copy-pasted lines for reshaping gradients due to broadcasting feature, and decided to make a helper class `ReshapeGradientsWRTBroadcasting` that will make everything for me.", "@DimanNe friendly ping!  Thanks", "I forgot to squash commits while merging, sorry.\r\n\r\n@vrv  As far as I understood this PR was blocked by [this PR](https://github.com/tensorflow/tensorflow/pull/11634), so I could not do anything here.", "Did something go wrong with Sanity Checks? Or have I done something wrong?", "@tensorflow-jenkins test this please", "@drpngx Did @DimanNe address your comments? \r\n@vrv Don't we need to also define the second derivative as well when moving something like this to a C++ kernel?", "Yes, if you add a new op that fuses existing python ops, you need to add the gradient for the fused op.", "@DimanNe Can you please add a gradient op for the new (gradient) op?", "@rmlarsen Yes, sure, I will do that.", "@DimanNe thanks!", "@rmlarsen Sorry for bothering, but I realized that I had misunderstood something. Can you, please, clarify where I should add gradient operation for the new gradient operation(s) `+-2(x-y)`?\r\n\r\nFor example, in order to make gradient op for the **named** operation `\"SquaredDifference\"`, I registered a C++ function using macro `REGISTER_GRADIENT_OP`, but how am I supposed to register function computing gradient for two **unnamed** functions `+-2(x-y)`? Or perhaps there is any example of doing it?\r\n\r\nIf it is not difficult, could you please take a step backward and elucidate why do we need at all gradient op for gradient? What is the rationale behind having hand-written second-order derivatives/gradients? Would not it be enough to have only first-order gradients and do everything else with automatic differentiation?", "@DimanNe My apologies. You are indeed just creating a composite op using the C++ interface. What I was referring to was cases in the past where people would add a specialized C++ kernel for computing a gradient, in which case the automatic differentiation would have no way of acting on it.\r\n\r\nI think what you have here is fine. @skye can you confirm this?", "@rmlarsen , @skye I understand that you are busy, and probably do not want at all to make C++ interface of TensorFlow great again. So, can I do anything to help you and/or to expedite merging the PR?", "Hey sorry, I meant to ping @suharshs but forgot!", "@suharshs any news/thoughts?)", "Ah, unfortunately it looks like someone recently just submitted this gradient :(\r\nhttps://github.com/tensorflow/tensorflow/commit/293020e83df4fdaa8bfb59275cf05028609b92cd\r\nThanks so much for implementing this though, and let us know if you see anything wrong with what was submitted."]}, {"number": 11529, "title": "Not able to import tensorflow python3.5.2 Tensorflow  installation from PIP3 on window machine ", "body": "import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", lin\r\ne 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.p\r\ny\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n", "comments": ["(1) Please review guidelines [here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) before raising an issue.\r\n(2) Follow easy guide listed [here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) for your solution.", "Yes, it's likely an installation issue, please report more information about how you installed, what system etc. It cannot load the tensorflow binary. Otherwise we won't be able to proceed.", "I followed below step to install Tensorflow\r\n1. Installe Python 3.5.2 on window7 machine.\r\n2. With the help of PIP, i installed Tensorflow \r\n      pip3 install --upgrade tensorflow \r\n     pip3 install --upgrade tensorflow-gpu\r\n\r\n", "Are you trying to install tensorflow or tensorflow-gpu? Don't install both in the same environment if you can avoid it. To start off with, uninstall both copies of tensorflow.\r\n`pip uninstall tensorflow`\r\n`pip uninstall tensorflow-gpu`\r\n\r\nStart off again by reinstalling the CPU only version first\r\n`pip install tensorflow`\r\nTry running tensorflow again. Make sure you enter any environments that you've set up first!\r\nIf you are receiving a similar error, try the advice in this s/o thread regarding missing DLLs. \r\nhttps://stackoverflow.com/questions/42011070/\r\nThe DLL mentioned there is available here: https://www.microsoft.com/en-us/download/details.aspx?id=48145\r\nFailing that, as I see you are trying to install on Windows, @mrry has a great installation troubleshooter script that can help pinpoint issues available here: [mrry's Self-check Script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c)\r\n\r\nCome back with your progress. Good luck!"]}, {"number": 11528, "title": "Not able to import tensorflow..", "body": "import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", lin\r\ne 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.p\r\ny\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_ten\r\nsorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Python35\\lib\\importlib\\__init__.py\", line 126, in impor\r\nt_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>\r\n", "comments": ["This thread is duplicated [same author](https://github.com/tensorflow/tensorflow/issues/11529)", "Closing as duplicate."]}, {"number": 11527, "title": "AttributeError: 'module' object has no attribute 'LinearClassifier'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["(1) Please review guidelines [here](https://github.com/printdhruv/tensorflow/blob/master/ISSUE_TEMPLATE.md) and fill all the information.\r\n\r\n(2) Missing `System information , source code`.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 11526, "title": "Update head.py", "body": "Use tf.losses instead of deprecated contrib", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Trying this again.\r\n\r\n@tensorflow-jenkins, test this please.", "@tensorflow-jenkins, test this please.\r\n\r\nedit: Am I not allowed to get Jenkins to test it?", "@tensorflow-jenkins Test this please", "Here is the error: \r\n\r\n```======================================================================\r\nERROR: testBinarySVMWith1DWeights (__main__.BinarySvmHeadTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head_test.py\", line 1581, in testBinarySVMWith1DWeights\r\n    logits=self._predictions)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1258, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 638, in _create_model_fn_ops\r\n    loss, weighted_average_loss = loss_fn(labels, logits, weight_tensor)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1216, in _loss_fn\r\n    return _compute_weighted_loss(loss, weights)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1796, in _compute_weighted_loss\r\n    weight = weights_broadcast_ops.broadcast_weights(weight, loss_unweighted)\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/python/ops/weights_broadcast_ops.py\", line 167, in broadcast_weights\r\n    with ops.control_dependencies((assert_broadcastable(weights, values),)):\r\n  File \"/var/lib/jenkins/workspace/tensorflow-pull-requests-cpu/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-opt/bin/tensorflow/contrib/learn/head_test.runfiles/org_tensorflow/tensorflow/python/ops/weights_broadcast_ops.py\", line 103, in assert_broadcastable\r\n    weights_rank_static, values.shape, weights.shape))\r\nValueError: weights can not be broadcast to values. values.rank=0. weights.rank=2. values.shape=(). weights.shape=(2, 1).```", "@tensorflow-jenkins, test this please.", "@tensorflow-jenkins, test this please.", "@tensorflow-jenkins, test this please.", "Great! @alanyee Would using debugger might have helped me ?", "@printdhruv I just read the code and documentation really closely. My initial confusion came from the lack of static types, so luckily my PL theory and compiler classes help me get pass the confusion.", "@alanyee Nice to hear that one.How you are planning to fix tf.summary ?", "I will eventually read the code closely, but right now, I am too sleepy to focus and I need to watch the latest episode of Games of Thrones.", "No problem :) I think it will work with what I tried in my version . Finger crossed;)"]}, {"number": 11525, "title": "tf.where outputs the wrong tensor dtype", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution**: Ubuntu 14.04\r\n- **TensorFlow installed from**: Source\r\n- **TensorFlow version**: ('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.13\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: Cuda 8.0\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nx = tf.Variable([2, 1], dtype=tf.float32)\r\ny = tf.Variable(2, dtype=tf.float32)\r\nz = tf.where(tf.equal(x, y))\r\nprint z\r\n\r\n>> Tensor(\"Where_1:0\", shape=(?, 1), dtype=int64)\r\n```\r\n\r\n### Describe the problem\r\nIn the documentation for [tf.where.py](https://www.tensorflow.org/api_docs/python/tf/where), it states that it returns `A Tensor with the same type and shape as x, y if they are non-None.` In the example above, we see that `x` and `y` are both dtype `tf.float32`. Yet, the output is `tf.int64`. Was this intended or a bug?\r\n\r\nThis bug is a bit problematic for me as I use `tf.where` in my loss function so I need to ensure that all my tensors are `tf.float32` since `tf.cast` is not a differentiable `ops`.", "comments": ["(1) Please review guidelines [here](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md) before raising an issue \r\n(2) Following is a definition of tf.where and comments to match with your code.\r\n ```\r\ntf.where (\r\n    condition,                                 // tf.equal(x,y) , Dimensions must be equal\r\n    x = None,                                  // x = x \"Your variable x\"\r\n    y = None,                                  // y = y \"Your variable y\"\r\n    name = None                                // Name = \"String\"\r\n)\r\n```\r\n(3) Please see the following code for your reference,\r\n\r\n```\r\nx = tf.Variable([2, 1], dtype = tf.float32)\r\ny = tf.Variable([2, 3], dtype = tf.float32)\r\nz = tf.where(tf.equal(x,y), x , y , name = \"float_Equal\")\r\nprint(z)\r\n```\r\n\r\nOutput : -\r\n\r\n`Tensor(\"float_Equal_2:0\", shape=(2,), dtype=float32)`\r\n"]}, {"number": 11524, "title": "Build fails on FreeBSD 11", "body": "```\r\nexternal/grpc/src/core/lib/iomgr/socket_utils_common_posix.c:101:39: error: use of undeclared identifier 'IP_PKTINFO'\r\n  if (0 != setsockopt(fd, IPPROTO_IP, IP_PKTINFO, &get_local_ip,\r\n                                      ^\r\n```", "comments": ["Seems like a gRPC portability issue. \n\nTF currently doesn't support BSD officially, but contributions are always welcomed.", "Thanks @yurivict . As @byronyi pointed out, FreeBSD is not officially supported but this does seem to be an incompatibility in GRPC. I'd encourage you to make a fix to [grpc](https://github.com/grpc/grpc/blob/7897ae9/src/core/lib/iomgr/socket_utils_common_posix.c#L85) (perhaps by ensuring that `GRPC_HAVE_IP_PKTINFO` is false for FreeBSD)."]}, {"number": 11523, "title": "[OpenCL] Stats tracking", "body": "* Adds buffer size tracking to SYCL allocator (#114)\r\n    \r\n    The SYCL buffers underlying tensors already keep track of their sizes,\r\n    so we can easily provide this tracking information for debugging\r\n    purposes.\r\n\r\n* Adds stat tracking to the SYCL allocator\r\n    \r\n    The SYCLAllocator will now find the max allocation size on construction,\r\n    and keep track of the allocation stats, as given in AllocationStats.\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11522, "title": "Fixes 'batch_size' may be used uninitialized", "body": " clang++-3.6: warning: 'batch_size' may be used uninitialized in this function", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11521, "title": "[go bindings] Printing graph in a text format", "body": "I was looking for a function  that allows me (like in python) to print the graph in a readable way.\r\n \r\nIf you open a GitHub issue, here is our policy:\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOs X\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Python version**: 2.7.13\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**:\r\nIn python i can do:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.InteractiveSession()\r\nwith tf.gfile.FastGFile(\"graphname.pb\", 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n    \r\n# All operations\r\nsess.graph.get_operations()\r\n```\r\n\r\nthat prints the operations of my graph\r\n\r\nIn go i just have the option:\r\n\r\n```\r\ngraph := tf.NewGraph()\r\nif err := graph.Import(model, \"\"); err != nil {\r\n\t\tlog.Fatal(err)\r\n\t}\r\nf, err := os.Create(\"logWritter.txt\")\r\n\tif err != nil {\r\n\t\tlog.Fatal(err)\r\n\t}\r\n\tgraph.WriteTo(f)\r\n```\r\nit prints the binary graph.\r\n\r\n### Describe the problem\r\nWhen using a model, previously trained by somebody else, it is very useful to know the nodes to reference them.  It should be nice to have the same option with go.", "comments": ["Did you tried tensor board?\r\n [https://www.tensorflow.org/get_started/summaries_and_tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\r\n\r\n[https://www.tensorflow.org/get_started/graph_viz](https://www.tensorflow.org/get_started/graph_viz)", "Yes it is very useful, but when using go language, i would like to be able to print all the nodes of my graph  in a programatic way ", "Did you try to use graph libraries with the tensor flow ?", "I'm with the  Go binding to TensorFlow", "Is this issue resolved? Please update here If it was not resolved already. Thanks!", "Closing due to lack of recent activity, but please let me know if I'm mistaken. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 11520, "title": "Show usage when no arguments passed to import_pb_to_tensorboard.py", "body": "It seems like `--model_dir` and `--log_dir` should be required arguments.  After adding at least one required argument, then `parseargs` handles no arguments cleanly.\r\n\r\nTESTING\r\n\r\nRunning it before the fix:\r\n```\r\n$ python import_pb_to_tensorboard.py \r\nTraceback (most recent call last):\r\n  File \"import_pb_to_tensorboard.py\", line 74, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nTypeError: run() got an unexpected keyword argument 'argv'\r\n```\r\n\r\nRunning it after the fix:\r\n```\r\n$ python import_pb_to_tensorboard.py \r\nusage: import_pb_to_tensorboard.py [-h] --model_dir MODEL_DIR --log_dir\r\n                                   LOG_DIR\r\nimport_pb_to_tensorboard.py: error: argument --model_dir is required\r\n```\r\n\r\nI was unable to test the success path due to this issue: https://github.com/tensorflow/tensorflow/issues/11519\r\n\r\nTesting done on Mac OSX Sierra 10.12.5 using Python 2.7.6.\r\n\r\nALTERNATIVE SUGGESTION\r\n\r\nLet me know if you prefer that no arguments are required.  If that is the case, I was considering this solution (from [this Stackoverflow answer](https://stackoverflow.com/a/29293080/112705)):\r\n```\r\n  try:\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  except:\r\n    # Print the usage if the user provides no arguments\r\n    parser.print_help()\r\n    sys.exit(0)\r\n```\r\n\r\nOTHER THOUGHTS\r\n\r\nIt seems like `model_dir` should maybe be renamed to `model_path` (assuming that it should link the file, not the directory which it is in)?", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11519, "title": "import_pb_to_tensorboard.py fails with TypeError", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution**: Mac OSX Sierra 10.12.5\r\n- **TensorFlow installed from**: Source.\r\n- **TensorFlow version**: ('v0.10.0-1727-g484ca8a-dirty', '0.11.0rc0')\r\n- **Python version**: 2.7.6\r\n- **Bazel version**: 0.4.4-homebrew\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```\r\npython import_pb_to_tensorboard.py --model_dir ~/Code/AndroidTensorFlowMNISTExample/app/src/main/assets/mnist_model_graph.pb --log_dir /tmp/tensorboard/\r\n```\r\n\r\n### Describe the problem\r\nI'm trying to use `import_pb_to_tensorboard.py` to import an existing TensorFlow model into TensorBoard.  It appears to always return a TypeError due to line 74:\r\n\r\n```\r\npython import_pb_to_tensorboard.py --model_dir ~/Code/AndroidTensorFlowMNISTExample/app/src/main/assets/mnist_model_graph.pb --log_dir /tmp/tensorboard/\r\nTraceback (most recent call last):\r\n  File \"import_pb_to_tensorboard.py\", line 74, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\nTypeError: run() got an unexpected keyword argument 'argv'\r\n```\r\n\r\nI discovered `import_pb_to_tensorboard.py` in [this Stackoverflow answer](https://stackoverflow.com/a/44955005/112705).", "comments": ["Marking this as \"Contributions Welcome\" since it seems you already have a PR to improve the tool.", "I recommend you to reference these methods.\r\nIt must be run 0.12 tensorflow.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/learn/text_classification.py\r\n![image](https://user-images.githubusercontent.com/29457193/29652320-e338afce-88df-11e7-822e-e2c4aebd0153.png)\r\nHe references another import.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/python/platform/app.py\r\nHe use sys.exit() function.", "Thanks to @easyhangul, that was the problem!  I followed [the instructions to upgrade my TensorFlow](https://www.tensorflow.org/install/install_mac) installation, and then  `import_pb_to_tensorboard.py` worked successfully!\r\n\r\nEnhancement suggestion: it would be great if the Python tools could check that the local installation of TensorFlow is at the required minimum version.  The error returned here is not very user-friendly.\r\n\r\n**STEPS TO FIX**\r\n\r\nI had to upgrade `pip` from 8.1.2 to 9.0.1, then I ran `pip install --upgrade tensorflow`.\r\n\r\nNow I'm running v1.3.0:\r\n```\r\n$ python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n```\r\n...and `import_pb_to_tensorboard.py` appears to run successfully:\r\n\r\n```\r\n$ python import_pb_to_tensorboard.py --model_dir ~/Code/AndroidTensorFlowMNISTExample-daj/model/mnist_model_graph.pb --log_dir /tmp/tensorflow_logdir\r\n<warnings snipped>\r\nModel Imported. Visualize by running: > tensorboard --logdir=/tmp/tensorflow_logdir\r\n```\r\nI can now see something in my TensorBoard (double clicking on the \"import\" blob reveals the whole graph):\r\n\r\n![screen shot 2017-08-27 at 3 38 51 pm](https://user-images.githubusercontent.com/739125/29753357-e76cd524-8b3d-11e7-8dd4-e8a7062dada9.png)\r\n\r\n\r\n", "Closing as this is resolved"]}, {"number": 11518, "title": "Fixing a typo in a README", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11517, "title": "ImportError: No module named pbr.version", "body": "Hi everybody. I've this problem also when i run openstack --version. Solutions?", "comments": ["Please review guidelines before raising an issue here. [https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md](https://github.com/tensorflow/tensorflow/blob/master/ISSUE_TEMPLATE.md)", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "hey, my solution:\r\nfor windows:\r\nsudo /bin/env pip uninstall pbr ## important\r\nsudo /bin/env python pip install pbr\r\n\r\nfor MAC\r\nsudo /usr/bin/env pip uninstall pbr ## important\r\nsudo /usr/bin/env python pip install pbr\r\n\r\ncheckout this link for why use '/usr/bin/env' beforehand: http://www.junlulocky.com/blog/pythontricks", "On mac, pip was not available outside of the virtualenv, so:\r\nsudo easy_install pbr\r\n\r\ndid the trick.\r\n", "sudo pip install pbr --ignore-installed"]}, {"number": 11516, "title": "how to feed more than one input into my tensorflow model", "body": "i want to use python code to feed more than one input and then use concat method how can i do it \uff1fby using more placeholder\uff1f", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11515, "title": "Add white spaces", "body": "This PR adds white spaces on docstrings.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 11514, "title": "tf.assign is much slower than tf.assign_add on CPU", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v0.8.0rc0-16474-gac98d11', '1.2.1')\r\n- **Python version**: Python 2.7.13 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**:cuda8.0/cudnn6.0\r\n- **GPU model and memory**:old titan x / 12GB\r\n- **Exact command to reproduce**: a python script\r\n\r\n### Describe the problem\r\n`tf.assign` is much slower than `tf.assign_add` on CPU when `intra_op_parallelism_threads` set to 10. Although tensorflow treats these two operations a little different(maybe because `tf.assign` allows uninitialized tensor, accepts more tensor types), they all use a same class in eigen. I find this problem when doing some test about #11411.\r\n\r\n### Source code / logs\r\nscript is on [gist](https://gist.github.com/suiyuan2009/24315b35915bddbe2d53b764164bb8fb),\r\n```\r\ndtype is <dtype: 'float32'>\r\nuse tf.assign: 3480.53776469 MB/s\r\nuse tf.assign_add: 10737.1193186 MB/s\r\n```\r\nset `intra_op_parallelism_threads` to 1, \r\n```\r\ndtype is <dtype: 'float32'>\r\nuse tf.assign: 3481.3296105 MB/s\r\nuse tf.assign_add: 4244.41816359 MB/s\r\n```\r\non GPU,\r\n```\r\ndtype is <dtype: 'float32'>\r\nuse tf.assign: 120361.172131 MB/s\r\nuse tf.assign_add: 77835.7152633 MB/s\r\n```", "comments": ["@andydavis1 and @benoitsteiner: do you know if tf.assign mutli-threaded and if setting intra_op_parallelism_threads take any effect on it?\r\n", "Nagging Assignee @zhangyaobit: It has been 263 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]