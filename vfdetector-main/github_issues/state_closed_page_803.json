[{"number": 29449, "title": "RegisterGradient not behaving as expected.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Docker\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: CUDA 9.0\r\n- GPU model and memory: NVIDIA GTX1070 8GB, 32 GB RAM\r\n\r\nSo I'm trying to modify the tensorflow gradients as it flows through the graph, but I'm getting peculiar results. So as a simple example, if I change the \"Relu\" gradient to my \"Custom\" gradient, which should just pass the gradient through, I get an array of all 1's, rather than expected correct gradient. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngraph = tf.get_default_graph()\r\nsess = tf.Session(graph=graph)\r\n\r\n@tf.RegisterGradient(\"Custom\")\r\ndef _no_modifications(unused_op, grad):\r\n    return grad\r\n\r\nc_input = tf.Variable((np.arange(10).reshape(5, 2) - 5), dtype=tf.float32)\r\n\r\nwith graph.gradient_override_map({\"Relu\": \"Custom\"}):\r\n    c_output = tf.nn.relu(c_input)\r\n\r\ngrad = tf.gradients(c_output, c_input)\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\ninput_val, output_val, grad_val = sess.run([c_input, c_output, grad])\r\n\r\ninput_val\r\narray([[-5., -4.],\r\n       [-3., -2.],\r\n       [-1.,  0.],\r\n       [ 1.,  2.],\r\n       [ 3.,  4.]], dtype=float32)\r\n\r\noutput_val\r\narray([[0., 0.],\r\n       [0., 0.],\r\n       [0., 0.],\r\n       [1., 2.],\r\n       [3., 4.]], dtype=float32)\r\n\r\ngrad_val #This is incorrect\r\n[array([[1., 1.],\r\n        [1., 1.],\r\n        [1., 1.],\r\n        [1., 1.],\r\n        [1., 1.]], dtype=float32)]\r\n\r\n\r\n#The correct gradient should be:\r\n[array([[0., 0.],\r\n        [0., 0.],\r\n        [0., 0.],\r\n        [1., 1.],\r\n        [1., 1.]], dtype=float32)]\r\n\r\n```\r\n\r\n\r\n", "comments": []}, {"number": 29448, "title": "[Intel MKL] Adding support to checkout specific PR", "body": "", "comments": ["Good suggestions, @penpornk. Thank you!", "@penpornk can you merge this?\r\n"]}, {"number": 29447, "title": "Update estimator nightly version to pick the checkpoint converter tool", "body": "changes.", "comments": []}, {"number": 29446, "title": "[TF 2.0] CrossShardOptimizer not working with global_step parameter", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Ubuntu 14.04\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow==2.0.0.a\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm trying to run CrossShardOptimizer with tf.optimizers.Adam, however, tf.Optimizer.Adam is updated to v2 (with no global_step_parameter), and CrossShardOptimizer is still in version 1 and still have that global_step parameter. So, CrossShardOptimizer is failing Badly.\r\n**Describe the expected behavior**\r\nThe CrossShardOptimizer shouldn't break.\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python\r\ndef model_fn(features, labels, mode, params):\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=[28, 28, 1]),\r\n        tf.keras.layers.Dense(128),\r\n        tf.keras.layers.Dense(10, activation=\"softmax\")\r\n    ])\r\n    optimizer = None\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.optimizers.Adam(params.get(\"learing_rate\", 1e-3))\r\n        if params.get(\"use_tpu\", True):\r\n          optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)\r\n\r\n    with tf.GradientTape() as tape:\r\n        logits = model(features)\r\n        if mode == tf.estimator.ModeKeys.PREDICT:\r\n            preds = {\r\n                \"predictions\": logits\r\n            }\r\n            return tpu_estimator.TPUEstimatorSpec(mode, predictions=preds)\r\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n            from_logits=True)(labels, logits)\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tpu_estimator.TPUEstimatorSpec(mode, loss=loss)\r\n\r\n    def train_fn():\r\n        assert optimizer is not None\r\n        gradient = tape.gradient(loss, model.trainable_variables)\r\n        global_step = tf.compat.v1.train.get_global_step()\r\n        update_global_step = tf.compat.v1.assign(global_step, global_step + 1, name='update_global_step')\r\n        with tf.control_dependencies([update_global_step]):\r\n          apply_grads = optimizer.apply_gradients(zip(gradient, model.trainable_variables))\r\n        return apply_grads\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\nreturn tpu_estimator.TPUEstimatorSpec(mode, loss=loss, train_op=train_fn())\r\n```\r\nThe complete code can be found at: \r\nhttps://github.com/captain-pool/tf2.0-tpu-sample/blob/master/image_retraining_tpu.py\r\n**Other info / logs**\r\nThis issue gets fixed when:\r\nhttps://github.com/tensorflow/tensorflow/blob/eed4d9c4c4f42c1c5338aa0bdca34c6efb956577/tensorflow/python/tpu/tpu_optimizer.py#L173\r\nline is modified to\r\n`self._opt.apply_gradients(summed_grads_and_vars, name=name)`\r\nand updating `global_step` manually.\r\nLike this:\r\nhttps://github.com/captain-pool/tf2.0-tpu-sample/blob/39d2af5caf1dfbb28d0821b5143a9c534aef6861/image_retraining_tpu.py#L75-L78\r\n\r\n**Do you want to contribute?**\r\nYes.", "comments": ["CC: @vbardiovskyg  @srjoglekar246 @dynamicwebpaige ", "@captain-pool Looks like Complete code link provided above is pointing to error 404.Can you please check and point to the correct link. Thanks!", "@gadagashwini Done :+1: ", "I'm not sure it would be a good idea to support this because of the changes in optimizer would make its behavior confusing. It definitely could use a better error message, though, which I'll add.\r\n\r\nYou can do this instead to sum the gradients across replicas:\r\n```\r\ngradient = [tf.compat.v1.tpu.cross_replica_sum(grad) for grad in gradient]\r\napply_grads = optimizer.apply_gradients(zip(gradient, model.trainable_variables))\r\n```\r\n\r\nIf you want to average the gradients, you'll need to also scale the loss down (divide by number of replicas).\r\n\r\nThe reason it would be confusing with OptimizerV2 is:\r\n- OptimizerV2 doesn't have compute_gradients, where we rescale the loss.\r\n- OptimizerV2 when used with TPUStrategy will already aggregate the gradients across replicas for you, and so it may end up being done twice.", "https://github.com/tensorflow/tensorflow/commit/06349f1973ccdd25c9d12ed495c519ebf6f3d797\r\n\r\nWe throw an exception now with the suggested workaround.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29446\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29446\">No</a>\n"]}, {"number": 29445, "title": "Feature Columns stock example fails on GPU", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No. this is a stock example, see collab notebook here to reproduce \r\nhttps://colab.research.google.com/drive/1O8dCWeYBVjFEax-ZK1XbJE_vfEzB2Ieq\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): '2.0.0-dev20190605'\r\n- Python version: \r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Collab\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nmodel.fit fails in the stock example with the following error:\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Expected D2 of index to be 2 got 3 at position 1\r\n\t [[node sequential/dense_features_6/age_bucketized_X_thal_indicator/SparseCross (defined at <ipython-input-20-bf1fb22dfeb0>:14) ]]\r\n  (1) Invalid argument:  Expected D2 of index to be 2 got 3 at position 1\r\n\t [[node sequential/dense_features_6/age_bucketized_X_thal_indicator/SparseCross (defined at <ipython-input-20-bf1fb22dfeb0>:14) ]]\r\n\t [[sequential/dense_features_6/age_bucketized_X_thal_indicator/SparseToDense/_56]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_keras_scratch_graph_2134]\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1O8dCWeYBVjFEax-ZK1XbJE_vfEzB2Ieq\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Following", "I was able to reproduce the issue on Colab (GPU) with TF 2.0.0-dev20190610 but the code ran fine on TF 2.0.0-alpha0.", "@johnkabler I could not reproduce the issue with the latest `tf-nightly`. This might have been resolved in `tf-nightly-gpu-2.0-preview==2.0.0.dev20190724`. Thanks!\r\n\r\nI am closing the issue. Please feel free to reopen the issue if the bug persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29445\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29445\">No</a>\n"]}, {"number": 29444, "title": "calling import_graph_def with op_dict dependency", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes although I'm far from an expert\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI'm using `import_graph_def` to load and deploy a model stored in an `.pb` file. I'm opening an issue because I saw the following message:\r\n\r\n> calling import_graph_def (from tensorflow.python.framework.importer) with op_dict is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> Please file an issue at https://github.com/tensorflow/tensorflow/issues if you depend on this feature.\r\n\r\nPlease let me know if there is an alternative that won't be removed soon.\r\n\r\n**Will this change the current api? How?**\r\n\r\nMost likely not\r\n\r\n**Who will benefit with this feature?**\r\n\r\nUsers who have such a dependency, including the users of our [CaImAn](https://github.com/flatironinstitute/CaImAn) package. ", "comments": ["Don't pass an op_dict then.", "can anybody help me in this issue \"[ValueError: node 'Placeholder' in input_map does not exist in graph (input_map entry: input_image:0->Placeholder:0)](https://github.com/tensorflow/tensorflow/issues/53593)\"\r\n"]}, {"number": 29443, "title": "TFLite: Slow float global pooling performance", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Raspberry pi, however issue is reproducible on desktop\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v1.12.1-3185-g1a4a0aee1f 1.13.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\nUsing tensor flow lite, global av/max pooling is much slower on float than it is for quantised models.  Using tensor flow 2.0 nightly, together with the instructions here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_integer_quant.ipynb\r\n\r\nUsing the following tf 2.0 code & tflite profiling tool generates the following outputs:\r\n\r\nFloat\r\n```python\r\nimport tensorflow as tf\r\n\r\nout_name = 'mobilenet_v2.tflite'\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\ninput = tf.keras.layers.Input(shape=(32, 32, 3,))\r\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1))(input)\r\nx = tf.keras.layers.Activation('relu')(x)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[x])\r\n\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(out_name, \"wb\").write(tflite_model)`\r\n\r\n\r\n============================== Summary by node type ==============================\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [Node type] \u00a0 [count] \u00a0 [avg ms] \u00a0 \u00a0 [avg %] \u00a0 \u00a0 [cdf %] \u00a0 [mem KB] [times called]\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MEAN \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 1.774 \u00a0 \u00a0 91.774% \u00a0 \u00a0 91.774% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CONV_2D \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.159 \u00a0 \u00a0 8.226% \u00a0 100.000% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\r\n```\r\nQuantized\r\n```python\r\nimport tensorflow as tf\r\n\r\nout_name = 'mobilenet_v2.tflite'\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\ninput = tf.keras.layers.Input(shape=(32, 32, 3,))\r\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1))(input)\r\nx = tf.keras.layers.Activation('relu')(x)\r\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[x])\r\n\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ncifar_train, _ = tf.keras.datasets.cifar10.load_data()\r\nimages = tf.cast(cifar_train[0], tf.float32) / 255.0\r\ncifar_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n  for input_value in cifar_ds.take(100):\r\n    yield [input_value]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model = converter.convert()\r\nopen(out_name, \"wb\").write(tflite_model)\r\n\r\n============================== Summary by node type ==============================\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [Node type] \u00a0 [count] \u00a0 [avg ms] \u00a0 \u00a0 [avg %] \u00a0 \u00a0 [cdf %] \u00a0 [mem KB] [times called]\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CONV_2D \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 4.625 \u00a0 \u00a0 95.165% \u00a0 \u00a0 95.165% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MEAN \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.226 \u00a0 \u00a0 4.650% \u00a0 \u00a0 99.815% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 QUANTIZE \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.009 \u00a0 \u00a0 0.185% \u00a0 100.000% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DEQUANTIZE \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.000 \u00a0 \u00a0 0.000% \u00a0 100.000% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n```\r\nI had a poke through the source code, I could find the arm neon accelerated pooling code for integers here:\r\n```\r\ntensorflow/lite/kernels/\u2068internal/optimized\u2069/integer_ops\u2069/pooling.h\r\n```\r\nIt seems to me that there are no neon optimisations for pooling with float datatype?\r\n\r\n", "comments": ["So I've done a bit more digging, I was able to get much better performance by using the average pooling function.  keras.GlobalAveragePooling2D by default uses mean:\r\n```\r\nbackend.mean(inputs, axis=[2, 3])\r\n```\r\n\r\nExample using tf.nn.avg_pool2d:\r\n```\r\nimport tensorflow as tf\r\n\r\nout_name = 'mobilenet_v2.tflite'\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\n\r\ninput = tf.keras.layers.Input(shape=(32, 32, 3,))\r\nx = tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1))(input)\r\nx = tf.keras.layers.Activation('relu')(x)\r\n\r\nx = tf.nn.avg_pool2d(x, ksize=(x.shape[1], x.shape[2]), padding='VALID', strides=(1, 1))\r\nx = tf.reshape(x, (1, x.shape[-1]))\r\n\r\nmodel = tf.keras.Model(inputs=[input], outputs=[x])\r\n\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ncifar_train, _ = tf.keras.datasets.cifar10.load_data()\r\nimages = tf.cast(cifar_train[0], tf.float32) / 255.0\r\ncifar_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\ndef representative_data_gen():\r\n  for input_value in cifar_ds.take(100):\r\n    yield [input_value]\r\n\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model = converter.convert()\r\nopen(out_name, \"wb\").write(tflite_model)\r\n```\r\n\r\n\r\n\r\n\r\n============================== Summary by node type ==============================\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [Node type] \u00a0 [count] \u00a0 [avg ms] \u00a0 \u00a0 [avg %] \u00a0 \u00a0 [cdf %] \u00a0 [mem KB] [times called]\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CONV_2D \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.160 \u00a0 \u00a0 75.472% \u00a0 \u00a0 75.472% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 AVERAGE_POOL_2D \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.052 \u00a0 \u00a0 24.528% \u00a0 100.000% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RESHAPE \u00a0 \u00a0 \u00a0 \u00a0 1 \u00a0 \u00a0 0.000 \u00a0 \u00a0 0.000% \u00a0 100.000% \u00a0 \u00a0 0.000 \u00a0 \u00a0 \u00a0 \u00a0 1\r\n\r\n", "First, AVERAGE_POOL_2D (corresponds to tf.nn.avg_pool2d) has been optimized for the float path while MEAN (corresponds to GlobalAveragePooling2D) has not yet been optimized in tflite.\r\n\r\nSecond, your code of converting the tflite model using AVERAGE_POOL_2D does not seem right. You need to set the flag for optimization: converter.optimizations = [tf.lite.Optimize.DEFAULT]. Without it, you'll end up with undefined state, and most likely it will be a float model. ", "> First, AVERAGE_POOL_2D (corresponds to tf.nn.avg_pool2d) has been optimized for the float path while MEAN (corresponds to GlobalAveragePooling2D) has not yet been optimized in tflite.\r\n\r\n@lu-wang-g @ymodak What do you think about converting `TFL_MeanOp` to `TFL_AveragePool2DOp` and `TFL_ReshapeOp` in the new MLIR based converter for the case of `GlobalAveragePooling2D`?\r\nThis would be very helpful for users trying to convert Keras models to TFLite since it is quite common to use `GlobalAveragePooling2D` or `tf.reduce_mean` in models.", "Thanks to everyone! I was considering about removing squeeze and excite block from the network as it is too slow. But, after I switched `GlobalAveragePoolin2D` to `tf.nn.avg_pool2d`, it becomes faster \ud83d\udc4d "]}, {"number": 29442, "title": "[2.0a0] different names and initial values of variables with and without tf.function", "body": "**System information**\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0alpha0\r\n- Python version: 3.6.5\r\n\r\n**Code to reproduce the issue**\r\n<pre>\r\n# -*- coding: utf-8 -*-\r\n# @Author  : Lin Lan (ryan.linlan@gmail.com)\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(123)\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(100)\r\n        self.dense2 = tf.keras.layers.Dense(100)\r\n        self.dense3 = tf.keras.layers.Dense(100)\r\n\r\n    # @tf.function\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        x = self.dense2(x)\r\n        outputs = self.dense3(x)\r\n        return outputs\r\n\r\n\r\nmodel = Model()\r\nmodel(tf.random.normal((1, 10), dtype=tf.float32))\r\nfor weight in model.trainable_weights:\r\n    print(weight.name)\r\n</pre>\r\n\r\n**Other info / logs**\r\nWithout `tf.function`, the output is\r\n<pre>\r\nmodel/dense/kernel:0\r\nmodel/dense/bias:0\r\nmodel/dense_1/kernel:0\r\nmodel/dense_1/bias:0\r\nmodel/dense_2/kernel:0\r\nmodel/dense_2/bias:0\r\n</pre>\r\n\r\nWith `tf.function`, the output is\r\n<pre>\r\ndense/kernel:0\r\ndense/bias:0\r\ndense_1/kernel:0\r\ndense_1/bias:0\r\ndense_2/kernel:0\r\ndense_2/bias:0\r\n</pre>\r\n\r\nAlso, the initial values also are different with and without `tf.function`.\r\n\r\nI guess the difference is because the code generated by `tf.function` executes in a different graph. But I think most users would expect consistent behaviors between eager and graph modes.\r\n\r\nA workaround is to first call `model(inputs)` to initialize weights in eager mode, and then replace the method with `model.call = tf.function(model.call)`.\r\n", "comments": ["@fchollet @alextp Would it be possible for either tf.function to respect the outer name scopes, or for Keras to communicate the desired name scope to tf.function?", "Names in tf2 are no longer load-bearing, so getting slightly different names for the same variables is fine. There is still checkpoint compatibility if you use tf.train.Chekpoint and tf.saved_model.save.\r\n\r\nSo I think this is not ideal but harmless.", "@alextp \r\n- Regarding to variable names (actually, omitting parent name scope), it may make tensorboard graph display badly organized.\r\n- Except for names, the initial values of variables are also different with and without `tf.function`, which will result in that the eager and graph modes have inconsistent behaviors with the same random seed.", "Re the initial values, they will be the same once initializers switch to the new tf random ops (the existing ones are ~impossible to make deterministic across eager and graphs).\r\n\r\nRe the names, tensorboard will likely change to better display tf2 graphs. Stay tuned!"]}, {"number": 29441, "title": "[TF 2.0 API Docs]tf.data.experimental.shuffle_and_repeat", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/shuffle_and_repeat\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nNo.\r\nit has warning and it is kinda abstract on when to use the function\r\n\r\n\r\n### Raises listed and defined\r\nNo.\r\nyet while comparing  dataset.shuffle(buffer_size, reshuffle_each_iteration=True).repeat(count).the difference is in the actions performed on the datasets. \r\n\r\n\r\n### Usage example\r\nNo.\r\nthere is need of an example to clearly explain the difference between the two shuffles\r\n\r\n\r\n", "comments": ["The docs are updated in TF 2.4.1 api docs.\r\nSince [`tf.data.experimental.shuffle_and_repeat`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat) function is deprecated we may want to refer [`tf.data.Dataset.shuffle(buffer_size, seed)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) and [`tf.data.Dataset.repeat(count)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) to know more about the usage.\r\nThanks!"]}, {"number": 29440, "title": "add allow_pickle for reuters", "body": "Starting with `numpy 1.16.3` the default of `allow_picke` in `np.load` has changed from `True` to `False`.\r\n\r\nThis means that all occurrences of \r\n\r\n```\r\nwith np.load(path) as f:\r\n```\r\n\r\nin Keras datasets need to be changed to\r\n\r\n```\r\nwith np.load(path, allow_pickle=True) as f:\r\n```\r\n\r\notherwise we get\r\n\r\n```\r\nValueError: Object arrays cannot be loaded when allow_pickle=False\r\n```\r\n\r\nSee also https://github.com/tensorflow/tensorflow/commit /79a8d5cdad942b9853aa70b59441983b42a8aeb3 which did this for `imdb`.\r\nThis PR does this for `reuters`.", "comments": ["Hi, any news on this? Thanks!", "@skeydan thank you for your contribution , can you please add some description to the PR why the change is required ?", "Sorry - description updated :-)", "you're welcome :-)"]}, {"number": 29439, "title": "Unittest and test_session interaction", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.1 / 1.13.1 / 1.14.0rc0\r\n- Python version: 3.5 / 3.6 / 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.0.130\r\n- GPU model and memory: 7.5.0\r\n\r\nEnvironment capture available at: https://pastebin.com/N26BUeSy\r\n\r\n**Describe the current behavior**\r\nAdditional \"ghost\" tests are being run but skipped when using unittest with Tensorflow TestCase class. This behavior is present in 1.12.1. When upgrading to 1.13.1 or 1.14.0rc0, the tests are being skipped entirely as the \"ghost\" test is in regards to the test_session method that you have within the tensorflow.python.framework.testutils and unittest believes that the tests are not actually tests.\r\n\r\n**Describe the expected behavior**\r\nNo \"ghost\" tests should be run at all in 1.12.1 and the tests work in 1.13.1 and 1.14.0rc0.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport unittest\r\n\r\nprint(tf.__version__)\r\n\r\ndef get_entry_np(t, indices_d1, indices_d2, batch_size):\r\n    result = np.zeros(batch_size)\r\n    for i in range(batch_size):\r\n        result[i] = t[i, indices_d1[i], indices_d2[i]]\r\n    return result\r\n    \r\n\r\ndef get_entry_tf(t, indices_d1, indices_d2, batch_size):\r\n    indices = tf.stack([tf.range(batch_size), indices_d1, indices_d2], axis=1)\r\n    return tf.gather_nd(t, indices)\r\n\r\n## Start of region of interest\r\n# Please enable and disable this region with Tensorflow 1.12.1 and then with 1.13.1 or 1.14.0rc0 and the behaviour will be seen   \r\ntry:\r\n    delattr(tf.test.TestCase,'test_session')\r\nexcept AttributeError:\r\n    pass\r\n\r\nclass OwnTestCase(tf.test.TestCase):\r\n    pass\r\n## End of region of interest\r\nclass TestCaseTest(tf.test.TestCase):\r\n        \r\n    def test_get_entry(self):\r\n        success = True\r\n        for _ in range(10):\r\n            # sample input\r\n            batch_size, d1, d2 = map(int, np.random.randint(low=2, high=100, size=3))\r\n            test_input = np.random.random([batch_size, d1, d2])\r\n            test_indices_d1 = np.random.randint(low=0, high=d1-1, size=[batch_size])\r\n            test_indices_d2 = np.random.randint(low=0, high=d2-1, size=[batch_size])\r\n            # evaluate the numpy version\r\n            test_result = get_entry_np(test_input, test_indices_d1, test_indices_d2, batch_size)\r\n            # evaluate the tensorflow version\r\n            with self.cached_session() as sess:\r\n                tf_input = tf.constant(test_input, dtype=tf.float32)\r\n                tf_indices_d1 = tf.constant(test_indices_d1, dtype=tf.int32)\r\n                tf_indices_d2 = tf.constant(test_indices_d2, dtype=tf.int32)\r\n                tf_result = get_entry_tf(tf_input, tf_indices_d1, tf_indices_d2, batch_size)\r\n                tf_result = sess.run(tf_result)\r\n                # check that outputs are similar\r\n                success = success and np.allclose(test_result, tf_result)\r\n    \r\n        self.assertEqual(success, True)\r\n```", "comments": ["To expand on this, unittest is search for method names and finds tf.test.TestCast.test_session. This is treated as a test which is erroneous and so gets logged as \"Not a test.\". However it also causes actual tests to be ignored as well in this manner. Deleting the method entirely allows tests to run as the above code shows.", "Why would it cause actual tests to be ignored? I'm confused.", "What appears to be happening is the unittest framework is being used to search for methods beginning with \"test\". The test_session method is found to be one of these erroneously and causes tests to be skipped. We haven't been able to figure out why this wasn't a problem in the past. Our tests in TF 1.12 work correctly but having upgraded to 1.13 (and then tried 1.14 rc0) they acquire this skipping behaviour.", "@gunan do you know who maintains TestCase?", "I have been aware of the issue around \"test_session\", but afaik it has been that way for a very very long time. Here is the line that skips \"test_session\"\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L1747\r\n\r\nA little investigation showed that we moved around the \"skip\" logic recently with this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/f9f50b6cf831cdfef15d952152f43ba6542a14ad\r\nAnd I think that may be why you are seeing your tests that have the name \".test_session\" skipped.\r\nPinging @eddie-zhou @mrry for the change.\r\n\r\nFor the resolution, I think this is just the fallout from a small bad design decision we had within TensorFlowTestCase. We should document this behaviour to avoid the pain you went through. But my recommendation for the resolution would be to rename the tests, but have this behaviour documented. Maybe instead of \"Not a test\" the message can say \"due to `test_session` method in tensorflowTestCase, all tests with this name are skipped. Please rename your tests\"?", "I believe @gunan is correct, and his suggestion seems reasonable.", "We can try something like that. We're changing tests around a bit anyway so we can do that. The issue we noticed was that tests were getting skipped that didn't even use test_session nor have that in their names at all. We found it hard to correlate behaviour with what methods we were calling. Our solution of deleting test_session from the class works but of course is horrid. Thanks for the help.", "I agree with you that it is not a great solution.\r\nThankfully, test_session is deprecated and is planned to be removed. At which point we can remove it completely from the TestCase class and remove the hack. Sorry for the inconvenience!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29439\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29439\">No</a>\n", "For someone who might find this in the future by Googling:\r\n\r\nIf you are using pytest, you could write a `conftest.py` (in the project root, etc.) file including the following hook, to have `test_session`s **not collected** rather than *skipped*:\r\n\r\n```python\r\ndef pytest_collection_modifyitems(session, config, items):\r\n  \"\"\"Do not collect TensorFlowTestCase.test_sesion as a test case.\"\"\"\r\n  items[:] = [item for item in items if not (\r\n    item.location[0].endswith('test_util.py') and item.name == 'test_session')]\r\n```\r\n\r\nReference: https://docs.pytest.org/en/latest/reference.html", "Unfortunately, TF is not using pytest, our testing libraries are derived from [python unittest](https://docs.python.org/2/library/unittest.html). So I do not think the above solution will work in this case.", "> What appears to be happening is the unittest framework is being used to search for methods beginning with \"test\". The test_session method is found to be one of these erroneously and causes tests to be skipped. We haven't been able to figure out why this wasn't a problem in the past. Our tests in TF 1.12 work correctly but having upgraded to 1.13 (and then tried 1.14 rc0) they acquire this skipping behaviour.\r\n\r\nYes this appears to have happened "]}, {"number": 29438, "title": "TimeDistributed wrapper around DepthwiseConv2D broken (AttributeError) in 1.13", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- Linux Ubuntu 18.04:\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from pip3:\r\n- TensorFlow version b'v1.13.1-0-g6612da8951' 1.13.1\r\n- Python version:\r\n- CUDA/cuDNN version: 10.0.130 / 7.4.1\r\n- GPU model and memory: NVIDIA K80; 11GB\r\n\r\n**Describe the current behavior**\r\nTimeDistributed wrapper around DepthwiseConv2D fails with AttributeError: 'tuple' object has no attribute 'dims'\r\n\r\n**Describe the expected behavior**\r\nWrapper should succesfully apply to layer; previously worked in TF 1.11.0.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Conv2D, DepthwiseConv2D, TimeDistributed\r\n\r\ntest_td_input = Input(shape=(None, 1, 128, 8))\r\nTimeDistributed(DepthwiseConv2D(depth_multiplier=1,\r\n                                      kernel_size=(1, 4), strides=(1, 1)))(test_td_input)\r\n\r\n\r\n**Other info / logs**\r\nTraceback (most recent call last):\r\n  File \"time_distributed_bug.py\", line 12, in <module>\r\n    kernel_size=(1, 4), strides=(1, 1)))(test_td_input))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 538, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1603, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py\", line 216, in build\r\n    self.layer.build(tuple(child_input_shape))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 1811, in build\r\n    if input_shape.dims[channel_axis].value is None:\r\nAttributeError: 'tuple' object has no attribute 'dims'\r\n\r\n```", "comments": ["I am able to reproduce the issue with tf 1.13.1 on colab. Thanks!", "Thanks for reporting this.\r\nRoughly looking through stack trace, I think the issue is that TimeDistributed layer does not wrap input shape as TensorShape before calling Conv2D.build\r\n\r\nI will see if I can fix it", "Sorry I meant DepthWiseConv2D", "Fixed and should be available in tf nightly. Let me know if it doesn't work for you. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29438\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29438\">No</a>\n"]}, {"number": 29437, "title": "Nesting Variables in tensorflow", "body": "Hi,\r\nHope this is simple to answer for experts. Why cant we nest variables in tensorflow. For example, in below snippet, why w1 will not be updated by optimizer?\r\n\r\n```\r\ndef l():\r\n    x_placeholder=tf.placeholder(shape=(5,4),dtype=tf.float64)\r\n    #x=tf.cast(x,tf.float32)\r\n    w1=tf.Variable(initial_value=tf.random_normal_initializer(dtype=tf.float64)((4,2)))\r\n    w=tf.Variable(w1+w1)\r\n    b=tf.Variable(initial_value=tf.random_normal_initializer(dtype=tf.float64)((2,)))\r\n    logits=tf.matmul(x_placeholder,w)+b\r\n    loss=tf.reduce_sum(logits)\r\n    opt=tf.train.AdamOptimizer(0.02)    \r\n    g_v=opt.compute_gradients(loss,var_list=[w1,w,b])\r\n    optim=opt.apply_gradients(g_v)\r\n    \r\n    return x_placeholder,optim,w1,w,b\r\n\r\nx_feed=np.random.randn(5,4)\r\nopt=tf.train.AdamOptimizer(0.02)\r\nx_placeholder,optim,w1,w,b=l()\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  \r\n  print(\"initial values for variables w1 w b\")\r\n  wt1,wt2,wt3=sess.run([w1,w,b],feed_dict={x_placeholder:x_feed}) \r\n  print(wt1)\r\n  print(wt2)\r\n  print(wt3)\r\n  out=sess.run([optim],feed_dict={x_placeholder:x_feed})\r\n  print(\"after optimizer run, variables w1 w b\")\r\n  wt4,wt2,wt3=sess.run([w1,w,b],feed_dict={x_placeholder:x_feed})\r\n  print(wt1)\r\n  print(wt2)\r\n  print(wt3)\r\n  print(np.all(wt1==wt4))\r\n```\r\n", "comments": ["Closing this s this is no longer an issue."]}, {"number": 29436, "title": "Error using deepbinner:  ImportError: libcuda.so.1: cannot open shared object file: No such   file or directory   Failed to load the native TensorFlow runtime.", "body": "Hello.\r\nI'm trying to use deepbinner software for demultiplexing native barcodes that separate by patients after MinION sequencing. I've obtained this error. Has anyone got any idea about what's happening? Thank you!\r\n\r\n`(Deepbinner) [ugm@et8 Lecturas_30_05_2019]$ deepbinner classify  \r\n--native fast5_pass/ > classifications\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in  \r\n<module>\r\n     from tensorflow.python.pywrap_tensorflow_internal import *\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in  \r\n<module>\r\n     _pywrap_tensorflow_internal = swig_import_helper()\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in  \r\nswig_import_helper\r\n     _mod = imp.load_module('_pywrap_tensorflow_internal', fp,  \r\npathname, description)\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/imp.py\",  \r\nline 243, in load_module\r\n     return load_dynamic(name, filename, file)\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/imp.py\",  \r\nline 343, in load_dynamic\r\n     return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such  \r\nfile or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n   File \"/programas/anaconda/3-4.4.0/envs/Deepbinner/bin/deepbinner\",  \r\nline 10, in <module>\r\n     sys.exit(main())\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/deepbinner/deepbinner.py\", line 59, in  \r\nmain\r\n     from .classify import classify\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/deepbinner/classify.py\", line 24, in  \r\n<module>\r\n     from keras.models import load_model\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/keras/__init__.py\", line 3, in  \r\n<module>\r\n     from . import utils\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in  \r\n<module>\r\n     from . import conv_utils\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in  \r\n<module>\r\n     from .. import backend as K\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/keras/backend/__init__.py\", line 89, in  \r\n<module>\r\n     from .tensorflow_backend import *\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in  \r\n<module>\r\n     import tensorflow as tf\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in  \r\n<module>\r\n     from tensorflow.python import pywrap_tensorflow  # pylint:  \r\ndisable=unused-import\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in  \r\n<module>\r\n     from tensorflow.python import pywrap_tensorflow\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in  \r\n<module>\r\n     raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in  \r\n<module>\r\n     from tensorflow.python.pywrap_tensorflow_internal import *\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in  \r\n<module>\r\n     _pywrap_tensorflow_internal = swig_import_helper()\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in  \r\nswig_import_helper\r\n     _mod = imp.load_module('_pywrap_tensorflow_internal', fp,  \r\npathname, description)\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/imp.py\",  \r\nline 243, in load_module\r\n     return load_dynamic(name, filename, file)\r\n   File  \r\n\"/programas/anaconda/3-4.4.0/envs/Deepbinner/lib/python3.6/imp.py\",  \r\nline 343, in load_dynamic\r\n     return _load(spec)\r\nImportError: libcuda.so.1: cannot open shared object file: No such  \r\nfile or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.`\r\n", "comments": ["Please provide following information; Thanks!\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 29435, "title": "Updated doc for tf.keras.backend.maximum", "body": "I added some information concerning the returns and a basic example.\r\nAny improvments or ideas on that?", "comments": ["@lufol Could you please resolve the conflicts? Thanks!", "@gbaned Done \u2705"]}, {"number": 29434, "title": "Python3 Issue with Keras Custom Layer", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary on tx2. (followed this https://devtalk.nvidia.com/default/topic/1038957/jetson-tx2/tensorflow-for-jetson-tx2-/) \r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda-10.0 , cudnn7.3\r\n- GPU model and memory: TX2 (Nvidia jetson)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to load a model (hdf5) which has a keras custom layer. \r\nI also see the same error when creating a keras model from scratch. Conv2D however works alright. Note I use python3. Is there a modification needed for custom layer? \r\n\r\nHowever, the script fails with the following error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\", line 558, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\", line 558, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py\", line 61, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got 1\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"try.py\", line 82, in <module>\r\n    out = NetVLADLayer( num_clusters=16 )(input_img)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 538, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1603, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"try.py\", line 21, in build\r\n    trainable=True )\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 349, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py\", line 607, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 145, in make_variable\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 213, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 176, in _variable_v1_call\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 155, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2488, in default_variable_creator\r\n    import_scope=import_scope)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 217, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 294, in __init__\r\n    constraint=constraint)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 406, in _init_from_args\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\", line 127, in <lambda>\r\n    shape, dtype=dtype, partition_info=partition_info)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py\", line 266, in __call__\r\n    shape, self.minval, self.maxval, dtype, seed=self.seed)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\", line 239, in random_uniform\r\n    shape = _ShapeTensor(shape)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\", line 44, in _ShapeTensor\r\n    return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1039, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1097, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1175, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 245, in constant\r\n    allow_broadcast=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\", line 283, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py\", line 562, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (1, 1, Dimension(256), 16). Consider casting elements to a supported type.\r\n\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://gist.github.com/mpkuse/c2daacc3a5b8e07697ea7c595f900413\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I could verify the issue is encountered for both Python 2 and 3. Created a PR #29567 for the fix.", "Since I am running tensorflow with precompiled binaries (from Nvidia) on tx2 is there a simpler way to get your fix to my environment? Will it be necessary for me to recompile tensorflow again? \r\n\r\nI understand you have changed the following, in file ` tensorflow/python/ops/random_ops.py` : \r\n```\r\nfrom tensorflow.python.framework import tensor_shape\r\n\r\n    if isinstance(shape, (tuple, list)):\r\n      shape = [val.value if isinstance(\r\n          val, tensor_shape.Dimension) else val for val in shape]\r\n```\r\n\r\nCan you tell me how (if possible) I can patch my tensorflow? ", "@mpkuse Not familiar with your environment. You can modify the python script in place though it will be replaced if installed by pip again.", "These changes worked for me on Nvidia Jetson TX2. Thanks a ton @yongtang ", "Closing the issue since its resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29434\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29434\">No</a>\n"]}, {"number": 29433, "title": "Inmediate overfitting when using Eager Execution", "body": "**System information**\r\n- OS Platform and Distribution: Google Colab in GPU mode\r\n- TensorFlow version: 1.13.1\r\n- Python version: Python 3.6.7\r\n\r\nI am feeding a 3D Resnet with 3D images from a tfrecord file. Using exactly the same code, I obtain very different results when using Eager Execution. In fact, when I use eager execution, the model has 99% training accuracy since the first epoch, with just 20% validation accuracy. When not using eager execution, the behavior is completely normal, gradually incrementing accuracy with each epoch.\r\n\r\nThe 3D Resnet model I am using is from this rep: https://github.com/JihongJu/keras-resnet3d\r\n\r\n**Code to reproduce the issue**\r\n````\r\nBATCH_SIZE = 8\r\nSTEPS_PER_EPOCH = int(n_training_samples / BATCH_SIZE)\r\nVALIDATION_STEPS = int(n_val_samples / BATCH_SIZE)\r\n\r\nimage_tensor, label_tensor = dataset_parser(training_tfrec, BATCH_SIZE)\r\nmodel = Resnet3DBuilder.build_resnet_18(\r\n    input_shape=IMG_SHAPE,\r\n    num_outputs=N_CLASSES,\r\n    reg_factor=0.01\r\n)\r\n\r\noptimizer = keras.optimizers.Adam(lr=0.0001, decay=1e-6)\r\nmodel.compile(optimizer=optimizer, \r\n              loss='categorical_crossentropy', \r\n              metrics=['acc'])\r\n\r\nval_tensor = dataset_parser(validation_tfrec, BATCH_SIZE)\r\n\r\nhistory = model.fit(x=image_tensor, y=label_tensor, \r\n                    epochs=10, steps_per_epoch=STEPS_PER_EPOCH, \r\n                    validation_data=val_tensor, validation_steps=VALIDATION_STEPS)\r\n\r\n````\r\n\r\nI realize this code is probably not enough, but maybe there are some clear reasons why this might be happening, and someone can help.\r\n", "comments": ["It is difficult to figure out what the cause might be at this point. Perhaps you can provide a complete code snippet and output for both versions, google colab gist will be great. Thanks!", "I'm sorry but I was in a hurry and went back to graph execution to avoid the problem. Now I can't find the version of my Colab notebook where I had the code ready for eager execution. It would be nice to know what was happening, but I am afraid I cannot provide any more code. Thanks anyway."]}, {"number": 29432, "title": "4e0cd04 breaks r1.14 and master", "body": "Kindly ping @aaroey for 4e0cd04.\r\n\r\n```\r\nERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:4164:1: C++ compilation of rule '//tensorflow/core/kernels:fused_batch_norm_op' failed (Exit 1)\r\ntensorflow/core/kernels/fused_batch_norm_op.cc: In member function 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool)':\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:417:44: error: there are no arguments to 'BatchnormSpatialPersistentEnabled' that depend on a template parameter, so a declaration of 'BatchnormSpatialPersistentEnabled' must be available [-fpermissive]\r\n         (BatchnormSpatialPersistentEnabled() &&\r\n                                            ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:417:44: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)\r\ntensorflow/core/kernels/fused_batch_norm_op.cc: In instantiation of 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]':\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:780:46:   required from 'void tensorflow::FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(tensorflow::OpKernelContext*, bool) [with Device = Eigen::GpuDevice; T = Eigen::half; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:1092:1:   required from here\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:417:43: error: 'BatchnormSpatialPersistentEnabled' was not declared in this scope\r\n         (BatchnormSpatialPersistentEnabled() &&\r\n          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/framework/allocator.h:28,\r\n                 from ./tensorflow/core/framework/tensor.h:22,\r\n                 from ./tensorflow/core/util/tensor_format.h:22,\r\n                 from ./tensorflow/core/kernels/conv_2d.h:23,\r\n                 from tensorflow/core/kernels/fused_batch_norm_op.cc:21:\r\n./tensorflow/core/platform/default/logging.h:112:5: error: invalid use of 'auto'\r\n   (([](int level, const char* fname) {                                      \\\r\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n     static const bool vmodule_activated =                                   \\\r\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n         ::tensorflow::internal::LogMessage::VmoduleActivated(fname, level); \\\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n     return vmodule_activated;                                               \\\r\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   })(lvl, __FILE__))\r\n   ~~^~~~~~~~~~~~~~~~\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:117:20: note: in expansion of macro 'VLOG_IS_ON'\r\n   TF_PREDICT_TRUE(!VLOG_IS_ON(level))                            \\\r\n                    ^~~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:426:5: note: in expansion of macro 'VLOG'\r\n     VLOG(2) << \"FusedBatchNorm:\"\r\n     ^\r\nIn file included from /usr/include/c++/6/bits/move.h:57:0,\r\n                 from /usr/include/c++/6/bits/nested_exception.h:40,\r\n                 from /usr/include/c++/6/exception:173,\r\n                 from /usr/include/c++/6/ios:39,\r\n                 from /usr/include/c++/6/istream:38,\r\n                 from /usr/include/c++/6/sstream:38,\r\n                 from /usr/include/c++/6/complex:45,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:43,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/conv_2d.h:19,\r\n                 from tensorflow/core/kernels/fused_batch_norm_op.cc:21:\r\n/usr/include/c++/6/type_traits: In instantiation of 'class std::result_of<tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>&()>':\r\n/usr/include/c++/6/functional:1913:9:   required by substitution of 'template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>; <template-parameter-1-2> = void; <template-parameter-1-3> = <missing>]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:532:5:   required from 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:780:46:   required from 'void tensorflow::FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(tensorflow::OpKernelContext*, bool) [with Device = Eigen::GpuDevice; T = Eigen::half; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:1092:1:   required from here\r\n/usr/include/c++/6/type_traits:2496:12: error: invalid use of incomplete type 'std::__result_of_impl<false, false, tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>&>::type'\r\n     struct result_of<_Functor(_ArgTypes...)>\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/c++/6/type_traits:2291:12: note: declaration of 'std::__result_of_impl<false, false, tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>&>::type'\r\n     struct __result_of_success : __success_type<_Tp>\r\n            ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc: In instantiation of 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]':\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:780:46:   required from 'void tensorflow::FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(tensorflow::OpKernelContext*, bool) [with Device = Eigen::GpuDevice; T = Eigen::half; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:1092:1:   required from here\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:532:5: error: conversion from 'tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>' to non-scalar type 'std::function<void()>' requested\r\n     };\r\n     ^\r\ntensorflow/core/kernels/fused_batch_norm_op.cc: In instantiation of 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]':\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:780:46:   required from 'void tensorflow::FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(tensorflow::OpKernelContext*, bool) [with Device = Eigen::GpuDevice; T = float; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:1092:1:   required from here\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:417:43: error: 'BatchnormSpatialPersistentEnabled' was not declared in this scope\r\n         (BatchnormSpatialPersistentEnabled() &&\r\n          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~\r\nIn file included from ./tensorflow/core/platform/default/logging.h:24:0,\r\n                 from ./tensorflow/core/platform/logging.h:25,\r\n                 from ./tensorflow/core/framework/allocator.h:28,\r\n                 from ./tensorflow/core/framework/tensor.h:22,\r\n                 from ./tensorflow/core/util/tensor_format.h:22,\r\n                 from ./tensorflow/core/kernels/conv_2d.h:23,\r\n                 from tensorflow/core/kernels/fused_batch_norm_op.cc:21:\r\n./tensorflow/core/platform/default/logging.h:112:5: error: invalid use of 'auto'\r\n   (([](int level, const char* fname) {                                      \\\r\n   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n     static const bool vmodule_activated =                                   \\\r\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n         ::tensorflow::internal::LogMessage::VmoduleActivated(fname, level); \\\r\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n     return vmodule_activated;                                               \\\r\n     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n   })(lvl, __FILE__))\r\n   ~~^~~~~~~~~~~~~~~~\r\n./tensorflow/core/platform/macros.h:88:49: note: in definition of macro 'TF_PREDICT_TRUE'\r\n #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n                                                 ^\r\n./tensorflow/core/platform/default/logging.h:117:20: note: in expansion of macro 'VLOG_IS_ON'\r\n   TF_PREDICT_TRUE(!VLOG_IS_ON(level))                            \\\r\n                    ^~~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:426:5: note: in expansion of macro 'VLOG'\r\n     VLOG(2) << \"FusedBatchNorm:\"\r\n     ^\r\nIn file included from /usr/include/c++/6/bits/move.h:57:0,\r\n                 from /usr/include/c++/6/bits/nested_exception.h:40,\r\n                 from /usr/include/c++/6/exception:173,\r\n                 from /usr/include/c++/6/ios:39,\r\n                 from /usr/include/c++/6/istream:38,\r\n                 from /usr/include/c++/6/sstream:38,\r\n                 from /usr/include/c++/6/complex:45,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:43,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/conv_2d.h:19,\r\n                 from tensorflow/core/kernels/fused_batch_norm_op.cc:21:\r\n/usr/include/c++/6/type_traits: In instantiation of 'class std::result_of<tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>&()>':\r\n/usr/include/c++/6/functional:1913:9:   required by substitution of 'template<class _Functor, class, class> std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>; <template-parameter-1-2> = void; <template-parameter-1-3> = <missing>]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:532:5:   required from 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:780:46:   required from 'void tensorflow::FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(tensorflow::OpKernelContext*, bool) [with Device = Eigen::GpuDevice; T = float; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:1092:1:   required from here\r\n/usr/include/c++/6/type_traits:2496:12: error: invalid use of incomplete type 'std::__result_of_impl<false, false, tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>&>::type'\r\n     struct result_of<_Functor(_ArgTypes...)>\r\n            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/c++/6/type_traits:2291:12: note: declaration of 'std::__result_of_impl<false, false, tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>&>::type'\r\n     struct __result_of_success : __success_type<_Tp>\r\n            ^~~~~~~~~~~~~~~~~~~\r\ntensorflow/core/kernels/fused_batch_norm_op.cc: In instantiation of 'void tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]':\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:780:46:   required from 'void tensorflow::FusedBatchNormOpBase<Device, T, U>::ComputeWithReservedSpace(tensorflow::OpKernelContext*, bool) [with Device = Eigen::GpuDevice; T = float; U = float]'\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:1092:1:   required from here\r\ntensorflow/core/kernels/fused_batch_norm_op.cc:532:5: error: conversion from 'tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>' to non-scalar type 'std::function<void()>' requested\r\n     };\r\n     ^\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:74:0,\r\n                 from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/core/kernels/conv_2d.h:19,\r\n                 from tensorflow/core/kernels/fused_batch_norm_op.cc:21:\r\n/usr/include/c++/6/functional: At global scope:\r\n/usr/include/c++/6/functional:2106:7: error: 'std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>; <template-parameter-2-2> = void; <template-parameter-2-3> = void; _Res = const stream_executor::DeviceMemory<float>&; _ArgTypes = {}]', declared using local type 'tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = Eigen::half; U = float]::<lambda()>', is used but never defined [-fpermissive]\r\n       function<_Res(_ArgTypes...)>::\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/usr/include/c++/6/functional:2106:7: error: 'std::function<_Res(_ArgTypes ...)>::function(_Functor) [with _Functor = tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>; <template-parameter-2-2> = void; <template-parameter-2-3> = void; _Res = const stream_executor::DeviceMemory<float>&; _ArgTypes = {}]', declared using local type 'tensorflow::functor::FusedBatchNorm<Eigen::GpuDevice, T, U>::operator()(tensorflow::OpKernelContext*, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, const tensorflow::Tensor&, U, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::TensorFormat, stream_executor::ScratchAllocator*, stream_executor::ScratchAllocator*, bool) [with T = float; U = float]::<lambda()>', is used but never defined [-fpermissive]\r\nIn file included from ./tensorflow/core/kernels/conv_2d.h:23:0,\r\n                 from tensorflow/core/kernels/fused_batch_norm_op.cc:21:\r\n./tensorflow/core/util/tensor_format.h: In function 'int tensorflow::GetTensorSpatialDims(int, tensorflow::TensorFormat)':\r\n./tensorflow/core/util/tensor_format.h:127:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\n./tensorflow/core/util/tensor_format.h: In function 'int tensorflow::GetTensorDimsFromSpatialDims(int, tensorflow::TensorFormat)':\r\n./tensorflow/core/util/tensor_format.h:151:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 271.568s, Critical Path: 137.87s\r\nINFO: 13455 processes: 3220 remote cache hit, 212 local, 10023 remote.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```", "comments": ["Ping @bananabowl for r1.14. Does it poison r1.14-rc1?", "@bananabowl it seems we need afeacf3083527681d925f83fae8d983acc17606a as well, @ezhulenev to confirm.", "Yes, that should go in as well.", "Sorry about that, fix is in here: https://github.com/tensorflow/tensorflow/pull/29454", "As afeacf3 is already in master and it's reported broken, are there additional changes needed? Is master still affected by this?", "I will check again for both r1.14 and master today.", "Close this as it\u2019s resolved."]}, {"number": 29431, "title": "Redundant transpose removal - for conv3d", "body": "Similar transpose removal optimization with https://github.com/tensorflow/tensorflow/pull/23152, but this time for conv3d op.\r\n\r\nThis PR is supposed to improve inference performance of keras-based 3d-unet model by 50%. ", "comments": ["@penpornk Fixed.", "> Thank you for the changes! I believe there is one unanswered comment.\r\n\r\n@penpornk Hmm.. I miss this one. Fixed now.", "@penpornk Hi, any updates?", "> Your edits in the last commit have lines that are longer than 80 characters. Please reformat.\r\n\r\n@penpornk Sorry, my bad. Surprisingly clang-format didn't fix this for me.\r\n\r\nAnyway, I've manually fixed it. Please take a look.", "> Thank you! The latest commit adds a lot of unrelated files (167k lines). Please remove them.\r\n\r\nI accidentally checked-in my working environment files... Sorry, rookie mistake.\r\n\r\nFiles removed now.\r\n\r\n"]}, {"number": 29430, "title": "Initializing a tf.Variable with he_normal", "body": "Hi. I am trying to implement [NALU](https://arxiv.org/abs/1808.00508) using TF 2.0. I am on MacBook Air 2015 (CPU) and TensorFlow version is `2.0.0-alpha0`. \r\n\r\nSo, I was looking for ways to initialize the `W_hat` parameter using _He Initialization_ in TensorFlow 2.0. \r\n\r\nThings I have tried:\r\n- ```python \r\n    W_hat = tf.Variable(initial_value=tf.initializers.he_normal(),                        \r\n    trainable=True, \r\n    name='W_hat', shape=(2,1))\r\n   ```\r\n  \ud83d\udc46results in - \r\n    ```\r\n     \r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-25-b78fa0d268a9> in <module>\r\n      1 tf.Variable(initial_value=tf.initializers.he_normal(),                        \r\n      2             trainable=True,\r\n      ----> 3             name='W_hat', shape=(2,1))\r\n\r\n     /miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/variables.py in __call__(cls, \r\n    *args, **kwargs)\r\n    212       return cls._variable_v1_call(*args, **kwargs)\r\n    213     elif cls is Variable:\r\n     --> 214       return cls._variable_v2_call(*args, **kwargs)\r\n    215     else:\r\n    216       return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n\r\n    TypeError: _variable_v2_call() got an unexpected keyword argument 'shape'\r\n   ```\r\n\r\n- The second try was bound to produce the error I think but anyhow I did it just to be sure - \r\n   ```python\r\n         W_hat = tf.Variable(initial_value=tf.initializers.he_normal(),                        \r\n         trainable=True, \r\n          name='W_hat')\r\n   ```\r\n   The trace for this one is attached. \r\n   [trace.txt.zip](https://github.com/tensorflow/tensorflow/files/3256057/trace.txt.zip)\r\n\r\nWould be great to see suggestions. ", "comments": ["The error about shape missing is probably just because the shape argument to tf.Variable was recently introduced into the TF API and you're running a version before that.", "I should have searched a bit more before raising the issue. It was really simple of a solution:\r\n```python\r\nshape = (2,)\r\ninitializer = tf.initializers.he_normal()\r\nvar = tf.Variable(initializer(shape=shape))\r\n```"]}, {"number": 29429, "title": "fit_generator and predict_generator ignores steps_per_epoch parameter when using sequence", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: TITAN\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nHi,\r\n\r\nWhen running fit_generator (and predict_generator) with the steps_per_epoch parameter (and steps in predict) using a custom sequence, the whole dataset is used instead of the number of batches passed in the steps_per_epoch parameter.\r\nAccording to the documentation:\r\nsteps_per_epoch: Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of samples of your dataset divided by the batch size. Optional for Sequence: if unspecified, will use the len(generator) as a number of steps.\r\n\r\nThe code below works as expected in tensorflow 1.12.0 and 2.0.0a0 but not in 1.13.1\r\n \r\nSeems like the 'convert_to_generator_like' function in file 'engine/training_generator.py' overrides the 'steps_per_epoch' parameter with len(data) without checking if it is not None in case of a sequence:\r\n\r\n```\r\n  if data_utils.is_generator_or_sequence(data) or isinstance(\r\n      data, iterator_ops.EagerIterator):\r\n    if isinstance(data, data_utils.Sequence):\r\n      steps_per_epoch = len(data)\r\n    return data, steps_per_epoch\r\n\r\n```\r\nThanks\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow import keras\r\n#import keras\r\n\r\nINPUT_SIZE = 3\r\nDENSE_OUTPUTS = 2\r\nNUM_OF_SAMPLES = 1000\r\nBATCH_SIZE = 2\r\nNUM_OF_BATCHES = 5\r\n\r\n\r\nclass DummySequence(keras.utils.Sequence):\r\n\r\n    def __len__(self):\r\n        return NUM_OF_SAMPLES // BATCH_SIZE\r\n\r\n    def __getitem__(self, index):\r\n        data = [np.full(shape=(INPUT_SIZE,), fill_value=(index*BATCH_SIZE + i)) for i in range(BATCH_SIZE)]\r\n        labels = [np.full(shape=(DENSE_OUTPUTS,), fill_value=(index*BATCH_SIZE + i))*INPUT_SIZE for i in range(BATCH_SIZE)]\r\n        return np.stack(data), np.stack(labels)\r\n\r\n\r\nclass CountBatchesCallback(keras.callbacks.Callback):\r\n\r\n    def __init__(self):\r\n        super(CountBatchesCallback, self).__init__()\r\n\r\n        self.batches = 0\r\n\r\n    def on_batch_begin(self, batch, logs=None):\r\n        self.batches += 1\r\n\r\n\r\nx = keras.layers.Input(shape=(INPUT_SIZE,))\r\ndense_layer = keras.layers.Dense(DENSE_OUTPUTS)\r\ny = dense_layer(x)\r\nmodel = keras.Model(x, y)\r\n\r\nmodel.compile(optimizer=\"sgd\", loss=keras.losses.mean_squared_error)\r\n\r\nshapes = [v.shape for v in dense_layer.weights]\r\ndense_layer.set_weights([np.full(shape=shapes[0], fill_value=1.0), np.full(shape=shapes[1], fill_value=0.0)])\r\n\r\nseq = DummySequence()\r\n\r\nsteps = 5\r\nbatch_counter_callback = CountBatchesCallback()\r\n\r\nprint(\"running fit with {} steps\".format(steps))\r\nmodel.fit_generator(\r\n    seq,\r\n    epochs=1,\r\n    steps_per_epoch=steps,\r\n    callbacks=[batch_counter_callback]\r\n)\r\nprint(\"batches processed: {}\".format(batch_counter_callback.batches))\r\n\r\nresults = model.predict_generator(seq, steps=steps)\r\nprint(\"\\npredict\\nexpected number of results: {}.\\nactual number of results: {}.\\npredictions:\\n{}\".format(\r\n    steps*BATCH_SIZE, len(results), results)\r\n)\r\n\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This is fixed with latest version of TF-Nightly.\r\nOutput in tf-nightly version '1.15.0-dev20190821' is printed below:\r\n```python\r\nrunning fit with 5 steps\r\n5/5 [==============================] - 0s 26ms/step - loss: 0.0000e+00\r\nbatches processed: 5\r\n\r\npredict\r\nexpected number of results: 10.\r\nactual number of results: 10.\r\npredictions:\r\n[[ 0.  0.]\r\n [ 3.  3.]\r\n [ 6.  6.]\r\n [ 9.  9.]\r\n [12. 12.]\r\n [15. 15.]\r\n [18. 18.]\r\n [21. 21.]\r\n [24. 24.]\r\n [27. 27.]]\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29429\">No</a>\n"]}, {"number": 29428, "title": "failed to pip install tensorflow gpu on win10 and Could not install packages due to an EnvironmentError", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:tf_nightly_gpu_2.0_preview\r\n- Python version:python 3.6.3\r\n- Installed using virtualenv? pip? conda?:pip conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:  cuda_10.0.130_411.31_win10.exe  cudnn-10.0-windows10-x64-v7.5.0.56.zip\r\n- GPU model and memory:GTX 1050\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to install tensorflow gpu on win10,but faild after many times.\r\nI also try to use conda,but failed. It takes me alomost one day to figure it out but failed finnaly,\r\nSo upset and come here for help,thank you!\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tf_nightly_gpu_2.0_preview-2.0.0.dev20190604-cp36-cp36m-win_amd64.whl\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nThe error occurs when pip install tf-nightly-gpu-2.0-preview\r\nCould not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\XXXX\\\\AppData\\\\Local\\\\Temp\\\\pip-install-9z06414_\\\\tf-nightly-gpu-2.0-preview\\\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190604.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/Eigen/src/Core/products/GeneralMatrixMatrixTriangular_BLAS.h'", "comments": ["`pip` is trying to write a temporary file at `C:\\Users\\XXXX\\AppData\\Local\\Temp\\pip-install-9z06414_\\tf-nightly-gpu-2.0-preview\\tf_nightly_gpu_2.0_preview-2.0.0.dev20190604.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/Eigen/src/Core/products/GeneralMatrixMatrixTriangular_BLAS.h` which has 260 characters. If the `XXXX` for your username is different, this means you'll be over 260 characters. By default, this hits a limit in Windows APIs, which exists for compatibility with previous versions, but isn't really needed.\r\n\r\nYou can see on [Windows documentation](https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation) instructions on how to disable that limit, which should solve your problem.", "duplicate #24705 "]}, {"number": 29427, "title": "[TF 2.0a0] fail to use If within GradientTape which is within tf.range loop.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0alpha0\r\n- Python version: 3.6.5\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n<pre>\r\n# -*- coding: utf-8 -*-\r\n# @Author  : Lin Lan (ryan.linlan@gmail.com)\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(100)\r\n        self.dense2 = tf.keras.layers.Dense(100)\r\n        self.dense3 = tf.keras.layers.Dense(100)\r\n\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        x = self.dense2(x)\r\n        outputs = self.dense3(x)\r\n        return outputs\r\n\r\n\r\nmodel = Model()\r\noptimizer = tf.keras.optimizers.SGD()\r\n\r\n\r\n@tf.function\r\ndef train(data):\r\n    batch_size = data.shape[0]\r\n    grads = tf.TensorArray(tf.float32, size=batch_size)\r\n    for i in tf.range(batch_size):\r\n        with tf.GradientTape() as tape:\r\n            if True:\r\n                y = model(data[i][tf.newaxis, :])\r\n                loss = tf.reduce_mean(tf.square(y))\r\n        this_grads = tape.gradient(loss, model.trainable_weights)\r\n        this_grads = tf.concat([tf.reshape(g, [-1]) for g in this_grads], axis=0)\r\n        grads = grads.write(i, this_grads)\r\n    return grads.stack()\r\n\r\n\r\nx = np.random.rand(10, 100).astype(np.float32)\r\nprint(train(x))\r\n</pre>\r\n**Other info / logs**\r\n`ValueError: TensorFlow requires that the following symbols must be initialized to a Tensor, Variable or TensorArray before the loop: ('loss',)`.\r\n\r\nIf we comment `if True`, the code works.\r\n", "comments": ["I found that interchanging `with ... as tape` and `if True` also fails, i.e.,\r\n<pre>\r\n...\r\nfor i in tf.range(batch_size):\r\n  if True:\r\n    with tf.GradientTape() as tape:\r\n...\r\n</pre>\r\n", "Hi,\r\nwhat will happen if your \"IF\" condition evaluated to false? it will never create loss tensor. I think that is why it did not work. Below code works.\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense1 = tf.keras.layers.Dense(100)\r\n        self.dense2 = tf.keras.layers.Dense(100)\r\n        self.dense3 = tf.keras.layers.Dense(100)\r\n\r\n    def call(self, inputs):\r\n        x = self.dense1(inputs)\r\n        x = self.dense2(x)\r\n        outputs = self.dense3(x)\r\n        return outputs\r\n\r\n\r\nmodel = Model()\r\noptimizer = tf.keras.optimizers.SGD()\r\n\r\n\r\n@tf.function\r\ndef train(data):\r\n    batch_size = data.shape[0]\r\n    grads = tf.TensorArray(tf.float32, size=batch_size)\r\n    for i in tf.range(batch_size):\r\n        with tf.GradientTape() as tape:\r\n            #loss=tf.constant(1)\r\n            if True:\r\n                y = model(data[i][tf.newaxis, :])\r\n                loss = tf.reduce_mean(tf.square(y))\r\n            else:\r\n                loss=tf.constant(1)\r\n        this_grads = tape.gradient(loss, model.trainable_weights)\r\n        this_grads = tf.concat([tf.reshape(g, [-1]) for g in this_grads], axis=0)\r\n        grads = grads.write(i, this_grads)\r\n    return grads.stack()\r\n\r\n\r\nx = np.random.rand(10, 100).astype(np.float32)\r\nprint(train(x))\r\n```", "@hegman12 I get it! Thanks for your attention. This is not a tf bug, and I'll close this issue."]}, {"number": 29426, "title": "Update README.md", "body": "Updated README for s390x Nightly artifacts.\r\nWe are working on release artifacts and will do PR for release artifacts soon.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29426) for more info**.\n\n<!-- need_sender_cla -->", "@gunan Could you please check this", "I signed it!", "@Nayana-ibm please sign CLA so that we can merge this internally.", "@rthadur I have created new PR with cla signed \r\nPlease chcek https://github.com/tensorflow/tensorflow/pull/29484"]}, {"number": 29425, "title": "failed to pip install tensorflow gpu on win10 and Could not install packages due to an EnvironmentError", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": []}, {"number": 29424, "title": "Fix GRU/LSTM to properly use cuDNN", "body": "Neither LSTM nor GRU were using the cuDNN implementation on the nightly version.\r\n\r\nThe problem is that _context.executing_eagerly()_ doesn't return True and so, the logic for executing  cuDNN never continues. By changing back this line to _opt.executing_eagerly_outside_function()_, it works correctly. \r\n\r\nI suspect that the root of the problem lies in the context manager, as the layer is not called in an eager context. This commit proposes a simple fix. However, if you think that this solution is not adequate, let me know and I'll keep investigating.\r\n\r\nThis is my first commit, so thanks for your patience!", "comments": ["The check of context.executing_eagerly() is intended here. \r\n\r\nWhen the layer is used by keras model, even the outmost context is eager, under the hood, keras is using graph mode for performance. In that case, the grappler will do the correct selection based on the device placement and hardware availability. \r\n\r\nWe have strict e2e for the correct selection of normal/cudnn kernel based on different hardware. Could u report a bug with the details if you feel the GPU kernel is not correctly used?", "Btw, context.executing_eagerly() will also return False if your model is wrapped by tf.function, since it will use graph mode for performance reason.", "Thanks for your review, @qlzh727 , I see it now.\r\n\r\nThe issue must then be around `register_cudnn_defun()`, which was added [here](https://github.com/tensorflow/tensorflow/commit/ac04087f7fb9d535d33b800d6e2bfb82c7df7077#diff-a9f256601f2626075300a37eeb4cea5f). From then on, the nightlies stop utilizing cuDNN properly for me.\r\n\r\nI filled the bug report [here](https://github.com/tensorflow/tensorflow/issues/29506).\r\nThanks!", "Closing this as an alternative fix has been implemented in [e691be7](https://github.com/tensorflow/tensorflow/commit/e691be7814bcd7065950ec940456a4c9d8991645)."]}, {"number": 29423, "title": "[TF 2.0] Dataset isn't working with TPUStrategy", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colaboratory\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): v1.12.1-3283-geff4ae822a 2.0.0-dev20190604\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen executing a keras model's `fit` method built within a TPUStrategy with eager execution disabled, a series of errors referring to unknown NodeDef attributes occur (the actual unknown attr varies each time). \r\nExecuting the same model (with the same input dataset) not defined within a TPUStrategy's scope works fine.\r\n\r\n**Describe the expected behavior**\r\nIt should work without a problem.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n# Disable eager execution, otherwise TPUStrategy won't work at all\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Load dataset\r\nds = tfds.load('mnist', split=tfds.Split.TRAIN, as_supervised=True)\r\nds = ds.map(lambda x,y : (tf.cast(x, tf.float32), y))\r\nds = ds.shuffle(100).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n# Prepare strategy\r\nimport os\r\nTPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\r\ntf.config.experimental_connect_to_host(TPU_ADDRESS)\r\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\r\ntf.tpu.experimental.initialize_tpu_system(resolver)\r\nstrategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n\r\nwith strategy.scope():\r\n    net = tf.keras.Sequential([tf.keras.layers.Input([28,28,1]),\r\n                                tf.keras.layers.Flatten(),\r\n                                tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\r\n    net.compile(loss='categorical_crossentropy',\r\n                optimizer=tf.keras.optimizers.SGD(0.01))\r\n\r\n# training will raise an exception\r\nnet.fit(ds)\r\n```\r\n**Other info / logs**\r\n```python\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1355     try:\r\n-> 1356       return fn(*args)\r\n   1357     except errors.OpError as e:\r\n\r\n11 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1338       # Ensure any changes to the graph are reflected in the runtime.\r\n-> 1339       self._extend_graph()\r\n   1340       return self._call_tf_sessionrun(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _extend_graph(self)\r\n   1373     with self._graph._session_run_lock():  # pylint: disable=protected-access\r\n-> 1374       tf_session.ExtendSession(self._session)\r\n   1375 \r\n\r\nInvalidArgumentError: NodeDef mentions attr 'parallel_copy' not in Op<name=PaddedBatchDatasetV2; signature=input_dataset:variant, batch_size:int64, padded_shapes:N*int64, padding_values:, drop_remainder:bool -> handle:variant; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1>; NodeDef: {{node PaddedBatchDatasetV2}}. (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-8-fc146f239c6d> in <module>()\r\n----> 1 net.fit(ds)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    641         max_queue_size=max_queue_size,\r\n    642         workers=workers,\r\n--> 643         use_multiprocessing=use_multiprocessing)\r\n    644 \r\n    645   def evaluate(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    622         validation_split=validation_split,\r\n    623         shuffle=shuffle,\r\n--> 624         epochs=epochs)\r\n    625     if not dist_utils.is_distributing_by_cloning(model):\r\n    626       with model._distribution_strategy.scope():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _distribution_standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, validation_split, shuffle, epochs, allow_partial_batch)\r\n   2118         session = None\r\n   2119       else:\r\n-> 2120         session = K.get_session()\r\n   2121 \r\n   2122       first_x_value = nest.flatten(x)[0]\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in get_session(op_input_list)\r\n    462   if not _MANUAL_VAR_INIT:\r\n    463     with session.graph.as_default():\r\n--> 464       _initialize_variables(session)\r\n    465   return session\r\n    466 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in _initialize_variables(session)\r\n    886       v._keras_initialized = True\r\n    887     if uninitialized_vars:\r\n--> 888       session.run(variables_module.variables_initializer(uninitialized_vars))\r\n    889 \r\n    890 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    948     try:\r\n    949       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 950                          run_metadata_ptr)\r\n    951       if run_metadata:\r\n    952         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1171     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1172       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1173                              feed_dict_tensor, options, run_metadata)\r\n   1174     else:\r\n   1175       results = []\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1348     if handle is None:\r\n   1349       return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n-> 1350                            run_metadata)\r\n   1351     else:\r\n   1352       return self._do_call(_prun_fn, handle, feeds, fetches)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1368           pass\r\n   1369       message = error_interpolation.interpolate(message, self._graph)\r\n-> 1370       raise type(e)(node_def, op, message)\r\n   1371 \r\n   1372   def _extend_graph(self):\r\n\r\nInvalidArgumentError: NodeDef mentions attr 'parallel_copy' not in Op<name=PaddedBatchDatasetV2; signature=input_dataset:variant, batch_size:int64, padded_shapes:N*int64, padding_values:, drop_remainder:bool -> handle:variant; attr=Toutput_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=N:int,min=1>; NodeDef: node PaddedBatchDatasetV2 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/dataset_builder.py:378) . (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node PaddedBatchDatasetV2:\r\n ParallelMapDataset (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/core/dataset_builder.py:741)\r\n```", "comments": ["I just saw that TPU support isn't expected for keras until the RC release. I'm closing this issue since there are already other issues tracking overall progress on TPU support."]}, {"number": 29422, "title": "UnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 24: invalid start byte when using TensorBoardDebugWrapperSession", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Linux-4.15.0-50-generic-x86_64-with-Ubuntu-16.04-xenial\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 1.14.1-dev20190424\r\n- Python version: Python 2.7.12\r\n- Bazel version: bazel release 0.24.1\r\n- GCC/Compiler version: gcc version 4.8.5 (Ubuntu 4.8.5-4ubuntu2) & gcc version 4.8.5 (Ubuntu 4.8.5-4ubuntu2)\r\n- CUDA/cuDNN version: CUDA Version: 10.0 /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n\r\n\r\n**Describe the current behavior**\r\nI want to use TensorBoardDebugWrapperSession/LocalCLIDebugWrapperSession to check whether my code is right by step by step debugging while inference.\r\n\r\nHowever, when I add TensorBoardDebugWrapperSession/LocalCLIDebugWrapperSession like [tutorial](https://www.tensorflow.org/guide/debugger), it occurs this error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\n<ipython-input-1-ea4f4b4649f9> in <module>()\r\n     20 convert_start = time.time()\r\n     21 # hyps, src_frames, en_frames, scores = pred.Run(['hypotheses', 'src_frames', 'encoder_frames', 'scores'], wav=read_data)\r\n---> 22 hyps, scores = pred.Run(['hypotheses', 'scores'], wav=read_data)\r\n     23 convert_end = time.time()\r\n     24 diff = convert_end - convert_start\r\n\r\n/root/.cache/bazel/_bazel_root/17eb95f0bc03547f4f1319e61997e114/execroot/__main__/bazel-out/k8-opt/bin/lingvo/ipython_kernel.runfiles/__main__/lingvo/core/predictor.py in Run(self, fetch_keys, **kwargs)\r\n    224     #     tf.Session.run, fetches, feed_dict=feeds, options=run_options)\r\n    225     return self._RunWithValidSession(\r\n--> 226         tf_debug.TensorBoardDebugWrapperSession.run, fetches, feed_dict=feeds, options=run_options)\r\n    227     # return self._RunWithDebugSession(\r\n    228     #     tf_debug.LocalCLIDebugWrapperSession.run, fetches, feed_dict=feeds, options=run_options)\r\n\r\n/root/.cache/bazel/_bazel_root/17eb95f0bc03547f4f1319e61997e114/execroot/__main__/bazel-out/k8-opt/bin/lingvo/ipython_kernel.runfiles/__main__/lingvo/core/retry.pyc in wrapper(*args, **kwargs)\r\n     48       for retries in itertools.count(0):\r\n     49         try:\r\n---> 50           return func(*args, **kwargs)\r\n     51         except retry_value as e:\r\n     52           if retries >= max_retries:\r\n\r\n/root/.cache/bazel/_bazel_root/17eb95f0bc03547f4f1319e61997e114/execroot/__main__/bazel-out/k8-opt/bin/lingvo/ipython_kernel.runfiles/__main__/lingvo/core/predictor.py in _RunWithValidSession(self, fn, *args, **kwargs)\r\n    161     try:\r\n    162       # self._sess = tf_debug.TensorBoardDebugWrapperSession(self._sess, \"dm-System-Product-Name:6008\")\r\n--> 163       return fn(self._sess, *args, **kwargs)\r\n    164     except py_utils.transient_tf_errors:\r\n    165       # self._sess is invalid, most likely due to the worker being preempted.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/grpc_wrapper.pyc in run(self, fetches, feed_dict, options, run_metadata, callable_runner, callable_runner_args, callable_options)\r\n    221       self._sent_graph_version = publish_traceback(\r\n    222           self._grpc_debug_server_urls, self.graph, feed_dict, fetches,\r\n--> 223           self._sent_graph_version)\r\n    224     return super(TensorBoardDebugWrapperSession, self).run(\r\n    225         fetches,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/wrappers/grpc_wrapper.pyc in publish_traceback(debug_server_urls, graph, feed_dict, fetches, old_graph_version)\r\n     58   # pylint:enable=g-import-not-at-top\r\n     59   if graph.version > old_graph_version:\r\n---> 60     run_key = common.get_run_key(feed_dict, fetches)\r\n     61     source_remote.send_graph_tracebacks(\r\n     62         debug_server_urls, run_key, traceback.extract_stack(), graph,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/debug/lib/common.pyc in get_run_key(feed_dict, fetches)\r\n     85   \"\"\"\r\n     86   return json.dumps(RunKey(get_flattened_names(feed_dict),\r\n---> 87                            get_flattened_names(fetches)))\r\n\r\n/usr/lib/python2.7/json/__init__.pyc in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, sort_keys, **kw)\r\n    242         cls is None and indent is None and separators is None and\r\n    243         encoding == 'utf-8' and default is None and not sort_keys and not kw):\r\n--> 244         return _default_encoder.encode(obj)\r\n    245     if cls is None:\r\n    246         cls = JSONEncoder\r\n\r\n/usr/lib/python2.7/json/encoder.pyc in encode(self, o)\r\n    205         # exceptions aren't as detailed.  The list call should be roughly\r\n    206         # equivalent to the PySequence_Fast that ''.join() would do.\r\n--> 207         chunks = self.iterencode(o, _one_shot=True)\r\n    208         if not isinstance(chunks, (list, tuple)):\r\n    209             chunks = list(chunks)\r\n\r\n/usr/lib/python2.7/json/encoder.pyc in iterencode(self, o, _one_shot)\r\n    268                 self.key_separator, self.item_separator, self.sort_keys,\r\n    269                 self.skipkeys, _one_shot)\r\n--> 270         return _iterencode(o, 0)\r\n    271 \r\n    272 def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\r\n\r\nUnicodeDecodeError: 'utf8' codec can't decode byte 0x80 in position 24: invalid start byte\r\n```\r\n\r\n**Describe the expected behavior**\r\nI want to be able to see the results of each step of the variable at runtime through TensorBoardDebugWrapperSession.\r\n\r\n**Code to reproduce the issue**\r\nI just [modify some codes in lingvo to use tf-debug.](https://github.com/tensorflow/lingvo/issues/94)\r\n\r\n**Other info / logs**\r\nI think the problem should be on tensorflow, so I also add the issue here. I hope you can help me. Thank you!", "comments": ["+1\r\nI encountered the same issue while using tf_debug", "@iamxiaoyubei This is a stale issue. Just checking whether it is still an issue for you? Can you please check recent TF/TB versions and let us know whether the issue was resolved for you. Thanks!\r\n\r\nSince the issue opened, there were lot of improvements and I think issue might have been resolved with the recent versions. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29422\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29422\">No</a>\n"]}, {"number": 29421, "title": "Speedup", "body": "speedup two methods", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29421) for more info**.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29421) for more info**.\n\n<!-- ok -->", "@SmokerX  Could you please resolve the conflicts? Thanks!", "@gbaned the branch of the file ( optimize_for_inference_lib.py ) add another case ( DepthwiseConv2dNative ) \r\nI speed up this method and just consider Conv2D, so I remove this change in this PR\r\nand I will push another PR about this speed up", "@petewarden I found that the request for review was deleted. May I have any questions about this PR?", "Can one of the admins verify this patch?", "@ebrevdo Any update on this PR, please. Thanks!", "Well, there is no unit test and no benchmark, so reviewer requested changes have not been implemented", "@SmokerX Can you please check mihaimaruseac's comments and keep us posted? Thanks!", "It has been 31 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29420, "title": "updated a broken link", "body": "", "comments": []}]