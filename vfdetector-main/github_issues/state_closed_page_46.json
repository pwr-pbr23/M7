[{"number": 53979, "title": "Fix `mktemp` usage", "body": null, "comments": []}, {"number": 53978, "title": "Prevent a crash due to heap OOB write in grappler.", "body": "PiperOrigin-RevId: 408318417\r\nChange-Id: If095feb8c001e3a8ac4a85b7387b81e8309df47d", "comments": []}, {"number": 53977, "title": "Remove a `DCHECK`-fail, log an error instead.", "body": "`DCHECK` in debug mode results in crashes. TensorFlow has had multiple vulnerabilities due to this.\r\n\r\nOutside of debug mode, `DCHECK` is a no-op.\r\n\r\nA better alternative is to report an error to the log buffer and continue. This should happen both in debug mode and in prod mode.\r\n\r\nPiperOrigin-RevId: 408375925\r\nChange-Id: Id5b3e19c73f3fbe0cc4bba26ca44ff9607bb6356", "comments": []}, {"number": 53976, "title": "[lite] Add some safety checks to avoid out of bound access for sparsi\u2026", "body": "\u2026ty format\r\n\r\nPiperOrigin-RevId: 416910386\r\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a", "comments": []}, {"number": 53975, "title": "[lite] Add some safety checks to avoid out of bound access for sparsi\u2026", "body": "\u2026ty format\r\n\r\nPiperOrigin-RevId: 416910386\r\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a", "comments": []}, {"number": 53974, "title": "[lite] Add some safety checks to avoid out of bound access for sparsi\u2026", "body": "\u2026ty format\r\n\r\nPiperOrigin-RevId: 416910386\r\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a", "comments": []}, {"number": 53973, "title": "Remove a `DCHECK`-fail, log an error instead.", "body": "`DCHECK` in debug mode results in crashes. TensorFlow has had multiple vulnerabilities due to this.\r\n\r\nOutside of debug mode, `DCHECK` is a no-op.\r\n\r\nA better alternative is to report an error to the log buffer and continue. This should happen both in debug mode and in prod mode.\r\n\r\nPiperOrigin-RevId: 408375925\r\nChange-Id: Id5b3e19c73f3fbe0cc4bba26ca44ff9607bb6356", "comments": []}, {"number": 53972, "title": "Remove a `DCHECK`-fail, log an error instead.", "body": "`DCHECK` in debug mode results in crashes. TensorFlow has had multiple vulnerabilities due to this.\r\n\r\nOutside of debug mode, `DCHECK` is a no-op.\r\n\r\nA better alternative is to report an error to the log buffer and continue. This should happen both in debug mode and in prod mode.\r\n\r\nPiperOrigin-RevId: 408375925\r\nChange-Id: Id5b3e19c73f3fbe0cc4bba26ca44ff9607bb6356", "comments": []}, {"number": 53971, "title": "[lite] add validation check for sparse fully connected", "body": "PiperOrigin-RevId: 417629354\r\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7", "comments": []}, {"number": 53970, "title": "[lite] add validation check for sparse fully connected", "body": "PiperOrigin-RevId: 417629354\r\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7", "comments": []}, {"number": 53969, "title": "[lite] add validation check for sparse fully connected", "body": "PiperOrigin-RevId: 417629354\r\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7", "comments": []}, {"number": 53968, "title": "Develop upstream sync 220124", "body": null, "comments": []}, {"number": 53967, "title": "Prevent segfault in `embedding_lookup_sparse.cc`", "body": "Previous fixes missed one additional case.\r\n\r\nPiperOrigin-RevId: 417676944\r\nChange-Id: I8ab412155cf9b1e897448a6611d209eaa7ca9e66", "comments": []}, {"number": 53966, "title": "Prevent segfault in `embedding_lookup_sparse.cc`", "body": "Previous fixes missed one additional case.\r\n\r\nPiperOrigin-RevId: 417676944\r\nChange-Id: I8ab412155cf9b1e897448a6611d209eaa7ca9e66", "comments": []}, {"number": 53965, "title": "Prevent segfault in `embedding_lookup_sparse.cc`", "body": "Previous fixes missed one additional case.\r\n\r\nPiperOrigin-RevId: 417676944\r\nChange-Id: I8ab412155cf9b1e897448a6611d209eaa7ca9e66", "comments": []}, {"number": 53964, "title": "Fix integer overflow", "body": "Cherrypick f19be71717c497723ba0cea0379e84f061a75e01 and 1de49725a5fc4e48f1a3b902ec3599ee99283043 on r2.5", "comments": []}, {"number": 53963, "title": "Fix integer overflow", "body": "Cherrypick f19be71717c497723ba0cea0379e84f061a75e01 and 1de49725a5fc4e48f1a3b902ec3599ee99283043 on r2.6", "comments": []}, {"number": 53962, "title": "Fix integer overflow", "body": "Cherrypick f19be71717c497723ba0cea0379e84f061a75e01 and 1de49725a5fc4e48f1a3b902ec3599ee99283043 on r2.7", "comments": []}, {"number": 53961, "title": "[lite] Update TfLiteIntArrayCreate to return size_t", "body": "PiperOrigin-RevId: 416439896\r\nChange-Id: I847f69b68d1ddaff4b1e925a09b8b69c1756653b", "comments": []}, {"number": 53960, "title": "[lite] Update TfLiteIntArrayCreate to return size_t", "body": "PiperOrigin-RevId: 416439896\r\nChange-Id: I847f69b68d1ddaff4b1e925a09b8b69c1756653b", "comments": []}, {"number": 53959, "title": "[lite] Update TfLiteIntArrayCreate to return size_t", "body": "PiperOrigin-RevId: 416439896\r\nChange-Id: I847f69b68d1ddaff4b1e925a09b8b69c1756653b", "comments": []}, {"number": 53958, "title": "[lite] Add validation check for dilation height/width to be positive \u2026", "body": "\u2026integers.\r\n\r\nPiperOrigin-RevId: 416429178\r\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25", "comments": []}, {"number": 53957, "title": "[lite] Add validation check for dilation height/width to be positive \u2026", "body": "\u2026integers.\r\n\r\nPiperOrigin-RevId: 416429178\r\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25", "comments": []}, {"number": 53956, "title": "[lite] Add validation check for dilation height/width to be positive \u2026", "body": "\u2026integers.\r\n\r\nPiperOrigin-RevId: 416429178\r\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25", "comments": []}, {"number": 53955, "title": "[lite] Add check for bias_size is zero to avoid division by zero. Thi\u2026", "body": "\u2026s shouldn't happen for properly converted models. Just safety check\r\n\r\nPiperOrigin-RevId: 416383645\r\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb", "comments": []}, {"number": 53954, "title": "[lite] Add check for bias_size is zero to avoid division by zero. Thi\u2026", "body": "\u2026s shouldn't happen for properly converted models. Just safety check\r\n\r\nPiperOrigin-RevId: 416383645\r\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb", "comments": []}, {"number": 53953, "title": "[lite] Add check for bias_size is zero to avoid division by zero. Thi\u2026", "body": "\u2026s shouldn't happen for properly converted models. Just safety check\r\n\r\nPiperOrigin-RevId: 416383645\r\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb", "comments": []}, {"number": 53952, "title": "Gracefully catch errors when importing an invalid graphdef in MLIR TFG", "body": "When importing a \"generic function\" in TFG, we don't build a Graph in memory and\r\nso we need to implement a bit more checking in the importer itself.\r\nThis particular case catches duplicated names between nodes or between nodes and\r\nfunction arguments, and fixes a crash found by the fuzzer.\r\n\r\nPiperOrigin-RevId: 409331027\r\nChange-Id: Ibaf6290f67908c020c5103a7e009bdffd88690e2", "comments": []}, {"number": 53951, "title": "Tensorflow Lite: Using output index causes segfault. ", "body": "I am using Tensorflow Lite C++ interface for running inference, commit hash `040585c0f25681b399c9087b53c982959bcca44f` (HEAD of master branch at the time of posting this). \r\nI am using a model with 2 inputs, and a single output (array). \r\n\r\nI am trying to run the following code as a sanity check: \r\n\r\n```\r\n#include <iostream>\r\n#include <fstream>\r\n#include <vector>\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n\r\n#include <opencv2/opencv.hpp>\r\n\r\nusing namespace tflite;\r\n\r\nint main() {\r\n    std::ifstream modelFile (\"../models/liveness.tflite\", std::ios::binary | std::ios::ate);\r\n    std::streamsize size = modelFile.tellg();\r\n    modelFile.seekg(0, std::ios::beg);\r\n\r\n    std::vector<char> buffer(size);\r\n\r\n    if (!modelFile.read(buffer.data(), size)) {\r\n        throw std::runtime_error(\"There was a issue reading the model file into memory\");\r\n    }\r\n\r\n    StderrReporter errorReporter;\r\n    auto model = FlatBufferModel::BuildFromBuffer(buffer.data(), size, &errorReporter);\r\n    if (model == nullptr) {\r\n        throw std::runtime_error(\"Error creating builder from model file\");\r\n    }\r\n\r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n\r\n    std::unique_ptr<Interpreter> interpreter;\r\n\r\n    if (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {\r\n        throw std::runtime_error(\"Unable to create interpreter\");\r\n        // Return failure.\r\n    }\r\n\r\n    auto inputs = interpreter->inputs();\r\n    for (const auto input: inputs) {\r\n        std::cout << \"Input: \" << input << std::endl;\r\n        std::cout << \"Input name: \" << interpreter->GetInputName(input) << std::endl;\r\n    }\r\n\r\n    auto outputs = interpreter->outputs();\r\n    for (const auto output: outputs) {\r\n        std::cout << \"Output: \" << output << std::endl;\r\n        std::cout << \"Output name: \" << interpreter->GetOutputName(output) << std::endl;\r\n    }\r\n\r\n    return 0;\r\n}\r\n```\r\n\r\nWhen I run the above, it prints:\r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nInput: 199\r\nInput name: batch_normalization/batchnorm/add_1\r\nInput: 200\r\nInput name: batch_normalization/batchnorm/add_1\r\nOutput: 193\r\nSegmentation fault (core dumped)\r\n\r\n```\r\nAs can be seen, the call to ` std::cout << \"Output name: \" << interpreter->GetOutputName(output) << std::endl;` causes a segfault. \r\n\r\nIf I instead change it to ` std::cout << \"Output name: \" << interpreter->GetOutputName(0) << std::endl;` it prints `Output name: dense/Softmax` \r\n\r\nWhy is the first version calling a segfault? \r\n\r\nAs a sanity check, I use python to run this snippet of code with the same model:\r\n```\r\n    output_details = interpreter.get_output_details()\r\n\r\n    print(output_details)\r\n\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\n[{'name': 'dense/Softmax', 'index': 193, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([1, 2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\n```\r\n\r\nThe reason I even ask this question is because I expect to be able to obtain the inference results by calling:\r\n```\r\nfloat* out = interpreter->typed_output_tensor<float>(interpreter->outputs()[0]);\r\n```\r\n\r\nHowever, this too causes a segmentation fault. \r\n\r\nRunning `float* out = interpreter->typed_output_tensor<float>(0);` does give me the correct output.\r\n", "comments": ["hi,\r\n\r\nthe `interpreter->GetOutputName(i)` API expects the index to between 0 and the outputs.size() - 1, it's not the tensor index. When you use `output` which returned from the `interpreter->outputs()`, this is the output tensor index in the graph, so it might be out of bound.", "Makes sense, thank you! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53951\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/53951\">No</a>\n"]}, {"number": 53950, "title": "Fix crash when importing an invalid graph with functions with empty n\u2026", "body": "\u2026ames\r\n\r\nFound by proto fuzzing.\r\n\r\nPiperOrigin-RevId: 413918956\r\nChange-Id: I2cb898d6561070cfbcf448ea0da5e838438f3e92", "comments": []}]