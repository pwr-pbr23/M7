[{"number": 55059, "title": "tensorflow/core/kernels/conv_ops_gpu.cc:336] None of the algorithms provided by cuDNN frontend heuristics worked", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.9.9\r\n- Bazel version (if compiling from source): bazel 5.0.0\r\n- GCC/Compiler version (if compiling from source): g++ (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: NVIDIA GeForce RTX 3090 sm_8.6 with 25447170048B RAM and 82 cores\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nafter I build and installed, when I run model with conv3d ops, it will return the following msg:\r\n\r\n> 2022-02-25 09:26:56.521222: W tensorflow/core/kernels/conv_ops_gpu.cc:336] None of the algorithms provided by cuDNN frontend heuristics worked; trying fallback algorithms.  Conv: batch: 1\r\n> in_depths: 1\r\n> out_depths: 40\r\n> in: 256\r\n> in: 256\r\n> in: 256\r\n> data_format: 1\r\n> filter: 3\r\n> filter: 3\r\n> filter: 3\r\n> dilation: 1\r\n> dilation: 1\r\n> dilation: 1\r\n> stride: 1\r\n> stride: 1\r\n> stride: 1\r\n> padding: 2\r\n> padding: 2\r\n> padding: 2\r\n> dtype: DT_FLOAT\r\n> group_count: 1\r\n> device_identifier: \"NVIDIA GeForce RTX 3090 sm_8.6 with 25447170048B RAM and 82 cores\"\r\n> version: 1\r\n> \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nanything that use conv3d ops\r\n\r\n**Any other info / logs**\r\n\r\nthe model, like unet, can be finished even with the above msg. But I think this msg shows that cuDNN is not envolved? and hence the gpu is not used?\r\n", "comments": ["Hi @WingsOfPanda ! Could you please try again after a building from a source with Cuda 11.2 and CudNN 8.1 \r\n with 2.8 Branch. Please use Bazel 4.2.1 while building from [the source](https://www.tensorflow.org/install/source). Thank you!", "Hi @mohantym . Thank you for your suggestions. I changed to coda 11.2 and cudnn 8.1 and it worked! I still use Bazel 5.0.0 btw. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55059\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55059\">No</a>\n"]}, {"number": 55058, "title": "Improve to cover scale value greater than one", "body": "Improve to cover scale value greater than one\n", "comments": []}, {"number": 55056, "title": "TestExecutionEnvironment extended to provide info about storages that support in/uint types.", "body": "TestExecutionEnvironment extended to provide info about storages that support in/uint types.\n", "comments": []}, {"number": 55055, "title": "Add GPU implementation of SparseSliceGrad", "body": "cc @nluehr ", "comments": []}, {"number": 55054, "title": "Migrates all `tf_python_pybind_extension` targets to the `cc_shared_library` implementation.", "body": "Migrates all `tf_python_pybind_extension` targets to the `cc_shared_library` implementation.\n", "comments": []}, {"number": 55053, "title": "tensorflow\n", "body": "<em>Please make sure that this is a bug. As per our\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\nwe only address code/doc bugs, performance issues, feature requests and\nbuild/installation issues on GitHub. tag:bug_template</em>\n\n**System information**\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\n- TensorFlow installed from (source or binary):\n- TensorFlow version (use command below):\n- Python version:\n- Bazel version (if compiling from source):\n- GCC/Compiler version (if compiling from source):\n- CUDA/cuDNN version:\n- GPU model and memory:\n\nYou can collect some of this information using our environment capture\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\nYou can also obtain the TensorFlow version with:\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\n\n**Describe the current behavior**\n\n**Describe the expected behavior**\n\n**[Contributing](https://www.tensorflow.org/community/contribute)**\n\n- Do you want to contribute a PR? (yes/no):\n- Briefly describe your candidate solution(if contributing):\n\n**Standalone code to reproduce the issue**\nProvide a reproducible test case that is the bare minimum necessary to generate\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\n\n**Other info / logs** Include any logs or source code that would be helpful to\ndiagnose the problem. If including tracebacks, please include the full\ntraceback. Large logs and files should be attached.\n", "comments": ["@davin67, \r\n\r\nWe see that the issue [template]( https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyze the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced]. Thanks!", "Empty issue"]}, {"number": 55052, "title": "[XLA] Fix the bug in Literal storage: make it consistent with on-device buffer representation", "body": "[XLA] Fix the bug in Literal storage: make it consistent with on-device buffer representation\n\nOtherwise, copies using TransferManager may result in OOB read/writes. As a\nbonus, the resulting code is also considerably simpler.\n\nWe have to over-allocate Literals (always allocate the \"margin\" for the dynamic\nsizes), but given that this allocation is tiny and is not used on\nperformance-critical path, this should not be an issue.\n", "comments": []}, {"number": 55050, "title": "Remove the sample add quantization recipe", "body": "Remove the sample add quantization recipe\n\nThe add quantization recipe is a demo implementation of the new quantization\nflow. It should be removed and the related tests should be updated with either\nmatmul or convolution cases.\n", "comments": []}, {"number": 55049, "title": "Apply clang-tidy fixes for llvm-header-guard in test_passes.h (NFC)", "body": "Apply clang-tidy fixes for llvm-header-guard in test_passes.h (NFC)\n", "comments": []}, {"number": 55048, "title": "Change call sites from deprecated `mlir::parseSourceFile()` to `mlir::parseSourceFile<::mlir::ModuleOp>()`.", "body": "Change call sites from deprecated `mlir::parseSourceFile()` to `mlir::parseSourceFile<::mlir::ModuleOp>()`.\n", "comments": []}, {"number": 55047, "title": "Test Change rdff", "body": "Test Change rdff\n", "comments": []}, {"number": 55046, "title": "Update comments on literal.h.", "body": "Update comments on literal.h.\n", "comments": []}, {"number": 55045, "title": "Add HLO module name to gpu_compiler VLOGs.", "body": "Add HLO module name to gpu_compiler VLOGs.\n\nThis way when we say that a module uses Xmb of temp memory, we can see *which*\nmodule it is.\n", "comments": []}, {"number": 55044, "title": "Disable dumping of dynamic-shaped args.", "body": "Disable dumping of dynamic-shaped args.\n\nThis is broken due to memory corruption, b/223010622.\n", "comments": []}, {"number": 55042, "title": "NN API delegate: Provide an API that either successfully creates a complete", "body": "NN API delegate: Provide an API that either successfully creates a complete\nNnApi structure, which has all its functions pointers set to a non-null address,\nor fails.\n", "comments": []}, {"number": 55041, "title": "Check that the Support lib struct functions are not null.", "body": "Check that the Support lib struct functions are not null.\n", "comments": []}, {"number": 55039, "title": "Update the RBE images to the latest container versions", "body": "Automated PR created once per week to get the latest Docker images.\nSee https://github.com/tensorflow/tensorflow/blob/master/.github/workflows/update-rbe.yml", "comments": []}, {"number": 55038, "title": "AutoGraph could not transform when fine-tuning HuggingFace Pretrained Model", "body": "**System information**\r\nMacbook OS 11.6\r\n\r\n```\r\nnumpy==1.19.5\r\npathlib2==2.3.5\r\npython==3.8.8\r\ntensorflow==2.7.0\r\ntransformers==4.2.0\r\n```\r\n\r\n**Describe the current behavior**\r\nDuring` .fit()`, receiving warnings: \r\n\r\n```\r\nWARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at XXX >> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\nand\r\n\r\n```\r\nThe parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\r\nThe parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\r\n```\r\n_Would not have submitted except for the \"please report\" phrase in first warning. Happy to close quickly if this is resolved/trivial._\r\n\r\n**Describe the expected behavior**\r\n\r\nNeither of these warnings to appear prior to training.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n# train_texts is a list of strings\r\n# train_labels is a list of integers (0,1)\r\n\r\nimport tensorflow as tf\r\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\r\nfrom sklearn.model_selection import train_test_split\r\n\r\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\r\ncheckpoint = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\r\n\r\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors='tf')['input_ids']\r\nval_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors='tf')['input_ids']\r\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='tf')['input_ids']\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\r\n    train_encodings,\r\n    train_labels\r\n))\r\nval_dataset = tf.data.Dataset.from_tensor_slices((\r\n    val_encodings,\r\n    val_labels\r\n))\r\ntest_dataset = tf.data.Dataset.from_tensor_slices((\r\n    test_encodings,\r\n    test_labels\r\n))\r\n\r\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\r\nmodel.compile(optimizer=optimizer, loss=model.compute_loss) \r\n## Warnings occur here:\r\nmodel.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)\r\n```\r\n\r\n**Other info / logs** \r\nVerbose==10 logs leading up to warning:\r\n[tf_verbose_log.txt](https://github.com/tensorflow/tensorflow/files/8191826/tf_verbose_log.txt)\r\n", "comments": ["@adam1brownell,\r\nYou can suppress all debugging logs using below code snippet. Try setting log level before importing tf\r\n\r\n```\r\nimport os\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\r\nimport tensorflow as tf\r\n```\r\n\r\nOR\r\n\r\n```\r\nimport logging\r\nlogging.getLogger('tensorflow').disabled = True\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55038\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55038\">No</a>\n"]}, {"number": 55037, "title": "Mulitple inputs not parsing properly with dictionary", "body": "I'm getting an error with multiple inputs \r\nsomething like this\r\n\r\n\r\n`    \r\n    customer = Input(shape=(1,), name=\"customer_id\")  # 1-length sequence of ints\r\n    age = Input(shape=(1,), name=\"age\")  # 1-length sequence of ints\r\n    other_customer_input = Input(shape=(None,7), name=\"customer_info\")# 7-length sequence of ints\r\n    other_customer_input = layers.Dense(7,kernel_regularizer='l1')(other_customer_input)\r\n    cust_embedding = layers.Embedding(num_of_cx, 256)(customer)\r\n    dem_embedding = layers.Embedding(num_of_age, 16)(age)\r\n\r\n    x = layers.concatenate(axis=2,inputs=[customer_embedding,postal_code_embedding,other_customer_input])\r\n    x = layers.Dense(450,kernel_regularizer='l1')(x)\r\n\r\n    pref = layers.Dense(619, name=\"pref\",kernel_regularizer='l1')(x)\r\n\r\n    model = tf.keras.Model(\r\n        inputs=[age, customer, other_customer_input],\r\n        outputs=[pref]\r\n    )\r\n\r\n    x = layers.concatenate(axis=2,inputs=[customer_embedding,postal_code_embedding,other_customer_input])\r\n    x = layers.Dense(450,kernel_regularizer='l1')(x)\r\n    model.compile()\r\n\r\n    X= {\"customer_id\": data['customer_id'], \"age\": data['age'], \"customer_info\": data[cols]}\r\n    Y={\"pref\": data[cols])}\r\n\r\n    pref_model.fit(X,Y,\r\n        epochs=2,\r\n        batch_size=64)\r\n`\r\nThis is the full error message \r\n\r\nIt's not accepting a dictionary as an input\r\n\r\nINFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x7fb010938940>: DoNotConvert rule for keras\r\nINFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x7faf82c8cd30>\r\n    args: (<tf.Tensor 'args_0:0' shape=(None,) dtype=int64>, ({'customer_id': <tf.Tensor 'args_2:0' shape=(1362281, 1) dtype=int64>, 'age': <tf.Tensor 'args_1:0' shape=(1362281, 1) dtype=float32>, 'customer_info': <tf.Tensor 'args_3:0' shape=(1362281, 7) dtype=float64>}, {'pref': <tf.Tensor 'args_4:0' shape=(1362281, 619) dtype=int64>}))\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Allowlisted: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x7faf82c8cd30>: DoNotConvert rule for keras\r\nEpoch 1/2\r\nINFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.train_function at 0x7faf82c8cdc0>\r\n    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7faf82c9ef40>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Cache hit for <function Model.make_train_function.<locals>.train_function at 0x7faf82c8cdc0> subkey ConversionOptions[{}]: <tensorflow.python.autograph.pyct.transpiler._PythonFnFactory object at 0x7fb0133f89a0>\r\nINFO:tensorflow:Error transforming entity <function Model.make_train_function.<locals>.train_function at 0x7faf82c8cdc0>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 427, in converted_call\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 269, in _convert_actual\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 282, in transform\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 490, in transform_function\r\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 201, in instantiate\r\nValueError: closure mismatch, requested ('self', 'step_function'), but source function had ()\r\nWARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7faf82c8cdc0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: closure mismatch, requested ('self', 'step_function'), but source function had ()\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nINFO:tensorflow:Converted call: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7faf82c8c790>\r\n    args: (({'customer_id': <tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=int64>, 'age': <tf.Tensor 'IteratorGetNext:0' shape=(None, 1) dtype=float32>, 'customer_info': <tf.Tensor 'IteratorGetNext:2' shape=(None, 7) dtype=float64>}, {'pref': <tf.Tensor 'IteratorGetNext:3' shape=(None, 619) dtype=int64>}),)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Allowlisted: <function Model.make_train_function.<locals>.step_function.<locals>.run_step at 0x7faf82c8c790>: DoNotConvert rule for keras\r\n", "comments": ["Hi @tjaqu787 ! Could you please share the above code as a Colab gist? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55037\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/55037\">No</a>\n"]}, {"number": 55036, "title": "[XLA:GPU] Unify vectorization logic for single instructions and for fusions", "body": "[XLA:GPU] Unify vectorization logic for single instructions and for fusions\n\nUse \"fusion\" logic for both cases.\n", "comments": []}, {"number": 55034, "title": "[XLA] Simplify `ShapeIndex` and `ShapeIndexView`.", "body": "[XLA] Simplify `ShapeIndex` and `ShapeIndexView`.\n\nThe encapsulation doesn't really gain anything except more code.\n", "comments": []}, {"number": 55033, "title": "Fix build failure for absl::StrContains()", "body": "Fix build failure for absl::StrContains()\n", "comments": []}, {"number": 55032, "title": "[XLA] Cleanup `ShapeTree`.", "body": "[XLA] Cleanup `ShapeTree`.\n", "comments": []}, {"number": 55031, "title": "Remove unused proto import", "body": "PR removes import of `coordination_config.proto` from `worker.proto`.  This import is unused since b2f1fd1.  Protoc warns/complains.", "comments": []}, {"number": 55030, "title": "How to make pipeline parallelism in Tensorflow runtime with custom CUDA steam", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.7\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 7.3.1\r\n- CUDA/cuDNN version: cuda-11.0, cudnn-8.2\r\n- GPU model and memory: A100, 40GB HBM.\r\n\r\n\r\n# Description\r\nI'm trying to run a recommender-system for advertisement application with TensorFlow, on A100 X 16, and run a sparse embedding_lookup on a model-parallized embedding layer (embedding layer is splitted to 16 partitions on each GPU, and every worker send the lookup indices to others, and gather the lookup  results to local).\r\n\r\nOne problem is: I'm trying to split a whole batch input into several mini-batches, and let the k-th mini-batch be parallized to the (k+1)-th mini-batch, with a stage step bebind, to overlap the element-wise computations between mini-batches. But I haven't found a method to implement the idea, since the TensorFlow runtime run an op in graph when its upstream ops are finished.\r\n\r\nAnother problem is in an AsyncOpKernel, I found that a cudaStream created by cudaStreamCreate(), not d.stream(), is not synchronizable to the host code:\r\n```c++\r\ncudaStreamCreate(&custom_stream);\r\n... ...\r\nCUDACHECK(cudaStreamSynchronize(custom_stream));  // Block the custom stream.\r\nCUDACHECK(cudaStreamSynchronize(d.stream()));  // stream from the context->eigen_device<GPUDevice>(), I found it somehow not working (will not wait the custom_stream finished). I'm really confused about it.\r\n\r\n```\r\n\r\nI struggled with it for several weeks, I will be really appreciated if someone could answer the issue (X0X) !", "comments": ["@Lifann,\r\n\r\nIn order to expedite the trouble-shooting process here, could you please give more information on this issue and fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55029, "title": "No public commit message needed for presubmit.", "body": "No public commit message needed for presubmit.\n", "comments": []}, {"number": 55028, "title": "trying to print the fusing decisions.", "body": "trying to print the fusing decisions.\n", "comments": []}, {"number": 55027, "title": "TensorFlow Developer Certificate error sign in on pycharm ", "body": "I am able to successfully sign in to the Certification Assistant. However, I keep getting this error on pycharm \"An error occurred while trying to sign in: Connection reset\". I don't know why it happened\r\n![Screenshot 2022-03-05 at 2 43 15 PM](https://user-images.githubusercontent.com/39478293/156871893-b6fcf86b-266a-48a2-9cd6-d568ddb150a1.png)\r\n.", "comments": ["Hi @callmekofi ! Did you try again after disabling your antivirus/VPN or whitelisting the IP address linked with Tensorflow exam ? Can you also confirm that you are on Pycharm version  2021.3?", "I am using pycharm version 2021.3 on apple macbook. I turned off my vpn once i sign in my google account but the result is the same. ", "Ok @callmekofi ! \r\n\r\n1. Please uninstall then reinstall the Tensorflow certification plugin  and accept policies wherever needed. \r\n\r\n2. Please check from different devices aside macbook( There are performance issue in Mac OS .You can refer https://developer.apple.com/metal/tensorflow-plugin/ for installation in Mac) .\r\n\r\nFor general support and questions regarding the Google Tensorflow Developer Certification, please contact tensorflow-certificate-support@google.com.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 55026, "title": "[ROCm] Enable ROCm support for the batched potrm functions for the cholesky \u2026", "body": "\u2026op in XLA.\r\n\r\nFor ROCm >= 4.5, this uses hipsolver and for ROCm < 4.5 this uses rocblas/rocsolver.", "comments": ["/cc @chsigg @cheshire @jurahul @jlebar ", "This has been merged."]}, {"number": 55025, "title": "[tf.numpy] Fixes linspace's rounding to follow numpy 1.20 .", "body": "[tf.numpy] Fixes linspace's rounding to follow numpy 1.20 .\n", "comments": []}]