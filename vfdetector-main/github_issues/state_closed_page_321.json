[{"number": 44573, "title": "Dataset.from_tensor_slices regression?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0, 2.2, and 2.3.0\r\n- Python version: 3.6, 3.7, 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\nI'm supporting someone trying to build a model. In Tensorflow 2.1.0, the code below works correctly. In version 2.2.0+, the last line fails. I am wondering if there is a regression or if there is a fix and 2.1.0 shouldn't have worked. \r\n**Describe the expected behavior**\r\nI expect the dataset to be created correctly. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n\r\ndata = [\r\n    [\r\n        np.array(['k1', '', '']), np.array(['t1', 't2', 't3']), np.array([.03, .02, .03])\r\n    ],\r\n    [\r\n        np.array(['k1', 'k2', 'k3']), np.array(['t4', 't5', 't6']), np.array([.03, .02, .03])\r\n    ]\r\n]\r\ntrain = pd.DataFrame(data, columns=['kwd', 'title', 'labels'])\r\nfeature_cols = ['kwd', 'title']\r\nlabels = train.pop('labels')\r\nfeatures = {\r\n    col: train[col] for col in feature_cols\r\n}\r\nbatch = tf.data.Dataset.from_tensor_slices((features, labels))\r\n\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/array_example.py\", line 17, in <module>\r\n    batch = tf.data.Dataset.from_tensor_slices((train, labels))\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 640, in from_tensor_slices\r\n    return TensorSliceDataset(tensors)\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2858, in __init__\r\n    element = structure.normalize_element(element)\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\", line 98, in normalize_element\r\n    ops.convert_to_tensor(t, name=\"component_%d\" % i))\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1341, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 321, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 262, in constant\r\n    allow_broadcast=True)\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 270, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/Users/oshiv/miniconda3/envs/py37tf22/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).\r\n```", "comments": ["@ymodak \r\nI am able to replicate the issue reported on tf 2.3 and nightly,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7f891dd203cffa4b613465fc055ec349/untitled458.ipynb#scrollTo=EV_xdzblRs5F).", "Hi @OmriShiv,\r\n\r\n`Dataset.from_tensor_slices` uses generic TensorFlow machinery to convert its inputs into tensors. It should be able to convert anything that can be passed to `tf.constant`. \r\n\r\nTo make `from_tensor_slices` work with DataFrames, convert the DataFrames to lists first:\r\n```python\r\nbatch = tf.data.Dataset.from_tensor_slices((list(features), list(labels)))\r\n```", "@aaudiber Let me take a look and see if I can make this work. I'm getting some dimension issues right now (difference between features and labels)\r\n\r\nMy question is more: why this works in 2.1, but not in 2.2+", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b3678f4c3f0837d74791e0e20fc53fb2/44573.ipynb). Thanks!", "This issue isn't specific to `from_tensor_slices` -- the same error will be raised by trying to create a TF constant using a `pandas.core.series.Series`. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44573\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44573\">No</a>\n"]}, {"number": 44572, "title": "Can't get custom output name from tfserving", "body": "I want the outputs from tfserving is:  `{\"outputs\":{\"scores\":[0.936071157,0.0527787767]}}`\r\nbut I got : `{ \"outputs\": [ 0.936071038, 0.0527787916 ] }`\r\n\r\nEnvironment:\r\npython 3.6\r\ntensorflow 2.3.0\r\n\r\nsave model code:\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.pmodel = model.model\r\n        self.out = keras.layers.Lambda(lambda x:x[:,1])\r\n        \r\n    @tf.function(input_signature=[[tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_q'), tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_a')]])\r\n    def call(self, inputs):\r\n        output = self.pmodel(inputs)\r\n        out = self.out(output)\r\n        return {\"scores\": out} \r\n\r\nNM = MyModel()\r\n\r\n# test\r\n# print(NM([qi, ai]))\r\n# {'scores': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.93607116, 0.05277878], dtype=float32)>}\r\n\r\n# save model\r\ntf.saved_model.save(NM, 'saved_model/202011032114')\r\n```\r\nBy the way, model.model is tf.keras.Model :\r\n![image](https://user-images.githubusercontent.com/9495054/97988975-d108df00-1e18-11eb-8915-d3d1fc35a8ed.png)\r\n\r\nSignatureDefs:\r\n![image](https://user-images.githubusercontent.com/9495054/97989755-f34f2c80-1e19-11eb-8edd-b624bdf2d0cd.png)\r\n\r\nStart tfserving\r\n![image](https://user-images.githubusercontent.com/9495054/97989821-0c57dd80-1e1a-11eb-87ec-ab4f9a59aa2b.png)\r\n\r\nWhat I got :(\r\n![image](https://user-images.githubusercontent.com/9495054/97990261-a1f36d00-1e1a-11eb-8708-8673f0422c96.png)\r\n\r\n**But!**, If model output two keys like `{\"scores\": out, \"give_up\":out}`, it works well.\r\n```\r\nclass MyModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(MyModel, self).__init__()\r\n        self.pmodel = model.model\r\n        self.out = keras.layers.Lambda(lambda x:x[:,1])\r\n        \r\n    @tf.function(input_signature=[[tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_q'), tf.TensorSpec(shape=[None,1], dtype=tf.string, name='input_a')]])\r\n    def call(self, inputs):\r\n        output = self.pmodel(inputs)\r\n        out = self.out(output)\r\n        return {\"scores\": out, \"give_up\":out}\r\n\r\nNM = MyModel()\r\n\r\n# test\r\n# print(NM([qi, ai]))\r\n# {'scores': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.93607116, 0.05277878], dtype=float32)>, 'give_up': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.93607116, 0.05277878], dtype=float32)>}\r\n\r\n# save model\r\ntf.saved_model.save(NM, 'saved_model/202011032126')\r\n```\r\n![image](https://user-images.githubusercontent.com/9495054/97990853-83da3c80-1e1b-11eb-94c9-30e83dcf9c3a.png)\r\n\r\n![image](https://user-images.githubusercontent.com/9495054/97990822-74f38a00-1e1b-11eb-8928-51d1d77747bb.png)\r\n\r\nAnd the result:\r\n![image](https://user-images.githubusercontent.com/9495054/97990916-9a809380-1e1b-11eb-9cee-05e8924455b6.png)\r\n\r\nI don't know why and didn't find any solution, Is there any kind person to help me ? Many thanks !\r\n", "comments": ["@xyangk,\r\nTensorFlow Serving issues are tracked in tensorflow/serving repo. Could you please submit a new issue from [this link](https://github.com/tensorflow/serving/issues/new) and fill in the template, so that we can track the issue there. Thanks!\r\n\r\n", "tfserving response: https://github.com/tensorflow/serving/issues/1766#issuecomment-724481376"]}, {"number": 44570, "title": "TF2.4 Build - An error occurred during the fetch of repository 'io_bazel_rules_go':", "body": "**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.4\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 3.7.0\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version: Cuda 11.0 / cuDNN 8.0.4\r\n- GPU model and memory: 2070 Max-Q\r\n\r\nWhen building from source, I receive the following error when running the following command.\r\n\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nERROR: An error occurred during the fetch of repository 'io_bazel_rules_go':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 181, column 30, in _git_repository_implementation\r\n                update = _clone_or_update(ctx)\r\n        File \"C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git.bzl\", line 36, column 20, in _clone_or_update\r\n                git_ = git_repo(ctx, directory)\r\n        File \"C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 91, column 12, in git_repo\r\n                _update(ctx, git_repo)\r\n        File \"C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 101, column 9, in _update\r\n                init(ctx, git_repo)\r\n        File \"C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 118, column 15, in init\r\n                _error(ctx.name, cl, st.stderr)\r\n        File \"C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git_worker.bzl\", line 190, column 9, in _error\r\n                fail(\"error running '%s' while working with @%s:\\n%s\" % (command_text, name, stderr))\r\nError in fail: error running 'git init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go' while working with @io_bazel_rules_go:\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"git\" init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go): The system cannot find the file specified.\r\n (error: 2)\r\nINFO: Repository com_google_protobuf instantiated at:\r\n  C:/sdks/tensorflow-dev/WORKSPACE:19:16: in <toplevel>\r\n  C:/sdks/tensorflow-dev/tensorflow/workspace.bzl:585:20: in tf_repositories\r\nRepository rule tf_http_archive defined at:\r\n  C:/sdks/tensorflow-dev/third_party/repo.bzl:131:34: in <toplevel>\r\nINFO: Repository envoy_api instantiated at:\r\n  C:/sdks/tensorflow-dev/WORKSPACE:116:10: in <toplevel>\r\n  C:/users/adam/_bazel_adam/ahelnrbq/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:235:21: in grpc_deps\r\nRepository rule http_archive defined at:\r\n  C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>\r\nERROR: no such package '@io_bazel_rules_go//go': error running 'git init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go' while working with @io_bazel_rules_go:\r\njava.io.IOException: ERROR: src/main/native/windows/process.cc(202): CreateProcessW(\"git\" init C:/users/adam/_bazel_adam/ahelnrbq/external/io_bazel_rules_go): The system cannot find the file specified.\r\n (error: 2)\r\nINFO: Elapsed time: 28.671s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    Fetching @upb; fetching\r\n    Fetching @build_bazel_rules_apple; fetching\r\n    Fetching @build_bazel_apple_support; fetching\r\n    Fetching ...al/upb; Extracting C:/users/adam/_bazel_adam/ahelnrbq/external/upb/temp630520761\\\r\n6925763744/9effcbcb27f0a665f9f345030188c0b291e32482.tar.gz\r\n    Fetching ..._apple; Extracting C:/users/adam/_bazel_adam/ahelnrbq/external/build_bazel_rules\\\r\n_apple/temp16001111932413110489/5131f3d46794bf227d296c82f30c2499c9de3c5b.tar.gz\r\n    Fetching ...upport; Extracting C:/users/adam/_bazel_adam/ahelnrbq/external/build_bazel_apple\\\r\n_support/temp14862261056236700618/501b4afb27745c4813a88ffa28acd901408014e4.tar.gz", "comments": ["@oracle3001 \r\nPlease share the steps you ran before you encountered the error.\r\nand please to follow [this guide](https://www.tensorflow.org/install/source_windows) \r\n\r\n", "I was following the guide.\r\n\r\nPulled 2.4 branch from github.\r\n\r\nRan `python ./configure.py`, used all the defaults i.e.\r\n\r\n```\r\nYou have bazel 3.7.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Adam\\anaconda3\\python.exe]:\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Adam\\anaconda3\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Adam\\anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\nEigen strong inline overridden.\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\n```\r\n\r\nRan the following command to build\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAlso tried,\r\n`bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nReceived error shown above. And I also tried a Bazel 3.1, with the same issue.\r\nAlso tried, `bazel clean --expunge` then repeating this, again the same result.\r\n\r\nFYI, TF 2.3 builds fine using the same approach.", "@oracle3001 \r\nCan you please let us know if you are using anaconda python and if yes please use normal python instead.", "Yes I am using Anaconda as standard. As stated above, this has not ever been an issue with building previous versions of python.\r\n\r\nI have tried with the stand-alone installation of python (3.8), reran` configure.py `to ensure it used this. \r\n\r\n```\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python38\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\Adam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]:\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n```\r\n\r\nThen ran \r\n\r\n```\r\nbazel clean --expunge\r\nazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nsame error as initially reported.", "I have managed to fix this problem. I don't know enough about msys64 to know why this worked, but.\r\n\r\nI reinstalled msys64 with the latest version. As stated on the TF instruction page, I ran `pacman -S git patch unzip`, but this still resulted in the reported error.\r\n\r\nI then ran `pacman -Syuu patch`, based upon a comment I saw on another github repo where people were having issues with go related things on windows. This appears to have fixed the problem.\r\n\r\nAs I say, I don't know anything really about msys64 or why adding patch would work, but it does. Perhaps it needs adding to the TF instructions?\r\n\r\nIt does still produce the following debug message. I don't know if this is of any importance. \r\n\r\n```\r\nDEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1557349968 -0400\"\r\nDEBUG: Repository io_bazel_rules_go instantiated at:\r\n  C:/sdks/tensorflow-dev/WORKSPACE:37:30: in <toplevel>\r\n  C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_toolchains/repositories/repositories.bzl:55:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/sdks/tensorflow-dev/WORKSPACE:37:30: in <toplevel>\r\n  C:/users/adam/_bazel_adam/ahelnrbq/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n```", "Moving this to closed status as the issue is resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44570\">No</a>\n"]}, {"number": 44569, "title": "Build v2.1.0 from source on Windows [ERROR: Linking of rule '//tensorflow/lite/python/optimize:_tensorflow_lite_wrap_calibration_wrapper.so' failed]", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 enterprise 1909 18363.1139\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v2.1.0\r\n- Python version: 3.6.12\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): VS 2019 Community\r\n- CUDA/cuDNN version:  CUDA: v10.1.243 / cuDNN: 7.6.5\r\n- GPU model and memory: CPU: Intel Core i7-4710HQ / Memory: 16G\r\n\r\n**Describe the problem**\r\n\r\nI encountered the following error message while building Tensorflow v2.1.0 from source on Windows 10\r\nThe detailed messages can be found in attached text file. \r\n```\r\nERROR: D:/kelvinwu/code/tensorflow/tensorflow/lite/python/optimize/BUILD:29:1: Linking of rule '//tensorflow/lite/python/optimize:_tensorflow_lite_wrap_calibration_wrapper.so' failed (Exit 1120)\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. Setup the environment following the instructions on Tensorflow page [[link](https://www.tensorflow.org/install/source_windows)].\r\n2. Git clone Tenorflow and checkout to v2.1.0\r\n3. Run <code>python configure.py</code>, the settings are shown below:\r\n```\r\nYou have bazel 0.29.1 installed.\r\nPlease specify the location of python. [Default is D:\\Users\\Asus_user\\anaconda3\\envs\\build-tensorflow-2.1\\python.exe]:\r\n\r\n\r\nFound possible Python library paths:\r\n  D:\\Users\\Asus_user\\anaconda3\\envs\\build-tensorflow-2.1\\lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [D:\\Users\\Asus_user\\anaconda3\\envs\\build-tensorflow-2.1\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\n```\r\n4. In order to fetch the http_archives without ERRORs, I had to add the following http_archives in WORKSPACE:\r\n```\r\nhttp_archive(\r\n    name = \"com_google_protobuf\",\r\n    sha256 = \"b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59\",\r\n    # This protobuf release is based on protobuf 3.8.0.\r\n    strip_prefix = \"protobuf-310ba5ee72661c081129eb878c1bbcec936b20f0\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\",\r\n        \"https://github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\",\r\n    ],\r\n)\r\nhttp_archive(\r\n    name = \"gif\",\r\n    build_file = \"//third_party:gif.BUILD\",\r\n    sha256 = \"31da5562f44c5f15d63340a09a4fd62b48c45620cd302f77a6d9acf0077879bd\",\r\n    strip_prefix = \"giflib-5.2.1\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz\",\r\n        \"http://pilotfiber.dl.sourceforge.net/project/giflib/giflib-5.2.1.tar.gz\",\r\n    ],\r\n)\r\nhttp_archive(\r\n    name = \"com_google_absl\",\r\n    build_file = \"//third_party:com_google_absl.BUILD\",\r\n    sha256 = \"acd93f6baaedc4414ebd08b33bebca7c7a46888916101d8c0b8083573526d070\",\r\n    strip_prefix = \"abseil-cpp-43ef2148c0936ebf7cb4be6b19927a9d9d145b8f\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz\",\r\n        \"http://github.com/abseil/abseil-cpp/archive/43ef2148c0936ebf7cb4be6b19927a9d9d145b8f.tar.gz\",\r\n    ],\r\n)\r\nhttp_archive(\r\n        name = \"cub_archive\",\r\n        build_file = \"//third_party:cub.BUILD\",\r\n        sha256 = \"6bfa06ab52a650ae7ee6963143a0bbc667d6504822cbd9670369b598f18c58c3\",\r\n        strip_prefix = \"cub-1.8.0\",\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/NVlabs/cub/archive/1.8.0.zip\",\r\n            \"https://github.com/NVlabs/cub/archive/1.8.0.zip\",\r\n        ],\r\n    )\r\nhttp_archive(\r\n        name = \"eigen_archive\",\r\n        build_file = \"//third_party:eigen.BUILD\",\r\n        sha256 = \"65d732985b593b553c20566e1f236f48dcc626730c418aed7b2aa1d0e3f1a0af\",\r\n        strip_prefix = \"eigen-4e696901f873a2347f76d931cf2f701e31e15d05\",\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz\",\r\n            \"https://gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz\",\r\n        ],\r\n    )\r\nhttp_archive(\r\n    name = \"png\",\r\n    build_file = \"//third_party:png.BUILD\",\r\n    sha256 = \"ca74a0dace179a8422187671aee97dd3892b53e168627145271cad5b5ac81307\",\r\n    strip_prefix = \"libpng-1.6.37\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/glennrp/libpng/archive/v1.6.37.tar.gz\",\r\n        \"https://github.com/glennrp/libpng/archive/v1.6.37.tar.gz\",\r\n    ],\r\n)\r\nhttp_archive(\r\n    name = \"icu\",\r\n    strip_prefix = \"icu-release-64-2\",\r\n    sha256 = \"10cd92f1585c537d937ecbb587f6c3b36a5275c87feabe05d777a828677ec32f\",\r\n    urls = [\r\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip\",\r\n        \"https://github.com/unicode-org/icu/archive/release-64-2.zip\",\r\n    ],\r\n    build_file = \"//third_party/icu:BUILD.bazel\"\r\n)\r\n```\r\n5. Run <code>bazel build //tensorflow/tools/pip_package:build_pip_package</code> and the ERROR showed up after hours of compiling\r\n\r\n**Any other info / logs**\r\nThe complete messages shown on console: [error_messages_20201104.txt](https://github.com/tensorflow/tensorflow/files/5484927/error_messages_20201104.txt)\r\n", "comments": ["@KodeWorker,\r\nFrom the error log, I see that you are using Anaconda Python to build TensorFlow.\r\n\r\nCould you please try building TensorFlow using just Python (i.e. outside the Anaconda environment) and check if it works. Thanks!", "@amahendrakar ,\r\nThank you for the advice.\r\n~~I used the base anaconda python, but doing this introduced new error.~~ \r\n(2020/11/06: I rebuild Tensorflow but show different error messages. And then I decide to build using MSYS Shell)\r\n\r\n~~**Thing I did**~~\r\n~~1. Run <code>bazel clean --expunge</code>~~\r\n~~2. Run <code>python configure.py</code>, and the configurations are shown below:~~\r\n```\r\nPlease input the desired Python library path to use.  Default is [D:\\Users\\Asus_user\\anaconda3\\lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: N\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: N\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.1 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\nFound cuDNN 7 in:\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64\r\n    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n\r\n\r\nWould you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: Y\r\nEigen strong inline overridden.\r\n```\r\n~~3. Run <code>bazel build //tensorflow/tools/pip_package:build_pip_package</code>, and got the following error:~~\r\n```\r\nERROR: D:/kelvinwu/code/tensorflow/tensorflow/core/kernels/BUILD:4793:1: C++ compilation of rule '//tensorflow/core/kernels:dilation_ops_gpu' failed (Exit 2)\r\n```\r\n\r\n~~The detailed messages are in [error_messages_20201105.txt](https://github.com/tensorflow/tensorflow/files/5491942/error_messages_20201105.txt)~~\r\n", "**Quick update**\r\nI build Tensorflow using MSYS Shell, and result is the same:\r\n<code> ERROR: D:/kelvinwu/code/tensorflow/tensorflow/lite/python/optimize/BUILD:29:1: Linking of rule '//tensorflow/lite/python/optimize:_tensorflow_lite_wrap_calibration_wrapper.so' failed (Exit 1120) </code> ", "**Quick update**\r\nI found that MSYS2 packages (git patch unzip) were not installed!\r\n<code>pacman -S git patch unzip</code> command is not working properly which causes the fetch errors stated in step 4.\r\nI finally install the packages by downloading the binary files in MSYS2 [website](https://packages.msys2.org/updates) and manually copy to msys64 folder.\r\nFor now, it's compiling without additional modifications in WORKSPACE.", "**Final Update**\r\nI did additional changes during last attempt.\r\n1. Downgrading base Anaconda python from 3.8 to 3.6.10\r\n2. Replacing -std=c++14 in .bazelrc with  /std:c++14 (fix warnings)\r\n\r\n```\r\nINFO: Elapsed time: 10051.227s, Critical Path: 1525.10s\r\nINFO: 6150 processes: 6150 local.\r\nINFO: Build completed successfully, 9158 total actions\r\nINFO: Build completed successfully, 9158 total actions\r\n```\r\nIt worked!\r\nI am closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44569\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44569\">No</a>\n"]}, {"number": 44568, "title": "Inconsistency in loss on SAME data for train and validation modes", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Colab)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: Colab\r\n- GPU model and memory: Colab has k80 I think\r\n\r\n\r\n**Describe the current behavior**\r\nI'm implementing a semantic segmentation model with images. As a good practice I tested my training pipeline with just one image and tried to over-fit that image. To my surprise, when training with the exactly the same images, the loss goes to 0 as expected but when evaluating THE SAME IMAGES, the loss is much much higher, and it keeps going up as the training continues. So the segmentation output is garbage when `training=False`, but when run with `training=True` is works perfectly.\r\n\r\nTo be able to anyone to reproduce this I took the official [segmentation tutorial](https://www.tensorflow.org/tutorials/images/segmentation\r\n) and modified it a little for training a convnet from scratch and just 1 image. The model is very simple, just a sequence of Conv2D with batch normalization and Relu. The results are the following\r\n\r\n![Screenshot from 2020-11-03 18-04-17](https://user-images.githubusercontent.com/8033598/98050621-18cf3c80-1e00-11eb-93b0-0470054ddc50.png)\r\n\r\nAs you see the loss and eval_loss are really different, and making inference to the image gives perfect result in training mode and in eval mode is garbage.\r\n\r\n**Describe the expected behavior**\r\nI know Batchnormalization behaves differently in inference time since it uses the averaged statistics calculated whilst training. Nonetheless, since we are training with just 1 same image and evaluating in the same image, this shouldn't happen right? Moreover I implemented the same architecture with the same optimizer in Pytorch and this does not happen there. With pytorch it trains and eval_loss converges to train loss\r\n\r\n**Standalone code to reproduce the issue**\r\nHere you can find the above mentioned https://colab.research.google.com/drive/18LipgAmKVDA86n3ljFW8X0JThVEeFf0a#scrollTo=TWDATghoRczu\r\nand at the end also the Pytorch implementation\r\n", "comments": ["@charlielito,\r\nI do not have access to the Colab Notebook you have linked. Could you please provide the required permissions to view the files. Thanks!", "@amahendrakar  sorry, forgot to change the permissions. Now you should be able to access it", "@amahendrakar ", "@charlielito,\r\nSorry for the delayed response. \r\n\r\n@ymodak,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/0fad546e9be6cb2836ed033cdeee99cf/44568.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/18d074388becf3d26924f8a1a0156be3/44568-tf-nightly.ipynb). Please find the attached gist. Thanks!", "I have found a very similar issue. Could this be linked in any way to the various discussions about Batch Normalization from 2018? e.g. http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/ and https://github.com/keras-team/keras/pull/9965\r\n\r\nMy understanding was that this had been fixed in TF2, but looks like a very similar problem, relating to differences in behaviour of batch norm with `training' value set to True or False.", "I'm also having a similar issue. However, I noticed that in the colab file above, if we set the number of epochs `EPOCHS` to a larger value (e.g., 400), the validation loss will eventually also decrease steadily and converge to 0 like the train loss. I suspect that, while there is definitely something to be checked in the way the training parameters of the `BatchNormalization` layer are learned (and possibly initialized as well), the problem might not necessarily be caused by a \"logical\" bug in the `tf.keras` implementation. Indeed, by the way the batch-normalization layer works:\r\n- In training mode, the layer will try to learn parameters that best fit the statistics of the training data (a single image in the case above), but these learned parameters are only used in *inference* mode. In training mode, the forward pass will use the statistics of the actual input data rather than the learned parameters. This means that **it's possible that at the same time the network overfits perfectly to the training data, but the learned parameters from the batch normalization layer do not represent the statistics of the input perfectly**. This is coherent with the fact when the network has overfitted, running in training mode its prediction is perfect: the learned parameters of the batch normalization layers will be ignored (the statistics from the input data will be used instead), while the other layers will use the parameters optimized for the data to which it has overfitted.\r\n- In inference mode, the layer will actually use the learned parameters rather than the input statistics. This is also coherent with the wrong prediction from the overfitted network: my claim is that at epoch 100 the learned parameters of batch normalization do not represent well the statistics of the training data yet, but they gradually do so later (epoch 400).\r\n\r\nI believe that the problem might be related also to the number of other learnable parameters, and that the network could somehow learn to overfit these parameter \"faster\" than the batch-normalization parameters. As a quick test, I reduced the number of `ConvBNReLU` layers in the model from 15 to 2, and the validation loss is still larger than the training loss at the beginning, but way closer to it than when 15 layers are used. Furthermore, it does not increase over time, and also eventually converges to 0.\r\n\r\nTo conclude, I believe that this is a major issue, since in practice I would expect the statistics of large datasets to take (much) longer to be learned than those from this toy example, but it could be that the layer is still doing \"what it should do\".", "It might be an idiomatic issue, but I think what you denote as _learned parameters_ are not actually _learned_ parameters (learned in sense of being updated by backprop), but just the exponential averaged statistics from the training batches (mean and var). \r\n\r\nBatch norm __does__ have learned parameters which are _beta_ and _alpha_, which are used indeed in botch training and testing modes. Just the normalization (mean and var) in training are calculated from the batch data whilst in inference from the values averaged while training. See https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\r\n\r\nSo I thought, as you said, it had something to do with the averaged statistics not converging fast enough to the real statistics . Because we are training with 1 image and testing with the same image I denote as _real_ statistics the statistics from that image. If you set `momentum=0.0` in the BatchNorm layer, the averaged statistics should match perfectly with the statistics from the current batch (which is just 1 image, `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`). If you try with that it converges almost immediately. Also I tried with `momentum=0.9` (which is the equivalent default value in pytorch) and it works and converges faster (as in pytorch).\r\n\r\nSo this issue could be closed since it has more to do with defaults values :sweat_smile: ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44568\">No</a>\n", "Great! Yes, by _learnable_ parameters I meant the \"non-trainable\" parameters `self.moving_mean` and `self.moving_var` which are updated with the data received during training. And good to see that it was indeed too high a `momentum`that was causing slow convergence, at least now we know where the problem was."]}, {"number": 44567, "title": "tf-nightly-gpu looking for cusolver64_10.dll on a Cuda 11.1 / cuDNN 8.0.4 RTX3080 Build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.5.0-dev20201103\r\n- Python version: 3.8.5\r\n- Installed using: pip\r\n- CUDA/cuDNN version: Cuda 11.1 cuDNN 8.0.4\r\n- GPU model and memory: Nvidia RTX 3080\r\n\r\n**Describe the problem**\r\nAfter installing Cuda 11.1 / cuDNN 8.0.4 and tf-nightly-gpu (today's 1132020), I tried to test that tensorflow sees my gpu, but it throws an error saying it 'Could not load dynamic library 'cusolver64_10.dll''\r\n\r\nI do have cusolver64_11.dll in my Nvidia bin folder, but for some reason TF is looking for the Cuda 10 version of that one specific file (it found and opened all the other Cuda11 dlls) \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npython\r\nimport tensorflow as tf\r\nassert tf.test.is_gpu_available()\r\n\r\n", "comments": ["@anisoulk \r\nPlease refer to these existing issues with same error and let us know:\r\n#44291 #44128 # 44381", "@Saduf2019 Thanks for getting back to me. I did look at the issues you mentioned, the applicable temporary fixes suggested there are: \r\n\r\n1. Revert to Cuda 10.x : I can't do that since I have an RTX3080 GPU card, which is only compatible with Cuda 11.x \r\n2. The other suggestion floating around is to \"rename\" the cusolver64_11.dll in my current Cuda11.1/bin folder to cusolver64_10.dll... Are you saying this is the only workaround for this issue?", "@anisoulk \r\nPlease try them and let us know.", "I did try them, here are the results: \r\n1. Revert to Cuda 10.x : Not Compatible with my GPU (Nvidia RTX 3080)\r\n2. Changed cusolver64_11.dll to cusolver64_10,dll : This indeed removed the error, however it broke every thing for me, I have all kinds of errors every time I try to train models (models that I know work fine from running them before on tf-cpu) ", "@Saduf2019 Any feedback on this ? ", "I have exactly the same problem, with exactly the same specs:\r\n\r\n- OS Platform and Distribution : Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.5.0.dev20201110\r\n- Python version: 3.8.6\r\n- Installed using: pip\r\n- CUDA/cuDNN version: Cuda 11.1 cuDNN 8.0.4\r\n- GPU model and memory: Nvidia RTX 3080", "exactly the same problem,with almost the same specs:\r\n\r\ntensorflow version: 2.4.0rc1", "After changing cusolver64_11.dll to cusolver64_10,dll, some **errors** are thrown as follows.\r\n\r\n ```\r\n File \"C:/Users/User/Desktop/deep-spatial-temporal/deep-spatial-temporal/test_gpu.py\", line 67, in <module>\r\n    validation_data=(x_test, y_test))\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1103, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 784, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 844, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2972, in __call__\r\n    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1948, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 561, in call\r\n    ctx=ctx)\r\n  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InternalError:  Blas xGEMM launch failed : a.shape=[1,128,4608], b.shape=[1,4608,128], m=128, n=128, k=4608\r\n\t [[node sequential/dense/MatMul (defined at /Users/User/Desktop/deep-spatial-temporal/deep-spatial-temporal/test_gpu.py:67) ]] [Op:__inference_train_function_790]\r\n```\r\n\r\nThen I added the following commands, it worked well.\r\n\r\n```\r\ngpus = tf.config.experimental.list_physical_devices(device_type='GPU')\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\n\r\n### System information\r\n\r\n> OS Platform and Distribution : Windows 10\r\n> Tensorflow: tf-nightly-gpu 2.5.0dev20201117\r\n> Python version: 3.7.8\r\n> Installed using: pip\r\n> CUDA/cuDNN version: Cuda 11.1 cuDNN 8.0.5\r\n> GPU model and memory: Nvidia RTX 3080\r\n\r\n", "@sanjoy @pkanwar23 ", "Can you please try with CUDA 11.0?  TF 2.4 (and nightly) is built and tested against CUDA 11.0, not 11.1.", "> After changing cusolver64_11.dll to cusolver64_10,dll, some **errors** are thrown as follows.\r\n> \r\n> ```\r\n> File \"C:/Users/User/Desktop/deep-spatial-temporal/deep-spatial-temporal/test_gpu.py\", line 67, in <module>\r\n>    validation_data=(x_test, y_test))\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1103, in fit\r\n>    tmp_logs = self.train_function(iterator)\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 784, in __call__\r\n>    result = self._call(*args, **kwds)\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 844, in _call\r\n>    return self._stateless_fn(*args, **kwds)\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2972, in __call__\r\n>    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1948, in _call_flat\r\n>    ctx, args, cancellation_manager=cancellation_manager))\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 561, in call\r\n>    ctx=ctx)\r\n>  File \"C:\\software\\python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 60, in quick_execute\r\n>    inputs, attrs, num_outputs)\r\n> tensorflow.python.framework.errors_impl.InternalError:  Blas xGEMM launch failed : a.shape=[1,128,4608], b.shape=[1,4608,128], m=128, n=128, k=4608\r\n>     [[node sequential/dense/MatMul (defined at /Users/User/Desktop/deep-spatial-temporal/deep-spatial-temporal/test_gpu.py:67) ]] [Op:__inference_train_function_790]\r\n> ```\r\n> \r\n> Then I added the following commands, it worked well.\r\n> \r\n> ```\r\n> gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\r\n> for gpu in gpus:\r\n>     tf.config.experimental.set_memory_growth(gpu, True)\r\n> ```\r\n> \r\n> ### System information\r\n> > OS Platform and Distribution : Windows 10\r\n> > Tensorflow: tf-nightly-gpu 2.5.0dev20201117\r\n> > Python version: 3.7.8\r\n> > Installed using: pip\r\n> > CUDA/cuDNN version: Cuda 11.1 cuDNN 8.0.5\r\n> > GPU model and memory: Nvidia RTX 3080\r\n\r\nThank u! It works!", "> Can you please try with CUDA 11.0? TF 2.4 (and nightly) is built and tested against CUDA 11.0, not 11.1.\r\n\r\n@sanjoy Thanks for the feedback, reverting back to CUDA 11.0 removed the error and GPU is now working. However something weird caught my eye: Training on GPU is considerably slower (3x slower) than on CPU, granted I'm not training a big network (5 layers NN) but still 3 times slower is odd, is this normal ? or is this some incompatibility issue ?  ", "3x slower is odd. We haven't seen this in our internal tests. You might want to use the profiling tools to see where the bottleneck lies.  (https://www.tensorflow.org/guide/profiler).\r\n\r\nI'm going to close the issue for now. If you're able identify specific Ops that are slower, or some other bottleneck, you can open another issue.  We'll be happy to look at it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44567\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44567\">No</a>\n", "@anisoulk \r\nI had the exact same issue and the fix worked. In regards to the performance drop, I also had the same problem. I tried increasing the complexity of the model as follows: 150, 300, 2000, 6600. I noticed the GPU began outperforming the CPU and remained roughly constant despite the increasing units. It's possible the bottleneck is in the transfer to the GPU, in which case for smaller models the CPU would outperform it as you described. ", "Same issue with tf 2.4 and CUDA11.1 CuDNN8.0.5 on GTX1660Ti. I download that cusolver64_10.dll from [https://drive.google.com/uc?id=1-3Yk-QZ1eUta1T40BaxFpO4uyn7BPU4o&export=download](url) and put it in C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1\\bin. Then it works well."]}, {"number": 44564, "title": "TF 2.3 S3 client having permission issues with `tf.data.TFRecordDataset()`", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:N/A\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nTF 2.3 `tf.data.TFRecordDataset()` failed to load files from S3.\r\nSide note: \r\n`tf_file_io` is working properly for the same setup.\r\n\r\n```\r\n>> from tensorflow.python.lib.io import file_io as tf_file_io\r\n>> tf_file_io.file_exists('SOME_S3_FILE')\r\n\r\nTrue\r\n```\r\n\r\nSee error log below.\r\n**Describe the expected behavior**\r\nSame API and environments works well with TF 2.1 and TF 1.15.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\ndataset = tf.data.TFRecordDataset('SOME_S3_PATH')\r\nfor raw_record in dataset.take(10):\r\n  print(repr(raw_record))\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 736, in __next__\r\n    return self.next()\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 772, in next\r\n    return self._next_internal()\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 764, in _next_internal\r\n    return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 2105, in execution_mode\r\n    executor_new.wait()\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/eager/executor.py\", line 67, in wait\r\n    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: AWS Credentials have not been set properly. Unable to access the specified S3 location\r\n```", "comments": ["@shaowei-su \r\nPlease provide with complete stand alone code for us to replicate the issue faced, or if possible share a colab gist with error reported.", "Thank you @Saduf2019 taking look into this.\r\nTo reproduce the error it requires:\r\n- S3 bucket access\r\n- AWS credential setup for the given S3 bucket\r\n\r\n(Unfortunately, my current setup is not public accessible/sharable) \r\nOnce its setup then you can run the following tests:\r\n\r\n1. Smoke test (should pass)\r\n```\r\nfrom tensorflow.python.lib.io import file_io as tf_file_io\r\ntf_file_io.file_exists('SOME_S3_TFRECORD_FILE')\r\n```\r\n2. TFRecordDataset load (should fail)\r\n```\r\ndataset = tf.data.TFRecordDataset('SOME_S3_TFRECORD_FILE')\r\nfor raw_record in dataset.take(10):\r\n  print(repr(raw_record))\r\n```", "Quick updates on my end:\r\nThis issue seems related to the S3 multi part download that's introduced in TF2.2. \r\nBy disable the functionality with `os.environ['S3_DISABLE_MULTI_PART_DOWNLOAD'] = '1'`,  tf.data.TFRecordDataset() can load data from S3 properly.", "@shaowei-su \r\n\r\nI am working on this error. But I am not able to reproduce it locally. Could you tried `tf.io.read_file('SOME_S3_TFRECORD_FILE')`. If it still falied for the same reason, could you please tell me the size of that file and check your environmental variables ( especially `AWS_REGION` because `s3` usually fails because of bucket region ) ?", "Hi @vnvo2409 \r\nYeah `read_file` failed with the same reason, see stacktrace\r\n```\r\n2020-12-16 19:07:50.187041: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at whole_file_read_ops.cc:116 : Failed precondition: AWS Credentials have not been set properly. Unable to access the specified S3 location\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 562, in read_file\r\n    filename, name=name, ctx=_ctx)\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 600, in read_file_eager_fallback\r\n    attrs=_attrs, ctx=ctx, name=name)\r\n  File \"/home/default_user/.conda/envs/user/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: AWS Credentials have not been set properly. Unable to access the specified S3 location [Op:ReadFile]\r\n```\r\nIn this use case, credentials are not passed in as environment variables but configured for Amazon EC2 instance that has an IAM role.\r\n\r\nSide note: `tf.io.read_file('SOME_S3_TFRECORD_FILE')` works fine though by disabling multi part data loading with\r\n```\r\nimport os\r\nos.environ['S3_DISABLE_MULTI_PART_DOWNLOAD'] = '1' \r\n```\r\n", "Unfortunately, I don't have access to an EC2 instance right now. Could you please set the environement as follow in order to see what did happend behind the scene ?\r\n\r\n```python\r\nimport os\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\r\nos.environ[\"TF_CPP_MIN_VLOG_LEVEL\"] = \"5\"\r\nos.environ[\"AWS_LOG_LEVEL\"] = \"trace\"\r\n```", "The stack trace is quite long I pasted it here: https://gist.github.com/shaowei-su/4485e00a7a2d1e78f39275be3e7dd8f1", "@shaowei-su \r\nMany thanks for the stack trace. I am unable to understand what are happening. Please wait till the modular filesystem is ready. With that filesystem, error should be clearer. In addition, please check if there are any sensitive information inside the stack trace and remove it !\n\nMaybe it could be related here https://github.com/tensorflow/tensorflow/issues/43344", "@vnvo2409 thanks for reminding! deleted the stack trace for now", "@shaowei-su \r\nMaybe your problem related to this issue https://github.com/aws/aws-sdk-cpp/issues/863 ?", "@vnvo2409 Yes! I can confirm that by adding `GetObejectVersion` permissions this issue is resolved. Thanks for all the help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44564\">No</a>\n"]}, {"number": 44562, "title": "Remove unnecessary check for TARGET in the xtensa_hifimini makefile.", "body": "Manually confirmed that:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa_hifimini XTENSA_CORE=<core> test_keyword_benchmark\r\n```\r\n\r\nGives the following output:\r\n```\r\ntensorflow/lite/micro/testing/test_xtensa_hifimini_binary.sh tensorflow/lite/micro/tools/make/gen/xtensa_hifimini_xtensa_hifimini/bin/keyword_benchmark '~~~ALL TESTS PASSED~~~'\r\nInitializeKeywordRunner() took 1389000 ticks (1389 ms)\r\nKeywordRunNIerations(1) took 89318 ticks (89 ms)\r\nKeywordRunNIerations(10) took 892739 ticks (892 ms)\r\ntensorflow/lite/micro/tools/make/gen/xtensa_hifimini_xtensa_hifimini/bin/keyword_benchmark: FAIL - '~~~ALL TESTS PASSED~~~' not found in logs.\r\n```\r\n\r\nFixes #43898\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44561, "title": "Convert TensorFlow to TensorFlow Lite - very small model after conversion", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\nFollowing my previous issues https://github.com/tensorflow/tensorflow/issues/44091 https://github.com/tensorflow/tensorflow/issues/44435 of converting the Tensorflow model to Tensorflow Lite I prepared a notebook inspired by the notebook from the link [Object_Detection_in_TFLite](https://github.com/sayakpaul/E2E-Object-Detection-in-TFLite/blob/master/Object_Detection_in_TFLite.ipynb). From notebook  we can see that after converting model ```ssd_mobilenet_v2_320x320_coco17_tpu-8``` [Training a pets detector model within minutes with TFOD API](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/Training_a_pets_detector_model_within_minutes_with_TFOD_API.ipynb) to TFLite the model size is 5711632 Bytes. In my cases the size of model is 516 bytes [mobilenet_v2_to_tflite](https://colab.research.google.com/drive/1GYKWFR7A6WknexnFa312eCa3ZST58i_3?usp=sharing)\r\n\r\nWhere Is the problem? I try to convert the model up to a month and the results are always the same. Model size is around 500 bytes. I thought it was a bug due to the fact that there is something wrong with training the model but I can see that even for the standard model with the repository the error is the same.\r\n\r\n", "comments": ["@Rariusz,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/a060a9cf582e283a38ba647000f2ee02/44561.ipynb). \r\n\r\nHowever, the issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/5e8c0f4835547183cb6629a2facde125/44561-tf-nightly.ipynb#scrollTo=cohE9r5hfI1C). On converting the model, the size of the `.tflite` file is around 11 MB. Please check the linked gist for reference. Thanks!", "@amahendrakar Thank you for your help !!! So far the results are the same. For model graph and tflite frozen graph. Now I will want to run the model on google coral. But first I need to quantization. We'll see if there will be any more problems", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44561\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44561\">No</a>\n"]}, {"number": 44560, "title": "Custom LearningRateSchedule not called within a Mirrored Strategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): \r\n**From the tutorial [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training) with a Custom LearningRateSchedule**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Colab**\r\n- TensorFlow version (use command below): **2.3.0**\r\n- Python version: **3.6.9**\r\n- GPU model and memory: **GPU from Colab**\r\n\r\n**Describe the current behavior**\r\nIn a distributed environment, when adding a custom LearningRateSchedule to Adam optimizer to decay the learning rate over epochs, the learning rate is not decayed as expected.\r\nI tested the same code in an environment without the distributed scope and the LR decay over the epochs.\r\n\r\n**Describe the expected behavior**\r\nThe optimizer to call the LearningRateSchedule __call__ method to decay the learning rate after applying the gradients\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease check this Colab from the TF tutorial: https://colab.research.google.com/drive/1sT3MZ5hdGvsPhFQDrCFXzMR760HPCSsc?usp=sharing\r\n\r\nSee chapter: **Training loop**\r\n\r\n**Other info / logs** \r\n```\r\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n    def __init__(self):\r\n      super(CustomSchedule, self).__init__()\r\n      self.lr = 0.01\r\n\r\n    def __call__(self, step):\r\n      self.lr = self.lr/10\r\n      print(\"NEW LR:\", self.lr)\r\n      return self.lr\r\n\r\nwith strategy.scope():\r\n    model = create_model()\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=CustomSchedule())\r\n    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\r\n```\r\nThe `learning_rate=CustomSchedule()` is the only line I changed from the tutorial.\r\n\r\nSee the Colab for the **train_step**", "comments": ["@houseofai \r\nPlease share simple stand alone code to replicate the issue or if possible share a colab gist with the error reported.", "After a deeper analysis, I was able to see that the scheduler was called", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44560\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44560\">No</a>\n"]}, {"number": 44559, "title": "Cannot install latest nightly version via pip install tf-nightly", "body": "Try `pip install tf-nightly` will install 2.5.0.dev20201029\r\n\r\nTry `pip install tf-nightly==2.5.0.dev20201102` will output\r\n```\r\nERROR: Could not find a version that satisfies the requirement tf-nightly==2.5.0.dev20201102 (from versions: 2.4.0.dev20200903, 2.4.0.dev20200904, 2.4.0.dev20200905, 2.4.0.dev20200906, 2.4.0.dev20200907, 2.4.0.dev20200908, 2.4.0.dev20200909, 2.4.0.dev20200910, 2.4.0.dev20200911, 2.4.0.dev20200912, 2.4.0.dev20200913, 2.4.0.dev20200914, 2.4.0.dev20200915, 2.4.0.dev20200916, 2.4.0.dev20200917, 2.4.0.dev20200918, 2.4.0.dev20200919, 2.4.0.dev20200920, 2.4.0.dev20200921, 2.4.0.dev20200922, 2.4.0.dev20200923, 2.4.0.dev20200924, 2.4.0.dev20200925, 2.4.0.dev20200926, 2.4.0.dev20200927, 2.4.0.dev20200928, 2.4.0.dev20200929, 2.4.0.dev20200930, 2.4.0.dev20201001, 2.4.0.dev20201002, 2.4.0.dev20201003, 2.4.0.dev20201004, 2.4.0.dev20201005, 2.4.0.dev20201007, 2.4.0.dev20201008, 2.4.0.dev20201010, 2.4.0.dev20201011, 2.4.0.dev20201012, 2.4.0.dev20201014, 2.4.0.dev20201015, 2.4.0.dev20201016, 2.4.0.dev20201017, 2.4.0.dev20201018, 2.4.0.dev20201019, 2.4.0.dev20201020, 2.4.0.dev20201021, 2.4.0.dev20201022, 2.4.0.dev20201023, 2.5.0.dev20201024, 2.5.0.dev20201025, 2.5.0.dev20201026, 2.5.0.dev20201027, 2.5.0.dev20201028, 2.5.0.dev20201029)\r\nERROR: No matching distribution found for tf-nightly==2.5.0.dev20201102\r\n```", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44559\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44559\">No</a>\n"]}, {"number": 44558, "title": "Fix link text with updated guide title", "body": "Update the link text with current guide title.", "comments": []}, {"number": 44557, "title": "ValueError: No gradients provided for any variable: ['Variable:0', 'Variable_1:0']", "body": "I am facing the same issue gradient issue, I google it but not found any working solution. can anyone suggest me how to\r\ndeal with this problem..\r\n```\r\nimport tensorflow as tf    version is 2.1.0\r\nprint('training data shape    ', X.shape)\r\nprint('training labels shape  ', y.shape)\r\nprint('validation data shape  ', X_v.shape)\r\nprint('validation labels shape', y_v.shape)\r\nprint('test data shape        ', X_t.shape)\r\n\"\"\"\r\nprint following\r\ntraining data shape     (10000, 784)\r\ntraining labels shape   (10000, 10) # 10 classes label one hot encoded\r\nvalidation data shape   (2000, 784) \r\nvalidation labels shape (2000, 10) # 10 classes label one hot encoded\r\ntest data shape         (4000, 784)\r\n\"\"\"\r\ndef build_computation_graph(X, y, X_val, y_val, X_test):\r\n    graph = tf.Graph()\r\n    IMAGE_SIZE, NUM_CLASSES, SEED = 28, 10, 42\r\n    learning_rate = 0.5\r\n    \r\n    with graph.as_default():    \r\n        ############### Data sets conversion into tensorflow constant ##########################\r\n        tf_train_dataset = tf.constant(X, dtype=tf.float64)\r\n        tf_valid_dataset = tf.constant(X_val, dtype=tf.float64)\r\n        tf_test_dataset = tf.constant(X_test, dtype=tf.float64)\r\n        tf_train_label = tf.constant(y, dtype=tf.float64)\r\n        tf_validation_label = tf.constant(y_val, dtype=tf.float64)\r\n\r\n        ###################### weight matrix and Biases initialization ####################################\r\n\r\n        # Initialized By using random values following a (truncated) normal distribution\r\n        weights = tf.Variable(tf.random.truncated_normal((IMAGE_SIZE * IMAGE_SIZE, NUM_CLASSES),\r\n                                                         seed=tf.random.set_seed(SEED), dtype=tf.float64),\r\n                                                         dtype=tf.float64, trainable=True)\r\n\r\n        biases = tf.Variable(tf.zeros([NUM_CLASSES] , dtype=tf.float64), trainable=True)\r\n\r\n        # multiply weight matrix and add Bias\r\n        logit = tf.add(tf.matmul(tf_train_dataset, weights), biases)\r\n\r\n        # generate loss function\r\n        # labels: Each row labels[i] must be a valid probability distribution\r\n        # logits: Unscaled log probabilities.\r\n        loss =  lambda : tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_label, logits=logit))\r\n\r\n        # optimizer\r\n        #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)\r\n        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(loss, var_list=[weights,biases])\r\n\r\n        train_prediction = tf.nn.softmax(logit)\r\n        valid_prediction = tf.nn.softmax(tf.nn.matmul(tf_valid_dataset, weights) + biases)\r\n        test_prediction = tf.nn,softmax(tf.nn.matmul(tf_test_dataset, weights) + biases)\r\n        return graph\r\n# End\r\n\r\nbuild_computation_graph(X, y, X_v, y_v, X_t)\r\n\r\n\r\n #########################   Error ##################################################\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-36-311f05332c36> in <module>\r\n----> 1 build_computation_graph(X, y, X_v, y_v, X_t)\r\n\r\n<ipython-input-35-f0e7793c1da2> in build_computation_graph(X, y, X_val, y_val, X_test)\r\n     31         # optimizer\r\n     32         #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(loss)\r\n---> 33         optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate).minimize(loss, var_list=[weights,biases])\r\n     34 \r\n     35         train_prediction = tf.nn.softmax(logit)\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)\r\n    316         loss, var_list=var_list, grad_loss=grad_loss)\r\n    317 \r\n--> 318     return self.apply_gradients(grads_and_vars, name=name)\r\n    319 \r\n    320   def _compute_gradients(self, loss, var_list, grad_loss=None):\r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_gradients(self, grads_and_vars, name)\r\n    424       ValueError: If none of the variables have gradients.\r\n    425     \"\"\"\r\n--> 426     grads_and_vars = _filter_grads(grads_and_vars)\r\n    427     var_list = [v for (_, v) in grads_and_vars]\r\n    428 \r\n\r\nC:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py in _filter_grads(grads_and_vars)\r\n   1037   if not filtered:\r\n   1038     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n-> 1039                      ([v.name for _, v in grads_and_vars],))\r\n   1040   if vars_with_empty_grads:\r\n   1041     logging.warning(\r\n\r\nValueError: No gradients provided for any variable: ['Variable:0', 'Variable_1:0'].\r\n```\r\nAll data value lies between [-1, 1]\r\n\r\n**System information**\r\n- OS Platform - window 10\r\n- tensorFlow version (use command below):- 2.1.0\r\n- Python version: 3.7.3\r\n- GPU model and memory: - No GPU support\r\n-", "comments": ["@Aksh-kumar,\r\nOn running the code, I am facing an error stating `NameError: name 'X' is not defined`. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide all the supporting files required to reproduce the issue reported here. Thanks!\r\n", "@amahendrakar  I am sharing pickled data. Extract Data_Sub.zip, inside Data_Sub folder 'data_subset.pkl'  file is there. please load the data by\r\n```\r\ndata = None\r\nwith open('Data_Sub/data_subset.pkl', 'rb') as f:  # provide  the path if require\r\n    data = pickle.load(f)\r\nX, y, X_v, y_v, X_t = data['X'], data['y'], data['X_v'], data['y_v'], data['X_t']\r\n```\r\n[Data_Sub.zip](https://github.com/tensorflow/tensorflow/files/5488015/Data_Sub.zip)", "It's been 2 days any progress guys....", "@Aksh-kumar,\r\nSorry for the delayed response.\r\n\r\n@rmothukuru,\r\nI was able to reproduce the issue with TF v2.1, [TF v2.3](https://colab.research.google.com/gist/amahendrakar/74390483fd9807e1057f19022655e55d/44557.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/8071432c4a86e466e221b7b250f16ad0/44557-tf-nightly.ipynb). Please find the attached gist. Thanks!", "@Aksh-kumar,\r\nAs mentioned in the [Documentation for Graphs](https://www.tensorflow.org/api_docs/python/tf/Graph#using_graphs_directly_deprecated), \r\n\r\n> A `tf.Graph` can be constructed and used directly without a `tf.function`, as was required in `TensorFlow 1`, but this is deprecated and it is recommended to use a `tf.function` instead.If a graph is directly used, other deprecated TensorFlow 1 classes are also required to execute the graph, such as a tf.compat.v1.Session.\r\n\r\nSo, please use `tf.function` instead of `tf.Graph`. Thanks!", "@rmothukuru sorry for the delayed response, I went through the documentation but I am not able to grasp it completely, can you please give me sample code changes inside [TF-nightly ](https://colab.research.google.com/gist/amahendrakar/8071432c4a86e466e221b7b250f16ad0/44557-tf-nightly.ipynb) that @amahendrakar share with us. Thanks in advance.", "any update guys..", "Hi @Aksh-kumar, Github is for bugs/performance issues. For support issues, it's best to try Stack Overflow. There is a larger community there that can help.\r\n\r\n+1 to the comment from @rmothukuru, using tf.Graph is not recommended in TF2.\r\nIt seems to me that your use case could be solved by using `tf.Keras`. This library makes it simple to build models. [I'm linking the documentation here](https://www.tensorflow.org/guide/keras/sequential_model). In general, there's no reason to write your model layers from scratch. Keras implements [a large number of layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers), and if none of those work for you, you can always [try creating your own custom layer with subclassing.](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\r\n\r\nHope this helps.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44557\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44557\">No</a>\n", "thanks @nikitamaia for responding back to me,I seen your suggestions, which ,I found satisfactory but I was just testing these API in python and my actual implementation rely on c++, can you guide me if I want to do the same thing in c++ what will be the approach?"]}, {"number": 44556, "title": "Second derivative of reduce_prod returns NaN", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: executing on CPU\r\n- GPU model and memory: executing on CPU\r\n\r\n**Describe the current behavior**\r\n\r\nComputing the second derivative of a function containing `tf.reduce_prod`, where one of the arguments is zero, returns NaN.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe second derivative should be finite in those cases.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\ndef grad(f):\r\n    x = tf.Variable([0.])\r\n    with tf.GradientTape() as t:\r\n        y = f(x)\r\n    g = t.gradient(y, x)\r\n    return g\r\n\r\ndef grad2(f): \r\n    x = tf.Variable([0.])\r\n    with tf.GradientTape() as t2:\r\n        with tf.GradientTape() as t1:\r\n             y = f(x)\r\n        g = t1.gradient(y, x)\r\n    g2 = t2.jacobian(g, x)\r\n    return g2\r\n\r\ngrad(lambda x: tf.reduce_prod(tf.constant([2.,0.])*x*x)) # returns 0 as expected\r\ngrad2(lambda x: tf.reduce_prod(tf.constant([2.,0.])*x*x)) # returns NaN\r\ngrad2(lambda x: 2.*x*x) # returns 4 as expected\r\n```", "comments": ["@swertz \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/c7429b868d443d188c6d029122e8ca89/untitled458.ipynb). Please share a colab gist with the error reported.", "I could reproduce in colab with v2.3.0, please find it [here](https://colab.research.google.com/drive/1Nm-KoLV1ZDRTS6NarXgMNjcEQJumjMbH?usp=sharing).", "@swertz \r\nI do not have access to the above link.", "> @swertz\r\n> I do not have access to the above link.\r\n\r\nSorry! Should be fine now...", "@swertz \r\nI ran the code and this had been fixed in nightly version, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/7a30d232977727a555780404b15d0d4c/untitled458.ipynb).", "Great, thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44556\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44556\">No</a>\n"]}, {"number": 44554, "title": "\" ImportError: cannot import name 'anchor_generator_pb2' from 'object_detection.protos' \" when trying to convert a custom model for use in the Object Detection Android example app", "body": "Hi, I'm want to use the Android example app for Object Detection with a model with greater resolution than the standard 300x300. I have downloaded the **ssd_resnet_50_fpn_coco** model from [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md](url), and I'm following the guide in the section _Custom model used_ on [https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android](url).\r\n\r\nWhen running:\r\n`python object_detection/export_tflite_ssd_graph.py --pipeline_config_path ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/pipeline.config --trained_checkpoint_prefix ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/model.ckpt --output_directory exported_model`\r\n\r\nI get the output:\r\n`2020-11-03 15:25:55.004086: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n2020-11-03 15:25:55.009217: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"object_detection/export_tflite_ssd_graph.py\", line 97, in <module>\r\n    from object_detection import export_tflite_ssd_graph_lib\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\python37\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\object_detection\\export_tflite_ssd_graph_lib.py\", line 27, in <module>\r\n    from object_detection import exporter\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\python37\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\object_detection\\exporter.py\", line 24, in <module>\r\n    from object_detection.builders import model_builder\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\python37\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\object_detection\\builders\\model_builder.py\", line 20, in <module>\r\n    from object_detection.builders import anchor_generator_builder\r\n  File \"C:\\Users\\Lucas\\.conda\\envs\\python37\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\object_detection\\builders\\anchor_generator_builder.py\", line 27, in <module>\r\n    from object_detection.protos import anchor_generator_pb2\r\nImportError: cannot import name 'anchor_generator_pb2' from 'object_detection.protos' (C:\\Users\\Lucas\\.conda\\envs\\python37\\lib\\site-packages\\object_detection-0.1-py3.7.egg\\object_detection\\protos\\__init__.py)`\r\n\r\nI have also tried this with the model **ssd_mobilenet_v2_oid_v4** as mentioned in the guide, which yields the same output.\r\n\r\nSome extra info:\r\n- Windows 10\r\n- Python 3.7.9\r\n- Tensorflow 1.15.0", "comments": ["Okay, I was able to solve this problem by downloading **proto.exe** from [https://github.com/protocolbuffers/protobuf/releases/tag/v3.13.0](url), placing it in /models/research/ and in there run:\r\n\r\n`protoc object_detection/protos/*.proto --python_out=.'\r\n\r\nand then copying the setup.py file from object_detection/packages/tf1/ and running:\r\n\r\n`python -m pip install .`\r\n\r\nI then can go to the next step in the guide and run:\r\n\r\n`tflite_convert --input_shape=1,640,640,3 --input_arrays=normalized_input_image_tensor --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --allow_custom_ops --graph_def_file=exported_model/tflite_graph.pb --output_file=export/detect.tflite`\r\n\r\nThis generates the model detect.tflite, which I can then place in the assets folder of the Android app.\r\n\r\nI then also change:\r\n`private static final int TF_OD_API_INPUT_SIZE = 300;`\r\nto\r\n`private static final int TF_OD_API_INPUT_SIZE = 640;`\r\n\r\nand\r\n`private static final boolean TF_OD_API_IS_QUANTIZED = true;`\r\nto\r\n`private static final boolean TF_OD_API_IS_QUANTIZED = false;`\r\n\r\nHowever when deploying the app I now get the error:\r\n\r\n`java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.`\r\n\r\nFull error log:\r\n\r\n`2020-11-03 15:54:25.008 14893-14893/org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 14893\r\n    java.lang.IllegalStateException: This model does not contain associated files, and is not a Zip file.\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.assertZipFile(MetadataExtractor.java:325)\r\n        at org.tensorflow.lite.support.metadata.MetadataExtractor.getAssociatedFile(MetadataExtractor.java:165)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:126)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\n        at org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\n        at android.view.TextureView.getTextureLayer(TextureView.java:415)\r\n        at android.view.TextureView.draw(TextureView.java:360)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21335)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21326)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21326)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.draw(View.java:22484)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21335)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1246)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.draw(View.java:22484)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21335)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21326)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21326)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21326)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21326)\r\n        at android.view.View.draw(View.java:22200)\r\n        at android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\n        at android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\n        at android.view.View.draw(View.java:22484)\r\n        at com.android.internal.policy.DecorView.draw(DecorView.java:812)\r\n        at android.view.View.updateDisplayListIfDirty(View.java:21335)\r\n        at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:559)\r\n        at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:565)\r\n        at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:647)\r\n2020-11-03 15:54:25.008 14893-14893/org.tensorflow.lite.examples.detection E/AndroidRuntime:     at android.view.ViewRootImpl.draw(ViewRootImpl.java:4332)\r\n        at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:4059)\r\n        at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3312)\r\n        at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2151)\r\n        at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8614)\r\n        at android.view.Choreographer$CallbackRecord.run(Choreographer.java:1316)\r\n        at android.view.Choreographer.doCallbacks(Choreographer.java:1113)\r\n        at android.view.Choreographer.doFrame(Choreographer.java:1013)\r\n        at android.view.Choreographer$FrameHandler.handleMessage(Choreographer.java:1239)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:245)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7945)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:631)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:971)`\r\n\r\nI found that a similar problem exists in the issue: [https://github.com/tensorflow/tensorflow/issues/44431](url). Maybe this is the same problem, I'm not sure. Either way that issue is not yet solved, and it seems weird to get this error when exactly following the official guide for using custom models for the Android app.", "I found a workaround by following this:\r\n[https://github.com/tensorflow/models/issues/9341](url)\r\n\r\nNow I no longer get any crash when using the app. However, I don't get any inference results at all. No bounding boxes. \r\nExample log:\r\n`2020-11-03 16:33:24.408 23251-25655/org.tensorflow.lite.examples.detection I/tensorflow: DetectorActivity: Preparing image 423 for detection in bg thread.\r\n2020-11-03 16:33:24.476 23251-25654/org.tensorflow.lite.examples.detection I/tensorflow: DetectorActivity: Running detection on image 423\r\n2020-11-03 16:33:26.413 23251-25654/org.tensorflow.lite.examples.detection I/tensorflow: MultiBoxTracker: Processing 0 results from 423`\r\n\r\nNote that this is using the **ssd_resnet_50_fpn_coco** model. When I use the **ssd_mobilenet_v2_oid_v4** model as in the example, I actually get bounding boxes, but these seem to have the wrong labels, even though I added the label map as described in the example.\r\n\r\nAs I need to use high-resolution inference, why could this problem occur?", "Can you share the labels file you are using to test the second model. One thing that causes issues is that the first label from COCO's list is a 'null' one, so you might just need to add a dummy label on the first line in your labels file.", "I'm using the same labels as for the original model.\r\n[labelmap.txt](https://github.com/tensorflow/tensorflow/files/5500276/labelmap.txt)\r\n\r\nBut I believe I figured out a solution by following the recently solved topic I linked above:\r\nhttps://github.com/tensorflow/tensorflow/issues/44431\r\n\r\nI changed the mean and standard deviation for the model in the **TFLiteObjectDetectionAPIModel.java** file to:\r\n\r\n`private static final float IMAGE_MEAN = 0f;\r\nprivate static final float IMAGE_STD = 1f`\r\n\r\nand now I get bounding boxes!\r\n\r\nHowever the inference is terribly slow. I learned that mobilenet is supposed to be faster than resnet, so now I'm using the _ssd_mobilenet_v1_fpn_coco \u2606_ model (640x640) instead. However inference takes approximately 1.5s on my phone, as compared to just 20ms for the standard 300x300 model...", "I see. \r\nYeah the inference latency is expected. Can you try the [GPU delegate](https://www.tensorflow.org/lite/performance/gpu) for your use-case? We have seen much better latencies with acceleration.", "I have not tried that yet. Might do so soon!", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44554\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44554\">No</a>\n"]}, {"number": 44553, "title": "tf.keras.model fit() CUDA crash when using generator inputs", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 33\r\n-   **TensorFlow installed from (source or binary)**: pip3.7 (binary)\r\n-   **TensorFlow version (use command below)**: v2.3.0-54-gfcc4b966f1 2.3.1\r\n-   **Python version**: 3.7\r\n-   **CUDA/cuDNN version**: 10.1\r\n-   **GPU model and memory**: NVIDIA RTX 2070 Max-Q 8GB\r\n-   **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nFor academic purposes, I'm trying to preprocess CIFAR10 images before training a keras sequential model. CIFAR10 training/test images are 32x32x3 by default, I want to resize them to 244x244x3. This is not possible with a single `tf.image.resize(..)` call, as it quickly crashes due to OOM. Thus, I am using a generator function to feed the input data to the `model.fit(..)` `x` parameter.\r\n\r\n### Source code / logs\r\nHere is my generator function:\r\n```python\r\ndef data_generator(images, labels, w, h):\r\n    batch_size = 10\r\n\r\n    i = 0\r\n    while i < len(images):\r\n        remaining = len(images) - i\r\n        batch = batch_size\r\n        if batch >= remaining:\r\n            batch = remaining\r\n        print(\"### generator {}-{} of {}\".format(i, i+batch, len(images)))\r\n        image_batch = tf.image.resize(images[i:i+batch], (w, h))\r\n        #image_batch = images[i:i+batch]\r\n        label_batch = labels[i:i+batch]\r\n        i += batch\r\n\r\n        yield image_batch, label_batch\r\n```\r\n\r\nThis is how I load the images:\r\n```python\r\n    # get the CIFAR10 dataset\r\n    (train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\r\n\r\n    # normalize pixel values to be between 0 and 1\r\n    train_images, test_images = train_images / 255.0, test_images / 255.0\r\n```\r\n\r\nThis is how I train the model:\r\n```python\r\n    train_generator = data_generator(train_images, train_labels, input_shape[0], input_shape[1])\r\n    validation_generator = data_generator(test_images, test_labels, input_shape[0], input_shape[1])\r\n    history = model.fit(x=train_generator, epochs=10,\r\n                        validation_data=validation_generator,\r\n                        use_multiprocessing=True)\r\n```\r\n\r\n... and finally, this is the crash message:\r\n```\r\n2020-11-03 14:26:31.048556: F tensorflow/stream_executor/cuda/cuda_driver.cc:219] Failed setting context: CUDA_ERROR_NOT_INITIALIZED: initialization error\r\n```", "comments": ["Update: this is happening because of the `use_multiprocessing=True` parameter. Once that is removed, training starts. Should this still be considered a bug?", "For using multiple gpu using [tf.distribute.MirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy) is recommended.", "This is not about using multiple GPUs. My system only has a single CUDA compute executor (the RTX 2070 Max-Q).", "@raymanfx \r\nCan you not resize to 224, as the image resolution of 32 is too low for that, and try tf.data instead of generator and let us know.\r\nYou may refer to this [example](https://sayak.dev/tf.keras/data_augmentation/image/2020/05/10/augmemtation-recipes.html).", "Initially I tried `tf.image.resize`, but that quickly crashed due to OOM (50.000 images in the dataset). This is why I'm trying to use a generator in the first place - I'd very much like to avoid that.", "You may want to try with latest TF version. Similar issue https://github.com/tensorflow/tensorflow/issues/44137 with latest TF version seems to be fixed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44553\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44553\">No</a>\n"]}, {"number": 44552, "title": "Bilinear upsampling layer shift", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10 x64\r\n- TensorFlow installed from: pypy\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n\r\n**Describe the current behavior**\r\nInference with Opencv Dnn has Mask shift that apperas while tensorflow model inference, if model has Resize Bilinear Layer. There's no shift if ResizeBilinear layer replaced with ResizeNearest layer. Also tensorflow inference engine does not have this issue and works well with Bilinear and Nearest resize. \r\nI agree, that this looks like Opencv issue, but i found this bug (https://github.com/tensorflow/tensorflow/issues/29856) that took place in TF 2.0 beta and it looks similar to this issue. Now tensorflow inference works fine, but, probably, only inference was fixed, but layer still works unproperly.\r\n\r\n\r\nHere you can see results of inference:\r\n<details>\r\n <summary>Inference results</summary>\r\n\r\nOpenCV DNN Bilinear\r\n![test_res_bilin](https://user-images.githubusercontent.com/48096792/97987008-a3597100-1deb-11eb-9e38-050c5a83e6bb.png)\r\n\r\n![test_res_bilin_line](https://user-images.githubusercontent.com/48096792/97987117-d0a61f00-1deb-11eb-88fd-5e4094a4abb0.png)\r\n\r\nOpenCV DNN Nearest\r\n![test_res_nearest](https://user-images.githubusercontent.com/48096792/97987148-dac81d80-1deb-11eb-9975-920a494c94c2.png)\r\n\r\nTensorflow Bilinear\r\n![tf_res_bilin](https://user-images.githubusercontent.com/48096792/97987168-e3b8ef00-1deb-11eb-9a31-b59582d42508.png)\r\n\r\nTensorflow Nearest\r\n![tf_res_nearest](https://user-images.githubusercontent.com/48096792/97987186-eadffd00-1deb-11eb-8c3b-f5d364223174.png)\r\n</details>\r\n\r\n##### Steps to reproduce\r\n[reproduce.zip](https://github.com/tensorflow/tensorflow/files/5495370/reproduce.zip)\r\n\r\n", "comments": ["@ZakharovDenis \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/5579f91b0374d78431b4f8c4f8ecdc1a/untitled458.ipynb).", "Hello @Saduf2019 . You have to upload model to colab storage to run the code. Download link provided in `download_models.txt` file. I can't put models to the zip file due to github file size limit.\r\nI updated a `reproduce.zip` to run opencv segmentation from python, not from c++.", "@ZakharovDenis \r\nApologies for the delayed response, i ran the code and [face this error](https://colab.research.google.com/gist/Saduf2019/2073450670137954d8cfa09be761f16f/untitled462.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44552\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44552\">No</a>\n"]}, {"number": 44550, "title": "fatal error: tensorflow/core/framework/types.pb.h: No such file or directory", "body": "\r\nwhen i compiling file with CMakeLists.txt file it giving error.\r\n[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/5481075/CMakeLists.txt)\r\ncompiling file : tensorflow/tensorflow/examples/speech_commands/label_wav.cc\r\ntensor flow version is r2.2\r\nmy machine is Ubuntu 18.04.5 LTS\r\n\r\n\r\nerror:\r\n[ 50%] Building CXX object CMakeFiles/speech_recog.dir/label_wav.cc.o\r\nIn file included from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor.h:23:0,\r\n                 from /home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/examples/speech_commands/label_wav.cc:19:\r\n/home/sbojja/Qual-sdm/TF/tensorflow/tensorflow/core/framework/tensor_shape.h:22:10: fatal error: tensorflow/core/framework/types.pb.h: No such file or directory\r\n #include \"tensorflow/core/framework/types.pb.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nCMakeFiles/speech_recog.dir/build.make:62: recipe for target 'CMakeFiles/speech_recog.dir/label_wav.cc.o' failed\r\nmake[2]: *** [CMakeFiles/speech_recog.dir/label_wav.cc.o] Error 1\r\nCMakeFiles/Makefile2:67: recipe for target 'CMakeFiles/speech_recog.dir/all' failed\r\nmake[1]: *** [CMakeFiles/speech_recog.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed\r\nmake: *** [all] Error 2", "comments": ["@sairmreddy \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "steps:\r\n-> sudo apt install python3-dev python3-pip\r\n-> pip install -U --user pip numpy wheel\r\n-> pip install -U --user keras_preprocessing --no-deps\r\n-> git clone https://github.com/tensorflow/tensorflow.git\r\n-> cd tensorflow\r\n-> git checkout r2.2\r\n-> ./configure\r\n\r\nYou have bazel 2.0.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]:  \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=ngraph      \t# Build with Intel nGraph support.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n-> bazel build -c opt tensorflow:libtensorflow_cc.so\r\n-> after this i am run the cmake file.\r\n->cmake .. and make", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44550\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44550\">No</a>\n"]}, {"number": 44549, "title": "Add use_default_shell_env = True to all ctx.actions.run_shell rules", "body": "To start subprograms, even simple bash snippets, Bazel uses an\r\nexecutable `process-wrapper` which is potentially built using a custom\r\ntoolchain and hence requires a set up LD_LIBRARY_PATH.\r\nOmmitting the `use_default_shell_env` (defaulting to false) clears the\r\nwhole environment and the binary may try to use older system libs such\r\nas /lib64/libstdc++.so causing it to fail in case it is (much) older\r\nthan the used libstdc++ from the custom toolchain which is very common\r\nin HPC environments.\r\nHence I added `use_default_shell_env = True` as already done in e.g.\r\n`_local_genrule_impl`.", "comments": ["For some reason I was also seeing the environment dropped from a `flatc --python` invocation which happens through `flatbuffer_py_library`. However prior to 2.4 that was working. Checking the compile log for 2.3.1 I don't see any invocation of `flatc --python` so it seems it was simply not used until 2.4 and hence not compiled which explains why the faulty Bazel rule didn't cause problems before", "@buchgr, would you mind to take a look at this change? I fear it might not play nice with RBE.", "> @buchgr, would you mind to take a look at this change? I fear it might not play nice with RBE.\r\n\r\n@buchgr confirmed that this prevent RBE caching between different users, and would cause problems when the host's `LD_LIBRARY_PATH` doesn't fit the remote environment.\r\n\r\nIs there any chance you could ldconfig to work around this in your environment?", "@Flamefire Can you please check @chsigg's comments and keep us posted ? Thanks!", "@gbaned If you are referring to \r\n\r\n> Is there any chance you could ldconfig to work around this in your environment?\r\n\r\nNo, this is not possible. Usual HPC environments have multiple versions of a software installed and decide via environment variables which is active (in particular LD_LIBRARY_PATH and PATH for binaries, CPATH & LIBRARY_PATH for dependencies)\r\n\r\nI'm unsure why this is now a problem to add this as e.g. `_local_genrule_impl` already uses this.\r\n\r\nUnfortunately I don't understand enough about Bazel and especially RBE but for us sysadmins it is a real pain that the environment gets cleared on multiple levels", "@chsigg Can you please take a look on above comments from @Flamefire. Thanks!\r\n", "Another idea, would [--action_env](https://docs.bazel.build/versions/master/command-line-reference.html#flag--action_env) or [--host_action_env](https://docs.bazel.build/versions/master/command-line-reference.html#flag--host_action_env) maybe do what you need?", "@chsigg No this does not work. This came up in a recent Bazel issue (https://github.com/bazelbuild/bazel/issues/12579#issuecomment-746336369) where it was observed that action_env and such are seemingly not passed when `use_default_shell_env=True` is not set.\r\nI haven't tried `--host_action_env` yet (it is very new) but I assume `--action_env` already sets those, especially as we use `--distinct_host_configuration=false` already. It also seems like there is an as-of-yet unreleased bugfix to the host_action_env flag: https://github.com/bazelbuild/bazel/commit/e6670825b1e183f81f5c864aafd425d512fa9ff5", "@Flamefire  Can you please resolve conflicts? Thanks!", "To be honest, I don't know how we can resolve this. @buchgr, do you have any suggestions?\r\n\r\nThere is a need to properly pass environment variables to build actions, but bazel sometimes drops them (due to bugs, IIUC). The change here would work around those bugs, but it doesn't play well with RBE.", "> There is a need to properly pass environment variables to build actions, but bazel sometimes drops them (due to bugs, IIUC). The change here would work around those bugs, but it doesn't play well with RBE.\r\n\r\nIIUC the `use_default_shell_env=True` makes Bazel pass the `--action_env` variables and `PATH`/`LD_LIBRARY_PATH` while without (`=False`) it will pass a cleaned up environment with nothing in it.\r\nSo this \"sometimes\" is the case for `use_default_shell_env=False`, which seems to me like the design, not a bug.\r\nHaving said that: There certainly are bugs, some of which I reported and some seem to be \"unfixable\" (Bazel contains a binary which requires LD_LIBRARY_PATH itself, \"fixed\" by building Bazel statically)\r\n\r\nIn the case of `flatc` (one of the fixed things here) a binary is called which has dependencies, so LD_LIBRARY_PATH must be passed. And as Bazel doesn't seem to have anything else but `use_default_shell_env=True` to do so, it should be used.\r\nIt could be checked what effect `--distinct_host_configuration=true` (default) has here in respect to RBE, maybe it already means that the hosts LD_LIBRARY_PATH isn't passed to the RBE even with `use_default_shell_env=True`? I'd expect so but am not using RBE.\r\n\r\nAnyway, 3 strong arguments in favor of this change:\r\n- It is only the flatbuffers compilation i.e. a code generation which is likely fast, so even a potential cache miss isn't expensive\r\n- Short of patching the source there is no other way (AFAIK) by now to run a flatc build with a different (LD_)LIBRARY_PATH (which most likely happens when it is modified at all)\r\n- The same is already used for `protoc`, where it is seemingly ok. So why the discussion here?", "@Flamefire  Can you please resolve conflicts? Thanks!", "@Flamefire  Can you please resolve conflicts? Thanks!", "@gbaned Rebased. Again.", "Failure not caused by this PR:\r\n> tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/chlo_ops.cc:410:41: error: call of overloaded 'BroadcastSelectOpAdaptor(mlir::ValueShapeRange&)' is ambiguous\r\n", "Not sure why I was added as a reviewer here. I don't work on TensorFlow and while I have an unfortunate amount of experience with the vagaries of Bazel, I don't know about anything about `default_shell_env`. I'm certainly not in a position to decide the tradeoff with RBE caching for this. One idea as a potential compromise since I'm here. I wonder if it's possible to detect whether running under RBE (or some other remote configuration) and set `default_shell_env` only if not.", "@Flamefire  Can you please resolve conflicts? Thanks!", "@gbaned Rebased", "Had to close+reopen due to Github not detecting the change..."]}, {"number": 44548, "title": "RuntimeError: tensorflow/lite/kernels/conv.cc:238 input->dims->data[3] != filter->dims->data[3] (32 != 1)Node number 1 (CONV_2D) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\nUbuntu 16.04\r\nTensorFlow version 2.3.1\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Copy and paste the output here.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nmodel\r\nhttps://drive.google.com/file/d/1KKDE7RwQ8qVXm4AdFnoYCadcQuI1x4O2/view?usp=sharing\r\n\r\ntflite\r\nhttps://drive.google.com/file/d/1mB557Fg5eh86Hhr8GSxWzVe7rufuNq87/view?usp=sharing\r\n\r\nh5 file\r\nhttps://drive.google.com/file/d/1xvMLkTO6jlsBIHEj8LSMOIHI51FuNyqI/view?usp=sharing\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n2020-11-03 20:43:17.312502: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\r\n2020-11-03 20:43:17.312653: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"C:/Users/daniel/PycharmProjects/MobileNeXt-3DMPPE-Extra-Experiment/tflite/check_tflite.py\", line 5, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"C:\\Users\\daniel\\Anaconda3\\envs\\3DMPPE_POSENET_RELEASE-master\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py\", line 244, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\n  File \"C:\\Users\\daniel\\Anaconda3\\envs\\3DMPPE_POSENET_RELEASE-master\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py\", line 106, in AllocateTensors\r\n    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)\r\nRuntimeError: tensorflow/lite/kernels/conv.cc:238 input->dims->data[3] != filter->dims->data[3] (32 != 1)Node number 1 (CONV_2D) failed to prepare.\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It happens when I try to convert Pytorch model -> Keras model -> TFlite model\r\n\r\nOf course this happens in android too", "I'm not sure it might help but converting into onnx and tensorflow succeed", "@SangbumChoi,\r\nI am unable to untar the `exp2_mobilenext_headnet_24.pth.tar` file you have shared. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a30db81e07a49b17d4c0aa6348fceae7/44548.ipynb). \r\n\r\nCould you please share the contents of the file in another format. Also, please share the complete code you have executed to build the model itself. Thanks!", "> @SangbumChoi,\r\n> I am unable to untar the `exp2_mobilenext_headnet_24.pth.tar` file you have shared. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/a30db81e07a49b17d4c0aa6348fceae7/44548.ipynb).\r\n> \r\n> Could you please share the contents of the file in another format. Also, please share the complete code you have executed to build the model itself. Thanks!\r\n\r\nanother pth.tar\r\nhttps://drive.google.com/file/d/1Dl5zC29g03UWc_Pb5alO1SgVgz25fUf7/view?usp=sharing\r\n\r\nbasline\r\nhttps://drive.google.com/drive/folders/1HP-_4ggQDjmoRnIYj4msonVWRWwsJJGE?usp=sharing\r\n\r\ntwo models are slightly different but i think it might regenerate the error\r\n\r\nI'm not sure if this might can extract properly.", "Also this model is generated with torch.nn.parallel.DataParallel so i load with \r\nthis kind of step\r\n\r\nargs = parse_args()\r\n\r\nprint(\"load keras model for MobileNeXt\")\r\nkeras_model = ResPoseNet_Tensorflow((256, 256, 3), args.joint)\r\n\r\n#Lucky for us, PyTorch includes a predefined Squeezenet\r\nprint(\"load pytorch model for MobileNeXt\")\r\npytorch_model = get_pose_net(args.backbone, args.frontbone, False, args.joint)\r\npytorch_model = DataParallel(pytorch_model).cuda()\r\n\r\n#Load the pretrained model\r\npytorch_model.load_state_dict(torch.load(args.modelpath)['network'])\r\n\r\n#Time to transfer weights\r\nsingle_pytorch_model = pytorch_model.module\r\nconverter = PytorchToKeras(pytorch_model, keras_model)\r\nconverter.convert((3,256,256))\r\n\r\n#Save the weights of the converted keras model for later use\r\nconverter.save_weights(\"../output/baseline.h5\")\r\nconverter.save_model(\"../output/baseline\")\r\n", "However by using onnx @amahendrakar it succeed but still have problem with \r\n\r\njava.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference\r\n\r\nNode number 3 (FlexAddV2) failed to prepare.\r\n\r\nFlexAddV2", "> java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference\r\n\r\n@SangbumChoi,\r\nLooking at issue [#41558](https://github.com/tensorflow/tensorflow/issues/41558) with a similar error, seems like the issue was resolved with the latest TF-nightly. \r\n\r\nCould you please check if you are facing the same issue with the latest TF-nightly as well? Thanks!", "> > java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference\r\n> \r\n> @SangbumChoi,\r\n> Looking at issue [#41558](https://github.com/tensorflow/tensorflow/issues/41558) with a similar error, seems like the issue was resolved with the latest TF-nightly.\r\n> \r\n> Could you please check if you are facing the same issue with the latest TF-nightly as well? Thanks!\r\n\r\nI will try it but can you explain how to enable using TF-nightly in Android Studio? (because it is built for mobile devices)", "> > java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference\r\n> \r\n> @SangbumChoi,\r\n> Looking at issue [#41558](https://github.com/tensorflow/tensorflow/issues/41558) with a similar error, seems like the issue was resolved with the latest TF-nightly.\r\n> \r\n> Could you please check if you are facing the same issue with the latest TF-nightly as well? Thanks!\r\n\r\nHi, when I use tf.version with 2.5.0-dev20201116\r\nit successfully build with\r\n\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2020-11-17 22:53:40.493786: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-17 22:53:40.496937: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\r\n2020-11-17 22:53:40.497171: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-11-17 22:53:40.505254: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: DESKTOP-4DSFS8C\r\n2020-11-17 22:53:40.505730: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: DESKTOP-4DSFS8C\r\n2020-11-17 22:53:40.505847: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 17443 nodes with 1 partitions.\r\n\r\ninput_details :  [{'name': 'input', 'index': 0, 'shape': array([  1,   3, 256, 256]), 'shape_signature': array([  1,   3, 256, 256]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\noutput_details :  [{'name': 'Identity', 'index': 51481, 'shape': array([ 1, 18,  3]), 'shape_signature': array([ 1, 18,  3]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n(1, 18, 3)\r\n\r\nHowever, as I mentioned how to use tf-nightly in android studio?", "@amahendrakar just adding     \r\nimplementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\nsuccessfully build I will close this! Thanks\r\n"]}, {"number": 44547, "title": "No Keras callback for sumary database writer", "body": "**System information**\r\n- TensorFlow version (you are using): 2.3.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently there is no `tf.keras.callbacks.Callback` writing tensorboard summary data into a sql database, but only which writes it into a log folder.\r\n\r\n**Will this change the current api? How?**\r\n\r\nEither the option  to specify a database URI for `tensorflow.python.ops.summary_ops_v2.create_db_writer()` instead of a logdir should be added to the `tf.keras.callbacks.TensorBoard` class or a new subclass of `tf.keras.callbacks.Callback`, which saves the summary in a sql database, should be created.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone how prefers saving the summary data in databases over all kinds of folders and files. (Me and @Informa-Tiger)", "comments": ["@rpkak Keras development moved to another repository to focus on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)\r\nTo know more see;\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44546, "title": "Install tensorflow in python 3", "body": "Hello!\r\nI have easy question, how i can install tensorflow?\r\n\r\nRequired Info | \u00a0\r\n-- | --\r\nOperating System | Ubuntu 18.04\r\nLanguage | Python 3.6\r\nPlatform | NVIDIA Jetson Nano\r\nCPU | Quad-core ARM A57\r\nGPU | 128-core Maxwell\r\n\r\n### Issue Description\r\nI performed a million actions in the console and now I'm at this stage:\r\npip3 install tensorflow => no error\r\nimport tensorflow => \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "@RarDay \r\nPlease [verify](https://github.com/tensorflow/tensorflow/issues/1711#issuecomment-207794680) if you have installed the correct version,please let us know the tf version used.\r\nJust to verify did you follow the instructions in [tensorflow website](https://www.tensorflow.org/install/source?hl=en) . Please, see tested build configurations from [here](https://www.tensorflow.org/install/source?hl=en#tested_build_configurations).\r\nPlease refer to: #38552 [link](https://github.com/tensorflow/tensorflow/issues/25315), \r\n", "I have:\r\n```\r\ntensorboard==1.12.2\r\ntensorflow @ https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.3.0-py3-none-any.whl\r\ntensorflow-estimator==1.13.0\r\ntensorflow-gpu==1.13.1+nv19.3\r\ntensorflow-tensorboard==0.1.8\r\ntensorrt==7.1.3.0\r\n```\r\n", "1.13 is too old. I'd suggest starting a new virtual environment and installing at least 1.15.", "@RarDay\r\nCould you please install 1.15 and update us.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44546\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44546\">No</a>\n"]}, {"number": 44545, "title": "Keras: custom data validation callback on training data always returns validation data results", "body": "Just for information: I already asked for help regarding this problem on [stackoverflow](https://stackoverflow.com/questions/64645579/keras-custom-data-validation-callback-on-training-data-always-returns-validatio). Since I did not get an answer yet, I assume that this is a non-trivial problem and therefore open an issue here.\r\n\r\n**System information**\r\n`python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"` returned\r\nv2.3.0-54-gfcc4b966f1 2.3.1\r\n\r\n**Describe the current behavior**\r\nWhen calling model.evaluate on training data (or any other data set) within a callback, always the results of the validation data set are returned. \r\n\r\n**Describe the expected behavior**\r\nI expected to get results for the data set passed to model.evaluate - no matter if called from within a callback or not.\r\n\r\n**Standalone code to reproduce the issue**\r\nI created a notebook that shows the problem:\r\nhttps://colab.research.google.com/drive/1H-3ULqyRZCpaasXpU1foLkEg12fYNkYK?usp=sharing", "comments": ["I am able to replicate the issue reported,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/34c1ed837a6ba2a1e704799f4fe146a9/untitled458.ipynb).", "@oXwvdrbbj8S4wo9k8lSN May be I am missing something. I checked your colab. Data looks different and the results (losses) are also different as expected. Can you please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/cdb65957379a264c3121b2543595a5da/untitled458.ipynb) where I printed different datasets and results. Thanks!\r\n\r\n", "@jvishnuvardhan I do not totally get the concept of gists. Therefore, please excuse if I state the obvious. \r\nBetween the colab notebook and the gist, something in the behavior of Keras/Tensorflow changed. In my notebook (with tensorflow version 2.3.0), the output of the history is \r\n```\r\n{'custom_loss': [0.7624925374984741, 0.5331208109855652],\r\n 'loss': [0.9665887951850891, 0.6637843251228333],\r\n 'val_loss': [0.7624925374984741, 0.5331208109855652]}\r\n```\r\nHere, the results for *custom_loss* and *val_loss* are the same although they should differ since the inputs differ. In the gist (currently tensorflow version 2.5.0-dev20201104), however, the output is \r\n```\r\n{'custom_loss': [0.7694963216781616, 0.541864812374115],\r\n 'loss': [0.9665887951850891, 0.6637843251228333],\r\n 'val_loss': [0.7624925374984741, 0.5331208109855652]}\r\n```\r\nHere, the outputs differ, which is the behavior I expected. I tested different versions of tensorflow and it seems that the change happened between version 2.3.1 and version 2.4.0rc0. Therefore, I guess that it is actually a bug but you already fixed it at least in the development versions. ", "@jvishnuvardhan I tested version 2.2.1 and also with this version, the outputs differ (as expected). It seems that the problem exists only in versions 2.3.0 and 2.3.1. ", "@oXwvdrbbj8S4wo9k8lSN May be there was a bug in `TF2.3` but I just checked with most recent `tf-nightly` and `!pip install tensorflow==2.4.0rc0` and both showed different results for `custom_loss` and `val_loss`.\r\n\r\nPlease use recent TF versions. In near future there will be stable `TF2.4` which works better than `TF2.3`.\r\n\r\nI am closing this issue as this was resolved in recent versions. If you see the same issue with recent version, feel free to reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44545\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44545\">No</a>\n"]}, {"number": 44544, "title": "Cannot export Keras sub-classed model with 2 args as a SavedModel", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.3.1 (also tested on 2.2)\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nAttempting to save the model produces the error:\r\n`ValueError: Structure of Python function inputs does not match input_signature:`\r\n\r\nIf I remove one of the args, everything works fine. I looked at https://github.com/tensorflow/tensorflow/issues/32488 and https://github.com/tensorflow/tensorflow/issues/28165, and those also fail for me.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should be able to save correctly.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import (\r\n    float32,\r\n    function,\r\n    TensorSpec,\r\n)\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Dense\r\n\r\nclass TestModel(Model):\r\n    def __init__(\r\n        self\r\n    ):\r\n        super(TestModel, self).__init__()\r\n\r\n        self.dense = Dense(100)\r\n\r\n    @function(\r\n        input_signature=[\r\n            TensorSpec([None, 512], float32, name=\"responses\"),\r\n            TensorSpec([None, 512], float32, name=\"contexts\"),\r\n        ]\r\n    )\r\n    def call(\r\n        self,\r\n        responses,\r\n        contexts\r\n    ):\r\n        return self.dense(responses + contexts)\r\n\r\nmodel = TestModel()\r\nx = tf.random.normal((1, 512))\r\n_ = model(x, x)\r\ntf.saved_model.save(model, \"directory\")\r\n```\r\n\r\n**Other info / logs**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature)\r\n   2687         expand_composites=True,\r\n-> 2688         check_types=False)  # lists are convert to tuples for `tf.data`.\r\n   2689   except ValueError:\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/util/nest.py in flatten_up_to(shallow_tree, input_tree, check_types, expand_composites)\r\n    951                            check_types=check_types,\r\n--> 952                            expand_composites=expand_composites)\r\n    953   # Discard paths returned by _yield_flat_up_to.\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/util/nest.py in assert_shallow_structure(shallow_tree, input_tree, check_types, expand_composites)\r\n    853             _STRUCTURES_HAVE_MISMATCHING_LENGTHS.format(\r\n--> 854                 input_length=len(input_tree), shallow_length=len(shallow_tree)))\r\n    855       elif len(input_tree) < len(shallow_tree):\r\n\r\nValueError: The two structures don't have the same sequence length. Input structure has length 1, while shallow structure has length 2.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-d373a86dbc49> in <module>\r\n----> 1 tf.saved_model.save(model, \"directory\")\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in save(obj, export_dir, signatures, options)\r\n    974\r\n    975   _, exported_graph, object_saver, asset_info = _build_meta_graph(\r\n--> 976       obj, export_dir, signatures, options, meta_graph_def)\r\n    977   saved_model.saved_model_schema_version = constants.SAVED_MODEL_SCHEMA_VERSION\r\n    978\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in _build_meta_graph(obj, export_dir, signatures, options, meta_graph_def)\r\n   1045   if signatures is None:\r\n   1046     signatures = signature_serialization.find_function_to_export(\r\n-> 1047         checkpoint_graph_view)\r\n   1048\r\n   1049   signatures, wrapped_functions = (\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py in find_function_to_export(saveable_view)\r\n     73   # If the user did not specify signatures, check the root object for a function\r\n     74   # that can be made into a signature.\r\n---> 75   functions = saveable_view.list_functions(saveable_view.root)\r\n     76   signature = functions.get(DEFAULT_SIGNATURE_ATTR, None)\r\n     77   if signature is not None:\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py in list_functions(self, obj, extra_functions)\r\n    143     if obj_functions is None:\r\n    144       obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n--> 145           self._serialization_cache)\r\n    146       self._functions[obj] = obj_functions\r\n    147     if extra_functions:\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _list_functions_for_serialization(self, serialization_cache)\r\n   2588     self.predict_function = None\r\n   2589     functions = super(\r\n-> 2590         Model, self)._list_functions_for_serialization(serialization_cache)\r\n   2591     self.train_function = train_function\r\n   2592     self.test_function = test_function\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _list_functions_for_serialization(self, serialization_cache)\r\n   3017   def _list_functions_for_serialization(self, serialization_cache):\r\n   3018     return (self._trackable_saved_model_saver\r\n-> 3019             .list_functions_for_serialization(serialization_cache))\r\n   3020\r\n   3021   def __getstate__(self):\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py in list_functions_for_serialization(self, serialization_cache)\r\n     85         `ConcreteFunction`.\r\n     86     \"\"\"\r\n---> 87     fns = self.functions_to_serialize(serialization_cache)\r\n     88\r\n     89     # The parent AutoTrackable class saves all user-defined tf.functions, and\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in functions_to_serialize(self, serialization_cache)\r\n     77   def functions_to_serialize(self, serialization_cache):\r\n     78     return (self._get_serialized_attributes(\r\n---> 79         serialization_cache).functions_to_serialize)\r\n     80\r\n     81   def _get_serialized_attributes(self, serialization_cache):\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py in _get_serialized_attributes(self, serialization_cache)\r\n     93\r\n     94     object_dict, function_dict = self._get_serialized_attributes_internal(\r\n---> 95         serialization_cache)\r\n     96\r\n     97     serialized_attr.set_and_validate_objects(object_dict)\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py in _get_serialized_attributes_internal(self, serialization_cache)\r\n     49     # cache (i.e. this is the root level object).\r\n     50     if len(serialization_cache[constants.KERAS_CACHE_KEY]) == 1:\r\n---> 51       default_signature = save_impl.default_save_signature(self.obj)\r\n     52\r\n     53     # Other than the default signature function, all other attributes match with\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py in default_save_signature(layer)\r\n    203   original_losses = _reset_layer_losses(layer)\r\n    204   fn = saving_utils.trace_model_call(layer)\r\n--> 205   fn.get_concrete_function()\r\n    206   _restore_layer_losses(original_losses)\r\n    207   return fn\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in get_concrete_function(self, *args, **kwargs)\r\n   1165       ValueError: if this object has not yet been called on concrete values.\r\n   1166     \"\"\"\r\n-> 1167     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n   1168     concrete._garbage_collector.release()  # pylint: disable=protected-access\r\n   1169     return concrete\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n   1071       if self._stateful_fn is None:\r\n   1072         initializers = []\r\n-> 1073         self._initialize(args, kwargs, add_initializers_to=initializers)\r\n   1074         self._initialize_uninitialized_variables(initializers)\r\n   1075\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\r\n    695     self._concrete_stateful_fn = (\r\n    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n--> 697             *args, **kwds))\r\n    698\r\n    699     def invalid_creator_scope(*unused_args, **unused_kwds):\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\r\n   2853       args, kwargs = None, None\r\n   2854     with self._lock:\r\n-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n   2856     return graph_function\r\n   2857\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3211\r\n   3212       self._function_cache.missed.add(call_context_key)\r\n-> 3213       graph_function = self._create_graph_function(args, kwargs)\r\n   3214       self._function_cache.primary[cache_key] = graph_function\r\n   3215       return graph_function, args, kwargs\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   3073             arg_names=arg_names,\r\n   3074             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 3075             capture_by_value=self._capture_by_value),\r\n   3076         self._function_attributes,\r\n   3077         function_spec=self.function_spec,\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    984         _, original_func = tf_decorator.unwrap(python_func)\r\n    985\r\n--> 986       func_outputs = python_func(*func_args, **func_kwargs)\r\n    987\r\n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\r\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\r\n    599         # the function a weak reference to itself to avoid a reference cycle.\r\n--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n    602\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py in _wrapped_model(*args)\r\n    132     with base_layer_utils.call_context().enter(\r\n    133         model, inputs=inputs, build_graph=False, training=False, saving=True):\r\n--> 134       outputs = model(inputs, training=False)\r\n    135\r\n    136     # Outputs always has to be a flat dict.\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    983\r\n    984         with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n--> 985           outputs = call_fn(inputs, *args, **kwargs)\r\n    986\r\n    987         if self._activity_regularizer:\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    778       else:\r\n    779         compiler = \"nonXla\"\r\n--> 780         result = self._call(*args, **kwds)\r\n    781\r\n    782       new_tracing_count = self._get_tracing_count()\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    805       # In this case we have created variables on the first call, so we run the\r\n    806       # defunned version which is guaranteed to never create variables.\r\n--> 807       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n    808     elif self._stateful_fn is not None:\r\n    809       # Release the lock early so that multiple threads can perform the call\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2826     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   2827     with self._lock:\r\n-> 2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2830\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   3169     if self.input_signature is None or args is not None or kwargs is not None:\r\n   3170       args, kwargs = self._function_spec.canonicalize_function_inputs(\r\n-> 3171           *args, **kwargs)\r\n   3172\r\n   3173     cache_key = self._cache_key(args, kwargs)\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in canonicalize_function_inputs(self, *args, **kwargs)\r\n   2620           inputs,\r\n   2621           self._input_signature,\r\n-> 2622           self._flat_input_signature)\r\n   2623       return inputs, {}\r\n   2624\r\n\r\n~/.pyenv/versions/smart-replies/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _convert_inputs_to_signature(inputs, input_signature, flat_input_signature)\r\n   2690     raise ValueError(\"Structure of Python function inputs does not match \"\r\n   2691                      \"input_signature:\\n%s\" %\r\n-> 2692                      format_error_message(inputs, input_signature))\r\n   2693\r\n   2694   need_packing = False\r\n\r\nValueError: Structure of Python function inputs does not match input_signature:\r\n  inputs: (\r\n    [<tf.Tensor 'responses:0' shape=(None, 512) dtype=float32>, <tf.Tensor 'contexts:0' shape=(None, 512) dtype=float32>])\r\n  input_signature: (\r\n    TensorSpec(shape=(None, 512), dtype=tf.float32, name='responses'),\r\n    TensorSpec(shape=(None, 512), dtype=tf.float32, name='contexts'))\r\n```", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/de86338f39048ea941709c338e77ffca/44544.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/e367af691ecab7cae831345a60d2ea7c/44544-tf-nightly.ipynb). Please find the attached gist. Thanks!", "The same problem[#44427](https://github.com/tensorflow/tensorflow/issues/44427)\uff08The problem hasn't been solved\uff0c But I\r\n close it.\uff09\r\n[#44512](https://github.com/tensorflow/tensorflow/issues/44512)", "I have the same problem.\r\n@ymodak Is there any idea to solve this problem\uff1f", "Here's what I found to solve the problem.\r\n\r\nFirst, you need to condense all your inputs for the call function into one tuple or array which you can then unpack.\r\n\r\n`@tf.function(input_signature = [(tf.TensorSpec(shape=[None,1], dtype=tf.int32, name='x'),\r\n                                  tf.TensorSpec(shape=[None,64,512], dtype=tf.float32, name='features'),\r\n                                  tf.TensorSpec(shape=[None,1024], dtype=tf.float32, name='hidden'))])\r\n  \r\ndef call(self, x):\r\n    \r\n    (x, features, hidden) = x\r\n    \r\n    ...`\r\n\r\nMake sure your TensorSpecs are nested like so when defining the input_signature. (Keep in mind you will have to bundle your variables together before feeding them to the model now)\r\n\r\nNow you need to use TFLiteConverter.from_keras_model, using anything else will fail still for some reason:\r\n\r\n`converter = tf.lite.TFLiteConverter.from_keras_model(decoder)\r\n\r\ntflite_model = converter.convert()\r\n\r\n\r\nwith open('./drive/MyDrive/decoder.tflite', 'wb') as f:\r\n  \r\n    f.write(tflite_model)`\r\n\r\nI'm having a lot of trouble formatting my code on github for some reason, sorry about that", "@setu4993 Generally, `call` methods takes only two arguments (`self`, `inputs`). So, I updated your `call` method as shown below. With that modification, I can run your code without any error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/df992c2d61fdcc5800ee792c6f70ebff/44544-tf-nightly.ipynb)\r\n\r\n```\r\ndef call(self,inputs):\r\n      responses,contexts = inputs\r\n      return self.dense(responses + contexts)\r\n```\r\n\r\nPlease verify once and let me know what you notice. Thanks!\r\n", "@jvishnuvardhan : Interesting solution there, but I can confirm that it works. Thanks for the help! Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44544\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44544\">No</a>\n"]}, {"number": 44543, "title": "ERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. ERROR: Node number 484 (FlexFusedBatchNormV3) failed to prepare.", "body": "I use the version tf-nightly==2.5.0-dev20201029 and tested the tflite model\r\n\r\nI converted the model into tflite and the python code below can run successfully. \r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\r\nfor samples in data_queue:\r\n            input_details = interpreter.get_input_details()\r\n            output_details = interpreter.get_output_details()\r\n            interpreter.resize_tensor_input(input_details[0]['index'], samples[\"input\"].shape)\r\n            interpreter.allocate_tensors()\r\n            interpreter.set_tensor(input_details[0]['index'], samples[\"input\"])\r\n            interpreter.invoke()\r\n            features = interpreter.get_tensor(output_details[0]['index'])\r\n            self.vocoder(features.numpy()) \r\n```\r\n\r\nHowever, when I use C++ code to test the tflite model, the erros below showed:\r\n\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\nERROR: Node number 484 (FlexFusedBatchNormV3) failed to prepare.", "comments": ["@cookingbear \r\nPlease refer ot this issie with same error and let us know: #40157 , [link](https://github.com/tensorflow/tensorflow/issues/40677#issuecomment-653680600).\r\n\r\nElse please share a simple standalone to reproduce the error? Thanks!", "> @cookingbear\r\n> Please refer ot this issie with same error and let us know: #40157 , [link](https://github.com/tensorflow/tensorflow/issues/40677#issuecomment-653680600).\r\n> \r\n> Else please share a simple standalone to reproduce the error? Thanks!\r\n\r\nthe code can be seen in https://github.com/tensorflow/tensorflow/issues/44520\r\nand I do not think the problem is same as that from the link you shared.\r\ncan you please look into this?", "@cookingbear \r\nThe code shared in #$4520 does not have the error reported, i ran the code and see aonly a warning, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/88d4f6ce278ff55dc0bb404a4eb7f331/untitled458.ipynb).\r\nCan you please share a colab gist of the error reported,", "> @cookingbear\r\n> The code shared in #$4520 does not have the error reported, i ran the code and see aonly a warning, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/88d4f6ce278ff55dc0bb404a4eb7f331/untitled458.ipynb).\r\n> Can you please share a colab gist of the error reported,\r\n\r\n\r\nI just found that if I delete the line below, the similar errors metioned above would show during the tflite model converting process. the other codes remained the same.\r\n\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n                                                                     tf.lite.OpsSet.SELECT_TF_OPS]\r\n```\r\n\r\nerrors:\r\n\r\nerror: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\n\r\n<unknown>:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n        tf.FusedBatchNormV3 {data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = true}\r\n\r\n\r\nThe errors above are also related with FusedBatchNormV3 which is the same as that showed by inference using c++", "> > @cookingbear\r\n> > The code shared in #$4520 does not have the error reported, i ran the code and see aonly a warning, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/88d4f6ce278ff55dc0bb404a4eb7f331/untitled458.ipynb).\r\n> > Can you please share a colab gist of the error reported,\r\n> \r\n> I just found that if I delete the line below, the similar errors metioned above would show during the tflite model converting process. the other codes remained the same.\r\n> \r\n> ```\r\n> converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\r\n>                                                                      tf.lite.OpsSet.SELECT_TF_OPS]\r\n> ```\r\n> \r\n> errors:\r\n> \r\n> error: 'tf.FusedBatchNormV3' op is neither a custom op nor a flex op\r\n> \r\n> :0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n> tf.FusedBatchNormV3 {data_format = \"NHWC\", device = \"\", epsilon = 1.000000e-03 : f32, exponential_avg_factor = 1.000000e+00 : f32, is_training = true}\r\n> \r\n> The errors above are also related with FusedBatchNormV3 which is the same as that showed by inference using c++\r\n\r\nI mean that if you delete that line mentioned above in the colab, the errors would show. If this problem can be solved, maybe the original problem can be solved too.", "I just found that the error is related with LayerNormalization. If I replace it with BatchNormalization, it can run successfully. But in my code, LayerNormalization is necessary. Although I can convert the code to the tflite model, it still show the error during inference using c++", "@cookingbear Please check this [comment](https://github.com/tensorflow/tensorflow/issues/43934#issuecomment-714832778) which is similar to your issue. Also, check [this resource](https://www.tensorflow.org/lite/guide/ops_select) on how to enable `Select_TF_Ops`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44543\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44543\">No</a>\n"]}, {"number": 44542, "title": "systemlibs: unbundle typing_extensions", "body": "Signed-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["Good catch, the systemlibs file is also missing although the workspace.bzl already has a reference to it. Related MR: https://github.com/tensorflow/tensorflow/pull/44524"]}, {"number": 44541, "title": "unable to trace dense layer in GNN", "body": "**Describe the current behavior**\r\nHi,\r\nI'm trying to implement GNN using tensorflow and export it to TFlite.\r\nMy implementation is largely inspired from pytorch_geometric.\r\n\r\nAfter training my model I found tensorflow is not tracing my model's execution appropriately\r\nThe message was below.\r\n```\r\nWARNING:absl:Found untraced functions such as dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\r\nWARNING:absl:Found untraced functions such as dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\r\n```\r\n\r\nMy model consists of 2 graph convolutional layers and each layer computes dense layer and then some kind of scatter & gather operation.\r\n**Describe the expected behavior**\r\nI want tensorflow to trace my model's execution properly, so that I can painlessly export my model to tflite and execute it.\r\n\r\n**Standalone code to reproduce the issue**\r\nBelow gist is simplified version of my code.\r\nIt is a bit long but I couldn't simplify it more to reproduce my error. sorry\r\nhttps://colab.research.google.com/gist/junhyk/3744c6f54074a20b4972eada113aa76d/untitled1.ipynb\r\n", "comments": ["@junhyk,\r\nIn spite of the warnings, I was able to convert the model to tflite format without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8cde728ac94bc76eaa70ef802132a397/44541-tf-nightly.ipynb). Thanks!", "Same problem in tf2.4. tf2.3 is fine.", "@Liu-Da,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Same issue with tf24, I can't test on tf23 cause I'm on arm apple chips\r\n", "@arthurserres,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "I have a similar issue. I was trying to save a custom model using tensorflow nighty 2.4 because I have an rtx 3080 (can't use the stable version). Tried saving the custom model on another computer with tensorflow 2.2 and I did not get these warnings when I saved. I agree that the issue is likely with tensorflow 2.4.", "I have the same issue after upgrading tf 2.4 from tf 2.2.", "@hoondy, @zndr27,\r\nCould y'all please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "Have the same issue with tf2.4.\r\nInheriting from `Model` instead of `Layer` for custom layer subclassing did greatly reduce the warnings. Now I only have the warnings for LSTM, like in https://github.com/tensorflow/tensorflow/issues/47554"]}, {"number": 44540, "title": "Add GPU kernel for SparseApplyFtrl", "body": "Also applies to Resource and V2 versions of the op.\r\n\r\ncc @sanjoy @nluehr ", "comments": ["I confirmed that the GPU kernel is being called in the CI tests."]}, {"number": 44538, "title": "OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.(Using Tensorflow data pipeline map function with tfrecords)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.5\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: NO\r\n-   **TensorFlow installed from (source or binary)**: pip install tf-nightly \r\n-   **TensorFlow version (use command below)**:'2.4.0-dev20201019'\r\n-   **Python version**: 3.7.5\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: NA\r\n-   **GPU model and memory**:NA\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI converted ms-coco format data into tfrecords, loaded the same using   \r\n\r\n autotune = tf.data.experimental.AUTOTUNE\r\ndata = tf.data.TFRecordDataset(path)\r\ndata = data.map(_decode_record,num_parallel_calls=autotune)\r\ndata = data.map(preprocess_data,num_parallel_calls= autotune)\r\n## I've pasted the functions used below in the source code block\r\n\r\npreprocess function have some condition check, but when using map all are executed in eager mode so gives error :\r\n\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\ntried @tf.function, tf.py_function, functool.partial but no success, Before using tf nightly it was throwing error : Tensor' object has no attribute 'numpy'\r\n\r\n\r\n### Source code / logs\r\n###Creating tfrecords code:\r\nwith tf.io.TFRecordWriter(my_path) as writer:\r\n    def _bytes_feature(value):\r\n        if isinstance(value, type(tf.constant(0))):\r\n            value = value.numpy()\r\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n    def _convert_and_serialize(value):\r\n        value = tf.convert_to_tensor(value)\r\n        value = tf.io.serialize_tensor(value)\r\n        return value\r\n    image = np.asarray(PIL.Image.open(img_path))\r\n   feature = {}\r\n    image = _convert_and_serialize(image)\r\n   feature['image'] = _bytes_feature(image)tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n   writer.write(tf_example.SerializeToString())\r\n\r\n\r\n### Reading tfrecords \r\n\"\"\"\r\ndef _decode_record(record):\r\n    feature_description = {\r\n    'image': tf.io.FixedLenFeature([], tf.float64, default_value=0),\r\n    }\r\n    parsed = tf.io.parse_single_example(record,feature_description)\r\n    return parsed\r\n\"\"\"\r\n#preprocess function\r\n\"\"\"\r\ndef random_flip_horizontal(image, boxes):\r\n    \"\"\"Flips image and boxes horizontally with 50% chance\r\n\r\n    Arguments:\r\n      image: A 3-D tensor of shape `(height, width, channels)` representing an\r\n        image.\r\n      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\r\n        having normalized coordinates.\r\n\r\n    Returns:\r\n      Randomly flipped image and boxes\r\n    \"\"\"\r\n    if tf.random.uniform(()) > 0.5:\r\n        image = tf.image.flip_left_right(image)\r\n        boxes = tf.stack(\r\n            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1\r\n        )\r\n    return image, boxes\r\n\r\n\r\ndef resize_and_pad_image(\r\n    image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\r\n):\r\n    \"\"\"Resizes and pads image while preserving aspect ratio.\r\n\r\n    1. Resizes images so that the shorter side is equal to `min_side`\r\n    2. If the longer side is greater than `max_side`, then resize the image\r\n      with longer side equal to `max_side`\r\n    3. Pad with zeros on right and bottom to make the image shape divisible by\r\n    `stride`\r\n\r\n    Arguments:\r\n      image: A 3-D tensor of shape `(height, width, channels)` representing an\r\n        image.\r\n      min_side: The shorter side of the image is resized to this value, if\r\n        `jitter` is set to None.\r\n      max_side: If the longer side of the image exceeds this value after\r\n        resizing, the image is resized such that the longer side now equals to\r\n        this value.\r\n      jitter: A list of floats containing minimum and maximum size for scale\r\n        jittering. If available, the shorter side of the image will be\r\n        resized to a random value in this range.\r\n      stride: The stride of the smallest feature map in the feature pyramid.\r\n        Can be calculated using `image_size / feature_map_size`.\r\n\r\n    Returns:\r\n      image: Resized and padded image.\r\n      image_shape: Shape of the image before padding.\r\n      ratio: The scaling factor used to resize the image\r\n    \"\"\"\r\n    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\r\n    if jitter is not None:\r\n        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\r\n    ratio = min_side / tf.reduce_min(image_shape)\r\n    if ratio * tf.reduce_max(image_shape) > max_side:\r\n        ratio = max_side / tf.reduce_max(image_shape)\r\n    image_shape = ratio * image_shape\r\n    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\r\n    padded_image_shape = tf.cast(\r\n        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\r\n    )\r\n    image = tf.image.pad_to_bounding_box(\r\n        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\r\n    )\r\n    return image, image_shape, ratio\r\n\r\n\r\ndef preprocess_data(sample):\r\n    \"\"\"Applies preprocessing step to a single sample\r\n\r\n    Arguments:\r\n      sample: A dict representing a single training sample.\r\n\r\n    Returns:\r\n      image: Resized and padded image with random horizontal flipping applied.\r\n      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\r\n        of the format `[x, y, width, height]`.\r\n      class_id: An tensor representing the class id of the objects, having\r\n        shape `(num_objects,)`.\r\n    \"\"\"\r\n    image = sample[\"image\"]\r\n    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\r\n    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\r\n\r\n    image, bbox = random_flip_horizontal(image, bbox)\r\n    image, image_shape, _ = resize_and_pad_image(image)\r\n\r\n    bbox = tf.stack(\r\n        [\r\n            bbox[:, 0] * image_shape[1],\r\n            bbox[:, 1] * image_shape[0],\r\n            bbox[:, 2] * image_shape[1],\r\n            bbox[:, 3] * image_shape[0],\r\n        ],\r\n        axis=-1,\r\n    )\r\n    bbox = convert_to_xywh(bbox)\r\n    return image, bbox, class_id\r\n\"\"\"\r\n\r\n\r\n#error log \r\n\r\n<ipython-input-66-b4ebcba6c559> in preprocess_data(sample)\r\n     91 \r\n     92 #     image, bbox = random_flip_horizontal(image, bbox)\r\n---> 93     image, image_shape, _ = resize_and_pad_image(image)\r\n     94     image_shape = image.shape\r\n     95 \r\n\r\n<ipython-input-66-b4ebcba6c559> in resize_and_pad_image(image, min_side, max_side, jitter, stride)\r\n     58     print('ratio',ratio,type(ratio))\r\n     59     print(tf.reduce_max(image_shape),type(tf.reduce_max(image_shape)))\r\n---> 60     if ratio * tf.reduce_max(image_shape) > max_side:\r\n     61         ratio = max_side / tf.reduce_max(image_shape)\r\n     62     image_shape = ratio * image_shape\r\n\r\nenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __bool__(self)\r\n    883       `TypeError`.\r\n    884     \"\"\"\r\n--> 885     self._disallow_bool_casting()\r\n    886 \r\n    887   def __nonzero__(self):\r\n\r\nenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _disallow_bool_casting(self)\r\n    490     else:\r\n    491       # Default: V1-style Graph execution.\r\n--> 492       self._disallow_in_graph_mode(\"using a `tf.Tensor` as a Python `bool`\")\r\n    493 \r\n    494   def _disallow_iteration(self):\r\n\r\nenv/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _disallow_in_graph_mode(self, task)\r\n    479     raise errors.OperatorNotAllowedInGraphError(\r\n    480         \"{} is not allowed in Graph execution. Use Eager execution or decorate\"\r\n--> 481         \" this function with @tf.function.\".format(task))\r\n    482 \r\n    483   def _disallow_bool_casting(self):\r\n\r\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.\r\n\r\nIf there is better way to do the same then also let me know.\r\n\r\nBasically i wanted to replicate the same as given below example : https://keras.io/examples/vision/retinanet/\r\nBut with tf-nightly this also gives same error.\r\n\r\nThanks,", "comments": ["@bhupendrathore \r\ncode shared is not indeneted, please provide a colab gist with the error reported.also can you verify if the error exist in nightly.", "Hi @Saduf2019,\r\n\r\nTried to reproduce the same error but could not install tf-nightly same version (2.4.0-dev20201019),\r\n\r\ngist can be found at this URL : https://gist.github.com/bhupendrathore/c7c737e53a3917a87c27b443d3620ee5\r\n\r\ncolab also can be found at this URL : https://colab.research.google.com/drive/1-c4OYKe42pMqouVJAFcznhIfv8GKOVSH?usp=sharing\r\n\r\nError has now changed back to old error.", "@bhupendrathore \r\nI ran the code shared there are too many errors, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/a42035b1419048093449eac1fc8381c9/untitled458.ipynb)", "@Saduf2019 find the updated [colab notebook here](https://colab.research.google.com/drive/1-c4OYKe42pMqouVJAFcznhIfv8GKOVSH#scrollTo=Q8kksBJoQ5Bs)\r\n\r\nnow the error has changed with new tf-nightly which is \r\n'''\r\nValueError: in user code:\r\n\r\n    <ipython-input-52-b4ebcba6c559>:93 preprocess_data  *\r\n        image, image_shape, _ = resize_and_pad_image(image)\r\n    <ipython-input-58-b4ebcba6c559>:63 resize_and_pad_image  *\r\n        image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:1647 resize_images_v2\r\n        skip_resize_if_same=False)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py:1313 _resize_images_common\r\n        raise ValueError('\\'images\\' contains no shape.')\r\n\r\n    ValueError: 'images' contains no shape.\r\n'''\r\nIt's same as error with tf 2.3, to avoid i used tf-nightly so in the version (2.4.0-dev20201019) i was getting the tf.Tensor as a python bool is not allowed in graph execution but now with the latest tf-nightly its same as images contains no shape.\r\n\r\nSuggest if you want me to close this and reopen new, but I believe both are because of the execution in eager mode and image tensor not able to find shape attribute in eager mode.\r\ncorrect me if I'm wrong\r\nThanks \r\n", "I am able to replicate the issue faced, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/dee1694852cf01f77a51df342a1964d0/untitled458.ipynb)", "@bhupendrathore Can you please provide simple standalone code to reproduce the issue. The current code is too long to support. Thanks!", "Sorry, by mistake I closed. Reopened. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44538\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44538\">No</a>\n"]}, {"number": 44537, "title": "Update PendingReleaseVersion to 2.4.0", "body": "This version should be updated along with the release.\r\nPiperOrigin-RevId: 340388895\r\nChange-Id: I725b1e40522a3d40972615e3c6f73cd7d48299cf", "comments": []}]