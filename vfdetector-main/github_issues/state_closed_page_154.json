[{"number": 50210, "title": "Disable io_test  ", "body": "", "comments": []}, {"number": 50209, "title": "[INTEL MKL] Fix compiler test failures when oneDNN is enabled.", "body": "Fix for a number of compiler xla related failures, where the runtime doesn't find registered kernels for _Mkl* OpKernels for 'XLA_CPU'. The bug is caused by mkl layout pass rewriting an operator which was assigned to XLA_CPU.  The fix is to block the rewrite if device is an XLA_CPU.", "comments": []}, {"number": 50207, "title": "Added missing description", "body": "Added missing description for get, deserialize, serialize \r\nThis is my first contribution to open-source so pls review", "comments": ["Can you make this against master branch please? We no longer merge PRs into the `r2.0` branch and for newer branches only if we do patch releases and only as needed for the patch releases.", "> Can you make this against master branch please? We no longer merge PRs into the `r2.0` branch and for newer branches only if we do patch releases and only as needed for the patch releases.\n\nYeah sure "]}, {"number": 50206, "title": "[MLIR][TOSA] Rewrite tf.MatMul to TOSA legalization with 3D tosa.matmul.", "body": "Replace soon-to-be-removed 2D TOSA.MatMul target with 3D one.", "comments": ["Tagging @stellaraccident and @rsuderman for review request."]}, {"number": 50205, "title": "Tensorflow Training Crashing", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.3\r\n- GPU model and memory: Tesla K80 and 12GB\r\n\r\nI have created a GCP VM with Tesla K80 GPU attached to it. I have installed Nvidia 465 drivers for Ubuntu 20.04 along with Cuda 11.\r\n\r\nI am trying to use tensorflow on the GCP machine and each time when the training starts the machine crashes after few epochs. Here is the log\r\n\r\n```\r\n216/216 [==============================] - ETA: 0s - loss: 2.5774 - accuracy: 0.2203   \r\n216/216 [==============================] - 173s 800ms/step - loss: 2.5774 - accuracy: 0.2203 - val_loss: 47.4114 - val_accuracy: 0.1372 - lr: 0.0100\r\nEpoch 2/50\r\n216/216 [==============================] - ETA: 0s - loss: 1.9055 - accuracy: 0.3265  \r\n216/216 [==============================] - 137s 633ms/step - loss: 1.9055 - accuracy: 0.3265 - val_loss: 46.8945 - val_accuracy: 0.2023 - lr: 0.0100\r\nEpoch 3/50\r\n216/216 [==============================] - ETA: 0s - loss: 1.7601 - accuracy: 0.3899  \r\n216/216 [==============================] - 137s 633ms/step - loss: 1.7601 - accuracy: 0.3899 - val_loss: 1.9010 - val_accuracy: 0.3895 - lr: 0.0100\r\nEpoch 4/50\r\n216/216 [==============================] - ETA: 0s - loss: 1.5993 - accuracy: 0.4417  \r\n216/216 [==============================] - 137s 632ms/step - loss: 1.5993 - accuracy: 0.4417 - val_loss: 1.7880 - val_accuracy: 0.3919 - lr: 0.0100\r\nEpoch 5/50\r\n216/216 [==============================] - ETA: 0s - loss: 1.2965 - accuracy: 0.5580  \r\n216/216 [==============================] - 134s 618ms/step - loss: 1.2965 - accuracy: 0.5580 - val_loss: 1.9468 - val_accuracy: 0.3919 - lr: 0.0100\r\nEpoch 6/50\r\n 60/216 [=======>......................] - ETA: 1:20 - loss: 1.0874 - accuracy: 0.63542021-06-10 19:12:36.997237: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n2021-06-10 19:12:36.997296: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\nAborted (core dumped)\r\n\r\n```\r\n\r\nPlease advise if you have run into a similar sort of error before.\r\n\r\n", "comments": ["@nauyan  Please provide the simple standalone code to reproduce the issue at our end.Thanks", "@saikumarchalla \r\n```\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\nimport tensorflow\r\nimport matplotlib.pyplot as plt\r\n\r\n# config = tensorflow.compat.v1.ConfigProto()\r\n# config.gpu_options.allow_growth = True\r\n# session = tensorflow.compat.v1.Session(config=config)\r\n\r\n\r\nbatch_size = 8\r\nimage_dir = \"data/TrainingFrames/\"\r\nimg_height, img_width = 256, 256\r\ntrain_datagen = ImageDataGenerator(rescale=1./255,\r\n    #shear_range=0.2,\r\n    #zoom_range=0.2,\r\n    #horizontal_flip=True,\r\n    validation_split=0.2) # set validation split\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    image_dir,\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    subset='training') # set as training data\r\n\r\nvalidation_generator = train_datagen.flow_from_directory(\r\n    image_dir, # same directory as training data\r\n    target_size=(img_height, img_width),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    subset='validation') # set as validation data\r\n\r\n\r\n\r\nfrom classification_models.tfkeras import Classifiers\r\n\r\nResNet18, preprocess_input = Classifiers.get('seresnet50')\r\nmodel = ResNet18((img_height, img_width, 3), weights=None, classes=10)\r\n\r\noptimizer = tensorflow.keras.optimizers.Adam(learning_rate=0.01) # 0.001\r\n\r\nmodel.compile(optimizer=optimizer, loss = tensorflow.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\r\n\r\n\r\nEPOCHS = 50\r\n\r\n# STEPS_PER_EPOCH = TRAINSET_SIZE // BATCH_SIZE\r\n# VALIDATION_STEPS = VALSET_SIZE // BATCH_SIZE\r\n\r\n\r\ncallbacks = [\r\n    tensorflow.keras.callbacks.ModelCheckpoint('results/weights/best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=False),\r\n    tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, min_lr=0.00001)\r\n]\r\n\r\n\r\n\r\n\r\nresults = model.fit(train_generator, epochs=EPOCHS,\r\n                        #steps_per_epoch=STEPS_PER_EPOCH,\r\n                        #validation_steps=VALIDATION_STEPS,\r\n                        callbacks=callbacks,\r\n                        validation_data=validation_generator,\r\n                        use_multiprocessing=False)\r\n\r\nplt.figure(figsize=(8, 8))\r\nplt.title(\"Learning curve\")\r\nplt.plot(results.history[\"loss\"], label=\"loss\")\r\nplt.plot(results.history[\"val_loss\"], label=\"val_loss\")\r\nplt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"log_loss\")\r\nplt.legend();\r\nplt.savefig('./results/plots/train_loss.png')\r\n\r\n\r\n\r\nplt.figure(figsize=(8, 8))\r\nplt.title(\"Learning curve\")\r\nplt.plot(results.history[\"accuracy\"], label=\"accuracy\")\r\nplt.plot(results.history[\"val_accuracy\"], label=\"val_accuracy\")\r\nplt.plot( np.argmax(results.history[\"val_accuracy\"]), np.max(results.history[\"val_accuracy\"]), marker=\"x\", color=\"r\", label=\"best model\")\r\nplt.xlabel(\"Epochs\")\r\nplt.ylabel(\"accuracy\")\r\nplt.legend();\r\nplt.savefig('./esults/plots/train_accuracy.png')\r\n```", "> E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered\r\n\r\nFrom the above stacktrace it looks like you have incompatible cuda/cudnn/tf versions installed on your system.\r\nNote that recently released TF 2.5 binary supports cuda 11.2 and cudnn 8.1\r\nAlso see https://www.tensorflow.org/install/source#gpu\r\nYou may quickly want to check your example with google colab where above configuration is pre installed for use.\r\nThis will help us to determine if changing your system config helps to fix the problem.\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50205\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50205\">No</a>\n"]}, {"number": 50204, "title": "tensorflow removed numpy 1.20.3", "body": "So I built tensorflow 2.5.0 successfully, but when I installed the wheel it removed my numpy 1.20.3 and downgraded it to 1.19.5 instead. \r\n\r\nIs this version pin necessary?", "comments": ["It's a deliberate decision by TF, but the reasons are not very well documented.\r\n\r\nSee #47691, #48918, #48935.\r\n\r\n", "With numpy 1.20, a lot of tests will fail with\r\n\r\n```\r\nNotImplementedError: Cannot convert a symbolic Tensor (rnn/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported\r\n```\r\n\r\nPR that tried to fix that got rolled back as it broke internal stuff.\r\n\r\nUnforutnately, Google uses py3.6 in most of the CI and there's no numpy 1.20 there.", "> Unforutnately, Google uses py3.6 in most of the CI and there's no numpy 1.20 there.\r\n\r\nI think before anything else, this needs to be resolved first. See also https://github.com/tensorflow/tensorflow/pull/48918#issuecomment-847921424", "Yes, but moving a behemoth is slower. There are people working on that, takes a lot of time.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "*activity*\r\n\r\nThis is not stale. It is *blocked by a slow moving behemoth* and should be kept open.", "This makes `tensorflow >= 2.2` incompatible with `gensim >= 4.1`  because `gensim` requires `numpy >= 1.20` on `python 3.8`.", "This should be fixed now, I think?", "Hi @beew! We are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet?  Thanks!", "@mohantym \r\n> Hi @beew! We are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6 yet? Thanks!\r\n\r\nTF 2.6 was released on Aug 9, https://github.com/tensorflow/tensorflow/tags, is it the one you meant?\r\n", "`ERROR: tensorflow 2.6.0 has requirement numpy~=1.19.2,`", "Hi @banderlog @beew, I upgraded Numpy version to 1.21.2 in Colab  succesfully in this[ issue](https://github.com/tensorflow/tensorflow/issues/52380),  will  check it in my local system too. Thanks!", "@mohantym numpy version is hardcoded in the METADATA file: `Requires-Dist: numpy (~=1.19.2)`. You could change it manually and upgrade numpy, but then you may find that something ceased to work abruptly.", "TF 2.7 will have wider range for numpy.", "As numpy 1.20 is compatible with TF2.7 and later version (`tf-nightly`), I think this was resolved.\r\n\r\nI am closing this issue as this was resolved. Please feel free to reopen if I am wrong. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50204\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50204\">No</a>\n"]}, {"number": 50203, "title": "Don't force reinstall TB", "body": "Force reinstalling causes all TB's dependencies to also be reinstalled and that results in numpy 1.20 being used. TF is currently incompatible with numpy 1.20", "comments": []}, {"number": 50201, "title": "Update raw_ops_test.py", "body": "", "comments": []}, {"number": 50200, "title": "Mixed precision failed with OOM on A100 GPUs and Tensorflow 2.5", "body": "We have a keras based code that we would like to run in mixed precision.\r\n\r\nWe enable the mixed precision computation with\r\n\r\n```\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_global_policy(policy)\r\n```\r\n\r\nThe code works when mixed precision policy is disabled.\r\nThe code works when mixed precision is enabled with Tensorflow 2.4 on a nvidia titan RTX cards\r\nThe code fails  with OOM error when mixed precision is enabled with Tensorflow 2.5 on A100 GPU even with a reduced batch size compared to float32 version\r\n\r\n```\r\nnode06:632909:635501 [6] bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/alloc.h:41 NCCL WARN Cuda failure 'out of memory'\r\n```", "comments": ["@jrabary Could you please provide the simple standalone code/ colab link to reproduce the issue at our end.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@jrabary  Please check this [colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/mixed_precision.ipynb#scrollTo=ADgK2L5hC_x5) for mixed precison example and it should support A100 GPU.\r\n\r\nCould you Please check with TF 2.4 on A100 GPU and let us know whether  code is working or not. Thanks!", "@jrabary Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50200\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50200\">No</a>\n"]}, {"number": 50199, "title": "Subtract minimum area from AUC metric", "body": "**System information**\r\n- TensorFlow version (you are using): nightly\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behavior is to report ROC and PR AUC as a raw number.  Since the theoretical AUC for ROC of a chance classifier is 0.5 and for PR is the ratio of true positives to all data points, make this an option for reporting.\r\n\r\n**Will this change the current api? How?**\r\nThis will add a new optional Boolean argument, subtract_minimum_area, to the dunder init.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who wants to know whether their classifier is better than flipping a balanced coin.\r\n\r\n**Any Other info.**\r\nCode is on my fork at https://github.com/brethvoice/tensorflow\r\n", "comments": ["@brethvoice\r\n\r\n Could you please elaborate the issue with complete details,and what are the changes to be needed.Thanks\r\n", "> @brethvoice\r\n> \r\n> Could you please elaborate the issue with complete details,and what are the changes to be needed.Thanks\r\n\r\nComplete details of changes requested (already in my fork):\r\n\r\n1. Add new Boolean argument `subtract_minimum_area` to dunder init of AUC metric (default `False`)\r\n2. If `False`, do nothing\r\n3. If `True', subtract 0.5 from ROC AUC, or subtract proportion of true positives to overall number of data points from PR AUC\r\n\r\nYou can see the changes in the `result` method of the AUC class.  My [fork](https://github.com/brethvoice/tensorflow/blob/master/tensorflow/python/keras/metrics.py) only implements it for Python, and only in the case (for PR AUC) where:\r\n`self.summation_method != metrics_utils.AUCSummationMethod.INTERPOLATION` so these details would have to be worked out.", "@brethvoice, for ROC curve this is simply subtracting 0.5, and anyone can do it for themselves. For PR AUC it may be somewhat useful, but I'm not convinced that it is used widely enough to warrant an API change.", "> @brethvoice, for ROC curve this is simply subtracting 0.5, and anyone can do it for themselves. For PR AUC it may be somewhat useful, but I'm not convinced that it is used widely enough to warrant an API change.\r\n\r\n@deeb02 it is definitely not used widely.  However, I wonder if that is simply because nobody has made it available?  I certainly would like to know when the PR-AUC is above its minimum value, and by how much.  Agreed about subtracting 0.5 from ROC AUC, however."]}, {"number": 50197, "title": "Refactor SparseSegmentGrad kernel into a functor (NFC)", "body": "This is in preparation for adding a GPU implementation. No functional change.\r\n\r\ncc @nluehr ", "comments": []}, {"number": 50196, "title": "Overwhelming issues on Mac M1 with newest TF 2.5", "body": "Several issues after installing the TensorFlow metal plugin for Mac M1 according to the provided [documentation](https://developer.apple.com/metal/tensorflow-plugin/):\r\n\r\n- Model crashes when using Adam optimizer (logs will follow)\r\n- Even on SGD optimizer, though it does not crash, the loss is as high as possible and does not change\r\n- More issues, but since these were so fundamentally flawed, I think I'll just leave it at that\r\n\r\nHere's a basic script, taken from a short [demo notebook](https://www.tensorflow.org/tutorials/quickstart/beginner):\r\n```python\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10)\r\n])\r\n\r\npredictions = model(x_train[:1]).numpy()\r\ntf.nn.softmax(predictions).numpy()\r\n\r\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\nloss_fn(y_train[:1], predictions).numpy()\r\n\r\nmodel.compile(optimizer = 'sgd', loss = loss_fn)\r\nmodel.fit(x_train, y_train, epochs=100)\r\n```\r\n\r\nHere's what one would expect the logs to look like (run on a Colab runtime):\r\n```\r\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\r\n11493376/11490434 [==============================] - 0s 0us/step\r\nEpoch 1/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.7249\r\nEpoch 2/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3882\r\nEpoch 3/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.3217\r\nEpoch 4/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.2853\r\nEpoch 5/5\r\n1875/1875 [==============================] - 2s 1ms/step - loss: 0.2566\r\n```\r\n\r\nAnd here's what they look like when run on the Mac M1:\r\n```\r\nInit Plugin\r\nInit Graph Optimizer\r\nInit Kernel\r\nMetal device set to: Apple M1\r\n2021-06-10 08:05:36.921503: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2021-06-10 08:05:36.921605: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n2021-06-10 08:05:37.062193: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-06-10 08:05:37.062396: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\r\nEpoch 1/5\r\n2021-06-10 08:05:37.140640: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000\r\nEpoch 2/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000\r\nEpoch 3/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000\r\nEpoch 4/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000\r\nEpoch 5/5\r\n1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000\r\n```\r\n\r\nHere's what happens when you try to supplement that with the `'adam'` optimizer:\r\n```\r\nInit Plugin\r\nInit Graph Optimizer\r\nInit Kernel\r\nMetal device set to: Apple M1\r\n2021-06-10 08:07:02.300603: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\r\n2021-06-10 08:07:02.300696: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\r\n2021-06-10 08:07:02.375056: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-06-10 08:07:02.375226: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\r\nEpoch 1/5\r\n2021-06-10 08:07:02.458202: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\r\n2021-06-10 08:07:02.487 python3[42618:4085236] -[MPSNDArrayIdentity encodeToCommandEncoder:commandBuffer:sourceArrays:resultState:destinationArray:kernelDAGObject:]: unrecognized selector sent to instance 0x1695be290\r\n2021-06-10 08:07:02.490 python3[42618:4085236] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[MPSNDArrayIdentity encodeToCommandEncoder:commandBuffer:sourceArrays:resultState:destinationArray:kernelDAGObject:]: unrecognized selector sent to instance 0x1695be290'\r\n*** First throw call stack:\r\n(\r\n\t0   CoreFoundation                      0x0000000199a4a320 __exceptionPreprocess + 240\r\n\t1   libobjc.A.dylib                     0x0000000199778c04 objc_exception_throw + 60\r\n\t2   CoreFoundation                      0x0000000199ad9020 -[NSObject(NSObject) __retain_OA] + 0\r\n\t3   CoreFoundation                      0x00000001999ac184 ___forwarding___ + 1444\r\n\t4   CoreFoundation                      0x00000001999abb30 _CF_forwarding_prep_0 + 96\r\n\t5   libmetal_plugin.dylib               0x0000000134a55db4 ___ZN12metal_plugin14MPSApplyAdamOpIfE7ComputeEPNS_15OpKernelContextE_block_invoke.115 + 92\r\n\t6   libdispatch.dylib                   0x0000000199725420 _dispatch_client_callout + 20\r\n\t7   libdispatch.dylib                   0x0000000199733a98 _dispatch_lane_barrier_sync_invoke_and_complete + 60\r\n\t8   libmetal_plugin.dylib               0x0000000134a533c0 _ZN12metal_plugin14MPSApplyAdamOpIfE7ComputeEPNS_15OpKernelContextE + 4884\r\n\t9   libmetal_plugin.dylib               0x0000000134a51e78 _ZN12metal_pluginL15ComputeOpKernelINS_14MPSApplyAdamOpIfEEEEvPvP18TF_OpKernelContext + 44\r\n\t10  _pywrap_tensorflow_internal.so      0x0000000148076460 _ZN10tensorflow15PluggableDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE + 148\r\n\t11  libtensorflow_framework.2.dylib     0x0000000115d78664 _ZN10tensorflow12_GLOBAL__N_113ExecutorStateINS_15PropagatorStateEE7ProcessENS2_10TaggedNodeEx + 3080\r\n\t12  libtensorflow_framework.2.dylib     0x0000000115d79dbc _ZNSt3__110__function6__funcIZN10tensorflow12_GLOBAL__N_113ExecutorStateINS2_15PropagatorStateEE7RunTaskIZNS6_13ScheduleReadyEPN4absl14lts_2020_09_2313InlinedVectorINS5_10TaggedNodeELm8ENS_9allocatorISB_EEEEPNS5_20TaggedNodeReadyQueueEEUlvE0_EEvOT_EUlvE_NSC_ISL_EEFvvEEclEv + 56\r\n\t13  _pywrap_tensorflow_internal.so      0x00000001487aa230 _ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi + 1508\r\n\t14  _pywrap_tensorflow_internal.so      0x00000001487a9b28 _ZZN10tensorflow6thread16EigenEnvironment12CreateThreadENSt3__18functionIFvvEEEENKUlvE_clEv + 80\r\n\t15  libtensorflow_framework.2.dylib     0x0000000116314b08 _ZN10tensorflow12_GLOBAL__N_17PThread8ThreadFnEPv + 120\r\n\t16  libsystem_pthread.dylib             0x00000001998d206c _pthread_start + 320\r\n\t17  libsystem_pthread.dylib             0x00000001998ccda0 thread_start + 8\r\n)\r\nlibc++abi.dylib: terminating with uncaught exception of type NSException\r\n[1]    42618 abort      python3 tf-demo.py\r\n```\r\n\r\nRun on both Python 3.8 and Python 3.9, the same errors occur.", "comments": ["Same error on my end. M1 Mac mini, 16 GB RAM. I can't get anything to work since updating to tf2.5 from the Apple fork that they just archived.", "FYR. on my MBP M1, I created a new python 3.8 venv,  then `pip install tensorflow-macos` and `pip install tensorflow-metal`. So far didn't meet any problem. Your little test python script worked with either sgd or adam.", "> FYR. on my MBP M1, I created a new python 3.8 venv,  then `pip install tensorflow-macos` and `pip install tensorflow-metal`. So far didn't meet any problem. Your little test python script worked with either sgd or adam.\n\nDid you do the two steps before this in the instructions here https://developer.apple.com/metal/tensorflow-plugin/? Installing latest miniforge3 and installing the apple tensorflow-deps?", "> > FYR. on my MBP M1, I created a new python 3.8 venv,  then `pip install tensorflow-macos` and `pip install tensorflow-metal`. So far didn't meet any problem. Your little test python script worked with either sgd or adam.\r\n> \r\n> Did you do the two steps before this in the instructions here https://developer.apple.com/metal/tensorflow-plugin/? Installing latest miniforge3 and installing the apple tensorflow-deps?\r\n\r\nNope, I am an old-school unix user. I use [macports](https://www.macports.org/) to build python and some dependency, e.g., BLAS for numpy, and use pip to build pip wheels needed by `tensorflow-macos`. ", "> > FYR. on my MBP M1, I created a new python 3.8 venv,  then `pip install tensorflow-macos` and `pip install tensorflow-metal`. So far didn't meet any problem. Your little test python script worked with either sgd or adam.\n> \n> Did you do the two steps before this in the instructions here https://developer.apple.com/metal/tensorflow-plugin/? Installing latest miniforge3 and installing the apple tensorflow-deps?\n\nYes, I did this.", "> > FYR. on my MBP M1, I created a new python 3.8 venv,  then `pip install tensorflow-macos` and `pip install tensorflow-metal`. So far didn't meet any problem. Your little test python script worked with either sgd or adam.\r\n> \r\n> Did you do the two steps before this in the instructions here https://developer.apple.com/metal/tensorflow-plugin/? Installing latest miniforge3 and installing the apple tensorflow-deps?\r\n\r\nYes, I did this and can exactly replicate the issue.", "@aseembehl, I did just that and did not get the same results. Here is exactly what I run:\r\n\r\n```\r\nconda create -n tf python=3.8\r\nconda activate tf\r\nconda install -c apple tensorflow-deps\r\npython -m pip install tensorflow-macos\r\npython -m pip install tensorflow-metal\r\n```\r\n\r\nThen, I run the demo script and get exactly the original output I uploaded. Not sure why this differs between you and I.", "Is it possible that I'd have to reinstall miniforge3? When I try to do this (using the `-u` (upgrade) flag), I get this error towards the end:\r\n\r\n```\r\nEncountered problems while solving:\r\n  - nothing provides requested ca-certificates 2020.12.5 h4653dfc_0\r\n  - nothing provides requested tzdata 2021a he74cb21_0\r\n  - nothing provides requested idna 2.10 pyh9f0ad1d_0\r\n  - nothing provides requested six 1.16.0 pyh6c4a22f_0\r\n  - nothing provides requested wheel 0.36.2 pyhd3deb0d_0\r\n  - nothing provides requested certifi 2020.12.5 py39h2804cbe_1\r\n  - nothing provides requested pyopenssl 20.0.1 pyhd8ed1ab_0\r\n  - nothing provides requested urllib3 1.26.4 pyhd8ed1ab_0\r\n  - nothing provides requested requests 2.25.1 pyhd3deb0d_0\r\n\r\nERROR   Could not solve for environment specs\r\n```", "OH, did not see that MacOS Monterey is the requirement. I'm still on Big Sur but will update to the developer beta soon. That is almost certainly the problem here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50196\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50196\">No</a>\n", "> @aseembehl, I did just that and did not get the same results. Here is exactly what I run:\r\n> \r\n> ```\r\n> conda create -n tf python=3.8\r\n> conda activate tf\r\n> conda install -c apple tensorflow-deps\r\n> python -m pip install tensorflow-macos\r\n> python -m pip install tensorflow-metal\r\n> ```\r\n> \r\n> Then, I run the demo script and get exactly the original output I uploaded. Not sure why this differs between you and I.\r\n\r\nMaybe you misread my comment. TF with macOS doesn't work for me as well. I ran into the same problems as you did, NSexception with Adam and loss blowups with SGD. ", "> OH, did not see that MacOS Monterey is the requirement. I'm still on Big Sur but will update to the developer beta soon. That is almost certainly the problem here.\r\n\r\nWhere do you see the Monterey requirement? Are you referring to the mention on Metal homepage?", "@aseembehl, it says it [here](https://developer.apple.com/metal/tensorflow-plugin/)\r\n> ### **OS Requirements**\r\n> * macOS 12.0+", "Just updated to MacOS Monterey and this fixed the issue.", "Updating to 12.0 beta worked for me as well! Perhaps that should be displayed in bold on the requirements page or be stressed that it is a beta OS. I mistakenly assumed it was the current stable release Big Sur (11.x). Thanks!!!", "I am having same issue , apple M1, python3.8 . I have tried the fix and similar things but nothing has worked \r\n\r\nhere is the error stack  (I don't understand anything here )\r\n\r\n`Epoch 1/100\r\n2021-11-10 15:09:53.313171: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\r\n2021-11-10 15:09:53.336 python[77750:5229041] -[MPSGraph adamUpdateWithLearningRateTensor:beta1Tensor:beta2Tensor:epsilonTensor:beta1PowerTensor:beta2PowerTensor:valuesTensor:momentumTensor:velocityTensor:maximumVelocityTensor:gradientTensor:name:]: unrecognized selector sent to instance 0x15e61f2c0\r\n2021-11-10 15:09:53.354 python[77750:5229041] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[MPSGraph adamUpdateWithLearningRateTensor:beta1Tensor:beta2Tensor:epsilonTensor:beta1PowerTensor:beta2PowerTensor:valuesTensor:momentumTensor:velocityTensor:maximumVelocityTensor:gradientTensor:name:]: unrecognized selector sent to instance 0x15e61f2c0'\r\n*** First throw call stack:\r\n(\r\n\t0   CoreFoundation                      0x0000000187e53838 __exceptionPreprocess + 240\r\n\t1   libobjc.A.dylib                     0x0000000187b7d0a8 objc_exception_throw + 60\r\n\t2   CoreFoundation                      0x0000000187ee4694 -[NSObject(NSObject) __retain_OA] + 0\r\n\t3   CoreFoundation                      0x0000000187db4cd4 ___forwarding___ + 1444\r\n\t4   CoreFoundation                      0x0000000187db4670 _CF_forwarding_prep_0 + 96\r\n\t5   libmetal_plugin.dylib               0x000000013ecae290 _ZN12metal_plugin14MPSApplyAdamOpIfEC2EPNS_20OpKernelConstructionE + 656\r\n\t6   libmetal_plugin.dylib               0x000000013ecadebc _ZN12metal_pluginL14CreateOpKernelINS_14MPSApplyAdamOpIfEEEEPvP23TF_OpKernelConstruction + 52\r\n\t7   libtensorflow_framework.2.dylib     0x0000000133d985d4 _ZN10tensorflow12_GLOBAL__N_120KernelBuilderFactory6CreateEPNS_20OpKernelConstructionE + 88\r\n\t8   libtensorflow_framework.2.dylib     0x0000000133e1a158 _ZN10tensorflow14CreateOpKernelENS_10DeviceTypeEPNS_10DeviceBaseEPNS_9AllocatorEPNS_22FunctionLibraryRuntimeEPNS_11ResourceMgrERKNSt3__110shared_ptrIKNS_14NodePropertiesEEEiPPNS_8OpKernelE + 784\r\n\t9   libtensorflow_framework.2.dylib     0x0000000133ff52b8 _ZN10tensorflow21CreateNonCachedKernelEPNS_6DeviceEPNS_22FunctionLibraryRuntimeERKNSt3__110shared_ptrIKNS_14NodePropertiesEEEiPPNS_8OpKernelE + 272\r\n\t10  libtensorflow_framework.2.dylib     0x0000000133f9fc20 _ZN10tensorflow26FunctionLibraryRuntimeImpl12CreateKernelERKNSt3__110shared_ptrIKNS_14NodePropertiesEEEPNS_22FunctionLibraryRuntimeEPPNS_8OpKernelE + 600\r\n\t11  libtensorflow_framework.2.dylib     0x0000000133e1dd80 _ZN10tensorflow9OpSegment12FindOrCreateERKNSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES9_PPNS_8OpKernelENS1_8functionIFNS_6StatusESC_EEE + 272\r\n\t12  _pywrap_tensorflow_internal.so      0x000000011efc1258 _ZNSt3__110__function6__funcIZN10tensorflow13DirectSession15CreateExecutorsERKNS2_15CallableOptionsEPNS_10unique_ptrINS3_16ExecutorsAndKeysENS_14default_deleteIS8_EEEEPNS7_INS3_12FunctionInfoENS9_ISD_EEEEPNS3_12RunStateArgsEE4$_11NS_9allocatorISJ_EEFNS2_6StatusERKNS_10shared_ptrIKNS2_14NodePropertiesEEEPPNS2_8OpKernelEEEclESS_OSV_ + 140\r\n\t13  libtensorflow_framework.2.dylib     0x000000013400a430 _ZN10tensorflow22ImmutableExecutorState10InitializeERKNS_5GraphE + 1192\r\n\t14  libtensorflow_framework.2.dylib     0x0000000133ff5064 _ZN10tensorflow16NewLocalExecutorERKNS_19LocalExecutorParamsERKNS_5GraphEPPNS_8ExecutorE + 304\r\n\t15  libtensorflow_framework.2.dylib     0x0000000134002e6c _ZN10tensorflow12_GLOBAL__N_124DefaultExecutorRegistrar7Factory11NewExecutorERKNS_19LocalExecutorParamsERKNS_5GraphEPNSt3__110unique_ptrINS_8ExecutorENS9_14default_deleteISB_EEEE + 48\r\n\t16  libtensorflow_framework.2.dylib     0x00000001340037e8 _ZN10tensorflow11NewExecutorERKNSt3__112basic_stringIcNS0_11char_traitsIcEENS0_9allocatorIcEEEERKNS_19LocalExecutorParamsERKNS_5GraphEPNS0_10unique_ptrINS_8ExecutorENS0_14default_deleteISG_EEEE + 92\r\n\t17  _pywrap_tensorflow_internal.so      0x000000011efbb1e4 _ZN10tensorflow13DirectSession15CreateExecutorsERKNS_15CallableOptionsEPNSt3__110unique_ptrINS0_16ExecutorsAndKeysENS4_14default_deleteIS6_EEEEPNS5_INS0_12FunctionInfoENS7_ISB_EEEEPNS0_12RunStateArgsE + 3160\r\n\t18  _pywrap_tensorflow_internal.so      0x000000011efbdd80 _ZN10tensorflow13DirectSession12MakeCallableERKNS_15CallableOptionsEPx + 276\r\n\t19  _pywrap_tensorflow_internal.so      0x000000011bb744c4 _ZN10tensorflow10SessionRef12MakeCallableERKNS_15CallableOptionsEPx + 192\r\n\t20  _pywrap_tensorflow_internal.so      0x000000011bb6e7e8 _ZN10tensorflow12_GLOBAL__N_118MakeCallableHelperEPNS_7SessionEPK9TF_BufferPxP9TF_Status + 116\r\n\t21  _pywrap_tf_session.so               0x0000000135755eec _ZZN8pybind1112cpp_function10initializeIZL32pybind11_init__pywrap_tf_sessionRNS_7module_EE3$_0xJP10TF_SessionPK9TF_BufferEJNS_4nameENS_5scopeENS_7siblingEEEEvOT_PFT0_DpT1_EDpRKT2_ENUlRNS_6detail13function_callEE_8__invokeESQ_ + 212\r\n\t22  _pywrap_tf_session.so               0x0000000135741934 _ZN8pybind1112cpp_function10dispatcherEP7_objectS2_S2_ + 3216\r\n\t23  python                              0x00000001005cd8cc cfunction_call_varargs + 292\r\n\t24  python                              0x00000001005ccf50 _PyObject_MakeTpCall + 640\r\n\t25  python                              0x00000001006e23e0 call_function + 680\r\n\t26  python                              0x00000001006deebc _PyEval_EvalFrameDefault + 29472\r\n\t27  python                              0x00000001005cdd90 function_code_fastcall + 128\r\n\t28  python                              0x00000001005ccc1c _PyObject_FastCallDict + 284\r\n\t29  python                              0x00000001005cecc4 _PyObject_Call_Prepend + 156\r\n\t30  python                              0x0000000100645044 slot_tp_init + 288\r\n\t31  python                              0x0000000100652f28 type_call + 308\r\n\t32  python                              0x00000001005ccf50 _PyObject_MakeTpCall + 640\r\n\t33  python                              0x00000001006e23e0 call_function + 680\r\n\t34  python                              0x00000001006deebc _PyEval_EvalFrameDefault + 29472\r\n\t35  python                              0x00000001005cdd90 function_code_fastcall + 128\r\n\t36  python                              0x00000001006e2348 call_function + 528\r\n\t37  python                              0x00000001006deea0 _PyEval_EvalFrameDefault + 29444\r\n\t38  python                              0x00000001005cdd90 function_code_fastcall + 128\r\n\t39  python                              0x00000001006e2348 call_function + 528\r\n\t40  python                              0x00000001006deea0 _PyEval_EvalFrameDefault + 29444\r\n\t41  python                              0x00000001005cdd90 function_code_fastcall + 128\r\n\t42  python                              0x00000001005ccc1c _PyObject_FastCallDict + 284\r\n\t43  python                              0x00000001005cecc4 _PyObject_Call_Prepend + 156\r\n\t44  python                              0x0000000100642774 slot_tp_call + 296\r\n\t45  python                              0x00000001005ccf50 _PyObject_MakeTpCall + 640\r\n\t46  python                              0x00000001006e23e0 call_function + 680\r\n\t47  python                              0x00000001006def34 _PyEval_EvalFrameDefault + 29592\r\n\t48  python                              0x00000001006d7584 _PyEval_EvalCodeWithName + 3340\r\n\t49  python                              0x00000001005cdf10 _PyFunction_Vectorcall + 236\r\n\t50  python                              0x00000001005ccb84 _PyObject_FastCallDict + 132\r\n\t51  python                              0x0000000100793d0c partial_call + 444\r\n\t52  python                              0x00000001005ccf50 _PyObject_MakeTpCall + 640\r\n\t53  python                              0x00000001006e23e0 call_function + 680\r\n\t54  python                              0x00000001006defac _PyEval_EvalFrameDefault + 29712\r\n\t55  python                              0x00000001006d7584 _PyEval_EvalCodeWithName + 3340\r\n\t56  python                              0x00000001005cdf10 _PyFunction_Vectorcall + 236\r\n\t57  python                              0x00000001005d191c method_vectorcall + 156\r\n\t58  python                              0x00000001006e2348 call_function + 528\r\n\t59  python                              0x00000001006defac _PyEval_EvalFrameDefault + 29712\r\n\t60  python                              0x00000001006d7584 _PyEval_EvalCodeWithName + 3340\r\n\t61  python                              0x00000001005cdf10 _PyFunction_Vectorcall + 236\r\n\t62  python                              0x00000001005d191c method_vectorcall + 156\r\n\t63  python                              0x00000001006e2348 call_function + 528\r\n\t64  python                              0x00000001006defac _PyEval_EvalFrameDefault + 29712\r\n\t65  python                              0x00000001005cdd90 function_code_fastcall + 128\r\n\t66  python                              0x00000001006e2348 call_function + 528\r\n\t67  python                              0x00000001006def34 _PyEval_EvalFrameDefault + 29592\r\n\t68  python                              0x00000001006d7584 _PyEval_EvalCodeWithName + 3340\r\n\t69  python                              0x00000001007349d8 PyRun_StringFlags + 292\r\n\t70  python                              0x00000001006d20a0 builtin_eval + 748\r\n\t71  python                              0x000000010061f970 cfunction_vectorcall_FASTCALL + 284\r\n\t72  python                              0x00000001006e2348 call_function + 528\r\n\t73  python                              0x00000001006def34 _PyEval_EvalFrameDefault + 29592\r\n\t74  python                              0x00000001005cdd90 function_code_fastcall + 128\r\n\t75  python                              0x00000001006e2348 call_function + 528\r\n\t76  python                              0x00000001006def34 _PyEval_EvalFrameDefault + 29592\r\n\t77  python                              0x00000001006d7584 _PyEval_EvalCodeWithName + 3340\r\n\t78  python                              0x00000001005cdf10 _PyFunction_Vectorcall + 236\r\n\t79  python                              0x00000001006e2348 call_function + 528\r\n\t80  python                              0x00000001006def34 _PyEval_EvalFrameDefault + 29592\r\n\t81  python                              0x00000001006d7584 _PyEval_EvalCodeWithName + 3340\r\n\t82  python                              0x0000000100730e3c PyRun_SimpleFileExFlags + 1052\r\n\t83  python                              0x0000000100758eb0 Py_RunMain + 2980\r\n\t84  python                              0x000000010075a0a8 pymain_main + 1244\r\n\t85  python                              0x00000001005a41e0 main + 56\r\n\t86  libdyld.dylib                       0x0000000187cf5430 start + 4\r\n)\r\nlibc++abi: terminating with uncaught exception of type NSException\r\n[1]    77750 abort `", "I need help as I cannot get tensorflow installed in M1 using macOS Monterey 12.0.1. \r\nI have managed to create conda virtual env and managed to install tensorflow-deps for apple.\r\nHowever, I cannot install tensroflow-macos for some reason.\r\n\r\n(tf) [~/Downloads/TensorFlow]\r\nkelvin > python -m pip install tensorflow-macos\r\nERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\r\nERROR: No matching distribution found for tensorflow-macos\r\n\r\nDoes anyone how to fix it?\r\n\r\nThanks\r\nKel\r\n\r\nNOTE: I have created a virtual env called tf. As shown above, it is activated. \r\nI still cannot install tensroflow-macos using pip install under the tf virtual env.\r\nSee below:\r\n\r\n(tf) [~]\r\nkelvin > python -m pip install tensorflow-macos\r\nERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\r\nERROR: No matching distribution found for tensorflow-macos\r\n\r\n(following this link: https://developer.apple.com/forums/thread/683757)\r\nI have fixed the problem by adding the following in the tf vir env.\r\n\r\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install tensorflow-macos\r\nGRPC_PYTHON_BUILD_SYSTEM_ZLIB=true pip install tensorflow-metal\r\n\r\nIt is working now. "]}, {"number": 50195, "title": "Update activations.py", "body": "Add Activation named `Softclip`", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50195) for more info**.\n\n<!-- need_sender_cla -->", "@tanujdhiman  Can you please sign CLA. Thanks!", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50195) for more info**.\n\n<!-- need_sender_cla -->", "Hi @gbaned \r\nI was thinking of this activation function to be added in this repository. I've tried this in my local system too. This works !!!\r\n\r\nThanks", "> @tanujdhiman Can you please sign CLA. Thanks!\r\n\r\nYes, I have !!!", "@googlebot I signed it!", "We are trying to minimize the Keras API to the most widely used functions.\r\nPlease refer to https://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md#carefully-weigh-whether-a-new-feature-should-be-included\r\nWe do not believe this met the guidelines for creating a new Keras API."]}, {"number": 50194, "title": "Error while converting .pb file into tflite ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed using pip3\r\n\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc. \r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\ntflite_convert --output_file=./model_converted.tflite --graph_def_file=./Pretrained_models/DNN/DNN_L.pb --input_shapes=0 --input_arrays=wav_data --output_arrays=labels_softmax --enable_select_tf_ops.\r\n\r\nLink of model: https://github.com/ARM-software/ML-KWS-for-MCU/tree/master/Pretrained_models\r\n\r\n\r\n", "comments": ["Please consider turning on the Select TF option for unsupported TF ops through TFLite builtin op set.\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select", "Instead of \"--enable_select_tf_ops\", use \"--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\"", "Thanks a lot. It worked.\n\nOn Thu, Jun 10, 2021 at 5:49 PM Jae sung Chung ***@***.***>\nwrote:\n\n> Instead of \"--enable_select_tf_ops\", use\n> \"--target_ops=TFLITE_BUILTINS,SELECT_TF_OPS\"\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/50194#issuecomment-858571784>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AKCP2BRQYS5KNMPWKSD3UXTTSCUTXANCNFSM46OG6QDA>\n> .\n>\n"]}, {"number": 50193, "title": "Why tensorflow convert the strings to bytes when using from_tensor_slices", "body": "tensorflow version: 2.5.0\r\nds.df is a pandas dataframe whose content are sentences.\r\n\r\n\r\n```\r\ndstf = tf.data.Dataset.from_tensor_slices((ds.df['content'].values, ds.df['label'].values))\r\ndstf = dstf.shuffle(buffer_size=10000).batch(32)\r\nfor mm in dstf.map(lambda x, y: (x, y) ).take(5):\r\n    print(mm)\r\n```\r\nhowever, it converts the `content` to bytes.\r\nHow to avoid this? \r\n\r\n![image](https://user-images.githubusercontent.com/26405281/121509758-8b993980-ca19-11eb-87e7-0f7db8eaddda.png)\r\n\r\n", "comments": ["@yananchen1989 ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks", "@yananchen1989 that is because TensorFlow relies on NumPy `dtypes` while constructing tensors. By default, numpy represents strings as variable-length byte arrays. [Reference](https://www.tensorflow.org/guide/tensor#string_tensors)\r\n\r\nPerhaps you can use a `map()` operation to decode those strings.", "> @yananchen1989 that is because TensorFlow relies on NumPy `dtypes` while constructing tensors. By default, numpy represents strings as variable-length byte arrays. [Reference](https://www.tensorflow.org/guide/tensor#string_tensors)\r\n> \r\n> Perhaps you can use a `map()` operation to decode those strings.\r\n\r\nThanks.  `byte-string` seems to be the default encoding format.\r\nIs there a formal way to convert byte-string to unicode string?\r\nI tries` tf.strings.unicode_decode` but it returns numbers.", "@yananchen1989 https://www.tensorflow.org/api_docs/python/tf/strings/unicode_transcode, maybe this can help. However, I think as long as you use `tf.string` dtype tensors, they will be byte-strings only.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50193\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50193\">No</a>\n"]}, {"number": 50192, "title": "Unable to install tensorflow 2.5.0 on jetson nano", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04, Jetpack 4.5.1\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): automated pip install (see log below)\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: python3-venv\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Nvidia\r\n\r\n**Describe the problem**\r\nUnable to install tensorflow 2.5.0 on nvidia jetson nano. I'm running the installation inside a virtual environment.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI followed https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md\r\nI was at the step: python -m pip install --use-feature=2020-resolver ., which is \"python -m pip install  .\" with the new version of pip.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nProcessing /home/leap/workspace/models/research\r\nRequirement already satisfied: Cython in /home/leap/workspace/tf/lib/python3.7/site-packages (from object-detection==0.1)\r\nCollecting apache-beam (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/dd/46/145223f4f89c6b2db0256d3770e28d02ab88fb8b700a84afa89a343cd094/apache-beam-2.30.0.zip\r\nCollecting avro-python3 (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/cc/97/7a6970380ca8db9139a3cc0b0e3e0dd3e4bc584fb3644e1d06e71e1a55f0/avro-python3-1.10.2.tar.gz\r\nCollecting contextlib2 (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\r\nCollecting lvis (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl\r\nCollecting lxml (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/e5/21/a2e4517e3d216f0051687eea3d3317557bde68736f038a3b105ac3809247/lxml-4.6.3.tar.gz\r\nCollecting matplotlib (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/60/d3/286925802edaeb0b8834425ad97c9564ff679eb4208a184533969aa5fc29/matplotlib-3.4.2.tar.gz\r\nCollecting pandas (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/e8/81/f7be049fe887865200a0450b137f2c574647b9154503865502cfd720ab5d/pandas-1.2.4.tar.gz\r\nCollecting pillow (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/21/23/af6bac2a601be6670064a817273d4190b79df6f74d8012926a39bc7aa77f/Pillow-8.2.0.tar.gz\r\nCollecting pycocotools (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/de/df/056875d697c45182ed6d2ae21f62015896fdb841906fe48e7268e791c467/pycocotools-2.0.2.tar.gz\r\nCollecting scipy (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/fe/fd/8704c7b7b34cdac850485e638346025ca57c5a859934b9aa1be5399b33b7/scipy-1.6.3.tar.gz\r\nCollecting six (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\r\nCollecting tf-models-official (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/96/08/81bbc275e8e9c6d1e03dd26daec3a67f45e6322804cbce3d51f93eae1961/tf_models_official-2.5.0-py2.py3-none-any.whl\r\nCollecting tf-slim (from object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl\r\nCollecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz\r\nCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz\r\nCollecting fastavro<2,>=0.21.4 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/6b/74/ea2e40fb661dcfcca3ab7744f8719869954514c69c1be69b409393860668/fastavro-1.4.1.tar.gz\r\nCollecting future<1.0.0,>=0.18.2 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz\r\nCollecting grpcio<2,>=1.29.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/8c/34/7dafc9052bd9b2b41c5a8912aeeca01e179d16de17e9c275633d4b807330/grpcio-1.38.0.tar.gz\r\nCollecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\r\nCollecting httplib2<0.20.0,>=0.8 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/15/dc/d14bce03f4bfd0214b90a3f556d7c96f75bb94ad597c816a641b962f22e9/httplib2-0.19.1-py3-none-any.whl\r\nRequirement already satisfied: numpy<1.21.0,>=1.14.3 in /home/leap/workspace/tf/lib/python3.7/site-packages (from apache-beam->object-detection==0.1)\r\nCollecting oauth2client<5,>=2.0.1 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/95/a9/4f25a14d23f0786b64875b91784607c2277eff25d48f915e39ff0cff505a/oauth2client-4.1.3-py2.py3-none-any.whl\r\nCollecting protobuf<4,>=3.12.2 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/d5/e0/20ba06eb42155cdb4c741e5caf9946e4569e26d71165abaecada18c58603/protobuf-3.17.3-py2.py3-none-any.whl\r\nCollecting pyarrow<4.0.0,>=0.15.1 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/62/d3/a482d8a4039bf931ed6388308f0cc0541d0cab46f0bbff7c897a74f1c576/pyarrow-3.0.0.tar.gz\r\nCollecting pydot<2,>=1.2.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/ea/76/75b1bb82e9bad3e3d656556eaa353d8cd17c4254393b08ec9786ac8ed273/pydot-1.4.2-py2.py3-none-any.whl\r\nCollecting pymongo<4.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/c4/2f/79e933655adcf6dbd00738b556cecae5f8ec709301ac10df6f488d83bb53/pymongo-3.11.4.tar.gz\r\nCollecting python-dateutil<3,>=2.8.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\r\nCollecting pytz>=2018.3 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl\r\nCollecting requests<3.0.0,>=2.24.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl\r\nCollecting typing-extensions<3.8.0,>=3.7.0 (from apache-beam->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\r\nCollecting kiwisolver>=1.1.0 (from lvis->object-detection==0.1)\r\n  Downloading https://files.pythonhosted.org/packages/90/55/399ab9f2e171047d28933ae4b686d9382d17e6c09a01bead4a6f6b5038f4/kiwisolver-1.3.1.tar.gz (53kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 2.1MB/s \r\nCollecting opencv-python>=4.1.0.25 (from lvis->object-detection==0.1)\r\n  Using cached https://files.pythonhosted.org/packages/bb/08/9dbc183a3ac6baa95fabf749ddb531bd26256edfff5b6c2195eca26258e9/opencv-python-4.5.1.48.tar.gz\r\nRequirement already satisfied: pyparsing>=2.4.0 in /home/leap/workspace/tf/lib/python3.7/site-packages (from lvis->object-detection==0.1)\r\nCollecting cycler>=0.10.0 (from lvis->object-detection==0.1)\r\n  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\r\nRequirement already satisfied: setuptools>=18.0 in /home/leap/workspace/tf/lib/python3.7/site-packages (from pycocotools->object-detection==0.1)\r\nCollecting tensorflow>=2.5.0 (from tf-models-official->object-detection==0.1)\r\n  Could not find a version that satisfies the requirement tensorflow>=2.5.0 (from tf-models-official->object-detection==0.1) (from versions: )\r\nNo matching distribution found for tensorflow>=2.5.0 (from tf-models-official->object-detection==0.1)\r\n\r\n\r\nThanks for the help.\r\n\r\nKashyap", "comments": ["@gitgkk  Could you please raise an issue in TF models [repo](https://github.com/tensorflow/models/issues/new/choose)  as it seems more related to object detection. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50192\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50192\">No</a>\n"]}, {"number": 50191, "title": "[MLIR][DISC] Add RAL (Runtime abstraction layer) Dialect", "body": "DISC is a e2e flow, including both compiler side and runtime side. For\r\nruntime side, we have different targeting environments (e.g. tensorflow,\r\npytorch, or sometimes even a standalone binary). In order to simplify\r\nthe design of the compiler side, we design a Runtime Abstraction Layer\r\n(RAL) to sperate the compiler side and runtime side. Thus the compiler\r\nside only need to target RAL itself and it is the responsibility of RAL\r\nto handle the differences between different targeting environments.\r\n\r\nOne of the most important functions of RAL is to manage stateful\r\nresources. To this end, it provides a context object, and hides all\r\nstateful operations behind this context, thus the compiler side itself\r\ndoesn't need to care about the resource initialization. For example, a\r\nkernel must be loaded before it can be launched on GPU. However, the\r\nloading operation should only be taken once during the whole lifetime of\r\nthe context in order to achieve the best performance. Based on the\r\ninitialization-free interfaces provided by RAL, compiler side can focus\r\non its core optimization logic and lets the RAL to manage the resource\r\nstatus.\r\n\r\nThe context mentioned above is passed as a parameter to the entry\r\nfunction and all RAL APIs should always use the context as their first\r\nargument. This CR also provides a pass to help to ensure this property.\r\nThe pass rewrites the entry function to make sure their first argument\r\nis the context. For entry function, the pass also rewrites its inputs\r\nand outputs. To be concrete, all the original inputs and outputs of the\r\nentry function are received from and sent to RAL through a sequence of\r\nRAL API calls correspondingly. The motivation behind this is to hide the\r\nimplementation details of I/Os. This design may also potentially enable\r\npartial execution of the compiled module when some of the inputs are\r\nready.", "comments": ["> Can you add the missing CMakeLists.txt so that this all test and run in [tensorflow/mlir-hlo](https://github.com/tensorflow/mlir-hlo) ?\r\n\r\nDone", "@joker-eph Hi, it seems that importing to inside code review stage is failed. Does it need to re-run again?", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 50190, "title": "Code in the Documentation is resulting in Warning Message", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#using_the_gradienttape_a_first_end-to-end_example\r\n\r\n## Description of issue (what needs changing): \r\nNeed to modify the code such that Warning goes away.\r\n\r\nPlease find the [Github Gist](https://colab.research.google.com/gist/worldpeaceaspirer/73e9f295a64cb513d6b7cd3aefdc2c38/warning_in_the_tutorial.ipynb) that demonstrates the Warning.", "comments": ["@worldpeaceaspirer\r\n\r\nIn order to expedite the trouble-shooting process, Could you please elaborate your  issue  with complete details. And what's need to be modified.Thanks\r\n", "I see the warning in your gist, but not in the tensorflow.org page.\r\n\r\nSo I think the page is working as intended.\r\n\r\nOr am I misunderstanding something?", "I think you get the warning because you are using `softmax` activation function in the output layer whereas original example uses none. The warning goes away by removing `softmax` activation function.\r\n```python\r\noutputs = tf.keras.layers.Dense(10, name=\"predictions\")(x2)\r\n```", "@MarkDaoust,\r\nWarning is not appearing in the documentation and is appearing when we execute the code and that's the reason for my confusion. The code in the documentation should be **Error-Free** and **Warning-Free**.\r\n\r\nDo you mean to say the **Warning** is expected? If it is so, we can specify the same in the page itself, to avoid confusion. If the warning is not intended, code should be modified to remove that warning.\r\n\r\nCan you please reopen this issue", "> I think you get the warning because you are using `softmax` activation function in the output layer whereas original example uses none. The warning goes away by removing `softmax` activation function.\r\n> \r\n> ```python\r\n> outputs = tf.keras.layers.Dense(10, name=\"predictions\")(x2)\r\n> ```\r\n\r\n@ymodak,\r\nIt is not technically correct to remove **`Softmax Activation`** from the **`Output Layer`** in case of **`Multi-Class Classification`** right? ", "@worldpeaceaspirer It depnds.\r\n\r\nThe last layer output, the loss function, and the metrics all need to be set to work in the same units (probability or logits)."]}, {"number": 50187, "title": "When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'", "body": "When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'. But when we try to send one tuple that includes one more parameters to the net instead of parameters directly,the net worked smoothly.Could someone tells me the reason?", "comments": ["@a1391651300 \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@a1391651300 \r\n\r\nFill the [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md) to analyse the issue reported here. Please refer similar issues, [#47390](https://github.com/tensorflow/tensorflow/issues/47390), [#33388](https://github.com/tensorflow/tensorflow/issues/33388).Thanks", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50187\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50187\">No</a>\n"]}, {"number": 50186, "title": "When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'. ", "body": "When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'. But when we try to send one tuple that includes one more parameters to the net instead of parameters directly,the net worked smoothly.Could someone tells me the reason?   ", "comments": ["@a1391651300 ,\r\n\r\nLooks like this is duplicate of issue #50187.Can you please close this issue, since it is already being tracked there? Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50186\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50186\">No</a>\n"]}, {"number": 50185, "title": "Fix the nonpip builds for MacOS.", "body": "PiperOrigin-RevId: 346205138\nChange-Id: I146b5486171e1d91dc3c239bd10fb904972375bb", "comments": []}, {"number": 50184, "title": "Update common_win.bat", "body": "Force reinstalling causes numpy 1.20 to be installed on the host which then breaks a ton of tests that were compiled with numpy 1.19.", "comments": []}, {"number": 50183, "title": "Fix cherrypick issue", "body": "", "comments": []}, {"number": 50182, "title": "Unable to read TFLite after conversion", "body": "### Problem Statement\r\nI tried converting a frozen garph - https://github.com/blaueck/tf-mtcnn/blob/master/mtcnn.pb to tflite using the TF v2.5.0. The  conversion was successful, but I am unable to load the generated TFLite for inference.\r\n\r\n### Code for conversion to tflite\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_arrays = ['input', 'min_size', 'thresholds', 'factor']\r\n\r\noutput_node_names = ['prob', 'landmarks', 'box']  # Output nodes\r\n\r\ngraph_def_file = 'mtcnn.pb'\r\n\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_node_names)\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n]\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ntflite_model = converter.convert()\r\n\r\nif __name__ == \"__main__\":\r\n    with tf.io.gfile.GFile('mtcnn.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n```\r\n\r\n### Code for loading TFLite \r\n```\r\n    model_path = r'mtcnn.tflite'\r\n    interpreter = tf.lite.Interpreter(model_path=model_path)\r\n```\r\n\r\n### Error while loading TFLite\r\n```\r\nFile \"C:\\Users\\a84191678\\Anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py\", line 348, in __init__\r\n    _interpreter_wrapper.CreateWrapperFromFile(\r\nValueError: Did not get operators or tensors in subgraph 1.\r\n```\r\n\r\n### System information\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n-   **TensorFlow installed from (source or binary)**: pip\r\n-   **TensorFlow version (use command below)**:2.5.0\r\n-   **Python version**:3.8\r\n\r\nNo CUDA/GPU.\r\n", "comments": ["There seems to be no problem with Ubuntu. I'm just a hobby programmer.\r\n- Ubuntu20.04 (Docker + Ubuntu 18.04 + Python 3.6)\r\n- TensorFlow v2.5.0 (source build)\r\n\r\n[model_from_pb_float32.tflite.tar.gz](https://github.com/tensorflow/tensorflow/files/6642049/model_from_pb_float32.tflite.tar.gz)\r\n\r\n```bash\r\n$ xhost +local: && \\\r\n  docker run -it --rm \\\r\n  -v `pwd`:/home/user/workdir \\\r\n  -v /tmp/.X11-unix/:/tmp/.X11-unix:rw \\\r\n  --device /dev/video0:/dev/video0:mwr \\\r\n  --net=host \\\r\n  -e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR \\\r\n  -e DISPLAY=$DISPLAY \\\r\n  --privileged \\\r\n  pinto0309/openvino2tensorflow:latest\r\n```\r\n```bash\r\n$ cd workdir\r\n$ wget https://github.com/blaueck/tf-mtcnn/raw/master/mtcnn.pb\r\n$ pb_to_tflite \\\r\n--pb_file_path mtcnn.pb \\\r\n--inputs input,min_size,thresholds,factor \\\r\n--outputs prob,landmarks,box\r\n$ python3\r\n```\r\n```bash\r\n>>> import tensorflow as tf\r\n>>> model_path = 'saved_model_from_pb/model_from_pb_float32.tflite'\r\n>>> interpreter = tf.lite.Interpreter(model_path=model_path)\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2021-06-12 11:10:20.333686: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-12 11:10:20.336650: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib::/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/omp/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-06-12 11:10:20.336665: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-06-12 11:10:20.336691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubuntu2004\r\n2021-06-12 11:10:20.336696: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubuntu2004\r\n2021-06-12 11:10:20.336721: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2021-06-12 11:10:20.336759: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1\r\nINFO: TfLiteFlexDelegate delegate: 7 nodes delegated out of 46 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 97 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 116 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 2 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 7 nodes delegated out of 63 nodes with 3 partitions.\r\n\r\n>>> tf.__version__\r\n'2.5.0'\r\n>>> \r\n```", "Hi @PINTO0309  \r\n\r\nThanks for responding, I tried the same on Ubuntu, didn't work for me.\r\n![image](https://user-images.githubusercontent.com/25407062/121927449-0c538080-cd0d-11eb-89aa-5ae579bf94d6.png)\r\n\r\nMay be I am doing something wrong in the script. Could you please share your pb to tflite script?\r\n", "@ankitShuklaDev\r\n\r\n> May be I am doing something wrong in the script. Could you please share your pb to tflite script?\r\n\r\nSorry. For example,\r\n\r\n```\r\n$ xhost +local: && \\\r\n  docker run -it --rm \\\r\n  -v `pwd`:/home/user/workdir \\\r\n  -v /tmp/.X11-unix/:/tmp/.X11-unix:rw \\\r\n  --device /dev/video0:/dev/video0:mwr \\\r\n  --net=host \\\r\n  -e XDG_RUNTIME_DIR=$XDG_RUNTIME_DIR \\\r\n  -e DISPLAY=$DISPLAY \\\r\n  --privileged \\\r\n  pinto0309/openvino2tensorflow:latest\r\n\r\n$ cd workdir\r\n\r\n$ pb_to_saved_model \\\r\n--pb_file_path mtcnn.pb \\\r\n--inputs input:0,min_size:0,thresholds:0,factor:0 \\\r\n--outputs prob:0,landmarks:0,box:0\r\n\r\n$ saved_model_to_tflite \\\r\n--saved_model_dir_path saved_model_from_pb \\\r\n--input_shapes [1,64,64,3] \\\r\n--output_no_quant_float32_tflite\r\n```\r\n```\r\n$ python3\r\n>>> import tensorflow as tf\r\n>>> model_path = 'tflite_from_saved_model/model_float32.tflite'\r\n>>> interpreter = tf.lite.Interpreter(model_path=model_path)\r\n\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2021-06-14 17:10:08.595271: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-06-14 17:10:08.595945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/intel/openvino_2021/data_processing/dl_streamer/lib:/opt/intel/openvino_2021/data_processing/gstreamer/lib:/opt/intel/openvino_2021/opencv/lib:/opt/intel/openvino_2021/deployment_tools/ngraph/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/tbb/lib::/opt/intel/openvino_2021/deployment_tools/inference_engine/external/hddl/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/omp/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/gna/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/external/mkltiny_lnx/lib:/opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n2021-06-14 17:10:08.596105: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\r\n2021-06-14 17:10:08.596117: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubuntu2004\r\n2021-06-14 17:10:08.596122: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubuntu2004\r\n2021-06-14 17:10:08.596145: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\r\n2021-06-14 17:10:08.596163: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1\r\nINFO: TfLiteFlexDelegate delegate: 7 nodes delegated out of 46 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 97 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 0 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 116 nodes with 2 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 2 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 7 nodes delegated out of 63 nodes with 3 partitions.\r\n```\r\n[model_float32.tflite.tar.gz](https://github.com/tensorflow/tensorflow/files/6650077/model_float32.tflite.tar.gz)\r\n\r\nA TensorFlow or TensorFlowLite runtime with FlexDelegate enabled is required. For example, I have committed the FlexDelegate enabled installer for armv7l and aarch64 below.\r\nhttps://github.com/PINTO0309/Tensorflow-bin\r\n\r\nAs an example, for an x86 machine, you can use the . After running **`./configure`**, you can build it with the following Bazel command.\r\n```\r\n$ git clone -b v2.5.0 https://github.com/tensorflow/tensorflow.git && cd tensorflow\r\n$ ./configure\r\n\r\n$ sudo bazel build \\\r\n--config=monolithic \\\r\n--config=noaws \\\r\n--config=nohdfs \\\r\n--config=nonccl \\\r\n--config=v2 \\\r\n--define=tflite_pip_with_flex=true \\\r\n--define=tflite_with_xnnpack=true \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```", "Hi @PINTO0309 ,\r\n\r\nThanks for a swift response. Could you share the  pb_to_saved_model and saved_model_to_tflite script too.\r\n\r\n", "My tool.\r\nHere: https://github.com/PINTO0309/openvino2tensorflow", "Thanks a ton @PINTO0309.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50182\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50182\">No</a>\n"]}, {"number": 50181, "title": "Fix backend of XLA for Fused ConvBiasRelu", "body": "This PR is mainly to fix the issue of XLA sharing the same backend with native TF when calling fused ConvBiasRelu (both will call into `stream->FusedConvolveWithAlgorithm()`), which will cause segmentation fault problem when cudnn frontend APIs are used.\r\n\r\nTherefore, this PR separate the `FusedConvolveWithAlgorithm` to (1) `FusedConvolveWithAlgorithm()` for legacy cudnn APIs and (2) `FusedConvolveWithExecutionPlan()` for cudnn frontend APIs. This behavior follows the `ConvolveWithAlgorithm()` and `ConvolveWithExecutionPlan()`.\r\n\r\nBesides, the code has been greatly simplified by following the design of `ConvolveWithAlgorithm()` and removing many overloaded functions.\r\n\r\ncc. @nluehr ", "comments": ["@awpr is changing XLA to use the new cuDNN APIs as well.  Will this be PR be needed once that change lands?", "> @awpr is changing XLA to use the new cuDNN APIs as well. Will this be PR be needed once that change lands?\r\n\r\nCurious how the XLA is going to support the new cuDNN's execution plan which is an opaque data structure at this moment. My understanding is that the execution plan (similar with algorithm index in the legacy cuDNN terms) needs to be serializable and our cuDNN+XLA team are working on that. @bas-aarts for vis.\r\n\r\nAlso, I think the PR is still needed since most of the PR is to simplify the code of FusedConvolveWithAlgorithm() to make it consistent with the design of ConvolveWithAlgorithm().", "My tentative plan was to make all the APIs internally look at the AlgorithmDesc to decide whether to use the Frontend API or the legacy entry point, and then switch over XLA by having it create an ExecutionPlan-based AlgorithmDesc instead of an enum-based one.  This goes in the opposite direction by having two separate entry points in the CudnnSupport for the two APIs.  I can probably work with either approach equally well.\r\n\r\nAs for deduplicating the DnnSupport methods, I was actually just looking into doing the same thing, so +1 to that.\r\n\r\nRe: needing to serialize ExecutionPlans, I'm still looking into the details there, but my impression is that the backend API makes the entire underlying data available via `cudnnBackendGetAttribute`, so I should be able to painstakingly collect all the attributes (recursively for nested descriptor attributes), serialize that, and re-inflate the graph afterwards.  That won't work if the opaque structs behind the descriptor pointers contain data that's not visible via the attribute API, but I'm hopeful that's not the case, since the set of attributes looks at least plausibly sufficient to describe an underlying kernel launch.", "Just Note, I included two commits from https://github.com/tensorflow/tensorflow/pull/50216 to make sure the tests get passed. It resolved an issue related to the general use of frontend API. cc @timshen91 "]}, {"number": 50180, "title": "TFLite: `Segmentation fault (core dumped)` when converting `CropAndResize` operation", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nI am having trouble converting the `tf.image.crop_and_resize` operation into TFLite when I use the `representative_dataset`. It can be converted without it but when I add it, a `Segmentation fault (core dumped)` error is raised during the conversion. Thank you for your upcoming answer.\r\n\r\n**Describe the expected behavior**\r\nThe layer and the model created around the layer is working well in Tensorflow, the error seems to occur only during the conversion.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nIMG_SIZE = 128\r\nNUM_BOXES = 100\r\nCROP_SIZE = 28\r\nNB_DATA_SAMPLES = 32\r\nBATCH_SIZE = 1\r\n\r\n\r\nclass CropLayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def call(self, inputs, **kwargs):\r\n        images_, boxes_ = inputs\r\n        box_indices = tf.reshape(\r\n            tf.repeat(\r\n                tf.expand_dims(tf.range(tf.shape(images_)[0], dtype=tf.int32), axis=-1),\r\n                NUM_BOXES,\r\n                axis=-1\r\n            ),\r\n            shape=(-1,)\r\n        )\r\n        cropped_images = tf.image.crop_and_resize(\r\n            image=images_,\r\n            boxes=tf.reshape(boxes_, (-1, 4)),\r\n            box_indices=box_indices,\r\n            crop_size=(CROP_SIZE, CROP_SIZE))\r\n        return cropped_images\r\n\r\n\r\nimages = tf.random.normal(\r\n    shape=(NB_DATA_SAMPLES, IMG_SIZE, IMG_SIZE, 3))\r\nboxes = tf.random.uniform((NB_DATA_SAMPLES, NUM_BOXES, 4), maxval=1)\r\n\r\nlayer = CropLayer()\r\n# Ensure that it is working, should be (NB_DATA_SAMPLES * NUM_BOXES, CROP_SIZE, CROP_SIZE, 3)\r\nprint('Should be (NB_DATA_SAMPLES * NUM_BOXES, CROP_SIZE, CROP_SIZE, 3), i.e. (3200, 28,28, 3):')\r\nprint(layer([images, boxes]).shape)\r\n\r\ninputs_model = [\r\n    tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), batch_size=BATCH_SIZE),\r\n    tf.keras.Input(shape=(NUM_BOXES, 4), batch_size=BATCH_SIZE)\r\n]\r\n\r\nmodel = tf.keras.models.Model(inputs=inputs_model, outputs=layer(inputs_model))\r\n\r\n\r\ndef representative_dataset_generator():\r\n    for image, bboxes in zip(images.numpy(), boxes.numpy()):\r\n        image_ = tf.expand_dims(image, 0)\r\n        bboxes_ = tf.expand_dims(bboxes, 0)\r\n        yield [image_, bboxes_]\r\n\r\n# Run the model on the representative dataset generator\r\nfor sample in representative_dataset_generator():\r\n    model(sample).shape\r\n\r\n# Converter\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\n\r\nprint('Converting without representative_dataset')\r\nquant_model = converter.convert()\r\nwith open('model.tflite', \"wb\") as f:\r\n    f.write(quant_model)\r\nprint('Converted and saved')\r\n\r\nprint('Converting with representative_dataset')\r\nconverter.representative_dataset = representative_dataset_generator\r\nquant_model = converter.convert()\r\n```\r\n\r\n**Other information**\r\nTF 2.5.0 does not fix the issue, neither does TF-nightly 2.6.0-dev20210607", "comments": ["Could you triage this MOT issue? @teijeong ", "Could you take a look? - @teijeong @Xhark ", "Additional information, I have the error on two computers operating on ubuntu (one is 18.04, the other 20.04) with CUDA 11.3, I don't have the error on some other computer with CUDA 11.2. I don't have the error if I run with TensorFlow on CPU", "@YannPourcenoux \r\n\r\nThe tested build configuration for tf2.4 is cuda-11.0.Please refer [this](https://www.tensorflow.org/install/source#gpu).Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50180\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50180\">No</a>\n"]}, {"number": 50179, "title": "Added missing files to BUILD.", "body": "Fixes #50145", "comments": ["FYI- when \"--spawn_strategy=sandboxed\" is specified in the \".bazelrc\" file (commit 259399ee, previous commit 9171e39a2), build with \"config=mkl\" fails in llvm_openmp. The changes in this PR fix this build issue.", "@mihaimaruseac  friendly ping. \r\nCan you please review this PR. \r\n\r\nThank.\r\n", "@gbaned \r\nIs there any chance this PR get reviewed?\r\n", "Apologies for the delay. OOO and hiring took most of my time these past days.", "Thanks a lot @mihaimaruseac \r\nThere are some failures on MacOS, but I do not think these are related to the PR. It looks like some global flags defined twice....\r\n\r\nQuick grepping suggest that same flags (e.g. `save_model_path`)  might be defined in  \r\n```\r\ntensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common_v1.py\r\n```\r\nand\r\n```\r\ntensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common.py\r\n```\r\n\r\n\r\nFollowing is an example of a test failure:\r\n\r\n```\r\nFAIL: MLIR tf_saved_model :: basic_v1_no_variable_lifting.py (1 of 1)\r\n******************** TEST 'MLIR tf_saved_model :: basic_v1_no_variable_lifting.py' FAILED ********************\r\nScript:\r\n--\r\n: 'RUN: at line 16';   /Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting | /Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/llvm-project/llvm/FileCheck /Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py\r\n--\r\nExit Code: 2\r\n\r\nCommand Output (stderr):\r\n--\r\n2021-06-15 13:43:12.632076: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\r\n2021-06-15 13:43:12.632150: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\r\n2021-06-15 13:43:12.632169: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\r\n2021-06-15 13:43:12.632187: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\r\nTraceback (most recent call last):\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py\", line 24, in <module>\r\n    from tensorflow.compiler.mlir.tensorflow.tests.tf_saved_model import common_v1\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/common_v1.py\", line 34, in <module>\r\n    flags.DEFINE_string('save_model_path', '', 'Path to save the model to.')\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/absl_py/absl/flags/_defines.py\", line 241, in DEFINE_string\r\n    DEFINE(parser, name, default, help, flag_values, serializer, **args)\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/absl_py/absl/flags/_defines.py\", line 82, in DEFINE\r\n    flag_values, module_name)\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/absl_py/absl/flags/_defines.py\", line 104, in DEFINE_flag\r\n    fv[flag.name] = flag\r\n  File \"/Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/absl_py/absl/flags/_flagvalues.py\", line 430, in __setitem__\r\n    raise _exceptions.DuplicateFlagError.from_flag(name, self)\r\nabsl.flags._exceptions.DuplicateFlagError: The flag 'save_model_path' is defined twice. First from tensorflow.compiler.mlir.tensorflow.tests.tf_saved_model.common, Second from tensorflow.compiler.mlir.tensorflow.tests.tf_saved_model.common_v1.  Description from first occurrence: Path to save the model to.\r\nFileCheck error: '<stdin>' is empty.\r\nFileCheck command line:  /Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/llvm-project/llvm/FileCheck /Volumes/BuildData/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py.test.runfiles/org_tensorflow/tensorflow/compiler/mlir/tensorflow/tests/tf_saved_model/basic_v1_no_variable_lifting.py\r\n```", "That doesn't seem to be related. LEt's try again", "@mihaimaruseac \r\nNow it worked!\r\n\r\nP. S.\r\nOn a side note -- it might be my bad luck, but pretty much every commit has some unrelated failures.... It seems that the internal TF checks are not that strict. Namely, some CLs that cause build errors somehow made their way into the code...", "We don't run Windows and MacOS presubmits internally as they take too much time. We have buildcop rotations where people should watch postsubmit builds and quickly turn them back to green but sometimes this process gets ignored/delayed"]}, {"number": 50177, "title": "Mathjax small correction in `tf.math.igammac`", "body": "", "comments": []}, {"number": 50176, "title": "Whose bug is this: gcc, MKL, or TF?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):11\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc (Debian 11.1.0-2) 11.1.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nTF build fails with a strange error:\r\n```shell\r\nexternal/mkl_dnn_v1/src/common/primitive_cache.cpp: In member function 'virtual void dnnl::impl::lru_primitive_cache_t::update_entry(const key_t&, const dnnl::impl::primitive_desc_t*)':\r\nexternal/mkl_dnn_v1/src/common/primitive_cache.cpp:155:60: error: no match for 'operator!=' (operand types are 'const std::thread::id' and 'const std::thread::id')\r\n  155 |     if (it == cache_mapper_.end() || it->first.thread_id() != key.thread_id())\r\n      |                                      ~~~~~~~~~~~~~~~~~~~~~ ^~ ~~~~~~~~~~~~~~~\r\n      |                                                         |                  |\r\n      |                                                         |                  const std::thread::id\r\n      |                                                         const std::thread::id\r\nIn file included from /usr/include/c++/11/utility:70,\r\n\r\n```\r\n\r\nIt looks that the `!=`  operator is not defined for `std::thread::id`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI am building with MKL:\r\n```\r\nbazel build --verbose_failures --config=mkl --config=v2 --config=nogcp --config=nonccl  -c opt --copt=-march=native --copt=\"-O3\" -s //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could this be caused by C++20 which removes the \"!=\"?\r\nhttps://en.cppreference.com/w/cpp/thread/thread/id", "There is a known issue with GCC 11 and oneDNN v2.2 discussed in https://github.com/oneapi-src/oneDNN/issues/1023.", "> Could this be caused by C++20 which removes the \"!=\"?\r\n> https://en.cppreference.com/w/cpp/thread/thread/id\r\n\r\nGCC 11 defaults to `-std=gnu++17` according to the documentation, so this should not be the issue unless C++20 was explicitly enabled. oneDNN v2.3-rc builds correctly with GCC 11.1 and default C++ standard setting.\r\n", "I think TF is compiles with `-std=c++14`, so, it cannot be the  issue. Furthermore, running the same command outside bazel (namely, without sandboxing) does not raise any error. \r\n", "Closing it since it is fixed in oneDNN upstream.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50176\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50176\">No</a>\n", "TF doesn't yet support C++17 (based on some comments above)", "@mihaimaruseac \r\nIt seems that some includes were missing in Intel's opeAPI library. The error surfaces in newer GCC versions (11+). ", "@eli-osherovich, @mihaimaruseac,\r\n\r\nThe build erros are the result of changes in header dependencies introduced in GCC 11. [oneDNN v2.2.4](https://github.com/oneapi-src/oneDNN/releases/tag/v2.2.4) and [oneDNN v2.3-rc2](https://github.com/oneapi-src/oneDNN/releases/tag/v2.3-rc2) include the fix. TF master branch [was just updated](https://github.com/tensorflow/tensorflow/pull/50329) to v2.3-rc2."]}, {"number": 50175, "title": "Skip unnecessary ops in embedding_lookup_sparse", "body": "- Skips the `unique` + `embedding_lookup` calls when possible and instead passes the embedding parameters directly to the sparse segment reduction op (or the corresponding ops for the weighted case), which is significantly more efficient.\r\n- This same optimization is already done in the Grappler [SimplifyEmbeddingLookupStage](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc#L4085-L4107) pass, but that does not apply in Eager mode or when weights are used.\r\n\r\ncc @nluehr ", "comments": ["The test failures have brought to light the fact that the gradient is no longer sparse, because `sparse_segment_mean_grad` returns a dense tensor. A sparse-output version of the gradient op may need to be added before this PR is viable.\r\n\r\nIf you agree, then this PR can be closed for now.", "Closing this due to the non-sparse gradients issue.\r\nI'll investigate whether a sparse-output version of sparse segment reduce grad is viable (additional motivation for this is the fact that \r\nGrappler cannot fully elide the Unique op because it's still needed for the backward graph)."]}]