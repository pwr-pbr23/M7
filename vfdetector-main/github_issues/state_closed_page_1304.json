[{"number": 13988, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "R1.3 changes have been merged back into master already."]}, {"number": 13987, "title": "Feature Request: C++ gradient for LRN", "body": "this resolves #13789 \r\n\r\nRegisters the `LRNGrad` op as the gradient for `LRN` by implementing `LRNGradHelper`\r\n\r\nSingle test case proves that everything was wired up correctly since I assume the `LRNGrad` code was thoroughly tested.\r\n\r\nThis is my first PR to TensorFlow. When giving feedback, please assume that I barely know what I'm doing.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "TF Test Suite and Ubuntu CC state have been in \"Waiting for status to be reported\" for a while now", "@benoitsteiner @suharshs @skye @andydavis1 @asimshankar\r\n\r\nSorry for the broadcast ping, but not sure who would be best at this point.\r\n\r\nThis has been sitting for a while. Can it be merged? \r\nIf not, why? It's not clear.\r\n\r\nthanks", "Jenkins test this please.\r\n\r\nSorry for the delay, I think we're a little behind on PRs this week.", "@gdeer81 it looks like the test you added is failing: https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/7468/consoleFull (search for NNGradTest.LRN)", "@skye looks like the failure was due to test timeout that I didn't experience in my local environment.\r\nWhen I changed the test data I got the test time from 171s to 12s on my laptop so now it shouldn't timeout during the jenkins test run", "Jenkins test this please.", "@gdeer81 can you check how long just your test takes to run on your machine? (You can do this via: `bazel test //tensorflow/cc:gradients_nn_grad_test --test_filter=*LRN`) On my machine, all the current tests run in < 3 seconds. Assuming your test actually adds 9 seconds to the total test time, I think it'd be good to shrink it even more.", "@skye here is the output when I do `bazel test //tensorflow/cc:gradients_nn_grad_test --test_filter=*LRN`\r\n![nn_grad_test](https://user-images.githubusercontent.com/1340575/32812358-34bd3b1a-c96a-11e7-87b8-7e15ab9da4be.png)\r\n\r\nI might have been looking at the wrong value when I said it takes 12 seconds so I'll just let the screenshot speak for itself and hopefully clear up any confusion\r\n", "Ah ok. I'm honestly not sure what all this means either but looks good to me :) FYI I would do something like: `time ./bazel-bin/tensorflow/cc/gradients_nn_grad_test`\r\n\r\nJenkins test this please.", "excellent, all checks have passed", "@jhseu can you merge? I don't have permission apparently.", "@skye or @jhseu is there anything else that I need to do? Those Internal CI build failures don't look like they have anything to do with the changes I made "]}, {"number": 13986, "title": "tensorflow building graph very slow \u2014 for more than two hours when looping", "body": "### Describe the problem\r\nTensorflow is really slow when using loops. A very simple code can even last for hours and this is strange.\r\nCan anyone help me with issue?\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\n# define a guassian function so that it picks a value of 1 when the required value is met and otherwise a small neglegible value\r\ndef gaussian(x_box, x_tensor):\r\n    x_box=tf.convert_to_tensor(x_box,dtype=tf.float32)\r\n    return tf.exp(tf.divide(tf.square(x_box-x_tensor),-0.02))\r\n\r\na=time.time()\r\n# the input tensor with shape=[1,1000]\r\nlength = 1000\r\ninput_= tf.placeholder(dtype= tf.float32, shape= [1, length])\r\n\r\n\r\n# the box length is 100\r\nbox=np.zeros(100, dtype='O')\r\n#zero_tensor=tf.convert_to_tensor(0,dtype=tf.float32)\r\n#box[:]=zero_tensor\r\n\r\n\r\n# in order to put the element in the correct box, I use a gaussian function, so that when the element\r\n# in input_ is  near the box , it contributes a large number\r\nfor i in range(length):\r\n    for j in range(len(box)):\r\n        box[j] += gaussian(j, input_[0][i])\r\nb=time.time()\r\nprint(b-a)\r\n\r\n##########\r\n# really slow when length=1000 and box len 100, but if I set length =10 and box len also 10\r\n# it get the correct ansewr and faster as expected\r\n# for example\r\ninput_value = [[0,0,0,0,0,1,1,1,1,1]]\r\nwith tf.Session() as sess:\r\n    for i in range(len(box)):\r\n        box[i] = int(sess.run(box[i], {input_: input_value}))\r\nprint(box)\r\n\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@jacobisdsa Have you solved the slow problem?"]}, {"number": 13985, "title": "Update adding_an_op.md", "body": "I am proposing several changes to this file.  I am a new user to TF, and used this documentation extensively while trying to put a together my custom activation function, since I wanted to explore how different activation functions might affect NN training and performace.  I eventually figured out everything I needed to know, but it took weeks because this documentation is lacking in several areas, so I have a good feel for how this documentation can be improved to make life easier for future new users.  Changes 1 and 2 are pretty clear and I expect to not be controversial.  Change 3 you will probably need to think about, and might want to do further work to make things more clear.\r\n\r\n1) correct example.cc (221-228):\r\nI added a REGISTER_OP macro to the \"example\" code.  The previous absence of these lines was clearly an omission in the documentaion - the example code does not work without this line.  This is the least controversial change I am making.\r\n\r\n2) explain how to use Bazel to compile a CUDA op (1256-1272:\r\nI eventually figured out how to do this on my own, but it sure would have been useful had this information been in this documentation.\r\n\r\n3) explaining how the gradient actually works in TF (1293-1301):\r\nNow that I know the answer, I can read the documentation and sort of parse what it means, but I found this extremely confusing when I was reading it for the first (and tenth) time.  I have a masters in physics, so I understand calculus, but it wasn't until I had implemented backpropagation twice by hand and spent a bunch of time trying to imagine how TF must be working under the hood before I got this.  So I was tempted to delete this entire section and start over.  But I am not sure I can win this battle, because a major portion of what makes this confusing for me is the way TF uses the word gradient.  The thing that all of the TF documentation refers to as \"gradient\" I would much prefer to refer to as \"error\".  But I doubt I am going to get everybody to change everything.\r\n\r\nSo I have opted to just append another paragraph to this section, which I hope will explain things better.  It might be nice to update the mathematical notation a little bit, but since I am currently making these edits in a simple text editor I don't want to wade too deeply into trying to figure out the notation.\r\n\r\n4) Putting it all together (1467-1778):\r\nI was able to find pieces of example code while googling, but somehow I wasn't really able to find a complete tutorial that documented all of the steps start to finish of designing and using a custom activation function.  So I think it might be useful to post a complete example here which is just complicated enough to demonstrate all the parts needed to write a custom activation function working together.  But you might decide that this belongs elsewhere.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@MarkDaoust can you take a look? Some significant new tutorial content.", "Any update on my change request? Marking as stalled for now.", "Sorry about the delay.  I hesitated when you first requested this change, since I didn't really know what to do, and kind of forgot this until you reminded me.\r\n\r\nCode files:  I am not quite sure what you want me to do, since I am new to open source collaboration, but I my guess is that you want me to create three files with the smoothReLu kernel source code hosted somewhere on github and include links to those files in the md.  But if this is correct, where is the correct place to host the files?  Should they be stored somewhere within the TensorFlow project directory or should I host them on my personal github account.\r\n\r\nTesting:  I did some testing, though perhaps not enough, but I don't really know what standards you hold for how much testing is sufficient.  I did test the files to make sure they compile and load in python properly.  I believe I tested the activation function in an actual neural network, which isn't sufficient to prove that it is working completely as desired, but does demonstrate that it is at least most of the way there.  I don't believe I actually tested that the function returns the correct values, and I didn't try to test any edge cases / behavior with invalid inputs.", "@martinwicke could you elaborate on the requested changes? It looks like @ecpoppenheimer needs more guidance.", "Hi @ecpoppenheimer\r\n\r\n> where is the correct place to host the files?\r\n\r\nThis directory:\r\n\r\nhttps://github.com/tensorflow/models/tree/master/samples/core\r\n\r\nMake a new sub-directory called `extend/` and put the files there. (We're trying to keep examples out of the main repo going forward.)\r\n\r\nAssign that PR to me after you create it.\r\n\r\n> I did some testing, though perhaps not enough.\r\n\r\nThe main thing we'd want here is a small test script that runs it a couple of times, so we know if it breaks. Something like [this](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_input_test.py), [or this](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader_test.py).\r\n\r\nI'm not sure about the best way to setup the test to compile it, we'll ask the infrastructure people, but `subprocess.check_call` might be enough, we'll figure that out in the other PR.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@ecpoppenheimer Any progress on this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 112 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 13984, "title": "Issues importing external tf_library from project ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14\r\n- **TensorFlow installed from (source or binary)**: Source (bazel)\r\n- **TensorFlow version (use command below)**:  1.2\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**: \r\nbazel version\r\nBuild label: 0.7.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Oct 18 14:27:19 2017 (1508336839)\r\nBuild timestamp: 1508336839\r\nBuild timestamp as int: 1508336839\r\n\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n2x1080ti\r\n- **Exact command to reproduce**:\r\n\r\n\r\nI import tensorflow with Bazel into my workspace with:\r\n```\r\n#WORKSPACE\r\npatched_http_archive(\r\n    name = \"org_tensorflow\",\r\n    urls = [\r\n        \"https://github.com/tensorflow/tensorflow/archive/v1.2.0.tar.gz\",\r\n    ],\r\n    sha256 = \"03dbf7548d1fc1c11ed58da5fa68616f795c819f868f43478cbcaa26abed374f\",\r\n    strip_prefix = \"tensorflow-1.2.0\",\r\n    patch_file = \"//tools/third_party:tensorflow_bazel054.patch\",\r\n)\r\n\r\nload('@org_tensorflow//tensorflow:workspace.bzl', 'tf_workspace')\r\n\r\ntf_workspace(path_prefix = \"\", tf_repo_name = \"org_tensorflow\")\r\n```\r\n\r\nI am able to correctly reference the tf_library from my BUILD file but it fails to resolve the //tensorflow/... targets because its relative to MY workspace which expects tensorflow targets to be under @org_tensorflow//tensorflow/...\r\n```\r\n\r\nload(\"@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(...)\r\n```\r\n\r\nGet error:\r\n` no such package 'tensorflow/compiler/tf2xla/kernels': BUILD file not found on package path and referenced by '//my/local/bazel/target:test_graph_tfmatmul'\r\n`\r\nSee the output of my bazel query:\r\n```\r\ncc_library(\r\n  name = \"test_graph_tfmatmul\",\r\n  generator_name = \"test_graph_tfmatmul\",\r\n  generator_function = \"tf_library\",\r\n  generator_location = \"ros/src/visual_orientation/bench/BUILD:13\",\r\n  deps = [\"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\", \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\", \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\", \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\", \"//tensorflow/compiler/aot:runtime\", \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\", \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\", \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\", \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\", \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\", \"//tensorflow/compiler/xla:executable_run_options\", \"//third_party/eigen3:eigen3\", \"//tensorflow/core:framework_lite\"],\r\n  srcs = [\"//ros/src/visual_orientation/bench:test_graph_tfmatmul.o\"],\r\n  hdrs = [\"//ros/src/visual_orientation/bench:test_graph_tfmatmul.h\"],\r\n)\r\n```\r\nThe deps reference //tensorflow/... which I do not have in my WORKSPACE. \r\nbazel build @org_tensorflow//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32 succeeds\r\nbazel build //tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32 fails\r\n\r\nCan the tf_library reference targets more inteligently or am I making a mistake? Example targets:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L313\r\n\r\n", "comments": ["I think TensorFlow needs to use labels that use relative_to_caller_repository=True\r\nhttps://docs.bazel.build/versions/master/skylark/lib/Label.html#Label", "I somewhat managed to solve this problem by forking tensorflow and applying this patch:\r\n```\r\ndiff -r tensorflow-1.2.0/tensorflow/compiler/aot/tfcompile.bzl tensorflow-1.2.0_original/tensorflow-1.2.0/tensorflow/compiler/aot/tfcompile.bzl\r\n7c7\r\n< load(\"@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n---\r\n> load(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n19c19\r\n< load(\"@org_tensorflow//tensorflow:tensorflow.bzl\", \"if_android\", \"tf_copts\")\r\n---\r\n> load(\"//tensorflow:tensorflow.bzl\", \"if_android\", \"tf_copts\")\r\n26c26\r\n<                tfcompile_tool=\"@org_tensorflow//tensorflow/compiler/aot:tfcompile\",\r\n---\r\n>                tfcompile_tool=\"//tensorflow/compiler/aot:tfcompile\",\r\n110c110\r\n<         cmd=(\"$(location @org_tensorflow//tensorflow/python/tools:freeze_graph)\" +\r\n---\r\n>         cmd=(\"$(location //tensorflow/python/tools:freeze_graph)\" +\r\n112c112\r\n<         tools=[\"@org_tensorflow//tensorflow/python/tools:freeze_graph\"],\r\n---\r\n>         tools=[\"//tensorflow/python/tools:freeze_graph\"],\r\n166,176c166,176\r\n<           \"@org_tensorflow//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\r\n<           \"@org_tensorflow//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\r\n<           \"@org_tensorflow//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\r\n<           \"@org_tensorflow//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\r\n<           \"@org_tensorflow//tensorflow/compiler/aot:runtime\",\r\n<           \"@org_tensorflow//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n<           \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n<           \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n<           \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n<           \"@org_tensorflow//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n<           \"@org_tensorflow//tensorflow/compiler/xla:executable_run_options\",\r\n---\r\n>           \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int32\",\r\n>           \"//tensorflow/compiler/tf2xla/kernels:gather_op_kernel_float_int64\",\r\n>           \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_1d\",\r\n>           \"//tensorflow/compiler/tf2xla/kernels:index_ops_kernel_argmax_float_2d\",\r\n>           \"//tensorflow/compiler/aot:runtime\",\r\n>           \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n>           \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n>           \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n>           \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n>           \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n>           \"//tensorflow/compiler/xla:executable_run_options\",\r\n178c178\r\n<           \"@org_tensorflow//tensorflow/core:framework_lite\",\r\n---\r\n>           \"//tensorflow/core:framework_lite\",\r\n203c203\r\n<             \"@org_tensorflow//tensorflow/compiler/aot:test.cc\",\r\n---\r\n>             \"//tensorflow/compiler/aot:test.cc\",\r\n208c208\r\n<              \" $(location @org_tensorflow//tensorflow/compiler/aot:test.cc) \" +\r\n---\r\n>              \" $(location //tensorflow/compiler/aot:test.cc) \" +\r\n219,222c219,222\r\n<             \"@org_tensorflow//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n<             \"@org_tensorflow//tensorflow/compiler/aot:runtime\",\r\n<             \"@org_tensorflow//tensorflow/compiler/aot:tf_library_test_main\",\r\n<             \"@org_tensorflow//tensorflow/compiler/xla:executable_run_options\",\r\n---\r\n>             \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n>             \"//tensorflow/compiler/aot:runtime\",\r\n>             \"//tensorflow/compiler/aot:tf_library_test_main\",\r\n>             \"//tensorflow/compiler/xla:executable_run_options\",\r\n224,225c224,225\r\n<             \"@org_tensorflow//tensorflow/core:lib\",\r\n<             \"@org_tensorflow//tensorflow/core:test\",\r\n---\r\n>             \"//tensorflow/core:lib\",\r\n>             \"//tensorflow/core:test\",\r\n233c233\r\n<     benchmark_main = (\"@org_tensorflow//tensorflow/compiler/aot:\" +\r\n---\r\n>     benchmark_main = (\"//tensorflow/compiler/aot:\" +\r\n265,268c265,268\r\n<             \"@org_tensorflow//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n<             \"@org_tensorflow//tensorflow/compiler/aot:benchmark\",\r\n<             \"@org_tensorflow//tensorflow/compiler/aot:runtime\",\r\n<             \"@org_tensorflow//tensorflow/compiler/xla:executable_run_options\",\r\n---\r\n>             \"//tensorflow/compiler/tf2xla:xla_local_runtime_context\",\r\n>             \"//tensorflow/compiler/aot:benchmark\",\r\n>             \"//tensorflow/compiler/aot:runtime\",\r\n>             \"//tensorflow/compiler/xla:executable_run_options\",\r\n271c271\r\n<             \"@org_tensorflow//tensorflow/compiler/aot:benchmark_extra_android\",\r\n---\r\n>             \"//tensorflow/compiler/aot:benchmark_extra_android\",\r\n282,285c282,285\r\n<       \"@org_tensorflow//tensorflow:android_arm\": \"armv7-none-android\",\r\n<       \"@org_tensorflow//tensorflow:android_arm64\": \"aarch64-none-android\",\r\n<       \"@org_tensorflow//tensorflow:android_x86\": \"i686-none-android\",\r\n<       \"@org_tensorflow//tensorflow:linux_ppc64le\": \"ppc64le-ibm-linux-gnu\",\r\n---\r\n>       \"//tensorflow:android_arm\": \"armv7-none-android\",\r\n>       \"//tensorflow:android_arm64\": \"aarch64-none-android\",\r\n>       \"//tensorflow:android_x86\": \"i686-none-android\",\r\n>       \"//tensorflow:linux_ppc64le\": \"ppc64le-ibm-linux-gnu\",\r\n\r\n```\r\n\r\n```\r\ndiff -r tensorflow-1.2.0/third_party/llvm/llvm.bzl tensorflow-1.2.0_original/tensorflow-1.2.0/third_party/llvm/llvm.bzl\r\n98c98\r\n<   expand_cmake_vars_tool = \"@org_tensorflow//third_party/llvm:expand_cmake_vars\"\r\n---\r\n>   expand_cmake_vars_tool = \"@//third_party/llvm:expand_cmake_vars\"\r\n```\r\n\r\nIs this something Tensorflow would accept as a patch? There is also @%ws% but it only seems to work in BUILD files not bzl files....", "@gunan can you comment or redirect? Thanks!", "I think to avoid such issues, we need to make our skylark files refer to anything in tensorflow as `@org_tensorflow//tensorflow/...`. Initially we were not able to, because bazel 0.4.x had some bugs preventing us to use this way to refer to our own libraries.\r\n@tadeegan Would you like to create a pull request for the bzl file you are having a problem with?\r\nPlease mention me in your pull request, and I can review your change.", "I think the @org_tensorflow prefix I used is WORKSPACE specific. I could give the tensorflow project any name I want.\r\n\r\nSeems like absolute targets //tensorflow resolve relative to the external repository as you would expect within BUILD files. However, when I load a .bzl file, its essentially inlines that into my own BUILD file within my repository so its referring to //tensorflow from within my repository which doesn't exist. I'm not sure what the best way to accomplish this is with Bazel unless you can force that @org_tensorflow is always the repository name people have to use..", "You can, but because of this line:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/WORKSPACE#L1\r\n`org_tensorflow` in tensorflow repo should always resolve to tensorflow repo.", "Okay. I will make a pull request.  Is there anyway we can back port it to one of the stable released versions (1.3)?", "1.4 is possible if we make the change these days, but previous versions we probably will not update.\r\n@case540 This will be a small change that wont affect binaries, once the PR is merged, we can cherrypick this into the release without an extra release candidate, if the merge happens before the final 1.4 release.", "Acknowledged. If this merges before the 1.4 release I'll cherrypick it.", "Reverted by baf490ba79acaacb458078370e4bad1c3fd17563 reintroducing error. Request reopen.", "Nagging Assignee @gunan: It has been 195 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think this has been resolved in more recent versions with the `clean_dep` skylark macro."]}, {"number": 13983, "title": "gradient not working with aggregation on TensorArray in tf.while_loop", "body": "### Problem description\r\nI am trying to compute gradient of an aggregation on the currently available elements in `tf.TensorArray` in a `tf.while_loop`, but got an `InvalidArgumentError: TensorArray TensorArray_4_21@while_63/gradients: Could not write to TensorArray index 0 because it has already been read.`\r\n\r\n### Minimum code to reproduce the error\r\n```python\r\ndef make_loop_test():\r\n\r\n    def _cond(i, *_):\r\n        return i < 3\r\n\r\n    def _body(i, var, var_hist):\r\n        # write current element\r\n        var_hist = var_hist.write(i, var)\r\n\r\n        # retrieve all current previous elements as well as the one appended just now, and compute the sum\r\n        util = tf.reduce_sum(var_hist.gather(tf.range(0, i+1))) * 2.0 + 1.0\r\n\r\n        # take gradient, where I think the problem comes from\r\n        grad = tf.gradients(util, [var])[0]\r\n\r\n        return i + 1, var - grad * 0.1, var_hist\r\n\r\n    _init_state = (0, 2e3, tf.TensorArray(dtype=tf.float32, size=3, clear_after_read=False))\r\n\r\n    loop_i, loop_var, loop_var_hist = tf.while_loop(_cond, _body, _init_state, parallel_iterations=1)\r\n\r\n    return loop_i, loop_var, loop_var_hist.stack()\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(make_loop_test()))\r\n```\r\n\r\nI used to add a bunch of tf.Print statements and found the error was coming from gradient statement in the second iteration. The error message looks weird to me since I am not writing to index 0 at that time.\r\n\r\n\r\n### Complete logs\r\n```\r\nFile \"<ipython-input-132-3da3dc3dd8f4>\", line 18, in <module>\r\n  print(sess.run(make_loop_test()))\r\nFile \"<ipython-input-132-3da3dc3dd8f4>\", line 13, in make_loop_test\r\n  loop_i, loop_var, loop_var_hist = tf.while_loop(_cond, _body, _init_state, parallel_iterations=1)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2775, in while_loop\r\n  result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2604, in BuildLoop\r\n  pred, body, original_loop_vars, loop_vars, shape_invariants)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2554, in _BuildLoop\r\n  body_result = body(*packed_vars_for_body)\r\nFile \"<ipython-input-132-3da3dc3dd8f4>\", line 8, in _body\r\n  grad = tf.gradients(util, [var])[0]\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in gradients\r\n  grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 348, in _MaybeCompile\r\n  return grad_fn()  # Exit early\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 542, in <lambda>\r\n  grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 162, in _TensorArrayGatherGrad\r\n  u_g = g.scatter(indices, grad)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py\", line 175, in wrapped\r\n  return _add_should_use_warning(fn(*args, **kwargs))\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 441, in scatter\r\n  name=name)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 2649, in _tensor_array_scatter_v3\r\n  name=name)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n  op_def=op_def)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n  original_op=self._default_original_op, op_def=op_def)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n  self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n...which was originally created as op 'while_62/TensorArrayGatherV3', defined at:\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\r\n  \"__main__\", mod_spec)\r\n[elided 23 identical lines from previous traceback]\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2554, in _BuildLoop\r\n  body_result = body(*packed_vars_for_body)\r\nFile \"<ipython-input-132-3da3dc3dd8f4>\", line 7, in _body\r\n  util = tf.reduce_sum(var_hist.gather(tf.range(0, i+1))) * 2.0 + 1.0\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py\", line 93, in fn\r\n  return method(self, *args, **kwargs)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 360, in gather\r\n  element_shape=element_shape)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 2401, in _tensor_array_gather_v3\r\n  element_shape=element_shape, name=name)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n  op_def=op_def)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n  original_op=self._default_original_op, op_def=op_def)\r\nFile \"/home/ecsark/envs/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n  self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\nInvalidArgumentError (see above for traceback): TensorArray TensorArray_3_20@while_62/gradients: Could not write to TensorArray index 0 because it has already been read.\r\n [[Node: while_62/gradients/while_62/TensorArrayGatherV3_grad/TensorArrayScatter/TensorArrayScatterV3 = TensorArrayScatterV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](while_62/gradients/while_62/TensorArrayGatherV3_grad/TensorArrayGrad/TensorArrayGradV3, while_62/range, while_62/gradients/while_62/Sum_grad/Tile, while_62/gradients/while_62/TensorArrayGatherV3_grad/TensorArrayGrad/gradient_flow)]]\r\n```\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 7.11 (wheezy)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.1-0-g48c54ee 1.3.1 \r\n- **Python version**:  3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "This seems like a basic corner case bug.  Would you like to submit a PR to fix this behavior?", "@ecsark ?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 60 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 13982, "title": "Feature Request: C++ gradient for Floor", "body": "Implement the gradient for Floor in c++ so that it is available for TF_AddGradients\r\n\r\n@suharshs I will implement this one with some guidance from @bpiel \r\n\r\nI believe this is the python code I'll be porting over:\r\nhttps://github.com/tensorflow/tensorflow/blob/962ed613cf1087637848d3e2b23f5b01d93c7eda/tensorflow/python/ops/math_grad.py#L1004-L1006\r\n", "comments": ["cc @skye ", "the python implementation uses None to indicate \"no connection\" but I'm not sure that returning null in c++ will have the same effect.  \r\n\r\nSo I have three options: return 0, return the input gradient, or return a zero matrix. \r\n\r\nAlso, there is no test for the python version, so I'm really lost on how to test this one. \r\n\r\n", "Use NoGradient() (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/framework/gradients.h#L48). For example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/gradients/nn_grad.cc#L193\r\n\r\nAs for testing, look at cc/gradients/math_grad_test.cc and see if you can add a test case to CWiseUnaryGradTest for Floor. If you still need help post back here (I haven't looked at these tests myself so I can't offer guidance off the top of my head, but it looks like there are plenty of examples).", "@gdeer81 Are you working on that ?", "@theflofly I know he was, but I haven't heard from him (on slack) since he posted that last comment (Nov 6).", "Thanks @bpiel, is this a slack related to TF developments ?", "@theflofly The slack I was referring to above is the tensorflow channel of the clojure slack (https://clojurians.slack.com/), but since you asked, I've also made a channel in a TF-focused slack for non-python TF collaboration. It doesn't see a lot of action, but if you're interested, please email me. I don't want to hijack this issue.", "@theflofly I haven't made much progress with this one if you're looking for a feature request that's probably in your wheelhouse.", "@gdeer81 you should do it, it is easy enough to get you started. What is your problem ?\r\n\r\nSomething like this should work:\r\n```\r\nStatus FloorGrad(const Scope& scope, const Operation& op,\r\n               const std::vector<Output>& grad_inputs,\r\n               std::vector<Output>* grad_outputs) {\r\n  grad_outputs->push_back(NoGradient());\r\n  return scope.status();\r\n}\r\nREGISTER_GRADIENT_OP(\"Floor\", FloorGrad);\r\n```", "@theflofly that's exactly what I have. =D\r\nI was struggling to write the test. \r\nSomeone said to look at your PR for ProdGrad and it was kind of intimidating so I was worried that I left something out.\r\n\r\nI'll push what I have tonight let everyone see what I've done. Sunlight is the best disinfectant.\r\n\r\n", "This is what I have so far:\r\nhttps://github.com/gdeer81/tensorflow/commit/5275fc4dfd584ea3e95ea5227259c4e9d36d2f13\r\n\r\nIt took a bit of fiddling to get the test to go from erroring out to just plain failing. Using the x_fn variable like the other tests do just caused a seg fault so I commented it out and just started plugging in random values for the second arg to the test method. The only acceptable value so far has been 0.\r\n\r\nBut the test output doesn't say why the test failed so I'm kind of at a loss for how to write a proper test for this one despite all the examples  whose test inputs look arbitrary and don't really provide any helpful information to anyone who is trying to write new tests.\r\n", "@gdeer81 I'll check and come back to you.", "@theflofly any thoughts on how to test my changes?", "@gdeer81 Sorry for the delay. \r\n\r\nAs a NoGradient cut the flow of the gradient I don't see what we can test. I mean if you do something like that:\r\n```\r\nauto y = Square(Floor(Matmul(x1, X2)))\r\n```\r\nAnd you try to differentiate y w.r.t x1 or x2, it will just do nothing (no error, no result) whereas in Python there is:  `No gradients provided for any variable, check your graph for ops that do not support gradients, between variables...`. I think that the warning should be also added, else it will be very difficult to debug.\r\n\r\nAlso I think that:\r\n```\r\nREGISTER_NO_GRADIENT_OP(\"Floor\");\r\n```\r\nis enough, no need for the code that I previously pasted, NoGradientOp are already defined like that for LogicalNot, LogicalOr, ... and they aren't tested either.\r\n\r\nIn summary, I'd say add only `REGISTER_NO_GRADIENT_OP(\"Floor\");` and we should test if the variables are reachable taking into account NoGradient links. As I worked on reachable nodes I can eventually do it, what do you think @skye ?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@skye Can you check my last comment please ?", "Sorry for the delay, I missed this and then went on vacation.\r\n\r\n@theflofly I think you're right and we can just use REGISTER_NO_GRADIENT_OP, sorry for not catching that earlier! And no need to test in that case, we should already have test coverage for gradientless ops.", "@skye No problem. Shouldn't we add a check if the graph is not cut by a NoGradient op in order to display a helpful message ?", "Ah, I think I didn't fully parse your previous comment. Are you suggesting we should add a similar \"No gradients provided for any variable\" error to the C++ gradient code? If so I disagree, this error is actually generated by Optimizer, not the low-level gradients code.", "@skye Sorry for the delay. I agree with you, we'll add that when the optimizers will be available in C++.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gdeer81: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gdeer81: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gdeer81: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gdeer81: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@theflofly @skye how time flies, I forgot I still had this issue open.\r\n\r\nSo the consensus we came to was that I can just put \r\n\r\n``` \r\nREGISTER_NO_GRADIENT_OP(\"Floor\");\r\n```\r\n\r\nand call it a day?", "I think so.", "I think so as well.", "Nagging Assignee @gdeer81: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @gdeer81: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I created a pull request https://github.com/tensorflow/tensorflow/pull/19662 so that we can close this issue.", "I think if you add \"Fixes #13982\" to the commit description, this issue should be automatically closed when the PR is merged."]}, {"number": 13981, "title": "Changed GPU driver version assumption", "body": "Fixes #9669", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.\r\n"]}, {"number": 13980, "title": "NadamOptimizer throws InvalidArgumentError (incompatible shapes)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, but this issue can be replicated by changing one line in the example script `word2vec_basic.py`\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux CentOS 7, MacOS High Sierra \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.4.0-rc0-21-g1e25994', '1.4.0-rc1'). I also know someone who encountered this in the 1.3 release.\r\n- **Python version**: 2.7.10, 2.7.14\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: Titan X 12gb\r\n- **Exact command to reproduce**:\r\nChange line 190 in `tensorflow/examples/tutorials/word2vec/word2vec_basic.py` to:\r\n`optimizer = tf.contrib.opt.NadamOptimizer(1.0).minimize(loss)`. \r\nRun.\r\n\r\n### Describe the problem\r\nWhen I attempt to train a model with `tf.controb.opt.NadamOptimizer`, TensorFlow crashes with an InvalidArgumentError (Incompatible shapes: [105] vs. [50000]), which is thrown from the optimizer (in my code it's when trying to apply updates to the word embedding table.) It looks like it's trying to apply sparse updates to the embeddings using dense operations, causing a shape mismatch, or something along those lines. `AdamOptimizer` and `LazyAdamOptimizer` work fine.\r\n\r\n### Source code / logs\r\nTraceback can be found [here](https://gist.github.com/strubell/37897084888252989750b58260f76ff5).\r\nThe error is easily replicable in the example script `word2vec_basic.py`.\r\n", "comments": ["@alisidd can you look into this?", "@skye sorry for the delayed response. Yes, I can look into it.", "Nagging Assignee @rohan100jain: It has been 276 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 291 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Seems like there is a PR that should fix this issue. Lets wait for that to go in.", "@strubell For `tf.contrib.opt.NadamOptimizer` you would need to modify more code base to make it function.  In this case you can use `optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)`, which should work and serve the similar purpose. "]}, {"number": 13979, "title": "Update docs for `begin_params_axis`", "body": "This fix fixes the issue raised in #13975 where `begin_shift_axis` should actually be `begin_params_axis`.\r\n\r\nThis fix fixes #13975.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The CLA bot isn't looking at this PR for some reason. I'll try to force merge it.", "Jenkins, test this please.", "gunan: The CLA bot didn't run and prevents the merge of this PR. Can you handle this ?", "@yongtang Could you try commenting with this phrase:\r\n`I signed it!`", "I signed it!", "The failures look unrelated. Merging"]}, {"number": 13978, "title": "Initial SRU Implementation", "body": "Hi, as per https://github.com/tensorflow/tensorflow/issues/13094, I'm submitting an initial PR for SRU implementation. I have checked the results with implementation [here](https://github.com/desire2020/SRU-tensorflow).\r\n\r\nI intend to implement SRUBlock as well; but I have been distracted away by other high priority things until recently. So I will incrementally add the SRUBlock here as well.\r\n\r\nIn addition, it occurs to me that SRU only allows num_units equal to input_size. Can someone verify my interpretation? Otherwise we cannot perform equaiton 7 in the [paper](https://arxiv.org/pdf/1709.02755.pdf) because of last-dimension size mismatch.", "comments": ["Can one of the admins verify this patch?", "Just a reminder, quote from my original PR comment:\r\n\r\n> In addition, it occurs to me that SRU only allows num_units equal to input_size. Can someone verify my interpretation? Otherwise we cannot perform equaiton 7 in the paper because of last-dimension size mismatch.\r\n", "@ebrevdo any update?", "@ebrevdo I've updated my code according to your comments.", "@ebrevdo any updates?", "@ebrevdo any update?", "@tjingrant quick ping on this.", "@ebrevdo should be good now.", "Jenkins, test this please", "Thanks!\n\nOn Sat, Dec 16, 2017, 5:01 PM Benoit Steiner <notifications@github.com>\nwrote:\n\n> Merged #13978 <https://github.com/tensorflow/tensorflow/pull/13978>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13978#event-1390926551>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzytgw56Fzet3IEiWAUvQMnBDwAPks5tBGfRgaJpZM4QGdlg>\n> .\n>\n", "This PR introduces an API change. We will need to revert this and resubmit after an API review.", "@tjingrant : Could you add some more detail to the docstring, the current one liner seems a bit ... succinct :)", "@asimshankar yes, of course. But let's talk about it over here (https://github.com/tensorflow/tensorflow/pull/15436). ", "let's revert this and resubmit with the SRUCell going into tf.contrib.rnn (not just the test!)", "about this function 'tf.contrib.rnn.SRUCell(units)'   \r\nHow many states does the return value of this function call directly? I found it in debugging and it seems that there is only one state.\r\nWhat should I do if I want to stack several layers of SRU?Because I don't understand that this function has only one return value. How does the internal state of c work when stacked on the SRU layers?"]}, {"number": 13977, "title": "Feature request: adding spectral functions for preserving phase information", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (attempting to)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA v8.0.60, cuDNN 6.0\r\n- **GPU model and memory**: GTX 1060 6 GB\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nI am working on a data processing pipeline directly in my tensorflow graph. In that case, I would like to use continuous wavelet transforms (CWT) in order to transform time-series data into scalograms instead of spectrograms in order to increase time-frequency resolution. I can only find a single API for performing the CWT that is compatible directly with TF, but unfortunately, it is not 'enough' since the tf.conv2 operator does not work with inputs of different types (real, complex). I therefore am trying to fix my own op using TF's standard ops, but have hit a snag. The spectral ops rfft and irfft only return and accept positive frequency components, and for my purpose, I would have to apply phase transformations of the spectrum, resulting in non-Hermitian symmetry (in which case I cannot use the tf.spectral.irfft to inverse transform!). So what I request, are spectral ops that return and accept the full complex spectra of input signals. \r\n\r\n### Source code / logs\r\nN/A", "comments": ["@rmlarsen are you the right person to look at this feature request?", "Hi @doktorneergaard, \r\n\r\nI think [`tf.spectral.fft`](https://www.tensorflow.org/api_docs/python/tf/fft) and [`tf.spectral.ifft`](https://www.tensorflow.org/api_docs/python/tf/ifft) are what you're looking for. They return the full FFT, not the \"real\" FFT.", "By the way, I think many people would love to have the CWT available in TensorFlow (perhaps as part of `tf.spectral` or `tf.contrib.signal`), or at least as a library. Please do publish your code once you get it working! ", "@rryan I think that was exactly what I needed, thank you! Can't believe I overlooked `tf.spectral.fft`.... -_-\r\n\r\nI will def put it out once I finish it!", "@doktorneergaard great to hear, thanks!", "@rryan How can I add a fix fft length to the function tf.spectral.ifft and tf.spectral.fft?", "@pachpandepriti I'm afraid you'll have to slice the input to `fft`/`ifft` like so:\r\n```python\r\nfft_size = 256\r\n# A [batch_size, num_frames, frame_length] signal. \r\nx = tf.random_normal([1, 24000, 512])\r\n# Take the 1D DFT of the inner dimension with a 256-point FFT. \r\n# Without the slice, a 512-point FFT would be taken, since the inner dimension is 512.\r\ns = tf.spectral.fft(x[:, :, :fft_size])\r\n```\r\n\r\nAdding an `fft_length` parameter to `tf.spectral.fft` and `tf.spectral.ifft` has been on my TODO list for a while, though. ", "@rryan can u guide me how to implement fftshift(as in matlab) on a tensor? I need to shuffle the elements in a tensor and then reshuffle it", "> @rryan can u guide me how to implement fftshift(as in matlab) on a tensor? I need to shuffle the elements in a tensor and then reshuffle it\r\n\r\nSure -- fftshift circular-shifts the given axis by `size // 2` (we can confirm this from [numpy's implementation](https://github.com/numpy/numpy/blob/v1.15.4/numpy/fft/helper.py#L22-L75)). \r\n\r\nHere's an example that uses `split` and `concat`. It should work for tensors whose rank is unknown at graph construction time, and it should match NumPy's handling of odd-length axes:\r\n```python\r\ndef fftshift(spectrum, axis=-1):\r\n  try: \r\n    shape = spectrum.shape[axis].value\r\n  except:\r\n    shape = None\r\n  if shape is None:\r\n    shape = tf.shape(spectrum)[axis]\r\n  # Match NumPy's behavior for odd-length input. The number of items to roll is\r\n  # truncated downwards.\r\n  b_size = shape // 2\r\n  a_size = shape - b_size\r\n  a, b = tf.split(spectrum, [a_size, b_size], axis=axis)\r\n  return tf.concat([b, a], axis=axis)\r\n```\r\nI will add this to `tf.signal`.", "@rryan Hi I want to shuffle a tensor using lambda layer, but it is giving me an error :\r\norder = range(len(l));\r\nTypeError: object of type 'Tensor' has no len()\r\n\r\ndef shuffle_forward(l, axis=-1):\r\norder = range(len(l));\r\nrandom.shuffle(order)\r\nreturn list(np.array(l)[order]), order\r\n\r\ndef shuffle_backward(l, order):\r\nl_out = [0] * len(l)\r\nfor i, j in enumerate(order):\r\nl_out[j] = l[i]\r\nreturn l_out\r\ncan u guide me what am I doing wrong", "Unfortunately Tensor objects can't be treated like NumPy arrays in the way you are using them.\r\n\r\nBeyond the len issue that you are already seeing (you can use `tf.shape(l)[axis]` for that), Tensors also don't support advanced indexing that you are using in your example above.\r\n\r\nIt will not be most efficient but you can unstack on the desired axis, permute, then restack. Something like this (I haven't tested it):\r\n\r\n```python\r\ndef shuffle_forward(l, axis=-1):\r\n  order = range(l.shape[axis].value)\r\n  random.shuffle(order)\r\n  slices = tf.unstack(l, axis=axis)\r\n  return tf.stack([slices[i] for i in order], axis=axis), order\r\n\r\ndef shuffle_backward(l, order, axis=-1):\r\n  slices = tf.unstack(l, axis=axis)\r\n  out = [None] * len(order)\r\n  for i, j in enumerate(order):\r\n    out[j] = slices[i]\r\n  return tf.stack(out, axis=axis)\r\n```", "@rryan for tf.ifft function if I don't give the fft length will it take the batch_size as fft length when the function is used inside a model?", "> @rryan for tf.ifft function if I don't give the fft length will it take the batch_size as fft length when the function is used inside a model?\r\n\r\nIt will use the inner-most dimension as the FFT length (not the batch size, which is usually the outer-most dimension).", "@rryan Thanks for the help. Slicing the input  option worked for me.", "> Adding an `fft_length` parameter to `tf.spectral.fft` and `tf.spectral.ifft` has been on my TODO list for a while, though.\r\n\r\nAny chance of getting an axis parameter, like in numpy? (Maybe I should open a new issue for that feature request?)"]}, {"number": 13976, "title": "Branch 173415707", "body": "", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 13975, "title": "begin_shift_axis not defined in tf.contrib.layers.layer_norm", "body": "The live version of tensorflow's api docs here:\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/layers/layer_norm\r\n\r\nRefer to begin_shift_axis, which isn't defined.  I assume this is a typo and is meant to be begin_params_axis\r\n\r\nApologies if documentation bugs are supposed to be tracked elsewhere, it's not clear how those should be reported.", "comments": ["Thanks @techmatt. Yes I think it should be `begin_params_axis`. Created a PR #13979 for the fix.", "Marking contributions welcome while we wait for the PR: thanks!"]}, {"number": 13974, "title": "Update device string comments in node_def.proto", "body": "Clarifying comments for valid device string in NodeDef, as discussed in PR #13874.\r\n\r\nNotes:\r\n1. The device string is as returned by device.py to_string() \r\n2. parse_from_string() in device.py seems currently allows an empty string for device_type (line 167). This commit reflects that.  Is this actually intended behavior?\r\n3. I notice our regex convention does not use '+' (e.g. XX* instead of X+).", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 13973, "title": "import error :cudnnConvolutionBiasActivationForward because tensorflow cpu and tensorflow gpu, both were installed", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:Tensorflow_gpu_1.3.0\r\n- **Python version**: on both 2.7 and 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:CUDA 8/Cudnn 5\r\n- **GPU model and memory**:Nvidia GeForce GTX 1080 Ti, 11 GB\r\n- **Exact command to reproduce**:1)pip install tensorflow\r\n2)pip install tensorflow-gpu\r\n3)pin unistall tensorflow\r\n4)pip uninstall tensorflow-gpu\r\n5)pip install tensorflow-gpu\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI installed tensorflow cpu and tensorflow gpu by mistake .I uninstalled both the versions after  realising my mistake .I then got afresh install with tensorflow gpu.Then i get an import error as below\r\n`ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cudnnConvolutionBiasActivationForward`\r\n\r\nI tried restarting and force reinstall.but both did not work.I also tried with a virtual environment and failed.\r\nWhen i downgraded to tensorflow-gpu-1_2-1 it works.However i need 1.3\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@zishanahmed08  could you try manually remove the packages from `/usr/local/lib/python3.5/dist-packages`?", "@yifeif  I deleted the folders tensorflow and tensorflow_gpu-1.3.0.dist-info from /usr/local/lib/python3.5/dist-packages and  tensorflow-gpu. \r\nIt does not work.Import error still exists", "Hi, \r\nI also encountered the same error. Below is my setup in Google VM instance:\r\n\r\nGPU : Nvidia Tesla K80\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\nTensorFlow installed from (source or binary): sudo pip  install \u2014upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp27-none-linux_x86_64.whl\r\nTensorFlow version (use command below):Tensorflow_gpu_1.3.0\r\nPython version:  2.7 \r\nCUDA/cuDNN version:CUDA 8/Cudnn 5.1\r\n\r\n\r\nWould really appreciate some help here!!\r\n", "Ah the 1.3 binaries are built for cudnn 6. Could you guys try upgrade your cudnn to 6.0? ", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing this. Feel free to re-open if this is still an issue."]}, {"number": 13972, "title": "Fix typos", "body": "This PR fixes some typos: `prediciton`, `iteraton`.", "comments": ["Jenkins, test this please."]}, {"number": 13971, "title": " is there a way to restore and predict without doing \"dummy training\"  ", "body": "We are trying **text_classification.py** example of tensorflow  and separated the training and prediction parts of the code and trained the model using model_dir and tried to predict using the same model_dir.\r\n\r\nModel is being saved but when we comment the training code and try to predict something, we get the following error:\r\n\r\n**InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [10,50] rhs shape= [5386,50]\r\n         [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@EmbedSequence/embeddings\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](EmbedSequence/embeddings, save/RestoreV2)]]**\r\n\r\n**Where:** 5386 and 10 are words count in training and prediction data.\r\n\r\n\r\nBut when we uncomment the preparation of train_input_fn without using it anywhere in the code , without training the model but using the same model_dir then prediction works fine.\r\n\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={WORDS_FEATURE: x_train},\r\n        y=y_train,\r\n        batch_size=len(x_train),\r\n        num_epochs=None,\r\n        shuffle=True)\r\n\r\nWe are wondering what exactly is being done by this function which actually overcome the error without training the model (using the function at all) ?\r\n\r\nWe have gone through many posts but following post looks more relevant where someone said that prediction can't be done without doing at least some training (though the model was already trained and restored using model_dir ):  https://github.com/tensorflow/tensorflow/issues/3340\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Tried both Python 2.7.6 / Python 3.4.3**:\r\n- **TF Version :  1.3.0**:\r\n- **python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\" :  ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')**\r\n\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "As explained in the https://github.com/tensorflow/tensorflow/issues/3340  , it was a to be implemented feature as told by @martinwicke : \r\n\"Right now, there isn't a way to restore and predict without running at least one step of training (which is a missing feature).\"\r\n\r\nHas it been released ?\r\n\r\n", "Yes, tf.estimator.* does not have this restriction. ", "\r\n[text_classification.txt](https://github.com/tensorflow/tensorflow/files/1421562/text_classification.txt)\r\n\r\n\r\nWe are using TF 1.3.0 and  [text_classification.py]([url](url))  uses tf.estimator but we still face that issue.\r\nYou can reproduce the issue by running the attached code (convert .txt to .py).\r\n\r\n\r\nTo train without testing use (no error):\r\nSP_TRAIN1=1\r\nSP_TRAIN=1\r\nSP_TEST=0\r\n\r\nTo test without training use (you will get the error in this case):\r\nSP_TRAIN1=0\r\nSP_TRAIN=0\r\nSP_TEST=1\r\n\r\nTo test without training but uncomment training input function, use (you will NOT get the error in this case):\r\nSP_TRAIN1=1\r\nSP_TRAIN=0\r\nSP_TEST=1\r\n\r\n\r\n", "Thank you for the repro code!\r\n\r\n@ispirmustafa can you take a look? ", "I agree with @martinwicke that tf.estimator.* does not have this restriction (at least one step train before predict).\r\n\r\nIf I understand you correctly, a simple code below might work as you expected:\r\n```python\r\nif Train:\r\n    estimator.train(train_input_fn, xxx)\r\nelse:\r\n    estimator.predict(test_input_fn, xxx)\r\n```\r\n\r\nnote: Do not comment training code anywhere, because they might be used to reconstruct the graph.", ">> note: Do not comment training code anywhere, because they might be used to reconstruct the graph.\r\n\r\nThats exactly the point that if we are trying to predict on a trained model then why do we need the training code at all?\r\n\r\nBelow line of code should take care of everything related to reconstructing the graph etc in case of predicting so that we need not to use any training code during prediction:\r\nclassifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=MODEL_DIR)\r\n\r\n", "@ispirmustafa  could you please take a look at it? ", "is there any update on this yet ?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Hi @satendrapratap ,\r\nI think this issue is about VocabularyProcessor. if only SP_TEST is set, then there isn't any call to VocabularyProcessor.fit_transform. in SP_TEST you call VocabularyProcessor.transform. that creates a dependency to your train code.\r\nCould you please move VocabularyProcessor related logic outside of train and test functionality?", "Thats the whole point if we are doing testing then why do we need training code (as the model is already trained and we provided the path for all the files in tf.estimator.Estimator(model_fn=model_fn, model_dir=MODEL_DIR) ) . Please checkout comment \"satendrapratap commented on Oct 30 \u2022  edited \" above.", "Estimator does not need that. but the utility that you're using (VocabularyProcessor) is independent then Estimator. It doesn't save it's state to model-dir. it's in memory training. That's why you have the problem.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 13970, "title": "import tensorflow error: No Module Named '_pywrap_tensorflow_internal'", "body": "Dear All,\r\n\r\nI am new to the CNNs and Tensorflow. I wanted to start my journey from Keras.\r\nAt this moment I facing problems with onstalling Tensorflow.\r\nI am using Windows 10, I installed CUDA 9 with cuDNN 7, and was getting the following errors.\r\nThe I unstalled it and installed tensorflow-gpu, which automatically installs CUDA 8 with cuDNN 6 and I am still facing the same issue. I tried installing Microsoft Visual C++ 2015 Redistributable Update 3 but with no results. My Python version is 3.5.4 and I am using Anaconda 3.\r\nDo you have any ideas what else can I check?  \r\n\r\nHere is the error:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 985, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 968, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 957, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 938, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": [" could you please try running [this script](https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c) and see what it gives you and make sure you handle the issues it will point out to you?", "Thanks Carmezim for your tip. \r\nI have already run the script and its results are a bit confusing for me.\r\nThis is what I got:\r\n\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.5.\r\n\r\n- TensorFlow is installed at: C:\\Users\\Adam\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\r\n\r\n- Could not load 'cudart64_80.dll'. The GPU version of TensorFlow\r\n  requires that this DLL be installed in a directory that is named in\r\n  your %PATH% environment variable. Download and install CUDA 8.0 from\r\n  this URL: https://developer.nvidia.com/cuda-toolkit\r\n\r\nFirst of all I have installed CUDA 9 with cuDNN7, I am not sure why tensorflow is looking for the older version.\r\nSecond, during the tensorflow-gpu installation I remeber that CUDA8 was also installed. Nevertheless, I tried to dowlad it from the website again but it seems to no longer be available/distributed.\r\n\r\nDo you think I should try to get CUDA8 or I can somehow fix this issue having CUDA9?", "You're guessing it correctly. If you read the [installation docs for Windows](https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support) you'll notice TensorFlow requires and only supports, as of now, CUDA v8 and cuDNN v6. Later versions will only be supported from TensorFlow v1.5.", "You are a lifesaver! I do not know how could I miss it. It is a bit embarrassing.\r\nThanks so much again, it works now!\r\n", "No worries! Glad you got it going! If you face further issues please feel free to ask. \r\nStack Overflow under the TensorFlow tag is also great to obtain support from the community.\r\nThis issue can be closed now.", "Thanks again. \r\nI am closing the issue.", "I ran the script, its says:\r\n\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\r\n\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\nAn exception has occurred, use %tb to see the full traceback."]}, {"number": 13969, "title": "Centered batch padding with tf.data.Dataset API", "body": "Hi,\r\n\r\nFirst of all, kudos for the PaddedBatchDataset op and all the tf.data.Dataset API.\r\n\r\nRight now, all the examples in the batch are aligned at coordinate (0, 0, ...). I think it is very convenient in many applications (e.g. image processing) to have the examples \"centered\" in the batch. \r\n\r\nI would like to know if you would accept a pull request in that direction. If so, I am willing to work on this. I suggest to add a flag to the operator so that the user can decide whether she wants the data centered, or aligned at (0, 0, ...), i.e. the current behavior. This changes should not be difficult to make.\r\n", "comments": ["@mrry what do you think?", "If it would be useful, we could accept a custom transformation function in `tf.contrib.data` to add this feature (similar to how `tf.contrib.data.batch_and_drop_remainder()` or `tf.contrib.data.dense_to_sparse_batch()` are implemented). It might be good to discuss the proposed interface in this issue before going ahead with the implementation, because I imagine there could be other padding options, e.g. inspired by the features in `tf.pad()`, and it would be nice to come up with a generally useful transformation. \r\n\r\n/cc @ebrevdo, a notorious padder, who might have additional desires in this regard.", "Indeed, there are different ways of padding. I think that in most of cases, especially when your operations are able to deal with variable sizes within each batch, you are just fine with the current features of PaddedBatchDataset.\r\n\r\nHowever, when some of your operations assume that all the examples in the minibatch have the same shape (e.g. LSTM wrapper for cudnn's LSTM), it is very useful to align the examples to share the same center coordinates. This is actually the problem that I'm facing now.\r\n\r\nSome practical use cases:\r\nA dataset containing three tensors of different sizes: ```[ [1, 1, 1, 1], [2, 2], [3] ]```\r\n\r\n- **Current padding:**\r\n```python\r\ndataset.padded_batch(2, [None])\r\n```\r\nwould produce:\r\n```python\r\n[ [1, 1, 1, 1],\r\n  [2, 2, 0, 0],\r\n  [3, 0, 0, 0] ]\r\n```\r\n\r\n- **Centered padding:**\r\n```python\r\ndataset.padded_batch(2, [None], centered=True)\r\n```\r\nwould produce something like:\r\n```python\r\n[ [1, 1, 1, 1],\r\n  [0, 2, 2, 0],\r\n  [0, 3, 0, 0] ]\r\n```\r\nOf course one can think of many other alignment strategies. For instance, aligning to the right. However, this can be generally implemented with the current padding and pre/post map() operations.\r\n\r\nHowever *relative* alignments (the centered case that I just showed is a special case of a relative alignment) cannot be implemented with the current API, but I can't think of any practical case where handling generic relative alignments would be useful.\r\n\r\nThat's why I suggested to simply add a boolean flag to the PaddedBatchDataset class to deal with the centering. This will keep the API clean, and would cover most practical use cases that I can think of.\r\n", "@ebrevdo would you let us know if you have any comments before @jpuigcerver starts working on a patch?", "I'd like to understand how you plan to use centered features in the model\nitself.  Do you plan to also provide outputs containing the beginning and\nend of each sequence?\n\nDo you plan to provide masking of loss terms associated with where the\npadding is?  If so, how, and what function do you plan to use to do this?\nWhat about ensuring that any state is initialized at zero properly for each\nentry?\n\nOn Fri, Oct 27, 2017 at 10:11 AM, Michael Isard <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> would you let us know if you have\n> any comments before @jpuigcerver <https://github.com/jpuigcerver> starts\n> working on a patch?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-340030205>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimznKffF_ZpMiFmf1rLzIGwu5fov2ks5swg7PgaJpZM4QFz_p>\n> .\n>\n", "Hi @ebrevdo,\r\n\r\nTo give some perspective, I've been using this trick for handwritten text recognition for a while (you can also think of speech recognition, as an application potentially affected by this). The main reason for this is that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not support examples of multiple lengths. So, after padding one must assume that all examples have the same size.\r\n\r\nThis, by itself, is not a big issue. The problems arise when one uses **bidirectional LSTMs** and the length of the sequences varies a lot within each batch.\r\n\r\nNotice that the **left-to-right LSTM has to deal with lots of zeros in the input at the end of the sequence**, while the **right-to-left LSTM presents this problem at the start of the sequence**. Centering the examples within the batch mitigates this problem.\r\n\r\nIn terms of the loss (CTC), I just ignore the fact that I padded the sequences. In practice, this means that the model has to learn to be invariant to the amount of \"zeros\" at the beginning *and* the end of the examples.\r\n\r\nIdeally, I would like that all recurrent operations support examples of multiples sizes within the batch, but that requires a significant effort from many other developers. In any case, I found that this simple trick of centering the features within the batch works quite well in practice, so it's a good thing to have this option available if some recurrent op does not support multiple layers.\r\n\r\nRegarding the PaddedBatchDataset operation in TF, we could add an `output_offset` attribute, similar to `output_types` and `output_shapes` that contains the offset of each element within the batch. The default (left-aligned padding) would be a zeros tensor. This, and the `output_shapes` would be enough if the user wants to take into account the padding for any masking in later ops.", "I see.  Certainly LSTMs have traditionally had no problem dealing with\npadding even when masking is not available.\n\nI like the idea of a padding_alignment argument, though the output_offset\ncan't be an attribute (in the sense that attributes are like\nhyperparameters set at graph construction time); it has to be a tensor.  So\nconsider instead adding a AlignedPaddedBatchDataset in tf.contrib.data\nwhich takes an alignment= (string) argument and emits offset and length\ntensors.\n\nI guess one of my main questions is regarding \"peeking\", or having the\nmodel learn things from the batch sizes that you didn't mean to teach it,\nand which depends on the batch size and how you batch.\n\nFor example, if you use group_by_window after padding to ensure your\nbatches all contain entries that are either all few time steps or many time\nsteps, then the LSTM will learn a bias from just seeing how much padding\nthere is at the beginning and/or end of the sequence.  Then at inference\ntime, or if you change your bucketing, this bias goes away and your model\nwill not perform as well as it did before.  Even if you don't\ngroup_by_window, if you use a large window size then you'll always have\nsome long sequences and the shorter ones will see a lot of padding at the\nbeginning.  Again in this case the LSTM will learn a bias when it sees a\nlot of padding at the beginning, and at inference time in batch_size=1\nmode, there'll be no padding and it will not perform as well.\n\nWe've seen this happen with experiments in both rnns and in tensor2tensor\n(Transformer) models.\n\nSo keep this in mind.\n\nOn Fri, Oct 27, 2017 at 12:36 PM, Joan Puigcerver <notifications@github.com>\nwrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo>,\n>\n> To give some perspective, I've been using this trick for handwritten text\n> recognition for a while (you can also think of speech recognition, as an\n> application potentially affected by this). The main reason for this is\n> that, as it happens with TF, the Torch's wrapper to cudnn LSTM does not\n> support examples of multiple lengths. So, after padding one must assume\n> that all examples have the same size.\n>\n> This, by itself, is not a big issue. The problems arise when one uses *bidirectional\n> LSTMs* and the length of the sequences varies a lot within each batch.\n>\n> Notice that the *left-to-right LSTM has to deal with lots of zeros in the\n> input at the end of the sequence*, while the *right-to-left LSTM presents\n> this problem at the start of the sequence*. Centering the examples within\n> the batch mitigates this problem.\n>\n> In terms of the loss (CTC), I just ignore the fact that I padded the\n> sequences. In practice, this means that the model has to learn to be\n> invariant to the amount of \"zeros\" at the beginning *and* the end of the\n> examples.\n>\n> Ideally, I would like that all recurrent operations support examples of\n> multiples sizes within the batch, but that requires a significant effort\n> from many other developers. In any case, I found that this simple trick of\n> centering the features within the batch works quite well in practice, so\n> it's a good thing to have this option available if some recurrent op does\n> not support multiple layers.\n>\n> Regarding the PaddedBatchDataset operation in TF, we could add an\n> output_offset attribute, similar to output_types and output_shapes that\n> contains the offset of each element within the batch. The default\n> (left-aligned padding) would be a zeros tensor. This, and the\n> output_shapes would be enough if the user wants to take into account the\n> padding for any masking in later ops.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13969#issuecomment-340065773>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2qf2wb50nRW0ebpvHN0xtPeYKA0ks5swjC8gaJpZM4QFz_p>\n> .\n>\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Closing due to lack of activity. @jpuigcerver Feel free to reopen if you pick this up again.", "I encounter the same issue currently but not only for LSTMS.\r\n\r\nWhen you train an arbitrary convolutional network, that needs fixed image size inputs (i.e. it has a dense layer inside) with images from a `dataset` object, you can use the function `dataset.padded_batch` with `None` or `-1` at the corresponding places to pad your data with some constant values according to the biggest dimension in the batch. However the type of padding is not customizable. As described above, the values (i.e. zeros) are places on the right and the bottom of the image:\r\n\r\nA picture with \r\n[[1,1,1,1]\r\n [1,1,1,1]\r\n [1,1,1,1]\r\n [1,1,1,1]]\r\n\r\npadded by 2 would look like:\r\n[[1,1,1,1,0,0]\r\n [1,1,1,1,0,0]\r\n [1,1,1,1,0,0]\r\n [1,1,1,1,0,0]\r\n [0,0,0,0,0,0]\r\n [0,0,0,0,0,0]]\r\n\r\nThe problem now is, that if you train a network with images padded like this, that net will learn eventually that the right and bottom part of the input is not very contributing to the classification or labeling of the image, so the weights referring to the receptive field of the black border will be more neglected by the net.\r\nDuring inference time, we the classifier will be presented only with image without this border, which means, that we'll get worse accuracy if important image features lay on the right and bottom side of the picture.\r\nThis effect could be minimized in 2 possible ways:\r\n1.: Center padding.\r\ni.e. creating a picture like this:\r\n[[0,0,0,0,0,0]\r\n [0,1,1,1,1,0]\r\n [0,1,1,1,1,0]\r\n [0,1,1,1,1,0]\r\n [0,1,1,1,1,0]\r\n [0,0,0,0,0,0]]\r\nIn this \"_centered_\" padded image the receptive field of neurons will see more of the picture at the borders and therefore reduce the described effect.\r\nNote: this is similar to the suggestion of **jpuigcerver** above.\r\n\r\n2.: Mirror padding\r\nThe image is padded as before to the right and bottom if necessary. But the values are not constant but mirrored from the input image.\r\ni.e for an input picture:\r\n[[1,2,3,4]\r\n [1,2,3,4]\r\n [1,2,3,4]\r\n [2,2,2,4]]\r\nmirrored batch would create a picture like:\r\n[[1,2,3,4,4,3]\r\n [1,2,3,4,4,3]\r\n [1,2,3,4,4,3]\r\n [2,2,2,4,4,2]\r\n [2,2,2,4,4,2]\r\n [2,2,2,4,4,2]]\r\n\r\nI think these features would really help improving the output of the network when feeding it padded images.\r\n\r\nI would appreciate to reopen this issue.", "+1 I also need this feature!", "Hi,\r\n\r\nI'm so sorry for being silent for so long. Unfortunately, I'm writing my thesis now and I don't have much time for writing code. If anyone wants to work on this, I (and apparently others) think it's a very useful feature. If nobody has implemented it yet, I'll come back to this once I defend my thesis.", "Hi @StphMe and all: did you, by any chance, figure out some hack for doing this using the existing API?", "Hi @anjany, unfortunately not. I also had no time to contribute. You may try to create a work around that works for your exact case using the `tf.pad()` function, which supports different types of padding: https://www.tensorflow.org/api_docs/python/tf/pad", "For anyone who might need this, I figured out a hack for centering the padded image. It can be done using `tf.contrib.image.translate()` function and original shape tensor. Example for centering along width has been given below:\r\n\r\nim = Right padded images given by tf.data.Dataset API. Shape = (batch_size, height, width, channels)\r\norig_width = Width of the image before padding  \r\n```\r\nx_shift = (tf.shape(im)[2] - orig_width) / tf.constant(2)\r\ny_shift = tf.constant(0.0, shape=[tf.shape(im)[0], dtype=tf.float64)\r\ntranslation_vector = tf.cast(tf.stack([x_shift, y_shift], axis=1), tf.float32)\r\nim = tf.contrib.image.translate(im, translation_vector)\r\n```"]}, {"number": 13968, "title": "A unexpected read op is in my pb, It make convertion to dlc fail", "body": "I want to conver the pb to dlc. but fail. \r\nI find a unexpected read op between the const and the op.\r\nwhy ? how can I remove the read op ?\r\n\r\n\r\nconver error message:\r\n/home/nubiaml/snpe-sdk/snpe-1.2.2/lib/python/converters/tensorflow/layers/eltwise.py:105: RuntimeWarning: error_code=1002; error_message=Layer paramter value is invalid. Layer layer2/Mul: at least two inputs required, have 1; error_component=Model Validation; line_no=582; thread_id=140151840044864\r\noutput_name)\r\n/home/nubiaml/snpe-sdk/snpe-1.2.2/lib/python/converters/tensorflow/layers/eltwise.py:83: RuntimeWarning: error_code=1002; error_message=Layer paramter value is invalid. Layer output/output: at least two inputs required, have 1; error_component=Model Validation; line_no=582; thread_id=140151840044864\r\noutput_name)\r\n\r\nthe code which save the pb:\r\n`import tensorflow as tf\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n\r\nwith tf.name_scope(\"input\"):\r\n    X = tf.placeholder(tf.float32, shape=(None, 1), name=\"input\");\r\n\r\nwith tf.name_scope(\"layer2\"):\r\n    a = tf.Variable(tf.zeros([1, 1], tf.float32), name=\"a\");\r\n    ax = X * a;\r\n\r\nwith tf.name_scope(\"output\"):\r\n    b = tf.Variable(tf.zeros([1, 1], tf.float32), name=\"b\");\r\n    h = tf.add(ax, b, name=\"output\");\r\n\r\ny = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\");\r\nJ = tf.reduce_mean(tf.square(h-y))/2;\r\noptimizer = tf.train.GradientDescentOptimizer(0.1);\r\ntrain = optimizer.minimize(J, var_list=[a, b]);\r\n\r\nsess = tf.Session();\r\nsess.run(tf.global_variables_initializer());\r\n\r\nfor i in range(10000):\r\n    sess.run([train, J], feed_dict={X:[[1], [2]], y:[[1], [2]]})\r\n\r\nprint(sess.run([a, b]));\r\n\r\ngraph = convert_variables_to_constants(sess, sess.graph_def, [\"output/output\"])\r\ntf.train.write_graph(graph, '.', 'graph.pb', as_text=False)\r\n\r\nsess.close();`", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13967, "title": "build the libtensorflow-core.a from the tf1.2 source, but get the SIGABRT running the codes, seems TestCPUFeature fail", "body": "Hi TF Experts,\r\n\r\nI build the libtensorflow-core.a from the  TF1.2 source codes in one machine with gcc 4.8.2. but when I running the example codes, I got one segment fault. below is the debug info in GDB\uff1a\r\n\r\n#0  0x00007ffff75f3625 in raise () from /lib64/libc.so.6\r\n#1  0x00007ffff75f4e05 in abort () from /lib64/libc.so.6\r\n#2  0x0000000001d8c395 in __gnu_cxx::__verbose_terminate_handler() () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:95\r\n#3  0x0000000001d59806 in __cxxabiv1::__terminate(void (*)()) () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:38\r\n#4  0x0000000001d59833 in std::terminate() () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:48\r\n#5  0x0000000001d5a58e in __cxa_throw () at ../../../../libstdc++-v3/libsupc++/eh_throw.cc:84\r\n#6  0x0000000001d88c90 in std::__throw_system_error(int) () at ../../../../../libstdc++-v3/src/c++11/functexcept.cc:104\r\n# #7  0x0000000000cd5051 in tensorflow::port::TestCPUFeature(tensorflow::port::CPUFeature) ()\r\n#8  0x0000000000cd4b0e in _GLOBAL__sub_I_cpu_feature_guard.cc ()\r\n#9  0x0000000001db4966 in __do_global_ctors_aux ()\r\n#10 0x0000000000a3954b in _init ()\r\n#11 0x00007fffffffe588 in ?? ()\r\n#12 0x0000000001db48f5 in __libc_csu_init ()\r\n#13 0x00007ffff75dfcf0 in __libc_start_main () from /lib64/libc.so.6\r\n#14 0x0000000000a40c19 in _start ()\r\n\r\nif seems that TestCPUFeature failed. I don't why? I build in this machine, and why it will fail running in the same machine?", "comments": ["**I found that if I link my program using the libprotobuf.a, the issue occurred, but if linking with libprotobuf.so, issue not found, but below warning showed:**\r\n\r\n017-10-25 08:57:44.735515: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-25 08:57:44.735613: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-25 08:57:44.735626: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-25 08:57:44.735634: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-25 08:57:44.735641: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-10-25 08:57:44.735670: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "yes, it is", "Nagging Assignee @tatatodd: It has been 158 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@rigorosyang  Hi, could you please provide the information asked in [this template](https://github.com/tensorflow/tensorflow/issues/new). Also appreciate if you provide the exact command to produce the output included in your test case. It's really difficult to understand the issue unless this information is provided. Thank you!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 13966, "title": "ImportError: libmpich.so.12: cannot open shared object file: No such file or directory", "body": "\r\n### System information\r\n- **OS** : Ubuntu 17.10\r\n- **TensorFlow installed from **: source \r\n- **TensorFlow version (use command below)**: r1.3\r\n- **Python version**: 3.6\r\n- **CUDA/cuDNN version**: 8.0/ 6\r\n\r\nWhen I try to import tensorflow, I get error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jihao/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/jihao/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/jihao/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libmpich.so.12: cannot open shared object file: No such file or directory\r\n```\r\nI try to fix it by command\r\n```bash\r\nsudo apt install libmpich-dev\r\n```\r\nIt did not work.\r\n", "comments": ["Try searching for and installing other libmpich packages?", "Already installed.\r\nI tried find which package contains that file and got nothing.", "Unfortunately we can't help too much with environment problems here. Try asking on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 13965, "title": "compile tensorflow-1.4.0-rc1 failed with error: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10 beta2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: TensorFlow 1.4.0-rc1\r\n- **Python version**:  Python 2.7.14\r\n- **Bazel version (if compiling from source)**:  bazel 0.6.0\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n_**bazel build --config=mkl --copt=\"-g\" --copt=\"-DEIGEN_USE_VML\" --copt=\"-mavx2\" --copt=\"-mfma\" --copt=\"-O3\" --verbose_failures --copt=\"-L/opt/intel/gcc/lib64\" -s -c opt //tensorflow/tools/pip_package:build_pip_package**_\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nFailed to use  bazel 0.6.0 to compile tensorflow 1.4.0-rc1 on Ubuntu 17.10beta2\r\n\r\n1. install Bazel\r\ndownload bazel-0.6.0-installer-linux-x86_64.sh\r\nbash bazel-0.6.0-installer-linux-x86_64.sh to install bazel\r\nsource /usr/local/lib/bazel/bin/bazel-complete.bash\r\n\r\n2. compile tensorflow\r\n./configure\r\n\r\n_**bazel build --config=mkl --copt=\"-g\" --copt=\"-DEIGEN_USE_VML\" --copt=\"-mavx2\" --copt=\"-mfma\" --copt=\"-O3\" --verbose_failures --copt=\"-L/opt/intel/gcc/lib64\" -s -c opt //tensorflow/tools/pip_package:build_pip_package**_\r\n\r\n3. compile failed\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n_ERROR: /home/automation/tensorflow-1.4.0-rc1/tensorflow/python/BUILD:2953:1: SWIGing tensorflow/python/tensorflow.i failed (Segmentation fault): swig failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/35d546f7441fd09e73ff30ea3d9aa112/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i): swig failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/35d546f7441fd09e73ff30ea3d9aa112/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n  bazel-out/host/bin/external/swig/swig -c++ -python -module pywrap_tensorflow_internal -o bazel-out/local-opt/bin/tensorflow/python/pywrap_tensorflow_internal.cc -outdir bazel-out/local-opt/bin/tensorflow/python -ltensorflow/python/client/device_lib.i -ltensorflow/python/client/events_writer.i -ltensorflow/python/client/tf_session.i -ltensorflow/python/client/tf_sessionrun_wrapper.i -ltensorflow/python/framework/cpp_shape_inference.i -ltensorflow/python/framework/python_op_gen.i -ltensorflow/python/grappler/cluster.i -ltensorflow/python/grappler/cost_analyzer.i -ltensorflow/python/grappler/item.i -ltensorflow/python/grappler/model_analyzer.i -ltensorflow/python/grappler/tf_optimizer.i -ltensorflow/python/lib/core/py_func.i -ltensorflow/python/lib/core/strings.i -ltensorflow/python/lib/io/file_io.i -ltensorflow/python/lib/io/py_record_reader.i -ltensorflow/python/lib/io/py_record_writer.i -ltensorflow/python/platform/base.i -ltensorflow/python/pywrap_tfe.i -ltensorflow/python/training/quantize_training.i -ltensorflow/python/training/server_lib.i -ltensorflow/python/util/kernel_registry.i -ltensorflow/python/util/port.i -ltensorflow/python/util/py_checkpoint_reader.i -ltensorflow/python/util/stat_summarizer.i -ltensorflow/python/util/tfprof.i -ltensorflow/python/util/transform_graph.i -Ibazel-out/local-opt/genfiles -Iexternal/eigen_archive -Iexternal/grpc -Iexternal/protobuf_archive -Iexternal/swig -Iexternal/boringssl -Ibazel-out/local-opt/genfiles/external/local_config_python -Iexternal/nsync -Iexternal/gemmlowp -Iexternal/jpeg -Iexternal/com_googlesource_code_re2 -Iexternal/mkl -Iexternal/jsoncpp_git -Iexternal/zlib_archive -Iexternal/highwayhash -Iexternal/gif_archive -Iexternal/mkl_dnn -Ibazel-out/local-opt/genfiles/external/jpeg -Iexternal/lmdb -Iexternal/png_archive -Iexternal/farmhash_archive -Iexternal/sqlite_archive -Iexternal/swig/Lib -Iexternal/swig/Lib/cffi -Iexternal/swig/Lib/python -Iexternal/swig/Lib/std -Iexternal/swig/Lib/typemaps tensorflow/python/tensorflow.i)._\r\n\r\n", "comments": ["Confirm with the above configuration,\r\ntensorflow-1.4.0-rc1 could be compiled on Ubuntu17.04\r\nQuestion is the failed message on Ubuntu17.10 seems relates with the swig", "What version of SWIG is installed on that system?", "@martinwicke How could check the swig version?\r\nI used these commands: but no swig package find\r\n_swig -version\r\nThe program 'swig' is currently not installed. You can install it by typing:\r\napt install swig_\r\n\r\ndpkg -l | grep swig.*\r\n\r\n\r\nIf I didn't installed the swig on the OS, Does any swig package inside tensorflow src would be used?\r\n\r\nBy the way, apt list swig could get the swig package, but I think it's not been installed \r\n_apt list swig\r\nListing... Done\r\nswig/artful 3.0.10-1.2 amd64_\r\n\r\n\r\n", "Yes, I think that would be the issue. Please install swig. "]}, {"number": 13964, "title": "Callback in tf.Print to access raw data ", "body": "### System information\r\n- **Have I written custom code**: No\r\n- **OS**: Win 10\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: 1.3.0\r\n- **Python version**:  3.5\r\n- **Bazel version (if compiling from source)**: /\r\n- **CUDA/cuDNN version**:  CPU\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**: /\r\n\r\n### Feature request\r\n\r\nThe `tf.Print` op lets us print tensor values when it gets evaluated. However, it does not provide any way to access the raw data programmatically. AFAIK, there is no way to access this data during computation. Please correct me if I'm wrong\r\n\r\nWith Theano, I was able to provide a custom callback to the printing facility, to access the tensor values when tensor was evaluated. This is extremely useful for image tensors for instance to be able to plot them at evaluation time. With TF I cannot do that.\r\n\r\nBasically, what I am proposing is adding a custom `callback` parameter to the op:\r\n```\r\nPrint(\r\n    input_,\r\n    data,\r\n    message=None,\r\n    first_n=None,\r\n    summarize=None,\r\n    callback=None,\r\n    name=None\r\n)\r\n```\r\nIf callback is undefined, `tf.Print` behavior remains identical. Otherwise, `tf.Print` will call the callback with `([values], message, name).\r\n\r\n", "comments": ["You can use the `tf.py_func()` op to register a Python callback with access to the raw tensor values (as a NumPy array). Simply return the arguments after executing your Python code, and it will act as an identity op."]}, {"number": 13963, "title": "Tensorflow crashes when importing certain modules before tensorflow", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary both from conda-forge and from pip install using the tfBinaryURL\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.3.0\r\n\r\n- **Python version**:\r\n3.6.2 from Anaconda\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0 cuDNN 6.0\r\n\r\n- **GPU model and memory**:\r\nGTX 1060 6 GB RAM\r\n\r\n- **Exact command to reproduce**:\r\n```python\r\nimport psi4\r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n```\r\n\r\n### Describe the problem\r\nRunning the above code produces the following error\r\n\r\n```\r\n2017-10-24 22:17:35.570620: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Could not parse default value '4000' from Attr(\"upper_frequency_limit: float = 4000\") for Op Mfcc\r\nCould not parse default value '20' from Attr(\"lower_frequency_limit: float = 20\") for Op Mfcc\r\nforrtl: error (76): Abort trap signal\r\n```\r\n\r\nStrangely if tensorflow is imported before psi4 then the same error does not occur. Also, the error seems to be related to one of the Ops in tensorflow/tensorflow/core/kernels/mfcc.h which as far as I can tell is not an Op that I'm even using.\r\n\r\nThis issue occurs with at least two different python modules that I know of, psi4 as I have run into, and lycon which has their own issue open regarding this same error ethereon/lycon#3. In the linked issue it seems that this error occurs for others on python 2 and 3.5 with Ubuntu 16.04 both with and without GPU support for tensorflow, but another user states that this did not occur for him on Ubuntu 14.04 or on macOS 10.13. It seems like this may be an issue specific to at least Ubuntu 16.04 then.\r\n\r\n### Source code / logs\r\nFull output from the error\r\n\r\n```\r\n2017-10-24 22:17:35.570620: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Could not parse default value '4000' from Attr(\"upper_frequency_limit: float = 4000\") for Op Mfcc\r\nCould not parse default value '20' from Attr(\"lower_frequency_limit: float = 20\") for Op Mfcc\r\nforrtl: error (76): Abort trap signal\r\nImage              PC                Routine            Line        Source             \r\nlibpcm.so.1        00007FCA4B14A725  Unknown               Unknown  Unknown\r\nlibpcm.so.1        00007FCA4B148347  Unknown               Unknown  Unknown\r\nlibpcm.so.1        00007FCA4B05FAA2  Unknown               Unknown  Unknown\r\nlibpcm.so.1        00007FCA4B05F8F6  Unknown               Unknown  Unknown\r\nlibpcm.so.1        00007FCA4B02DEFD  Unknown               Unknown  Unknown\r\nlibpcm.so.1        00007FCA4B031ABF  Unknown               Unknown  Unknown\r\nlibpthread.so.0    00007FCA5609F390  Unknown               Unknown  Unknown\r\nlibc.so.6          00007FCA55CF9428  Unknown               Unknown  Unknown\r\nlibc.so.6          00007FCA55CFB02A  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0DA71B94  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D94AAFE  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D94B21A  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D924199  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D946EFE  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D934760  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D68F8D7  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D62136E  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D621488  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0D621876  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0B615970  Unknown               Unknown  Unknown\r\n_pywrap_tensorflo  00007FCA0B434177  Unknown               Unknown  Unknown\r\npython             000055BE1E6850C6  Unknown               Unknown  Unknown\r\npython             000055BE1E7181DC  Unknown               Unknown  Unknown\r\npython             000055BE1E7398BA  Unknown               Unknown  Unknown\r\npython             000055BE1E711FEB  Unknown               Unknown  Unknown\r\npython             000055BE1E7182B5  Unknown               Unknown  Unknown\r\npython             000055BE1E7398BA  Unknown               Unknown  Unknown\r\npython             000055BE1E713116  Unknown               Unknown  Unknown\r\npython             000055BE1E713B84  Unknown               Unknown  Unknown\r\npython             000055BE1E684F8E  Unknown               Unknown  Unknown\r\npython             000055BE1E73AF74  Unknown               Unknown  Unknown\r\npython             000055BE1E711334  Unknown               Unknown  Unknown\r\npython             000055BE1E712221  Unknown               Unknown  Unknown\r\npython             000055BE1E7182B5  Unknown               Unknown  Unknown\r\npython             000055BE1E7398BA  Unknown               Unknown  Unknown\r\npython             000055BE1E711908  Unknown               Unknown  Unknown\r\npython             000055BE1E712221  Unknown               Unknown  Unknown\r\npython             000055BE1E7182B5  Unknown               Unknown  Unknown\r\npython             000055BE1E7398BA  Unknown               Unknown  Unknown\r\npython             000055BE1E711FEB  Unknown               Unknown  Unknown\r\npython             000055BE1E7182B5  Unknown               Unknown  Unknown\r\npython             000055BE1E7398BA  Unknown               Unknown  Unknown\r\npython             000055BE1E711334  Unknown               Unknown  Unknown\r\npython             000055BE1E712221  Unknown               Unknown  Unknown\r\npython             000055BE1E7182B5  Unknown               Unknown  Unknown\r\npython             000055BE1E7398BA  Unknown               Unknown  Unknown\r\npython             000055BE1E712D58  Unknown               Unknown  Unknown\r\npython             000055BE1E713B0C  Unknown               Unknown  Unknown\r\npython             000055BE1E78DF04  Unknown               Unknown  Unknown\r\npython             000055BE1E65475A  Unknown               Unknown  Unknown\r\npython             000055BE1E6548FD  Unknown               Unknown  Unknown\r\npython             000055BE1E65495E  Unknown               Unknown  Unknown\r\npython             000055BE1E656B65  Unknown               Unknown  Unknown\r\npython             000055BE1E65951E  Unknown               Unknown  Unknown\r\nlibc.so.6          00007FCA55CE4830  Unknown               Unknown  Unknown\r\npython             000055BE1E7403D9  Unknown               Unknown  Unknown\r\nAborted (core dumped)\r\n```\r\n", "comments": ["(I'm assuming this is a symbol conflict based on the symptoms you've posted; I haven't looked into it thoroughly, so feel free to correct me)\r\n\r\nThe good news is that newer versions of TensorFlow (1.4+) will not cause problems like this, since we're no longer loading our symbols into the global symbol table. The bad news is that other libraries which export symbols via RTLD_GLOBAL can still cause problems for TensorFlow.\r\n\r\nYou could theoretically work around this problem by adding RTLD_DEEPBIND to the dlopen call in pywrap_tensorflow.py. This would break workflows which depend on redefining TensorFlow symbols (LD_PRELOAD), so we don't do it by default.\r\n\r\nSo basically this is contributions welcome if you have a reasonable way for us to avoid this symbol conflict (i.e. not contorting too much). Otherwise you can file a bug with the other library (since they're the ones exporting symbols globally; but do make sure you can reproduce in TF 1.4, since TF 1.3 can definitely cause these problems).", "@jeherr some relevant background on fixing this kind of crash due to PyTorch. Similar solution would be to encourage psi4 to stop exporting unnecessary symbols into global symbol namespace\r\n\r\nhttps://github.com/pytorch/pytorch/issues/3059", "I'll try to test this with the current Tensorflow 1.4 branch once I get some time. Should be able to soon.", "@jeherr did you get around to trying this on TensorFlow 1.4?", "Sorry I meant to get back to this sooner. So I've been working on getting tensorflow 1.4 installed. I was using python 3.6 and it's seems that I ran into issue #14182. So until that gets worked out I decided I would try to use python 3.5. So I set up a new conda environment, installed everything I needed, and now I'm hitting a \"terminate called after throwing an instance of 'std::bad_cast'\" error, which I am pretty sure is stemming from psi4 at this point. \r\n\r\nSince this is just a personal project for me, I won't have the time to try to work out this error I'm getting with psi4 and python 3.5. So at this point I think I am stuck waiting until a fix comes for the tensorflow 1.4 issue with python 3.6 is resolved.", "For what it's worth, the \"Python versions don't match\" warning from Cython is harmless. You can try it on 3.6 with TF 1.4.", "Marking as \"community support\" for now, in accordance with issue triage.", "@allenlavoie On Ubuntu 16.04, it seems to be treated more like an error rather than a warning. I seem to recall that in the issue linked above that several on MacOS stated their code still runs after throwing the warning, but at least one person mentioned that on Ubuntu their code wouldn't run after throwing this warning. The latter is the case which is happening to me.", "What's the error? The QT/matplotlib one? The warning gets printed unconditionally, so it would be printed before any other failure even if that other failure isn't related. ", "I had this problem myself. Adding the workaround for other's reference: Make sure tensorflow is the first library to be imported.", "I also meet this issue, tensorflow /keras (with tensorflow backend) abort if I import some module before. (some internal module).\r\n\r\nso I move import tensorflow and keras to the very begining then the issue disappeared.", "This isn't a TensorFlow issue. Import order can work around other libraries exporting conflicting symbols, or RTLD_DEEPBIND can let TF ignore them. Closing this to keep the tracker focused on actionable issues."]}, {"number": 13962, "title": "Compiling TensorFlow 1.4.0 GPU on Windows 10 x64", "body": "There doesn't seem to be any detailed documentation for how to compile TensorFlow 1.4.0 GPU on Windows 10 x64.\r\n\r\nI need to recompile TF to add missing functionality for a Windows 7 x64 production system.\r\n\r\n I can use Bazel or CMake to compile something but how do I incorporate the Python and NVIDIA dependencies into that build?\r\n\r\nHow are the Windows wheels at https://pypi.python.org/pypi/tensorflow-gpu compiled?", "comments": ["Could you try taking a look on [theses instructions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) to compile with CMake?\r\nWhen you're configuring TensorFlow you will choose if your binary will support GPU or not, however, you can't incorporate CUDA or cuDNN into the binary AFAIK.\r\nEach user who installs TF with that compiled binary needs to have CUDA and cuDNN installed.\r\n\r\nDisclaimer: it's been awhile since those docs were updated so probably some stuff, as some of the current limitations and cuDNN version for instance, could be ignored as a lot has changed since then. In case of the cuDNN you can use v6.1. All you need to compile though is present on it.", "Thanks, @Carmezim that worked.\r\nAdditional issues that came up are addressed at https://github.com/tensorflow/tensorflow/issues/5600\r\n\r\ndistutils is no longer available on the Anaconda3 win-64 channel.", "Cool! Thanks for following up. If you have any further questions let us know. This can be closed now.", "Not yet. I've still got some compile error issues I'm working through.", "Oh I thought by \"that worked\" you had success compiling. No problem though. Could you post them so we can take a look and also how to reproduce? Or was it just the distutils problem?\r\nCc @mrry", "I'm working with the tensorflow r1.4 branch.   When I CMake configure\r\n\r\n```\r\ncmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/Users/Kevin/dev/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=python.exe -DPYTHON_LIBRARIES=C:/ProgramData/Anaconda3/libs/python36.lib -DPYTHON_INCLUDE_DIR=C:\\ProgramData\\Anaconda3\\include -DNUMPY_INCLUDE_DIR=C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\numpy\\core\\include -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX \r\n```\r\nand run\r\n \r\n`MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj`\r\n\r\nI get the error\r\n\r\n```\r\n\"C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n(PostBuildEvent target) ->\r\n  C:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: The command \"setlocal\\r [C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: \"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E copy C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/tools/pip_package/setup.py C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/contrib/cmake/build/tf_python/\\r [C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\n...\r\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(133,5): error MSB3073: :VCEnd\" exited with code 9009. [C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj]\r\n    7016 Warning(s)\r\n    1 Error(s)\r\nTime Elapsed 03:53:21.42\r\n```\r\n\r\n[CMakeOutput.log](https://github.com/tensorflow/tensorflow/files/1419162/CMakeOutput.log)\r\n[CMakeError.log](https://github.com/tensorflow/tensorflow/files/1419163/CMakeError.log)\r\n\r\n", "Neither of those log files seems to include that error message (I searched for \"1 Error(s)\" and got no hits in either). Can you send the full log of the failing build?\r\n\r\nThis substring in the error message is suspicious: `C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/contrib/cmake/build/tf_python/\\r`. Assuming the `\\r` is part of the intended target path, it looks like there is a stray line break in some part of the configuration.", "Oops, sorry.  The CMake* files only show the CMake errors.  Builds on Windows aren't dumped to a logfile but to command prompt output.  Here's the last 9999 lines of that output.\r\n[tf bug.txt](https://github.com/tensorflow/tensorflow/files/1419225/tf.bug.txt)\r\n", "I'm currently trying to build a Python wheel with \r\n`MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj`\r\n\r\nThis failed at the wheel-building stage so while I wait for your answer on the compilation error I'm trying to build the test trainer with \r\n`MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj`   \r\nThis appears to be rebuilding the TF code from scratch.\r\n\r\nThat can't be right, can it?", "The build isn't hermetic (its only supported purpose is building the PIP package on our CI server), so I believe MSBuild isn't able to detect opportunities for incremental building. If this is easy to fix, and somebody who knows how to do that is reading, we'd welcome contributions on that topic.\r\n\r\nThis suggests a possible cause for the error you are seeing: if your build directory contains some files created during a previous failed build, these could be causing the new build to fail. It might be easiest to try deleting the entire `tensorflow/contrib/cmake/build/` directory, then creating a new build from scratch.", "I've already done that once.  What CMake and MSBuild command lines do you use on your CI server?  I'm wondering if there's a problem with the particular arg combination I'm using.", "Here's the script: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/gpu/cmake/run_build.bat", "This build together with the CMake listed above compiled without errors\r\n`>MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj`\r\n\r\nso the problem is specifically with this build\r\n`MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj`\r\n", "I believe the '\\r' in the MSBuild message is just an artifact of the way that MSBuild prints messages and the way that Windows uses \"\\r\\n\" instead of \"\\n\"", "https://stackoverflow.com/questions/14916729/visual-studio-rebuilds-unmodified-projects", "The reason why these are completely rebuilt from scratch is that you have MSBuild running CMake commands like this every time you build:\r\n```\r\n\"C:\\Program Files\\CMake\\bin\\cmake.exe\" -E copy_directory C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/core C:/Users/Kevin/dev/tensorflow-r1.4/tensorflow/contrib/cmake/build/tf_python/tensorflow/include/tensorflow/core\r\n```", "I think I may have found the problem with the failed wheel build.\r\n\r\nIn the CMake command I replaced\r\n```-DPYTHON_EXECUTABLE=python.exe```\r\nwith\r\n```-DPYTHON_EXECUTABLE=C:/ProgramData/Anaconda3/python.exe```\r\n\r\nThe problem appears to be that despite python.exe being in the PATH, CMake will prepend a local pathname to python.exe if it doesn't have an absolute pathname.\r\n\r\nTesting now.\r\n\r\nThe \"build always\" problem still unresolved.", "Compiled successfully after almost 5 hours.  Haven't tested the running of the wheel yet.  Need to resolve \"build always\" problem.", "```\r\nc:\\python35\\scripts\\pip3 install PythonWheels/tensorflow_gpu-1.4.0rc1-cp36-cp36m-win_amd64.whl\r\ntensorflow_gpu-1.4.0rc1-cp36-cp36m-win_amd64.whl is not a supported wheel on this platform.\r\n```\r\n\r\nWhat could cause this error?  \r\n\r\nIs it compiling on Windows 10 rather than Windows 7?  What version of Windows is your CI build server?\r\n\r\nCould it be a difference in the CMake flags?\r\n\r\nYours (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/windows/gpu/cmake/run_build.bat):\r\n`%CMAKE_EXE% %CMAKE_DIR% -A x64 -DSWIG_EXECUTABLE=%SWIG_EXE% -DPYTHON_EXECUTABLE=%PY_EXE% -DCMAKE_BUILD_TYPE=Release -DPYTHON_LIBRARIES=%PY_LIB% -Dtensorflow_BUILD_PYTHON_TESTS=%BUILD_PYTHON_TESTS% -Dtensorflow_BUILD_CC_TESTS=%BUILD_CC_TESTS% -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=%CUDNN_HOME% -Dtensorflow_TF_NIGHTLY=%TF_NIGHTLY%`\r\n\r\nMine:\r\n`cmake C:\\Users\\Kevin\\dev\\tensorflow-r1.4\\tensorflow\\contrib\\cmake -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/Users/Kevin/dev/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=C:/ProgramData/Anaconda3/python.exe -DPYTHON_LIBRARIES=C:/ProgramData/Anaconda3/libs/python36.lib -DPYTHON_INCLUDE_DIR=C:\\ProgramData\\Anaconda3\\include -DNUMPY_INCLUDE_DIR=C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\numpy\\core\\include -Dtensorflow_ENABLE_GPU=ON -DCUDNN_HOME=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\" -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX `\r\n\r\nIn yours but not mine:\r\n- -Dtensorflow_BUILD_PYTHON_TESTS=%BUILD_PYTHON_TESTS% \r\n- -Dtensorflow_BUILD_CC_TESTS=%BUILD_CC_TESTS% \r\n- -Dtensorflow_TF_NIGHTLY=%TF_NIGHTLY%\r\n\r\nIn mine but not yours:\r\n- -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX  \r\n\r\nYour MSBuild:\r\n`%MSBUILD_EXE% /p:Configuration=Release /maxcpucount:32 /verbosity:minimal tf_python_build_pip_package.vcxproj`\r\n\r\nMine.  I don't see any substantive difference:\r\n`MSBuild /p:Configuration=Release tf_python_build_pip_package.vcxproj`", "@johnsrude `not a supported wheel` error occurs when there's incompatibility between the Python version you're trying to install this binary for and the wheel target version, in this case Python 3.6. You are probably installing it for Python 3.5. \r\n\r\nSecond guess which I think might not be the case is you have Python v3.5 and v3.6 installed and your `pip3` is linked to the former.", "@Carmezim  Yes, that's definitely a possibility.  I'll test it. I'm switching to a bigger build server to speed up the compiles so Monday.", "@Carmezim Yes, the problem here was Python 3.5 vs 3.6. Thanks!   \r\n\r\nStill need to resolve \"build always\" problem.", "I'm glad to hear it's building now! I'm going to close this issue, but feel free to open another one to track work on making the CMake build more incremental. (To set expectations, it's unlikely that anyone from the TF team will spend time on this, but we would welcome contributions that make things better for CMake users.)", "@johnsrude I get the same error \"error MSB3073\". But I have input the full path of Anaconda pothon.exe in the cmake command, like  -DPYTHON_EXECUTABLE=\"D:\\Program Files\\Anaconda3\\python.exe\", and this error still happens. How can I solve this to compile wheel? Thanks!"]}, {"number": 13961, "title": "Use 'LABEL maintainer=' in Dockerfile", "body": "This fix is a follow up of #13661 to replace `MAINTAINER` with `LABEL maintainer=` in Dockerfile.\r\n\r\nThe keyword `MAINTAINER` has long been deprecated and is replaced by `LABEL`, which is much more flexible and is easily searchable through `docker inspect`.\r\n\r\nThis fix replaces remaining `MAINTAINER` with `LABEL`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13960, "title": "Merge v1.4-rc1 back into master branch.", "body": "Mostly version string changes, but also adds some documentation that was directly committed\r\nto r1.4 branch.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "This merge was pretty messy. For some reason I had tons of merge conflicts. Pretty sure I resolved everything correctly but please give it a glance over. Thanks!", "Weird, the bot is trying to build a target added a few days ago and can't find it. Going to try to trigger the test again to see if it works now.", "+gunan. Do you know what to do here. Ubuntu Makefile seems to be trying to build \r\n//tensorflow/contrib/makefile:ci_makefile_test\r\n\r\nwhich is claims does not exist.", "Jenkins, test this please.", "@gunan The cla bot is not running and prevents the merge. Assigning to you.", "Mike could you resolve conflicts?", "Jenkins, test this please", "Mike, could you look into the test failures?\r\nOnce they are addressed, I can merge.\r\n\r\nJenkins, test this please.", "The failures seem to be related to some Jenkins Github plugin. I dont see any actual test failures."]}, {"number": 13959, "title": "Allow build_all_ios.sh to build just one arch", "body": "This change allows build_all_ios.sh to take an -a flag with a\r\nspecific arch so you dont have to waste time building unwanted\r\narchitectures (32bit etc)\r\n\r\nTEST: tensorflow/contrib/makefile/build_all_ios.sh  #builds fat lib\r\ntensorflow/contrib/makefile/build_all_ios.sh -a \"arm64\" #only arm64", "comments": ["Can one of the admins verify this patch?", "@petewarden please review. ", "    An incremental build on a 2013 MBP with just touching one file and running\r\n    \r\n    time ./tensorflow/contrib/makefile/build_all_ios.sh -T -a arm64\r\n    \r\ngives:\r\n\r\n    real    0m8.845s\r\n    user    0m6.934s\r\n    sys     0m1.638s\r\n", "Thanks @petewarden . I have updated the README.md so please give it a gander. ", "@benoitsteiner @martinwicke please let me know your thoughts on this PR. I have a few more enhancements for the iOS build on top of this PR. Thanks.", "@petewarden any comments?", "Jenkins, test this please.", "@gunan anything we can do to get this change in ? Thx", "Jenkins, test this please."]}]