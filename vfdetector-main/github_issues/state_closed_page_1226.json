[{"number": 16385, "title": "Estimator built with keras.estimator.model_to_estimator fails on Estimator.export_savedmodel", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')\r\n- **Python version**: 2.7.9\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See gist\r\n\r\n### Describe the problem\r\n\r\nIf I create a model with `tf.keras`, compile it and then turn it into an estimator by simply passing it thru to `tf.keras.estimator.model_to_estimator`, I am able to train and evaluate the model just fine -- however when I got to export it with `Estimator.export_savedmodel`, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"endtoend_noweights_trainer_keras.py\", line 302, in <module>\r\n    get_serving_input_fn(hyperparameters),\r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 517, in export_savedmodel\r\n    serving_input_receiver.receiver_tensors_alternatives)\r\n  File \"/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/export/export.py\", line 193, in build_all_signature_defs\r\n    raise ValueError('export_outputs must be a dict.')\r\nValueError: export_outputs must be a dict.\r\n```\r\n\r\nI'm not sure what `export_outputs` is, but if I had to guess it should be a mapping of output names to output tensors from the `keras` model.\r\n\r\nHere's the (very unclean) [code](https://gist.github.com/zmjjmz/8e3a7e5430f2e700a9e89bf2b4f6259b) I'm using to get to this, although there's a lot of dependencies that won't work for y'all. If you need a repro I can take the time to put it together, just let me know. Notably the error occurs on line `300`.", "comments": ["@zmjjmz yes, please create a minimum repro.\r\n\r\nAdding @fchollet in case there's anything obvious that might be wrong.", "Ok, sure, here it is: https://gist.github.com/zmjjmz/9b013c6524839512b85dfc08a56f28c2\r\n\r\nIt's possible that something about how I defined `serving_input_fn` is incorrect (it works for normal estimators), but I also couldn't find any documentation on how to properly / differently define it for an Estimator that came from `tf.keras.estimator.model_to_estimator`.\r\n", "I'm not a python or keras expert, but your imports are not idiomatic:\r\n\r\nYou're doing this:\r\n```\r\nimport tensorflow\r\nfrom tensorflow.python.keras._impl import keras\r\nfrom tensorflow.python.estimator.export.export_output import PredictOutput\r\nfrom tensorflow.python.estimator.export.export import build_raw_serving_input_receiver_fn\r\nfrom tensorflow.python.saved_model import signature_constants\r\nfrom tensorflow.python import debug as tf_debug\r\n```\r\n\r\nNormally we do this:\r\n```\r\nimport tensorflow as tf\r\n```\r\n\r\nThen in your code, you would refer to things as, e.g. `tf.keras.estimator` and things like that.  I don't know whether this has anything to do with your actual issue, but bypassing the standard `__init__.py` logic seems like a bad idea.  Perhaps you can change that first?", "It's certainly unidiomatic -- I probably did that because in the original code this repro comes from I was converting some code (with a ton of custom layers) that was used to the `keras` structure and `tf.keras` is slightly different (e.g. `tf.keras.layers.core.Dense` doesn't exist, but is instead `tf.keras.layers.Dense`). \r\n\r\nI'll rectify that and see if I can reproduce, but I'd be surprised (and still concerned) if that's the issue.\r\n\r\nEDIT: Yeah, cleaning up those imports made no difference. What I'm curious about is if I can get the EstimatorSpec from an Estimator? I haven't quite figured out how to do that, but it would illuminate where those `export_outputs` are getting filled.", "Could you share a short, standalone code snippet to reproduce this issue?", "I believe [this](https://gist.github.com/zmjjmz/1ebd3f6b78f012326a660ad126cdd4d5) is standalone and hopefully isn't too long.", "I'm also having this exact problem.", "I am having this problem as well on AWS SageMaker. I am using a simple example and transforming it to an estimator with ```python tf.keras.estimator.model_to_estimator(keras_model=model)```. I can train the model but cannot save it, I get the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-27-ed0372b11bae> in <module>()\r\n      1 \r\n      2 \r\n----> 3 exported_model = model.export_savedmodel(export_dir_base = 'export/Servo/', serving_input_receiver_fn = serving_input_fn)\r\n      4 \r\n      5 print (exported_model)\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in export_savedmodel(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path)\r\n    515           serving_input_receiver.receiver_tensors,\r\n    516           estimator_spec.export_outputs,\r\n--> 517           serving_input_receiver.receiver_tensors_alternatives)\r\n    518 \r\n    519       if not checkpoint_path:\r\n\r\n~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/export/export.py in build_all_signature_defs(receiver_tensors, export_outputs, receiver_tensors_alternatives)\r\n    191     receiver_tensors = {_SINGLE_RECEIVER_DEFAULT_NAME: receiver_tensors}\r\n    192   if export_outputs is None or not isinstance(export_outputs, dict):\r\n--> 193     raise ValueError('export_outputs must be a dict.')\r\n    194 \r\n    195   signature_def_map = {}\r\n\r\nValueError: export_outputs must be a dict.\r\n\r\n```\r\n\r\nMy code is:\r\n\r\n```python\r\n\r\n\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.externals import joblib\r\n\r\n\r\ndef featureTransform(features, max_words):\r\n    tokenize = tf.keras.preprocessing.text.Tokenizer(num_words=max_words, char_level=False)\r\n    tokenize.fit_on_texts(features) \r\n    return tokenize.texts_to_matrix(features).astype(np.float32) \r\n\r\ndef encodeLabels(labels):\r\n    encoder = LabelEncoder()\r\n    encoder.fit(labels)\r\n    y = encoder.transform(labels)\r\n    num_classes = np.max(y) + 1\r\n    print(\"num classes: {}\".format(num_classes))\r\n    return tf.keras.utils.to_categorical(y, num_classes).astype(np.float32)\r\n\r\ndef estimator_fn(run_config, params):\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(500, activation='relu', input_shape=(500,), name=\"features\"))\r\n    model.add(tf.keras.layers.Dropout(0.5))\r\n    model.add(tf.keras.layers.Dense(500, activation='relu'))\r\n    model.add(tf.keras.layers.Dropout(0.5))\r\n    model.add(tf.keras.layers.Dense(123, activation='softmax'))\r\n    model.compile(loss='categorical_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n    print(\"input names: {}\".format(model.input_names))\r\n    print(\"output names: {}\".format(model.output_names))\r\n    \r\n    return tf.keras.estimator.model_to_estimator(keras_model=model)\r\n\r\ndef serving_input_fn():\r\n    feature_spec = {'features_input': tf.FixedLenFeature(dtype=tf.float32, shape=[500])}\r\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\r\n\r\ndef train_input_fn(training_dir, params):\r\n    \"\"\"Returns input function that would feed the model during training\"\"\"\r\n    return _generate_input_fn(training_dir, 'assignment_train.csv')\r\n\r\ndef eval_input_fn(training_dir, params):\r\n    \"\"\"Returns input function that would feed the model during evaluation\"\"\"\r\n    return _generate_input_fn(training_dir, 'assignment_test.csv')\r\n\r\n\r\ndef _generate_input_fn(training_dir, training_filename, shuffle=False):\r\n    training_set = tf.contrib.learn.datasets.base.load_csv_without_header(\r\n    filename=os.path.join(training_dir, training_filename), target_dtype=np.str, features_dtype=np.float32)\r\n    \r\n    input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"features_input\":  np.array(training_set.data)}, \r\n        y=encodeLabels(training_set.target),\r\n        num_epochs=100,\r\n        shuffle=shuffle\r\n    )\r\n    return input_fn\r\n\r\n```\r\n\r\nI can train the model locally in Jupyter in SageMaker.\r\n\r\n```python\r\n\r\nfrom assignments import estimator_fn, _generate_input_fn\r\nfrom assignments import serving_input_fn\r\n\r\nmodel = estimator_fn(run_config = None, params = None)\r\n\r\nmodel.train(input_fn=_generate_input_fn('data','assignment_train.csv', shuffle=True))\r\n\r\n\r\nexported_model = model.export_savedmodel(export_dir_base = 'export/Servo/', serving_input_receiver_fn = serving_input_fn)\r\n\r\nprint (exported_model)\r\nimport tarfile\r\nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\r\n    archive.add('export', recursive=True)\r\n\r\n```\r\n\r\nI have spend quite a bit of time trying to get around this.", "Which version of TensorFlow were you using? Do you still see this issue with version 1.5+?", "It seems that SageMaker's TensorFlow version is '1.4.0'. I suppose that means I am stuck with this until they update the version?\r\n\r\nI ended up writing a custom estimator in TensorFlow with Keras layers, i.e.:\r\n\r\n```python first_hidden_layer = tf.keras.layers.Dense(128, activation='relu', name='first-layer')(features[INPUT_TENSOR_NAME])```\r\n\r\nHowever I noticed that when I trained on SageMaker's infrastructure it would give me a dimension mismatch if my labels were one-hot encoded, I have to just use label encoding. I think this follows how the pre-made TensorFlow estimators work. However I had no problem using one-hot encoding in Keras to train locally, but did have a problem locally training the estimator with Keras with encoded labels:\r\n\r\n```Tensor conversion requested dtype int64 for Tensor with dtype float32```\r\n\r\nCurrently I am going to stick to the custom estimators. Thanks for responding. ", "Tried the repro snippets with TF 1.5 and I don't see the error anymore. Let me know if this is still an issue.\r\nBtw @random-residual the type cast issue you mentioned should be fixed with [this commit](https://github.com/tensorflow/tensorflow/commit/4c86ece040cb96ea689f5c0d084b6959274eab91). It should be available in master and v1.6.0-rc0+.", "Btw took another look and the export_savedmodel issue should have been fixed in [this commit](https://github.com/tensorflow/tensorflow/commit/fe8406149feec453250905965a14285465cd2063), which is available in v.1.5.0-rc0+. Closing this issue. Thanks!"]}, {"number": 16384, "title": "fix typos", "body": "fix typos", "comments": ["@ManHyuk Thanks for the fix!"]}, {"number": 16383, "title": "Parameterized docker build now supports a local pip whl file path.", "body": "PiperOrigin-RevId: 183148798", "comments": []}, {"number": 16382, "title": "Disable bfloat16 for sparse_matmul for 1.5.0", "body": "I'm using this PR to test out simple workarounds for the sparse_matmul problem.\r\nIt doesn't need a reviewer yet.\r\n\r\nNote: it looks like we're going to try to release 1.5.0 with the fix anyway. I will keep this PR available until that's finished.", "comments": ["@angersson 'I think it would be sad if we disabled this operation in 1.5. ", "@rmlarsen Right. Ideally, this PR will be dropped. I've copied you on the internal thread about this.", "@angersson You can disregard my comment. I initially read this as float16, a.k.a. fp16, a.k.a. Eigen::half, a.k.a. half in Cuda. I don't have much of an opinion about disabling sparse matmul for bfloat16 or not. Disabling float16 would be sad."]}, {"number": 16380, "title": "Branch 183148922", "body": "", "comments": []}, {"number": 16379, "title": "No module named 'tensorflow'. Anaconda+windows10+tensorflow-gpu+cuda8+cudnn6", "body": "I had anaconda on my windows 10.\r\nI installed CUDA 8.0 with cuDNN 6 and then followed [http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html](url) to activate tensorflow-gpu environment. Now when I import tensorflow in the console, it works but with jupyter notebook opened right in this environment it throws the error. I even upgraded setuptools as mentioned in a previous issue.\r\n![capture5](https://user-images.githubusercontent.com/10391022/35360420-a5cd8f98-0183-11e8-919a-53da56df5ee8.JPG)\r\n![capture6](https://user-images.githubusercontent.com/10391022/35360450-c27e44e8-0183-11e8-9c0a-5aaaa05000c4.JPG)\r\n![capture7](https://user-images.githubusercontent.com/10391022/35360455-c7e24a42-0183-11e8-964b-7186fb233078.JPG)\r\n![capture8](https://user-images.githubusercontent.com/10391022/35360549-174292c2-0184-11e8-9d6a-93ba6139a275.JPG)\r\n\r\n\r\n", "comments": ["If, instead of just typing `jupyter` on command line from inside your active conda environment, you type `jupyter --paths` is it listing paths in the virtual environment?", "I don't think so.\r\nI just started my computer and did `activate tensorflow-gpu`. Do I need to create a conda env?\r\n![capture5](https://user-images.githubusercontent.com/10391022/35377497-6711e3f6-01d5-11e8-9688-d50af933a6cb.JPG)\r\n", "I realised jupyter is not installed in the tensorflow-gpu environment so I did `conda install jupyter` and now the `--path` shows the env. When I see`conda list` I realize many packages missing and everytime I have to install, I have to download, which consumes data. Is there a way to download once and install into every new environment I create? \r\nBtw,  now TF can be imported in jupyter, but still not in spyder. I guess I need to download and install spyder in this environment again? (I feel like there is a better solution)"]}, {"number": 16378, "title": "Update version names to 1.5.0 from 1.5.0-rc1", "body": null, "comments": []}, {"number": 16377, "title": "R1.4", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "r1.4 was already merged back to master"]}, {"number": 16376, "title": "Apply non-bfloat-related final 1.5.0 cherry-picks.", "body": null, "comments": []}, {"number": 16375, "title": "Branch 183115307", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@yongtang this PR contains a modified version of your PR #16101. Could you please approve this or acknowledge that you are OK with including the code.", "Thanks @rmlarsen. Looks good to me \ud83d\udc4d ", "@yongtang Thanks!"]}, {"number": 16374, "title": "Windows does not build tflite: No module named 'tensorflow.contrib.lite.toco.python'", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Build 16299.192  and Windows 7\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0rc1 and tf-nightly  1.6.0.dev20180124\r\n- **Python version**: 3.6.2 and 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: Nvidia GT 740M 2GB\r\n- **Exact command to reproduce**: toco --help\r\n\r\n### Describe the problem\r\nI am trying to run the codelab tutorial of tensorflow lite. After installing tf-nightly, when I try to run the command \"toco --help\", I get the error ModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python'.\r\n\r\nI have tried this on 3 computers( all Windows) and the same problem persists.\r\n\r\n\r\n### Source code / logs\r\nC:\\Users\\HP\\Downloads>toco --help\r\nTraceback (most recent call last):\r\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"c:\\programdata\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\toco.exe\\__main__.py\", line 5, in <module>\r\nModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python'\r\n", "comments": ["Did you build the toco library using bazel? What worked for me is (from within the TF repo folder)\r\n\r\n`bazel build tensorflow/contrib/lite/toco:toco`\r\n\r\n`bazel-bin/tensorflow/contrib/lite/toco/toco --help`", "No, I didn't build the tensorflow repository but installed tf-nightly from pip.\r\nI tried to build toco using bazel, but the build failed. Here's the traceback:\r\n\r\n```\r\nC:\\Users\\HP\\Documents\\tensorflow>bazel build tensorflow/contrib/lite/toco:toco --verbose_failures\r\nDEBUG: C:/users/hp/appdata/local/temp/_bazel_hp/izh3tvro/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:37:3:\r\nAuto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.\r\nDEBUG: C:/users/hp/appdata/local/temp/_bazel_hp/izh3tvro/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:37:3:\r\nAuto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables,eg. VS140COMNTOOLS\r\nDEBUG: C:/users/hp/appdata/local/temp/_bazel_hp/izh3tvro/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:37:3:\r\nAuto-Configuration Warning: Visual C++ build tools found at C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\\r\nINFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: C:/users/hp/documents/tensorflow/tensorflow/contrib/lite/toco/BUILD:151:1: C++ compilation of rule '//tensorflow/contrib/lite/toco:toco_port' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/hp/appdata/local/temp/_bazel_hp/izh3tvro/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\WINDOWS\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;;C:\\WINDOWS\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET TEMP=C:\\Users\\HP\\AppData\\Local\\Temp\r\n    SET TMP=C:\\Users\\HP\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /c tensorflow/contrib/lite/toco/toco_port.cc /Fobazel-out/x64_windows-opt/bin/tensorflow/contrib/lite/toco/_objs/toco_port/tensorflow/contrib/lite/toco/toco_port.o /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/genfiles/external/com_google_absl /Iexternal/gif_archive /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive /Iexternal/jpeg /Ibazel-out/x64_windows-opt/genfiles/external/jpeg /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/genfiles/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/genfiles/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/genfiles/external/highwayhash /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Iexternal/bazel_tools/tools/cpp/gcc3 /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/gif_archive/lib /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/lib /Iexternal/gif_archive/windows /Ibazel-out/x64_windows-opt/genfiles/external/gif_archive/windows /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/farmhash_archive/src /Iexternal/png_archive /Ibazel-out/x64_windows-opt/genfiles/external/png_archive /Iexternal/zlib_archive /Ibazel-out/x64_windows-opt/genfiles/external/zlib_archive /DEIGEN_MPL2_ONLY /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTENSORFLOW_USE_ABSL /DTF_USE_SNAPPY /showIncludes /MD /O2 /DNDEBUG\r\ntensorflow/contrib/lite/toco/toco_port.cc(164): error C3861: 'open': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(173): error C3861: 'read': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(176): error C3861: 'close': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(180): error C3861: 'close': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(193): error C3861: 'open': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(201): error C3861: 'write': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(203): error C3861: 'close': identifier not found\r\ntensorflow/contrib/lite/toco/toco_port.cc(208): error C3861: 'close': identifier not found\r\nTarget //tensorflow/contrib/lite/toco:toco failed to build\r\nINFO: Elapsed time: 4.349s, Critical Path: 2.64s\r\nFAILED: Build did NOT complete successfully\r\n```", "I'd suggest instead of doing pip install, git clone the TF repo and run the bazel build toco command from the root of the TF repo. If there are no issues it takes about 10-15 min to build the binary.", "Yes, I would certainly like to do that, but building tensorflow from source on Windows gives a lot of errors (with Visual Studio and Bazel) like the one above. I would try to do it in a linux environment at a later date.", "Sorry the issue is that toco is currently not supported on Windows build for cmake. I will look into adding this.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Am geetting TOCO_CONVERT error. . .Kindlyhelp. . .\r\n\r\n**AttributeError: module 'tensorflow.contrib.lite.python.lite' has no attribute 'toco_convert'**\r\n_see last two  lines for error causing code_\r\n//=================================\r\nimport tensorflow  as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nimport seaborn as sns\r\nimport matplotlib as lib\r\n\r\nfrom tensorflow.python.tools import freeze_graph\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\n\r\n\r\nprint(tf.__version__)\r\ndata=pd.read_csv('C:\\ml\\irisInAndroid\\iris.data',names=['f1','f2','f3','f4','f5'])\r\n\r\ns=np.asarray([1,0,0])\r\nve=np.asarray([0,1,0])\r\nvi=np.asarray([0,0,1])\r\n\r\ndata['f5']=data['f5'].map({'Iris-setosa':s,'Iris-versicolor':ve,'Iris-virginica':vi})\r\n\r\n\r\ndata=data.iloc[np.random.permutation(len(data))]\r\n\r\nprint(data)\r\n\r\ndata=data.reset_index(drop=True)\r\n\r\ntrainFeats=data.ix[0:105,['f1','f2','f3','f4']]\r\ntemp=data['f5']\r\ntrainlabels=temp[0:106]\r\n\r\ny=tf.placeholder(tf.float32,shape=[None, 3])\r\n\r\nm=tf.Variable(tf.zeros([4,3]))\r\nx=tf.placeholder(tf.float32,shape=[None,4],name=\"Input\")\r\nc=tf.Variable(tf.zeros([3]))\r\n\r\nmxc = tf.nn.softmax((tf.matmul(x, m) + c) ,name=\"output\")\r\n\r\nloss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(mxc), reduction_indices=[1]))\r\n\r\ntrain_step = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\nsess = tf.InteractiveSession()\r\ninit = tf.initialize_all_variables()\r\nsess.run(init)\r\n\r\n\r\n\r\n\r\nepoch=2000\r\nfor step in range(epoch):\r\n  print(sess.run([train_step,loss], feed_dict={x: trainFeats, y:[t for t in trainlabels.as_matrix()]}))\r\n\r\ntestData=data.ix[130,['f1','f2','f3','f4']]\r\ntestDataInFrormat=testData.reshape(1,4)\r\nprint(sess.run(tf.argmax(mxc),feed_dict={x:testDataInFrormat}))\r\n\r\ntf.train.write_graph(sess.graph_def,'pbtxtFiles/','savegraph.pbtxt',as_text=True)\r\n\r\ntf.train.Saver().save(sess,'pbtxtFiles/model.ckpt')\r\n\r\nMODEL_NAME = 'iris'\r\ninput_graph_path = 'pbtxtFiles/savegraph.pbtxt'\r\ncheckpoint_path = 'pbtxtFiles/model.ckpt'\r\ninput_saver_def_path = \"\"\r\ninput_binary = False\r\noutput_node_names = \"output\"\r\nrestore_op_name = \"save/restore_all\"\r\nfilename_tensor_name = \"save/Const:0\"\r\noutput_frozen_graph_name = 'pbtxtFiles/frozen_model_'+MODEL_NAME+'.pb'\r\noutput_optimized_graph_name = 'pbtxtFiles/optimized_inference_model_'+MODEL_NAME+'.pb'\r\nclear_devices = True\r\n\r\nfreeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\n                          input_binary, checkpoint_path, output_node_names,\r\n                          restore_op_name, filename_tensor_name,\r\n                          output_frozen_graph_name, clear_devices, \"\")\r\n\r\n\r\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n        sess.graph_def,\r\n        [\"Input\"], # an array of the input node(s)\r\n        [\"output\"], # an array of output nodes\r\n        tf.float32.as_datatype_enum)\r\n\r\ntflite_model=tf.contrib.lite.toco_convert(sess.graph_def,[x],[mxc])\r\nopen(\"wow.tflite\",\"w\").write(tflite_model)\r\n\r\n@Azureum @ushnish @aselle @tensorflowbutler @tatianashp @d0k @e-lin @fabriciojoc @gaohuazuo @HaebinShin @iamaziz @JacksonKontny @k-w-w @lahwran @m-colombo @n3011 @oahziur @paderijk @qbx2 @rachellim @saeta @t13m @ucdmkt @vade @wagonhelm @xiaoyaozhuzi @yacoder @zackchase \r\nam getting .pb abd .ckpt", "has this problem been solved ? ", "nope @StarOverSeas . . . it works fine in Ubuntu but sucks in windows. . .", "why was I mentioned on this issue?", "Sorry Lawren. . . Accept my humble apologies. . . If possible kindly look\nat this issue. . . https://github.com/tensorflow/tensorflow/issues/17349\nand help me if possible. . .\n", "when will this be solved ?\r\n", "While I have previously contributed code to tensorflow, I am not an ongoing team member; I work for an unrelated company doing unrelated things, and my contributions to tensorflow have been trivial fixes to issues I noticed in my spare time. Please give the tensorflow team time to see this thread and work through the issue, and be aware that if you found me to ping, you probably also pinged other people who are not core team members.", "Any idea when will this be solved?  Is there any temporary workaround that we could use?", "Guys I've got mine running on Linux. I don't think windows as of now supports Toco but Linux does. So just use it on Linux, problem solved for now ", "Thank you sdsouza33. I also started looking into running it on Linux because there is no info on when it will be supported on Windows", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@aselle any update on how to get this working ??\r\n@ushnish Hi does git cloning the TF repo and running the bazel build toco command from the root of the TF repo work for you ??\r\n", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "\u3160\u3160", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think we can close this", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Using Linux is not an ultimate solution", "Duplicate of #20975.", "@ushnish hi, I am following the instructions at:\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2\r\nFor CentOS, I have installed Bazel successfully when using your suggestions:\r\n```\r\nbazel build tensorflow/contrib/lite/toco:toco\r\nbazel-bin/tensorflow/contrib/lite/toco/toco --help\r\n```\r\nbut when I run the command to convert model to TFLite:\r\n```\r\nIMAGE_SIZE=224\r\ntoco \\\r\n  --input_file=tf_files/retrained_graph.pb \\\r\n  --output_file=tf_files/optimized_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n  --input_array=input \\\r\n  --output_array=final_result \\\r\n  --inference_type=FLOAT \\\r\n  --input_data_type=FLOAT\r\n```\r\nI got error: \r\n\r\n```\r\n[nhandt@dev-bigdata02 tensorflow]$ toco \\\r\n>   --input_file=retrained_graph.pb \\\r\n>   --output_file=optimized_graph.lite \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n>   --input_array=input \\\r\n>   --output_array=final_result \\\r\n>   --inference_type=FLOAT \\\r\n>   --input_data_type=FLOAT\r\n/home/nhandt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nTOCO from pip install is currently not working on command line.\r\nPlease use the python TOCO API or use\r\nbazel run tensorflow/contrib/lite:toco -- <args> from a TensorFlow source dir.\r\n```\r\nThen I changed to run toco from source dir:\r\n\r\n```\r\n[nhandt@dev-bigdata02 tensorflow]$ IMAGE_SIZE=224\r\n[nhandt@dev-bigdata02 tensorflow]$ bazel run tensorflow/contrib/lite:toco \\\r\n>   --input_file=retrained_graph.pb \\\r\n>   --output_file=optimized_graph.lite \\\r\n>   --input_format=TENSORFLOW_GRAPHDEF \\\r\n>   --output_format=TFLITE \\\r\n>   --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n>   --input_array=input \\\r\n>   --output_array=final_result \\\r\n>   --inference_type=FLOAT \\\r\n>   --input_data_type=FLOAT\r\n```\r\n**ERROR: Unrecognized option: --input_file=retrained_graph.pb**\r\nretrained_graph.pb is under tensorflow directory: tensorflow/retrained_graph.pb\r\nPlease advise me how to run this script with toco. Thanks\r\n", "Hi @AliceDinh, when you execute a binary via `bazel run`, you need to insert `--` before any command-line arguments you want to pass to the target binary. Try changing\r\n\r\n`bazel run tensorflow/contrib/lite:toco --input_file=...`\r\n\r\nto\r\n\r\n`bazel run tensorflow/contrib/lite:toco -- --input_file=...`", "@aselle  hi, wheather the toco is currently  supported on Windows ? i still met the wrong on Windows.\r\nRuntimeError: TOCO failed see console for info.\r\nb'e:\\\\anaconda3\\\\lib\\\\site-packages\\\\h5py\\\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\r\\n  from ._conv import register_converters as _register_converters\\r\\nTraceback (most recent call last):\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\imp.py\", line 297, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"E:\\\\Anaconda3\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"e:\\\\anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone", "> \r\n> \r\n> @aselle hi, wheather the toco is currently supported on Windows ? i still met the wrong on Windows.\r\n> RuntimeError: TOCO failed see console for info.\r\n> b'e:\\anaconda3\\lib\\site-packages\\h5py\\**init**.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\\r\\n from ._conv import register_converters as _register_converters\\r\\nTraceback (most recent call last):\\r\\n File \"e:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n fp, pathname, description = imp.find_module('_tensorflow_wrap_toco', [dirname(**file**)])\\r\\n File \"e:\\anaconda3\\lib\\imp.py\", line 297, in find_module\\r\\n raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named '_tensorflow_wrap_toco'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n File \"e:\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\\r\\n \"**main**\", mod_spec)\\r\\n File \"e:\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\\r\\n exec(code, run_globals)\\r\\n File \"E:\\Anaconda3\\Scripts\\toco_from_protos.exe\\**main**.py\", line 5, in \\r\\n File \"e:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\toco_from_protos.py\", line 22, in \\r\\n from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n File \"e:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py\", line 28, in \\r\\n _tensorflow_wrap_toco = swig_import_helper()\\r\\n File \"e:\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\lite\\toco\\python\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named '_tensorflow_wrap_toco'\\r\\n'\r\n> None\r\n\r\nI also have the same problem. Please any solutions?\r\n", "@cjr0106 same problem here. How can we fix this problem?\r\n\r\n> Traceback (most recent call last):\r\n  File \"E:\\adwqe.py\", line 22, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\lite.py\", line 439, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 309, in toco_convert_impl\r\n    input_data.SerializeToString())\r\n  File \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\", line 109, in toco_convert_protos\r\n    (stdout, stderr))\r\nRuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18, in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\imp.py\", line 296, in find_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\runpy.py\", line 193, in _run_module_as_main\\r\\n    \"__main__\", mod_spec)\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\runpy.py\", line 85, in _run_code\\r\\n    exec(code, run_globals)\\r\\n  File \"C:\\\\Users\\\\thang\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\", line 5, in <module>\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 29, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 21, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nImportError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\n\r\n", "Please I am having this same issue too. Pleas help anyone, please help.\n\nOn Tue, 16 Oct 2018, 7:10 a.m. thangvip4321, <notifications@github.com>\nwrote:\n\n> @cjr0106 <https://github.com/cjr0106> same problem here. How can we fix\n> this problem?\n> Traceback (most recent call last): File \"E:\\adwqe.py\", line 22, in\n> <module> tflite_model = converter.convert() File\n> \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\lite.py\",\n> line 439, in convert **converter_kwargs) File\n> \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\",\n> line 309, in toco_convert_impl input_data.SerializeToString()) File\n> \"C:\\Users\\thang\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\contrib\\lite\\python\\convert.py\",\n> line 109, in toco_convert_protos (stdout, stderr)) RuntimeError: TOCO\n> failed see console for info. b'Traceback (most recent call last):\\r\\n File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\",\n> line 18, in swig_import_helper\\r\\n fp, pathname, description =\n> imp.find_module(\\'_tensorflow_wrap_toco\\', [dirname(__file__)])\\r\\n File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\imp.py\",\n> line 296, in find_module\\r\\n raise ImportError(_ERR_MSG.format(name),\n> name=name)\\r\\nImportError: No module named\n> \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception,\n> another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n\n> File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\runpy.py\",\n> line 193, in _run_module_as_main\\r\\n \"__main__\", mod_spec)\\r\\n File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\runpy.py\",\n> line 85, in _run_code\\r\\n exec(code, run_globals)\\r\\n File\n> \"C:\\\\Users\\\\thang\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python35\\\\Scripts\\\\toco_from_protos.exe\\\\__main__.py\",\n> line 5, in <module>\\r\\n File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\",\n> line 22, in <module>\\r\\n from tensorflow.contrib.lite.toco.python import\n> tensorflow_wrap_toco\\r\\n File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\",\n> line 29, in <module>\\r\\n _tensorflow_wrap_toco = swig_import_helper()\\r\\n\n> File\n> \"c:\\\\users\\\\thang\\\\appdata\\\\local\\\\programs\\\\python\\\\python35\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\",\n> line 21, in swig_import_helper\\r\\n import\n> _tensorflow_wrap_toco\\r\\nImportError: No module named \\'_tensorflow_wr\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16374#issuecomment-430111832>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/Aiv7orTpN8xcI7BMDy4WLbGzBP3cT64Aks5ulXhugaJpZM4RryXV>\n> .\n>\n", "https://stackoverflow.com/questions/48435006/modulenotfounderror-no-module-named-tensorflow-contrib-lite-toco-python/54355457#54355457"]}, {"number": 16373, "title": "Allowing override of common_env.sh python directory.", "body": "PiperOrigin-RevId: 180806246", "comments": []}, {"number": 16372, "title": "Revert #15967. Reduce pip package size.", "body": "", "comments": []}, {"number": 16371, "title": "Tegra Nvidia Jetson TX2 build python 2.7 new CUDA and CUDNN", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux4Tegra 28.2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5-rc1\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.9\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: Denver2 8GB\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nClean installation with the new CUDA 9 and cudnn 7 from nvidia\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nCollecting system information...\r\nTraceback (most recent call last):\r\n  File \"/tmp/check_tf.py\", line 1, in <module>\r\n    import tensorflow as tf;\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\nWrote environment to tf_env.txt. You can review the contents of that file.\r\nand use it to populate the fields in the github issue template.\r\n\r\ncat tf_env.txt\r\n\r\n```\r\n", "comments": ["@Davidnet can you provide more details on how your built / installed from source?\r\n\r\nIn particular, what did you specify during ./configure?\r\nhttps://www.tensorflow.org/install/install_sources#configure_the_installation", "```\r\nubuntu@tegra-ubuntu:~/Documents/programs/tensorflow$ ./configure\r\nYou have bazel 0.9.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]:\r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:\r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]:\r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]:\r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]:\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]:\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]:\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL support? [y/N]:\r\nNo OpenCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 8.0]: 9.0\r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 6.0]: 7\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.2\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]:\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]:\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nAdd \"--config=mkl\" to your bazel command to build with MKL support.\r\nPlease note that MKL on MacOS or windows is still not supported.\r\nIf you would like to use a local MKL instead of downloading, please set the environment variable \"TF_MKL_ROOT\" every time before build.\r\nConfiguration finished\r\n", "Same with 1.4\r\n\r\n```\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml\r\n\r\n```", "@Davidnet Here's a thought: demangling `_ZN3Aws4Time9LocalTimeEP2tml` (the symbol that's not found) results in `Aws::Time::LocalTime(tm*, long)`.\r\n\r\nDo you actually need aws support?  Otherwise, perhaps you can retry ./configure and answer \"no\" to this question:\r\n`Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]:`\r\n\r\nAlong the same lines, if you're not using Hadoop or Google Cloud Platform, you can say \"no\" for those too, to avoid unnecessary dependencies, and to get faster build times and smaller binaries.", "Solve it, thank you!, in the way of learning? Would you mind telling me how to demangle not founded symbols? Thank you. ", "Cool, glad that helped.  :)\r\n\r\nI found a demangler by searching [demangle c++ symbol] on google:\r\nhttps://www.google.com/search?q=demangle+c%2B%2B+symbol\r\n\r\nAnd I picked the first link, which seemed reasonable:\r\nhttps://demangler.com/"]}, {"number": 16370, "title": "add hooks for mutate variables in tf.Estimator", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.0.0 (clang-800.0.42.1)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.3)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-serving-api (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nRight now there is no elegant way we can mutate variable in the `tf.estimator.Estimator` \r\nAnd we do have some situation that we want to discard the checkpoints but save the variable in other format fits better with our infra.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n", "comments": ["@yupbank Could you please post an example of the functionality that you are asking for?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 16369, "title": "Unittesting Models with Tensorflow - How to clear the existing graph ?", "body": "Hello dear tensorflowers,\r\n\r\nI have already asked the question of [StackOverflow](https://stackoverflow.com/questions/48421308/tensorflow-and-unittests-layer-already-defined), however it seams like nobody can answer my question.\r\n\r\nSo I hope you will forgive me about reposting it here:\r\n\r\nI am developing unittests for a product I implemented with TF.\r\n\r\nEach part of the model is tested separately then all together in different conditions.\r\n\r\nLet's take the example of a simple GAN, I have the following tests:\r\n\r\n - **GeneratorTest** Class: With all tests concerning G inside\r\n - **DiscriminatorTest** Class: With all tests concerning D inside\r\n - **GAN_Train_Test** Class: G and D connected all together: 1 training step is tested.\r\n - **GAN_Inference_Test** Class: G and D connecteed all together: 1 inference run is tested.\r\n\r\n------------\r\n\r\nWhen the files are executed independently, everything is working nicely and fine. Tests are all fine.\r\n\r\nProblems start occuring when I try to create one file to launch them all from one master file.\r\n\r\n**master_test_launcher.py:**\r\n\r\n```python\r\nimport unittest\r\nimport time\r\n    \r\nimport tensorflow as tf\r\n    \r\nfrom tests.test_generator import GeneratorTest\r\nfrom tests.test_discriminator import DiscriminatorTest\r\nfrom tests.test_anovae_model import GAN_Train_Test\r\nfrom tests.test_inference import GAN_Inference_Test\r\n    \r\nrunner = unittest.TextTestRunner(verbosity=2)\r\n    \r\nif __name__ == '__main__':\r\n   tf.logging.set_verbosity(tf.logging.DEBUG)\r\n    \r\n    for test in [GeneratorTest, DiscriminatorTest, GAN_Train_Test, GAN_Inference_Test]:\r\n        tf.logging.debug(\"Running tests for: %s ...\" % test.__str__())\r\n    \r\n        tf.reset_default_graph()\r\n    \r\n        time.sleep(2)\r\n    \r\n        test_suite = unittest.TestSuite()\r\n        test_suite.addTest(unittest.makeSuite(test))\r\n    \r\n        runner.run(test_suite)\r\n```\r\n\r\nI repeatedly obtain the same error when I run the tests related to G and D connected together: \r\n\r\n```python\r\nException: Layer 'encoder/input' already exists, please choice other 'name' or reuse this layer\r\nHint : Use different name for different 'Layer' (The name is used to control parameter sharing)\r\n```\r\n\r\nThe error is quite simple to understand, each test file is independant and thus create its own session and graph. While testing only G or D, there is no problem because they have different name_scope/variable_scope. However, when testing the whole model *Layers* already have been defined by previous tests and thus leading to issue.\r\n\r\nI would like to find a way to completely drop the graph and reset the whole TF state as **brand new and clean**. However, everything I try seem to  fail.\r\n\r\nI would like to avoid creating a new graph for each test, leaving the old one in memory (could lead to very high amount of memory waste after a few tests).\r\n\r\n\r\nSo my question is easy: ** How can I reset the whole TF state and internal vars as \"clean\" as if you relaunch a new python shell ? By some black-magic I can't find any way doing it (after looking for it for hours).\r\n\r\nFor information here are the things I tried and which failed:\r\n- tf.reset_default_graph()\r\n- cleaning everything in graph collections\r\n- creating a new graph + new session before executing each Test File: A graph is still built somewhere containing my Layers and I can't manage to find it.\r\n- reading the TF code and trying to find any __exit__ or close function which I didn't find\r\n\r\nThanks a lot,\r\n\r\nJonathan D.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\n(Might I suggest waiting more than a couple of hours for someone to respond on StackOverflow :))", "@DEKHTIARJonathan even though this topic is closed, but you might want to reset the graph in your `tearDown` method like the code snippet below. This will insure you start every test with a clean graph.\r\n\r\n```python\r\ndef tearDown(self):\r\n    tf.reset_default_graph()\r\n```", "@pechyonkin that's the code I use today \ud83d\udc4d", "@DEKHTIARJonathan so this solution does solve your problem? I saw your post on Stackoverflow and noticed that you do use `tf.reset_default_graph()` in your main loop. But it wasn't clear to me if that was the right way to do it.", "Yeap if does solve the problem "]}, {"number": 16368, "title": "Fix of issue #13164 (Merges #13382)", "body": "Deconflicts #13382\r\nFixes #13164\r\n@dantkz /cc\r\n@ebrevdo /cc", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Could the authors please sign confirm in this thread that they agree to contribute their code to TensorFlow?", "@Androbin Before we can merge, we need all authors to confirm in this thread that they are OK with submitting the code to TensorFlow. ", "@rmlarsen Sure. @dantkz Are you okay with it?", "@Androbin I'm okay with it.", "@Androbin @dantkz Thanks.", "@dantkz Can we leave `testBadIndicesGPU` like this?", "@Androbin No, this `testBadIndicesGPU` will fail on GPU, because scatter_nd and gather_nd do not check the boundaries on GPU for efficiency.", "@ebrevdo Is there a way to catch the CUDA error or shall we disable for now?", "@dantkz Why doesn't this fail for current `master` then?", "@Androbin Last time I checked (a while back), the master branch does the test on CPU only.\r\nSee discussion here: https://github.com/tensorflow/tensorflow/issues/3638", "> Last time I checked (a while back), the master branch does the test on CPU only.\r\n\r\n@dantkz Shouldn't [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/gather_nd_op_test.py#L201) make it use the GPU?\r\n`with self.test_session(use_gpu=True):`\r\n\r\nDoesn't seem to crash there...", "> @dantkz Shouldn't this line make it use the GPU?\r\n> with self.test_session(use_gpu=True):\r\n> \r\n> Doesn't seem to crash there...\r\n\r\nI think `(use_gpu=True)` makes the op run on GPU only if the corresponding GPU kernel exists. There is a separate flag `(force_gpu=True)`.\r\nIn the test code, the inputs are ints, and master branch only has CPU kernel for ints. Again, I'm basing this on the master branch from a while back.\r\n", "Okay, disabling those GPU tests for now.", "@ebrevdo another look, please?", "Looks like you're adding a declaration for some higher level template but\nnot for an implementation.  The demangled error:\n\ntensorflow::functor::ScatterNdFunctor<Eigen::GpuDevice, int, int,\n(tensorflow::scatter_nd_op::UpdateOp)0, 6>::operator()(Eigen::GpuDevice\nconst&, int, Eigen::array<long, 6ul>, Eigen::TensorMap<Eigen::Tensor<int,\n2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int\nconst, 2, 1, long>, 16, Eigen::MakePointer>,\nEigen::TensorMap<Eigen::Tensor<int const, 2, 1, long>, 16,\nEigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16,\nEigen::MakePointer>)\n\nPerhaps you forgot to remove a TF_call somewhere?  Here it looks like you\nstill have a TF_call for int type on GPU.  You removed it somewhere in the\nimpl but not in the declaration.\n\n\nOn Mon, Jan 29, 2018 at 12:04 PM, Robin Richtsfeld <notifications@github.com\n> wrote:\n\n> We're getting this weird error all over the place:\n> ImportError: /tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-\n> opt/bin/tensorflow/examples/adding_an_op/cuda_op_test.\n> runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so:\n> undefined symbol: _ZN10tensorflow7functor16Scatte\n> rNdFunctorIN5Eigen9GpuDeviceEiiLNS_13scatter_nd_\n> op8UpdateOpE0ELi6EEclERKS3_iNS2_5arrayIlLm6EEENS2_9TensorMapINS2_\n> 6TensorIiLi2ELi1ElEELi16ENS2_11MakePointerEEENSB_INSC_\n> IKiLi2ELi1ElEELi16ESE_EESI_SF_\n>\n> The last part looks somewhat like tensorflow functor ScatterNdFunctor\n> Eigen GpuDevice scatter_nd_op UpdateOp array TensorMap Tensor MakePointer\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/16368#issuecomment-361368991>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim2SbHb3zuntkbTIgozyNwC6_5ji9ks5tPiRlgaJpZM4Rrbx3>\n> .\n>\n", "I see. @dantkz do we yet have CudaAtomic* kernels for int64?\r\nIn other words, should we comment [DECLARE_GPU_SPECS](https://github.com/tensorflow/tensorflow/pull/16368/files#diff-5d2c10864fe0081c8272db95dd2fad87R49) or uncomment [DEFINE_GPU_SPECS](https://github.com/tensorflow/tensorflow/pull/16368/files#diff-6e486a435530b37fa19b6d23d1fea1ceR41)?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@Androbin I think you've hit the issue that stopped me after I had merged master branch. Gather_nd and scatter_nd stopped working with int64 declarations somewhere between tf.1.3 and tf1.4 versions. I can't figure out which of the CudaAtomic* kernels is causing the problem. Can you check if int32 compiles and it is int64 kernel missing?", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This time, I commented all int64 versions. If it works now, we can start re-adding them.", "@ebrevdo another look?", "Nagging Reviewer : It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer : It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Any updates? @ebrevdo \r\nIt has been __eight__ months since the original PR.", "@rmlarsen The CLA-Bot has a hiccup again.\r\n@ebrevdo Would you mind elaborating on https://github.com/tensorflow/tensorflow/pull/16368#issuecomment-361370094?\r\n\r\nHere are the logs from the last build:\r\nhttps://source.cloud.google.com/results/invocations/edcda7d3-0a86-4618-b940-91951312aeb3/log", "@Androbin You declared the scatter_nd op with int32, but did not declare or define the corresponding kernels.\r\nMaybe let's do one thing at a time. Removing the changes on scatter_nd op should at least make it build.", "Could you run the builds and tests locally? Otherwise the progress will be really slow.\r\nI did see cuda atomic error with int64 now, but it only happens with scatter and scatter_nd. Gather with int64 can build.", "Okay, I have removed `int64` versions of `scatter` and `scatter_nd`:\r\n* `tf.gather` now supports `int64` on GPU\r\n* `tf.gather_nd` now supports `int32` and `int64` on GPU\r\n* `tf.scatter_nd` now supports `int32` on GPU\r\n\r\n@ppwwyyxx I'm trying to test locally, but I seem to be doing it wrong...", "@ebrevdo Could you clarify if the `int32` `_nd` versions are fine?", "I'm okay with it. We have a couple of important GPU workloads e.g. those that deal with text like the transformer model where this may create a performance regression.  It's hard to tell where this would make a difference.  I say we push the _nd int32 versions and roll back if we see regressions.", "@rmlarsen I think we've fixed the bemangled errors. Let's restart the builds.", "@Androbin You'll probably need https://github.com/ppwwyyxx/tensorflow/commit/2919f8202fb09a335637cfffe15535e27f115f1a#diff-5f4f48f44c131a218fdb75162e12b138 for the tests to pass.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@Androbin Could you please fix the linter errors: https://source.cloud.google.com/results/invocations/c0838386-0b52-4be9-843a-90c453b7ea76/log", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@rmlarsen Seems like the tests have passed but the CI reports some unknown error.", "Well, maybe if we keep running it :-/", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->", "@Androbin thanks for your patience.\r\n"]}, {"number": 16367, "title": "Andrewharp patch 1", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16366, "title": "I have an issue with http://projector.tensorflow.org/ always getting stuck", "body": "Can anyone help me with this issue? I start up the _**Visualizing High-Dimensional Space**_ webstie and it loads until it says something about metadata and never does anything from there. Plese, help. I'm so confused?", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 16364, "title": "Standardizing the saved format and/or converting to big-endian on read", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 s390x\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.1\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:framework_meta_graph_test which invokes meta_graph.import_scoped_meta_graph.\r\n\r\n### Describe the problem\r\nThe testdata for //tensorflow/python:framework_meta_graph_test is not platform independent and causes test to fail on Big Endian systems.\r\n\r\nAs per discussion in #16003 , correct approach would be standardizing the stored format and/or conversion on load based on endianness. Can someone have a look?  \r\n", "comments": ["For context, this has come up before (see https://github.com/tensorflow/tensorflow/issues/11290#issuecomment-321380067 and https://github.com/tensorflow/tensorflow/issues/11290#issuecomment-321396972 for example).", "Thank you @asimshankar!\r\n\r\nYes, I think we should just standardize on little endian storage format and byteswap on load for big endian. Feel free to send a PR.", "As discussed on the mailing list (https://groups.google.com/a/tensorflow.org/d/msg/developers/GOzGNx8GnHg/oxBFij5ACAAJ), I have implemented a fix for this issue that involves inspecting the `endianness` field of the bundle header on read and byte-swapping tensor values as needed. I should have a PR shortly.", "Looks liked fixed by https://github.com/tensorflow/tensorflow/pull/28490\r\nThis issue can be closed ", "@namrata-ibm @Nayana-ibm @frreiss @asimshankar  I tried loading the model graph on s390x from a protobuf file of the model but it failed. I think the issue is similar to this, can you please look into it. I've described the details here - #41652 "]}, {"number": 16363, "title": "Warning: Table trying to initialize from file ... is already initialized", "body": "Python: 3.6.2\r\nTensorflow: 1.5.0rc1 pip\r\nOS: Windows 10\r\nNo CUDA, just CPU\r\n\r\nI get this when creating a lookup table with `tf.contrib.lookup.index_table_from_file`.\r\nAs I have multiple graphs I create that table in each graph (from the same file) I need it in. This results in the warning from the title of this issue.\r\n\r\n1. Are tables shared across graphs?\r\n2. How to figure out if a table from a specific file is already existing/initialized?\r\n\r\nThis also occurs with `tensorflow/nmt`:\r\nhttps://github.com/tensorflow/nmt/issues/234", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Any ideas?", "Please fill out the template, as @tensorflowbutler suggested. Also can you provide a short code sample to reproduce? Thank you.", "This has been a while. I'll try to add everything necessary.\r\nThe nmt code is a code sample. Not short though ;)", "It's very hard to debug this without a short code sample. I'll ping the authors of the nmt code in the nmt issue though.\r\n\r\n/CC @ysuematsu, any ideas what the issue could be?", "Steps to reproduce:\r\n1. Create a file called `demo.txt` and add the following lines:\r\n```\r\nhi\r\nho\r\nha\r\n```\r\n2. Run the following code:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntable = tf.contrib.lookup.index_table_from_file('demo.txt')\r\ns = tf.Session()\r\ns.run(tf.tables_initializer())\r\ns.run(tf.tables_initializer())\r\n```\r\nThis will print something like\r\n`<DATE_OMITTED>: I tensorflow/core/kernels/lookup_util.cc:373] Table trying to initialize from file demo.txt is already initialized.`", "I don't see a way to track the initialized status of the table.", "This also prints the warning\r\n```python\r\nimport tensorflow as tf\r\n\r\ntable = tf.contrib.lookup.index_table_from_file('demo.txt')\r\ns = tf.Session()\r\ns.run(tf.tables_initializer())\r\ntable = tf.contrib.lookup.index_table_from_file('demo.txt')\r\ns.run(table.init)\r\n```", "@sleighsoft ,\r\nPlease check the updated guide here https://www.tensorflow.org/guide/migrate\r\n\r\nClosing this issue as this was resolved. Thanks!"]}, {"number": 16362, "title": "`tf.foldl` should have more robust input handling (like `tf.scan`)", "body": "### System information\r\n- Windows 10 x64\r\n- Installed from binary\r\n- TensorFlow 1.4.0 (Cpu version)\r\n- Python 3.6.1\r\n\r\n### Bug Description\r\n`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):\r\n\r\n```\r\ndef foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\r\n          swap_memory=False, name=None):\r\n  if not callable(fn):\r\n    raise TypeError(\"fn must be callable.\")\r\n\r\n  with ops.name_scope(name, \"foldl\", [elems]):\r\n    # Any get_variable calls in fn will cache the first call locally\r\n    # and not issue repeated network I/O requests for each iteration.\r\n    varscope = vs.get_variable_scope()\r\n    varscope_caching_device_was_none = False\r\n    if varscope.caching_device is None:\r\n      # TODO(ebrevdo): Change to using colocate_with here and in other methods.\r\n      varscope.set_caching_device(lambda op: op.device)\r\n      varscope_caching_device_was_none = True\r\n\r\n    # Convert elems to tensor array.\r\n    elems = ops.convert_to_tensor(elems, name=\"elems\")\r\n    n = array_ops.shape(elems)[0]\r\n    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\r\n                                            dynamic_size=False,\r\n                                            infer_shape=True)\r\n    elems_ta = elems_ta.unstack(elems)\r\n\r\n    if initializer is None:\r\n      a = elems_ta.read(0)\r\n      i = constant_op.constant(1)\r\n    else:\r\n      a = ops.convert_to_tensor(initializer)\r\n      i = constant_op.constant(0)\r\n\r\n    def compute(i, a):\r\n      a = fn(a, elems_ta.read(i))\r\n      return [i + 1, a]\r\n    _, r_a = control_flow_ops.while_loop(\r\n        lambda i, a: i < n, compute, [i, a],\r\n        parallel_iterations=parallel_iterations,\r\n        back_prop=back_prop,\r\n        swap_memory=swap_memory)\r\n\r\n    if varscope_caching_device_was_none:\r\n      varscope.set_caching_device(None)\r\n    return r_a\r\n\r\n```\r\n\r\nModifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the \"useModifications\" flag:\r\n\r\n```\r\ndef foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,\r\n          swap_memory=False, name=None):\r\n    if not callable(fn):\r\n        raise TypeError(\"fn must be callable.\")\r\n\r\n    with ops.name_scope(name, \"foldl\", [elems]):\r\n        # Any get_variable calls in fn will cache the first call locally\r\n        # and not issue repeated network I/O requests for each iteration.\r\n        varscope = vs.get_variable_scope()\r\n        varscope_caching_device_was_none = False\r\n        if varscope.caching_device is None:\r\n            # TODO(ebrevdo): Change to using colocate_with here and in other methods.\r\n            varscope.set_caching_device(lambda op: op.device)\r\n            varscope_caching_device_was_none = True\r\n\r\n        # Convert elems to tensor array.\r\n        elems = ops.convert_to_tensor(elems, name=\"elems\")\r\n        n = array_ops.shape(elems)[0]\r\n        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\r\n                                                dynamic_size=False,\r\n                                                infer_shape=True)\r\n        elems_ta = elems_ta.unstack(elems)\r\n\r\n        if initializer is None:\r\n            a = elems_ta.read(0)\r\n            i = constant_op.constant(1)\r\n        else:\r\n            useModifications = True\r\n            if useModifications:\r\n                output_is_sequence = nest.is_sequence(initializer)\r\n                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]\r\n                initializer_flat = output_flatten(initializer)\r\n                a = [ops.convert_to_tensor(init) for init in initializer_flat]\r\n            else:\r\n                a = ops.convert_to_tensor(initializer)\r\n\r\n            i = constant_op.constant(0)\r\n\r\n        def compute(i, a):\r\n            a = fn(a, elems_ta.read(i))\r\n            return [i + 1, a]\r\n\r\n        _, r_a = control_flow_ops.while_loop(\r\n            lambda i, a: i < n, compute, (i, a),\r\n            parallel_iterations=parallel_iterations,\r\n            back_prop=back_prop,\r\n            swap_memory=swap_memory)\r\n\r\n        if varscope_caching_device_was_none:\r\n            varscope.set_caching_device(None)\r\n        return r_a\r\n```\r\n\r\nHere is a MWE:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\na = tf.constant( 1, dtype = tf.float32 )\r\nb = tf.constant( 2, dtype = tf.int64   )\r\n\r\nuseTuple = False\r\n\r\ndef body( ab, i ):\r\n    a = ab[0]\r\n    b = ab[1]\r\n    if useTuple:\r\n        return (a,b)\r\n    else:\r\n        return [a,b]\r\n\r\nN = 3\r\nwith tf.Session() as sess:\r\n    if useTuple:\r\n        ab = (a,b)\r\n    else:\r\n        ab = [a,b]\r\n    print( \"new foldl :\", sess.run(   foldl(  body, tf.range(N), ab ) ) )  \r\n    print( \"tf.scan   :\", sess.run( tf.scan(  body, tf.range(N), ab ) ) )\r\n    print( \"tf.foldl  :\", sess.run( tf.foldl( body, tf.range(N), ab ) ) )\r\n```\r\n\r\nwith useTuple = False, this returns \r\n\r\n```\r\nnew foldl : [1.0, 2]\r\ntf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]\r\n# Crash for tf.foldl with error: \r\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)\r\n\r\n```", "comments": ["@mholzel Thank you for suggesting this feature. As you already have the code, please submit it as a pull request."]}, {"number": 16361, "title": "Fix typo", "body": "fix typo", "comments": ["@ManHyuk Thanks for the fix!"]}, {"number": 16360, "title": "Python: Make an alias for \"tf.variable\" (with a lower \"v\") so the naming of it is consistent with \"tf.placeholder\"/\"tf.constant\"", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nMany developers learn the naming standards of the software so they can write code faster. It does not make any sense to have to things \"tf.placeholder\" and \"tf.Variable\" named using different schema. Constant, Placeholder and Variable are similar entities and can be used interchangeably. They should be named in same style even if tf.Variable is a class. \r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/placeholder\r\nhttps://www.tensorflow.org/api_docs/python/tf/Variable\r\nhttps://www.tensorflow.org/api_docs/python/tf/constant\r\n\r\n### Source code / logs\r\nN/A\r\n", "comments": ["@olegserov Thank you for your feedback. You can use tf.get_variable() instead of tf.Variable() to keep everything lower case.  Please see https://www.tensorflow.org/api_docs/python/tf/get_variable for details on the API."]}, {"number": 16359, "title": "Return type annotation", "body": "Added type annotations in the docstring to the return types of dataset functions.\r\nPresented like this, they can automatically be read by tools (I have tested that this works in PyCharm) to improve auto-completion when coding.\r\n\r\nThis is really useful in case of datasets, because they often result in long chained calls (something like `Dataset.generate...(...).map(...).repeat(...).batch(...)`). With this patch, code completion works after every `.` (again, I have only tested PyCharm).", "comments": ["@ngc92 thanks for the cleanup."]}, {"number": 16358, "title": "Request for updating keras/datasets files to r1.5", "body": "### System information\r\n- **executes Keras sample code imdb_fasttext.py https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py**:\r\n- **Windows 7**:\r\n- **TensorFlow installed from binary**:\r\n- **TensorFlow version 1.5.0rc0**:\r\n- **Python version 3.5.1**: \r\n\r\n### Describe the problem\r\nKeras sample program does not work.\r\n There is a bug for numpy arange method wrong usage.\r\n   (Need to fix from arrange to arange) \r\nThis issue is already solved on master branch. (not in 1.5.0rc1)\r\nWould you update these source codes?\r\n\r\n### Source code / logs\r\nError messages are follows\r\n===\r\nC:\\Users\\sakaia\\work\\tensorflow\\keras>python imdb_fasttext.py\r\nLoading data...\r\nTraceback (most recent call last):\r\n  File \"imdb_fasttext.py\", line 75, in <module>\r\n    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features\r\n)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\_imp\r\nl\\keras\\datasets\\imdb.py\", line 77, in load_data\r\n    indices = np.arrange(len(x_train))\r\nAttributeError: module 'numpy' has no attribute 'arrange'\r\n===\r\n\r\nFollowing are just checking np.arrange (not np.arange)\r\n>git branch r1.5\r\n>grep -rn np.arrange *\r\ntensorflow/python/keras/_impl/keras/datasets/boston_housing.py:51:  indices = np.arrange(len(x))\r\ntensorflow/python/keras/_impl/keras/datasets/reuters.py:76:  indices = np.arrange(len(xs))\r\ntensorflow/python/keras/_impl/keras/datasets/imdb.py:77:  indices = np.arrange(len(x_train))\r\ntensorflow/python/keras/_impl/keras/datasets/imdb.py:82:  indices = np.arrange(len(x_test))\r\n>git branch -\r\n>grep -rn np.arrange *\r\n(This line is intentionally blank)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Part of  parameters are already written, I add it\r\n\r\nBazel version                      NOT used (since it is binary)\r\nCUDA/cuDNN version       NOT used\r\nGPU model and memory   NOT used\r\nExact command to reproduce \r\n\r\n       get file from https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py\r\n       edit import path from keras to tensorflow.python.keras like follows\r\n ```python\r\nfrom tensorflow.python.keras.preprocessing import sequence\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.layers import Embedding\r\nfrom tensorflow.python.keras.layers import GlobalAveragePooling1D\r\nfrom tensorflow.python.keras.datasets import imdb\r\n```\r\n       execute python imdb_fasttext.py\r\n\r\n\r\n\r\n\r\n", "Thanks for letting us know about the typo. It will be fixed in 1.6 release once it comes out. Please work around it till then. \r\n\r\n", "+1 for this... had to hack my installed version to get this working...", "As for me, I am doing like follows as workaround\r\n```Python\r\n# Changed from keras to tensorflow.python.keras\r\nfrom tensorflow.python.keras.preprocessing import sequence\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.layers import Embedding\r\nfrom tensorflow.python.keras.layers import GlobalAveragePooling1D\r\nfrom tensorflow.python.keras.callbacks import TensorBoard\r\n# Followings are workaround for https://github.com/tensorflow/tensorflow/issues/16358\r\nfrom keras.datasets import imdb\r\n```\r\nFor this workaround, it needs to install original keras separately.\r\nRef.\r\n  https://github.com/tensorflow/tensorflow/issues/16532", "here is how i got the same error:\r\n\r\n\r\n\r\n```\r\nfrom tensorflow.python import keras\r\nfrom tensorflow.python.keras.datasets import imdb\r\n\r\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\n```\r\nERROR: \r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ahmed/Code/deep-learning-with-python/classifying-movie-reviews.py\", line 5, in <module>\r\n    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/datasets/imdb.py\", line 77, in load_data\r\n    indices = np.arrange(len(x_train))\r\nAttributeError: module 'numpy' has no attribute 'arrange'\r\n\r\n\r\n\r\n\r\n", "you should change from\r\nfrom tensorflow.python.keras.datasets import imdb\r\nto\r\nfrom keras.datasets import imdb\r\n\r\nbecause of this issue in tensorflow/keras package.\r\noriginal keras (not tensorflow/keras) works fine."]}, {"number": 16357, "title": "Increase tolerance in `losses_impl_test.py`. fixes #16238", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Hi @joel-shor could you please sign the CLA?"]}, {"number": 16356, "title": "How leave only 1 app and how edit drawing boxes?", "body": "", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "As this question is about general Android programming, it would be better asked on StackOverflow where there is a larger support community."]}, {"number": 16355, "title": "minor spelling tweaks for eager execution docs", "body": "", "comments": ["@brettkoonce Thanks for the contribution!"]}, {"number": 16354, "title": "Dataset map func shape inference ", "body": "\r\n------------------------\r\n\r\n### System information\r\n```\r\ntf.VERSION = 1.4.1\r\ntf.GIT_VERSION = v1.4.0-19-ga52c8d9\r\ntf.COMPILER_VERSION = v1.4.0-19-ga52c8d9\r\nSanity check: array([1], dtype=int32)\r\n```\r\n\r\n### Describe the problem\r\n\r\nI use dataset api to build  input_fn  in train and evaluate process\uff0c the input_fn is as follows\r\n```\r\ndef input_fn(data_dir, num_epochs, shuffle, batch_size):\r\n  \"\"\"Generate an input function for the Estimator.\"\"\"\r\n  data_files = tf.gfile.Glob(data_dir)\r\n  assert len(data_files), ('%s has no files!' % data_files) \r\n  \r\n  def parse_line(value):\r\n    columns = module_decode_file.decode_file(value, \r\n            record_defaults=_COLUMN_DEFAULTS,\r\n            output_size=_COLUMN_SIZESt,\r\n            field_outer_delim=',',\r\n            field_inner_delim='%')\r\n    features = dict(zip(_COLUMN_NAMES, columns))\r\n    labels = features.pop('label')\r\n    return features, labels\r\n\r\n  # Extract lines from input files using the Dataset API.\r\n  dataset = tf.data.TextLineDataset(data_files)\r\n  if shuffle:\r\n    dataset = dataset.shuffle(buffer_size=batch_size*3)\r\n  dataset = dataset.map(parse_line, num_parallel_calls=1)\r\n  dataset = dataset.repeat(num_epochs)\r\n  dataset = dataset.batch(batch_size)\r\n\r\n  iterator = dataset.make_one_shot_iterator()\r\n  features, labels = iterator.get_next(\"iterator\")\r\n  return features, labels\r\n```\r\nI use custom op decode_file, which is like tf.decode_csv.\r\n\r\nthe decode_file's SetShapeFn is as follows: \r\n```\r\n    .SetShapeFn([](InferenceContext* c) {\r\n      std::vector<int> output_size;\r\n      c->GetAttr(\"output_size\", &output_size);\r\n\r\n      for (int i = 0; i < c->num_outputs(); ++i) {\r\n        ShapeHandle s = c->MakeShape({DimensionOrConstant(output_size[i])});\r\n        c->set_output(i, s); \r\n      }   \r\n      return Status::OK();\r\n    })  \r\n```\r\nthis custom op  can work well  in train and evaluate ,  but  when it is used for export_savedmodel api, i need modify  SetShapeFn as follows ( to support batch predict)\r\n``` \r\n .SetShapeFn([](InferenceContext* c) {\r\n      std::vector<int> output_size;\r\n      c->GetAttr(\"output_size\", &output_size);\r\n      for (int i = 0; i < c->num_outputs(); ++i) {\r\n          c->set_output(i, c->Matrix(c->Dim(c->input(0), 0), output_size[i]));\r\n      }\r\n      return Status::OK();\r\n}\r\n\r\n# my serving_input_fn\r\ndef string_decode_serving_input_receiver_fn():\r\n  string_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.string)\r\n\r\n  columns = module_decode_file_serve.decode_file_serve(string_placeholder,\r\n          record_defaults=_COLUMN_DEFAULTS,\r\n          output_size=_COLUMN_SIZESt, \r\n          field_outer_delim=',',\r\n          field_inner_delim='%')\r\n  features = dict(zip(_COLUMN_NAMES, columns))\r\n  features.pop('label')\r\n  return tf.estimator.export.ServingInputReceiver(features, string_placeholder)\r\n```\r\n\r\nWhy does dataset map api (dataset = dataset.map(parse_line)) only handle single record?  and pase_line's shape inference does not include batch dimmension\r\nAt present I need use different SetShapeFn in train and export_savedmodel\r\nCan dataset map function add new feature to  handle batch dimmension ?\r\n@mrry  thanks! (sorry for bad description ...\uff09\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "The `Dataset.map()` transformation works with any shape of input or output, and you can use it before or after a `Dataset.batch()` transformation (to handle single elements or batches of elements, respectively).\r\n\r\nI'm not sure what's missing. Can you give an example of what goes wrong, such as a code snippet and an error message?", "mrry, thank your reply.   I put Dataset.map() after Dataset.batch() , it can work for train input_fn and serving input_fn ( use same parse op ), it's my fault!\r\n"]}]