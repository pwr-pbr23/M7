[{"number": 4251, "title": "pip installation 0.10.0 requires cuDNN v5, not cuDNN v4", "body": "[Download and Setup, Pip installation instructions for 0.10.0](https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation) say that `Ubuntu/Linux 64-bit, GPU enabled, Python 2.7` version `Requires CUDA toolkit 7.5 and CuDNN v4.`,\n\nThat setup results in:\n\n`E tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 4007 (compatibility version 4000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.`\n\nEdit Sep 9, 2016: as @vinaynath pointed out, `cudnn-7.5-linux-x64-v5.1.tgz` should be installed for CUDA 7.5\n**Installation of cuDNN v5 (`cudnn-7.5-linux-x64-v5.1.tgz`~~`cudnn-8.0-linux-x64-v5.1`~~) resolves the problem.**\nI think the instructions need an update to `Requires CUDA toolkit 7.5 and CuDNN v5.`.\n### Environment info\n\nOperating System: Ubuntu 14.04.4 LTS (GNU/Linux 3.19.0-68-generic x86_64)\n\nInstalled version of CUDA and cuDNN: CUDA 7.5, cuDNN v4 (`cudnn-7.0-linux-x64-v4.0`)\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /usr/local/cuda/lib/libcud*\n-rw-r--r-- 1 root root 189170 Jul 25 11:00 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jul 25 11:00 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jul 25 11:00 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jul 25 11:00 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jul 25 11:00 /usr/local/cuda/lib/libcudart_static.a\n```\n1. A link to the pip package you installed: `https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl`\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.10.0\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\n$ python -m tensorflow.models.image.mnist.convolutional\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GRID K2\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.745\npciBusID 0000:00:04.0\nTotal memory: 3.44GiB\nFree memory: 3.41GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K2, pci bus id: 0000:00:04.0)\nInitialized!\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 4007 (compatibility version 4000) but source was compiled with 5103 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\nF tensorflow/core/kernels/conv_ops.cc:457] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nAborted (core dumped)\n```\n", "comments": ["Thanks for the report @jakubsimanek!  Note that we haven't officially released 0.10 yet (we're still shaking out issues like this one), but it's always good to notice and fix these things.\n\nIn fact #4236 just went in yesterday to fix this up.  If you notice any other problems, please don't hesitate to file more issues!\n", "Just wanted to point out further, its **CuDNN v5.1 for CUDA toolkit 7.5 (cudnn-7.5-linux-x64-v5.1.tgz)** that solves the problem. cudnn-8.0-linux-x64-v5.1 should be used if you have CUDA toolkit 8.0\n", "Confirmed. I installed CuDNN v5.1 and it works!\n", "Won't work for me, I just installed CUDA 8.0 with CuDNN v5.1 and this is the output I get. Any ideas?\r\n\r\n`sudo python /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/mnist/convolutional.py\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\nExtracting data/train-images-idx3-ubyte.gz\r\nExtracting data/train-labels-idx1-ubyte.gz\r\nExtracting data/t10k-images-idx3-ubyte.gz\r\nExtracting data/t10k-labels-idx1-ubyte.gz\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:04:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x35484f0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:05:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:04:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla K80, pci bus id: 0000:05:00.0)\r\nInitialized!\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:378] Loaded runtime CuDNN library: 5005 (compatibility version 5000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\nF tensorflow/core/kernels/conv_ops.cc:532] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n`", "I have the same problem", "I fixed it installing everything from sources:\n\nhttps://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#installing-from-sources <https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#installing-from-sources>\n\n\n> On Dec 4, 2016, at 9:32 PM, hsiao yi <notifications@github.com> wrote:\n> \n> I have similar problems\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/4251#issuecomment-264754715>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEwt1DgS7bcQijR-CTGxaYZ5A5_MsS1sks5rE3e8gaJpZM4J2_By>.\n\n", "I don't know if this is the right place to ask but can someone explain to me what the error is even say so that I can start to try to debug this?\r\n\r\nIn particular I would like to start with the first line:\r\n\r\n```\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:347] Loaded runtime CuDNN library: 4007 (compatibility version 4000) but source was compiled with 5103 (compatibility version 5100).\r\n```\r\n", "@brando90 you probably have a wrong version (v4) of cuDNN installed. If you are using the [pip installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation), be sure you meet the requirements (versions 0.10-0.12 require CuDNN v5)", "use tensorflow 0.9  i am ok.", "In my case (with TF0.12.1) CuDNNv5 was installed, but was not the default.\r\nsetting  `export LD_LIBRARY_PATH=\"/usr/local/lib/cuda-8.0/lib64:/usr/local/lib/cudann5/lib64/\"` solved the issue", "Thanks to @yuanbyu. If you installed CuDNN v5.1 and confirmed with `ls -l /usr/local/cuda/lib64/libcud*`, but still see the same issue, it is probably what @yuanbyu described. "]}, {"number": 4250, "title": "Warning produced by Logging and Monitoring Basics with tf.contrib.learn example", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nSimilar issue here:\n1. http://stackoverflow.com/questions/39249704/tensorflow-valueerror-duplicate-feature-column-key-found-for-column\n2. have tried example at \nhttps://www.tensorflow.org/versions/master/tutorials/monitors/index.html\nstill exhibits same issue\n3. I will look at github deltas in 0.10.0rc0 between 1 above head and see if same fix applied to tf.contrib.learn \n### Environment info\n\nOperating System:\nOS-X 10.11.6 Python 3.5 vi Homebrew \nInstalled version of CUDA and cuDNN: \nN/A\n## installed from binary pip package:\n1. link to the pip package you installed:\n   https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.10.0rc0-py3-none-any.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n### minimal reproducible example (\n\n``` python\n\nimport tensorflow as tf\nimport numpy as np\n\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\n\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TRAINING,\n                                                       target_dtype=np.int)\ntest_set = tf.contrib.learn.datasets.base.load_csv(filename=IRIS_TEST,\n                                                   target_dtype=np.int)\n\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=3,\n                                            model_dir=\"/tmp/iris_model\")\n\n# Fit model.\nclassifier.fit(x=training_set.data,\n               y=training_set.target,\n               steps=2000)\n\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(x=test_set.data,\n                                     y=test_set.target)[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n\n# Classify two new flower samples.\nnew_samples = np.array(\n    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\ny = classifier.predict(new_samples)\nprint('Predictions: {}'.format(str(y)))\n```\n### What other attempted solutions have you tried?\n\nReferenced head example \n### Logs or other output that would be helpful\n\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nWARNING:tensorflow:Given features: Tensor(\"input:0\", shape=(?, 4), dtype=float32), required signatures: TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False).\nWARNING:tensorflow:Given targets: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\n", "comments": ["You can change the logging level if you want to avoid them.\n", "@ReaddyEddy thanks for reporting an issue!  It's unclear to me from your description what the actual problem is.  Is it...\n1. You see warnings, but otherwise things work as you expected.  Or...\n2. You see warnings, and immediately afterwards things crash.\n\nNote that the first stackoverflow link you provided has some extra details about things working at HEAD.\n", "Automatically closing due to lack of recent activity, please reopen or provide more information when it becomes available.\n", "Note for posterity, these warnings are gone now. They were a bug.\n"]}, {"number": 4249, "title": "Inception-v3 checkpoint - Tensor name not found", "body": "Trying to run the Inceptionv3 Tensorflow model with the architecture and the checkpoint provided by Google [here](https://research.googleblog.com/2016/08/improving-inception-and-image.html).\n\nThe issue is that tensorflow crashes on `saver.restore(sess, \"./inception_v3.ckpt\")` with the following error:\n\n`tensorflow.python.framework.errors.NotFoundError: Tensor name \"InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/biases\" not found in checkpoint files ./inception_v3.ckpt`\n\nI get the same errors with the inception resnet v2 checkpoint/model combination.\nMissing tensor in the checkpoint or issue with the model file?\n\nThank you\n", "comments": ["Have you seen the documentation here?  It might be useful in getting started:\nhttps://github.com/tensorflow/models/tree/master/slim\nhttps://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\n\nThe first link describes how to run eval:\n\n```\nCHECKPOINT_FILE = ${CHECKPOINT_DIR}/inception_v3.ckpt  # Example\n$ python eval_image_classifier.py \\\n    --alsologtostderr \\\n    --checkpoint_path=${CHECKPOINT_FILE} \\\n    --dataset_dir=${DATASET_DIR} \\\n    --dataset_name=imagenet \\\n    --dataset_split_name=validation \\\n    --model_name=inception_v3\n```\n\nhttps://github.com/tensorflow/models/blob/master/slim/eval_image_classifier.py\n\nYou can use eval_image_classifier.py as a starting point to do what you'd like.  Hope that helps!\n", "I have tried to run eval with those scripts too, and with randomly generated images as shown in the .ipynb file, but keep getting this error:\n\n`tensorflow.python.framework.errors.NotFoundError: Tensor name \"InceptionResnetV2/Repeat/block35_9/Branch_2/Conv2d_0b_3x3/biases\" not found in checkpoint files`\n", "Need to use the right arg_scope \n\n```\nwith tf.Session() as sess:\n  image = tf.read_file('./file.jpg')\n  # code to decode, crop, convert jpeg\n  eval_inputs = tf.pack([image])\n  with slim.arg_scope(inception_v3.inception_v3_arg_scope()):\n    logits, _ = inception_v3.inception_v3(eval_inputs, num_classes=1001, is_training=False)\n  sess.run(tf.initialize_all_variables())\n```\n", "@tenscraft does @sguada's suggestion fix your problem?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "same problem here, with @sguada 's suggestion,  solved my problems.\n"]}, {"number": 4248, "title": "Dose CTC need SOFTMAX?", "body": "It seems CTC dose not need SOFTMAX in many example,so how it works?,and if ctc_loss itself has a SOFTMAX,dose it share SOFTMAX with ctc_greedy_decoder,I mean if there is no SOFTMAX how can I use it to predict ?\nThanks for any help\n", "comments": ["We primarily use github issues to track bugs and installation issues.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n\nThat said, a cursory search on stackoverflow shows a link that might help:\nhttp://stackoverflow.com/questions/38059247/using-tensorflows-connectionist-temporal-classification-ctc-implementation/38069255#38069255\n", "It's in the [documentation](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/connectionist_temporal_classification__ctc_) of `tf.nn.ctc_loss`.\r\n\r\n> This class performs the softmax operation for you, so inputs should be e.g. linear projections of outputs by an LSTM."]}, {"number": 4247, "title": "Which TensorFlow-Slim is the \"real\" one?", "body": "I have found 3 TensorFlow-Slim in repo `tensorflow/tensorflow` and `tensorflow/model`\n- (tensorflow/tensorflow) [`tensorflow.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)\n- (tensorflow/model) [`model.slim`](https://github.com/tensorflow/models/blob/master/slim)\n- (tensorflow/model) [`model.inception.inception.slim`](https://github.com/tensorflow/models/blob/master/inception/inception/slim)\n\nAmong these places, there are some duplicated codes.\ne.g.\n- (tensorflow/tensorflow) [`tensorflow/tensorflow/contrib/slim/python/slim/nets/overfeat.py`](https://github.com/tensorflow/tensorflow/blob/3d1ee95e612b1987e664ca46a7c584872d36dde9/tensorflow/contrib/slim/python/slim/nets/overfeat.py)\n- (tensorflow/model) [`slim/nets/overfeat.py`](https://github.com/tensorflow/models/blob/master/slim/nets/overfeat.py)\n### Question:\n1. What are these 3 `slim` packages used for? What's the differences?\n2. In the future, which one may be deprecated? Or which one will be the future.\n", "comments": ["cc @nathansilberman @lukaszkaiser @sguada\n", "Use tensorflow.contrib.slim for all the core functionality.\n\nUse tensorflow/model/slim for the models and nets.\n\nThis one is deprecated: (tensorflow/model) model.inception.inception.slim\n", "@sguada Thanks for your explanation!\n"]}, {"number": 4246, "title": "tf.train.Coordinator not closing threads", "body": "I am using a number of threads to feed training examples to a `tf.RandomShuffleQueue`, however gracefully closing the threads seems not to be working nicely. This is related to https://github.com/tensorflow/tensorflow/issues/2130 and [this question on StackOverflow](http://stackoverflow.com/questions/36210162/tensorflow-stopping-threads-via-coordinator-seems-not-to-work) however I am wondering whether there is going to be a better way of closing the threads.\n\nThe problem is that there are pending enqueue operations when you request the threads to stop. You can force these to be cancelled using `queue.close(cancel_pending_enqueues=True)`, but then the threads raise an `tf.errors.CancelledError`. As a work around you need to surround the enqueue operation with a try/except block, however the whole process is not very elegant. I am wondering whether there are any plans to improve the process of closing the threads using the Coordinator. \n\nThe code below illustrates a working example, however as you can see it is not a very nice solution.\n\n``` python\n\nq = tf.RandomShuffleQueue( ... )\nenqueue_op = q.enqueue_many([self.queue_inputs, self.queue_targets])\n\ndef load_and_enqueue( ... ):\n\n    while not coord.should_stop():\n        # ...\n        feed_dict = {\n            queue_inputs:  inputs,\n            queue_targets: targets\n         }\n\n         # Catch the exception that arises when you ask to close pending enqueue operations\n         try:\n             sess.run(enqueue_op, feed_dict=feed_dict)\n         except tf.errors.CancelledError:\n             return\n\n# Coordinator for threads\ncoord = tf.train.Coordinator()\n\n# Start a thread to enqueue data asynchronously, and hide I/O latency.\nthreads = [threading.Thread(target=load_and_enqueue, args=(...,)) for i in range(4)]\n\nfor t in threads: t.start()\n\n# ... training loop\n\n# Ask the threads to stop and wait until they do it\nsess.run(q.close(cancel_pending_enqueues=True))\ncoord.request_stop()\ncoord.join(threads, stop_grace_period_secs=5)\n\nsess.close()\n```\n", "comments": ["It sounds like your code is almost identical to a `tf.train.QueueRunner` - which takes care of this exception handling - with the exception that it _feeds_ values to the `sess.run()` call that runs `enqueue_op`. Perhaps the experimental [`FeedingQueueRunner`](https://github.com/tensorflow/tensorflow/blob/7e7dff529fd35edea443580d58e95f5de39b5356/tensorflow/contrib/learn/python/learn/dataframe/queues/feeding_queue_runner.py#L29) would do the trick?\n", "Thanks, I didn't know of the `FeedingQueueRunner`, but this seems like it is exactly what my code is currently doing. I guess you can close this issue for now unless I run into problems with the queue runner.\n", "Glad to help!\n"]}, {"number": 4245, "title": "Docker build parameterized_docker_build.sh breaks on cURL ssl issue", "body": "I checked out 2ab7e6326296987ea0ce975afb3434a16d1aa21a, set some options,\n\n```\nexport TF_DOCKER_BUILD_TYPE=GPU TF_DOCKER_BUILD_IS_DEVEL=NO TF_DOCKER_BUILD_CENTRAL_PIP=1\n```\n\nand then executed the Docker build script:\n\n```\n./tensorflow/tools/docker/parameterized_docker_build.sh\n```\n\nI from there got an error:\n\n```\nStep 4 : RUN curl -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py\n ---> Running in f267d0b1207b\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\nMore details here: http://curl.haxx.se/docs/sslcerts.html\n\ncurl performs SSL certificate verification by default, using a \"bundle\"\n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn't adequate, you can specify an alternate file\n using the --cacert option.\nIf this HTTPS server uses a certificate signed by a CA represented in\n the bundle, the certificate verification probably failed due to a\n problem with the certificate (it might be expired, or the name might\n not match the domain name in the URL).\nIf you'd like to turn off curl's verification of the certificate, use\n the -k (or --insecure) option.\nThe command '/bin/sh -c curl -O https://bootstrap.pypa.io/get-pip.py &&     python get-pip.py &&     rm get-pip.py' returned a non-zero code: 60\nFAIL: docker build of hholst/tensorflow:latest-gpu with Dockerfile /tmp/tmp.Ppxl6vutKu/Dockerfile.gpu failed\n```\n", "comments": ["That looks like an issue with how your local machine is set up, and not a tensorflow issue.\n\nHere are some links that may be helpful:\nhttp://stackoverflow.com/questions/24611640/curl-60-ssl-certificate-unable-to-get-local-issuer-certificate\nhttps://www.google.com/search?q=curl%20ssl%20certificate%20problem\n\nHope that helps!\n"]}, {"number": 4244, "title": "Feature request: Docker image build (parameterized_docker_build.sh) lacks option for AVX2 optimization", "body": "As far as I can understand there is no way to tell the script `parameterized_docker_build.sh`  to build an AVX2 optimized docker image.\n\nI see a note about Python 2 and 3 but nothing about AVX2:\n\n```\n# TODO(cais): Add support for TF_DOCKER_BUILD_PYTHON_VERSION (PYTHON2/PYTHON3)\n```\n", "comments": ["@caisq can you comment on this?  Thanks!\n", "BTW in the meantime, I wanted to mention that we're happy to accept pull requests.  :)\n"]}, {"number": 4243, "title": "CTC ERROR: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 0 num_classes: 12 labels: 2,10,5,1", "body": "I use TF rc 0.10 to run a project (https://github.com/synckey/tensorflow_lstm_ctc_ocr), but the program raise a error:\nInvalid argument: Saw a non-null label (index >= num_classes - 1) following a null label, batch: 0 num_classes: 12 labels: 2,10,5,1\n\nwhat does it mean?\n", "comments": ["@LinJM I see that you've already filed a similar issue under the synckey repo, which is great!\n\nIn general it's hard for us on tensorflow to provide support for third-party programs.  I do hope you get your issue figured out!\n", "@LinJM Have you solve the problem? I'm in the same trouble~", "Have anyone solved this problem??"]}, {"number": 4242, "title": "Why is --whole-archive neccesary when building with the session api?", "body": "### Environment info\n\nOperating System:\nLinux version 4.8.0-rc1-ga0cba21 \n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nBuilding a project using the C++ session API fails without the --whole-archive option but this slows down the linking process immensely and the output binaries are huge (by embedded systems standards). Why can't the linker link this archive normally?\n", "comments": ["TensorFlow uses a static registration mechanism (in particular behind the `REGISTER_OP()` and `REGISTER_KERNEL_BUILDER()` macros) in the implementation of the kernel library, and the default build rules create a single binary that contains all kernels. Without `--whole-archive`, these static registrations would be stripped, and the kernels would not be registered. See [this Stack Overflow answer](http://stackoverflow.com/a/842770/3574081) for a description of a similar registration scheme.\n\nIf you're targeting an embedded platform, you might find the experimental support for selective (kernel) registration useful. We've used that to reduce binary sizes for the Android build. See [`tensorflow/core/framework/selective_registration.h`](https://github.com/tensorflow/tensorflow/blob/f71cc62282bf2e066f9ebd08cf3f605fc98c6e41/tensorflow/core/framework/selective_registration.h) for more details.\n", "Thanks for the detailed response @mrry, and hope that helps @normads!\n\nI'm closing this out, but if you have further issues just reopen this bug.  Note that we use github issues primarily for bugs and installation issues; please ask general questions on StackOverflow and tag with the `tensorflow` tag.  Thanks!\n", "Thanks a lot @mrry! That makes sense. \n@tatatodd will post on stackoverflow next time :+1: \n"]}, {"number": 4241, "title": "R0.10", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Hi, can you please explain what this pull request is for?\n", "When you see R0.10 as the title, it's usually a mistaken push\n"]}, {"number": 4239, "title": "Improve documentation about custom GPU op compilation", "body": "Fixes #4050. See issue for discussion.\n", "comments": ["Can one of the admins verify this patch?\n", "@zplizzi, thanks for your PR! By analyzing the annotation information on this pull request, we identified @keveman, @vrv and @rryan to be potential reviewers\n", "@keveman  PTAL\n", "Saving the environment by not testing this docs only change.\n"]}, {"number": 4238, "title": "Fix typo 'sttdev' to 'stddev'(2) (in how_tos/summaries_and_tensorboar\u2026", "body": "\u2026d/index.md)\n\nBecause of finding same typo error in other file, I open a PR again.\n\nAnd also, I originally find it at website[[https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html#tensorboard-visualizing-learning](https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html#tensorboard-visualizing-learning)]. \n", "comments": ["Please fix it in this file too: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n", "@danmane  Okay... did you mean reset the passed related commit #4221 (and merge commit) and make a  commit with fixing two same typo errors. And then, PR again. right?\n", "Oh, I didn't see that the other one already merged.\n\nI would have just moved both commits into one pull request. But since the other one already merged, we should merge this one too.\n"]}, {"number": 4237, "title": "Branch 132376998", "body": "", "comments": ["See test timeout at: https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/1375/consoleFull\n\nTrying again.\n@tensorflow-jenkins test this please.\n"]}, {"number": 4236, "title": "Update installation instructions to use cuDNN v5 across builds.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the annotation information on this pull request, we identified @martinwicke, @vrv and @raix852 to be potential reviewers\n", "other than two small suggestions, LGTM\n", "Thanks @vrv! I have made the changes accordingly. \n"]}, {"number": 4235, "title": "How to implement dynamic network with TensorFlow ?", "body": "Hi, I am trying to implement a dynamic network, which is able to change the network structure according to the input data. Here is an example  https://arxiv.org/abs/1511.02799\n\nI wonder it that possible to use TensorFlow to implement dynamic network?\nI think we may need to use placeholder to control the network?\n\nThank you very much.\n", "comments": ["We primarily use github issues to track bugs and installation issues.  This is a question better suited for StackOverflow.  Please ask it there and tag it with the `tensorflow` tag.  Thanks!\n"]}, {"number": 4234, "title": "Error installing from source.", "body": "I'm installing tensorflow from source (git hash 891f8f73cc1967a5c2da89057884bc3dc1f9091a, bazel version 0.3.1).\n\n```\n./configure\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n```\n\nThe first two steps work, but the last one gives an error:\n\n```\nerror: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist or not a regular file\n```\n\nThat file indeed does not exist.\n", "comments": ["I recommend that you pick a release branch, rather than something so close to HEAD, which may exhibit arbitrary failures.  We try to keep HEAD working at all times, but inevitably issues creep in.\n\nTo clone at the latest official release (0.10.0rc0) run the following.  Note that 0.10 hasn't been officially released yet; the \"rc\" means that this is a release candidate:\n\n```\ngit clone -b v0.10.0rc0 https://github.com/tensorflow/tensorflow\n```\n\nFeel free to re-open if that doesn't fix your problem.\n", "I have same error stated above.\n(Tensorflow is installed from source.)\nIn tensorflow/models/embedding/ directory, there is word2vec.py not gen_word2vec.py.\nCommand \"bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\" requires gen_word2vec.py not word2vec.py.\nI installed latest version of tensorflow by using \"git clone https://github.com/tensorflow/tensorflow\"\nError message is like this \"error: can't copy 'tensorflow/models/embedding/gen_word2vec.py': doesn't exist or not a regular file\".\nHow can I solve it?\n", "As above, I recommend that you pick a release branch.  Note that 0.10 has been released.\n\n```\ngit clone -b r0.10 https://github.com/tensorflow/tensorflow\n```\n"]}, {"number": 4233, "title": "Fix bazel dependency failure for non-standard cuda toolkit path", "body": "ArchLinux cuda package puts the toolkit to `/opt/cuda` without symlinks (no /opt/cuda-7.5 etc).\n", "comments": ["Can one of the admins verify this patch?\n", "@nsuke, thanks for your PR! By analyzing the annotation information on this pull request, we identified @davidzchen, @keveman and @ville-k to be potential reviewers\n", "Jenkins, test this please.\n", "@davidzchen addressed your comment.\n\nJust noticed that you were already planning to fix it (#4096).\nThat would be diffinitely better when bazel requirement is met.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Thanks! LGTM aside from a couple of minor comments.\n", "@davidzchen tweaked the comments as per your suggestions.\nLooks much nicer, thanks :)\n", "Jenkins, test this please.\n", "@vrv This change looks good to me. Can you take a look and merge if there is no other feedback? Thanks!\n"]}, {"number": 4232, "title": "Create tf.substr Op", "body": "Would be useful for a number of purposes, such as extracting data, substring matches (#4009), etc. Proposed API (matches [C++ style substrings](http://www.cplusplus.com/reference/string/string/substr/)):\n\n```\nArgs:\n  inputs: a `Tensor` of type `string`. The strings that will have substrings extracted\n  pos: `int32` or `int64` `Tensor`. Position of the first character to be copied as a substring\n  len: `int32` or `int64` `Tensor`. Number of characters to include in the substring\n\nReturns:\n  A `Tensor` of type `string` containing substrings of `inputs`\n```\n", "comments": ["@samjabrahams  I want to try adding the substr Op :). Should I add it to the [`String_ops.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/string_ops.py)?\n", "@DjangoPeng Oh no! I already sent in a PR two days ago with #4338. I'm planning on using the Op to finish work on the [`tf.decode_image`](https://github.com/tensorflow/tensorflow/pull/4222) Op, so I knocked out a version over the weekend.\n", "@samjabrahams Sounds good~ Look forward to your update~\n", "Substr (#4338) has been merged into master\n"]}, {"number": 4231, "title": "variable_scope raise exception if reuse=False and no name_or_scope", "body": "Right now `variable_scope` [raises an exception](https://github.com/tensorflow/tensorflow/blob/5c07198777fd641ffb5f55fa6604050ccf55e8c8/tensorflow/python/ops/variable_scope.py#L1373) if `reuse=True` and there is no `name_or_scope`, but there is no similar error when `reuse=False` (explicitly set and not using default of `None`). \n\nObviously no error should be raised when `reuse=None` but I think as a safety check if I have explicitly set `reuse=False`, then I do want to raise an error if I have forgotten to set the scope for a given layer / node.\n\nI am using this construct in tf-slim to deal with creating a different graph for training vs validation. Right now if I forget to assign a `scope` to a given layer, than the call to create the graph will not fail even though I want it to.\n", "comments": ["This is a bit complicated, and I'm not sure I fully understand it, but I believe the explanation of this is here, in the paragraph that starts with \"Note that you cannot set the reuse flag to False.\"\nhttps://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html#basics-of-tf-variable-scope\n\nLet me know if that answers your question; if not just ping this issue.\n", "Automatically closing due to lack of recent activity, please reopen or provide more information when it becomes available.\n"]}, {"number": 4230, "title": "Fix nightly docker builds", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the annotation information on this pull request, we identified @vrv, @keveman and @ebrevdo to be potential reviewers\n", "lgtm\n"]}, {"number": 4229, "title": "Tensors have values set to 0 if code is run on GPU.", "body": "Hello team, \nI have a strange issue, which very well might be my own fault, but appreciate if somebody could comment on it.\nTF 0.10 with CUDA 8\nRunning code like this on GPU (GeForce GTX Titan X) :\n\n`import tensorflow as tf\nimport numpy as np\nwith tf.device(\"/gpu:0\"):\n    a = tf.placeholder(tf.float32, shape=[2,2])\n    b = tf.placeholder(tf.float32, shape=[2,2])\n    r = tf.squared_difference(a, b)\n\nt1 = np.asarray([[1,1],[1,-5]])\nt2 = np.asarray([[2,3],[4,5]])\n\nfeed_dict = {a:t1, b:t2}\n\nwith tf.Session() as sess:\n     t1, t2, sd = sess.run([a,b,r], feed_dict=feed_dict)\n    print(\"a = {}\\n b = {}\\n sqared diff = {}\".format(t1, t2, sd))`\n\nPrints result like this \n\n`a = [[ 1.  1.]\n [ 1. -5.]]\n b = [[ 2.  3.]\n [ 4.  5.]]\n sqared diff = [[ 0.  0.]\n [ 0.  0.]]`\n\nWhile if I run the same code on CPU , the result is correct \n`a = [[ 1.  1.]\n [ 1. -5.]]\n b = [[ 2.  3.]\n [ 4.  5.]]\n sqared diff = [[   1.    4.]\n [   9.  100.]]`\n\nI'd expect squared_difference to be a perfect operation for GPU and as I tensors \"a\" and \"b\" are populated properly, it doesn't look like TF has issues getting those values into GPU\n\nAppreciate any comments on this.\n\nOperating System: 4.4.0-34-generic #53-Ubuntu\n\n`python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\n0.10.0rc0`.\n", "comments": ["A couple of shots in the dark:\n1. Try installing version 0.10 rather than 0.10.0rc0.  We haven't officially released 0.10 yet, but there have definitely been fixes applied since the rc \"release candidate\".  See the following for details on installation of 0.10; the instructions are different from \"master\" since 0.10 isn't officially released yet.\n   https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation\n2. I know of some other issues being tracked wrt cuda 8 in #2559 that you might want to look at.\n\nLet me know how that goes; just ping if you're still stuck.\n", "Hi\nThank you, discovered yesterday an interesting thing. It actually started working magically after some time and it seems that at the time when it wasn't working correctly, the colleague of mine was running something on the same GPU which occupied almost all memory. When that task was stopped , my example worked as expected.\nSo, it seems to be related to memory usage of GPU. Though, it's a bit strange that TF/GPU interaction in this situation doesn't produce some error (like when GPU runs out of memory) but instead stops calculating floats properly.\n", "I see, I think #4196 is also related, wrt concurrent usage of the same GPU.\n\n@zheng-xq can you comment on the failures modes we're likely to see when the same GPU is used concurrently?  And also whether we can prevent these problems, or at least provide better errors?  Empirically in this case we seem to silently get bad results, while in #4196 we seem to either get bad results or strange errors.  Thanks!\n", "TensorFlow should definitely report error in this case. It will try to grab all the memory it can get from the system, and if that fails, that should be fatal. \n\nAre you using a recent Cuda driver? We've had several incidents with older Cuda drivers. One remote possibility is that the Cuda driver gives the same memory to multiple running processes, and they all thought they own the memory. \n", "Might be, I'm running 361.77 nvidia driver which seems to be a bit old.\n\n```\nnvidia-smi \nWed Sep  7 21:48:02 2016       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 361.77                 Driver Version: 361.77                    |\n|-------------------------------+----------------------+----------------------+\n\n```\n", "@berlogb have you attempted using newer drivers, to see we at least report a reasonable error when the gpu is used concurrently, rather than silently giving bad results?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "@tatatodd  Sorry, work caught up with me. \nSo, repeated test on old driver simply copy pasting the same code as above into 3 different python processes on the same machine, using the same CPU.\n\nUnfortunately, doesn't seem that situation changes much. On old 361 driver I've got :\n`a = [[ 1.  1.]\n [ 1. -5.]]\n b = [[ 2.  3.]\n [ 4.  5.]]\n sqared diff = [[ nan  nan]\n [ nan  nan]]`\n\nand after upgrading \n\n`nvidia-smi \nWed Oct  5 15:28:26 2016       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |`\n\n`sqared diff = [[ -1.70120404e+38  -1.70141163e+38]\n [             nan              nan]]\n`\n\n(When it works fine , it should look like this:\n`sqared diff = [[   1.    4.]\n [   9.  100.]]`\n)\nInterestingly, it seems that if I open another python , I finally get error I was looking for :\n\n`W tensorflow/core/common_runtime/gpu/gpu_init.cc:163] Not initializing the GPU, could not create GPU MachineManager. Error: Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuCtxCreate: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073709551615`\n\nSo, looks like we do get that error, but somewhere in between, there is a situation when we fail to calculate stuff properly, but still don't through that error.\n", "@berlogb thanks for the info.  \n\nCan you try the tensorflow 0.11 release?  I don't know of anything specific in that release that should help, but it'd be good to get on the latest version so we can narrow in on the possibilities.\nhttps://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup\n", "@berlogb TensorFlow grabs all GPU memory by default, so if you open up another process you will get `CUDA_ERROR_OUT_OF_MEMORY`\n", "@yaroslavvb I think the issue is that he's not getting `CUDA_ERROR_OUT_OF_MEMORY` under certain circumstances with multiple concurrent processes; instead he just gets arbitrary output with no errors.\n", "I see. I've gotten random/incorrect results from GPU when I compiled with the wrong Compute Capability. The description says it's for CUDA 8.0, but there are were no official releases for CUDA 8.0 in September. So maybe there's been a user mistake in ./configure step. A way to rule that out is to reproduce\u00a0with the official wheel\n", "Hi Yaroslav\nThanks, good point. It wasn't me who upgrade setup to 8.0, but looks like official release was in the end of September. I'll try to reinstall with official package and see how it goes.\nThanks.\n", "Did you get this to work? If so, please close the issue @berlogb.\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n", "running within ``tensorflow:gpu``  container from tensorflow repo\r\n\r\ngetting  ```CUDA_ERROR_OUT_OF_MEMORY```  when trying to run  ``convolutional.py`` from tensorflow ``models`` repo\r\n\r\n```\r\nE tensorflow/core/common_runtime/direct_session.cc:135] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744071515537408\r\nTraceback (most recent call last):\r\n  File \"convolutional.py\", line 339, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"convolutional.py\", line 284, in main\r\n    with tf.Session() as sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1186, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 551, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n```\r\n", "@mandar that indicates your GPU doesn't have enough memory for this network\n\nOn Feb 11, 2017 5:01 PM, \"Mandar Upadhye\" <notifications@github.com> wrote:\n\n> getting CUDA_ERROR_OUT_OF_MEMORY when trying to run convolutional.py from\n> tensorflow models repo\n>\n> E tensorflow/core/common_runtime/direct_session.cc:135] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744071515537408\n> Traceback (most recent call last):\n>   File \"convolutional.py\", line 339, in <module>\n>     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\n>     sys.exit(main(sys.argv[:1] + flags_passthrough))\n>   File \"convolutional.py\", line 284, in main\n>     with tf.Session() as sess:\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1186, in __init__\n>     super(Session, self).__init__(target, graph, config=config)\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 551, in __init__\n>     self._session = tf_session.TF_NewDeprecatedSession(opts, status)\n>   File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n>     self.gen.next()\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\n>     pywrap_tensorflow.TF_GetCode(status))\n> tensorflow.python.framework.errors_impl.InternalError: Failed to create session.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4229#issuecomment-279188153>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHJsUrq5d1uC71grJVv7proTlzC7Jks5rblnugaJpZM4J1_e2>\n> .\n>\n", "thanks @yaroslavvb ! jupyter notebook locked in full memory even when it was idle!, i didn't check on that one!", "Yes TF is greedy, it takes all GPU memory by default\n\nOn Feb 11, 2017 6:23 PM, \"Mandar Upadhye\" <notifications@github.com> wrote:\n\n> thanks @yaroslavvb <https://github.com/yaroslavvb> ! jupyter notebook\n> locked in full memory even when it was idle!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4229#issuecomment-279191482>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHMrCU2yQIe2eUuPyX2GwwkUjvzxjks5rbm0YgaJpZM4J1_e2>\n> .\n>\n", "@yaroslavvb  so TF wouldn't release GPU memory once TF session exits?  "]}, {"number": 4228, "title": "AttributeError: No svd operator in 0.10.0rc0", "body": "Issue #2207 and [documentation](https://www.tensorflow.org/versions/r0.10/api_docs/python/math_ops.html#svd) both indicate that tensorflow has a svd operator.\nHowever, I can't find it in fresh install of tensorflow0.10 with pip for python3.5 (both cpu and gpu )\n### Environment info\n\nOperating System: Linux (arch, kernel 4.7.2)\nPip installed (in two different virtualenv):\n- https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl  \n- https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0rc0-cp35-cp35m-linux_x86_64.whl\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n``` python3\nimport tensorflow as tf\ntf.svd\n\nAttributeError: module 'tensorflow' has no attribute 'svd'\n```\n", "comments": ["Thanks for the note @tomMoral!\n\nNote that the in \"0.10.0rc0\" the \"rc\" stands for \"release candidate\", which is a preliminary version cut before the actual 0.10 release, to shake out issues.  Support for tf.svd was added after 0.10.0rc0 was cut, which is why it doesn't show up in your installs.\n\nTry one of the following instead for either cpu or gpu installs: \nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n\nNote that these are listed in the pip installation instructions for version 0.10.  The instructions for master haven't been flipped over yet, since 0.10 hasn't been officially released yet.\nhttps://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation\n\nThis should fix your problem; if not feel free to re-open.\n"]}, {"number": 4227, "title": "SparseTensor doesn't have get_shape() function", "body": "I've been trying to add SparseTensor support to Keras (https://github.com/fchollet/keras/pull/3695) and one of the issues I've run into is that because of the lack of get_shape() I need to add SparseTensor-specific code.\n\nLuckily so far I've only needed the rank of SparseTensors, so sparse.shape.get_shape()[0] has worked for me, but if I pass in a shape in the sparse_placeholder constructor (https://github.com/tensorflow/tensorflow/issues/4226) I would like to be able to retrieve it with sparse.get_shape()\n", "comments": ["Related to #4226, and again @concretevitamin might know more.\n", "We very much welcome a PR for this.  Should just be an alias of `def shape(..)`, I think.\n", "I don't think this is just an alias of def shape(..) since get_shape() returns the value through shape inference without running the graph; this is pretty close to the code that should be fixed for #4226 so hopefully this can be resolved in the same pass.\n", "@concretevitamin, is this already resolved?", "I think this issue has been fixed and could be closed.\r\n\r\nThe `get_shape()` was added in commit https://github.com/tensorflow/tensorflow/commit/2939f53ea2d5dc9de018d9ad64a3ac03d03d4925, and PR #4226 was fixed through commit https://github.com/tensorflow/tensorflow/commit/b00e940445a03dd5edbe47ff34762ec8ff4087fb"]}, {"number": 4226, "title": "sparse_placeholder doesn't accept shapes with None parts", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nNone\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\ntf.sparse_placeholder(tf.int32, shape=(None, 2, 3))\n```\n### What other attempted solutions have you tried?\n\nFor the moment I'm passing in a value of the wrong shape but correct rank, which seems to work fine combined with passing in a correct shape when feeding it, but this is inconsistent with the normal placeholder.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\nTypeErrorTraceback (most recent call last)\n<ipython-input-8-c6a6ad1fab2e> in <module>()\n----> 1 tf.sparse_placeholder(tf.int32, shape=(None, 2, 3))\n\n/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc in sparse_placeholder(dtype, shape, name)\n   1318   else:\n   1319     shape = ops.convert_to_tensor(\n-> 1320         shape, name=(name + \"/shape\") if name is not None else None)\n   1321   return ops.SparseTensor(\n   1322       values=placeholder(\n\n/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in convert_to_tensor(value, dtype, name, as_ref)\n    627     for base_type, conversion_func in funcs_at_priority:\n    628       if isinstance(value, base_type):\n--> 629         ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    630         if ret is NotImplemented:\n    631           continue\n\n/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in _constant_tensor_conversion_function(v, dtype, name, as_ref)\n    178                                          as_ref=False):\n    179   _ = as_ref\n--> 180   return constant(v, dtype=dtype, name=name)\n    181 \n    182 \n\n/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc in constant(value, dtype, shape, name)\n    161   tensor_value = attr_value_pb2.AttrValue()\n    162   tensor_value.tensor.CopyFrom(\n--> 163       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n    164   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\n    165   const_tensor = g.create_op(\n\n/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc in make_tensor_proto(values, dtype, shape)\n    420   if numpy_dtype == dtypes.string and not isinstance(values, np.ndarray):\n    421     proto_values = _FlattenToStrings(values)\n--> 422     tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])\n    423     return tensor_proto\n    424 \n\n/home/ubuntu/virtualenv/local/lib/python2.7/site-packages/tensorflow/python/util/compat.pyc in as_bytes(bytes_or_text)\n     43   else:\n     44     raise TypeError('Expected binary or unicode string, got %r' %\n---> 45                     (bytes_or_text,))\n     46 \n     47 \n\nTypeError: Expected binary or unicode string, got (None, 2, 3)\n```\n", "comments": ["@concretevitamin might know more about this.\n"]}, {"number": 4225, "title": "fix typo(sttdev-->stddev) in image_retraining/retrain.py", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@wangg12, thanks for your PR! By analyzing the annotation information on this pull request, we identified @tensorflower-gardener, @petewarden and @benoitsteiner to be potential reviewers\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4224, "title": "Print command not being called through gradient operation.", "body": "I have the following code.  When I call the image_graident function I expect it to print the image_reg variable.  It does not print anything when imageGraident() is called.\n\n```\nimage_reg = tf.reduce_mean(tf.abs(input_image))\ndream_obj = tf.nn.l2_loss(net.fc8) - image_reg\ndream_obj = tf.Print(dream_obj, [image_reg])\ndelta_image = tf.gradients(dream_obj, input_image)[0]\n\ndelta_image = tf.Print(delta_image, [image_reg])\n\ndef imageGraident(img):\n    orig_shape = img.shape\n    img = np.reshape(img, (-1, 224, 224, 3))\n    dimg = sess.run(delta_image, feed_dict = {input_image: img})\n    return np.reshape(dimg, orig_shape)\n```\n\nHowever the following does work fine and it prints the value.\n\n```\nimage_reg = tf.reduce_mean(tf.abs(input_image))\ndream_obj = tf.nn.l2_loss(net.fc8) - image_reg\n\ndelta_image = tf.gradients(dream_obj, input_image)[0]\ndelta_image = tf.Print(delta_image, [image_reg])\n\ndef imageGraident(img):\n    orig_shape = img.shape\n    img = np.reshape(img, (-1, 224, 224, 3))\n    dimg = sess.run(delta_image, feed_dict = {input_image: img})\n    return np.reshape(dimg, orig_shape)\n```\n\nThis seems like a bug to me but maybe I am missing something.\n", "comments": ["`<wild_guess>`\nI'm guessing that this is because tf.Print is an identity op, which happens to have a side-effect of printing out the data in the second argument.  Identity ops don't have any impact on the gradient computation, and are either dropped when tf.gradients adds ops to the graph, or in some later optimization pass.\n`</wild_guess>`\n\nBut @girving would know for sure.\n", "@rmlarsen probably knows whether my explanation had any truth behind it.  Sorry for the redirects; I'm just getting ramped up on everything, and want to make sure the answer is correct.  Thanks!\n", "@tatatodd tf.Print does have a gradient op, which is inserted in the gradient subgraph, but it does not log anything:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/logging_ops.py#L87\n\nIt is also true that ops that are not on a path from sink to source get pruned by the gradient computation:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients.py#L368\n\nDoes that answer your question?\n", "I think we can close this. \n", "Would it be possible to add this to the documentation of opt.`computer_gradient()`? - I've just spent excessive amount of time trying to debug why the print and control dependencies in my graph don't get executed."]}, {"number": 4223, "title": "merge master (2ab7e6326) into go branch and fix build", "body": "This merges master into the go branch and fixes up the build along the way. The build just needed the new filepaths to the C API header. And I ran `go generate` to get the latest protobuf.\n\nUpdates #10\n", "comments": ["Can one of the admins verify this patch?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Ah, the googlebot seems to not be able to check if commits listed are already in master and whitelist them from CLA checks. \n\nThat's what all of these commits are except for the last two that are from me (who has a CLA)\n\nAnything we can do to fix this?\n", "Hey, would love to figure this one out! Lmk what I need to do!\n", "@jmhodges I believe you need to rebase `origin/go` on top of `master` and then `go_updates` on top of the rebased `go`.\n\nsee:\n\n``` sh\n$> git fetch --all -p\n$> git checkout go\n$> git rebase master\n$> git checkout go_updates\n$> git rebase go\n$> git l | head\n* 9c5c181 (HEAD -> go_updates) run go generate on contrib/go\n* 71c9cde some updates to header paths\n* 9b5568d (go) remove non idiomatic NewGraphFromReader(reader, bool) and add NewGraphFromBuffer and New Graph\n* f04f5df fix documentation link\n* b6e9ec8 fix mispless and add minor corrections\n* 5ddeb6e Go bindings tensors (#2479)\n*   f71cc62 (origin/master, origin/HEAD, master) Merge pull request #4237 from caisq/branch_132376998\n|\\  \n| *   fe19bcb Merge commit for internal changes\n| |\\  \n|/ /  \n| * 0949ece Starting the server before we (optionally) sleep for the case where no device_filters are set. Otherwise, the servers will wait to connect to each other before starting to train. We might as well start as soon as we can. Change: 132376998\n| * cde9adb All other tensors from this list would be ignored, and instead the previous output of the decoder would be used. Change: 132374855\n| * 6809660 Added a check for output_buffer_size <= 1 for ZlibOutputBuffer. Also adding some tests for Zlib compression reading / writing. Change: 132370925\n| * d7bc08f Update generated Python Op docs. Change: 132368885\n| * d41348b Cumulative bug fixes to get tf.learn working with open source TF. Change: 132368553\n| * 6e369da Add new str_util::HumanReadableElapsedTime that formats a 'double seconds' value using appropriate units based on the magnitude of the time interval, and add tests for this. Change: 132365945\n| * 51a8d2e Update ops-related pbtxt files. Change: 132363192\n| * 73bc0a4 Disable \"label\" mode. Change: 132360445\n| * 5c8bff0 tfdbg: Allow debug ops to watch uninitialized tensors\n| * 264f2a7 Update generated Python Op docs. Change: 132348660\n| * 7bad05c Delete deprecated TensorFlowDNN(Regressor|Classifier). Also add tf_random_seed to fix flakiness for the affected tests. Change: 132347779\n```\n\nhth.\n", "Okay, rebasing `go` against `master` and then `go_updates` against `go` kept giving me merge conflicts when I would push here. I think because I wasn't able to push to `origin/go`.\n\nI've just merged in master.\n\nWe could do this in two steps if someone who can push `origin/go` can step in. First step: rebase `go` against `master`, and then I can apply these last two commits from myself.\n", "I rebased go onto master. You should be able to update your branch.\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "There we go!\n\nNow, the next thing after this PR is the generated protobuf code. It doesn't currently build using `go install` or `go get` (which I think is a goal)\n", "@jhseu PTAL. \n", "It's hard to review the generated protocol buffer code (from go generate), particularly the gzipped file descriptor stuff. Mind omitting those (I can run it manually if it's needed)? Alternatively we could abandon this in favor of the work-in-progress Go bindings, since they're different:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/go\n\nThere needs to be a few more additions to the C API to complete them. We want to build all new bindings on top of the C API (rather than SWIG to the C++ one).\n", "@jmhodges friendly ping\n", "I was informed that this wasn't likely to go in since the tensorflow team was working on a go wrapper already and that I wouldn't be able to do anything to help move their version along. This happened in the comments of #10.\n\nAnyone else is free to take my work up\n", "Ok. I will close this PR then.\n"]}, {"number": 4222, "title": "Add decode_image Op", "body": "For #4009: Automatically detects whether the bytes passed in are JPEG, PNG, or neither and returns a `decode_jpeg` op, `decode_png` op, or ValueError respectively.\n\nBefore merging, need to hard-code the [`imghdr.what()`](https://docs.python.org/2/library/imghdr.html#imghdr.what) function, whose source is available as [pure Python](https://hg.python.org/cpython/file/2.7/Lib/imghdr.py)\n", "comments": ["Can one of the admins verify this patch?\n", "The `_image_type()` function is placeholder logic that needs to go into a separate TensorFlow operation. The code currently pushed works if pre-loaded bytes are sent to the Op, but will crash and burn if the output from another op is passed in (say, from `tf.read_file()`)\n", "I'm assuming that since this is still a WIP we should hold off on reviewing and testing it. Let me know if that isn't the case.\n", "@danmane That's correct. The current code works if you load the bytes of a file ahead of time, but would throw an error if you passed in code from a function like `tf.read_file`.\n\nThe current game plan is to write a `tf.substr` Op and then utilize it in this function. [Here's that conversation for context.](https://github.com/tensorflow/tensorflow/issues/4009#issuecomment-244863602)\n\nThanks!\n", "Any updates on this?\n", "@rohan100jain Waiting for feedback on #4338: I can close this in the meantime if you'd prefer.\n", "@rohan100jain @danmane - `tf.substr` has been merged, and I've update this Op to utilize it. There are a few things left to do, but the main functionality of the op is here. Left to do:\n- More/better documentation\n- More tests (should we test with small jpeg/png/gif files in addition to hard-coding in bytes?)\n- Tests for gifs\n", "Thanks @samjabrahams !  Ping us when you think this is ready for a final review.\n", "@vrv ready for review! Couple of judgement calls:\n- Included GIF support. `decode_gif` returns a 4-D tensor as opposed to 3-D for `decode_jpeg` and `decode_png` due to the extra \"number-of-frames\" dimension. I'm also not sure if `decode_gif` is supposed to be exposed publicly yet (currently not in the API).\n- Did not add actual image files to be read from disk and tested- stuck with hard-coded byte strings.\n", "@girving let me know if you have time to look at this.  I worry that the variable shape output of this convenience op will make it inconvenient :), but maybe it'll be alright.  Also, it might make sense to think about how to pass the options to decode_image in a better way -- otherwise the op's arguments are going to keep increasing to account for every image option that is added.\n", "The different shaped output definitely seems a bit clunky. Couple options (which may or may not be any less clunky):\n1. Return `tf.squeeze(tf.decode_gif(...))`. Since the `decode_gif` option always has three color channels, this will return a 3-D tensor for single-framed gifs (assuming the image is larger than 1x1) and 4-D tensors otherwise. If `decode_gif` ever gets an option for number of color channels, this will have to be modified.\n2. Reshape the png and jpeg results to be 4-D. This keeps it consistent, but means that this Op requires a different workflow to the standard `decode_png` and `decode_jpeg`\n3. Remove the `decode_gif` portion \n4. Leave it as is and have the end user figure it out.\n\nFor the growing number of parameters: I could use `**kwargs` and either let the user look up the options, or just leave the options out of the function signature but include them in the documentation.\n", "I can look at it, but could we squash it before the initial review?\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@girving Squashed!\n", "@samjabrahams What's the status here? I think @girving is on vacation but I can likely find someone else to review if you're ready to move forward. Also, please rebase and fix conflicts.", "@danmane Sorry, Github state machine failure.  I had forgotten about this.  Looking now.", "Jenkins, test this please.", "@samjabrahams Looks like I've delayed long enough that it needs conflict resolution.  Sorry about that. :/", "@girving No worries- I've rebased and cleaned up the implementation a bit. The last major headache here is dealing with `channels` along the GIF path. Unfortunately, creating the path isn't as simple as the following:\r\n\r\n```python\r\ndef _gif():\r\n    if channels not in (None, 0, 3):\r\n        raise ValueError('GIF images must be decoded with channels in (None, 0, 3)\r\n    # ... do the op\r\n```\r\n\r\nBecause `tf.cond` always ends up running the `_gif` function, the check happens even if there are no GIF files being decoded. That means that `channels` can't be set to 1 if we go with this approach. \r\n\r\nIf we try using `tf.Assert` instead, it might look like this:\r\n\r\n```python\r\ngood_op = tf.not_equal(channels, 1)   # Already checked that channels in (None, 0, 1, 3)\r\nassert_good = tf.Assert(good_op, ['channels can't be 1!'])\r\nwith tf.control_dependencies([assert_good]):\r\n    # ...do the rest\r\n```\r\nThis doesn't work either, as we run into the issue of `channels` being set to `None`, which can't be passed into `equal` or `not_equal`.\r\n\r\nIn short, I'm not sure how I'd allow different channels for different image types. Here are two different ways to handle `channels` I know I can make work:\r\n\r\n1. Use `rgb_to_grayscale` to convert the GIF image to black/white if `channels` is set to 1. It might be a little bit much going on under-the-hood, but I imagine it accomplishes the intent of what users want. The current code does this.\r\n2. Only allow channels to be in (None, 0, 3), excluding 1 for all image types. This is a bit cleaner, though it reduces functionality.", "Also, after the move to image_ops_impl.py, it's no longer possible to use `decode_*` without prefixing with `gen_image_ops`, hence their return.", "I think you should be able to use a `tf.Assert(false, ...)` as long as it has enough control dependencies to only run in the gif case (which should happen automatically if the assert is placed in try_gif).", "I'm also fine with only allowing `(None, 0, 3)` channels.", "@girving I believe I've got it working with an Assert checking that `channels` isn't set to 1 along the gif path.   I was being a dummy, since `None` can be interpreted as `0` by default. Let me know if this looks alright!", "Jenkins, test this please.", "@mrry: The newly added `decode_image` tests are failing to find the test files on Windows.  Do you know why that would happen on Windows but not on other platforms?", "Looks like it's a path issue: `tensorflow/core/lib/gif/testdata/scan.gif : The system cannot find the path specified.` Curse you backslashes! I'll wrap the files with `os.path.join` and see if that works.\r\n\r\nEdit: it probably isn't this, but I'll keep in the `os.path` as a best-practices thing (unless thats no-bono)", "I assume it's because the working directory isn't what the test expects. (It looks like the similar code `image_ops_test.py` isn't running as part of the Windows build because it's in `python/ops` rather than `python/kernel_tests`.)\r\n\r\nAs a quick workaround, you might try adding `WORKING_DIRECTORY ${tensorflow_source_dir}` to [this line](https://github.com/tensorflow/tensorflow/blob/9d5c3b35f59c42445a0d1e01d46e606d27ae6f7c/tensorflow/contrib/cmake/tf_tests.cmake#L102) and see if it fixes things.", "@girving @mrry I pushed the change to `tf_tests.cmake` so we can see if that does the trick.", "Jenkins, test this please.", "May need to rerun- the GPU test had an aneurysm when trying to `git fetch`", "Jenkins, test this please.", "@zffchen78 The Linux GPU build has twice failed during git fetch.  Do you know what might be going wrong?", "The fact that it returns different tensors shapes based on the  file format is not the 'usual' tensoflow way.\r\nCan we just return the first frame in case of a gif image instead? If the user *really* wants the whole gif sequence then they can use the specialized version. This function will be very useful when you have mixed images formats in your data folder.  ", "@Mistobaan What fixed shape do the other image decode functions return?", "Right now `decode_jpeg` and `decode_png` return tensors with rank 3, while `decode_gif` returns one with rank 4, as it can decode multiple frames from a gif image.", "Jenkins, test this please.", "@Mistobaan you can always `tf.squeeze` the output of this Op of you know that you have gifs that only contain a single frame. Personally, I don't really know what the \"TensorFlowian\" way to do it would be, but it seems wrong to discard image data (by automatically only keeping the first frame of multi-frame gifs). That said, I wasn't sure I should even include gif support when I started this Op due to the rank differences. Here are my thoughts for options:\r\n\r\n1. Leave it as is. Let users figure out how they want to handle gifs\r\n2. Don't allow multi-frame gifs. Throw an assertion error if the number of frames is greater than 1, and squeeze the first dimension out\r\n3. Leave it as is, and create a `contrib` layer that does extra work to ensure that the output ranks match for jpeg, png, and gif", "I'd prefer to either leave it as is or automatically squeeze the first dimension of the gif if it's a single frame (unfortunately that requires another `tf.cond`).  Silently discarding data by picking the first frame is not a good idea.", "If we want to add the optional squeeze to gif I think it can be done in a second pass.  I'm going to merge for now."]}, {"number": 4221, "title": "Fix 'sttdev' to 'stddev' in tutorial(mnist_with_summaries.py)", "body": "When I study the tensorflow recently, I found the wrong characters of the summary directory in tutorial code.\n\nSo I made the pull request to fix it.\n\nThanks for making tutorials!\n", "comments": ["Can one of the admins verify this patch?\n", "@AppleHolic, thanks for your PR! By analyzing the annotation information on this pull request, we identified @danmane, @keveman and @nsthorat to be potential reviewers\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "@tensorflow-jenkins test this please.\n", "@googlebot I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "PR merged. Thank you.\n"]}]