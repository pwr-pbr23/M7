[{"number": 24781, "title": "Failed to build, multiple definition of mkl_dnn", "body": "Failed to build, multiple definition of mkl_dnn, what could be the work around? Thanks\r\n\r\nFollowing is the information that might be helpful\r\n\r\nBazel version\r\n\r\n-bash-4.2$ bazel --help\r\nINFO: Invocation ID: 46e9d78c-edd1-4c29-ae93-1d51511023b9\r\n                                              [bazel release 0.21.0- (@non-git)]\r\n\r\nFollowing is  .tf_configure.bazelrc\r\nbuild --action_env PYTHON_BIN_PATH=\"/opt/hadoop/anaconda3/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/opt/hadoop/anaconda3/lib/python3.7/site-packages\"\r\nbuild --python_path=\"/opt/hadoop/anaconda3/bin/python\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=false\r\nbuild:s3 --define with_s3_support=false\r\nbuild:xla --define with_xla_support=false\r\nbuild:gdr --define with_gdr_support=false\r\nbuild:verbs --define with_verbs_support=false\r\nbuild --action_env TF_NEED_OPENCL=\"0\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\"\r\nbuild:opt --cxxopt=-mavx --copt=-mavx --host_cxxopt=-march=native --host_copt=-march=native\r\nbuild:opt --cxxopt=-mavx2 --copt=-mavx2 --host_cxxopt=-march=native --host_copt=-march=native\r\nbuild:opt --cxxopt=-mfma --copt=-mfma --host_cxxopt=-march=native --host_copt=-march=native\r\nbuild:opt --cxxopt=-mfpmath=both --copt=-mfpmath=both --host_cxxopt=-march=native --host_copt=-march=native\r\nbuild:mkl --define using_mkl=true\r\nbuild:mkl -c opt\r\nbuild:mkl --copt=\"-DEIGEN_USE_VML\"\r\nbuild:monolithic --define framework_shared_object=false\r\nbuild --define framework_shared_object=true\r\nbuild:android --crosstool_top=//external:android/crosstool\r\nbuild:android --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nbuild:android_arm --config=android\r\nbuild:android_arm --cpu=armeabi-v7a\r\nbuild:android_arm64 --config=android\r\nbuild:android_arm64 --cpu=arm64-v8a# Release 1.12.0\r\n\r\nTensorflow source release \r\n# Release 1.12.0\r\n\r\nFollowing is the bazel commandline run:\r\n\r\nbazel build -c opt --jobs 1 --local_resources 2048,0.5,1.0 --verbose_failures --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\n\r\nFollowing is the excerpt of error:\r\n\r\nERROR: /opt/hadoop/tensorflow/tensorflow/python/BUILD:4058:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1): gcc failed: error executing command\r\n  (cd /opt/hadoop/.cache/bazel/_bazel_hadoop/822cf5f4e40bf2cdc8cd17b156fe2dd7/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    PATH=/bin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/opt/hadoop/anaconda3/bin/python \\\r\n    PYTHON_LIB_PATH=/opt/hadoop/anaconda3/lib/python3.7/site-packages \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -shared -o bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow' '-Wl,-rpath,$ORIGIN/../../_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib' -Lbazel-out/k8-opt/bin/_solib_k8/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow -Lbazel-out/k8-opt/bin/_solib_k8/_U@mkl_Ulinux_S_S_Cmkl_Ulibs_Ulinux___Uexternal_Smkl_Ulinux_Slib -Wl,--version-script bazel-out/k8-opt/bin/tensorflow/python/pywrap_tensorflow_internal_versionscript.lds '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-soname,_pywrap_tensorflow_internal.so -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/tensorflow/python/_pywrap_tensorflow_internal.so-2.params)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/batch_normalization.pic.o: multiple definition of 'mkldnn_batch_normalization_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/batch_normalization.pic.o: multiple definition of 'mkldnn_batch_normalization_backward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/batch_normalization.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn::impl::conv_desc_init(mkldnn_convolution_desc_t*, mkldnn_prop_kind_t, mkldnn_alg_kind_t, mkldnn_memory_desc_t const*, mkldnn_memory_desc_t const*, mkldnn_memory_desc_t const*, mkldnn_memory_desc_t const*, int const*, int const*, int const*, int const*, mkldnn_padding_kind_t)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_convolution_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_dilated_convolution_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_convolution_backward_data_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_dilated_convolution_backward_data_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_convolution_backward_weights_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution.pic.o: multiple definition of 'mkldnn_dilated_convolution_backward_weights_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/convolution_relu.pic.o: multiple definition of 'mkldnn_convolution_relu_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/convolution_relu.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_deconvolution_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_dilated_deconvolution_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_deconvolution_backward_data_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_dilated_deconvolution_backward_data_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_deconvolution_backward_weights_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/deconvolution.pic.o: multiple definition of 'mkldnn_dilated_deconvolution_backward_weights_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/deconvolution.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_eltwise_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_eltwise_backward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_relu_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/eltwise.pic.o: multiple definition of 'mkldnn_relu_backward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/eltwise.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_concat_implementation_list() const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_reorder_implementation_list() const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_sum_implementation_list() const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine::get_implementation_list() const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_get_count'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn::impl::engine_factories'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_get_kind'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/engine.pic.o: multiple definition of 'mkldnn_engine_destroy'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/engine.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/inner_product.pic.o: multiple definition of 'mkldnn_inner_product_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/inner_product.pic.o: multiple definition of 'mkldnn_inner_product_backward_data_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/inner_product.pic.o: multiple definition of 'mkldnn_inner_product_backward_weights_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/inner_product.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/lrn.pic.o: multiple definition of 'mkldnn_lrn_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/lrn.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/lrn.pic.o: multiple definition of 'mkldnn_lrn_backward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/lrn.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_primitive_desc_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_view_primitive_desc_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_primitive_desc_equal'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_primitive_desc_get_size'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_get_data_handle'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_memory_set_data_handle'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_concat_primitive_desc_create_v2(mkldnn_primitive_desc**, mkldnn_memory_desc_t const*, int, int, mkldnn_primitive_desc const**, mkldnn_primitive_attr const*)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_concat_primitive_desc_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_sum_primitive_desc_create_v2(mkldnn_primitive_desc**, mkldnn_memory_desc_t const*, int, float const*, mkldnn_primitive_desc const**, mkldnn_primitive_attr const*)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory.pic.o: multiple definition of 'mkldnn_sum_primitive_desc_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory_desc_wrapper.pic.o: multiple definition of 'mkldnn::impl::memory_desc_wrapper::memory_desc_wrapper(mkldnn::impl::memory_pd_t const*)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory_desc_wrapper.pic.o: multiple definition of 'mkldnn::impl::memory_desc_wrapper::memory_desc_wrapper(mkldnn::impl::memory_pd_t const*)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/memory_desc_wrapper.pic.o: multiple definition of 'mkldnn::impl::memory_desc_wrapper::compute_blocking(mkldnn_memory_desc_t&)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/memory_desc_wrapper.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_status2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_dt2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_rmode2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_fmt2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_prop_kind2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_prim_kind2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/mkldnn_debug.pic.o: multiple definition of 'mkldnn_alg_kind2str'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/mkldnn_debug.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/pooling.pic.o: multiple definition of 'mkldnn_pooling_forward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/pooling.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/pooling.pic.o: multiple definition of 'mkldnn_pooling_backward_desc_init'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/pooling.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_desc_destroy'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_get_primitive_desc'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_get_input_at'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_get_output'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_destroy'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive.pic.o: multiple definition of 'mkldnn_primitive_at'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn::impl::scales_t::set(int, int, float const*)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn::impl::scales_t::scale(float)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops::append_sum(float)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops::append_eltwise(float, mkldnn_alg_kind_t, float, float)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr::set_round_mode(mkldnn_round_mode_t)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr::set_post_ops(mkldnn_post_ops const&)'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_clone'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_destroy'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_get_int_output_round_mode'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_set_int_output_round_mode'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_get_output_scales'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_set_output_scales'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_get_post_ops'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_primitive_attr_set_post_ops'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops_create'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/primitive_attr.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/primitive_attr.pic.o: multiple definition of 'mkldnn_post_ops_destr", "comments": ["Hm. Living on the edge with bazel 0.21 and python 3.7. Are you building master branch or a specific tag? ", "This error appears when building using master branch. As a workaround you can try to checkout the 1.12 branch and build.", "Thanks for the comments,  I will checkout the 1.12 branch and rerun", "Thanks for the tips.  \r\n\r\nI checkout the r1.13 and finally successfully build\r\n\r\n(base) -bash-4.2$ tail -f nohup.out\r\nWARNING: /opt/hadoop/tf_branch/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /opt/hadoop/tf_branch/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /opt/hadoop/tf_branch/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 2 targets configured).\r\nINFO: Found 1 target...\r\n[0 / 1] [-----] BazelWorkspaceStatusAction stable-status.txt\r\n[6,161 / 6,162] [-----] Executing genrule @pasta//:augment/__init___py\r\n[9,947 / 9,948] [-----] Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1\r\n[9,947 / 9,948] Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1; 1s local\r\n[9,960 / 9,961] [-----] ProtoCompile tensorflow/contrib/boosted_trees/proto/learner_pb2.py\r\n[9,965 / 9,966] Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2; 1s local\r\n[9,970 / 9,971] [-----] Executing genrule //tensorflow:tf_python_api_gen_v1\r\n[10,093 / 10,095] Executing genrule //tensorflow/python/keras/api:keras_python_api_gen; 1s local\r\n[10,147 / 10,153] Compiling tensorflow/contrib/hadoop/ops/dataset_ops.cc [for host]; 0s local\r\n[10,451 / 10,462] no action\r\n[10,903 / 10,915] [-----] ProtoCompile tensorflow/contrib/decision_trees/proto/generic_tree_model_extensions_pb2.py\r\n[11,226 / 11,237] no action\r\n[11,227 / 11,238] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/stochastic_hard_routing_gradient_op.cc [for host]; 1s local\r\n[11,229 / 11,240] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_function_op.cc [for host]; 0s local\r\n[11,231 / 11,242] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/stochastic_hard_routing_function_op.cc [for host]; 0s local\r\n[11,232 / 11,243] Compiling tensorflow/contrib/tensor_forest/kernels/tree_utils.cc [for host]; 3s local\r\nINFO: From ProtoCompile tensorflow/contrib/rpc/python/kernel_tests/test_example.pb.cc:\r\nbazel-out/k8-opt/genfiles/external/protobuf_archive/src: warning: directory does not exist.\r\n[11,386 / 11,400] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_feature_routing_function_op.cc; 0s local\r\n[11,389 / 11,400] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/stochastic_hard_routing_gradient_op.cc; 0s local\r\n[11,393 / 11,403] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_feature_gradient_op.cc; 1s local\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n\r\n\r\nNeed to note,  I did have some errors that I have resolved by installing below\r\n\r\n conda install -c conda-forge keras-applications\r\n conda install -c conda-forge keras-preprocessing\r\n\r\nYou can do pip install keras-applications, keras-processing, they are required\r\n\r\nand install/upgrade\r\n\r\npip install scipy --upgrade\r\npip install cython --upgrade\r\n\r\nwhich appears resolving error\r\n... \r\n\r\nFile \"stringsource\", line 104, in init scipy.interpolate.interpnd\r\nAttributeError: type object 'scipy.interpolate.interpnd.array' has no attribute '__reduce_cython__'\r\n\r\nIn short, as long as you have proper installation of python 3.x, mine is anaconda python 3.7, latest bazel, latest python modules above, compile tensorflow with MKL enabled will be successful.\r\n\r\nThanks again for your timely tips!\r\n\r\n\r\n", "Just needed to update, I ran the newly build tensorflow on an Xeon server, that tensorflow with MKL enabled, to run recurrent_neural_network.py that recognize MNIST, performance (time to finish) seems to be far worse than stock tensorflow that I had \"pip install\" before.  I expect it should have been better not worse in theory.", "Most optimizations in tensorflow-mkl are targeted for CNN architectures(e.g Resnet, VGGnet). To see if MKLDNN primitives are being called, set the MKLDNN_VERBOSE to 1 (export MKLDNN_VERBOSE=1) before running you script which prints MKL calls on the console. Are you running any standard benchmarking script? Can you please attach the script if possible so we can try from our end.", "\"\"\" Recurrent Neural Network.\r\n\r\nA Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\r\nThis example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\r\n\r\nLinks:\r\n    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\r\n    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\r\n\r\nAuthor: Aymeric Damien\r\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\r\n\"\"\"\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import rnn\r\n\r\n# Import MNIST data\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nwith tf.device('/cpu:0'):\r\n    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\r\n\r\n'''\r\nTo classify images using a recurrent neural network, we consider every image\r\nrow as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\r\nhandle 28 sequences of 28 steps for every sample.\r\n'''\r\n\r\n# Training Parameters\r\nlearning_rate = 0.001\r\ntraining_steps = 10000\r\nbatch_size = 128\r\ndisplay_step = 200\r\n\r\n# Network Parameters\r\nnum_input = 28 # MNIST data input (img shape: 28*28)\r\ntimesteps = 28 # timesteps\r\nnum_hidden = 128 # hidden layer num of features\r\nnum_classes = 10 # MNIST total classes (0-9 digits)\r\n\r\n# tf Graph input\r\nX = tf.placeholder(\"float\", [None, timesteps, num_input])\r\nY = tf.placeholder(\"float\", [None, num_classes])\r\n\r\n# Define weights\r\nweights = {\r\n    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\r\n}\r\nbiases = {\r\n    'out': tf.Variable(tf.random_normal([num_classes]))\r\n}\r\n\r\n\r\ndef RNN(x, weights, biases):\r\n\r\n    # Prepare data shape to match `rnn` function requirements\r\n    # Current data input shape: (batch_size, timesteps, n_input)\r\n    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\r\n\r\n    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\r\n    x = tf.unstack(x, timesteps, 1)\r\n\r\n    # Define a lstm cell with tensorflow\r\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\r\n\r\n    # Get lstm cell output\r\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n\r\n    # Linear activation, using rnn inner loop last output\r\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\r\n\r\nlogits = RNN(X, weights, biases)\r\nprediction = tf.nn.softmax(logits)\r\n\r\n# Define loss and optimizer\r\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\r\n    logits=logits, labels=Y))\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\r\ntrain_op = optimizer.minimize(loss_op)\r\n\r\n# Evaluate model (with test logits, for dropout to be disabled)\r\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n\r\n# Initialize the variables (i.e. assign their default value)\r\ninit = tf.global_variables_initializer()\r\n\r\n# Start training\r\nwith tf.Session() as sess:\r\n\r\n    # Run the initializer\r\n    sess.run(init)\r\n\r\n    for step in range(1, training_steps+1):\r\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\r\n        # Reshape data to get 28 seq of 28 elements\r\n        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\r\n        # Run optimization op (backprop)\r\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\r\n        if step % display_step == 0 or step == 1:\r\n            # Calculate batch loss and accuracy\r\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\r\n                                                                 Y: batch_y})\r\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\r\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\r\n                  \"{:.3f}\".format(acc))\r\n\r\n    print(\"Optimization Finished!\")\r\n\r\n    # Calculate accuracy for 128 mnist test images\r\n    test_len = 128\r\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\r\n    test_label = mnist.test.labels[:test_len]\r\n    print(\"Testing Accuracy:\", \\\r\n        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\r\n", "excerpt of the mkldnn call after  export MKLDNN_VERBOSE=1\r\n\r\n\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0568848\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0720215\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.065918\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0710449\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0610352\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0751953\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0561523\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0720215\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0568848\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.072998\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0639648\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0710449\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0578613\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0739746\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.110107\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.129883\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.128174\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.118896\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.127197\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.109863\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.124023\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.126953\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.126953\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.123047\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.126953\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125977\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.126953\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.12207\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.123047\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.12085\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.123047\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0749512\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.133057\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0661621\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.11792\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.131104\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.147949\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.126953\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.123047\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.126221\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125977\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0710449\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.11499\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.129883\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.12207\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.12793\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.128906\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0700684\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0859375\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.130859\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.116943\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125977\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.126953\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.136963\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.126953\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0710449\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.120117\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0759277\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.12207\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0690918\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.123047\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0668945\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.135986\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.153809\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.141846\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.137939\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.133057\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.14502\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.133057\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.130127\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.132812\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.12793\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.13208\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125977\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.136963\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.12915\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.129883\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0698242\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.13501\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.13208\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125977\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0739746\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.133057\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.130859\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0700684\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.131104\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.13208\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.131104\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.131104\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.13501\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0720215\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.13501\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.135986\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.134033\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.134033\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.13501\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.130859\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.128906\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.102783\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.134033\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.12793\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.134766\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.128174\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.126953\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.072998\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0629883\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.078125\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0830078\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0688477\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0810547\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0720215\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.0610352\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0710449\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.141113\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0720215\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.102051\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0690918\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.134033\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0720215\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.125\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0698242\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.13501\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0739746\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.133057\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0810547\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.130859\r\nmkldnn_verbose,exec,sum,simple:any,undef,in:f32_blocked out:f32_blocked,num:2,128x128,0.0749512\r\nmkldnn_verbose,exec,reorder,jit:uni,undef,in:f32_blocked out:f32_blocked,num:1,128x128,0.12793\r\n", "I ran convolutional_network.py without modification from \r\n\r\nhttps://github.com/aymericdamien/TensorFlow-Examples/tree/master/examples/3_NeuralNetworks\r\n\r\n1. On stock tensorflow pip installed\r\n\r\ntime python convolutional_network.py\r\n\r\nNote: it said:\r\n\r\n2019-01-11 00:17:24.826687: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n...\r\n***\r\n2019-01-11 00:17:24.826687: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n***\r\n\r\nWARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nTesting Accuracy: 0.9894\r\n\r\nreal    0m57.843s\r\nuser    13m16.496s\r\nsys     0m57.078s\r\n\r\n2. On mkl enabled tensorflow I built r1.13 branch:\r\n\r\nrunning the same py code\r\n\r\ntime python convolutional_network.py\r\n\r\n...\r\nWARNING:tensorflow:From /root/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.cprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nTesting Accuracy: 0.9888\r\n\r\nreal    4m2.680s\r\nuser    75m13.335s\r\nsys     115m34.449s\r\n\r\nSummary:\r\n\r\nPython 3.6.5, with stock tensorflow:\r\n\r\nreal    0m57.843s\r\nuser    13m16.496s\r\nsys     0m57.078s\r\n\r\nPython 3.7 with mkl built tensorflow\r\n\r\nreal    4m2.680s\r\nuser    75m13.335s\r\nsys     115m34.449s\r\n\r\nwhich is 4X worse.\r\n\r\nNote:\r\nThere is no stock tensorflow for Python 3.7.\r\n\r\nHope that observation may be helpful.\r\n\r\nI may again rebuild mkl tf for Python 3.6.5 to compare orange with orange, what could be the gap? Python version caused the performance degradation?\r\n\r\n", "Thank you for the information. I don't think tensorflow 1.13 is officially out yet. I don't see an announcement for 1.13. However, I did a quick test on the RNN example from your link with TF 1.12 on a Haswell processor and was able to see about 2x speed-up. Looks like you are using a conda package manager and If you are interested to try it out, rather than building from source you can just do a \"conda install tensorflow\" and that'll get you TF w/ MKLDNN optimizations . However, they are there available   only for 2.7 and 3.6", "Thanks for the info.\r\n\r\nBut I am more interested in build my own than to use a ready made tensorflow.\r\n\r\nIt appears this could be r1.13 specific, so I will try r1.12 and Python 3.6.5 rather than 3.7 (as Python 3.7 does not have matching stock tensorflow for you to pip nor conda install)\r\n\r\nGeorge ", "Sounds good. We will close this ticket since the original issue has been addressed. If you are having performance related issues/questions, please open a new ticket"]}, {"number": 24780, "title": "Fix broken image", "body": "The image tag in this markdown file seems to be broken when viewed on github.", "comments": []}, {"number": 24779, "title": "assert_shapes for validating tensor shapes and their relationships to other tensors ", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am under the impression that many errors users of TensorFlow (and users of libraries making use of TensorFlow) run into has their source in mistakes in passing it tensors with the correct/expected dimension order (and number of dimensions), especially when many tensors are passed together that have relationships expected of them. Due to broadcasting (something very useful and powerful in itself) or sheer misfortune, these mistakes can easily end up hidden as they may not raise any errors neither at graph construction time nor at graph runtime.\r\n\r\nWhat I suggest is an assert function something like the following (see 'assert_shapes' below): \r\n```python\r\ndef model(x, y, param, other_param):\r\n    assert_shapes({\r\n        (\"x\", x): ('N', 'Q'),\r\n        (\"y\", y): ('N', 'D'),\r\n        (\"param\", param): 'Q',\r\n        (\"other param\", other_param): 2,\r\n    })\r\n    ...\r\n```\r\nwhere x, y, param and other_param are tensors. The value of an entry in the dict above are representing the corresponding tensor shape: symbolically or explicitly. In the example above the first dimension of x and y are to always match in size, the second dimension of x must match the size of param and other_param must always have size two. \r\n\r\nIf the caller of the code above writes something like:\r\n```python\r\nm = model(\r\n    x=tf.ones([10, 2]),\r\n    y=tf.ones([1, 10]),\r\n    param=tf.ones([2]),\r\n    other_param=tf.ones([2])\r\n)\r\n```\r\nthis can raise an error as: AssertionError: Tensor 'y' dim 0 was of size 1 but was expected to be 10 as declared by 'x' dim 0.\r\n\r\nA quick demonstrative implementation with some more examples of this can be found here: https://github.com/bodin-e/tensorcheck\r\n\r\n**Will this change the current api? How?**\r\nNo.\r\n\r\n**Who will benefit with this feature?**\r\nUsers interested in catching mistakes in their (or others) code early and explicitly, where the mistakes have to do with tensor shapes and relationships between tensors' shapes. ", "comments": ["I like this idea. I haven't looked at your implementation but I'd approve adding something like this to core tf if you want to send a pull request.\r\n\r\nThe one API nitpick I'd do is replace your signature with something like:\r\n\r\n```\r\n  assert_shapes({\r\n        x: ('N', 'Q'),\r\n        y: ('N', 'D'),\r\n        param: 'Q',\r\n        other_param: 2,\r\n    })\r\n```\r\n\r\nSince this way we don't have to repeat the names of everything and you can already name tensors in tf if you want. WDYT?", "Great. I agree fully about the signature change to skip separate/redundant name passing. \r\n\r\nI'll send a pull request in the coming weeks.", "Closing this issue, as the feature has been added in PR #25422. \r\n\r\nThanks for your contribution to TensorFlow! \ud83d\ude04 "]}, {"number": 24778, "title": "SetDefaultDevice doesn't seem to work; Multiple Tensorflow Session on Separate GPU's Cannot Seem to Speed up Inference", "body": "I am trying to speed up Tensorflow inference by creating multiple Sessions, with each Session loading its own graph on its own GPU. When I ran this same model for a batch of 10 images on a single GPU, it took about 2700 ms. I was hoping I can run 2 batches, one per GPU and process 20 images in the same time frame. Instead, the run time actually took about 5300 ms. So it seems like I was not able to get the speed up I was hoping for.\r\n\r\nI am running Tensorflow 1.7 with 2 Quadro GV100's. I did not get any error messages running my code. Below is my code:\r\n\r\n\tauto options = SessionOptions();\r\n\toptions.config.mutable_gpu_options()->set_visible_device_list(\"0,1\");\r\n\r\n\tNewSession(options, &m_session[0]);\r\n\tNewSession(options, &m_session[1]);\r\n    \r\n\tGraphDef graph_def0;\r\n\tgraph::SetDefaultDevice(\"/device:GPU:0\", &graph_def0);\r\n\tReadBinaryProto(Env::Default(), graphPath, &graph_def0);\r\n\tm_session[0]->Create(graph_def0);\r\n\t\r\n\tGraphDef graph_def1;\r\n\tgraph::SetDefaultDevice(\"/device:GPU:1\", &graph_def1);\r\n\tReadBinaryProto(Env::Default(), graphPath, &graph_def1);\r\n\tm_session[1]->Create(graph_def1);\r\n\t\r\n\t//list0 and list1 are list of images, CallSessionRun()'s 2nd arg is index into m_session\r\n\tstd::future<std::vector<std::vector<tf_detection>>> fut0 = std::async([&]()->std::vector<std::vector<tf_detection>>{\r\n\t\tauto detections = CallSessionRun(list0, 0);\r\n\t\treturn detections;\r\n\t});\r\n\r\n\tstd::future<std::vector<std::vector<tf_detection>>> fut1 = std::async([&]()->std::vector<std::vector<tf_detection>>{\r\n\t\tauto detections = CallSessionRun(list1, 1);\r\n\t\treturn detections;\r\n\t});\r\n\r\n\tauto ans0 = fut0.get();\r\n\tauto ans1 = fut1.get();\r\n\r\ngraph::SetDefaultDevice is supposed to dedicate a GPU for a graph and calling m_session[i]->run() in std::async is supposed to utilize each session concurrently. But it didn't seem to work. Am I missing something?\r\n\r\nThe fact that the run time stays pretty much the same seems to suggest that graph::SetDefaultDevice() does not work and I am only using 1 GPU instead of 2.\r\n\r\nThank you very much for your help in advance!", "comments": ["Hi @asimshankar & @azaks2 , this is a recurrent query from users, i.e. allocation of sessions to GPUs and whether device assignment works on older TensorFlow releases. Please advise. Thanks.", "Hi, @GothamCityPro , have you solved this problem? if yes, could you share the solution with me?", "when I call set_visible_device_list(\"0\"), the 8 gpu card in my machine were stiil used for the tensorflow? why?  Does set_visible_device_list really work ? thank you!", "@GothamCityPro one possibility is that we should call `SetDefaultDevice` **after** reading the graph:\r\n```\r\nReadBinaryProto(Env::Default(), graphPath, &graph_def0);\r\ngraph::SetDefaultDevice(\"/device:GPU:0\", &graph_def0);\r\n```\r\ncould you try again with that fixed?", "Closing because of lack of activity.  If you want this reopened please try @aaroey 's [advice](https://github.com/tensorflow/tensorflow/issues/24778#issuecomment-497144222) first and see if that helps.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24778\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24778\">No</a>\n"]}, {"number": 24777, "title": "Mkl small size allocator fix for reducing lock overhead in critical path", "body": "This change is done to improve the performance of models with small size tensors where the bottleneck is in the memory allocation rather than the actual compute. The map keeping track of small allocations has been re-purposed to instead keep track of large allocations so that the access overhead is small. Also the mutex in the small allocation critical path has been removed to match default cpu allocator implementation.", "comments": ["Hi @penpornk . I have incorporated your feedback. Please let me know if it is okay.\r\n\r\nThanks\r\n"]}, {"number": 24776, "title": "Additional test fixes for 1.13", "body": "", "comments": []}, {"number": 24775, "title": "How to freeze low_latency_svdf model?", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OSX Mojave 10.14.2\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow (binary)\r\n- TensorFlow version (use command below):\r\nlatest\r\n- Python version:\r\nboth 2.7 and 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\n\r\nTrained the low_latency_svdf model according to the tutorial (https://www.tensorflow.org/tutorials/sequences/audio_recognition)\r\n\r\npython tensorflow/examples/speech_commands/train \\\r\n--model_architecture=low_latency_svdf \\\r\n--how_many_training_steps=100000,35000 \\\r\n--learning_rate=0.01,0.005\r\n\r\nThen try to freeze the graph after finished training:\r\n\r\npython tensorflow/examples/speech_commands/freeze.py \\\r\n--model_architecture=low_latency_svdf \\\r\n--start_checkpoint=/tmp/speech_commands_train/low_latency_svdf.ckpt-135000 \\\r\n--output_file=/tmp/my_frozen_graph.pb\r\n\r\nNow the errors are:\r\n\r\n\"  File \"/Users/michaelhe/ml/env/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1582, in restore\r\n    err, \"a mismatch between the current graph and the graph\")\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\r\n\r\nAssign requires shapes of both tensors to match. lhs shape= [256,12] rhs shape= [256,16]\"\r\n\r\n**Describe the expected behavior**\r\nShould generate pb file\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@auvilink is executing a widely used speech recognition tutorial of the binary release on MacOS. They seem to observe a shape mismatch error. @allenlavoie can you advise. Thanks.", "@petewarden do you know who's maintaining the audio recognition example? (This looks like it's more to do with the example itself than saving APIs)", "I stared from scratch, retrained it and it works now. Thanks guys."]}, {"number": 24774, "title": "why is tensorflow.map_fn slow, what is wrong with following code?", "body": "I am trying to use tensorflow map_fn to do parallel computation. However it seems to me that the performance gain is not significant.\r\n\r\nHere are example code running Python 3.6.5, Tensorflow version 1.12.0 on Ubuntu 14.04 LTS, 28 duo cores (Intel(R) Xeon(R) CPU E5-2697 v3 @ 2.60GHz) = 56 processors\r\n\r\nThese same codes running on Amazon AWS SagerMaker ml-p3-xlarge even took longer time, 227 seconds.\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nb'v1.12.0-0-ga6d8ffa' 1.12.0\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport time\r\n# version 1\r\ntic = time.time()\r\nelems = np.array(range(1,1000000), dtype=np.float64)\r\noutput = tf.map_fn(lambda x: x**6 , elems, dtype=tf.float64,  parallel_iterations=56)\r\nsess = tf.Session()\r\n\r\nres = sess.run(output)\r\ntoc = time.time() - tic\r\nprint(\"elapsed=\", toc)  # 29.47 (seconds)\r\n\r\n# version 2\r\ntic = time.time()\r\nelems = np.array(range(1,1000000), dtype=np.float64)\r\noutput = tf.map_fn(lambda x: x**6 , elems, dtype=tf.float64,   parallel_iterations=56)\r\nn_cpus=28\r\n\r\n\r\nwith  tf.Session(\r\nconfig=tf.ConfigProto(log_device_placement=True, \r\ndevice_count={ \"CPU\": n_cpus },\r\ninter_op_parallelism_threads=n_cpus,\r\nintra_op_parallelism_threads=1,\r\n\r\n)) as sess:\r\nres = sess.run(output)\r\n\r\ntoc = time.time() - tic\r\nprint(\"elapsed=\", toc)  # 29.26 (seconds)\r\n\r\n# version 3\r\ntic = time.time()\r\nelems = np.array(range(1,1000000), dtype=np.float64)\r\nx6 = [ x**6 for x in elems]\r\ntoc = time.time() - tic\r\nprint(\"elapsed time=\", toc) # 0.5 seconds\r\n```\r\n\r\nWhat is problem with the above codes? without map_fn, sequential execution version 3 only 0.5 (seconds). \r\n\r\n\r\n", "comments": ["Hi,\r\nI think you encounter this problem because it takes time for tensorflow to create the graph and create the session. A more accurate value for elapsed time will be as follows:\r\n```python\r\nelems = np.array(range(1,1000000), dtype=np.float64)\r\noutput = tf.map_fn(lambda x: x**6 , elems, dtype=tf.float64, parallel_iterations=56)\r\nsess = tf.Session()\r\n\r\ntic = time.time()\r\nres = sess.run(output)\r\ntoc = time.time() - tic\r\nprint(\"elapsed=\", toc)\r\n```", "Hi vidursatija,\r\nThe difference is only 1 second, I re-run with your code. It took 28 (seconds).", "Try increasing the parallel interations maybe? Tensorflow might be slow for such things as the graph has its own overhead. But for huge tasks which are magnitudes bigger than the overhead, the parallization would be faster", "Hello @minhhg , We think in your conversation with @vidursatija , the original query has been clarified. Hence we will close this now. Thanks.", "Hi msymp,\r\nThe issue is not  solved. It is 28 (seconds) with tensorflow vs 0.5 (seconds) with python. Thanks.\r\n", "Hi @tatianashp , can you please advise why the user is not seeing gains from tf.map_fn in parallel runs over 28 cores. Thanks.", "I'm having a similar issue where map_fn is the bottleneck!.. ", "Yes. I am experiencing almost the same issue on my side. I don't want to be rude for the developers who give a lot of contribution to this tool, but I don't understand why on earth the only way to parallelize user code is so slow. Is this happen because tf.map_fn or tf.while_loop is not designed for parallelization?", "I am leaving an additional comment because the former post is just grunting not suitable for asking for help. \r\n\r\nIn my case, I am using multiple tf.map_fn methods within __call()__ method of my custom layer implementation using \"tf.keras.layers.Layer\" class.  I am experiencing very slow graph construction and training time too. \r\n\r\nSince graph construction may have its own overhead, I live with a slow graph construction. However, my custom layer is optimized for some computation. It has to be fast if tf.map_fn is really doing parallelization inside. \r\n\r\nI even optimize the code, so the GPU has almost over 90 percent of utilization, but it gives only a slight speed up. The same algorithm on the custom ASIC(hardware) is verified working and it's fast. So it's not related to the algorithmic problem. \r\n\r\nCould you give me any clue on this? Maybe using tf.map_fn in the custom layer is a bad idea?", "@Han-sok - Can you please provide full information about your issue according to the issue template?", "`tf.map_fn` is really slow.  The bottleneck of my code is `tf.map_fn`. I defined my loss function like this. The input size is `(None, 9)`. In my experiment, the batch size is 1024. But later I found the code is really slow.\r\n\r\n```\r\n    def loss(phi, mu, sigma, t_phi, t_mu, t_sigma):\r\n        _loss = 0.0\r\n        for i in range(phi.shape[0]):\r\n            for j in range(phi.shape[0]):\r\n                _loss += phi[i] * phi[j] * pdf(mu[i], mu[j], tf.sqrt(sigma[i]**2 + sigma[j]**2))\r\n                _loss += t_phi[i] * t_phi[j] * pdf(t_mu[i], t_mu[j], tf.sqrt(t_sigma[i]**2 + t_sigma[j]**2))\r\n                _loss += -2 * phi[i] * t_phi[j] * pdf(mu[i], t_mu[j], tf.sqrt(sigma[i]**2 + t_sigma[j]**2))\r\n        return tf.sqrt(_loss)\r\n\r\n    def reduce_loss(phi, mu, sigma, t_phi, t_mu, t_sigma):\r\n        with tf.variable_scope('mog_loss') as loss:\r\n            stacked = tf.stack([phi, mu, sigma, t_phi, t_mu, t_sigma], 1)\r\n            return tf.map_fn(lambda x: loss(x[0], x[1], x[2], x[3], x[4], x[5]), stacked,\r\n                             parallel_iterations=4)\r\n```", "I experience the same problem with `tf.map_fn` being slow. Is there any way to accelerate this function?", "> I experience the same problem with `tf.map_fn` being slow. Is there any way to accelerate this function?\r\n\r\nFor me, the best solution is to avoid using it. Use vectorization as many as possible.", "> > I experience the same problem with tf.map_fn being slow. Is there any way to accelerate this function?\r\n> \r\n> For me, the best solution is to avoid using it. Use vectorization as many as possible.\r\n\r\nFor my problem i have to use the `tf.boolean_mask`-function, which can produce vectors of different lenghts. This output is further reduced to a single vector. \r\nThats the reason i have to use the `tf.map_fn` since it is possible to compute  the `tf.boolean_mask` for a single element, then reduce it to a scalar and when the computation is done it gets combined to a vector.", "> > I experience the same problem with `tf.map_fn` being slow. Is there any way to accelerate this function?\r\n> \r\n> For me, the best solution is to avoid using it. Use vectorization as many as possible.\r\n\r\nI implemented a customized loss similar to yours, and `tf.map_fn` is running very slow as well. is it possible to share how you vectorized your computation? thanks a lot.", "I am also facing this issue. I wrote a custom layer to process an color image. Input tensor for 16 batch size is(16, 128, 128, 32). Using tf.map_fn is taking indefinite time.", "Please suggest a workaround ", "mark, I also meet this problem ", "@alextp - do you have any thoughts on a workaround for tf.map_fn performance?", "Does tf.vectorized_map work here instead of map_fn?", "I have come across the same issue as yours @Asorie.\r\n\r\nIn my case, I have features which can be variable length vectors. Because tensorflow (and matrix in general) doesn't support variable-length vectors, I am storing them as string. My map_fn converts string type 1D tensor to double 1D tensor computes a single scalar for each row (intersection) and the output of map_fn returns a 1D vector.\r\n\r\nThis is impossible to do in vectorization @GoingMyWay.\r\n\r\nLet me know if you find a workaround @Asorie ", "@alextp I feel vectorized_map should solve this issue, but it comes with many limitations.\r\n\r\nWill revert back if it suits my use-case", "```map_fn``` will create a branch for every element, your code creates 1000000 graph branches which will cause the performance issue. You could try ```vectorized_map``` or custom op kernel.", "@fsx950223 I wanted to change the data type as well inside the `map_fn`. This change isn't allowed in `vectorized_map`, hence the choice", "Whoever is coming here, here are some of the pitfalls to be aware of while using `vectorized_map`: https://www.tensorflow.org/api_docs/python/tf/vectorized_map", "I had the same issues. Vectorized map works faster but still subpar compared with numpy alternatives. Using datasets with same size  `numpy.apply_along_axis` takes 50microseconds (over a numpy array) while `tf.vectorized_map` takes 25ms over a tensor (numpy is 500x faster), if comparing with `tf.map_fn` over a tensor, things go even worse, since it takes `300ms` (numpy is 6000x faster).", "I have same issue as well, trying to map a floor approximation function  to image pixels. Its horribly slow. The Only reason for using TF is faster parallel computation, its strange that Map_fn doesn't do that. \r\n\r\n![image](https://user-images.githubusercontent.com/7234284/85325721-f3f5b080-b480-11ea-9893-00ca4c78cbba.png)\r\n", "@nikhil1008 It looks like your code is also measuring the \"graph construction cost\" which is likely unintentional.\r\n\r\nHere is my measurement, using TF 2.x:\r\n```\r\ndef diff_floor(x):\r\n  xp = 10\r\n  a = tf.range(-xp, xp + 1, 1, 'float32')\r\n  b = tf.tile(x, (2 * xp + 1,))\r\n  b = b - a\r\n  b = 5000 * b\r\n  b = 1 + tf.math.exp(-b)\r\n  b = 1.0 / b\r\n  b = tf.reduce_sum(b) - xp -1\r\n  return b\r\n\r\n@tf.function(experimental_compile=True)\r\ndef f(a):\r\n  return tf.map_fn(diff_floor, tf.reshape(a, (-1, 1)), parallel_iterations=8)\r\n\r\nf(tf.constant(np.random.rand(224, 224, 3), dtype=tf.float32))  # warmup\r\n%timeit f(tf.constant(np.random.rand(224, 224, 3), dtype=tf.float32))\r\n -> 10 loops, best of 3: 21.8 ms per loop\r\n\r\n# Replacing the map_fn with a vectorized_map gives:\r\n->100 loops, best of 3: 5.02 ms per loop\r\n```", "@tejaslodaya Does tf.RaggedTensor work for you case instead of string encoding ? Another common approach is to pad the ragged data and then loop over it carefully to avoid the paddings.\r\n\r\nAlso vectorization support for control flow is better now and it can handle tf.while_loop and tf.cond. i.e. the `fn` passed to tf.vectorized_map can have loops and branches. \r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/control_flow_ops_test.py#L1429\r\nfor example that uses an internal `pfor` API for testing.", "@igormorgado A common pitfall with vectorized_map usage is that the call includes the cost of code vectorization in addition to code execution. So it should typically be placed within a tf.function\r\n\r\nExample:\r\n```\r\ndef f(x):\r\n  return tf.vectorized_map(lambda z: tf.math.log(z), x)\r\n\r\ninp = tf.random.uniform([200, 200])\r\nf(inp)  # warmup\r\n\r\n%timeit f(inp) # Slow call since it vectorizes code in each call!\r\n-> 10 loops, best of 3: 28.9 ms per loop\r\n\r\ncompiled_f = tf.function(f)\r\ncompiled_f(inp)  # warmup\r\n%timeit compiled_f(inp)  # Vectorization process is done once\r\n\r\n-> 1000 loops, best of 3: 328 \u00b5s per loop\r\n\r\nNumPy:\r\ndef np_f(x): \r\n  return np.apply_along_axis(lambda z: np.log(z), 0, arr=x)\r\n\r\nnp_inp = inp.numpy()\r\nnp_f(np_inp)  # warmup\r\n%timeit np_f(np_inp)\r\n-> 1000 loops, best of 3: 961 \u00b5s per loop\r\n\r\n\r\nnp.log(np_inp)  # warmup\r\n%timeit(np.log(np_inp))  # Manually vectorized NumPy\r\n-> 1000 loops, best of 3: 249 \u00b5s per loop\r\n```", "@minhhg @vidursatija\r\n\r\n```\r\n@tf.function(experimental_compile=True)\r\ndef f(elems):\r\n  return tf.map_fn(lambda x: x**6 , elems, dtype=tf.float64, parallel_iterations=56)\r\n\r\nelems = tf.range(1, 1000000, dtype=np.float64)\r\n\r\nf(elems) # warmup\r\n%timeit f(elems)\r\n-> 10 loops, best of 3: 68.3 ms per loop\r\n\r\n@tf.function(experimental_compile=True)\r\ndef vectorized_f(elems):\r\n  return tf.vectorized_map(lambda x: x**6 , elems)\r\n\r\nvectorized_f(elems) # warmup\r\n%timeit vectorized_f(elems)\r\n->1000 loops, best of 3: 1.26 ms per loop\r\n\r\ndef np_f(elems):\r\n  return [x**6 for x in elems]\r\n\r\nnp_elems = np.array(range(1, 1000000), dtype=np.float64)\r\nnp_f(np_elems) # warmup\r\n%timeit np_f(np_elems)\r\n-> 1 loop, best of 3: 498 ms per loop\r\n```", "@agarwal-ashish thanks for the code examples, would you mind formatting it to make it more readable?", "`tf.map_fn` was slow for me as well, which is why I was switching to `tf.vectorized_map`, however some TensorFlow Ops seem to be not supported / vectorized with `tf.vectorized_map`. \r\n\r\nFor example:\r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices([b'\\x00\\x00\\x00\\x00']*512)\r\n\r\ndef f(x):\r\n    x1 = tf.strings.substr(x, 0, 2)\r\n    x2 = tf.strings.substr(x, 2, 2)\r\n\r\n    return tf.io.decode_raw(x1, tf.uint8), tf.io.decode_raw(x2, tf.uint8)\r\n\r\ndataset = dataset.batch(32).map(lambda x: tf.vectorized_map(f, x))\r\n```\r\n\r\n```\r\nWARNING:tensorflow:Using a while_loop for converting Substr\r\nWARNING:tensorflow:Using a while_loop for converting DecodeRaw\r\nWARNING:tensorflow:Using a while_loop for converting Substr\r\nWARNING:tensorflow:Using a while_loop for converting DecodeRaw\r\n```\r\n\r\nWhich results in execution times of `vectorized_map` being similar to `map_fn`. \r\n\r\nAny ideas to solve this?", "vectorized_map does require registering a \"converter\" for each op that defines how that op is vectorized. Please see comments for RegisterPFor that describes how these can be defined, in case you want to contribute these.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parallel_for/pfor.py#L844", "@jsimsa for dataset map calls which has its own vectorization support", "Thanks @agarwal-ashish, but I fear that registering the a converter for the above ops is beyond my skill set, as I just started with TensorFlow. ", "@agarwal-ashish in my case, padding it with 0s and carefully parsing them out using raggedtensors would mean padding all the features till the max of length of all rows. This would mean extra storage cost on our end (inference time, considering we use redis as fast layer) \n\n\nFor anyone having the same problem, we overcame this requirement by creating an explicit training-prediction skew with transformed features used for training (till model function) and applying transformations only for prediction.\n\nConsidering our models are served on tensorflow serving, we have single, one-off predictions rather than a bulk call. Might not work if your models are offline, batch requests. \n", "Hi @minhg! We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24773, "title": "Fixes for rnn_keras_estimator.ipynb", "body": "* wrapping `range(batch_size)` into a `list` call so that it's convertible to `Tensor` in Python 3, where `range` alone returns just a generator\r\n* specifying a `dtype` argument to `LSTMBlockCell`'s constructor\r\n\r\nThese should fix the breakages in #24701.", "comments": []}, {"number": 24772, "title": "tfdbg gets stuck with nested tf.scan calls", "body": "**tfdbg gets stuck with nested tf.scan calls**\r\n\r\nI was running https://github.com/hmishra2250/NTM-One-Shot-TF implementation of MANN on Tensorflow and had some problems using tfdbg with it. The code runs fine without the debugger mode, but gets stuck after the first .run() call when wrapped with the debugger. I've tried greatly reducing parameter and batch sizes to see if memory or performance was the issue, but the problem persisted.\r\n\r\nAfter some tweaking, I believe the source of the debugger not responding is due to nested tf.scan calls (file mann/Model.py, fuction step() calls update_tensor from MANN/utils/tf-utils.py). The reason for that is that the program responded fine when I replaced the inner tf.scan call with a tf.zeros corresponding to the needed output.\r\n\r\nThe weird thing is that https://github.com/vineetjain96/one-shot-mann implementation, which is based on the code mentioned before, works perfectly on tfdbg. The main difference between the implementations is that the second one only uses simple tf.scan calls, without nesting. I find this behavior undesirable and thus report it in this issue.\r\n\r\n- Linux Ubuntu 16.04\r\n- TensorFlow 1.12.0 installed from source\r\n- Python 2.7.12\r\n- Bazel 0.21.0\r\n- GCC 5.4.0\r\n- CUDA 10\r\n- cuDNN 7.4\r\n- GeForce GTX 1080; 8GB RAM GDDR5x; PCI-EXP\r\n- Code to reproduce: wrap https://github.com/hmishra2250/NTM-One-Shot-TF main file (Omniglot.py) with tfdbg and run it.\r\n\r\n", "comments": ["Hello @hsilva664 , Just to be aligned, can we back up and build with bazel 0.15.0 , gcc 4.8 , CUDA 9 / cuDNN 7.4 and check if the tfdbg works then?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24771, "title": "tf.contrib.estimator.early_stopping doesn't work with tf.keras.estimator.model_to_estimator", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary/pip\r\n- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version: 3.5\r\n\r\n**Describe the current behavior**\r\ntf.keras.estimator.model_to_estimator returns tensorflow.python.estimator.estimator.Estimator class.\r\ntensorflow.contrib.estimator.make_early_stopping_hook checks for tf.estimator.Estimator class.\r\n\r\nTypeError: `estimator` must have type `tf.estimator.Estimator`. Got: <class 'tensorflow.python.estimator.estimator.Estimator'>\r\n\r\n**Code to reproduce the issue**\r\nmodel = tf.keras.applications.MobileNetV2()\r\nmodel.compile(tf.keras.optimizers.Adam(),\"categorical_crossentropy\")\r\nestimator = tf.keras.estimator.model_to_estimator(model)\r\nearly_stop = tf.contrib.estimator.stop_if_no_decrease_hook(estimator, \"loss\",1) <--crashes\r\n", "comments": ["@philnguyenresson I am not able to reproduce with tf-nightly. Can you give tf-nightly a try and see if the issue has been solved or not?", "I haven't been able to try tf-nightly, but it does work fine on a more up to date version of tensorflow"]}, {"number": 24770, "title": "TOCO can't convert SpaceToBatchND when current tensor is not a multiply of value", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (or github SHA if from source): 1.12\r\n\r\nI have this command working:\r\n'''\r\ntoco --graph_def_file=/Users/yuval/dev/research-serialization/person_production.pb --output_file=person_production.tflite --input_array=input_1 --output_array=results/truediv  --inference_type=FLOAT --input_shape=**1,512,512,3**\r\n'''\r\nwhile this doesn't\r\n'''\r\ntoco --graph_def_file=/Users/yuval/dev/research-serialization/person_production.pb --output_file=person_production.tflite --input_array=input_1 --output_array=results/truediv  --inference_type=FLOAT --input_shape=**1,800,800,3**\r\n'''\r\nI have some downsampling in my graph, and have dilated convolution with dilation 16 on tesor with sepcial dimensions of size 1/8 of the original. (i.e. 100 in the case that doesn't work) could this be fixed? ", "comments": ["any updates?", "Sorry for the delay. \r\n\r\nWhat error do you get with the second command? We are tracking a similar issue, and I would like to see if its the same one.\r\n\r\nThanks!", "2019-01-29 09:53:18.289956: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 3176 operators, 4966 arrays (0 quantized)\r\n2019-01-29 09:53:18.431102: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 3010 operators, 4717 arrays (0 quantized)\r\n2019-01-29 09:53:18.613614: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 3010 operators, 4717 arrays (0 quantized)\r\n2019-01-29 09:53:18.685014: I tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc:220] Replaced sub-network with Dilated Conv2D op outputting \"bn2.2_dilConv1/convolution\".\r\n2019-01-29 09:53:18.704242: I tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc:220] Replaced sub-network with Dilated Conv2D op outputting \"bn2.4_dilConv1/convolution\".\r\n2019-01-29 09:53:18.722119: F tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1154] Check failed: height_with_paddings % block_height == 0 (4 vs. 0)\r\n", "Any ideas?", "Can you try with the latest TF release? We've made significant improvements to the conversion infrastructure.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 24769, "title": "resolved incompatible dimension reshape before feed into final dense \u2026", "body": "modify simple error in rnn_ptb.py in an example in tf.eager mode\r\nsince rnn cell yield (sequence_length, batch_size, hidden_dim), hidden_dim is correct when reshaping tensor right before feed into the final dense layer, not embedding_dim", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I've signed cla", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 24768, "title": "[XLA] Make RemoveUnusedOperandFromSort a seperate pass", "body": "Current implementation will cause an infinite loop if we put the AlgebraicSimplifier inside the HloPassFix.", "comments": ["I was wrong on that thread, no objections to this PR.\n(@majnemer also agrees, FWTW)\n\nOn Tue, Jan 8, 2019 at 10:29 AM Sanjoy Das <notifications@github.com> wrote:\n\n> *@sanjoy* requested changes on this pull request.\n>\n> See @mkuperst <https://github.com/mkuperst> 's reply on\n> https://groups.google.com/d/msg/xla-dev/Qf-3dPULLEA/1r4QIPB_DwAJ\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24768#pullrequestreview-190378587>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWNe7q7bstPI8-LL-2TaVzQrrT92mRjXks5vBON_gaJpZM4Z1gVL>\n> .\n>\n", "Yeah, I didn't add it as I just wanted to check this is the right thing to do. Let me know if the pass needs to be added in more places.", "@sanjoy  So, do you want me to proceed with running kokoro build checks ?", "@hgadig Yes, Kokoro is fine; I just don't want this change to automatically get merged internally.", "Fixed the nit in the `tensorflow/compiler/xla/service/BUILD`", "@sanjoy  In order to proceed with this PR, your approval is required. Please approve or suggest changes(if any)"]}, {"number": 24767, "title": "Enhanced Operator overriding Eager testcase", "body": "Modified the testcase to verify operator overriding\r\nwith negative inputs.", "comments": ["I could see the \"Ubuntu Sanity\"  test on CI  is failed with below error. Could somebody please help me on this error ?\r\nI am not able to relate this error with the code change.\r\n\r\n```\r\nERROR: error loading package 'tensorflow/lite/experimental/swift': Extension file not found. Unable to load file '@bazel_skylib//lib:partial.bzl': file doesn't exist or isn't a file\r\nINFO: Elapsed time: 5.578s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (92 packages loaded)\r\nFAILED: Build did NOT complete successfully (92 packages loaded)\r\nFAIL: bazel build --nobuild  -- //tensorflow/... -//tensorflow/lite/java/demo/app/... -//tensorflow/lite/examples/android/... -//tensorflow/lite/schema/...\r\n  This is due to invalid BUILD files. See lines above for details.\r\n```", "Could you please consider to merge this PR?", "> Could you please consider to merge this PR?\r\n\r\nSure ,there is an error while merging , can you please correct naming of \r\n\r\n> \"_testOperations(v1, v2):\"\r\n\r\n in to snake_case.", "@rthadur modified the function name as per the review comment, could you please have a look?", "@joyalbin thank you , can you please check failed build errors."]}, {"number": 24766, "title": "Cherrypick additional release 1.13 build fixes.", "body": "", "comments": []}, {"number": 24765, "title": "Added missing test case in LinearModel", "body": "LinearModel is miising test case for sparse_combiner=sqrtn, added the\r\nsame to make the coverage complete.", "comments": ["@annarev Can you pls review this PR", "Nagging Reviewer @annarev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Overall looks good. I added one small comment.\r\nI also added @rohan100jain for review since he seems more familiar with the code.", "@annarev , @rohan100jain & @hgadig , i have updated the code as per the revidw comments, kindly check and if all ok, then approve the PR.", "@rohan100jain , thanks for your review i have updated the code as per your comments, kindly check and approve.", "Thanks @rohan100jain , @hgadig can you please update the label for the PR.\r\n\r\nRegards\r\nAmit"]}, {"number": 24764, "title": "Added missing test case in LinearModel", "body": "LinearModel is miising test case for sparse_combiner=sqrtn, added the\r\nsame to make the coverage complete.", "comments": []}, {"number": 24763, "title": "tf.estimator support multi saver and train_op for training multi model at the same time", "body": "", "comments": ["@zh794390558, Could you provide more details on the issue? As it is not clear to find the root-cause of the issue with the limited details provided, could you fill the following build/installation [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md).  Please provide any code to reproduce the issue. ", "I want using estimator to train multi model at the same time which is  not support now.", "@zh794390558,\r\nSorry for the delayed response. Can you please refer the [documentation of Distribution Strategy using Estimators](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator) and let us know if this is what you are looking for? Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24763\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24763\">No</a>\n"]}, {"number": 24762, "title": "Ensure variables are initialized", "body": "'ReadVarint32FromArray' & 'ReadVarint64FromArray' doesn't ensure output variable 'value' is filled always. Since 'unused_ok' is not checked for true/false initialize the temp variables, so that random values dont propogate.", "comments": []}, {"number": 24761, "title": "TensorFlowLite: failed to load native library: no tensorflowlite_jni in java.library.path", "body": "TensorFlowLite: failed to load native library: no tensorflowlite_jni in java.library.path\r\n", "comments": ["@majiali1995, as it is not clear to find the root-cause of the issue, could you fill the following build/installation [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md).  Please report the installation process followed and commands used. ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@majiali1995 Have you solved that problem? I met this in the unit test."]}, {"number": 24760, "title": "Bug related to tf.sparse_tensor_dense_matmul()", "body": "**Example**\r\nThe following example has some bugs that I can not solve.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport scipy.sparse as sp\r\nfrom scipy.sparse import csc_matrix\r\n\r\ndef sparse_to_tuple(sparse_mx):\r\n    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\r\n    def to_tuple(mx):\r\n        if not sp.isspmatrix_coo(mx):\r\n            mx = mx.tocoo()\r\n        coords = np.vstack((mx.row, mx.col)).transpose()\r\n        values = mx.data\r\n        shape = mx.shape\r\n        return coords, values, shape\r\n\r\n    if isinstance(sparse_mx, list):\r\n        for i in range(len(sparse_mx)):\r\n            sparse_mx[i] = to_tuple(sparse_mx[i])\r\n    else:\r\n        sparse_mx = to_tuple(sparse_mx)\r\n\r\n    return sparse_mx\r\n\r\nA = csc_matrix(np.diag(np.array([1, 2, 3])))\r\nA = sparse_to_tuple(A)\r\n\r\nB = tf.ones([3, 2], tf.float32)\r\nC = tf.sparse_placeholder(tf.float32, shape=(3, 3))\r\nD = tf.sparse_tensor_dense_matmul(C, B)\r\n\r\n# if this line, together with the last line, is commented, it can run\r\nH = tf.matmul(tf.sparse_tensor_dense_matmul(C, B), tf.transpose(B, [1, 0]))\r\n\r\nwith tf.Session() as sess:\r\n    out = sess.run(D, {C: A})\r\n    print('----------------')\r\n    print(out)\r\n    print('----------------')\r\n    print(sess.run(H, {C: A}))\r\n```\r\n\r\n**Error**\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-83-02386eb1014c> in <module>()\r\n     32 \r\n     33 with tf.Session() as sess:\r\n---> 34     out = sess.run(D, {C: A})\r\n     35     print('----------------')\r\n     36     print(out)\r\n\r\n/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    898     collected into this argument and passed back.\r\n    899 \r\n--> 900     Args:\r\n    901       fetches: A single graph element, a list of graph elements,\r\n    902         or a dictionary whose values are graph elements or lists of graph\r\n\r\n/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1111                 ' is not compatible with Tensor type ' + str(subfeed_dtype) +\r\n   1112                 '. Try explicitly setting the type of the feed tensor'\r\n-> 1113                 ' to a larger type (e.g. int64).')\r\n   1114 \r\n   1115           is_tensor_handle_feed = isinstance(subfeed_val,\r\n\r\nValueError: Tensor Tensor(\"Const_132:0\", shape=(2,), dtype=int64) may not be fed.\r\n\r\n**System information**\r\ntesorflow-gpu = 1.9.0\r\npython = Python 3.6.5 :: Anaconda, Inc.\r\nUbuntu 16.04\r\n\r\n\r\n", "comments": []}, {"number": 24759, "title": "tf.function raises OSError \"could not get source code\" when run in a Python shell", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMac OS X 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=\"1.13.0-dev20181226\" (this is the TF 2.0-preview)\r\nGIT_VERSION=\"b'v1.12.0-5133-gc343196842'\"\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\n`tf.function` raises an `OSError` exception with the message `\"could not get source code\"` when I run it in a Python shell. The error does not occur in Jupyter or ipython, and it does not occur when I revert back to TF 1.12.0.\r\n\r\n**Describe the expected behavior**\r\nI expect no error.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nRun this in a Python shell:\r\n\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef foo():\r\n    return 42\r\n\r\nfoo()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nHere is the output of the above commands when I run them in a Python shell:\r\n\r\n```pycon\r\n(tf2) $ python\r\nPython 3.6.6 (default, Jun 28 2018, 05:43:53)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> @tf.function\r\n... def foo():\r\n...     return 42\r\n...\r\n>>> foo()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 336, in __call__\r\n    self._initialize(args, kwds)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 309, in _initialize\r\n    *args, **kwds))\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1024, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1247, in _maybe_define_function\r\n    arg_names=arg_names),\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 456, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 261, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\", line 449, in wrapper\r\n    ), *args, **kwargs)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 293, in converted_call\r\n    experimental_partial_types=partial_types)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 415, in to_graph\r\n    arg_values, arg_types)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 175, in entity_to_graph\r\n    node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py\", line 338, in function_to_graph\r\n    node, source = parser.parse_entity(f)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 34, in parse_entity\r\n    source = tf_inspect.getsource(entity)\r\n  File \"/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py\", line 327, in getsource\r\n    return _inspect.getsource(tf_decorator.unwrap(object)[1])\r\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 968, in getsource\r\n    lines, lnum = getsourcelines(object)\r\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 955, in getsourcelines\r\n    lines, lnum = findsource(object)\r\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py\", line 786, in findsource\r\n    raise OSError('could not get source code')\r\nOSError: could not get source code\r\n```\r\n\r\nBelow is the output when I run the exact same commands in an ipython shell (within the same virtualenv):\r\n\r\n```pycon\r\n(tf2) $ ipython\r\nPython 3.6.6 (default, Jun 28 2018, 05:43:53)\r\nType 'copyright', 'credits' or 'license' for more information\r\nIPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\r\n\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: @tf.function\r\n   ...: def foo():\r\n   ...:     return 42\r\n   ...:\r\n\r\nIn [3]: foo()\r\n2019-01-08 13:02:33.785802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\nOut[3]: <tf.Tensor: id=5, shape=(), dtype=int32, numpy=42>\r\n```\r\n\r\nThe result is also good in Jupyter (within the same virtualenv).\r\n\r\nThis is the output when I switch to another virtualenv based on TF 1.12.0:\r\n\r\n```pycon\r\n(tf1) $ python\r\nPython 3.6.6 (default, Jun 28 2018, 05:43:53)\r\n[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.enable_eager_execution()\r\n>>> @tf.contrib.eager.defun\r\n... def foo():\r\n...     return 42\r\n...\r\n>>> foo()\r\n2019-01-08 12:55:53.087099: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n<tf.Tensor: id=5, shape=(), dtype=int32, numpy=42>\r\n```", "comments": ["I believe this is an autograph issue?", "Yes, it's a currently known limitation. Libraries like [dill](https://github.com/uqfoundation/dill) seem to be are able to get past that, we'll look into those.\r\n\r\nIn the mean time, you can use `@tf.function(autograph=False)` inside a Python shell, but that means you'll have to use graph code.\r\n\r\nWe could revert to `autograph=False` automatically and output a warning in cases like this, I wonder if that would be a better user experience?", "Thanks for your feedback @mdanatg.  Indeed, until a fix is found, I like the idea of reverting to `autograph=False` and outputting a warning. Or perhaps simply having a more explicit error message?", "Agreed, we'll definitely add a better error message.", "We now revert to autograph=False, so closing this issue for now."]}, {"number": 24758, "title": "Update protobuf to 3.6.1 in install_proto3.sh", "body": "This fix updates protobuf to 3.6.1 in ci build's install.sh.\r\n\r\nThe protobuf library in workspace.bzl has been updated to 3.6.1 for some time, though in CI build protoc still uses 3.6.0.\r\n\r\nWhen build a custom op (in docker image `tensorflow/tensorflow:custom-op`) and using grpc's `grpc_defs()` (`grpc/bazel/grpc_deps.bzl`) error happens because grpc ties to a specific Protobuf version:\r\nhttps://github.com/grpc/grpc/blob/ac6795a57e05523b8fa220bc5cef26abb876aae5/bazel/grpc_deps.bzl#L5-L7\r\n\r\n\r\n\r\nThe issue arise because grpc's dependency is 3.6.1 (and an immediate earlier version is 3.5)\r\nso there is no match to fit grpc+protobuf3.6.0 no matter which version of grpc is used.\r\n\r\nThis may not a problem for tensorflow itself as tensorflow does not call `grpc_defs()`. Tensorflow links to protobuf 3.6.1 explicitly.\r\n\r\nStill, it would be good to match the version of installed protoc (3.6.0 before this PR) with protobuf library (3.6.1).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 24757, "title": "Update gradle and build tools version and fix compilation errors", "body": "", "comments": ["What compilation errors are you seeing? Can you be more explicit?", "Unfortunately I am not able to reproduce the compiler errors", "Nagging Reviewer @aselle, @jdduke: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24756, "title": " tf.train.init_from_checkpoint incorrectly raises ValueError", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.0-rc2-3-ga6d8ffa 1.12.0\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)\r\n- CUDA/cuDNN version: CUDA 9.1, cuDNN 7.1.3\r\n- GPU model and memory: 1080Ti (11178MiB)\r\n\r\n**Describe the current behavior**\r\n1. Train a model.\r\n2. Add a new variable to the computation graph.\r\n3. Attempt to initialize the updated graph using `tf.train.init_from_checkpoint`.\r\n4. Notice that a `ValueError` is raised instead of `tf.errors.OpError`.\r\n\r\n```\r\n  File \".../model.py\", line 53, in warmup\r\n    tf.train.init_from_checkpoint(self._model_dir, {'/': 'model/'})\r\n  File \".../tensorflow/python/training/checkpoint_utils.py\", line 187, in init_from_checkpoint\r\n    _init_from_checkpoint, ckpt_dir_or_file, assignment_map)\r\n  File \".../tensorflow/python/training/distribute.py\", line 1053, in merge_call\r\n    return self._merge_call(merge_fn, *args, **kwargs)\r\n  File \".../tensorflow/python/training/distribute.py\", line 1061, in _merge_call\r\n    return merge_fn(self._distribution_strategy, *args, **kwargs)\r\n  File \".../tensorflow/python/training/checkpoint_utils.py\", line 268, in _init_from_checkpoint\r\n    tensor_name_in_ckpt, ckpt_dir_or_file\r\nValueError: Tensor params (params in /) is not found in [...] checkpoint\r\n```\r\n\r\n**Describe the expected behavior**\r\nDocumentation for `tf.train.init_from_checkpoint` states that `tf.errors.OpError` should be raised. It's unclear if the docs are wrong or the implementation is wrong but I think it's the implementation.", "comments": ["ValueError is a bit more standard/specific than OpError. Does seem like the docstring should be updated. Can you send a PR?", "Are there any other code paths in `tf.train.init_from_checkpoint` that could raise `tf.errors.OpError`? If so, simply updating the docstring doesn't seem like the right answer here. I'll also point out that making the doc change would make it harder to distinguish between two pretty distinct error modes: missing variables in graph vs. missing tensors in checkpoints.", "AFAICT the docstring was never correct. So unless there's a strong use-case for a different exception type I'd say it just needs a docstring update.", "Good to know. I won't be able to make the docstring change but hopefully someone can take this on.", "@sharvil Created a PR for that.", "Nice, thanks for the fix @yongtang!"]}, {"number": 24755, "title": "how to read tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\n# Copy and paste here\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["System information:Linux Ubuntu 16.04\r\nTensorFlow installed from:binary\r\nTensorFlow version:1.12 cpu\r\n Dear developers:\r\n    I have transformed the inceptionV3 tflite file using toco, and the type is uint8. I have one task is that to parse the tflite file and transform it to the format I want. The first step is to parse it, I have used tf.contrib.lite.Interpreter to get the tensor shape ,value. But I could not get the  GraphDef using tf.contrib.lite.Interpreter. Is there some way to get the graph and each OP's parameter like convolution stride, kerne size ?\r\nBest regards\r\n\r\n ", "There is no GraphDef equivalent to a .tflite file. If you need to make transformation to the GraphDef, the best approach is to do so before you use toco."]}, {"number": 24754, "title": "Support first_n in tf.print", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n**Describe the feature and the current behavior/state.**\r\n`tf.Print` is deprecated and `tf.print` doesn't support `first_n`. Would be good to support `first_n` in `tf.print` as well.\r\n\r\n**Will this change the current api? How?**\r\nAdd a `first_n` parameter to tf.print.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who has used `first_n` in tf.Print.\r\n\r\n**Any Other info.**\r\nSee Goomics #50.", "comments": ["It's unclear to us where would be the best place to store the state for the first_n argument. Do we make it global across all prints? Do we store it in a kernel (which makes it hard to disambiguate the \"same\" print in different function calls)? Etc.\r\n\r\nInstead, it's easy to workaround by using your own variable and tf conditionals. See the example below (note it's in tf2 style, so uses tf.function with autograph enabled because I'm too lazy to write tf.cond).\r\n\r\n```\r\ncounter = tf.Variable(0)\r\n\r\n@tf.function\r\ndef print_first_n(str, N):\r\n    if counter.assign_add(1) < N:\r\n      tf.print(str)\r\n```", "It seems there's already a solution to the \"where to store first_n state\": store it  in the op just like `tf.Print` does today.\r\n\r\nI have no experience with eager mode or autograph in TF, but it seems that the code you provided doesn't actually reproduce the behavior of `tf.Print`. First, there's just a single counter Variable - does that imply the state is shared across all calls to print_first_n? Second, it's my impression that *all* Variables are stored in checkpoints. If we had a per-print Variable, we'd have trouble restoring if someone decided to add / remove a print statement, no?", "In tf2 variables are only stored in checkpoints if you add them to\ncheckpoints.\n\nRe how many counters you have, this is up to you; as you can see it's easy\nto control with this workaround.\n\nAnd about where to put the state, the solution in tf.Print is\nnondeterministic in many ways, something we do not want to reproduce in\ntf.print.\n\nOn Wed, Jan 16, 2019 at 1:48 PM Sharvil Nanavati <notifications@github.com>\nwrote:\n\n> It seems there's already a solution to the \"where to store first_n state\":\n> store it in the op just like tf.Print does today.\n>\n> I have no experience with eager mode or autograph in TF, but it seems that\n> the code you provided doesn't actually reproduce the behavior of tf.Print.\n> First, there's just a single counter Variable - does that imply the state\n> is shared across all calls to print_first_n? Second, it's my impression\n> that *all* Variables are stored in checkpoints. If we had a per-print\n> Variable, we'd have trouble restoring if someone decided to add / remove a\n> print statement, no?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/24754#issuecomment-454954862>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxT_pAQB3e39NT6hi931Sn1ah6jSGks5vD54hgaJpZM4Z0r2G>\n> .\n>\n\n\n-- \n - Alex\n", "Gotcha, so these are all fair points, and looks like things will be cleaned up in tf2. In the meantime, `tf.Print` is deprecated in tf1 so is there another solution I can use before we switch over to tf2 or will we just have to accept and ignore the deprecation warning?", "If you're willing to get a little dirty, you can look into the silence context manager inside deprecation.py. It's not part of the official API (and it won't be), but you can use it to hackily suppress deprecation warnings."]}, {"number": 24753, "title": "Why name scope will add the prefix to the keras layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.6.0\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: 7.4\r\n\r\n**Describe the current behavior**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef conv_block(inputs, filters, kernel_size, strides, scope):\r\n    \"\"\"Create a simple Conv --> BN --> ReLU6 block\"\"\"\r\n    with tf.name_scope(scope):\r\n        x = tf.keras.layers.Conv2D(filters, kernel_size, strides)(inputs)\r\n        x = tf.keras.layers.BatchNormalization()(x)\r\n        x = tf.keras.layers.Activation(tf.nn.relu6)(x)\r\n        return x\r\n\r\n\r\nif __name__ == '__main__':\r\n    inputs = tf.keras.Input(shape=[224, 224, 3], batch_size=1, name='inputs')\r\n    hidden = conv_block(inputs, 32, 3, 2, scope='block_1')\r\n    outputs = conv_block(hidden, 64, 3, 2, scope='block_2')\r\n    model = tf.keras.Model(inputs, outputs)\r\n    for weight in model.weights:\r\n        print(weight)\r\n```\r\noutput:\r\n```\r\n<tf.Variable 'block_1/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>\r\n<tf.Variable 'block_1/conv2d/bias:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'block_1/batch_normalization/gamma:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'block_1/batch_normalization/beta:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'block_2/conv2d_1/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\r\n<tf.Variable 'block_2/conv2d_1/bias:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'block_2/batch_normalization_1/gamma:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'block_2/batch_normalization_1/beta:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'block_1/batch_normalization/moving_mean:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'block_1/batch_normalization/moving_variance:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'block_2/batch_normalization_1/moving_mean:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'block_2/batch_normalization_1/moving_variance:0' shape=(64,) dtype=float32>\r\n```\r\n\r\n**Describe the expected behavior**\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef conv_block(inputs, filters, kernel_size, strides):\r\n    \"\"\"Create a simple Conv --> BN --> ReLU6 block\"\"\"\r\n    x = tf.keras.layers.Conv2D(filters, kernel_size, strides)(inputs)\r\n    x = tf.keras.layers.BatchNormalization()(x)\r\n    x = tf.keras.layers.Activation(tf.nn.relu6)(x)\r\n    return x\r\n\r\n\r\nif __name__ == '__main__':\r\n    inputs = tf.keras.Input(shape=[224, 224, 3], batch_size=1, name='inputs')\r\n    hidden = conv_block(inputs, 32, 3, 2)\r\n    outputs = conv_block(hidden, 64, 3, 2)\r\n    model = tf.keras.Model(inputs, outputs)\r\n    for weight in model.weights:\r\n        print(weight)\r\n```\r\n\r\noutput\r\n```\r\n<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>\r\n<tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'batch_normalization/gamma:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'batch_normalization/beta:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 32, 64) dtype=float32>\r\n<tf.Variable 'conv2d_1/bias:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'batch_normalization/moving_mean:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'batch_normalization/moving_variance:0' shape=(32,) dtype=float32>\r\n<tf.Variable 'batch_normalization_1/moving_mean:0' shape=(64,) dtype=float32>\r\n<tf.Variable 'batch_normalization_1/moving_variance:0' shape=(64,) dtype=float32>\r\n```\r\n\r\nindeed, i think only variable scope will add a prefix to the variable name, but not name scope.\r\n", "comments": ["One of the reasons to add prefix is that it makes it easier to navigate through a stack of various scopes.\r\nYou can know more about the hierarchy from [here](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/framework/ops.py#L3911).", "@ymodak I think variable_scope can do this, but not name_scope, \r\n\r\nwe should use the variable_scope to add the prefix to variable name and use name_scope to navigate through a stack of various scopes like the operation name", "Also, I have test the api of `tf.layers.xx`, in the `tf.layers.xx`'s function `add_weight`, \r\nhttps://github.com/tensorflow/tensorflow/blob/9ef579d1ee7a5b8a420c061102f4e745c73976d6/tensorflow/python/layers/base.py#L413\r\nbut at the `tf.keras.layers.xxx`'s function `add_weight`, there is not such **set scope**\r\n", "variable_scope is going to be removed in tf 2.0, while name_scope is preserved.", "@facaiy how can i set prefix to variable name using tf.nn.xx but not tf.keras.layers?"]}, {"number": 24752, "title": "saved_model_portable_proto target doesn't exist", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.12.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source):\r\nBuild label: 63d51e23a26a25116e659a75113ee654a62213c5\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Jan 3 01:24:53 2019 (1546478693)\r\nBuild timestamp: 1546478693\r\nBuild timestamp as int: 1546478693\r\n\r\n- GCC/Compiler version (if compiling from source): 8.2.0\r\n\r\n**Describe the problem**\r\n\r\nDangling reference to `//tensorflow/core:saved_model_portable_proto` within `if_mobile()` guard here:\r\nhttps://github.com/tensorflow/tensorflow/blob/fbbecc81be3b1eab0c15020ce3b0566e60e25c7f/tensorflow/cc/saved_model/BUILD#L50\r\n\r\nSeems similar to:\r\nhttps://github.com/tensorflow/tensorflow/issues/4312\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nCaused failure of unrelated `genquery` rule; even though if_mobile was disabled, it seems to error out on undefined dependencies.", "comments": ["Thanks for the report! I don't know when we'll have a chance to work on this, since it's not directly causing a build error, so closing for now.", "Is there any actual work necessary? Wouldn't it be enough to just remove that  target?", "This dangling reference breaks configuration-agnostic Bazel queries (e.g. `bazel query`, not `bazel cquery`).", "This was fixed in b39b1ed24b4814db27d2f748dc85c10730ae851d btw"]}]