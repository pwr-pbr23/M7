[{"number": 2654, "title": "Saver producing large meta files after long run", "body": "The code is for deep q-learning and for reference it's [here](https://github.com/domluna/deep-rl-gym-tutorials/tree/master/q_learning) and the keras model is [here](https://github.com/domluna/deep-rl-gym-tutorials/blob/master/q_learning/models.py)\n\nTraining after 400 timesteps:\n\nsave file is 9.7MB\nmeta file is 25MB\n\nafter 2,750,000 timesteps\n\nsave file is 9.7MB\nmeta file is 1.4GB\n\nafter 3M timesteps\n\nsave file is 9.7MB\nmeta file is 1.5GB\n\nSo the meta file grows over 100MB per 275,000 timesteps. The saver keeps the last 5 checkpoints by default so you could image my shock when I woke up and my Dropbox was full!\n\nAnyway, this kind of growth is unreasonable is there a way to keep it in check?\n", "comments": ["@domluna: Assuming the meta file is a MetaGraphDef, can you check whether it's the `GraphDef` part that's growing?  One possibility is that you are adding new ops to the graph each step, causing the graph to grow linearly.\n", "FYI, if you aren't intending to use the `MetaGraphDef`, you can call `saver.save(..., write_meta_graph=False)` to avoid writing it with each checkpoint.\n", "Assuming I've done this right, using https://gist.github.com/domluna/f06921196b539a397a6a24ae7f571276 to check the ByteSize. I get the following step/size outputs:\n\n20000, 30376060\n30000, 35425130\n40000, 40474200\n50000, 45523270\n60000, 50572340\n\nSo `GraphDef` is growing, just not sure why ...\n", "@domluna: Try printing out the node names in the graph each step.  If you see it growing over time, it probably means you're adding new nodes to the graph over time. \n", "You can do \"tf.get_default_graph().finalize()\" after you've done modifying\nyour graph, that'll throw an error on any new modifications\n\nOn Wed, Jun 8, 2016 at 8:56 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> @domluna https://github.com/domluna: Try printing out the node names in\n> the graph each step. If you see it growing over time, it probably means\n> your adding new nodes to the graph over time.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2654#issuecomment-224636494,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHFIHCpBF19MbaRxJc1ZZyBQzE3viks5qJuYygaJpZM4IuJSB\n> .\n", "So an update on this. The `finalize` method is great I'm using it on all my code now to make sure nothing sketchy is going on.\n\nI think I found the problem with this particular code and it might be an issue with Keras. When I call `train_on_batch` on a Keras model.\n\nThe culprit being \n\nhttps://github.com/fchollet/keras/blob/a1610eb2747e2020b9460b1f60e6e750d253fffb/keras/engine/training.py#L673\n\nAfter some more digging it appears `K.gradients` and in turn `tf.gradients` is being called each time this is called.\n", "@fchollet: Is there a Keras issue here?\n", "@domluna no, if you read the code you linked, you can see that `tf.gradients` is called exactly once, then the results are cached in the model instance (`train_function` attribute). Calling `train_on_batch` many times only results in a single call to `tf.gradients` at the very beginning.\n\nSince `Model` cannot be modified after being compiled, this cache is never invalidated (unless you recompile the model).\n\n@girving as far as I can tell, no.\n", "@fchollet my mistake, thanks for the explanation.\n"]}, {"number": 2653, "title": "CUDA_VISIBLE_DEVICES not working?", "body": "gpu0 is used by other guys, So I want to use another gpu to run cifar10_train(cifar10_multi_gpu_train).\nBut it seems that tensorflow still try to use gpu0 even I specify CUDA_VISIBLE_DEVICES=2\n\n$ CUDA_VISIBLE_DEVICES=2 python ~/tensorflow2/lib/python2.7/site-packages/tensorflow/models/image/cifar10/cifar10_train.py --data_dir ~/cifar10_data --train_dir ~/cifar10_train_gpu\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:08:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:08:00.0)\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM\nAborted (core dumped)\n\n$ nvidia-smi\n|===============================+======================+======================|\n|   0  GeForce GTX TIT...  Off  | 0000:04:00.0     Off |                  N/A |\n| 32%   70C    P2   100W / 250W |  10135MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX TIT...  Off  | 0000:05:00.0     Off |                  N/A |\n| 22%   41C    P0    73W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX TIT...  Off  | 0000:08:00.0     Off |                  N/A |\n| 22%   41C    P0    67W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   3  GeForce GTX TIT...  Off  | 0000:09:00.0     Off |                  N/A |\n| 22%   42C    P0    74W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   4  GeForce GTX TIT...  Off  | 0000:84:00.0     Off |                  N/A |\n| 22%   39C    P0    75W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   5  GeForce GTX TIT...  Off  | 0000:85:00.0     Off |                  N/A |\n| 22%   39C    P0    69W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   6  GeForce GTX TIT...  Off  | 0000:88:00.0     Off |                  N/A |\n| 22%   38C    P0    74W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n|   7  GeForce GTX TIT...  Off  | 0000:89:00.0     Off |                  N/A |\n| 22%   38C    P0    69W / 250W |     23MiB / 12287MiB |      0%   E. Process |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      7266    C   /some/app       10111MiB \n", "comments": ["CUDA_VISIBLE_DEVICES is working but tensorflow still say \"/gpu0\".\nsee https://github.com/tensorflow/tensorflow/issues/2033\nit works if I build tensorflow from source\n", "I think tensorflow will automatically number the gpu form 0 to (NUM_CUDA_VISIBLE_DEVICES - 1). check the `pciBusID` of the GPU may be the correct option."]}, {"number": 2652, "title": "Backward pass of broadcasting on GPU is non-deterministic", "body": "``` python\nimport tensorflow as tf\n\ndef run(on_gpu):\n    tf.reset_default_graph()\n    tf.set_random_seed(42)\n    with tf.device('/gpu:0' if on_gpu else '/cpu:0'):\n        a = tf.random_normal([16, 16])\n        b = tf.get_variable('b', shape = [], initializer = tf.constant_initializer(value = 0.0))\n        c = a*b\n        grad = tf.gradients(c, [b], gate_gradients=tf.train.Optimizer.GATE_GRAPH)[0]\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    grad_val = sess.run(grad)\n    return grad_val\n\nfor i in xrange(20):\n    print repr(run(on_gpu=True)),\nprint ''\nfor i in xrange(20):\n    print repr(run(on_gpu=False)),\n```\n\nResult:\n\n```\n23.066511 23.066511 23.066513 23.066513 23.066511 23.066513 23.066509 23.066513 23.066513 23.066511 23.066513 23.066511 23.066513 23.066513 23.066509 23.066511 23.066513 23.066513 23.066511 23.066511 \n23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509 23.066509\n```\n\nAs you can see, consistent result across CPU runs but inconsistent result across GPU runs.\n\nNo doubt a CUDA reduction order issue, but it'd be really nice if we can have deterministic reduction. I am using tf 0.8.0 (self-compiled against CuDNN v5). CuDNN version is 5005 (not rc)\n", "comments": ["Unfortunately, the reduction ops on GPU use asynchronous atomic adds, and are therefore fundamentally nondeterministic for floating point.  Making them deterministic would require either tree-structured reductions or integer math, both significantly slower.\n\nI can leave this open with contributions welcome if you'd like (with an adjusted title), but it'll be a lot of work if someone tries to take it on, and it's unclear how best to make it happen automatically.  Even if one added deterministic reductions as an option (either as a separate op or as an attr on the existing ops), we'd need an unpleasant global flag to turn this on when building the backward pass.\n", "I can understand if that's the case. Thanks for the response.\n", "By the way, pure warp-shuffle (shfl_down, or shfl_xor for keep_dim) based block reduction doesn't seem to be that much slower than warp-shuffle+atomic\n", "@metap Do you have a link for that?  I don't quite follow, especially the bit about keep_dim since that doesn't change the computation structure.\n", "Here's the link: https://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/\nUsing shfl_xor just keeps the same copy of data with in a warp (not mentioned in the article) and should be better in keep_dim situations. One example is in a [fast CUDA word2vec implementation](https://github.com/ChenglongChen/word2vec_cbow/blob/master/cbow.cu) that utilizes the warp shuffle to reach 3 million tokens/s on a Titan X with the default (as in the original implementation given by Mikolov) CBOW negative sampling setting\n", "Cc @zheng-xq @benoitsteiner in case more GPU knowledgeable folk want to take a look.  Determinism would certainly be nice to have if we can get it.\n", "The shfl_down results are only useful within a single wrap. That technique itself would take a second pass to accumulate the results for each block. \n\nIn general, there is no guarantee of determinism on GPU. Therefore, we are not sure how much effort we want to spend on it. Even if we can fix this particular kernel, we have other Cudnn kernels that do have non-determinism. \n", "@zheng-xq could you give some examples of other CUDNN kernels that have non-determinism? I'd like to explore this a little. Just for education purposes, because like what you mentioned, it's probably not worth the effort unless some major thing happens down the road.", "Which op exactly is non-deterministic here? These are the ops in the graph:\r\n\r\n```\r\n[<tf.Operation 'random_normal/shape' type=Const>,\r\n <tf.Operation 'random_normal/mean' type=Const>,\r\n <tf.Operation 'random_normal/stddev' type=Const>,\r\n <tf.Operation 'random_normal/RandomStandardNormal' type=RandomStandardNormal>,\r\n <tf.Operation 'random_normal/mul' type=Mul>,\r\n <tf.Operation 'random_normal' type=Add>,\r\n <tf.Operation 'b/Initializer/Const' type=Const>,\r\n <tf.Operation 'b' type=VariableV2>,\r\n <tf.Operation 'b/Assign' type=Assign>,\r\n <tf.Operation 'b/read' type=Identity>,\r\n <tf.Operation 'mul' type=Mul>,\r\n <tf.Operation 'gradients/Shape' type=Const>,\r\n <tf.Operation 'gradients/Const' type=Const>,\r\n <tf.Operation 'gradients/Fill' type=Fill>,\r\n <tf.Operation 'gradients/mul_grad/Shape' type=Const>,\r\n <tf.Operation 'gradients/mul_grad/Shape_1' type=Const>,\r\n <tf.Operation 'gradients/mul_grad/BroadcastGradientArgs' type=BroadcastGradientArgs>,\r\n <tf.Operation 'gradients/mul_grad/mul' type=Mul>,\r\n <tf.Operation 'gradients/mul_grad/Sum' type=Sum>,\r\n <tf.Operation 'gradients/mul_grad/Reshape' type=Reshape>,\r\n <tf.Operation 'gradients/mul_grad/mul_1' type=Mul>,\r\n <tf.Operation 'gradients/mul_grad/Sum_1' type=Sum>,\r\n <tf.Operation 'gradients/mul_grad/Reshape_1' type=Reshape>,\r\n <tf.Operation 'gradients/mul_grad/tuple/group_deps' type=NoOp>,\r\n <tf.Operation 'gradients/mul_grad/tuple/control_dependency' type=Identity>,\r\n <tf.Operation 'gradients/mul_grad/tuple/control_dependency_1' type=Identity>,\r\n <tf.Operation 'init' type=NoOp>]\r\n```\r\nDo you expect that `BroadcastGradientArgs` is non-deterministic?\r\n\r\nFor reference, I tried to run this (with both TF 1.4.1, and also TF 1.12.0), and it seems deterministic to me (980 GTX, CUDA 9.1).\r\n\r\n", "The current high-level status is that there are now solutions for TensorFlow determinism when running on GPUs related to cuDNN (convolutions and max-pooling) and bias_add. Please see the following repo for up-to-date status: https://github.com/NVIDIA/tensorflow-determinism"]}, {"number": 2651, "title": "fix minor mispells in tensorflow/tools/dist_test", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2650, "title": "Bump version to 0.9.0rc0", "body": "", "comments": ["Jenkins, test this please\n", "LGTM\n", "2nd run passed all tests. Merging \n", "Already merged.\n"]}, {"number": 2649, "title": "Branch 124012080", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n"]}, {"number": 2648, "title": "Saver.restore() causes segfault in distributed mode", "body": "I use a custom saver object in distributed mode that operates over a subset of the parameters in my model so that I can perform transfer learning between my models. I'm running into a situation where loading weights for one of my models causes a segfault, and I can't seem to figure out why. I'm running this code:\n\n``` python\n...\nsupervisor = tf.train.Supervisor(is_chief=(task_index == 0),\n                                 logdir=log_dir,\n                                 init_op=init_op,\n                                 saver=None,\n                                 summary_op=training_summary_op,\n                                 global_step=global_step,\n                                 summary_writer=summary_writer)\n\nif task_index != 0:\n    logger.info('waiting for session.')\nelse:\n    logger.info('starting session.')\nwith supervisor.prepare_or_wait_for_session(\n        server.target, config=tf.ConfigProto(allow_soft_placement=True)) as sesh:\n    if task_index != 0:\n        while True:\n            if supervisor.should_stop():\n                logger.info('training completed')\n                return\n            if ready.eval(sesh):\n                break\n            sleep(0.01)\n    logger.info('session started.')\n\n    if task_index == 0:\n        if isfile(save_file):\n            logger.info('loading from save file: %s' % save_file.split('/')[-1])\n            saver.restore(sesh, save_file)\n        elif transfer_file is not None and isfile(transfer_file):\n            logger.info('transfering weights from file: %s' % transfer_file.split('/')[-1])\n            loader.restore(sesh, transfer_file)\n            logger.info('executing post transfer ops')\n            sesh.run(post_transfer_ops)\n        sesh.run(is_ready)\n...\n```\n\nThe transfer file exists on both the parameter servers and the chief worker and running the code results in this output:\n\n```\nINFO: starting session.\nINFO: session started.\nINFO: transfering weights from file: test_train5.0.ckpt\nSegmentation fault (core dumped)\n```\n\nThe graph I'm using is identical to one that I can deploy on a single machine without error, I can even load the weights into the local version, but even if an issue with parameter names were the issue I would still hope that it would give me an error instead of a segfault. Note that this issue does not appear for every model, though it is deterministic.\n", "comments": ["This is unexpected, and definitely a bug. When you run it in the single-machine mode, are you using the `tf.train.Supervisor`?\n\nAlso, which process segfaults? Is it the Python client that calls `saver.restore()` or the parameter server that runs the restore op?\n", "On a single machine I use a simple session. The segfault is in the chief worker process, which initiates the call to Saver.restore(). I don't know what the parameter server does as my current setup forwards all logging to the chief worker process.\n", "Can you try running the chief worker under `gdb` and reporting the stack trace for the failure? Also, are you running the 0.8 release or a nightly/from-source build of TensorFlow?\n", "its a from source build from a few days ago, I'll try gdb and be back in a sec\n", "```\nProgram received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7f4c577fe700 (LWP 8517)]\n0x00007f4f5aee4ff6 in google::protobuf::MessageLite::ParseFromCodedStream(google::protobuf::io::CodedInputStream*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n(gdb) backtrace\n#0  0x00007f4f5aee4ff6 in google::protobuf::MessageLite::ParseFromCodedStream(google::protobuf::io::CodedInputStream*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007f4f5aac8dfb in grpc::DeserializeProto(grpc_byte_buffer*, google::protobuf::Message*, int) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007f4f5aa820e6 in grpc::CallOpSet<grpc::CallOpRecvInitialMetadata, grpc::CallOpRecvMessage<tensorflow::RunGraphResponse>, grpc::CallOpClientRecvStatus, grpc::CallNoOp<4>, grpc::CallNoOp<5>, grpc::CallNoOp<6> >::FinalizeResult(void**, bool*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007f4f5aac82e2 in grpc::CompletionQueue::AsyncNextInternal(void**, bool*, gpr_timespec) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007f4f5aa53dba in std::_Function_handler<void (), tensorflow::GrpcWorkerCache::GrpcWorkerCache(tensorflow::GrpcChannelCache*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007f4f58ebea60 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at ../../../../../src/libstdc++-v3/src/c++11/thread.cc:84\n#6  0x00007f4f63e60184 in start_thread (arg=0x7f4c577fe700) at pthread_create.c:312\n#7  0x00007f4f63b8d37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n(gdb) print buf\n$1 = 0x159f4c0 \"Unknown error 58\"\n```\n", "I'm going to gdb my parameter server to see if I can get anything else\n", "Thanks, it looks like it is failing to deserialize a large proto. We just fixed a related bug, which will be part of https://github.com/tensorflow/tensorflow/pull/2649, so if you could try out your code against the PR, that would be great.\n\nThere's a higher level issue, which is that running the saver shouldn't be causing large amounts of data to be transferred across the network. Try passing `sharded=True` to the [`tf.train.Saver` constructor](https://www.tensorflow.org/versions/r0.8/api_docs/python/state_ops.html#Saver.__init__) and that might also solve the problem without rebuilding.\n", "Thanks! I'll let you know if it works.\n", "It looks like nonsharded savers save to the chief worker (if called from that process) and the sharded saver saves to the parameter server, weird\n", "It looks like I'm still getting a segfault, and now I also need to manage sharded files, I'll see if I can get the gdb backtrace later\n", "Looks like the exact same trace:\n\n```\nProgram received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7f15c7c32700 (LWP 67911)]\n0x00007f18888f6ff6 in google::protobuf::MessageLite::ParseFromCodedStream(google::protobuf::io::CodedInputStream*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n(gdb) backtrace\n#0  0x00007f18888f6ff6 in google::protobuf::MessageLite::ParseFromCodedStream(google::protobuf::io::CodedInputStream*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007f18884dadfb in grpc::DeserializeProto(grpc_byte_buffer*, google::protobuf::Message*, int) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007f18884940e6 in grpc::CallOpSet<grpc::CallOpRecvInitialMetadata, grpc::CallOpRecvMessage<tensorflow::RunGraphResponse>, grpc::CallOpClientRecvStatus, grpc::CallNoOp<4>, grpc::CallNoOp<5>, grpc::CallNoOp<6> >::FinalizeResult(void**, bool*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007f18884da2e2 in grpc::CompletionQueue::AsyncNextInternal(void**, bool*, gpr_timespec) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007f1888465dba in std::_Function_handler<void (), tensorflow::GrpcWorkerCache::GrpcWorkerCache(tensorflow::GrpcChannelCache*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007f18868d0a60 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at ../../../../../src/libstdc++-v3/src/c++11/thread.cc:84\n#6  0x00007f1891872184 in start_thread (arg=0x7f15c7c32700) at pthread_create.c:312\n#7  0x00007f189159f37d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n(gdb) print buf\n$1 = 0x13c9510 \"Unknown error 58\"\n```\n", "That's strange, because I think `grpc::DeserializeProto()` (`#1` on the stack trace) no longer exists in the 0.14 release of gRPC, which we're now using.\n\n#2649 is now merged, so can you try this again with either the latest nightly or the 0.9 release candidate?\n", "I'll update the cluster to r0.9 and try again\n", "```\nProgram received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fbc20ff9700 (LWP 59920)]\n0x00007fbe0a54fd16 in google::protobuf::MessageLite::ParseFromCodedStream(google::protobuf::io::CodedInputStream*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n(gdb) backtrace\n#0  0x00007fbe0a54fd16 in google::protobuf::MessageLite::ParseFromCodedStream(google::protobuf::io::CodedInputStream*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#1  0x00007fbe0a0c2b30 in grpc::UnlimitedSizeProtoSerializationTraits<tensorflow::RunGraphResponse>::Deserialize(grpc_byte_buffer*, tensorflow::RunGraphResponse*, int) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#2  0x00007fbe0a0c2f89 in grpc::CallOpSet<grpc::CallOpRecvInitialMetadata, grpc::CallOpRecvMessage<tensorflow::RunGraphResponse>, grpc::CallOpClientRecvStatus, grpc::CallNoOp<4>, grpc::CallNoOp<5>, grpc::CallNoOp<6> >::FinalizeResult(void**, bool*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007fbe0a10eac2 in grpc::CompletionQueue::AsyncNextInternal(void**, bool*, gpr_timespec) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007fbe0a09ade8 in std::_Function_handler<void (), tensorflow::GrpcWorkerCache::GrpcWorkerCache(tensorflow::GrpcChannelCache*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007fbe085bda60 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at ../../../../../src/libstdc++-v3/src/c++11/thread.cc:84\n#6  0x00007fbe13a36184 in start_thread (arg=0x7fbc20ff9700) at pthread_create.c:312\n#7  0x00007fbe1376337d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\n(gdb) print buf\n$1 = 0x26de4b0 \"Unknown error 58\"\n```\n", "Thanks for persisting with this. I've been trying to reproduce this, but haven't managed to get the exact same failure.\n\nOne potential issue is that protobuf doesn't handle messages larger than 2GB. We haven't optimized this because we haven't seen any realistic models where it is necessary to transfer this much in a single step, but it obviously shouldn't crash like this. If I set up two servers, and try to transfer a tensor larger than 2GB from one to the other, it fails with `SIGABRT` on the _sending_ side. However, I haven't managed to get it to \"successfully\" send a tensor that the other side fails to retrieve.\n\nAre there any very large (> ~2GB) variables in your model that could be brushing up against the protobuf limit?\n", "The weird thing is that the model is only about 10M on disk\n\n```\n$ ls -lh\n...\n-rw-r--r--    1 alexatknit  staff    10M Jun  3 12:09 test_train5.0.ckpt\n...\n```\n", "Also, like I said before, it doesn't happen for all of my models, but it is deterministic. \n\nIf the weights are loaded on the parameter server, why would the segfault happen on the chief worker? Wouldn't the worker receive the loaded parameters in the same way that it updates its parameters during async sgd?\n", "Ah, if the model is only 10M, then my hypothesis about protobuf overflow is almost certainly wrong. (There were certainly segfault- and `SIGTERM`-causing bugs in that path, which should be fixed now, or at least in the next push from the internal branch.)\n\nI see two potential options for fixing this, but I'm afraid I'm going to need your help.\n1. Can you package up a minimum failing example, which I'll try and run locally to track down the failure?\n2. Can you try building from source with `bazel build -c dbg`, and report a stack trace with line numbers, so that we can see exactly where it's failing?\n\nIf you're already familiar with building from source, I suspect option 2 might be easier.\n", "1 is going to be a problem, but I can do 2\n", "Does the whole cluster need to be in debug, or just the master worker? (I've temporarily repurposed those machines)\n", "Thanks! Only the process that you expect to crash needs to be in debug - it should be able to communicate with other workers that are running release binaries.\n", "I don't see any obvious debug logging, where can I expect to find the stack trace?\n", "The easiest way to get it is probably to run under `gdb` (otherwise you could turn on core file recording and load one of those in gdb).\n", "```\n(gdb) bt no-filters full\n#0  0x00007f63b97ac264 in InlineParseFromCodedStream (message=0x7f6111bba7a0, input=0x7f61ba7fbc30) at external/protobuf/src/google/protobuf/message_lite.cc:128\nNo locals.\n#1  google::protobuf::MessageLite::ParseFromCodedStream (this=0x7f6111bba7a0, input=0x7f61ba7fbc30) at external/protobuf/src/google/protobuf/message_lite.cc:168\nNo locals.\n#2  0x00007f63b8ea53c6 in grpc::UnlimitedSizeProtoSerializationTraits<tensorflow::RunGraphResponse>::Deserialize (buffer=0x7f61a801c520, msg=0x7f6111bba7a0, max_message_size=0)\n    at ./tensorflow/core/distributed_runtime/rpc/grpc_serialization_traits.h:187\n        reader = {<google::protobuf::io::ZeroCopyInputStream> = {_vptr.ZeroCopyInputStream = 0x7f63c331a630 <vtable for grpc::tensorflow_helper::GrpcBufferReader+16>}, byte_count_ = 0, backup_count_ = 0, \n          reader_ = {buffer_in = 0x7f61a801c520, buffer_out = 0x7f61a801c520, current = {index = 0}}, slice_ = {refcount = 0x7f61ba7fbd50, data = {refcounted = {bytes = 0x0, length = 140057717488880}, \n              inlined = {length = 0 '\\000', bytes = \"\\000\\000\\000\\000\\000\\000\\000\\360\\274\\177\\272a\\177\\000\"}}}}\n        decoder = {buffer_ = 0x0, buffer_end_ = 0x0, input_ = 0x7f61ba7fbc80, total_bytes_read_ = 0, overflow_bytes_ = 0, last_tag_ = 0, legitimate_message_end_ = false, aliasing_enabled_ = false, \n          current_limit_ = 2147483647, buffer_size_after_limit_ = 0, total_bytes_limit_ = 2147483647, total_bytes_warning_threshold_ = 2147483647, recursion_budget_ = 100, recursion_limit_ = 100, \n          extension_pool_ = 0x0, extension_factory_ = 0x0, static kDefaultTotalBytesLimit = 67108864, static kDefaultTotalBytesWarningThreshold = 33554432, static default_recursion_limit_ = 100}\n        result = {static OK = @0x7f63c42c3390, static CANCELLED = @0x7f63c42c33a0, code_ = grpc::OK, details_ = \"\"}\n#3  0x00007f63b8ea399e in grpc::CallOpRecvMessage<tensorflow::RunGraphResponse>::FinishOp (this=0x7f60e4002868, status=0x7f61ba7fbe1f, max_message_size=0) at external/grpc/include/grpc++/impl/codegen/call.h:284\nNo locals.\n#4  0x00007f63b8ea2928 in grpc::CallOpSet<grpc::CallOpRecvInitialMetadata, grpc::CallOpRecvMessage<tensorflow::RunGraphResponse>, grpc::CallOpClientRecvStatus, grpc::CallNoOp<4>, grpc::CallNoOp<5>, grpc::CallNoOp<6> >::FinalizeResult (this=0x7f60e4002828, tag=0x7f61ba7fbe20, status=0x7f61ba7fbe1f) at external/grpc/include/grpc++/impl/codegen/call.h:576\nNo locals.\n#5  0x00007f63b8f1c681 in grpc::CompletionQueue::AsyncNextInternal (this=0xd50afa8, tag=0x7f61ba7fbe20, ok=0x7f61ba7fbe1f, deadline=...) at external/grpc/src/cpp/common/completion_queue.cc:66\n        cq_tag = 0x7f60e4002828\n        ev = {type = GRPC_OP_COMPLETE, success = 1, tag = 0x7f60e4002828}\n#6  0x00007f63b8e577f7 in grpc::CompletionQueue::Next (this=0xd50afa8, tag=0x7f61ba7fbe20, ok=0x7f61ba7fbe1f) at external/grpc/include/grpc++/impl/codegen/completion_queue.h:137\nNo locals.\n#7  0x00007f63b8e71f21 in tensorflow::GrpcWorkerCache::GrpcWorkerCache(tensorflow::GrpcChannelCache*)::{lambda()#1}::operator()() const () at tensorflow/core/distributed_runtime/rpc/grpc_worker_cache.cc:38\n        std::__ioinit = {static _S_refcount = 585, static _S_synced_with_stdio = true}\n#8  0x00007f63b8e72a07 in std::_Function_handler<void (), tensorflow::GrpcWorkerCache::GrpcWorkerCache(tensorflow::GrpcChannelCache*)::{lambda()#1}>::_M_invoke(std::_Any_data const&) (__functor=...)\n    at /usr/include/c++/4.8/functional:2071\nNo locals.\n#9  0x00007f63b6adc598 in std::function<void ()>::operator()() const (this=0x9c52170) at /usr/include/c++/4.8/functional:2471\nNo locals.\n#10 0x00007f63b9463572 in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x9c52170) at /usr/include/c++/4.8/functional:1732\nNo locals.\n#11 0x00007f63b94634c9 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x9c52170) at /usr/include/c++/4.8/functional:1720\nNo locals.\n#12 0x00007f63b9463462 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x9c52158) at /usr/include/c++/4.8/thread:115\nNo locals.\n#13 0x00007f63b538ca60 in std::(anonymous namespace)::execute_native_thread_routine (__p=<optimized out>) at ../../../../../src/libstdc++-v3/src/c++11/thread.cc:84\n---Type <return> to continue, or q <return> to quit---\n        __t = <optimized out>\n        __local = warning: RTTI symbol not found for class 'std::_Sp_counted_ptr_inplace<std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >, std::allocator<std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> > >, (__gnu_cxx::_Lock_policy)2>'\nwarning: RTTI symbol not found for class 'std::_Sp_counted_ptr_inplace<std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >, std::allocator<std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> > >, (__gnu_cxx::_Lock_policy)2>'\nstd::shared_ptr (count 1, weak 0) 0x0\n#14 0x00007f63ca318184 in start_thread (arg=0x7f61ba7fc700) at pthread_create.c:312\n        __res = <optimized out>\n        pd = 0x7f61ba7fc700\n        now = <optimized out>\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {140057717491456, -6954491474313246661, 1, 0, 140057717492160, 140057717491456, 7043588202167502907, 7042287016126023739}, mask_was_saved = 0}}, priv = {\n            pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\n        not_first_call = <optimized out>\n        pagesize_m1 = <optimized out>\n        sp = <optimized out>\n        freesize = <optimized out>\n        __PRETTY_FUNCTION__ = \"start_thread\"\n#15 0x00007f63ca04537d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\nNo locals.\n```\n", "I figured out a workaround. As long as I match the directory structure used by the session manager object, and sync it completely from the master worker to the parameter servers and there are no exception during loading, theres no segfault. The saver also has unexpected behavior when saving sharded models, as the sharded model is saved on the parameter servers rather than the master worker like the nonsharded model. The saver seems to be very fragile in distributed mode, it could use some polish.\n", "Thanks for following up. Unfortunately, we still haven't been able to reproduce the failure you are seeing, but it does sound like there's a bug.\n\nJust to clarify: does the failure only occur when there is a mismatch in the directory structure on the master worker and the parameter servers?\n", "The important thing seemed to be that the checkpoint file wasn't overwritten by a save with a different name and that it existed on the parameter server. I also had an issue with collections occurring because I was transitioning to a frozen graph implementation. The resulting exception caused a segfault for some reason.\n", "Since there has been no activity and we have not been able to reproduce, I am closing for now.\n"]}, {"number": 2647, "title": "Added licenses to makefile sh scripts", "body": "", "comments": []}, {"number": 2646, "title": "Segfaults due to protobuf -std=c++11 mismatch", "body": "I was seeing segfaults in `std::unordered_map`, and the symptoms looked like potential memory corruption.  Took a fair amount of digging in gdb to figure out that it wasn't that, but was just a build/library version issue:\n- tensorflow's embedded protobuf is built with `-std=c++11`\n- a default protobuf build is built without `-std=c++11`\n\nI'm not super familar with the details of dlopen and symbol resolution, but the `RTLD_GLOBAL` when it loads `_pywrap_tensorflow.so` might be causing it to shadow the symbols in `libprotobuf.so` (for the cpp-enabled protobuf python package).  The C++11 vs non-C++11 libraries use different headers for `std::unordered_map` (in bits vs tr1), and things blow up when they start using the wrong library's version of the code.  (Correct me if my understanding of RTLD_GLOBAL is wrong and that's irrelevant).\n\nIn any case, building protobuf with C++11 fixed the segfaults I was seeing.\n\nNow as for solutions, here are some ideas:\n- Allow tensorflow to link to an external libprotobuf.so.  This is probably the best solution, since then only one protobuf library is loaded at runtime, and a version check is probably simple to add.\n- If RTLD_GLOBAL is a relevant factor, maybe _pywrap_tensorflow.so can be split up and only the parts that actually need to be global can be global.  Or maybe some gcc attributes to control visibility can do the trick.\n- Maybe we can shove the embedded protobuf into a namespace so the symbols don't collide\n\nPossibly useful info to reproduce:\n- gcc-4.8.x\n- libstdc++.so.6.0.19\n- r0.8 branch\n\nI've seen some other people post about similar segfault issues and I suspect many of them might be related to this.\n\nAlso now that I've found the issue, it seems obvious in retrospect, and a related issue on the protobuf issues is https://github.com/google/protobuf/issues/839.\n", "comments": ["I should add that I know nothing about bazel or the tensorflow codebase so I will need someone involved in the project to help.\n", "Seems like #2034 had similar issues with symbol visibility related to `RTLD_GLOBAL`, maybe that can be revisited and tightened up a bit?  We really only want it to make certain things global, and we should be careful what's exposed.\n", "Yeah I believe sizeof(std::unordered_map) is different between the c++0x (tr1 version of std library) and --std=c++11. If protobuf and tensorflow are not communicating unordered_maps amongst each other and protobuf functions are not inlined into tensorflow translation units it can be perfectly safe. However, if some of the inlined functions are not inlined and instead turned into exported symbols implicitly, the dso loader will attempt to share one symbol with both libraries which will not work (it will assume the wrong structure layout).  You could attempt to work around this by adding \n-fvisibility-inlines-hidden to tensorflow which will force tensorflow to use its own copies of all inlines that are spilled to function calls. OTOH, if this is due to inlines using a std::unordered_map in a tensorflow translation unit that was allocated in a protobuf translation unit, this will not solve the problem.\n\n-fvisibility-inlines-hidden\n", "@aselle: To pull an offline comment into the thread, is there a way we could detect the size mismatch (possibly at runtime) and bail with an explanatory error? \n", "@aselle Hiding inlines might solve one class of problems, but non-inlined functions could also be clobbered.  We really need to either make the two libraries hidden from each other (i.e. remove/restrict RTLD_GLOBAL-ness), or share the same libprotobuf.so.\n\nBy far the best solution would be to dynamically link to an existing protobuf library and sprinkle in one or two version checks to make sure the right one is loaded.  Is there any reason not to do this other than the work involved to put it in the bazel build configs?\n", "@girving a size check would be a good first line of defense, and would certainly be a good addition until this is ironed out, but the struct could also have different members/layout with the same size so it should only be a temporary solution.\n", "I think this may impact the other libraries that are embedded in tensorflow.  Suppose I do this:\n1. Import tensorflow, it loads _pywrap_tensorflow.so with RTLD_GLOBAL, libpng symbols are loaded with global visibility\n2. Import matplotlib.pyplot, it loads _png.so, which depends on libpng15.so, but when it's linked some of the symbols it needs are globally visible via _pywrap_tensorflow.so so it uses those instead\n3. Potential problems?  On my box, I have libpng-1.5.x, but tensorflow embeds libpng-1.2.x.  I don't know the details of the implementation but there could very well be struct changes between those versions.\n\nIn an ideal world we'd link to existing libraries whenever possible.  If I had to guess, we are embedding these libraries so we don't need to support the entire universe of different versions people might have installed.  That's fine while TF is still young, but if we're going to do that we really can't have those embedded libraries loaded with global visibility or they will cause problems with libraries loaded the normal way.\n", "@llchan With respect to libpng and other libraries embedded in `_pywrap_tensorflow.so`, look [here](https://github.com/tensorflow/tensorflow/blob/9078909131d5e91e8bd1878eac54136885656043/tensorflow/tensorflow.bzl#L651). The linker scripts strictly control the symbols exported by `_pywrap_tensorflow.so`. Even though the library is loaded with `RTLD_GLOBAL`, only tensorflow related symbols are available in the global address space.\n", "Ah my bad, I was building TF 0.8 so I had the r0.8 branch checked out.  Should have checked out master, looks like the exported symbols list was added after r0.8.  I'll try a r0.9 build.\n", "Sorry for the late reply. It's difficult to do this in general, because you'd need to put something in a translation unit for every project that might have possibly been compiled differently that you can query later. I.e. something already has to be compiled that remembers the size or that it is c++11. I don't really know enough about the toolchain to know if the compiler dumps something we could reliably use to determine if a c++11 or c++0x standard library was used. We could roll our own, but it seems difficult to control. It is best to avoid std::unordered_map being part of a systems API to avoid most times you would hit this, at least until c++0x tr stuff is put to bed.\n", "@keveman 0.9 is much, much better about symbol visibility, the libpng symbols I saw before in 0.8 are no longer global, and poking around in gdb, I now see symbols coming from the right place.  Thanks, for pointing me to the right place to look!\n\n@aselle Yeah, youre right, it sounds like it'd be a pain to check c++11 vs c++0x for every dependency, and it's unclear if theres enough information as-is to determine that.  I think the changes mentioned above that restrict global symbols should address my concerns though.\n\nFor anyone else reading, try r0.9 released a day or two ago, the issues related to shared libraries may not exist anymore.\n", "@llchan Thanks for your comments. I am closing this issue as there seems to be no outstanding action items. Feel free to open the issue if you feel that it has not been sufficiently addressed.\n"]}, {"number": 2645, "title": "Persistent Tensor deletion fails when tensor was moved", "body": "TLDR; handle movers register Python SessionHandle instead of string session handle, which causes `InternalError: Unable to get element from the feed.` when garbage collection is triggered on moved persistent tensors.\n\nThe solution is to replace `self._register_dead_handle(handle)` with `self._register_dead_handle(handle.handle)` in `session.py`\n## Discussion\n\nThere are two types of handle objects, Python TensorHandle and \"native\" string session handle. The conversion from Python `TensorHandle` object to native handle `string` object is done in `session.py` and `session_ops.py` in one of those 3 ways.\n- `self._handle = compat.as_str_any(handle)`\n- `str(handle)`\n- `handle.handle`\n\nThe \"str\" conversion is not very Python since `__str__` is supposed to be used for display/debugging purposes. Some suggestions to make it easier to avoid bugs like above:\n1. Annotate which kind of handle object is being dealt with. Perhaps `py_handle` variable should refer to Python TensorHandle objects and `handle` refer to native \"string\" handle objects\n2. Only use one kind of conversion method, perhaps as `py_handle.handle`\n3. `__str__` method of TensorHandle could be `return`\"TensorHandle(%s)\"%(self.handle)` for easier debugging\n## Test case\n\n```\nclass EnvTest(tf.test.TestCase):\n\n  def testHandleDeletion(self):\n    dtype = tf.float32\n    # soft-placement to work around #2587\n    config = tf.ConfigProto(log_device_placement=True,\n                            allow_soft_placement=True)\n    sess = tf.Session(config=config)\n\n    # initial values live on CPU\n    with tf.device(\"/cpu:0\"):\n      one = tf.constant(1, dtype=dtype)\n      one_handle = sess.run(tf.get_session_handle(one))\n      x = tf.get_session_handle(one)\n      x_handle = sess.run(tf.get_session_handle(one))\n\n    # addition lives on GPU\n    with tf.device(\"/gpu:0\"):\n      add_holder1, add_tensor1 = tf.get_session_tensor(dtype)\n      add_holder2, add_tensor2 = tf.get_session_tensor(dtype)\n      add_op = tf.add(add_tensor1, add_tensor2)\n      add_output = tf.get_session_handle(add_op)\n\n\n    # add 1 to tensor 20 times\n    for i in range(20):\n      x_handle = sess.run(add_output, feed_dict={add_holder1: one_handle.handle,\n                                                 add_holder2: x_handle.handle})\n```\n\nThe failure is\n\n```\n\n======================================================================\nERROR: testHandleDeletion (__main__.EnvTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/contrib/immediate/python/immediate/tensor_handle_test.py\", line 35, in testHandleDeletion\n    add_holder2: x_handle.handle})\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 333, in run\n    run_metadata_ptr)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 591, in _run\n    self._register_dead_handle(handle)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 726, in _register_dead_handle\n    self.run(fetches, feed_dict=feeds)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 333, in run\n    run_metadata_ptr)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 587, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 662, in _do_run\n    target_list, options, run_metadata)\n  File \"/Users/yaroslavvb/tfimmediate_macbook/tensorflow/bazel-bin/tensorflow/contrib/immediate/tensor_handle_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 682, in _do_call\n    raise type(e)(node_def, op, message)\nInternalError: Unable to get element from the feed.\n\n```\n\n@yuanbyu @keveman \n", "comments": ["Actually for fix above to work, `_auto_gc_enabled` has to be set to `False` for all TensorHandles. The issue is that when `update_with_movers` moves CPU Tensor to GPU, it'll register CPU tensor handle for deletion. But if `_auto_gc_enabled` is True (it is True by default), at the point when Python runtime garbage collects that `TensorHandle` object, it'll register same handle for deletion again, so the program will fail on next gc pass with something like\n\n`InvalidArgumentError: Failed to delete a tensor with handle 'GetSessionHandle_4;9;/job:localhost/replica:0/task:0/cpu:0' in the session store.\n`\n"]}, {"number": 2644, "title": "Add x permission back to test_tutorials.sh", "body": "", "comments": []}, {"number": 2643, "title": "Updated Eigen version in Makefile", "body": "", "comments": []}, {"number": 2642, "title": "TF Tutorial refers to cudnn-7.5-linux-x64-v4.tgz", "body": "In the GPU installation section.\n\nExcept, all the obvious sources on the [nvidia website](https://developer.nvidia.com/rdp/cudnn-download) only seem to have cudnn-7.0-linux-x64-v4.tgz. \n\nIs this a typo, or a reference to a now unavailable resource?\n", "comments": ["cudnn is not publicly available, you must register with nvidia to get them.\n\nThe notes say\n\n\"Uncompress and copy the cuDNN files into the toolkit directory. Assuming the toolkit is installed in /usr/local/cuda, run the following commands (edited to reflect the cuDNN version you downloaded):\"\n\nwhich should illustrate that you might need to edit the version.\n", "Ah the reason I asked is that I'm facing two issues. \n- the pip installed version of tensorflow relies on cuda7.5 and cudnn4.\n- however, configuring tensorflow with 7.5 and cudnn4 before compiling tensorflow serving fails. It succeeds however with cuda7.0 and cudnn4.\n\nI was wondering if there exists a version of that file as referenced in the tutorial that may solve the problem.\n", "@vrv after the registration, there is no cudnn4 for cuda7.5 on the nvidia cuddn downloads page.\n![cudnn download nvidia developer](https://cloud.githubusercontent.com/assets/282529/18237095/3890434e-734f-11e6-993d-4b4e12c3f430.png)\n"]}, {"number": 2641, "title": "Error in gradient of reduce_prod", "body": "```\nvars = tf.Variable([1., 2.])\ntf.initialize_all_variables().run()\ntf.gradients(tf.reduce_prod(vars), vars)[0].eval()\n```\n\nyields `[ 2.,  1.]` which is correct. But\n\n```\nvars = tf.Variable([0., 2.])\ntf.initialize_all_variables().run()\ntf.gradients(tf.reduce_prod(vars), vars)[0].eval()\n```\n\nyields `[ nan,   0.]` which is incorrect. The correct gradient is `[ 2.,  0.]`\n", "comments": ["Note that this is not an issue with regular TensorFlow multiplication. The following works correctly:\n\n```\nvar1 = tf.Variable(0.)\nvar2 = tf.Variable(2.)\ntf.initialize_all_variables().run()\nprint tf.gradients(var1 * var2, var1)[0].eval()\nprint tf.gradients(var1 * var2, var2)[0].eval()\n```\n\nIt prints `2.0` and `0.0` which is correct.\n", "I have reproduced this.\n", "Thanks @aselle.\n\nIdea: This could be caused if the gradients of `reduce_prod(.)` are computed like the gradients of `tf.exp(tf.reduce_sum(tf.log(.)))`\n", "The gradients are computed by computing the full prod and then dividing, which is broken as you point out.  For example, for a length two produce `x *  y` the gradient w.r.t. `x` is `x * y / y`.  Indeed, there's an ancient TODO in the code that it is broken.\n\n@ibab, @benoitsteiner: The easiest way to fix this would be to do two scan products to get the sequence of partial products from both directions, then multiply them together to get all products with one element removed.  Unfortunately, we don't yet have scan.\n", "Adding contributions welcome, but note that it'll have to wait until after #2711.\n", "I've had a shot at solving this today using `cumprod`, and it's working great for full reductions.\nBut if `reduce_prod` is performed with `reduction_indices`, things get more difficult.\nIn that case, the `cumprod` needs to be performed over the reduced dimensions, and not over the remaining ones.\nAny ideas on how this could be solved?\nMaybe if there was a `tf.reshape_selected` op that allows you to group all selected dimensions into a single dimension, and all remaining ones into a second one, like this:\n\n```\nx.shape = (a, b, c, d, e)\nreduction_indices = [1, 3]\ny = tf.reshape_selected(x, reduction_indices)\ny.shape ==> (b * d, a * c * e)\n```\n\nThen the `cumprod` could be performed over the first dimension.\nThis is essentially a transpose followed by a reshape.\nI've tried to build this using existing ops, but it seems that `tf.cond` is required to act based on the contents of the `reduction_indices` tensor.\n", "@ibab It does seem like it has to be transpose + reshape + stuff + reshape + transpose.  I don't think a custom `tf.reshape_selected` makes sense: separate `transpose` and `reshape` is cleaner especially since you have to invert it.  The `tf.cond` is indeed necessary in general.  This is getting ugly, but I don't know a cleaner way.\n", "I think `tf.listdiff` can be used to separate the axis indices into reduced and non-reduced parts:\n\n``` python\nidx = tf.range(0, dims)\nnonreduced = tf.listdiff(idx, reduced)\n```\n\nIf I concatenate both of these, I can use them as the permutation in `tf.transpose`.\nI can get the sizes of the two dimensions using `tf.segment_prod` or `tf.gather` and `tf.reduce_prod`.\nSo maybe this can be solved without resorting to `tf.cond`, but it's certainly not beautiful.\n", "@ibab Have you considered extending the cumsum operation to take a list of indices instead of a single index over which to sum? That should make the gradient computation for reduce_prod a lot simpler.\n", "@benoitsteiner The awkward thing about that is that cumprod on multiple axes at a time is a very strange operation.  It would implicitly flatten and expand, which would mean you have to do the same implicit flattening at the Eigen level.\n", "@benoitsteiner: That's a good idea!\nI just realized that I could make use of `tensor.shuffle` from Eigen to do the same operations as described above (permute the axes so that the scan axes are in the front, and then reshape).\nThis would also avoid copying the tensor, as shuffle is implemented through `.coeff`.\n@girving: My first impression was also that it would be awkward, but now I'm thinking that it could make sense: The list of indices identifies the sequence in which `cumsum` should traverse each sub-tensor. It also plays well with the existing `exclusive` and `reverse` options.\n\nI'm open to using either solution (making the gradient more complicated, or making cumsum/prod more complicated)\n", "For reference, here's the gradient implementation I came up with:\n\n``` python\n@ops.RegisterGradient(\"Prod\")\ndef _ProdGrad(op, grad):\n  \"\"\"Gradient for Prod.\"\"\"\n  input_shape = array_ops.shape(op.inputs[0])\n\n  # Expand grad to full input shape\n  output_shape_kept_dims = math_ops.reduced_shape(input_shape, op.inputs[1])\n  tile_scaling = _safe_shape_div(input_shape, output_shape_kept_dims)\n  grad = array_ops.reshape(grad, output_shape_kept_dims)\n  grad = array_ops.tile(grad, tile_scaling)\n\n  # If the list is empty, it defaults to float32\n  reduced = math_ops.cast(op.inputs[1], dtypes.int32)\n  idx = math_ops.range(0, array_ops.rank(op.inputs[0]))\n  other, _ = array_ops.listdiff(idx, reduced)\n  perm = array_ops.concat(0, [reduced, other])\n  reduced_num = math_ops.reduce_prod(array_ops.gather(input_shape, reduced))\n  other_num = math_ops.reduce_prod(array_ops.gather(input_shape, other))\n  permuted = array_ops.transpose(op.inputs[0], perm)\n  permuted_shape = array_ops.shape(permuted)\n  reshaped = array_ops.reshape(permuted, (reduced_num, other_num))\n\n  # Calculate product, leaving out the current entry\n  left = math_ops.cumprod(reshaped, axis=0, exclusive=True)\n  right = math_ops.cumprod(reshaped, axis=0, exclusive=True, reverse=True)\n  y = array_ops.reshape(left * right, permuted_shape)\n\n  out = grad * array_ops.transpose(y, array_ops.invert_permutation(perm))\n  # Reset statically known shape information\n  return array_ops.reshape(out, input_shape), None\n```\n\nThis makes the existing tests pass and also works if there are zeros in the input array.\n", "This has been fixed by #3351, so this issue can be closed.\n", "The gradient of `reduce_prod` does not support negative axis unlike `reduce_prod` itself.\r\nIt is caused by `gather` not supporting negative axes.\r\nThis code illustrates the problem.\r\n```python\r\nimport tensorflow as tf\r\n\r\nvars = tf.Variable([[1., 2.], [3., 4.]])\r\nprod = tf.reduce_prod(vars, -1) # Negative axis here\r\n\r\ntf.InteractiveSession()\r\ntf.global_variables_initializer().run()\r\nprint(prod.eval()) # Works fine\r\nprint(tf.gradients(prod, vars)[0].eval()) # Crashes\r\n```", "@pvanhaes Thanks for the bug report!  Can you file it as a separate issue, since it's unrelated to the current thread?  It helps us to keep Github issues organized."]}, {"number": 2640, "title": "libpthread.so error with android demo", "body": "Operating System: OSX Yosemite 10.10.5\n\nFollowed the [virtualenv installation instructions](https://github.com/hamidb/tensorflow/blob/api20/tensorflow/g3doc/get_started/os_setup.md#virtualenv-installation-virtualenv_install)\n\nr11 release\n\nI followed [these](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android) instructions to run the android demo\n\nHave tried Bazel releases 0.2.3, 0.2.2, 0.2.1 and 0.1.4\nHave the latest sdk and ndk\n\nHave tried with all the sdk and ndk info uncommented in Workspace and with the ndk commented out\n\nI get as far as \n`$ bazel build //tensorflow/examples/android:tensorflow_demo`\n\nand I get the following errors:\n\n`ERROR: /Users/lizzya/Documents/tensorflow-master/tensorflow/examples/android/BUILD:41:1: Linking of rule '//tensorflow/examples/android:libpthread.so' failed: false failed: error executing command \n  (cd /private/var/tmp/_bazel_lizzya/3ad59cfd62098d64f5760266c321e9a2/tensorflow-master && \\\n  exec env - \\\n  /bin/false -o bazel-out/android-stub_armeabi-v7a-fastbuild/bin/tensorflow/examples/android/libpthread.so -shared -Wl,-S -Wl,@bazel-out/android-stub_armeabi-v7a-fastbuild/bin/tensorflow/examples/android/libpthread.so-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: false failed: error executing command \n  (cd /private/var/tmp/_bazel_lizzya/3ad59cfd62098d64f5760266c321e9a2/tensorflow-master && \\\n  exec env - \\\n  /bin/false -o bazel-out/android-stub_armeabi-v7a-fastbuild/bin/tensorflow/examples/android/libpthread.so -shared -Wl,-S -Wl,@bazel-out/android-stub_armeabi-v7a-fastbuild/bin/tensorflow/examples/android/libpthread.so-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nprocess-wrapper: execvp(\"/bin/false\", ...): No such file or directory\nTarget //tensorflow/examples/android:tensorflow_demo failed to build\nINFO: Elapsed time: 1.509s, Critical Path: 0.16s\n\n`\n", "comments": ["I'm not sure what's going on there, but it looks like /bin/false is failing with no such file directory on execvp. That could be because Mac OS has /bin/false in /usr/bin/false instead. Could you try ln -s /usr/bin/false to /bin/false and see if it gets any further?\n", "Bazel 0.2.3 doesn't support NDK r11. Support was added in commit https://github.com/bazelbuild/bazel/commit/abdaff492440b373bacd016d772ef73611a27901, which will probably be in 0.2.4. Until then I'd suggest using r10e.\n\nAs far as libpthread.so in particular, this file is actually not even necessary for the android build. It's just that protobuf assumes it will be there, so I had to add a workaround to tensorflow/examples/android/BUILD. The newer version of the protobuf library is more intelligent about adding the option, but we hadn't upgraded TF to it until recently. I'm working on a commit right now to remove libpthread.so from the android/BUILD file.\n\nYou can try editing protobuf/BUILD and remove the pthreads entry from LINK_OPTS, and also remove all references to libpthread from tensorflow/examples/android/BUILD.\n\nHopefully switching to  NDK r10e and trying to excise libpthread.so from the build will solve your problem, but if it doesn't please update with the new error.\n\nedit: updated protobuf info\n", "For reference you can find links to download NDK r10e here: https://github.com/tensorflow/tensorflow/issues/1468\n", "Thank you, this solved my issue.\n"]}, {"number": 2639, "title": "Add OSX/iOS makefile build script", "body": "The build script can be used in Jenkins CI jobs.\n", "comments": ["There are some issues with the commits. I will close this one and open a new one.\n"]}, {"number": 2638, "title": "Makefile build", "body": "", "comments": []}, {"number": 2637, "title": "Possible way to apply gradients using tf.identity()?", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 7.5 and R4\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash: 79174afa30046ecdc437b531812f2cb41a32695e\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\n\na = tf.constant(1.)\nb = tf.Variable(2.)\nc = a * b\ngrad_symbolic = tf.gradients(c, tf.trainable_variables())\ngrad_vars = [tf.Variable(tf.zeros_like(_)) for _ in tf.trainable_variables()]\ngrad_zero_ops = [_.assign(tf.zeros_like(_)) for _ in grad_vars]\ngrad_accum_ops = [grad_vars[i].assign_add(grad_symbolic[i])\n                  for i in xrange(len(grad_vars))]\nopt = tf.train.GradientDescentOptimizer(1.)\ntrain_op = opt.apply_gradients(zip(\n    [tf.identity(_) for _ in grad_vars], tf.trainable_variables()))\nsess = tf.Session()\nsess.run(tf.initialize_all_variables()) # This will cause error.\n# This will run successfully.\n# for _ in tf.all_variables():\n#     sess.run(tf.initialize_variables([_]))\n```\n### What have you tried?\n1. [workaround](http://stackoverflow.com/questions/35656643/tf-initialize-variables-inconvenience-failedpreconditionerror-tensorflow)\n2. But in my real project code, the program stucks into the `variable initialization` for too many minutes if I initialize variables one by one.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n```\nTraceback (most recent call last):\n  File \"test_identity_gradient_apply.py\", line 26, in <module>\n    sess.run(tf.initialize_all_variables()) # This will cause error.\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 333, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 573, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 648, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 668, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value Variable\n     [[Node: Variable/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable)]]\nCaused by op u'Variable/read', defined at:\n  File \"test_identity_gradient_apply.py\", line 15, in <module>\n    b = tf.Variable(2.)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 211, in __init__\n    dtype=dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 319, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 788, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2249, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1224, in __init__\n    self._traceback = _extract_stack()\n\n```\n", "comments": ["The problem is in this line:\n\n``` python\ngrad_vars = [tf.Variable(tf.zeros_like(_)) for _ in tf.trainable_variables()]\n```\n\nIf you initialize a variable from (something derived from) another variable, you need to use the [`Variable.initialized_value()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/state_ops.html#Variable.initialized_value) method to tell TensorFlow about the ordering dependency when running the initializers. So your line should look like:\n\n``` python\ngrad_vars = [tf.Variable(tf.zeros_like(v.initialized_value())) for v in tf.trainable_variables()]\n```\n\nNote though that your `grad_vars` don't really depend on the _value_ of the trainable variables, and the trainable variables all have static shapes, so you could alternatively solve this with the following:\n\n``` python\ngrad_vars = [tf.Variable(tf.zeros(v.get_shape())) for v in tf.trainable_variables()]\n```\n"]}, {"number": 2636, "title": "Segment Fault after Import Tensorflow", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Debian 8.5\n\nInstalled version of CUDA and cuDNN:  No\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed: Ubuntu/Linux 64-bit, CPU only\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\": Segment Fault\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Run Python with `python`\n2. Import Tensorflow `import tensorflow`\n3. Got error `Segment Fault`\n### What have you tried?\n\nN/a\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n![screenshot from 2016-06-03 15 51 26](https://cloud.githubusercontent.com/assets/1730504/15772309/fde464e6-29a2-11e6-91bf-0428672c8b52.png)\n", "comments": ["Could you show uname -a on the machine and also let us know what PIP binary package you installed or what sha hash if you installed from source.\n", "I guess #2034 might be relevant\n", "Here's the result of `uname -a`:\n\n```\nhzxie@XieHaozhe-PC:~$ uname -a\nLinux XieHaozhe-PC 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux\n```\n\nAnd the output of installed packages are listed below:\n\n```\nhzxie@XieHaozhe-PC:~$ pip list\ncffi (0.8.6)\nchardet (2.3.0)\ncryptography (0.6.1)\ndefusedxml (0.4.1)\ndocutils (0.12)\nlxml (3.4.0)\nnose (1.3.7)\nnose-parameterized (0.5.0)\nnumpy (1.11.0)\npexpect (3.2)\nPillow (2.6.1)\npip (8.1.2)\nply (3.4)\nprotobuf (3.0.0b2)\npycparser (2.10)\npycups (1.9.63)\npycurl (7.19.5)\nPygments (2.0.1)\npygobject (3.14.0)\npyOpenSSL (0.14)\npysmbc (1.0.15.3)\npython-apt (0.9.3.12)\npython-debian (0.1.27)\npython-debianbts (1.11)\npyxdg (0.25)\nreportbug (6.6.3)\nreportlab (3.1.8)\nrequests (2.10.0)\nroman (2.0.0)\nscikit-learn (0.17.1)\nscipy (0.17.0)\nsetuptools (22.0.0)\nsix (1.10.0)\nSOAPpy (0.12.22)\nsocketIO-client (0.6.5)\ntensorflow (0.8.0)\nTheano (0.8.2)\nwebsocket-client (0.37.0)\nwheel (0.29.0)\nwstools (0.4.3)\n```\n", "@zjhzxhz: Can you check if doing `import numpy` before `import tensorflow` fixes the problem, as per #2034.  Alternatively, you could try upgrading to 0.9, which shouldn't have this problem. \n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2635, "title": "improve docker readme", "body": "We have very old readme in tools/docker leading to confusion - see #2533 \n", "comments": []}, {"number": 2634, "title": "When dose reusebale variable sync between device?", "body": "I want to validate model under training after certain iterations on train set.\nThe switch between train and validate is controlled by global set.\nTraining is on GPU:1, while validation is on CPU:0.\nIf share the variable between GPU and CPU using  tf.get_variable_scope().reuse_variables()\nwhen dose the two copy sync?\nEvery time after GPU apply_gradients or CPU used them in seesion.run(op on CPU)?\n", "comments": ["This is a question for StackOverflow\n"]}, {"number": 2633, "title": "Branch 123940786", "body": "Merge from internal.\n", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2632, "title": "Updated gRPC to release 0.14.", "body": "This patch updates the version of gRPC used to the latest release. It adds dependencies on nanopb and the same boringssl library as used in many of the models' workspaces.\n", "comments": ["@keveman: This update is a prerequisite to the fix for #2233. If the tests pass, alright to merge?\n", "@mrry can you please poke grpc folks to make the build more modular so we can remove the copied BUILD later?\n", "Yeah, maintaining build files for other people is asking for trouble. \n"]}, {"number": 2631, "title": "error building py package in format of tar.gz", "body": "OS: OS X Yosemite 10.10.4\n\nAfter installation of dependencies, I build it as following cmds:\n\ndelete `bdist_wheel` in `tensorflow/tensorflow/tools/pip_package/build_pip_package.sh`\n\n`${PYTHON_BIN_PATH:-python} setup.py bdist_wheel >/dev/null` to `${PYTHON_BIN_PATH:-python} setup.py`\n\n```\n\n\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\nsh +x tensorflow/tensorflow/tools/pip_package/build_pip_package.sh /tmp\n```\n\nthen I got the package named `tensorflow-0.8.0.tar.gz`, after that I installed it using pip, succeed.\n\nHowever, when I imported it, I got the error:\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named tensorflow\n```\n\nCould anybody help me? Any help will be highly appreciated.\n\nCould staff in Google build it or give some valuable instructions?\n", "comments": ["I think this might be related to the problems affecting \"setting up for development\" #2497 i.e. in \nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md\nsection \"Setting up TensorFlow for Development.\"\n\nSpecifically, The instructions have to be different on what you symlink into the python build directory dependent on the version of bazel (which ones to do when are outlined in #2497).  Are you able to get those instructions to work and sucessfully import tensorflow from python?\n", "Yes, I have made it worked. I'll update the doc later. \n\nI have another question:  where can I find the code before generating by `SWIG` of `bazel-bin/tensorflow/python/pywrap_tensorflow.py`. I want to make it more general when packaging it as tar.gz.\n\nThanks a lot. Awaiting for your reply.\n", "For the generated file location see bazel-out...\n\nFor example [pywrap_tensorflow.cc](http://ci.tensorflow.org/job/tensorflow-master-cpu/ws/bazel-out/local_linux-opt/bin/tensorflow/python//pywrap_tensorflow.cc) on [ci.tensorflow.org](http://ci.tensorflow.org/job/tensorflow-master-cpu/ws/bazel-out/local_linux-opt/bin/tensorflow/python/).\n", "Closing since it seems to be of no further problem. Reopen if further bugs are discovered.\n"]}, {"number": 2630, "title": "Getting \"readlink: illegal option -- f\" when switching builds in MacOS", "body": "I'm running into this error when building my targets with regular config, and after that trying to build with `config=cuda` in the same directory on latest MacOS (10.11.5) and bazel (0.2.3-homebrew). A work-around is to git-clone into a new directory, and build there with `config=cuda`\n\n```\n\n`/private/var/tmp/_bazel_yaroslavvb/1401dea20c857793b13cff5c293c0db6/tensorflow/'\n____[8 / 88] Compiling external/protobuf/python/google/protobuf/internal/api_implementation.cc\n____[8 / 165] Compiling external/re2/re2/filtered_re2.cc\n____[8 / 313] Compiling external/boringssl_git/crypto/x509/x509_ext.c\n____From Executing genrule //third_party/gpus/cuda:cuda_config_check:\nreadlink: illegal option -- f\nusage: readlink [-n] [file ...]\nERROR: /Users/yaroslavvb/tfimmediate_fresh/tensorflow/third_party/gpus/cuda/BUILD:204:1: declared output 'third_party/gpus/cuda/cuda.config' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\nERROR: /Users/yaroslavvb/tfimmediate_fresh/tensorflow/third_party/gpus/cuda/BUILD:204:1: not all outputs were created.\n____[195 / 385] Compiling external/protobuf/src/google/protobuf/wrappers.pb.cc [for host]\n\n```\n", "comments": ["Hm, yes -f doesn't exist on the Mac OS readlink.  We should probably emulate the canonicalization this way...\nhttp://stackoverflow.com/questions/1055671/how-can-i-get-the-behavior-of-gnus-readlink-f-on-a-mac\nthird_party/gpus/cuda/BUILD:181\n\nAsking the original author of that code to update it to work on Mac OS.\n", "A side-effect of this is that if you make a mistake during \"./configure\" step, you must start over in a new directory. IE, running configure to change capability to \"3.0\" after setting it to default at first, doesn't have the desired effect, and \"bazel clean\" doesn't work because of this error\n", "@aselle any updates here?\n", "On macOS you can use `greadlink` from homebrew'ed coreutils, which supports `-f`.\n", "@yifeif has a fix for this. Yifei, could you close this issue if your PR is merged?\n", "@yifeif you can have GitHub automatically reference commit/close issue if you [include](https://help.github.com/articles/closing-issues-via-commit-messages/) the following phrase anywhere in public part of CL description: closes #2630\n", "good to know @yaroslavvb :). The PR was merged a few days back. I linked this issue from it. Closing.\n"]}, {"number": 2629, "title": "Tensorflow distributed master worker saves unexpected: not raise exception and return the store checkpoint filepath,but check point file is not exist", "body": "**In distribution tensorflow  environment. the master worker saves checkpoint  fail.<br>\n## saver.save  has return ok_**(not raise exception and return the expected checkpoint file path) *__but, the return checkpoint file is not exist_**.<br>\n\nthis is different from the description of the tensorflow api<br>\n\n=============<br>\n**the related code is below:**\n\n```\n def def_ps(self):\n    self.saver = tf.train.Saver(max_to_keep=100,keep_checkpoint_every_n_hours=3)\n\ndef save(self,idx):    \n    ret = self.saver.save(self.sess,self.save_model_path,global_step=None,write_meta_graph=False)\n    if not os.path.exists(ret):\n        msg = \"save model for %u path %s not exists.\"%(idx,ret)\n        lg.error(msg)\n        raise Exception(msg);\n```\n\n=============<br>\n**the log is below**:\n\n 2016-06-02 21:33:52,323 root         ERROR    save model for 2 path model_path/rl_model_2 not exists.\n2016-06-02 21:33:52,323 root         ERROR    has error:save model for 2 path model_path/rl_model_2 not exists.\nTraceback (most recent call last):\n  File \"d_rl_main_model_dist_0.py\", line 755, in run_worker\n    model_a.save(next_model_idx)\n  File \"d_rl_main_model_dist_0.py\", line 360, in save\n    Trainer.save(self,save_idx)\n  File \"d_rl_main_model_dist_0.py\", line 289, in save\n    raise Exception(msg);\nException: save model for 2 path model_path/rl_model_2 not exists.\n\n===========<br>\n**not meets the tensorflow api which define Saver.save as below:**\n\nhttps://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#Saver\n\ntf.train.Saver.save(sess, save_path, global_step=None, latest_filename=None, meta_graph_suffix='meta', write_meta_graph=True)\n\nReturns:\n\nA string: path at which the variables were saved. If the saver is sharded, this string ends with: '-?????-of-nnnnn' where 'nnnnn' is the number of shards created.\n\nRaises:\n\nTypeError: If sess is not a Session.\nValueError: If latest_filename contains path components.\n", "comments": ["I posted an [answer](http://stackoverflow.com/a/37618688/3574081) to this same question on Stack Overflow. Feel free to continue the discussion over there!\n"]}, {"number": 2628, "title": "Fix gradient of complex op when broadcasting", "body": "The gradient of the complex op didn't take into account that it can broadcast its parameters.\nI've also added a test that should check this in the future.\nThis fixes #2627.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 2627, "title": "Complex Gradients in gather", "body": "### Environment info\n\nOperating System: Linux Ubuntu 14.04 LTS (64bit)\n\nInstalled version of CUDA and cuDNN: CUDA 7.5.18 and CUDNN 5.0.4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\nls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. Tensorflow 0.8.0 Nightly Python2.7 Linux (GPU) [Build 118](http://ci.tensorflow.org/job/nigntly-matrix-linux-gpu/118/)\n### Steps to reproduce\n\nI tried to run the following code using the tensorflow pip package  `storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl` \n\n``` python\nimport tensorflow as tf\n\nW = tf.Variable(tf.random_uniform( (10,1) ), tf.float32)\nW = tf.complex(W, 1.0)\nC = tf.constant(1.0, dtype=tf.float32, shape=(3,1))\n\nviews = tf.gather(W, [2,1,5])\nloss = tf.reduce_mean(tf.square( tf.complex_abs(views) - C))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\ntrain = optimizer.minimize(loss)\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n```\n\nI got\n`TypeError: DataType complex64 for attr 'T' not in list of allowed values: float32, float64, int32, int64, uint8, int16, int8, uint16`\n\nI looked it up and found issue #2255 so I installed the nightly build 118, and now I get this when trying to run the same code snippet\n\n``` python\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-1-0671dcd6f73e> in <module>()\n      9 \n     10 optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n---> 11 train = optimizer.minimize(loss)\n     12 \n     13 init = tf.initialize_all_variables()\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    191         aggregation_method=aggregation_method,\n    192         colocate_gradients_with_ops=colocate_gradients_with_ops,\n--> 193         grad_loss=grad_loss)\n    194     return self.apply_gradients(grads_and_vars, global_step=global_step,\n    195                                 name=name)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\n    248         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\n    249         aggregation_method=aggregation_method,\n--> 250         colocate_gradients_with_ops=colocate_gradients_with_ops)\n    251     if gate_gradients == Optimizer.GATE_GRAPH:\n    252       grads = control_flow_ops.tuple(grads)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\n    503           if in_grad is not None:\n    504             if isinstance(in_grad, ops.Tensor):\n--> 505               in_grad.set_shape(t_in.get_shape())\n    506             _SetGrad(grads, t_in, in_grad)\n    507         if loop_state:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shape(self, shape)\n    402         this tensor.\n    403     \"\"\"\n--> 404     self._shape = self._shape.merge_with(shape)\n    405 \n    406   @property\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in merge_with(self, other)\n    568       except ValueError:\n    569         raise ValueError(\"Shapes %s and %s are not compatible\" %\n--> 570                          (self, other))\n    571 \n    572   def concatenate(self, other):\n\nValueError: Shapes (?, 1) and () are not compatible\n```\n", "comments": ["Looping in @ibab. Did you expect your CL would cover this case as well?\n", "Thanks a lot for reporting this!\n\nI was working based off of the error message in #2255 and hoped that the gradient code would automatically work with complex numbers.\n\nI've reduced the example to\n\n``` python\nimport tensorflow as tf\n\nW = tf.Variable([1., 1.], tf.float32)\nW_ = tf.complex(W, 1.0)\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\nprint(sess.run(tf.gradients(W_, [W])))\n```\n\nThis fails with\n\n```\nValueError: Shapes (2,) and () are not compatible\n```\n\nSo it seems like it's not actually caused by the use of `gather` and is unrelated to #2255.\nRather, it looks like there might be a bug in the gradient code for `tf.complex`.\n\nThere's no problem if I use\n\n``` python\nW_ = tf.complex(W, W)\n```\n\nwhich might help to narrow it down.\nWill take a look at the gradient code now.\n", "The gradient code is very simple:\n\n``` python\n@ops.RegisterGradient(\"Complex\")\ndef _ComplexGrad(_, grad):\n  \"\"\"Returns the real and imaginary components of 'grad', respectively.\"\"\"\n  return math_ops.real(grad), math_ops.imag(grad)\n```\n\nI guess the problem occurs because the dimensions of `1.0` are not properly broadcasted and the gradient ends up having shape `()`, while `tf.imag(grad)` has shape `(2,)`.\n", "It can be fixed by rewriting it like this\n\n``` python\n@ops.RegisterGradient(\"Complex\")\ndef _ComplexGrad(op, grad):\n  \"\"\"Returns the real and imaginary components of 'grad', respectively.\"\"\"\n  x = op.inputs[0]\n  y = op.inputs[1]\n  sx = array_ops.shape(x)\n  sy = array_ops.shape(y)\n  rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  return (array_ops.reshape(math_ops.reduce_sum(math_ops.real(grad), rx), sx),\n          array_ops.reshape(math_ops.reduce_sum(math_ops.imag(grad), ry), sy))\n```\n\nI've looked through the rest of the `math_ops`, and couldn't spot another case where the broadcasting has been missed.\nWould be nice to write a bit of code to apply this reshape-reduce trick, though, as it's used very often.\n\nI'll check whether there are specific tests for this problem and make a PR.\n\nWhen implementing the gradient like this, @yshady's example above runs without problems \ud83d\udc4d \n", "Sounds awesome, thanks!\n", "@yshady: Your code should work once my PR has been merged.\nIn the meantime, you can use\n\n```\nW = tf.complex(W, tf.ones_like(W))\n```\n", "Thanks @ibab! That did the trick. \n"]}, {"number": 2626, "title": "Profiling: libcupti.so cannot be loaded", "body": "### Environment info\n\nOperating System: Linux Ubuntu 14.04 LTS (64bit)\n\nInstalled version of CUDA and cuDNN: CUDA 7.5.18 and CUDNN 4.0.7\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n\u276f\u276f\u276f ls -l /usr/local/cuda-7.5/lib/libcud*\n-rw----r-- 1 root root 189170  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwx---r-x 1 root root 311596  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18\n-rw----r-- 1 root root 558020  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib/libcudart_static.a\n```\n\n```\n\u276f\u276f\u276f ls -l /usr/local/cuda-7.5/lib64/libcud*\n-rw----r-- 1 root root   322936  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwx---r-x 1 root root   383336  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n-rw----r-- 1 root root   720192  2\uc6d4 24 18:12 /usr/local/cuda-7.5/lib64/libcudart_static.a\nlrwxrwxrwx 1 root root       13  3\uc6d4  3 03:30 /usr/local/cuda-7.5/lib64/libcudnn.so -> libcudnn.so.4\nlrwxrwxrwx 1 root root       17  3\uc6d4  3 03:30 /usr/local/cuda-7.5/lib64/libcudnn.so.4 -> libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 61453024  3\uc6d4  3 03:30 /usr/local/cuda-7.5/lib64/libcudnn.so.4.0.7\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed : **Tensorflow 0.8.0 Nightly** Python2.7 Linux (GPU) e.g. [Build 118](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/118/)\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\". : 0.8.0\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\nAlthough it is experimental, I am using the GPU profiling functionality with CUPTI. \n1. Run any tensorflow code that uses CUPTI or `tf.RunOptions.FULL_TRACE`.\n2. The following error (segfault) occurs.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:102] Couldn't open CUDA library libcupti.so. LD_LIBRARY_PATH:\nF tensorflow/core/common_runtime/gpu/cupti_wrapper.cc:57] Check failed: f != nullptr could not find cuptiActivityRegisterCallbacksin libcupti DSO; dlerror: /home/wookayin/.virtualenvs/tfdebug/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks\n[1]    82604 abort (core dumped)  python 19-mnist-profiling.py\n```\n\nAny TF code that invokes loading of `libcupti.so` will face the same error, but for convenience I will share a code that can run standalone: [19-mnist-profiling.py](https://gist.github.com/wookayin/06631c68bb48fc1d0a4eee77cbbba5f1)\n### What have you tried?\n\nThe problem is that the shared library `libcupti.so` cannot be loaded. However, in some older nightly version (such as [Build 103](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/103/)) it worked.\n\nUPD: I binary-searched to find the changeset to be blamed. Build 103 works, but [Build 104](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/104/) (Failed) and [Build 105](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/105/) does not work.\nI highly suspect that this regression is since commit 6bd964c (but not sure):\n- The path to `libcupti.so` would be `/usr/local/cuda/extras/CUPTI/lib64/libcupti.so`.\n- After this commit, it seems that path to `libcupti.so` goes wrong. (but why?)\n\nA strange thing to me is that tensorflow already has [a unit test](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline_test.py) for CUPTI and GPU tracing functionalities, so CI must have run this test as well. This bug might be happening in some environments only (like nightly build I installed via `pip`), or it can be a a bazel-related problem (when generating packages).\n\nI have not investigated into this problem in detail; it looks that after some troubleshooting I can figure out what the cause is.\n\nThanks!\n### Logs or other output that would be helpful\n\nN/A\n", "comments": ["Thank you for carefully narrowing this down to a commit SHA!\n\nIt looks like your local pre-sudo environment does not have access to libcupti.so but the default one in the sudo'd space does. The change that seems to have hit you is this\n\n![image](https://cloud.githubusercontent.com/assets/326106/15761987/2f44e606-28d0-11e6-8bd2-1e877c3424dc.png)\n\nSo previously LD_LIBRARY_PATH was spelled wrong so it wasn't actually preserving the environment (see man sudo ) of that variable. Whereas now it is wiping out the default environment in sudo'd space. environment variables are usually wiped out for security (i.e. LD_PRELOAD and LD_LIBRARY_PATH are vulnerable to code injection that would run as root). \n\nSo check the disparity with your environment between sudo'd and not sudo'd.\n", "I'm not sure that the change to 'with_the_same_user' is relevant.  (AFAIK, This is just used in our Jenkins test framework to ensure the LD_LIBRARY_PATH from the Docker container is used when running GPU tests as the user 'CI_BUILD_UID')\n\nNote: the same GIT commit also changed the interpretation of the trace level enum.  Prior to this change, the TRACE_FULL option was only recording the host-side enqueueing of ops (i.e. what is now done if you specify 'SOFTWARE_TRACE') \n\nIt is now the case the 'TRACE_FULL' will also try to enable the GPUTracer on CUDA builds.  \nIf you're running a python script, this requires libcupti to be on your LD_LIBRARY_PATH (since there's no other way to find the NVidia library.)\n\ne.g. LD_LIBRARY_PATH=/usr/loca/cuda/lib64:/usrlocal/cuda/extras/lib64\n\nPlease can you check your path, and see if this fixes the problem?\n", "Closing as (assumed) fixed.\n", "As of the current nightly build, I do not experience any segfault and it looks like that it is fixed with the latest available pip package.\n\nSome information: I have the environment variable `LD_LIBRARY_PATH` unset (i.e. empty), and `ldconfig -p | grep libcupti` shows nothing. However, it seems that tensorflow can succesfully load the cupti module.\n\nThanks for the support!\n", "My mistake, with the [latest nightly](http://ci.tensorflow.org/view/Nightly/job/nigntly-matrix-linux-gpu/TF_BUILD_CONTAINER_TYPE=GPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.8.0-cp27-none-linux_x86_64.whl) the segfault still happens.\n\nAs @prb12 said, I think it might be necessary to include `/usr/local/cuda/extras/CUPTI/lib64` in the LD_LIBRARY_PATH or `/etc/ld.so.conf`. After configuring the path to libraries, the segfault does not occur, but I see a warning message for every invocation of `session.run()` (with metadata requested)\n\n```\n...\nW tensorflow/core/common_runtime/gpu/gpu_tracer.cc:513] Unhandled API Callback for 2 41\nW tensorflow/core/common_runtime/gpu/gpu_tracer.cc:513] Unhandled API Callback for 2 41\n...\n```\n\nI think this might be another issue.\n", "> I think this might be another issue.\n\nYes, the warning messages are a separate (known) issue which we are currently working on.  (also reported in [this StackOverflow post](http://stackoverflow.com/questions/37849009/warnings-when-executing-tensorflow-sample-code-mnist-with-summaries-py))\n\nThey are most likely due to differeces between versions of libcupti, and should be harmless (other than the undesirable log spam)  \n\nIssue tracked under #2959\n", "Appending `/usr/local/cuda/extras/CUPTI/lib64` to LD_LIBRARY_PATH environment variable solved the problem.\n", "I fixed my problem according to shkr's suggestion\n", "As said above, this solved my issue. In my case, I edited my ./bashrc file in the home directory by appending `/usr/local/cuda/extras/CUPTI/lib64` to the LD_LIBRARY_PATH.\n\n`export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64`\n\nI got this error after following the [TensorBoard Tutorial](https://www.tensorflow.org/versions/r0.11/how_tos/summaries_and_tensorboard/index.html) from running their [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py). It occurred at the first summary logging (running FULL_TRACE).\n", "I'm facing this issue as well. Installed tf from source. Here's the log. Facing this issue while trying to run mnist_with_summaries.py.\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.8475\r\npciBusID 0000:4b:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 6.45GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:4b:00.0)\r\nAccuracy at step 0: 0.0938\r\nAccuracy at step 10: 0.6802\r\nAccuracy at step 20: 0.818\r\nAccuracy at step 30: 0.8561\r\nAccuracy at step 40: 0.8777\r\nAccuracy at step 50: 0.8767\r\nAccuracy at step 60: 0.8853\r\nAccuracy at step 70: 0.8833\r\nAccuracy at step 80: 0.8885\r\nAccuracy at step 90: 0.894\r\nI tensorflow/stream_executor/dso_loader.cc:116] Couldn't open CUDA library LD_LIBRARY_PATH: \r\nF tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks)could not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n\r\nWhat is suspicious to me is the error calls out libcupti.so.8.0. (I think the version is 7.5), wonder why it's doing that!! All the other cudnn libraries are fine and are found locally. Any pointers?\r\n\r\nI have made the suggested changes to .bashrc with LD_LIBRARY_PATH. No luck.", "> What is suspicious to me is the error calls out libcupti.so.8.0. (I think the version is 7.5), \r\n\r\nWhy would that be suspicious if you're compiling from source and all the other CUDA libraries mentioned are CUDA8?   Are you saying that you have a 7.5 libcupti in your CUDA 8 install directory?\r\n\r\n> I have made the suggested changes to .bashrc with LD_LIBRARY_PATH. No luck.\r\n\r\nThis sort of error report is not very meaningful without explaining how you configured your source tree (i.e. what versions of cuda libraries and header files did you point it at).    \r\n \r\nWhatever version of CUDA header files you *built* against will have to match the shared library versions which are supplied at runtime via the `LD_LIBRARY_PATH` variable.  \r\n \r\n", "> Installed tf from source. \r\n\r\nAlso important is which version you have. I think version 12 of TensorFlow might require cuda 8.0 :( \r\nBut if you're installing from source, you should be able to specify cuda 7.5. \r\n\r\nYou could also try version 11 of TensorFlow, but it will put you a little behind.", "I'm obviously new to this. I followed the instructions on getting started page for tf.\r\ntensorflow: 0.12\r\ncuDNN: v5.1\r\nCUDA: 8.0 \r\n\r\nThis is how I configured the source (I'm assuming you're asking for ./configure options in tf? Exactly as suggested on the getting started tf page.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python3.5/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5       \r\nPlease specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: 3.0\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n.......\r\n____Loading package: tensorflow/contrib/factorization\r\n____Loading package: tensorflow/contrib/tensor_forest/hybrid\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 742,277 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 1,488,145 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,232,595 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 2,977,045 bytes\r\n____Downloading http://bazel-mirror.storage.googleapis.com/github.com/google/protobuf/archive/008b5a228b37c054f46ba478ccafa5e855cb16db.tar.gz: 3,722,913 bytes\r\nINFO: All external dependencies fetched successfully.\r\n\r\n\r\n", "@siddharthkhullar Given those config values, your `/usr/local/cuda` directory presumably contains a CUDA8.0 SDK installation and header files.\r\nIn this case, `/usr/local/cuda/extras/CUPTI/lib64` should be on your `LD_LIBRARY_PATH` and should (typically) contain a `libcupti.so` symlink pointing to a `libcupti.so.8.0`\r\n", "@prb12 Yes, it does. /extras/CUPTI/lib64 is on my LD_LIBRARY_PATH. Yes libcupti.so points to libcupti.so.8.0 which further points to a third file called libcupti.so.8.0.44 in that directory.", "You can debug what libraries are being used by using LD_DEBUG environment variable to enable deubg printouts of the dynamic loader. \r\n```\r\nLD_DEBUG=libs python tens.py  2>&1 | grep \"calling init\"\r\n```\r\ntry that... make sure to resolve any generic ones... like i get\r\n```\r\n     39707:\tcalling init: /usr/local/cuda/lib64/libcudnn.so\r\n```\r\nbut that is actually\r\n```\r\n$ ls -l /usr/local/cuda/lib64/libcudnn.so\r\nlrwxrwxrwx 1 aselle xxx 13 Jul 26 22:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\r\n```\r\nTry removing the cuda 7.5 stuff. It likely is loading an incompatible library. If this doesn't work, build from a clean sandbox in case it built partially with 7.5 libraries.  Your initial report seemed to indicate you had 7.5 libraries installed.\r\n", "I've got a very similar error:\r\n\r\n2017-05-14 23:24:50.784478: F tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Non-OK-status: ::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f) status: Not found: /home/kostik/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: cuptiActivityRegisterCallbackscould not find cuptiActivityRegisterCallbacksin libcupti DSO\r\n\r\nI use 1.11.0. Everything was installed as in the instruction.", "I have a similar problem. Error is pasted below (only the final error part).\r\n\r\nMy problem is that my libcupti.so.8.0 file is not in /usr/local/cuda/extras/CUPTI/lib64, it is in /usr/local/cuda8.0/lib64. I tried adding this path to LD_LIBRARY_PATH but didn't help. I didn't install CUDA myself, and am not familiar with CUDA at all so not sure why the path is a bit different from everyone else.\r\n\r\nAny advice is highly appreciated. Thanks.\r\n\r\n\r\n2018\udae1\udea7\udae1\udeb4 13:58:56.835109: I tensorflow/stream_executor/dso_loader.cc:129] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: /lib64::/usr/local/cuda/extras/CUPTI/lib64\r\n2018\udae1\udea7\udae1\udeb4 13:58:56.835143: F ./tensorflow/stream_executor/lib/statusor.h:212] Non\u2011OK\u2011status: status_ status: Failed precondition: could not dlopen DSO: libcupti.so.8.0; dlerror: libcupti.so.8.0: cannot open shared object file: No such file or directory", "Hey Carolzheng,\r\n\r\nI'd suggest copying the file from /usr/local/cuda8.0/lib64 to whatever location you need it to be in. You can do it with the [cp command](https://www.computerhope.com/unix/ucp.htm) in the Linux terminal if you're on Ubuntu.\r\n\r\nI bet you could also make a [symbolic link](https://kb.iu.edu/d/abbe) too.\r\n\r\nGood luck.", "Thank you auxsophia for your advice, I tried that and now my error becomes:\r\n2018\udae1\udea7\udae1\udeb4 03:55:51.413954: F .tensorflow/stream_executor/lib/statusor.h:212] Couldn't open CUDA library libcupti.so.8.0. LD_LIBRARY_PATH: /lib64::/usr/local/cuda.0/extras/lib64\r\n\r\nAnd my file was actually libcublas.so.8.0 not the libcupti.so.8.0. Does this indicate some error in cuda installation? Thanks very much.", "I get this error:\r\n`W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory`\r\n\r\nappending the extras path to the `$LD_LIBRARY_PATH` did help, but, then I started to get permission errors. So I found that I need to load my IDE with sudo (I use PyCharm), so after entering `sudo charm .` in my working directory the permission errors were gone but the previous error returned!\r\nI tried to edit root's `.bashrc` but it didn't help.\r\n\r\nI have `tensorflow-gpu 2.2.0`, with CUDA release 10.1, V10.1.243\r\nOS: Ubuntu 18.04.4 LTS\r\nGPU: GeForce RTX 2080 Ti/PCIe/SSE2\r\n\r\nI am trying to use TensorBoard as a callback in Keras to monitor results. Please help."]}, {"number": 2624, "title": "bug fix in ApplyAdadelta update rule", "body": "Following the Zeiler ADADELTA paper, numerator of update equation\nshould be RMS(accum_update) \\* grad\n", "comments": ["Can one of the admins verify this patch?\n", "@vrv had requested that I also merge a fix to the test. I looked in [training_ops_test.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops_test.cc) and did not find any testing for adadelta. I also checked [adadelta_test.py](https://github.com/tensorflow/tensorflow/blob/16d395e5dea687ab3aece0a462e631de25c8d77d/tensorflow/python/training/adadelta_test.py), which has the correct rule in place. I do not know where else I should look for the bug.\n", "Correct me if I'm wrong, but you're saying that adadelta_test has the right equation, but that this test was passing before your change, and still passes after your change?\n", "Correct, the file [adadelta_test.py](https://github.com/tensorflow/tensorflow/blob/16d395e5dea687ab3aece0a462e631de25c8d77d/tensorflow/python/training/adadelta_test.py) passes before and after the fix. Maybe it is a tolerance issue? Or the values used in the test are too small for the sqrt to have an effect?\n", "doesn't that mean that the existing test is not really sufficient?  Can we modify the test so it would have failed before your fix?\n", "Probably - I suspect that decreasing the tolerance in the asserts (from 1e-3 to a smaller number) would cause the test to fail, although I don't know if there is a testing convention in place for tolerance. Is there a reason 1e-3 was chosen in the first place, or is that a default value? These tests use numerical approximations, right? So at some point a low enough tolerance will always cause a failure.\n", "There's no really good default, since every test has a different number of operations being performed.  Ideally we would test based off of expected number of ULP differences for every test, but that's somewhat impractical.\n\nIf you can find a setting that passes with your change and fails before it, that would be awesome.\n", "I'll look into it and get back to you.\n", "Ok, I added additional test variable options (learning rates, gradients, number of ADADELTA steps) in order to find a use case where the previously broken code fails and the newly patch code passes.\n\nI also rewrote the test to use loops over variables to reduce the amount of repeated code.\n", "Thanks for this!  mostly just lint / style issues, otherwise this looks great.\n", "Ok, I fixed the linting/style errors. Should I squash the commits or can you do that in the merge?\n", "We can squash now when we merge :)\n\n@tensorflow-jenkins test this please\n", "Thanks!\n", "Thanks for your help @vrv \n"]}]