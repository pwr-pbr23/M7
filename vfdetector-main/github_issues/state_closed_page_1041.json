[{"number": 22079, "title": "bazel build tensorflow/python/tools:print_selective_registration_header", "body": "ERROR: /Users/xihu/Desktop/tensorflow-1.10.1/tensorflow/BUILD:581:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\r\n  File \"/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/framework/framework_lib.py\", line 52, in <module>\r\n    from tensorflow.python.framework.importer import import_graph_def\r\n  File \"/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 27, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 39, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\n  File \"/private/var/tmp/_bazel_xihu/07e18ff1fe4322e207df6775908e278a/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 24, in <module>\r\n    import enum  # pylint: disable=g-bad-import-order\r\nImportError: No module named enum\r\nTarget //tensorflow/python/tools:print_selective_registration_header failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 7515.094s, Critical Path: 226.54s\r\nINFO: 7850 processes: 7850 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\nhow can i to fix error?", "comments": ["Depending on the python version, I think you may need to install `pip install enum`?", "I have install python 3.6.5,  \r\npip 18.0 from /usr/local/lib/python3.6/site-packages/pip (python 3.6)\r\n what should i  do next?\r\n", "Nagging Assignee @jart: It has been 91 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Apologies for the delay in response. @MartinWong-H Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 22078, "title": "Tensorflow runtime error (misaligned data) with ARM 32bit NEON.", "body": "With a c++ application using tensorflow in armv7 (32bit arm) + NEON, we had been experiencing the following errors in run-time:\r\n```\r\n[ 7128.415134] Alignment trap: not handling instruction f9048a1f at [<b28ca130>]\r\n[ 7128.420838] Unhandled fault: alignment exception (0x801) at 0xbef9d1d4\r\n[ 7128.427333] pgd = eb6dc000\r\n[ 7128.430014] [bef9d1d4] *pgd=6c6f5831, *pte=b2d5775f, *ppte=b2d57c7f\r\n[ 7128.436295] audit: type=1701 audit(1469483309.425:16): auid=0 uid=0 gid=0 ses=3 subj=User pid=8294 comm=\"hello-taos\" exe=\"/usr/bin/7\r\nBus error (core dumped)\r\n```\r\n\r\nWe are experiencing it even though we are using tensorflow 1.9.0, which has https://github.com/tensorflow/tensorflow/commit/88103d000add4ea7f8d1a34ee3c898fc79d9e3c7 included mentioned in #19158 .\r\n\r\nHowever, @again4you has successfully fixed the issue with a few compiler options mandated although the current fix is created quick and dirty.\r\n\r\n@again4you : Please send a PR when you embed the compiler options (for arm32) after rewriting the fix so that the compiler options are added in CMake script (and Bazel as well if it seems not too difficult) for ARM32.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Happens both with and without custom codes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 and Tizen\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No. But same architecture with such devices (armv7)\r\n- **TensorFlow installed from (source or binary)**: github.com source, built with cmake.\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 2.7.3\r\n- **Bazel version (if compiling from source)**: N/A (used cmake)\r\n- **GCC/Compiler version (if compiling from source)**: 6.2.1\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: ARM Mali. (not used)\r\n- **Exact command to reproduce**: Execute the C++ executable on a shell. (e.g., ```$ ./app ```\r\n\r\n### Describe the problem\r\nMemory misalignment error:\r\n```\r\n[ 7128.415134] Alignment trap: not handling instruction f9048a1f at [<b28ca130>]\r\n[ 7128.420838] Unhandled fault: alignment exception (0x801) at 0xbef9d1d4\r\n```\r\nHowever, we have a fix, which is going to be cleaned up to be sent as a PR.\r\n", "comments": ["Nagging Assignee @rohan100jain: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There were a couple of fixes to alignment (e.g. [this one](https://github.com/tensorflow/tensorflow/commit/88df5eaaa7313e1c714fc3e9413759faf043ff47#diff-b29424066d004b4c5bc4b563ba913954)) besides the original fix. \r\n@myungjoo : Sorry for the trouble but can you test this on master as well.", "Ok, we will try to cherry-pick fixes and remove workarounds and test. However, please wait for a while, I don't think we will get the confirm from our users soon (like within a few days) enough.\r\n\r\n@again4you : please cherry-pick alignment fixes and remove the fixes already causing problems in debian/ubuntu distros and release it both for Tizen & Ubuntu. (start the build & release process today and let's check the effects next week)", "@again4you : Please reopen if you are still seeing these issues.", "When the issues are resolved, https://launchpad.net/~nnstreamer/+archive/ubuntu/ppa will provide .deb packages for arm-ubuntu.", "what are you talking about a few compiler options ???can you tell me?", "@JasonSun99  @myungjoo Any updates on the fix ? I am experiencing a similar issue with tensorflow 1.12 on armv7 when starting a Session.\r\n```\r\nThread 9 \"python\" received signal SIGBUS, Bus error.\r\n[Switching to LWP 1435]\r\n0x712017b8 in std::condition_variable::condition_variable() () from /usr/lib/libstdc++.so.6\r\n(gdb) bt\r\n#0  0x712017b8 in std::condition_variable::condition_variable() () from /usr/lib/libstdc++.so.6\r\n#1  0x7551b47c in nsync::nsync_mu_semaphore_init(nsync::nsync_semaphore_s_*) ()\r\n   from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x75516e24 in nsync::nsync_waiter_new_() () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#3  0x75518ea4 in nsync::nsync_mu_lock(nsync::nsync_mu_s_*) ()\r\n   from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#4  0x7471e3a0 in ?? () from /usr/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so\r\nBacktrace stopped: previous frame identical to this frame (corrupt stack?)\r\n```\r\n\r\n```\r\n[ 3747.081027] Alignment trap: not handling instruction f9400a1f at [<712017b4>]\r\n[ 3747.081126] Alignment trap: not handling instruction f9400a1f at [<712017b4>]\r\n[ 3747.081133] Unhandled fault: alignment exception (0x801) at 0x003d359c\r\n[ 3747.081137] pgd = b91a0000\r\n[ 3747.081142] [003d359c] *pgd=3847f835, *pte=1e06e75f, *ppte=1e06ec7f\r\n[ 3747.081864] Unhandled fault: alignment exception (0x801) at 0x008be564\r\n[ 3747.082044] pgd = b91a0000\r\n[ 3747.082128] [008be564] *pgd=38481835, *pte=187c075f, *ppte=187c0c7f\r\n```", "@JasonSun99 @myungjoo,\r\nSame issue occurs on armv7a 32bits system with TensorFlow 2.4.\r\nI dig on gitHub and did not find any patch to solve this.\r\nCould you please elaborate on the compiler options you set to fix this issue?\r\n\r\nBR\r\nVincent", "> @JasonSun99 @myungjoo,\r\n> Same issue occurs on armv7a 32bits system with TensorFlow 2.4.\r\n> I dig on gitHub and did not find any patch to solve this.\r\n> Could you please elaborate on the compiler options you set to fix this issue?\r\n> \r\n> BR\r\n> Vincent\r\n\r\nFor TF 1.13, we use a fork at https://github.com/nnsuite/ubuntuport-tensorflow/\r\nFor TF 2.3, we've started using a fork at https://git.tizen.org/cgit/platform/upstream/tensorflow2/ , but we do not build non-lite TF here."]}, {"number": 22077, "title": "RoCE v1 error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: K40c 12G\r\n- **Exact command to reproduce**:\r\n **Worker :** python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+verbs\r\n**PS :** CUDA_VISIBLE_DEVICES='' python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=0 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=ps --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+verbs\r\n\r\nHi\uff0c\r\n\r\n I am trying to run TF on RoCE v1, and I have configured RoCE v1, including PFC, in my cluster. I configured PFC based on Trust L2. That is, I created a VLAN and used VLAN IP to communicate with each other. (I have tested it is correct by using \"ib_write_bw\", because I found packets are received by the pg 4). But when I used the VLAN IP to run distributed Tensorflow (source code from beachmark), I cannot find any packet in the pg 4. I looked up the source code, and found the RDMA_DEVICE_PORT is needed to be configured and the VLAN IP cannot be recognized corrcetly. Is it a bug? or I used RoCE v1 wrongly?\r\n\r\nThanks", "comments": ["Anyone can help me ?", "@eladweiss Mind to take a look?", "@shimonran +Shimon. ", "Hi wangshuzaizs, did you get any error messages in the log?", "@eladweiss \r\n\r\nHi,  eladweiss\r\n\r\nHere are the commands:\r\n\r\nPS:\r\n```\r\n$ CUDA_VISIBLE_DEVICES='0' python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=1 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=/home/shuai/imagenet-data --all_reduce_spec=pscpu --job_name=ps --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+verbs\r\n```\r\n\r\nWoker:\r\n```\r\n$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=/home/shuai/imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+verbs\r\n```\r\n\r\nAnd their outputs:\r\n\r\nPS:\r\n```\r\n2018-09-12 15:28:20.273246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-12 15:28:20.273318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-12 15:28:21.648094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-12 15:28:21.648164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-09-12 15:28:21.648177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-09-12 15:28:21.648712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:ps/replica:0/task:0/device:GPU:0 with 11475 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\n2018-09-12 15:28:21.881963: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-09-12 15:28:21.882020: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.10.10.5:3333}\r\n2018-09-12 15:28:21.888656: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2018-09-12 15:28:21.888705: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.10.10.5:3333}\r\n2018-09-12 15:28:21.896826: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-09-12 15:28:21.898419: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:375] Started server with target: grpc://localhost:2222\r\n2018-09-12 15:28:22.899591: I tensorflow/contrib/verbs/rdma_mgr.cc:129] Connected to remote node /job:worker/replica:0/task:0\r\n2018-09-12 15:28:27.106560: F tensorflow/contrib/verbs/rdma_mgr.cc:171] Check failed: s == IBV_WC_SUCCESS : transport retry counter exceeded(12) to /job:worker/replica:0/task:0\r\nAborted (core dumped)\r\n```\r\n\r\nWorker:\r\n```\r\n2018-09-12 15:28:20.043446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-12 15:28:20.217124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-12 15:28:20.379978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 2 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-12 15:28:20.552778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 3 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:84:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-09-12 15:28:20.553338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-09-12 15:28:21.710571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-12 15:28:21.710613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 2 3\r\n2018-09-12 15:28:21.710619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y N N\r\n2018-09-12 15:28:21.710623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N N N\r\n2018-09-12 15:28:21.710627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 2:   N N N Y\r\n2018-09-12 15:28:21.710630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 3:   N N Y N\r\n2018-09-12 15:28:21.711594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11473 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0, compute capability: 3.5)\r\n2018-09-12 15:28:21.810055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 11473 MB memory) -> physical GPU (device: 1, name: Tesla K40c, pci bus id: 0000:03:00.0, compute capability: 3.5)\r\n2018-09-12 15:28:21.907987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 11473 MB memory) -> physical GPU (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\n2018-09-12 15:28:22.058943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 11473 MB memory) -> physical GPU (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0, compute capability: 3.5)\r\n2018-09-12 15:28:22.214050: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.10.6:2222}\r\n2018-09-12 15:28:22.214083: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}\r\n2018-09-12 15:28:22.216382: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.10.6:2222}\r\n2018-09-12 15:28:22.216399: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}\r\n2018-09-12 15:28:22.222362: I tensorflow/contrib/verbs/rdma.cc:315] RoCE v2 is not configured for GID_INDEX 0\r\n2018-09-12 15:28:22.226575: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:375] Started server with target: grpc://localhost:3333\r\n2018-09-12 15:28:22.229269: I tensorflow/contrib/verbs/rdma_mgr.cc:129] Connected to remote node /job:ps/replica:0/task:0\r\n2018-09-12 15:28:26.522003: F tensorflow/contrib/verbs/rdma_mgr.cc:171] Check failed: s == IBV_WC_SUCCESS : transport retry counter exceeded(12) to /job:ps/replica:0/task:0\r\nAborted (core dumped)\r\n```\r\n\r\nThe relative configuration of network is as the following:\r\n\r\nPS\r\n```\r\n\r\neth2      Link encap:Ethernet  HWaddr 00:02:c9:b3:6c:a1\r\n          inet addr:12.12.10.16  Bcast:12.12.10.255  Mask:255.255.255.0\r\n          inet6 addr: fe80::202:c9ff:feb3:6ca1/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:1423943565 errors:0 dropped:5041404 overruns:5041404 frame:0\r\n          TX packets:1423659468 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:1000\r\n          RX bytes:2123250675808 (2.1 TB)  TX bytes:2129488926814 (2.1 TB)\r\n\r\neth3      Link encap:Ethernet  HWaddr 00:02:c9:b3:6c:a2\r\n          inet addr:12.12.11.16  Bcast:12.12.11.255  Mask:255.255.255.0\r\n          inet6 addr: fe80::202:c9ff:feb3:6ca2/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:47011941 errors:0 dropped:2525670 overruns:2525670 frame:0\r\n          TX packets:355985405 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:1000\r\n          RX bytes:3504413138 (3.5 GB)  TX bytes:503017410925 (503.0 GB)\r\n\r\neth2.10   Link encap:Ethernet  HWaddr 00:02:c9:b3:6c:a1\r\n          inet addr:10.10.10.6  Bcast:10.10.10.255  Mask:255.255.255.0\r\n          inet6 addr: fe80::202:c9ff:feb3:6ca1/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:141928452 errors:0 dropped:0 overruns:0 frame:0\r\n          TX packets:65669678 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:1000\r\n          RX bytes:2036642868180 (2.0 TB)  TX bytes:2039860904188 (2.0 TB)\r\n```\r\n\r\nWorker:\r\n```\r\n\r\neth2      Link encap:Ethernet  HWaddr f4:52:14:8c:bd:31\r\n          inet addr:12.12.10.15  Bcast:12.12.10.255  Mask:255.255.255.0\r\n          inet6 addr: fe80::f652:14ff:fe8c:bd31/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:1420277011 errors:0 dropped:3189942 overruns:3189942 frame:0\r\n          TX packets:1426906489 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:1000\r\n          RX bytes:2122949751945 (2.1 TB)  TX bytes:2129171627848 (2.1 TB)\r\n\r\neth3      Link encap:Ethernet  HWaddr f4:52:14:8c:bd:32\r\n          inet addr:12.12.11.15  Bcast:12.12.11.255  Mask:255.255.255.0\r\n          inet6 addr: fe80::f652:14ff:fe8c:bd32/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:60662288 errors:0 dropped:2492643 overruns:2492643 frame:0\r\n          TX packets:459009319 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:1000\r\n          RX bytes:4518366385 (4.5 GB)  TX bytes:650313430347 (650.3 GB)\r\n\r\neth2.10   Link encap:Ethernet  HWaddr f4:52:14:8c:bd:31\r\n          inet addr:10.10.10.5  Bcast:10.10.10.255  Mask:255.255.255.0\r\n          inet6 addr: fe80::f652:14ff:fe8c:bd31/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:89531729 errors:0 dropped:0 overruns:0 frame:0\r\n          TX packets:63422690 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:1000\r\n          RX bytes:2033861446400 (2.0 TB)  TX bytes:2039178863340 (2.0 TB)\r\n\r\n```", "@shamoya , can this be related to the GID configuration?\r\nRDMA_GID_INDEX: The GID index of the port. If not defined by user, a default suitable GID index will be set (RoCEV2 is favourable as default).", "Hi @wangshuaizs, \r\nI don't see your ROCE configuration while running the job.\r\nIt is not enough to use --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333, those are for the GRPC network, for the ROCE you need to set the env variable of the RDMA_* (see [verbs README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md))", "Thank you @shamoya ,\r\n\r\nI didn't set any env variables, and just left them as default.\r\n\r\nFor my cluster environment:\r\nEach machine has only one RDMA device with two ports, Mellanox connectx-3; For RDMA_GID_INDEX, this NIC doesn't support RoCEv2; For RDMA_TRAFFIC_CLASS, I think PCP in VLAN field is used for distinguishing different class in RoCEv1, and this env variable wasn't set, anyway.", "You still need to set the RDMA_GID_INDEX (especially if you are using RoCEv1).\r\nIf you are using Mellanox-OFED there a show_gids utility that will help you set it (maybe rdma-core has it or something similar as well). Otherwise, you can read the GIDs attr from the InfiniBand sysfs.\r\nYou will want to set it to the entry which belongs the vlan interface (eth2.10).\r\nYou can also use RDMA_DEVICE_PORT to make sure you are going from the correct port of the ConnectX3.", "@shamoya \r\n\r\nI tried to set relative env variables, including RDMA_DEVICE, RDMA_DEVICE_PORT, RDMA_GID_INDEX. Here are the outputs of two machines  (another two machines are used this time, and another ConnectX-5 NIC is added on each machine):\r\nps:\r\n```\r\nDEV     PORT    INDEX   GID                                     IPv4            VER     DEV\r\n---     ----    -----   ---                                     ------------    ---     ---\r\nmlx4_0  1       0       fe80:0000:0000:0000:0202:c9ff:feb3:8421                 v1      eth2\r\nmlx4_0  1       1       0000:0000:0000:0000:0000:ffff:0c0c:0c0c 12.12.12.12     v1      eth2\r\nmlx4_0  1       2       0000:0000:0000:0000:0000:ffff:0a0a:0a02 10.10.10.2      v1      eth2.10\r\nmlx4_0  2       0       fe80:0000:0000:0000:0202:c9ff:feb3:8422                 v1      eth3\r\nmlx4_0  2       1       0000:0000:0000:0000:0000:ffff:0c0c:0a0c 12.12.10.12     v1      eth3\r\nmlx5_0  1       0       fe80:0000:0000:0000:268a:07ff:fe8f:9bbc                 v1      eth4\r\nmlx5_0  1       1       fe80:0000:0000:0000:268a:07ff:fe8f:9bbc                 v2      eth4\r\nmlx5_0  1       2       0000:0000:0000:0000:0000:ffff:0c0c:0b0c 12.12.11.12     v1      eth4\r\nmlx5_0  1       3       0000:0000:0000:0000:0000:ffff:0c0c:0b0c 12.12.11.12     v2      eth4\r\nn_gids_found=9\r\n```\r\nand worker:\r\n```\r\nDEV     PORT    INDEX   GID                                     IPv4            VER     DEV\r\n---     ----    -----   ---                                     ------------    ---     ---\r\nmlx4_0  1       0       fe80:0000:0000:0000:0202:c9ff:feb3:6ca1                 v1      eth2\r\nmlx4_0  1       1       0000:0000:0000:0000:0000:ffff:0c0c:0c10 12.12.12.16     v1      eth2\r\nmlx4_0  1       2       0000:0000:0000:0000:0000:ffff:0a0a:0a06 10.10.10.6      v1      eth2.10\r\nmlx4_0  2       0       fe80:0000:0000:0000:0202:c9ff:feb3:6ca2                 v1      eth3\r\nmlx4_0  2       1       0000:0000:0000:0000:0000:ffff:0c0c:0a10 12.12.10.16     v1      eth3\r\nmlx5_0  1       0       fe80:0000:0000:0000:268a:07ff:fe8f:9bd8                 v1      eth4\r\nmlx5_0  1       1       fe80:0000:0000:0000:268a:07ff:fe8f:9bd8                 v2      eth4\r\nmlx5_0  1       2       0000:0000:0000:0000:0000:ffff:0c0c:0b10 12.12.11.16     v1      eth4\r\nmlx5_0  1       3       0000:0000:0000:0000:0000:ffff:0c0c:0b10 12.12.11.16     v2      eth4\r\nn_gids_found=9\r\n```\r\nI set env variables as following:\r\n```\r\nexport RDMA_DEVICE=\"mlx4_0\"\r\nexport RDMA_DEVICE_PORT=1\r\nexport RDMA_GID_INDEX=0/1/2 (respectively)\r\n```\r\n\r\nHowever, when RDMA_GID_INDEX is set as 0/1, the results are as before; when it is set as 2, i.e., VLAN, these 2 machines can communicate with each other, but I found no packets in prio_4 on end host, using ```ethtool -S eth2 | grep prio_4 ```. There seems something wrong, because I can find packets in prio_4 when using ```ib_write_bw```.", "Thanks @wangshuaizs \r\n\r\nIndeed GID index 2 is what you for going RoCEv1 on the VLAN network.\r\nWhen running the ib_write_bw (where it worked), did you set the SL to 4 (-S flag I think) ?\r\nYou can try here to set the RDMA_QP_SL=4 and see if that works for you.\r\nWhen using VLANs, usually the QP SL is used to configure the VLAN PCP.\r\nTell me if that works for you.", "hi @shamoya ,\r\n\r\nUnfortunately, that didn't work after using ```export  RDMA_QP_SL=4``` on both terminals. I didn't set SL when running ib_writer_bw, and the command and outputs are as following :\r\nreceiver:\r\n```\r\nib_write_bw -R --report_gbits --port=12500 -D 10                                                                           \r\n************************************\r\n* Waiting for client to connect... *\r\n************************************\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx5_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n CQ Moderation   : 100\r\n Mtu             : 1024[B]\r\n Link type       : Ethernet\r\n GID index       : 1\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : ON\r\n Data ex. method : rdma_cm\r\n---------------------------------------------------------------------------------------\r\n Waiting for client rdma_cm QP to connect\r\n Please run the same command with the IB/RoCE interface IP\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0000 QPN 0x025e PSN 0xb34c82\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:12:12:12:16\r\n remote address: LID 0000 QPN 0x025e PSN 0x7695fc\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:12:12:12:12\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\r\n 65536      410400           0.00               35.86              0.068393\r\n---------------------------------------------------------------------------------------\r\n```\r\nsend:\r\n```\r\nib_write_bw -R --report_gbits 10.10.10.6 --port=12500 -D 10\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx5_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n TX depth        : 128\r\n CQ Moderation   : 100\r\n Mtu             : 1024[B]\r\n Link type       : Ethernet\r\n GID index       : 1\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : ON\r\n Data ex. method : rdma_cm\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0000 QPN 0x025e PSN 0x7695fc\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:12:12:12:12\r\n remote address: LID 0000 QPN 0x025e PSN 0xb34c82\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:12:12:12:16\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\r\nConflicting CPU frequency values detected: 2899.951000 != 1200.000000. CPU Frequency is not max.\r\n 65536      410400           0.00               35.86              0.068393\r\n---------------------------------------------------------------------------------------\r\n```\r\nIn this case, I did see the number of packets in priority 4 increasing.", "@wangshuaizs \r\nI see with ib_write_bw you used rdma_cm (-R flag), we'll that works differently, since the rdma_cm auto-configures the SL and GID index of the connection. it's hard to say from the ib_write_bw output (maybe there are  some bugs in its prints), since it's showing 12.12.12.[12,16] GIDs were used (GID index 1), but you say that doesn't work.\r\nTry to run ib_write_bw without rdma_cm (exactly the verbs layer in TF) and see which parameters gets you priority setting correctly, then run the TF job same parameters.\r\nAlso check mlnx_qos output on the ConnectX3 to see the priority is enabled.\r\n", "@shamoya \r\n\r\nI run ib_write_bw as following:\r\nreceiver:\r\n```\r\nib_write_bw -d mlx4_0 -i 1 -x 2 --report_gbits --port=12500 -D 10\r\n\r\n************************************\r\n* Waiting for client to connect... *\r\n************************************\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx4_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n CQ Moderation   : 100\r\n Mtu             : 1024[B]\r\n Link type       : Ethernet\r\n GID index       : 2\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : OFF\r\n Data ex. method : Ethernet\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0000 QPN 0x0260 PSN 0x90ca24 RKey 0x6801010c VAddr 0x007f2529d77000\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:10:06\r\n remote address: LID 0000 QPN 0x0260 PSN 0x14f043 RKey 0xa8010105 VAddr 0x007f820fee5000\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:10:02\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\r\n 65536      415500           0.00               36.31              0.069247\r\n---------------------------------------------------------------------------------------\r\n```\r\nsender:\r\n```\r\nib_write_bw -d mlx4_0 -i 1 -x 2 -S 4 --report_gbits 10.10.10.6 --port=12500 -D 10\r\n---------------------------------------------------------------------------------------\r\n                    RDMA_Write BW Test\r\n Dual-port       : OFF          Device         : mlx4_0\r\n Number of qps   : 1            Transport type : IB\r\n Connection type : RC           Using SRQ      : OFF\r\n TX depth        : 128\r\n CQ Moderation   : 100\r\n Mtu             : 1024[B]\r\n Link type       : Ethernet\r\n GID index       : 2\r\n Max inline data : 0[B]\r\n rdma_cm QPs     : OFF\r\n Data ex. method : Ethernet\r\n---------------------------------------------------------------------------------------\r\n local address: LID 0000 QPN 0x0260 PSN 0x14f043 RKey 0xa8010105 VAddr 0x007f820fee5000\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:10:02\r\n remote address: LID 0000 QPN 0x0260 PSN 0x90ca24 RKey 0x6801010c VAddr 0x007f2529d77000\r\n GID: 00:00:00:00:00:00:00:00:00:00:255:255:10:10:10:06\r\n---------------------------------------------------------------------------------------\r\n #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]\r\nConflicting CPU frequency values detected: 1200.000000 != 3199.951000. CPU Frequency is not max.\r\n 65536      415500           0.00               36.31              0.069247\r\n---------------------------------------------------------------------------------------\r\n```\r\nSame parameters for TF job are these:\r\n```\r\nexport RDMA_DEVICE=\"mlx4_0\"\r\nexport RDMA_DEVICE_PORT=1\r\nexport RDMA_GID_INDEX=2\r\nexport RDMA_QP_SL=4\r\n```\r\nthey can let two machines communicate with each other, but no packets in corresponding priority_4 can be found (packets can be found when using ib_write_bw ).\r\n\r\nHere is the output of mlnx_qos  (same output on two machines):\r\n```\r\nmlnx_qos -i eth2\r\nPriority trust mode is not supported on your system\r\nPriority trust mode: none\r\nPFC configuration:\r\n        priority    0   1   2   3   4   5   6   7\r\n        enabled     0   1   1   0   1   0   0   0\r\n\r\ntc: 0 ratelimit: unlimited, tsa: vendor\r\n         priority:  0\r\ntc: 1 ratelimit: unlimited, tsa: vendor\r\n         priority:  1\r\ntc: 2 ratelimit: unlimited, tsa: vendor\r\n         priority:  2\r\ntc: 3 ratelimit: unlimited, tsa: vendor\r\n         priority:  3\r\ntc: 4 ratelimit: unlimited, tsa: vendor\r\n         priority:  4\r\ntc: 5 ratelimit: unlimited, tsa: vendor\r\n         priority:  5\r\ntc: 6 ratelimit: unlimited, tsa: vendor\r\n         priority:  6\r\ntc: 7 ratelimit: unlimited, tsa: vendor\r\n         priority:  7\r\n```", "Thanks @wangshuaizs \r\nFrom going over the code something is broken, it looks like it looks for env RDMA_SL when configuring the SL. No idea why, the documentation clearly says RDMA_QP_SL.\r\nTry setting RDMA_SL instead.", "Thanks @shamoya ,\r\n\r\nIt does work. It seems that the documentation doesn't match the source code. Hope that can be fixed. Thank you very much!", "Nagging Assignee @bignamehyp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@wangshuaizs Is your issue considered fixed now? If so, please close it. Thanks :)"]}, {"number": 22076, "title": "[INTEL MKL] Add MklSlice Op", "body": "Add MklSlice op, please review.", "comments": ["Hi @penpornk, thanks for your detailed comments! Have modified the code. This patch uses Mkl-dnn's primitive to implement slice, while the Mklslice already exists works for Intel Mkl. Any more comments?", "Thank you @penpornk ! I have modified the location of Slice shape function. And I also remember that there is a test added in mkl_layout_pass test(tensorflow/core/graph/mkl_layout_pass_test.cc). Besides, there already exists some UT tests for Slice op.", "Hi @penpornk , thanks for your advice! Modifications done.", "@pandaoxin can you look into the lint errors?", "@martinwicke The failed test said that:\r\n'Missing dependency: //tensorflow/contrib/eager/python:parameter_server',\r\nwhich seems not related to my committed code, while the lint error as well (cannot find my file).\r\nAre these errors known issues? Could you help me check with that? Thanks very much!", "Yeah, that does look unrelated. I'll try again, maybe it was broken at head.", "@pandaoxin Can you change the CHECKs for either DCHECKs or even better proper errors? We're trying to remove CHECKs since they have the ability to kill production serving system, while errors (non-OK Status) can be handled gracefully.", "@martinwicke Ok, thank you! Code modified.", "Thank you! Please also change `CHECK_NOTNULL` to `DCHECK_NOTNULL` and `LOG(FATAL)` to `LOG(DFATAL)` (in `mkl_slice_op.cc`).", "Hi @penpornk , since there is neither DCHECK_NOTNULL nor DFATAL, I turn both of them into DCHECK. Please review.", "Sorry about that. There might be a better substitute. Let's wait for @martinwicke.", "DCHECK for both these is fine.", "Thank you! @penpornk @martinwicke ", "It's making its way through the infra. The merge button is meaningless -- we use our scripts to merge anyway.", "@martinwicke ok, thanks."]}, {"number": 22075, "title": "[xrt] Allow zero-argument computations in XRTExecute", "body": "It can be sensible to have zero-argument computations (e.g. for\r\non-device generation of random numbers). XLA allows this fine,\r\nbut the `.Attr` for `Ninputs` implicitly defaulted to `>=1`,\r\npreventing `XRTExecute` requests with `Ninputs == 0`. Fix\r\nthe minimum and add a test to make sure this works end-to-end.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I believe the CLA should have gone through.", "Is there anything else I need to do to re-trigger the CLA check?", "CLAs look good, thanks!\n\n<!-- ok -->", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "What's the status of this? Looks like we've accumulated a merge conflict in the mean time. Do you need me to rebase?", "I'm sorry I didn't comment here earlier. Because the API changed under you it was easier for my colleague Davide to just fix it and submit internally, and the fix has now been pushed out. So I think this PR isn't needed any more. Can you confirm?", "Ah, perfect thanks, I missed that. For reference, the commit is 7cabc6be4e32dfb7f42c7f5e33549984bfdb68a3."]}, {"number": 22074, "title": "Add an explicit reason for NotImplementedError on eager model save", "body": "Should help with https://github.com/tensorflow/tensorflow/issues/22038", "comments": ["@allenlavoie Thanks!"]}, {"number": 22073, "title": "Fix ReLU layer serialization bug", "body": "This bug is reported at https://github.com/keras-team/keras/issues/11023, but actually ```keras-team/keras``` handles this correctly, ```tf.keras``` has the bug. You can also use the following code snippet to reproduce:\r\n```\r\nfrom tensorflow.python.keras.layers import Input, ReLU\r\nfrom tensorflow.python.keras.models import Model, save_model, load_model\r\nimport numpy as np\r\n\r\ninput = Input(shape=(5, 6, 3))\r\noutput = ReLU(6)(input)\r\nmodel = Model(inputs=input, outputs=output)\r\nmodel.summary()\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\r\n\r\nsave_model(model, \"/tmp/test1\")\r\nload_model(\"/tmp/test1\")\r\n```\r\nThis is the exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    load_model(\"/tmp/test1\")\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/saving.py\", line 229, in load_model\r\n    model = model_from_config(model_config, custom_objects=custom_objects)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/saving.py\", line 306, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 173, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1209, in from_config\r\n    process_layer(layer_data)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1195, in process_layer\r\n    layer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/layers/serialization.py\", line 64, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 175, in deserialize_keras_object\r\n    return cls.from_config(config['config'])\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1553, in from_config\r\n    return cls(**config)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/layers/advanced_activations.py\", line 302, in __init__\r\n    self.max_value = K.cast_to_floatx(max_value)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/keras/backend.py\", line 216, in cast_to_floatx\r\n    return np.asarray(x, dtype=_FLOATX)\r\n  File \"/Library/Python/2.7/site-packages/numpy/core/numeric.py\", line 492, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\nTypeError: float() argument must be a string or a number\r\n```\r\nThe bug is caused by the type of ```self.max_value``` is ```np.ndarray``` after ```K.cast_to_floatx```, so we should force to convert it to float when serializing it, otherwise, it would be serialized as ```np.ndarray``` and throws exception when loading back. This fix is following other layers in ```advanced_activations.py```, such as [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/advanced_activations.py#L63), they handle similar issue correctly.", "comments": ["Actually I found https://github.com/tensorflow/tensorflow/pull/21005 can fix this issue as well, we don't need to bother a separate PR, so we can wait that get merged. I'm closing this."]}, {"number": 22072, "title": "LSTMCell base article", "body": "resubmitting to master branch \r\nFor discussion see https://github.com/tensorflow/tensorflow/pull/22035", "comments": []}, {"number": 22071, "title": "MatMul flop incorrect for 3D inputs", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:N/A\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**:3.7\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:N/A\r\n- **GPU model and memory**:N/A\r\n- **Exact command to reproduce**:see below\r\n```python\r\nimport tensorflow as tf\r\na = tf.random_normal([1, 100, 100])\r\nb = tf.random_normal([1, 100, 100])\r\nc = tf.matmul(a, b)\r\ntf.profiler.profile(\r\n            tf.get_default_graph(),\r\n            cmd='op',\r\n            options=tf.profiler.ProfileOptionBuilder.float_operation())\r\n```\r\nThe outputs are correct (2M flops) if `a` and `b` have shape `[100, 100]`.\r\nBut with 3D inputs, it outputs no flops for matmul.\r\n\r\nThe reason is that for 3D inputs it uses the \"BatchMatMul\" op which does not have flop statistics implemented.", "comments": ["@tatatodd  -  Hi Todd, could you please take a look at this ?", "**Tensorflow version:** b'v1.13.1-6-gd32c49d4e3' 1.13.1\r\n\r\nThe following code which attempts to profile the flops and params of ResNet50 reports \r\nFLOPS:51,351,635 PARAMS:25,636,712\r\n\r\nParams seems ok but flops is way off (should be ~4GFLOPS).\r\nPotentially could be the same issue\r\n \r\n```python\r\nimport tensorflow as tf\r\nimport keras.backend as K\r\nfrom keras.applications.resnet50 import ResNet50\r\n\r\ndef main():\r\n    run_meta = tf.RunMetadata()\r\n    with tf.Session(graph=tf.Graph()) as sess:\r\n        K.set_session(sess)        \r\n        \r\n        m = ResNet50()\r\n\r\n        opts = tf.profiler.ProfileOptionBuilder.float_operation()    \r\n        flops = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd='op', options=opts)\r\n\r\n        opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()    \r\n        params = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd='op', options=opts)\r\n\r\n        print('FLOPS:{:,} PARAMS:{:,}'.format(flops.total_float_ops, params.total_parameters))\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```", "Hi @rmlarsen, since this issue hasn't got addressed yet, I created a commit that may solve the BatchMatMul op flops problem, could you help take a look? Thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=22071\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=22071\">No</a>\n"]}, {"number": 22070, "title": "Upgrade version references from 1.10.0 to 1.11.0-rc0", "body": "", "comments": ["Rats, I based this on the wrong branch. I'll fix it.\r\n\r\n**Edit:** fixed."]}, {"number": 22068, "title": "Tensorflow C++ error when using CV Mat image ", "body": "I am trying to create a custom OCR engine using Tensorflow(v1.10) in Ubuntu 18.04, with Bazel (0.16) in C++.\r\n------------------------\r\n## Issues Faced\r\n- Core Dumped Error for dtype mismatch (0 vs 1)\r\n- Predicted value of weights is higher than 1, when Tensorflow Session is re-run in loop\r\n-----------------------\r\n#### Code\r\n\r\n```\r\nfor (size_t i=0; i<filenames.size(); i++)\r\n\t{\r\n\t\tcv::Mat image = cv::imread(filenames[i]);\r\n\r\n\t\tTensor image_tensor (tensorflow::DT_FLOAT,tensorflow::TensorShape{1,tf_height,tf_width,3});\r\n\t\timage.convertTo(image, CV_32FC1);\r\n\t\ttensorflow::StringPiece tmp_data = image_tensor.tensor_data();\r\n\t\tmemcpy(const_cast<char*>(tmp_data.data()), (image.data), tf_height * tf_width * sizeof(float));\r\n\r\n\t\t// Creating a Session with the Graph\r\nstd::unique_ptr<tensorflow::Session>session(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\t\t//session->tensorflow::reset(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\t\ttensorflow::Status session_create_status = session->Create(graph_def);\r\n\r\n\t\tstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {{inputLayer, image_tensor}};\r\n\t\toutputs.clear();\r\n\t\tStatus runStatus = session->Run(inputs, {outputLayer}, {}, &outputs);\r\n\t\tif (!runStatus.ok()) {\r\n\t\t\tLOG(ERROR) << \"Running model failed: \" << runStatus;\r\n\t\t\treturn -1;\r\n\t\t}\r\n\t\tcv::imshow(\"Original Image\",image);\r\n\t\tcv::waitKey();\r\n\t\tsession->Close();\r\n\t\timage.release();\r\n\t}\r\n```\r\n\r\n-----------------------\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "That `CV_32FC1` is supposed to be a [`CV_32FC3`](https://github.com/PatWie/tensorflow-cmake/blob/master/examples/resize/opencv_version.cc#L30) since your tensor should have 3 channels, right?\r\nI am pretty sure [SO](http://stackoverflow.com) would be the better place to ask this question.", "Have I written custom code: yes\r\nOS Platform and Distribution: Ubuntu 18.04\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.10 (CPU version)\r\nBazel version:0.16\r\nCUDA/cuDNN version: 9.0 (installed but not used)\r\nGPU model and memory: Nvidia 1070 (present but not used)\r\nExact command to reproduce: N/A\r\nMobile device No\r\n\r\nIssues\r\n- Predicted values is higher than one. (It accumulates after every loop? Even though the tensor is cleared at the beginning of the loop )\r\n------------------------------------\r\nAttached herewith is the code\r\n\r\n```\r\n#include <fstream>\r\n#include <utility>\r\n#include <vector>\r\n#include <iostream>\r\n\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\n#include <opencv2/core/mat.hpp>\r\n#include <opencv2/videoio.hpp>\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include <opencv2/imgproc/imgproc.hpp>\r\n\r\nusing tensorflow::Flag;\r\nusing tensorflow::Tensor;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::int32;\r\n\r\nusing namespace std;\r\nusing namespace cv;\r\n\r\n\r\nconst int64 tf_height = 128;\r\nconst int64 tf_width = 128;\r\n\r\nint main()\r\n{\r\n\tstring folderpath = \"/home/ssatyanarayana/OCR-Engine-Local/data/alphabets/myTest/*.jpg\";\r\n\tstd::vector<String> filenames;\r\n\tcv::glob(folderpath, filenames);\r\n\r\n\tstring Labels = \"/home/ssatyanarayana/OCR-Engine-Local/k2tf-20180827143102/indices.txt\";\r\n\r\n\t// Set input & output nodes names\r\n\tstd::string inputLayer = \"conv2d_1_input\";\r\n\tstd::string outputLayer = \"k2tfout_0\";\r\n\r\n\t//Initializing the Graph\r\n\ttensorflow::GraphDef graph_def;\r\n\r\n\t// Specify file location of Output Graph\r\n\tstd::string graphFile = \"/home/ssatyanarayana/OCR-Engine-Local/output_graph.pb\";\r\n\r\n\t// Loading the graph to the given variable\r\n\ttensorflow::Status graphLoadedStatus = ReadBinaryProto(tensorflow::Env::Default(),graphFile,&graph_def);\r\n\tif (!graphLoadedStatus.ok()){\r\n\t\tstd::cout << graphLoadedStatus.ToString()<<std::endl;\r\n\t\treturn 1;\r\n\t}\r\n\r\n\tstd::vector<Tensor> outputs;\r\n\tstd::vector<std::pair<float,std::string>> sorted;\r\n\r\n\tfor (size_t i=0; i<filenames.size(); i++)\r\n\t{\r\n\t\tcv::Mat image = cv::imread(filenames[i]);\r\n\r\n\t\tTensor image_tensor (tensorflow::DT_FLOAT, tensorflow::TensorShape{1,tf_height,tf_width,3});\r\n\t\timage.convertTo(image, CV_32FC3);\r\n\t\ttensorflow::StringPiece tmp_data = image_tensor.tensor_data();\r\n\t\tmemcpy(const_cast<char*>(tmp_data.data()), (image.data), tf_height * tf_width * sizeof(float));\r\n\r\n\t\t// Creating a Session with the Graph\r\n\t\tstd::unique_ptr<tensorflow::Session> session(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\t\t//session->tensorflow::reset(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\t\ttensorflow::Status session_create_status = session->Create(graph_def);\r\n\r\n\t\tstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {{inputLayer, image_tensor}};\r\n\t\toutputs.clear();\r\n\t\tStatus runStatus = session->Run(inputs, {outputLayer}, {}, &outputs);\r\n\t\tif (!runStatus.ok()) {\r\n\t\t\tLOG(ERROR) << \"Running model failed: \" << runStatus;\r\n\t\t\treturn -1;\r\n\t\t}\r\n\r\n\t\t// Labels of the Prediction\r\n\t\tstd::cout << \"final output size=\" << outputs.size() << std::endl;\r\n\t\ttensorflow::Tensor output = std::move(outputs.at(0));\r\n\t\tauto scores = output.flat<float>();\r\n\t\tstd::cout << \"scores size=\" << scores.size() << std::endl;\r\n\r\n\t\t// Load Label File for comparison\r\n\t\tstd::string labelfile = \"/home/ssatyanarayana/OCR-Engine/data/alphabets/labels.txt\";\r\n\t\tstd::ifstream label(labelfile);\r\n\t\tstd::string line;\r\n\t\tstd::cout<<\"Label File Loaded\"<<std::endl;\r\n\r\n\t\t// Sort output for Top Labels\r\n\t\tsorted.clear();\r\n\t\tfor (unsigned int i =0; i<=1000 ;++i){\r\n\t\t\tstd::getline(label,line);\r\n\t\t\tsorted.emplace_back(scores(i),line);\r\n\t\t}\r\n\r\n\t\tstd::sort(sorted.begin(),sorted.end());\r\n\t\tstd::reverse(sorted.begin(),sorted.end());\r\n\t\tstd::cout << \"size of the sorted file is \"<<sorted.size()<< std::endl;\r\n\r\n\t\tfor(unsigned int i =0 ; i< 5;++i)\r\n\t\t{\r\n\t\t\tstd::cout << \"The output of the current graph has category  \" << sorted[i].second << \" with probability \"<< sorted[i].first << std::endl;\r\n\t\t}\r\n\r\n\t\tcv::imshow(\"Original Image\",image);\r\n\t\tcv::waitKey();\r\n\t\tsession->Close();\r\n\t\timage.release();\r\n\t}\r\n    return 0;\r\n}\r\n```\r\n\r\nThanks in advance for the help!!", "@PatWie.. Thank you for your response. That was helpful.  \r\n\r\nYour link that you posted is not valid. Could you kindly update that? \r\n\r\n@PatWie @skye \r\nAttached herewith is an updated code that I am currently working on:\r\n```\r\n/*\r\n * main.cpp\r\n *\r\n *  Created on: Aug 21, 2018\r\n *      Author: ssatyanarayana\r\n */\r\n\r\n\r\n#include <fstream>\r\n#include <utility>\r\n#include <vector>\r\n#include <iostream>\r\n\r\n#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n\r\n#include <opencv2/core/mat.hpp>\r\n#include <opencv2/videoio.hpp>\r\n#include <opencv2/highgui/highgui.hpp>\r\n#include <opencv2/imgproc/imgproc.hpp>\r\n\r\nusing tensorflow::Flag;\r\nusing tensorflow::Tensor;\r\nusing tensorflow::Status;\r\nusing tensorflow::string;\r\nusing tensorflow::int32;\r\n\r\nusing namespace std;\r\nusing namespace cv;\r\n\r\n\r\nconst int64 tf_height = 128;\r\nconst int64 tf_width = 128;\r\n\r\nint main()\r\n{\r\n\tstring folderpath = \"/home/ssatyanarayana/OCR-Engine-Local/data/alphabets/myTest/*.jpg\";\r\n\tstd::vector<String> filenames;\r\n\tcv::glob(folderpath, filenames);\r\n\r\n\t// Set input & output nodes names\r\n\tstd::string inputLayer = \"conv2d_1_input\";\r\n\tstd::string outputLayer = \"k2tfout_0\";\r\n\r\n\t//Initializing the Graph\r\n\ttensorflow::GraphDef graph_def;\r\n\r\n\t// Specify file location of Output Graph\r\n\tstd::string graphFile = \"/home/ssatyanarayana/OCR-Engine-Local/output_graph.pb\";\r\n\r\n\t// Loading the graph to the given variable\r\n\ttensorflow::Status graphLoadedStatus = ReadBinaryProto(tensorflow::Env::Default(),graphFile,&graph_def);\r\n\tif (!graphLoadedStatus.ok()){\r\n\t\tstd::cout << graphLoadedStatus.ToString()<<std::endl;\r\n\t\treturn 1;\r\n\t}\r\n\r\n\tstd::vector<Tensor> outputs;\r\n\r\n\tfor (size_t i=0; i<filenames.size(); i++)\r\n\t{\r\n\t\tcv::Mat image = cv::imread(filenames[i]);\r\n\r\n\t\tTensor image_tensor (tensorflow::DT_FLOAT, tensorflow::TensorShape{1,tf_height,tf_width,3});\r\n\t\timage.convertTo(image, CV_32FC3);\r\n\t\ttensorflow::StringPiece tmp_data = image_tensor.tensor_data();\r\n\t\tmemcpy(const_cast<char*>(tmp_data.data()), (image.data), tf_height * tf_width * sizeof(float));\r\n\r\n\t\t// Creating a Session with the Graph\r\n\t\tstd::unique_ptr<tensorflow::Session> session(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\t\t//session->tensorflow::reset(tensorflow::NewSession(tensorflow::SessionOptions()));\r\n\t\ttensorflow::Status session_create_status = session->Create(graph_def);\r\n\r\n\t\tstd::vector<std::pair<string, tensorflow::Tensor>> inputs = {{inputLayer, image_tensor}};\r\n\t\toutputs.clear();\r\n\r\n\t\tStatus runStatus = session->Run(inputs, {outputLayer}, {}, &outputs);\r\n\t\tif (!runStatus.ok()) {\r\n\t\t\tLOG(ERROR) << \"Running model failed: \" << runStatus;\r\n\t\t\treturn -1;\r\n\t\t}\r\n\r\n\t\tfor (auto &t : outputs)\r\n\t\t{\r\n\t\t\tstd::cout << t.DebugString()<<std::endl;\r\n\t\t\ttensorflow::TTypes<float, 2>::Tensor scores = t.flat_inner_dims<float>();\r\n\t\t\tauto dims = scores.dimensions();\r\n\t\t\tint imgCount = dims[0];\r\n\t\t\tint classesCount = dims[1];\r\n\t\t\tfor(int i = 0; i<imgCount; i++)\r\n\t\t\t{\r\n\t\t\t\tfloat maxVal = scores(i,0);\r\n\t\t\t\tint maxIndex = 0;\r\n\t\t\t\tfor(int j = 1; j<classesCount; j++) {\r\n\t\t\t\t\tfloat val = scores(i,j);\r\n\t\t\t\t\tif(val > maxVal) {\r\n\t\t\t\t\t\tmaxVal = val;\r\n\t\t\t\t\t\tmaxIndex = j;\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tstd::cout << \"Img\" << to_string(i) << \" prediction: \" << to_string(maxIndex) << \", score: \" << to_string(maxVal)<<std::endl;\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tcv::imshow(\"Original Image\",image);\r\n\t\tcv::waitKey();\r\n\t\tsession->Close();\r\n\t\timage.release();\r\n\t}\r\n    return 0;\r\n}\r\n\r\n\r\n```\r\n\r\n## Issues Faced:\r\n-Output tensor fails to clear and provide accurate weights for consecutive loops. \r\n-Same graph provides higher accuracy in Python (with Keras as TF as backend) when compared to using in C++ directly\r\n\r\nYou can access all the required files on [https://github.com/shashank2710/OCR-Engine](url)\r\n ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22067, "title": "Add 1.11 release notes", "body": "This change adds the release notes for 1.11, which were mostly shepherded by a new internal automation tool. Cool!", "comments": []}, {"number": 22066, "title": "`true_positives` returned Variable rather than Tensor or Operation as update_op in \"call_for_each_tower\"", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```python\r\ndef fn():\r\n  c1 = tf.constant([1,1,0,0])\r\n  c2 = tf.constant([1,0,1,0])\r\n  tp, op = tf.metrics.true_positives(c1, c2)\r\n  print(tp)\r\n  print(op)\r\n\r\n# Without DistributionStrategy\r\nfn()\r\n# Outputs:\r\n# Tensor(\"true_positives/Identity:0\", shape=(), dtype=float32)\r\n# Tensor(\"true_positives/AssignAdd:0\", shape=(), dtype=float32_ref)\r\n\r\n# With DistributionStrategy\r\nds = tf.contrib.distribute.OneDeviceStrategy(\"/cpu:0\")\r\nwith ds.scope():\r\n  ds.call_for_each_tower(fn)\r\n# Outputs:\r\n# Tensor(\"true_positives_1/Identity:0\", shape=(), dtype=float32, device=/device:CPU:0)\r\n# <tf.Variable 'true_positives_1/AssignAddVariableOp' shape=() dtype=float32>\r\n```\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhile using Estimator, we define the metrics in the `model_fn`, and pass it to the `EstimatorSpec` as `eval_metric_ops`. The `EstimatorSpec` raises an error regarding the `update_op` returned by `tf.metrics.true_positives` as it is neither a Tensor or an Operation when it is called through `ds.call_for_each_tower` in a `with ds.scope():` block. It appears that this originates from the `count` variable returned from `metric_variable` not having a reference dtype, which causes `state_ops.add_assign` to return `ref.assign_add` straight, without any Tensor wrapping. This makes it impossible to include `true_positives` in the `model_fn` code if a distribution strategy is to be used with Estimator. The situation holds also for `true_negatives`, `false_positives` and `false_negatives`.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["/CC @josh11b, can you take a look?", "I believe the issue is just the difference between reference and resource variables. Resource variables are the new thing and generally have better semantics, and we switch to using resource variables anytime you use a DistributionStrategy. @alextp , is there an easy way to trigger switching to resource variables without using a DistributionStrategy so we can validate this theory? The metrics function is using variable_scope.variable() to create the variable.\r\n\r\nAnyway, the return value from assign_add() is different between the two types: for reference variables you get an _ref tensor as you have observed, for resource variables you get an _UnreadVariable. However the _UnreadVariable should generally speaking match the semantics of the _ref tensor. When you say \"This makes it impossible to include true_positives in the model_fn code if a distribution strategy is to be used with Estimator.\", what error do you actually get?", "@josh11b The error comes from the type checking from `EstimatorSpec`: \r\n```\r\neval_metric_ops[true_positives] must be Operation or Tensor, given: <tf.Variable 'true_positives_1/AssignAddVariableOp' shape=() dtype=float32>\r\n```", "@alextp Looks like this is coming from https://github.com/tensorflow/tensorflow/blob/e970a022ef6a3602dd5c9ea15afa96a2291880b1/tensorflow/python/estimator/model_fn.py#L469 .  Perhaps it should be changed to use `is_tensor_like()` so _UnreadVariable passes?", "I agree that this will fix the issue. Does anyone want to make this pull request to Estimator?", "I have made a [pull request](https://github.com/tensorflow/tensorflow/pull/22264), please kindly review. Thanks", "@dave-msk thank you for submitting the PR! Is this issue fixed now and should it be closed? ", "Nagging Assignee @josh11b: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@guptapriya I can confirm that this issue is fixed.", "I have the same problem in tensorflow version 1.12.0. \r\nWhich tf version does not have this problem?", "Currently only nightly.\n\nOn Mon, Feb 18, 2019 at 5:11 AM Simpatech-app <notifications@github.com>\nwrote:\n\n> I have the same problem in tensorflow version 1.12.0.\n> Which tf version does not have this problem?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22066#issuecomment-464725181>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxW4neAJPSVwWU7eJ_CuFMTFbpK4nks5vOqaagaJpZM4WZwyX>\n> .\n>\n\n\n-- \n - Alex\n"]}, {"number": 22065, "title": "Fix for stringpiece build failure cherrypick to r1.11", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 22064, "title": "Can't save models trained with tf.contrib.cudnn_rnn.CudnnRNNRelu", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0.3.11\r\n- **GPU model and memory**: Quadro M1200\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I try to create a saver for a session with a graph containing a module from the `tf.contrib.cudnn_rnn` package I get `AttributeError: 'Tensor' object has no attribute 'assign'`\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 154, in <module>\r\n    update_interval=config['training']['update_interval'])\r\n  File \"train.py\", line 66, in train\r\n    saver = tf.train.Saver()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1281, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1293, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 1330, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 778, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\", line 419, in _AddRestoreOps\r\n    assign_ops.append(saveable.restore(saveable_tensors, shapes))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 291, in restore\r\n    self._variables, opaque_params, validate_shape=False)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py\", line 217, in assign\r\n    return ref.assign(value, name=name)\r\nAttributeError: 'Tensor' object has no attribute 'assign'\r\n```\r\n", "comments": ["Any update on this issue? Is there anything else I can do to help?"]}, {"number": 22063, "title": "Tensorflow segfaults in eager mode with DEVICE_PLACEMENT_EXPLICIT", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.4\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1\r\n- **Python version**: 1.10.1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 7.1.3\r\n- **GPU model and memory**: GeForce GTX 1080, 8Gb\r\n- **Exact command to reproduce**:\r\n[1]: import tensorflow as tf\r\n[2]: tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)\r\n[3]: a = tf.zeros((2,2))\r\n\r\n\r\n\r\n\r\nFull output:\r\n```\r\nIn [1]: import tensorflow as tf\r\n\r\nIn [2]: tf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT)\r\n\r\nIn [3]: tf.__version__\r\nOut[3]: '1.10.1'\r\n\r\nIn [4]: a = tf.zeros((2,2))\r\n2018-09-04 12:39:47.441463: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-09-04 12:39:47.524855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-04 12:39:47.525190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 3.93GiB\r\n2018-09-04 12:39:47.525203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-09-04 12:39:47.706125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-04 12:39:47.706148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-09-04 12:39:47.706153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-09-04 12:39:47.706287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3660 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nSegmentation fault (core dumped)\r\n```", "comments": ["I can also reproduce this in a similar environment (v1.10.1 on Ubuntu 16.04). Also, this does not occur in v1.9.0.", "Nagging Assignee @rohan100jain: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "On 1.11 I get a stack trace instead of a segfault. The way to fix this is to explicitly specify whether you want the CPU or GPU device, so while tf.zeros((2, 2)) fails, the following works\r\n\r\n```\r\nwith tf.device('gpu:0'):\r\n  tf.zeros((2,2))\r\n```"]}, {"number": 22062, "title": "Keras load_model crash for custom layer using tf.gather", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0-dev20180823\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7\r\n- **GPU model and memory**: Nvidia Quadro 4000\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nWhen having a custom layer using tf.gather, the model could be built successfully and run without problem. I can save that model successfully; however, when loading the model, the error occurs. If no tf.gather custom layer used, there is no problem. Is this a bug or a mis-use?\r\n\r\nMy codes are:\r\n\r\n\tdef test_lambda_layer():\r\n\t  data_input = keras.Input(shape=(1,4,5), dtype=float)\r\n\t  mask = [0,1,1,0,1]\r\n\t  valid_out_idx = np.nonzero(mask)[0]\r\n\t  x = Lambda(lambda t: tf.gather(t, valid_out_idx, axis=-1))(data_input)\r\n\t  x = Conv2D(5, 1, use_bias=False, kernel_initializer='ones', trainable=False)(x)\r\n\t  model = keras.Model(inputs=data_input, outputs=x)\r\n\t  model.summary()\r\n\r\n\t  data = np.ones((1,1,4,5), dtype=float)\r\n\t  print(model.predict(data))\r\n\r\n\t  model.save('test.h5')\r\n\r\n\t  new_model = keras.models.load_model('test.h5')\r\n\t  new_model.summary()\r\n\r\n\t  print(new_model.predict(data))\r\n\r\nThe error message is:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 527, in make_tensor_proto\r\n\t\tstr_values = [compat.as_bytes(x) for x in proto_values]\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 527, in <listcomp>\r\n\t\tstr_values = [compat.as_bytes(x) for x in proto_values]\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 61, in as_bytes\r\n\t\t(bytes_or_text,))\r\n\tTypeError: Expected binary or unicode string, got {'type': 'ndarray', 'value': [1, 2, 4]}\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"test_keras.py\", line 31, in <module>\r\n\t\tmain()\r\n\t  File \"test_keras.py\", line 28, in main\r\n\t\ttest_lambda_layer()\r\n\t  File \"test_keras.py\", line 16, in test_lambda_layer\r\n\t\tnew_model = keras.models.load_model('test.h5')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 230, in load_model\r\n\t\tmodel = model_from_config(model_config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 310, in model_from_config\r\n\t\treturn deserialize(config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\t\tprintable_module_name='layer')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 173, in deserialize_keras_object\r\n\t\tlist(custom_objects.items())))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1302, in from_config\r\n\t\tprocess_node(layer, node_data)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1260, in process_node\r\n\t\tlayer(input_tensors[0], **kwargs)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 770, in __call__\r\n\t\toutputs = self.call(inputs, *args, **kwargs)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\", line 716, in call\r\n\t\treturn self.function(inputs, **arguments)\r\n\t  File \"test_keras.py\", line 5, in <lambda>\r\n\t\tx = Lambda(lambda t: tf.gather(t, valid_out_idx, axis=-1))(data_input)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2650, in gather\r\n\t\treturn gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3804, in gather_v2\r\n\t\t\"GatherV2\", params=params, indices=indices, axis=axis, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 513, in _apply_op_helper\r\n\t\traise err\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper\r\n\t\tpreferred_dtype=default_dtype)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1127, in internal_convert_to_tensor\r\n\t\tret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 228, in _constant_tensor_conversion_function\r\n\t\treturn constant(v, dtype=dtype, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 207, in constant\r\n\t\tvalue, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 531, in make_tensor_proto\r\n\t\t\"supported type.\" % (type(values), values))\r\n\tTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'type': 'ndarray', 'value': [1, 2, 4]}. Consider casting elements to a supported type.", "comments": ["Hi @ybsave, the problem is that your Lambda layer is relying on the global variable `valid_out_idx` inside its `lambda`. This is going to prevent you from loading the saved model in a different file anyway, I'd recommend hardcoding `valid_out_idx` like below or passing it in as another Input to your Model.\r\n\r\nThis works for me:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.layers import Input, Lambda, Conv2D\r\n\r\ndef test_lambda_layer():\r\n  data_input = keras.Input(shape=(1,4,5), dtype=float)\r\n  mask = [0,1,1,0,1]\r\n  valid_out_idx = np.nonzero(mask)[0]\r\n  # hardcode valid_out_idx\r\n  x = Lambda(lambda t: tf.gather(t, np.nonzero([0,1,1,0,1])[0], axis=-1))(data_input)\r\n  x = Conv2D(5, 1, use_bias=False, kernel_initializer='ones', trainable=False)(x)\r\n  model = keras.Model(inputs=data_input, outputs=x)\r\n  model.summary()\r\n\r\n  data = np.ones((1,1,4,5), dtype=float)\r\n  print(model.predict(data))\r\n\r\n  model.save('test.h5')\r\n\r\n  new_model = keras.models.load_model('test.h5')\r\n  new_model.summary()\r\n\r\n  print(new_model.predict(data))\r\n```\r\n", "@omalleyt12 Thank you for your response. Actually, I have many such \"mask\" variables in my network, which are online calculated; these cannot be known before nor passed as inputs. Could there be any way of doing this? Thank you.", "@ybsave without knowing too much about your problem, my recommendation would be:\r\n\r\n1) If it never changes, hard code it\r\n2) If it's known when the batch starts, pass it in as an Input\r\n3) If it's calculated during a batch run, calculate it using a Lambda layer or custom Layer\r\n\r\nHope that helps! Going to close this out as this is intended behavior", "Hello @omalleyt12, thanks for your response. But I already mentioned, it cannot be hard coded nor passed as an Input.\r\n\r\nWhen the program starts, I can have a pre-calculation of all masks needed. However, this seems the same as my above codes: getting \"mask\" values first, and then use non-zero indies for processing data_input. \r\n\r\nAlso, my program only builds a model and sets appropriate weights, but does NOT need training. My task is to build a new model from an existing model, where the existing model has several unused depths and the new model wants to remove them. For example, in one layer, the input depth is 16, but only 12 are actually used for my convolution (the convolution only performed for some input depths); then in the new model, I want to remove the 4 unnecessary depths, and such information was stored in the original model as \"mask\". I have so many such layers needed to be processed automatically. Therefore, they cannot be hard coded nor set as inputs, as they are not inputs of the new model but the intrinsic parameter for building the new model.\r\n\r\nI do not understand what the 3rd option you mentioned; isn't it the same as my above codes? Would you please give an example or keep this issue open? \r\n\r\nI do not understand what more information you need, would you please clarify? \r\n\r\nThanks.", "@ybsave I think the idea of 3 is that you could subclass `Layer` and pass the NumPy array to its constructor. You may then have to define `get_config` and `from_config`, but that's one path to getting everything serialized. The NumPy array would live with the `Layer` subclass.\r\n\r\nWe are hoping to serialize/deserialize `Model`s as TensorFlow graphs soon, which should work around function bytecode issues.\r\n\r\nIt does kinda seem like `Lambda` layers could capture in this case, but since the captured reference is to a mutable Python object getting the sematics exactly right seems super tricky.", "@allenlavoie Thank you for your explanation. I tried to use a custom layer to perform the tf.gather; however, the load_model still crashed. My codes are\r\n\r\n\tclass IndexLayer(Layer):\r\n\t  def __init__(self, valid_idx, **kwargs):\r\n\t\tself.valid_idx = valid_idx\r\n\t\tsuper(IndexLayer, self).__init__(trainable=False, **kwargs)\r\n\r\n\t  def build(self, input_shape):\r\n\t\tsuper(IndexLayer, self).build(input_shape)\r\n\r\n\t  def call(self, inputs):\r\n\t\treturn tf.gather(inputs, self.valid_idx, axis=-1)\r\n\r\n\t  def get_config(self):\r\n\t\tbase_config = super(IndexLayer, self).get_config()\r\n\t\tbase_config['valid_idx'] = self.valid_idx\r\n\t\treturn base_config\r\n\r\n\t  def from_config(cls, config):\r\n\t\treturn cls(**config)\r\n\r\n\tdef test_lambda_layer():\r\n\t  data_input = keras.Input(shape=(1,4,5), dtype=float)\r\n\t  mask = [0,1,1,0,1]\r\n\t  valid_out_idx = np.nonzero(mask)[0]\r\n\t  #x = Lambda(lambda t: tf.gather(t, np.nonzero(mask)[0], axis=-1))(data_input)\r\n\t  x = IndexLayer(valid_out_idx)(data_input)\r\n\t  x = Conv2D(5, 1, use_bias=False, kernel_initializer='ones',\r\n\t\t\t\t trainable=False)(x)\r\n\t  model = keras.Model(inputs=data_input, outputs=x)\r\n\t  model.summary()\r\n\r\n\t  data = np.ones((1,1,4,5), dtype=float)\r\n\t  print(model.predict(data))\r\n\r\n\t  model.save('test.h5')\r\n\r\n\t  new_model = keras.models.load_model('test.h5')\r\n\t  new_model.summary()\r\n\r\n\t  print(new_model.predict(data))\r\n\r\nand the error messages are\r\n\r\n\t  File \"test_keras.py\", line 332, in <module>\r\n\t\tmain()\r\n\t  File \"test_keras.py\", line 329, in main\r\n\t\ttest_lambda_layer()\r\n\t  File \"test_keras.py\", line 317, in test_lambda_layer\r\n\t\tnew_model = keras.models.load_model('test.h5')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 230, in load_model\r\n\t\tmodel = model_from_config(model_config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 310, in model_from_config\r\n\t\treturn deserialize(config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\t\tprintable_module_name='layer')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 173, in deserialize_keras_object\r\n\t\tlist(custom_objects.items())))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1292, in from_config\r\n\t\tprocess_layer(layer_data)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1278, in process_layer\r\n\t\tlayer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\t\tprintable_module_name='layer')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 163, in deserialize_keras_object\r\n\t\traise ValueError('Unknown ' + printable_module_name + ': ' + class_name)\r\n\tValueError: Unknown layer: IndexLayer\r\n\r\n\r\nIf I change the line\r\n\r\n    new_model = keras.models.load_model('test.h5')\r\nto\r\n\r\n    new_model = keras.models.load_model('test.h5', custom_objects={'IndexLayer':IndexLayer})\r\n\r\nThe error becomes\r\n\r\n\tFile \"test_keras.py\", line 325, in test_lambda_layer\r\n\tcustom_objects={'IndexLayer':IndexLayer})\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 230, in load_model\r\n\tmodel = model_from_config(model_config, custom_objects=custom_objects)\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 310, in model_from_config\r\n\treturn deserialize(config, custom_objects=custom_objects)\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\tprintable_module_name='layer')\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 173, in deserialize_keras_object\r\n\tlist(custom_objects.items())))\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1292, in from_config\r\n\tprocess_layer(layer_data)\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1278, in process_layer\r\n\tlayer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\tprintable_module_name='layer')\r\n\tFile \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 175, in deserialize_keras_object\r\n\treturn cls.from_config(config['config'])\r\n\tTypeError: from_config() missing 1 required positional argument: 'config'\r\n\r\nDid I miss something or incorrectly implement something?\r\n\r\nBtw: do you mean that my original codes would be supported in future Tensorflow versions? Will the numpy data be serialized/deserialized in the future? Actually, I am quite confused in @omalleyt12 's original response; I do not understand why hard coded the values is OK but using a constant like variable is not. \r\n\r\nThanks.\r\n", "Neat. I think you just need to set the [`custom_objects` argument to load_model](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model).", "Oh, I just updated my test one minute ago....\r\n After setting the custom_objects, there is still error as shown above.", "And `from_config` [should have the `@staticmethod` decorator](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/python/keras/engine/base_layer.py#L1538). In this case you can just take it out and use the base class method.", "@allenlavoie Thank you for your quick response. I tested several combinations, and still cannot find anyone to work. \r\n\r\nIf I remove the definition of \"from_config\" function and define the custom layer as\r\n\r\n\tclass IndexLayer(Layer):\r\n\t  def __init__(self, valid_idx, **kwargs):\r\n\t\tself.valid_idx = valid_idx\r\n\t\tsuper(IndexLayer, self).__init__(**kwargs)\r\n\r\n\t  def build(self, input_shape):\r\n\t\tsuper(IndexLayer, self).build(input_shape)\r\n\r\n\t  def call(self, inputs):\r\n\t\treturn tf.gather(inputs, self.valid_idx, axis=-1)\r\n\r\n\t  def get_config(self):\r\n\t\tbase_config = super(IndexLayer, self).get_config()\r\n\t\tbase_config['valid_idx'] = self.valid_idx\r\n\t\treturn base_config\r\n\r\n\tdef test_lambda_layer():\r\n\t  data_input = keras.Input(shape=(1,4,5), dtype=float)\r\n\t  mask = [0,1,1,0,1]\r\n\t  x = IndexLayer(np.nonzero(mask)[0])(data_input)\r\n\t  x = Conv2D(5, 1, use_bias=False, kernel_initializer='ones',\r\n\t\t\t\t trainable=False)(x)\r\n\t  model = keras.Model(inputs=data_input, outputs=x)\r\n\t  model.summary()\r\n\r\n\t  data = np.ones((1,1,4,5), dtype=float)\r\n\t  print(model.predict(data))\r\n\r\n\t  model.save('test.h5')\r\n\t#  model.save_weights('./weights')\r\n\r\n\t  new_model = keras.models.load_model('test.h5', custom_objects={'IndexLayer':IndexLayer})\r\n\t#  new_model = keras.Model(inputs=data_input, outputs=x)\r\n\t#  new_model.load_weights('./weights')\r\n\t  new_model.summary()\r\n\r\n\t  print(new_model.predict(data))\r\n\r\nThe error message is\r\n\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 527, in make_tensor_proto\r\n\t\tstr_values = [compat.as_bytes(x) for x in proto_values]\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 527, in <listcomp>\r\n\t\tstr_values = [compat.as_bytes(x) for x in proto_values]\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 61, in as_bytes\r\n\t\t(bytes_or_text,))\r\n\tTypeError: Expected binary or unicode string, got {'type': 'ndarray', 'value': [1, 2, 4]}\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"test_keras.py\", line 341, in <module>\r\n\t\tmain()\r\n\t  File \"test_keras.py\", line 338, in main\r\n\t\ttest_lambda_layer()\r\n\t  File \"test_keras.py\", line 324, in test_lambda_layer\r\n\t\tcustom_objects={'IndexLayer':IndexLayer})\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 230, in load_model\r\n\t\tmodel = model_from_config(model_config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 310, in model_from_config\r\n\t\treturn deserialize(config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\t\tprintable_module_name='layer')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 173, in deserialize_keras_object\r\n\t\tlist(custom_objects.items())))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1302, in from_config\r\n\t\tprocess_node(layer, node_data)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1260, in process_node\r\n\t\tlayer(input_tensors[0], **kwargs)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 770, in __call__\r\n\t\toutputs = self.call(inputs, *args, **kwargs)\r\n\t  File \"test_keras.py\", line 284, in call\r\n\t\treturn tf.gather(inputs, self.valid_idx, axis=-1)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2650, in gather\r\n\t\treturn gen_array_ops.gather_v2(params, indices, axis, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3804, in gather_v2\r\n\t\t\"GatherV2\", params=params, indices=indices, axis=axis, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 513, in _apply_op_helper\r\n\t\traise err\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper\r\n\t\tpreferred_dtype=default_dtype)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1127, in internal_convert_to_tensor\r\n\t\tret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 228, in _constant_tensor_conversion_function\r\n\t\treturn constant(v, dtype=dtype, name=name)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 207, in constant\r\n\t\tvalue, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 531, in make_tensor_proto\r\n\t\t\"supported type.\" % (type(values), values))\r\n\tTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'type': 'ndarray', 'value': [1, 2, 4]}. Consider casting elements to a supported type.\r\n\r\nIf I add \"@staticmethod\" and defined the \"from_config\" function, as\r\n\r\n    @staticmethod\r\n    def from_config(cls, config):\r\n      return cls(**config)\r\n\r\nthe error message is\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"test_keras.py\", line 343, in <module>\r\n\t\tmain()\r\n\t  File \"test_keras.py\", line 340, in main\r\n\t\ttest_lambda_layer()\r\n\t  File \"test_keras.py\", line 326, in test_lambda_layer\r\n\t\tcustom_objects={'IndexLayer':IndexLayer})\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 230, in load_model\r\n\t\tmodel = model_from_config(model_config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\", line 310, in model_from_config\r\n\t\treturn deserialize(config, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\t\tprintable_module_name='layer')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 173, in deserialize_keras_object\r\n\t\tlist(custom_objects.items())))\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1292, in from_config\r\n\t\tprocess_layer(layer_data)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1278, in process_layer\r\n\t\tlayer = deserialize_layer(layer_data, custom_objects=custom_objects)\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\serialization.py\", line 64, in deserialize\r\n\t\tprintable_module_name='layer')\r\n\t  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\", line 175, in deserialize_keras_object\r\n\t\treturn cls.from_config(config['config'])\r\n\tTypeError: from_config() missing 1 required positional argument: 'config'\r\n\r\nIf I add \"@classmethod\" and defined the \"from_config\" function, the error would be the same as not defining \"from_config\". \r\n\r\nAs shown in my codes, I also tested to use \"save_weights\" and \"load_weights\". Those would work successfully. However, the \"load_model\" always crashed. :(", "I see, probably fixed by https://github.com/tensorflow/tensorflow/pull/21005", "Thank you for the reference, @allenlavoie. It seems that I have to wait until they fixed the numpy array serialization.", "@ybsave I met the same problem as you, how did you solve it ?", "@7wdeepin No solution yet. Not sure whether Google people have make tensorflow support the serialization of numpy. ", "@ybsave  Thank you for the response."]}, {"number": 22061, "title": "Add //tensorflow:install_headers target", "body": "Used to prepare all the header files so they can easily be installed\r\ninto /usr/include when packaging TF.\r\n\r\nSigned-off-by: Jason Zaman <jason@perfinion.com>", "comments": ["rebased and fixed buildifier error", "*now* hopefully the buildifier stuff is fixed", "@gunan it appears to have gotten stuck somewhere internally, can you help? :)", "Just wanted to make sure I understand this one.  If I use this, will it automatically place the C++ headers in the correct place?  It's a little unclear from the TF documentation on how to build from source for C++ and install the headers and libs in the right place. "]}, {"number": 22060, "title": "System libs: unbundle more and add variables for PREFIX support", "body": "This adds a few variables so some common variables like $PREFIX, $LIBDIR, $INCLUDEDIR are supported and unbundle double-conversion and boringssl deps.", "comments": ["I added some more commits to unbundle absl_py as well.\r\n@angersson can you in particular review the third_party/repo.bzl bits? I added a system_link_files attr just like the link_files from your third_party_http_archive. If syslibs is enabled and both link_files and system_link_files specify the same file then it should us the system_ one. or should it instead require the system_ one to fully specify all of the files again instead of sharing between link_files and system_link_files?", "oh and someone will need to mirror, i moved it from native.http_archive to tf_http_archive so i could use system_build_file\r\n\"https://mirror.bazel.build/github.com/google/double-conversion/archive/3992066a95b823efc8ccc1baf82a1cfc73f6e9b8.zip\",\r\n", "Sorry in advance -- I'll have slow responses on this. I'm the release owner for 1.11, and will be pretty busy for the next couple of weeks.\r\n\r\n", "@perfinion there are a few bazel failures that I think may be related to this PR, can you look?", "I think 7461ff7837bb9c57f0020d8adf46a73596dfb77d fixed it, i rebased on top, lets see how the tests go now", "@martinwicke it looks good now, the only failure is //tensorflow/compiler/xla/tests:exhaustive_f32_elementwise_op_test_cpu which has been failing for a few weeks so is unrelated. :)\r\n\r\nAlso I've included this patchset (+protobuf unbundling which isnt part of this PR) in the Gentoo package for tensorflow-1.11.0_rc{0,1,2} and its working well so far.", "> (+protobuf unbundling which isnt part of this PR)\r\n\r\n@perfinion would you mind pointing me to your changes for protobuf unbundling? I see in the build script for the gentoo package that you added protobuf to TF_SYSTEM_LIBS, but I can't find the changes to Tensorflow to observe that option.\r\n\r\nedit\r\n\r\nNevermind, found it here: https://dev.gentoo.org/~perfinion/patches/tensorflow-patches-1.11.0_rc2.tar.bz2\r\nThank you!\r\n\r\nAny plans to include this in master?", "@noahstier \r\nThe discussion on sig-build is here: https://groups.google.com/a/tensorflow.org/forum/#!topic/build/sSOdm79w-DI\r\nand the rebased patch is here: https://github.com/perfinion/tensorflow/tree/protobuf\r\n"]}, {"number": 22058, "title": "tf.estimator.Estimator with distribution strategy messes up TensorBoard readability", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.12.0\r\n- **Python version**: 2.7\r\n- **Bazel version**: N/A\r\n- **Mobile device**: N/A\r\n- **CUDA/cuDNN version**: cuda-9-0 / libcudnn.so.7\r\n- **GPU model and memory**: 2x NVIDIA T4, 16GB\r\n- **Exact command to reproduce**: example provided here: https://github.com/patzm/tf-estimator-distribute-so\r\n\r\n### Describe the problem\r\nUsing a `tf.estimator.Estimator` with a distribution strategy like `OneDeviceStrategy` or `MirroredStrategy` creates a lot of tensors / operations that are not enclosed in a common scope.\r\nIf one views the graph in TensorBoard, it is very annoying.\r\nTensorBoard automatically zooms out until all nodes are visible, making everything so tiny and unreadable.\r\nAlso TensorBoard becomes pretty slow.\r\nIn essence this is the\r\n```python\r\n    run_config = tf.estimator.RunConfig(\r\n        model_dir='/tmp/some/dir',\r\n        session_config=session_config,\r\n        train_distribute=MirroredStrategy(num_gpus=2),  # or OneDeviceStrategy(device)\r\n        save_summary_steps=1,\r\n        save_checkpoints_steps=1e4,\r\n    )\r\n\r\n    estimator = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        config=run_config,\r\n        params={},\r\n    )\r\n```\r\nHere are two example images:\r\n\r\n* `OneDeviceStrategy`:\r\n  \r\n  ![OneDeviceStrategy](https://raw.githubusercontent.com/patzm/tf-estimator-distribute-so/master/images/OneDeviceStrategy.png)\r\n\r\n* `MirroredStrategy`:\r\n\r\n  ![MirroredStrategy](https://raw.githubusercontent.com/patzm/tf-estimator-distribute-so/master/images/MirroredStrategy.png)\r\n\r\nThe row / column of `Read_<num>` / `group_deps_<num>` nodes continues way beyond the area of the screenshot.\r\nThe initial view of the TensorBoard could look like this, with the nodes almost invisible, because they are so tiny:\r\n![TensorBoardInitialView](https://i.imgur.com/iuhxLCK.png)\r\n\r\n### Source code / logs\r\nexample provided here: https://github.com/patzm/tf-estimator-distribute-so\r\nI also raised the issue on StackOverflow: https://stackoverflow.com/questions/50924287/how-to-influence-the-name-scope-of-collated-variables-slots\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nMobile device", "issue description update above with these two fields", "@yuefengz , can you take a look?", "any news on this? If you point me to the right direction, I can also do some debugging :slightly_smiling_face: ", "@yuefengz , can you comment/advise here?", "I think @josh11b did some work.", "Same problem, It made me difficult to read graphs of tensorboard. Any solution here? \ud83d\ude1e", "same here, will it be fixed in the next release?", "@patzm is this still an issue? Could you load TF 1.12 (stable) or latest version and check whether the issue persists? It is be good if you can share a simplified code to reproduce the issue. If it was solved by newer version, please close the issue. Thanks!", "@jvishnuvardhan yes it still is an issue. I just tried it on TF 1.12 and got the same result. I already provided an example code in a repository I linked above. Here is the link again: https://github.com/patzm/tf-estimator-distribute-so\r\nIf you point me in the right direction, I am also motivated to contribute :smiley: And thanks for picking this up again!", "I'm no longer working on TensorBoard. Can you reassign to someone from the TensorBoard team? Thanks!", "There's a related tensorboard issue here: https://github.com/tensorflow/tensorboard/issues/403  I'm not sure if it's the exact same situation with Estimator (chains of conditionals) but the effect of many nodes in a long row that force a too-far-out initial zoom is the same.\r\n\r\nAs described here though, it would be ideal to fix this on the Estimator side by introducing some scope to contain these, since it's hard for TensorBoard to necessarily determine what makes sense to collapse automatically.  I'll leave our GitHub issue open for possible mitigations on our end (feel free to follow along there), but I'm unassigning myself from this issue because I'm not familiar enough with Estimator to know what we'd need to change on that side.", "@nfelt So, do you want to close this here and move on https://github.com/tensorflow/estimator if it is in the estimator perimeter?", "Estimator doesn't have its own issue tracker, so this is still the right place for an issue asking for changes to Estimator.", "ping :-)", "The solution here is likely some combination of scoping in model_fn plus cleanup in the way dist strat and estimator replicate the graph. That said, we don't have the ability to look into this in the short term; opening up for contributions.", "I already tried my best to scope all variables well in the `model_fn`. I am certain that the not-well-scoped helper variables originate from the distribution strategy or the estimator graph replication.\r\nI am motivated to contribute, if you can point me in the right direction.", "@patzm We see that you are using old version of tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22058\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22058\">No</a>\n"]}, {"number": 22057, "title": "Tensorflow as a bazel dependency produces inconsistent builds", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: [ 1.10.1, 1.10, 1.9, 1.8]  v1.8 does not exhibit this bug.\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: release 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:  7.3.0\r\n- **CUDA/cuDNN version**: 9.2 / 7.2.1\r\n- **GPU model and memory**:  GeForce GTX 1050, 4 GB, Driver 396.54\r\n- **Exact command to reproduce**:\r\n  - Extract broken.zip [broken.zip](https://github.com/tensorflow/tensorflow/files/2349443/broken.zip)\r\n  - bazel build ...\r\n  - Add a z to changling.h::baz character array.\r\n  - bazel build ... [ NOTE there was no rebuild of any targets ]\r\n\r\n### Describe the problem\r\nIt appears that specifying `build --config=cuda` and `build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain` in bazel.rc are breaking bazels ability to detect changes.  Removing these two lines & bazel detects source changes & recompiles as appropriate.\r\n\r\nI tried [master, v1.10.1, v1.10, v1.9, v1.8] this bug appears somewhere between v1.8 & v1.9 of tensorflow.  (maybe I will binary search to find what rev...).\r\n\r\n### Source code / logs\r\n[broken.zip](https://github.com/tensorflow/tensorflow/files/2349443/broken.zip)\r\n", "comments": ["Corresponding bug posted to bazel: https://github.com/bazelbuild/bazel/issues/6052", "This commit introduced this bug: https://github.com/tensorflow/tensorflow/commit/6e6dcf4ed62324069631cdcd23e7187ae75f6340", "How I pinned the issue to ^ revision, I ran my repro from above with different revisions from tf repo (using a script), and got the following results for the revisions around the pin point.  Note that the other 'inconsistent: true' cases are revisions that didn't change any code so no rebuild happened (false positive for bug).\r\n\r\nrev: 8441db8afeee0efbeac9b457016d8558fbcde2b6 inconsistent: True\r\nrev: 896a6d74959c02b5c41087f96e77ef166fe484e3 inconsistent: True\r\nrev: ee303929b18745e7892d872ceac46c326a32ea93 inconsistent: True\r\nrev: 9d7daede6e94090f0efa4e9ac59328c0d3887cc8 inconsistent: True\r\nrev: 787f185cd73a8b090a3dadc1432e9bfdf527e345 inconsistent: True\r\nrev: 8a2972f5ddc9a986b702397f18203d1513511d2b inconsistent: True\r\nrev: 2c9d129cb8f6e3f6a27d074664033e4fccd83a63 inconsistent: False\r\nrev: 98d0286770d182de0dbf952c1ec0e079d8471a26 inconsistent: True\r\nrev: 7a82d0fd10901f4b59f38e838a24a04df8305f73 inconsistent: True\r\nrev: aa32906829fc84f5a8624885d74b7fd5f8cc8ff9 inconsistent: False\r\nrev: d1f44e1c60d38cc36bc438b59338c3a4eecf0615 inconsistent: True\r\nrev: 0c420d1036d0951a270654b43e4eab451bb956c7 inconsistent: True\r\nrev: 69895ff3b6910238b677477d4ad1cc6cf05121c5 inconsistent: True\r\nrev: bab1cf5d49a4ab8c5ea8930612a9ba1d5b0d5ef3 inconsistent: True\r\nrev: 9db974d895201b8ab2e9e34e142917db898da823 inconsistent: True\r\nrev: 2cf614fcab604f1b3d6271004f681f7d635eda5f inconsistent: True\r\nrev: 25ad31da87086a88d1d14ed5db8731bb9fc90787 inconsistent: True\r\nrev: 09620a1fd3f28cc23f6627884927b6098717355e inconsistent: True\r\nrev: 12ea31462d02326f14475516f8290d6e224ee70d inconsistent: True\r\nrev: d88aca3f1589760369e41fe2c05227e7d532dff8 inconsistent: False\r\nrev: 0a06a20fcc09b6af8bdd69a52f7015c926113c47 inconsistent: False\r\nrev: 86aedb620a3a9de73b4c6e2d24763ff22aa45d03 inconsistent: True\r\nrev: 7f75fc526898c4c030b5c6f30deb331fcff7b70c inconsistent: True\r\nrev: b7d3d31a78ce90fd9733d67247ae34c694199d19 inconsistent: True\r\nrev: 17272b4d1ccb5c7bd0bc3015c34f8bd769516354 inconsistent: True\r\nrev: 56502dc77e7ead9c9a4f63bf3405a937307a6f37 inconsistent: True\r\nrev: 679d6da028392c20f4323f158743e7370b47d1f9 inconsistent: True\r\nrev: a3a5e5cad0bfdd28f43223980f64ce367c732aad inconsistent: True\r\nrev: ee561a30a09a6464bd0c9a0ef69e5f0523477fc8 inconsistent: True\r\nrev: 384ab54a95d7dc1d242fd6adcd354e68864f4c67 inconsistent: False\r\nrev: a39fa2d5265fecfa765eb4a417c537d627899598 inconsistent: True\r\nrev: 8a362a264e2219872b390eb8c22286acba32d39f inconsistent: True\r\nrev: 9d2c6ff2a542b9bd89b42e3b88e6299eae9bdcc4 inconsistent: True\r\nrev: 4d134bad0403ebb5722144d8f859a04a5f21efc2 inconsistent: True\r\nrev: c710b7b4f331108ea7166848a2e7c8f20506a4a4 inconsistent: False\r\nrev: bf6644f9d274f549707d3f2a80c77e5eda163ebb inconsistent: True\r\nrev: a4c9efe6a5bf143f844b1cffbdc839c399620b9b inconsistent: True\r\nrev: 67b6696f9620734369ae99e7895fa6570d7faca6 inconsistent: True\r\nrev: c36266e1ed6bf632408461844fe3e8ef21f32839 inconsistent: False\r\nrev: b3e68e52f1f6488710c478596c30e0f0eb2dcad1 inconsistent: True\r\nrev: 193dec8aec6624454ec0776dbeeaca31d5d0db95 inconsistent: True\r\nrev: 2058a241f159a9a6658d48156cca2a2906e1d712 inconsistent: False\r\nrev: 4dbaa658e78a3dcbbd5c65449a3612b70dd8b22e inconsistent: False\r\nrev: dd2f3ebe3ede1e7b89819f40f53fdfb6c0433af0 inconsistent: True\r\nrev: 8e9a742c0cdf84e7e4ab596ee14cc11d6c281c9e inconsistent: False\r\nrev: d3052b421960bb386a75448512974fb23e76186d inconsistent: True\r\nrev: 1b976096672b96f16cb504deb8643eb443db5dcb inconsistent: True\r\nrev: 13e7f92d120de8f6f548493eb49b74810888ffd4 inconsistent: True\r\nrev: dbeea771d4d2a4931e3b5a9f49e9cbb6efbf3ec3 inconsistent: False\r\nrev: 77100add30dd9ca12e451a9cdd7449eb2daef886 inconsistent: False\r\nrev: 2c567ec96934eba1165e964dea2510b1191ecddd inconsistent: False\r\nrev: 5e7d41d3aeb8e2d39ddf9270a1c10d907836dece inconsistent: True\r\nrev: 55711f7038459485162cd735da8979faa97f5027 inconsistent: True\r\nrev: dabc133de019109918591870ac16ca551ea0e710 inconsistent: True\r\nrev: 8f5a71ed907d86d34e106ec4fc8f5dad0e07ffbc inconsistent: True\r\nrev: 62a0eefd853bcec78034a9d80c7d9a2d6af6fc65 inconsistent: True\r\nrev: 96f4fefefab793f9673252b1937f23e5e3a9801a inconsistent: True\r\nrev: 2e4a4b4a3b994abb118c247ed9ecc7cc79a26950 inconsistent: True\r\nrev: 294fdb0c5452715ec46d0555245e3b130308ebb4 inconsistent: False\r\nrev: 291f34928d029cca995d9c055a42170bc74af896 inconsistent: True\r\nrev: e727f746e7da6a51b76ef1abe23fc05a8da59553 inconsistent: True\r\nrev: a5f13f793db5560bf90f902745471ce3a9e47630 inconsistent: True\r\nrev: f5cb20e5eae992826e5ab71625e888687c6cced1 inconsistent: True\r\nrev: 3d428b1427b8f1541892b41cf3b6681bdd8f486a inconsistent: True\r\n**rev: 6e6dcf4ed62324069631cdcd23e7187ae75f6340 inconsistent: True**\r\nrev: 028993236b2ee9674ab11294e9985d7beaf376bb inconsistent: False\r\nrev: c9acdaad10c5e2b8a98dfb55cbbe4769025f93a9 inconsistent: False\r\nrev: eab53f2cea0506d869b14713c6c532e0bbfd9c52 inconsistent: False\r\nrev: 8ac1def6906caf391eb40a03c7e7c014bf68b447 inconsistent: False\r\nrev: c0bf28ecc311759ac80e12515ad931b077aae635 inconsistent: False\r\nrev: 065436d71e867f7622c0567cb6aaf78b3e69d2a0 inconsistent: False\r\nrev: ee03d18c66d74bde4db29f079025ce74d36ad2dc inconsistent: False\r\nrev: bbc8fe70603c21f2a2a7086530035364b6f5b207 inconsistent: False\r\nrev: 45e4c1c4a260b063dd62404bdbd2089f91d668a9 inconsistent: False\r\nrev: e4dcf28ad1c56c3a8e41ca52e7d87169eb7f93d5 inconsistent: False\r\nrev: 1d5c44cd876377eb296cee22567228ea6f72a7ac inconsistent: False\r\nrev: 7c3cd0842a41aac47069dcf14567b88c32ea7b28 inconsistent: False\r\nrev: b1139814f91c5216eb5ff229ee7e1982e5f4e888 inconsistent: False\r\nrev: d913a243196fa07d4728c8f7c1ce6444ecd086eb inconsistent: False\r\nrev: 243aa2215fd0ea453d4fc1e09a6f8fb91c068d36 inconsistent: False\r\nrev: 31ca159c1c86cb983c78e134cc756489653228f2 inconsistent: False\r\nrev: c3587c5f25ed9dfc173476e61b1ec0445c2989be inconsistent: False\r\nrev: 006e293c7bc8a30ea8f9618cd305bc8719a96638 inconsistent: False\r\nrev: 664f756cccff174feb868f28e9a4785ffd2edfd6 inconsistent: False\r\nrev: f72e6f7b410fd61cbcd5d6993602e1a3983a5d86 inconsistent: False\r\nrev: 75c0cfd1fdb1b460fc10786ef89376f8e86fe8d4 inconsistent: False\r\nrev: 0ad7f20ed6876809a2b804365293a5c21dbcd374 inconsistent: False\r\nrev: 753cc5b3f7461b0b3f59605cba10b965aca0e3ad inconsistent: False\r\nrev: 433bb8e1fa0b300961a430c2652ad0776dcef187 inconsistent: False\r\nrev: 148790aaf7bf3607e5d6e6d10bce2666535c4a07 inconsistent: False\r\nrev: 4d03411da6fcc803d9abcef97a59072144e325f9 inconsistent: False\r\nrev: d67a99406f986021df9a0f8a99ff6eaf801dcb25 inconsistent: False\r\nrev: a4afd46dc55c060b7f5132f0d088702169f864e4 inconsistent: False\r\nrev: 2746bc3aec9b881343e99c10d0edebca92e0ee1f inconsistent: False\r\nrev: c734063a84b505f6423cb2d2b7147eaccee1a8f2 inconsistent: False\r\nrev: e4471c403a9e9430839900bd92c067d04580a51b inconsistent: False\r\nrev: b28938c3672db9c23f84298160658787d0ccf69d inconsistent: False\r\nrev: 564fcf1e48224f518fa5b62bbc5e80f84270d0fb inconsistent: False\r\nrev: 1f10b08b375f6d9c4800dc4183ef836b3d729605 inconsistent: False\r\n", "Had some time for more digging.  Looks like the crosstools `cxx_builtin_include_directory` is getting nuked to `\\`, [here](https://github.com/tensorflow/tensorflow/blob/b91e27a9c33d038af79a0944eb9046b926d483c8/third_party/gpus/cuda_configure.bzl#L1355).\r\n\r\nSetting the line back to what the above comment says it should be:\r\n`cuda_defines[\"%{host_compiler_includes}\"] = (host_compiler_includes + \"\\n  cxx_builtin_include_directory: \\\"%s\\\"\" % cuda_include_path)`\r\n\r\nAppears to fix the inconsistency problem.\r\n\r\nNow how to actually fix it...", "Nagging Assignee @drpngx: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@gunan @meteorcloudy any comment?", "Should be fixed as of https://github.com/tensorflow/tensorflow/commit/2538e68a69e585696175bd972cae119e06bde294", "Thanks @r4nt ! \r\n\r\nClosing. Feel free to re-open if the fix didn't work."]}, {"number": 22056, "title": "Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED", "body": "TensorFlow: 1.10.1\r\ncuda: 9.0\r\ncudnn: cudnn7.1 for cuda9.0\r\npython3\r\nUbuntu 18.04\r\nnvidia-driver: 390\r\n\r\nTry running cnns error got:\r\n\r\n```\r\nCould not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2018-09-04 23:13:52.571542: E tensorflow/stream_executor/cuda/cuda_dnn.cc:360] Possibly insufficient driver version: 390.77.0\r\n\r\n```\r\nI think the cuda version are already the newest version, and my cudnn and cuda are OK. Why still got this problem? The error message throw from cuda_dnn.cc should be more specific what's going on.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Same problem here. Log:\r\n\r\n```\r\n2018-09-27 10:02:39.079409: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2018-09-27 10:02:39.079563: E tensorflow/stream_executor/cuda/cuda_dnn.cc:360] Possibly insufficient driver version: 396.26.0\r\n```\r\n\r\nHave I written custom code: Yes\r\nOS Platform and Distribution: COS on GCP (Host), Ubuntu 18.04 (Docker)\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.10.1\r\nBazel version: N/A\r\nCUDA/cuDNN version: 9.0/7.2\r\nGPU model and memory: K80c, 12GiB\r\nExact command to reproduce: Used in a COCO retrained task\r\nMobile device: N/A\r\n\r\nDockerfile: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/nvidia.Dockerfile", "I found my problem is caused by installing the wrong version of CUDNN. @jinfagang could you try `nvidia-smi` and `dpkg -l | grep cudnn` and paste the output here?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Same Problem here.\r\n\r\n```\r\n~/projects/DeepSeqPan/scripts$ python runner.py --train\r\nUsing TensorFlow backend.\r\n2018-10-16 10:07:35.808646: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2018-10-16 10:07:38.048505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties:\r\nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:3b:00.0\r\ntotalMemory: 15.78GiB freeMemory: 15.36GiB\r\n2018-10-16 10:07:38.383778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties:\r\nname: Tesla V100-PCIE-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\r\npciBusID: 0000:d8:00.0\r\ntotalMemory: 15.78GiB freeMemory: 15.36GiB\r\n2018-10-16 10:07:38.385505: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1\r\n2018-10-16 10:07:39.098841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-16 10:07:39.098884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1\r\n2018-10-16 10:07:39.098891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y\r\n2018-10-16 10:07:39.098895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N\r\n2018-10-16 10:07:39.099423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14864 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\r\n2018-10-16 10:07:39.100600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14864 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\r\n================================================================================\r\nTrain new model on [bdata.20130222.mhci.txt]\r\n================================================================================\r\n2018-10-16 10:07:39.141214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1\r\n2018-10-16 10:07:39.141283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-16 10:07:39.141293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1\r\n2018-10-16 10:07:39.141317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y\r\n2018-10-16 10:07:39.141323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N\r\n2018-10-16 10:07:39.141752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14864 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)\r\n2018-10-16 10:07:39.142052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14864 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\r\n\r\nEpoch 1/1000\r\n2018-10-16 10:07:55.381849: E tensorflow/stream_executor/cuda/cuda_dnn.cc:353] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2018-10-16 10:07:55.381944: E tensorflow/stream_executor/cuda/cuda_dnn.cc:361] Possibly insufficient driver version: 390.87.0\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n_OS Platform and Distribution:_ **Ubuntu 18.04.1 LTS**\r\n_TensorFlow installed from:_ **conda**\r\n_TensorFlow version:_ **1.11.0**\r\n_CUDA/cuDNN version:_ **9.0.176/ 7.3.1**\r\n_GPU model and memory:_ **Tesla V100-PCIE-16GB**\r\n\r\nIs it really the driver version being the problem?", "> Is it really the driver version being the problem?\r\n\r\nNo, the problem was an incompatible Cudnn version (7.1.2) being installed by creating an Anaconda environment which I had to manually overwrite with the previously installed system's version (7.3.1).", "> > Is it really the driver version being the problem?\r\n> \r\n> No, the problem was an incompatible Cudnn version (7.1.2) being installed by creating an Anaconda environment which I had to manually overwrite with the previously installed system's version (7.3.1).\r\n\r\nI'm having the same problem and I'm also working on a conda environment. Can you tell me how to overwrite the previous version?", "Actually, I finally succeeded in installing the right driver version and do not have to overwrite cuDNN anymore: https://github.com/pplcc/ubuntu-tensorflow-pytorch-setup/issues/1", "Is this still an issue ?\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "In my case (Ubuntu 16.04, driver 390.87) apt updated:\r\n- libcudnn7 from 7.2.1.38-1+cuda9.0 to 7.4.2.24-1+cuda10.0\r\n- libnccl2 from 2.2.13-1+cuda9.0 to 2.3.7-1+cuda10.0\r\ncausing the same issue in TF.\r\nA way to prevent that could be `sudo apt-get purge nvidia-machine-learning-repo-ubuntu1604`\r\n ", "I had the same/similar problem:\r\n```\r\n2019-02-20 15:39:47.506767: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n2019-02-20 15:39:47.506890: E tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Possibly insufficient driver version: 390.77.0\r\n2019-02-20 15:39:47.506951: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at cudnn_rnn_ops.cc:1214 : Unknown: Fail to find the dnn implementation.\r\n```\r\n\r\nusing docker container with dockerfile:\r\n```\r\nFROM tensorflow/tensorflow:1.12.0-gpu-py3\r\n...\r\nRUN apt-get update -y && apt-get install -y python3-dev python3-pip python3-lxml\r\n...\r\n```\r\nwith Keras layer `CuDNNLSTM`.\r\n\r\nI believe the problem was installing `python3-dev python3-pip python3-lxml`. When I removed that line the error disappeared.", "I get the same problem after updating, before update everything was running with no problem\r\n\r\n> could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\r\n> possibly insufficient driver version: 410.104.0\r\n\r\nI am using ubuntu-18.04LTS, python3.6.7, tensorflow 1.9, I've install it by using bazel -0.17.2 build from source, cuda 10.0, cudnn-7.5.0 and my gpu is titan xp\r\n\r\nAny ideas? Maybe rollback to an older version of the driver?", "Yeah, in my case I ran TF 1.13  using CUDA 9.0 with CuDNN for CUDA 10.0 (since I was testing new TF 2.0). Downgrade `sudo apt install libcudnn7=7.5.0.56-1+cuda9.0` solved it.", "I found this, after all, to work for me :\r\n \r\n\r\n> -from  software & updates > Additional Drivers > choose nvidia-418(downloads and install it) \r\n> - reboot PC\r\n\r\nAs result got upgraded to cuda-10.1 (from 10.0). It works for now !", "For windows 10 .\r\nTF - 1.12.0,\r\nCuda - 9\r\nCudnn - 7.4.2\r\n\r\nClose the jupyter notebook and restart it . It will work fine.\r\nbut when we open another jupyter notebook involving cuda , it will again cause issues.\r\nso closing jupyter and working on one notebook at a time is the solution for me."]}, {"number": 22055, "title": "TensorFlow Speech Recognition for Android", "body": "Hello,\r\n\r\nI am new to this exicted feature TensorFlow. I could import the Sample project and able to run it successfully. Now i just need help to how to build the Speech Recognition from TensorFlow what are the files required to add. Can someone give the steps, which would help me in my sample application\r\n\r\nThanks\r\nSravan Kumar Shetty", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Thanks for your quick response.\r\n\r\n\r\nOS Platform and Distribution : Android\r\nTensorFlow installed from : N/A\r\nTensorFlow version : N/A\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\nMobile device : Android Lollipop version\r\n\r\nHere actually i am beginner to this. So i just wanted to know what are the installation, libraries required for Speech Recognition in order to integrate to Android Mobile application. using Desktop : Window-10 \r\nDo i need to install Blaze? \r\n\r\nThanks", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 22054, "title": "map_fn gives colocation errors with integer typed input tensors", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nCentOS Linux release 7.4.1708\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (pip3 install tensorflow-gpu==1.9.0)\r\n- **TensorFlow version (use command below)**:\r\nv1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**:\r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n9.0.176\r\n- **GPU model and memory**:\r\n GeForce GTX-1080 - 8GB memory\r\n- **Exact command to reproduce**:\r\n`python3 map_fn_GPU_error.py`\r\n\r\n### Describe the problem\r\nThe `tf.map_fn` function raises the following error if I provide two tensors of input one of which is `tf.float32` and the other `tf.int64`. This error doesn't appear if both of them are `tf.float32`.\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'map/TensorArray_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/device:GPU:0'\r\nColocation Debug Info:                                                                                                     \r\nColocation group had the following types and devices:\r\nTensorArrayReadV3: CPU                                                                                                    \r\nEnter: GPU CPU                                                                        \r\nTensorArrayV3: CPU                         \r\nTensorArrayScatterV3: CPU                                                                                                                                      \r\nPlaceholder: GPU CPU                                                                                   \r\n                                                           \r\nColocation members and user-requested devices:                   \r\n  RaggedLengths (Placeholder) /device:GPU:0\r\n  map/TensorArray_1 (TensorArrayV3)                                                                                                                            \r\n  map/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3) /device:GPU:0\r\n  map/while/TensorArrayReadV3_1/Enter (Enter) /device:GPU:0\r\n  map/while/TensorArrayReadV3_1 (TensorArrayReadV3) /device:GPU:0\r\n                                                  \r\n         [[Node: map/TensorArray_1 = TensorArrayV3[clear_after_read=true, dtype=DT_INT64, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=\"\"](map/strided_slice)]]\r\n                                                                                                                            \r\nCaused by op 'map/TensorArray_1', defined at:\r\n  File \"map_fn_GPU_error.py\", line 33, in <module>                                                                              \r\n    dtype=tf.float32)                                      \r\n  File \"/my/python/package/path/tensorflow/python/ops/functional_ops.py\", line 420, in map_fn    \r\n    for elem in elems_flat]                    \r\n  File \"/my/python/package/path/tensorflow/python/ops/functional_ops.py\", line 420, in <listcomp>                               \r\n    for elem in elems_flat]                      \r\n  File \"/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py\", line 754, in __init__\r\n    name=name)                                                           \r\n  File \"/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py\", line 160, in __init__         \r\n    self._handle, self._flow = create()            \r\n  File \"/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py\", line 157, in create                                 \r\n    name=scope)   \r\n  File \"/my/python/package/path/tensorflow/python/ops/gen_data_flow_ops.py\", line 7157, in tensor_array_v3\r\n    tensor_array_name=tensor_array_name, name=name)                  \r\n  File \"/my/python/package/path/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)                                                                    \r\n  File \"/my/python/package/path/tensorflow/python/framework/ops.py\", line 3414, in create_op\r\n    op_def=op_def)                                                                                                                                             \r\n  File \"/my/python/package/path/tensorflow/python/framework/ops.py\", line 1740, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    \r\n\r\n```\r\nThis is triggered by the line\r\n`sess.run(tf.global_variables_initializer())` in the code below. This code works perfectly on CPU however, I cannot seem to force placement on the GPU.\r\n\r\n### Source code / logs\r\n\r\nThis is a minimal example to reproduce the above error. (This is the file named `map_fn_GPU_error.py` used in the command above)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nwith_gpu = True\r\nif with_gpu:\r\n    device_name = '/device:GPU:0'\r\nelse:\r\n    device_name = '/device:CPU:*'\r\n\r\nbatch_size = 32\r\nmax_ragged_dim = 10\r\nembedding_dim = 20\r\n\r\n\r\ndef ragged_function(ragged_input, length_of_input):\r\n    # length_of_input is not used here but is in the actual function\r\n    # The error is caused whether or not it is used\r\n    return tf.reduce_sum(ragged_input, axis=0)\r\n\r\n\r\ndef final_loss_function(ragged_function_outputs):\r\n    weights = tf.get_variable('LossWeights', dtype=tf.float32,\r\n                              shape=(ragged_function_outputs.shape[1], 1))\r\n    return tf.reduce_sum(tf.matmul(ragged_function_outputs, weights))\r\n\r\n\r\nwith tf.device(device_name):\r\n    input_ragged_mat = tf.placeholder(name='RaggedMatrix', dtype=tf.float32,\r\n                                      shape=(None, max_ragged_dim, embedding_dim))\r\n    input_ragged_lengths = tf.placeholder(name='RaggedLengths', dtype=tf.int64,\r\n                                          shape=(None,))\r\n\r\n    ragged_function_outputs = tf.map_fn(lambda x: ragged_function(*x),\r\n                                        [input_ragged_mat, input_ragged_lengths], \r\n                                        # Note that input_ragged_lengths is of type int64\r\n                                        # This leads to an error on GPU placement\r\n                                        dtype=tf.float32)\r\n    final_loss = final_loss_function(ragged_function_outputs)\r\n\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\r\n    train_op = optimizer.minimize(final_loss)\r\n\r\n# Create input ragged_matrices, with zeros beyond the actual length\r\nragged_input_mat = np.random.random(size=(batch_size, max_ragged_dim, embedding_dim))\r\nragged_input_length_mat = np.random.randint(max_ragged_dim, size=(batch_size,)) + 1\r\nfor i, rlen in enumerate(ragged_input_length_mat):\r\n    ragged_input_mat[i, rlen:max_ragged_dim] = 0\r\n\r\nwith tf.Session() as sess:\r\n    feed_dict = {input_ragged_mat: ragged_input_mat,\r\n                 input_ragged_lengths: ragged_input_length_mat}\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    print(\"Calculating Final Loss\")\r\n    sess.run([final_loss],\r\n             feed_dict=feed_dict)\r\n    print(\"Running Train operation\")\r\n    sess.run([train_op],\r\n             feed_dict=feed_dict)\r\n```", "comments": ["I have tried with tensorflow version 1.10.0 as well, the same error occurs.", "Added PR #22157 for int64 support on gpu."]}, {"number": 22053, "title": "Build Tensorflow on RHEL 7 with or without GPU (CUDA) support", "body": "Was searching high and wide for a guide that included all the steps to install TF on Rhel. Posting the solution that worked for me. Also includes CUDA install steps.\r\n\r\nMachine Specs:\r\nGCP Rhel 7 OS\r\nTested P-100 and V-100 GPU\r\nTensorflow 1.10.0\r\nCUDA 9.2\r\ncuDNN 7.2", "comments": ["First Install actions for CUDA 9.2, cuDNN 7.2\r\nMostly followed from: http://kehang.github.io/tools/2017/03/31/install-CUDA-cuDNN-on-Red-Hat/\r\n\r\n\r\nNeed wget to download drivers and git, etc.\r\n`sudo yum -y install wget git nano python-devel patch numpy python-pip`\r\n\r\nPre-install actions\r\n```\r\nsudo yum -y install pciutils\r\nsudo yum -y install gcc\r\nsudo yum -y install kernel-devel-$(uname -r) kernel-headers-$(uname -r)\r\n```\r\n\r\nPython packages\r\n`sudo pip install enum34 keras`\r\n\r\nCUDA 9.2\r\n\r\nGet the drivers and install\r\n`wget https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-rhel7-9-2-local-9.2.148-1.x86_64`\r\n\r\n\r\n\r\nRename to .rpm\r\n`mv cuda-repo-rhel7-9-2-local-9.2.148-1.x86_64 cuda-repo-rhel7-9-2-local-9.2.148-1.x86_64.rpm`\r\n\r\n```\r\nsudo rpm -i cuda-repo-rhel7-9-2-local-9.2.148-1.x86_64.rpm\r\nsudo yum clean all\r\nsudo yum -y install cuda\r\n```\r\n\r\n\r\n\r\nInstall cuDNN\r\nNeed an Nvidia developer account to download from Nvidia website\r\nUntar\r\n`tar -zxf cudnn-9.2-linux-x64-v7.2.1.38.tgz`\r\n\r\n```\r\ncd cuda\r\n\r\nsudo cp -av lib64/* /usr/local/cuda/lib64/\r\nsudo cp -av include/* /usr/local/cuda/include/\r\n```\r\n\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64/:$LD_LIBRARY_PATH\r\nexport LIBRARY_PATH=/usr/local/cuda/lib64/:$LIBRARY_PATH\r\n```\r\n\r\nInstall bazel, followed from: https://gist.github.com/gentaiscool/a628fab5cd98953af7f46b69463394b3\r\n\r\nAdd repo\r\n```\r\ncd /etc/yum.repos.d/\r\nsudo wget https://copr.fedorainfracloud.org/coprs/vbatts/bazel/repo/epel-7/vbatts-bazel-epel-7.repo\r\n```\r\n\r\n```\r\ncd\r\nsudo yum -y install bazel\r\n```\r\n\r\nInstall TF from source\r\n\r\nFirst, lets just install TF from python standard lib so we can get all the dependencies.\r\n`sudo pip install tensorflow-gpu`\r\n\r\nNow uninstall so we can build from source\r\n`sudo pip uninstall tensorflow-gpu`\r\n\r\nClone repo\r\n`git clone https://github.com/tensorflow/tensorflow`\r\n\r\nConfigure\r\n```\r\ncd tensorflow\r\n./configure\r\n```\r\nAnswer these for the prompts. May need to change. A couple of 'gotchas' listed below.\r\nPYTHON_INCLUDE_PATH - location of Python.h - https://github.com/tensorflow/tensorflow/issues/327\r\nFill in these answers to the prompts:\r\nOnly enter what is inside quotes\r\nTo accept a default press enter\r\n*1. Python Path: DO NOT ACCEPT DEFAULT, input: \"/usr/bin/python2.7\"\r\n2. Python Library Path: Default: \"/usr/lib/python2.7/site-packages\"\r\n3. jemalloc: no (n)\r\n4. GCS Support: Yes (y) - not needed right now but perhaps in future\r\n5. HDFS Support: No\r\n6. AWS Support: No\r\n7. Kafka Support: No\r\n8. XLA JIT Support: No\r\n9. GDR Support: No\r\n10. VERBS Support: No\r\n11. OpenCL SYCL Support: No\r\n*12. CUDA Support: Yes\r\n*13. CUDA Version: \"9.2\"\r\n14. CUDA Lib Location: Accept Default: \"/usr/local/cuda\"\r\n*15. cuDNN Version: \"7.2\"\r\n16. cuDNN Lib Path: Accept Default: \"usr/local/cuda\"\r\n17. TensorRT Support: No\r\n18. NCCL Version: \"1.3\"\r\n19. Accept Default\r\n20. clang CUDA compiler: No\r\n21. Default: \"/usr/bin/gcc\"\r\n22. Tensor MPI: No\r\n23. Optimization Flafs: Default\r\n24. Interactively configure WORKSPACE: No\r\n\r\nBuild with GPUs\r\n`bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nWithout GPUs\r\n`bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nBuild whl file\r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\nInstall - look in tmp/tensorflow_pkg/ directory to find <your whl file>\r\n`sudo pip install --upgrade /tmp/tensorflow_pkg/<your whl file>.whl`\r\n\r\nMine is\r\n`sudo pip install --upgrade /tmp/tensorflow_pkg/tensorflow-1.10.0-cp27-cp27mu-linux_x86_64.whl`\r\n\r\nMay need to upgrade protobuf, although I did not have to !!! - https://gist.github.com/jorgemf/d2f3d85fadeb6e9d88ab00a06fbca0a2\r\n\r\n\r\nLets check Tensorflow can use GPU. Copy python code at link into python session.\r\nhttps://www.tensorflow.org/guide/using_gpu\r\n\r\nCheck pytorch can see GPU\r\n```\r\npython2\r\n\r\n> import torch\r\n> torch.cuda.is_available()\r\n```\r\n\r\nThe above should return true\r\n\r\n#NOTES\r\n\r\n- Do not start python and import tensorflow from inside the tensorflow folder cloned from github - won't be able to find some modules - https://github.com/tensorflow/tensorflow/issues/374\r\n- Speed up build time for bazel by leveraging multiple CPU cores. Can use --jobs flag to specify num jobs - https://github.com/tensorflow/models/issues/195\r\n\r\n#Other Links:\r\nhttps://github.com/tensorflow/tensorflow/issues/327 - python.h missing help link\r\n", "Thanks @zacharymostowsky for the solution. I found it quite helpful while installing cuda/cudnn/TensorFlow on RHEL7. \r\n\r\nOne additional step that I had to do was to install third-party repository. \r\n\r\n`yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm`\r\n\r\nI followed this guide to get around a few errors\r\n\r\nhttps://developer.download.nvidia.com/compute/cuda/9.2/Prod2/docs/sidebar/CUDA_Installation_Guide_Linux.pdf \r\n\r\nAgain, thanks for your answer. "]}, {"number": 22052, "title": "Variables in bijectors cannot be reused.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.9.0 (tested in 1.10.0 too)\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: NVIDIA Tesla, 24GB\r\n- **Exact command to reproduce**: the code within the \"Source code / logs\"\r\n\r\n### Describe the problem\r\nI am trying to reuse the weights and biases in the neural network within the MaskedAutoregressiveFlow bijector, by placing it within a `tf.variable_scope` with `reuse=tf.AUTO_REUSE`. But found that the weights and biases are not reused in practice.\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.distributions.python.ops import bijectors as tfb\r\n\r\ndef get_bijector(name='my_bijector', reuse=None):\r\n  \"\"\"Returns a MAF bijector.\"\"\"\r\n  with tf.variable_scope(name, reuse=reuse):\r\n    shift_and_log_scale_fn = \\\r\n        tfb.masked_autoregressive_default_template([128])\r\n    return tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn)\r\n  \r\nx = tf.placeholder(shape=[None, 64], dtype='float32', name='x')\r\n\r\nbijector_0 = get_bijector(reuse=tf.AUTO_REUSE)\r\ny_0 = bijector_0.forward(x)\r\n\r\nbijector_1 = get_bijector(reuse=tf.AUTO_REUSE)\r\ny_1 = bijector_1.forward(x)\r\n\r\n# We were expecting that the `y_0` and `y_1` share the same dependent variables,\r\n# since we used `tf.AUTO_REUSE` within the `tf.variable_scope`. However, the following\r\n# will return a `False`.\r\nprint(get_dependent_variables(y_0) == get_dependent_variables(y_1))\r\n```\r\n\r\nwherein we have employed the function that gains all the variables a tensor depends on:\r\n\r\n```\r\nimport collections\r\n\r\ndef get_dependent_variables(tensor):\r\n  \"\"\"Returns all variables that the tensor `tensor` depends on.\r\n  \r\n  Forked from: https://stackoverflow.com/a/42861919/1218716\r\n  \r\n  Args:\r\n    tensor: Tensor.\r\n    \r\n  Returns:\r\n    List of variables.\r\n  \"\"\"  \r\n  # Initialize\r\n  starting_op = tensor.op\r\n  dependent_vars = []\r\n  queue = collections.deque()\r\n  queue.append(starting_op)\r\n  op_to_var = {var.op: var for var in tf.trainable_variables()}\r\n  visited = {starting_op}\r\n\r\n  while queue:\r\n    op = queue.popleft()\r\n    try:\r\n      dependent_vars.append(op_to_var[op])\r\n    except KeyError:\r\n      # `op` is not a variable, so search its inputs (if any). \r\n      for op_input in op.inputs:\r\n        if op_input.op not in visited:\r\n          queue.append(op_input.op)\r\n          visited.add(op_input.op)\r\n          \r\n  return dependent_vars\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@jvdillon : Could you triage?", "@shuiruge \r\nWe see that you are using old version of tensorflow 1.x which is not actively supported, We recommend that you upgrade to 2.4 or later version.Please have a look at the [migration](https://www.tensorflow.org/guide/migrate) guide for reference to Migrate from TensorFlow 1.x to TensorFlow 2.Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22052\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22052\">No</a>\n"]}, {"number": 22051, "title": "Integrating this framework into swift causing syntax errors", "body": "I am using this framework in a swift project and used a bridging header to implement some functionalities. I was included this framework with CocoaPods into my swift project. But while trying to build the project, there are so many syntax errors like #include not found, #include not found etc. I have been banging my head around this issue from past two days but no luck. The Xcode compiler is able to build if i am using an Objective C Project. Somebody please guide me to resolve the issue with swift project\r\n\r\nThanks in Advance", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "- i haven't written any custom code. I have created swift project and integrated tensor flow framework using CocoaPods. i have imported the objective C classes as well which you provided in examples section in GitHub project. After that i created one bridging header to imported the camera class into that.Then i tried to build the project and getting errors in header files of tensorflow framework.\r\n- iOS platform, \r\n- iPhone device\r\n\r\n", "Removing the declared c++ variables on header file has done the trick. "]}, {"number": 22050, "title": "Can't find out how put smartreply_jni.c file in android", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 22049, "title": "After building TensorFlow from source, seeing ImportError: __longjmp_chk: symbol not found error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Alpine\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**:  From binary https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.1-cp36-cp36m-linux_x86_64.whl\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: No\r\n- **GCC/Compiler version (if compiling from source)**: glibc 2.28\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**: import tensorflow\r\n\r\n### Describe the problem\r\nDocker image with Alpine Linux is built successfully. But the ImportError occured while running `import tensorflow`.\r\n\r\n### Source code / logs\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: Error relocating /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: __longjmp_chk: symbol not found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: Error relocating /usr/local/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: __longjmp_chk: symbol not found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n", "comments": ["Hi @tatatodd ,\r\n\r\nI even installed tensorflow inside alpine using `pip install .whl` and getting the similar error while importing. please find below error.\r\n\r\n```\r\n/app # python \r\nPython 3.5.6 (default, Sep 12 2018, 02:24:47) \r\n[GCC 6.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: Error loading shared library /usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: Exec format error\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/local/lib/python3.5/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/local/lib/python3.5/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: Error loading shared library /usr/local/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: Exec format error\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>>```", "@mvoitko is this still an issue? Could you try installing TF 1.12 (stable) or latest version and check whether the issue persists? If it was solved by newer version, please close the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 22048, "title": "[Bug] Clip by norm NaN gradients ", "body": "```\r\na = tf.zeros([3], dtype=tf.float32)\r\nb = tf.clip_by_norm(a, 1.)\r\nc = tf.gradients(b,a)\r\ns = tf.Session()\r\ns.run(c)\r\n[array([nan, nan, nan], dtype=float32)]\r\n```\r\n\r\nThe gradient should obviously be [1,1,1] for all vectors a of norm smaller than 1, since this function should be the identity for those vectors.\r\n\r\nHave I written custom code:\r\nOS Platform and Distribution: Ubuntu 14.10\r\nTensorFlow installed from: pip3\r\nTensorFlow version: 1.10.1\r\nBazel version:\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nExact command to reproduce: see above\r\nMobile device:", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "According to its [API](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm)\r\n\r\n> t * clip_norm / l2norm(t)\r\n\r\nThe gradient seems `t / t * clip_norm`, right ? I think the result is undefined when t == 0.\r\n\r\n@alextp Could you please comment or reassign?", "No, this rescaling is only valid \"If the L2-norm is greater than clip_norm\" (see API), otherwise t should remain unchanged, so gradient should be all ones.\r\n\"if the L2-norm of t is already less than or equal to clip_norm, then t is not modified.\"", "I think you're right.", "This is very similar with https://github.com/tensorflow/tensorflow/issues/20091 and it's essentially caused by the fact that the implementation of clip_by_norm requires computing the gradient w.r.t. tf.norm(t) even when |t| < clip_norm. I am not sure how this can be solved since TF uses static graphs. Would have been trivial in a framework that uses dynamic graphs ...", "```python\r\nIn [20]: a = tf.constant(1.0)\r\nIn [21]: b = tf.constant(2.0)\r\nIn [22]: c = tf.maximum(a, b)\r\nIn [23]: sess.run(tf.gradients(c, [a, b]))\r\nOut[23]: [0.0, 1.0]\r\n```\r\n\r\nBecause tf.maximum cannot block the gradient propagation for the smaller value (note that gradient w..rt. a is 0.0, rather than None), I have no idea of how to solve the problem. Let's wait for reply from @alextp .", "Yes, and similar:\r\n```\r\na = tf.zeros([3], dtype=tf.float32)\r\nb = tf.maximum(5., tf.norm(a))\r\nc = tf.gradients(b, [a])\r\ns = tf.Session()\r\ns.run(c)\r\n```\r\nGives\r\n`\r\n[array([nan, nan, nan], dtype=float32)]\r\n`", "I am not sure this solves the issue. The code still fails for very small vectors, e.g.:\r\n`a = tf.ones([3], dtype=tf.float32) * 1e-20`", "The general solution of this issue is somewhat harder.\n\nOn Sun, Sep 9, 2018 at 9:01 AM Octavian Ganea <notifications@github.com>\nwrote:\n\n> I am not sure this solves the issue. The code still fails for very small\n> vectors, e.g.:\n> a = tf.ones([3], dtype=tf.float32) * 1e-20\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22048#issuecomment-419725649>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxWxZntdSSmF3s1omGN4LHAYWZx2Fks5uZTt2gaJpZM4WYiLF>\n> .\n>\n\n\n-- \n - Alex\n", "Yes, I think it's called \"dynamic graphs\" :)", "Not quite, as you will find that you can reproduce this issue even with\neager execution enabled. Dynamic graphs would help if we only needed to\nsupport the case where the l2norm is a scalar (that is when there is no\naxis argument).\n\nThe underlying issue is that an op like tf.maximum, which we use here to\npick which coordinates of the tensor need to be divided by the norm,\nproduces a gradient of 0 wrt the inputs it did not use to compute the\noutput. At the same time, an op like tf.sqrt() produces a gradient of\nupstream_gradient * 1/2sqrt(input). If 1/2sqrt(input) is inf or NaN,\nmultiplying it by 0 (which is the upstream gradient for the coordinates\nwhich were not used) will result in a NaN, which is what you're seeing here.\n\nWe are looking into fixing this overall issue but it's tricky to do so\nwithout slowing down all operations whose gradients boil down up\nupstream_gradient * f(x) when f(x) can be inf or NaN.\n\nOn Thu, Sep 13, 2018 at 2:42 PM Octavian Ganea <notifications@github.com>\nwrote:\n\n> Yes, I think it's called \"dynamic graphs\" :)\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22048#issuecomment-421161926>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbP6PBJ4r1skKHmNNETdqcrB2jP9ks5uatE8gaJpZM4WYiLF>\n> .\n>\n\n\n-- \n - Alex\n", "I think the root is that tensorflow, in graph mode, cannot block the gradient backpropagation of a slice of Tensor, say tf.maximum op here. NaN propagation is really annoying, and two \"where\" trick hurts performance. Is there any solution to solve the problem totally? ", "It is especially annoying since it is usually very hard to debug and doesn't have a \"mathematical\" cause. It would be nice to have a tool that would help TF users to understand when this issue is generating \r\n their \"NaN problem\".", "The tool I used to debug this and partially fix, at least for the zeros\ncase, tf.add_check_numerics_ops, works pretty well for identifying these\nissues.\n\nNote that you can get around this using a tf.cond-based version (which\nbehaves the same as a dynamic graph) of clip_by_norm if you only care about\na scalar norm.\n\nOn Thu, Sep 13, 2018 at 4:04 PM Octavian Ganea <notifications@github.com>\nwrote:\n\n> It is especially annoying since it is usually very hard to debug and\n> doesn't have a \"mathematical\" cause. It would be nice to have a tool that\n> would help TF users to understand when this issue is generating\n> their \"NaN problem\".\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/22048#issuecomment-421178362>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZbDbQaFQeJNEMdEq7b5jIiEBttjks5uauSEgaJpZM4WYiLF>\n> .\n>\n\n\n-- \n - Alex\n", "Any updates or suggestions here? I am running into this and do not know how to solve it."]}]