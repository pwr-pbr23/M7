[{"number": 6175, "title": "What is All_op in tf.nn.dynamic_rnn?", "body": "Hi, I am doing an android demo for RNN. I built a two-layer GRU network for inference. I used the API tf.nn.dynamic_rnn. However, when using this API, the saved generated .pb file contains an op called \"All\". And after I moved my .pb file to assets directory and run my android app demo, it said the op \"All\" is not registered. I know I can register an op in /core/kernels/BUILD file. However, I didn't find such an op in BUILD file. What is it and how can I fix the problem?\r\nThank you very much!\r\n\r\nHere is the .pb file:\r\nnode {\r\n    name: \"Test/Model/RNN/RNN/All\"\r\n    op: \"All\"\r\n    input: \"Test/Model/RNN/RNN/Equal\"\r\n    input: \"Test/Model/RNN/RNN/Const\"\r\n    attr {\r\n       key: \"Tidx\"\r\n       value {\r\n          type: DT_INT32\r\n       }\r\n}\r\nattr {\r\nkey: \"keep_dims\"\r\n      value {\r\n      b: false\r\n     }\r\n   }\r\n }\r\n\r\n\r\n", "comments": ["The android app reports \"No OpKernel was registered to support Op 'All' with these attrs.\"", "\"All\" is one of the reduction ops, see [reduction_ops_all.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_all.cc). I'm not an expert on how to register those for the Android build, but LMK if you can't figure it out and I will look into it.", "Thank you so much. I will try to register it in the kernel BUILD file to see if it works.", "sorry I am also wondering if reduce_all and all are the same op?", "Yes, reduce_all is the python wrapper for the C++ All Op I believe.", "Thank you! Now I understand.", "I have registered the reduction_ops_all.cc under the android group in /tensorflow/core/kernels/BUILD file and it worked. Thanks for your comment and I will close this.", "Hi, @LiangKlausQiu , May I ask how to register `reduction_ops_all.cc` in `/tensorflow/core/kernels/BUILD`?\r\nI tried to append it to `srcs` under `\"android_all_ops\"`, but got a duplicate error.\r\nMany thanks!", "Thanks,I find the way to register.", "Great, @shijieKika . I remember I just appended it under 'android_extended_ops_group2'. "]}, {"number": 6174, "title": "DBPEDIA_URL is not a valid download url anymore", "body": "DBPEDIA_URL in [text_datasets.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/text_datasets.py) is not a valid url anymore because web hosting in Google Drive has been deprecated, https://gsuite-developers.googleblog.com/2015/08/deprecating-web-hosting-support-in.html\r\n\r\nProposed fix: \r\n- Move [dbpedia_csv.tar.gz](https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M) to cloud storage.\r\n\r\nWhat is affected:\r\n- Text classification examples that depend on DBpedia data like [text_classification.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification.py) are now broken.", "comments": []}, {"number": 6173, "title": "Fix bug in docs of summary.merge", "body": "From the code, the default collection of `summary.merge` should be `[]`.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6172, "title": "Make license available for when OpenCL support is enabled.", "body": "", "comments": ["This should fix #6158", "FYI: https://github.com/tensorflow/tensorflow/pull/6161 fixed the issue in a slightly different way.", "Closing as should be fixed in #6161"]}, {"number": 6171, "title": "tfprof: Python3 incompatibility", "body": "[This line in tfprof_logger.py](https://github.com/tensorflow/tensorflow/blob/20c3d37ecc9bef0e106002b9d01914efd548e66b/tensorflow/contrib/tfprof/python/tools/tfprof/tfprof_logger.py#L127) uses `dict.iteritems()`, which breaks my Python 3 code.\r\n", "comments": ["Seems like that should be `six.iteritems(dict)`\n\nOn Wed, Dec 7, 2016 at 12:46 PM, Denny Britz <notifications@github.com>\nwrote:\n\n> This line in tfprof_logger.py\n> <https://github.com/tensorflow/tensorflow/blob/20c3d37ecc9bef0e106002b9d01914efd548e66b/tensorflow/contrib/tfprof/python/tools/tfprof/tfprof_logger.py#L127>\n> uses dict.iteritems(), which breaks my Python 3 code.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6171>, or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHIRRWmSWEUEGsT3CtYxPjRpN-ZlWks5rFxsRgaJpZM4LHFgr>\n> .\n>\n", "Can I work on this? or anyone is working on this internally already?", "A change has been submitted. Should be available at next release.", "Fixed #6171", "@panyx0718  Is there more documentation for this tool.  The tool's output is hard to analysis for me.\r\n\r\n1. _TFProfRoot (0us/312.56ms), what is the meaning of 0us, and what is the meaning of 312.56ms.\r\n\r\n\r\n\r\n\r\n\r\n", "I will add more docs.\n\nx/y. x is the current ops stats. y is aggregated stats of itself and all\nits descendants (according to name scopes)\nIn your case, the number is the sum of execution times of all ops.\n\nOn Thu, Dec 8, 2016 at 7:30 PM, Lin JM <notifications@github.com> wrote:\n\n> @panyx0718 <https://github.com/panyx0718> Is there more documentation for\n> this tool. The tool's output is hard to analysis for me.\n>\n>    1. _TFProfRoot (0us/312.56ms), what is the meaning of 0us, and what is\n>    the meaning of 312.56ms.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6171#issuecomment-265924634>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ACwQexEaMq6kOdrEw7jNRSmt74oc6-bpks5rGMtogaJpZM4LHFgr>\n> .\n>\n\n\n\n-- \nThanks\nXin\n"]}, {"number": 6170, "title": "training process dies without warning", "body": "I have two GPUs. I start an LSTM training process on one GPU by masking CUDA_VISIBLE_DEVICES in the python script:\r\n\r\n```python\r\ngpu_id='0'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = gpu_id\r\n```\r\nI then start a second training process using the same technique, but with `gpu_id='1'`. Shortly after launching the second training process, the first one dies without any message. \r\n\r\nEach invocation looks something like this\r\n\r\n```bash\r\nnohup python train.py .... >& train.log &\r\n```\r\n\r\nWhen I do the same thing without redirecting to a file, I see simply `Killed`\r\n\r\nThis only happens when I try to run two training processes. I am able to run training and evaluation at the same time on different cards. \r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNothing looks similar. \r\n\r\n### Environment info\r\nOperating System: CentOS 7\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA = 8.0 cuDNN = 5.1\r\n\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): \r\nhttps://paste.debian.net/901121/\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n*  I tried setting CUDA_VISIBLE_DEVICES from the command line before starting the training script. e.g. `CUDA_VISIBLE_DEVICES=0`\r\n\r\n### Logs or other output that would be helpful\r\nDriver Version: 367.48\r\n\r\n", "comments": ["The process was killed by the kernel. The machine only has 64GB of memory. \r\n\r\nhttp://stackoverflow.com/questions/726690/who-killed-my-process-and-why\r\n"]}, {"number": 6169, "title": "Docker build: add support for python3 (#6030)", "body": "", "comments": []}, {"number": 6168, "title": "Placeholder mismatch", "body": "I am trying to train two classifiers in one session. I declared two separate placeholders and pass each of them to different classifiers in run-time. The first one works fine, but the second one does not. The error message tells me that the second classifier is trying to look for the placeholder for the first classifier (instead of its own).\r\n\r\nThe code is attached below\r\n```\r\nfrom __future__ import absolute_import, division, print_function\r\nimport os\r\nimport prettytensor as pt\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\ndataset_name = \"data/MNIST\"\r\nif not os.path.exists(dataset_name):\r\n    os.makedirs(dataset_name)\r\ndataset = input_data.read_data_sets(\r\n    dataset_name,\r\n    reshape=False,\r\n    one_hot=True\r\n)\r\n\r\n########################\r\n# Construct Classifier #\r\n########################\r\ndef construct_lenet(lenet_images, lenet_labels):\r\n    images = pt.wrap(lenet_images)\r\n    with pt.defaults_scope(activation_fn=tf.nn.relu, l2loss=0.00001):\r\n        return (\r\n            images\r\n            .conv2d(5, 20)\r\n            .max_pool(2, 2)\r\n            .conv2d(5, 50)\r\n            .max_pool(2, 2)\r\n            .flatten()\r\n            .fully_connected(500)\r\n            .softmax_classifier(10, lenet_labels)\r\n        )\r\n\r\n####################\r\n# First Classifier #\r\n####################\r\nclassifier_images_placeholder_1 = tf.placeholder(\r\n    tf.float32,\r\n    [\r\n        50,\r\n        28,\r\n        28,\r\n        1\r\n    ]\r\n)\r\nclassifier_labels_placeholder_1 = tf.placeholder(\r\n    tf.float32,\r\n    [\r\n        50,\r\n        10\r\n    ]\r\n)\r\n\r\nclassifier_result_1 = construct_lenet(\r\n    classifier_images_placeholder_1,\r\n    classifier_labels_placeholder_1\r\n)\r\nclassifier_optimizer_1 = tf.train.AdamOptimizer(0.01)\r\nclassifier_train_op_1 = pt.apply_optimizer(\r\n    classifier_optimizer_1,\r\n    losses=[classifier_result_1.loss]\r\n)\r\n\r\n#####################\r\n# Second Classifier #\r\n#####################\r\nclassifier_images_placeholder_2 = tf.placeholder(\r\n    tf.float32,\r\n    [\r\n        50,\r\n        28,\r\n        28,\r\n        1\r\n    ]\r\n)\r\nclassifier_labels_placeholder_2 = tf.placeholder(\r\n    tf.float32,\r\n    [\r\n        50,\r\n        10\r\n    ]\r\n)\r\n\r\nclassifier_result_2 = construct_lenet(\r\n    classifier_images_placeholder_2,\r\n    classifier_labels_placeholder_2\r\n)\r\nclassifier_optimizer_2 = tf.train.AdamOptimizer(0.01)\r\nclassifier_train_op_2 = pt.apply_optimizer(\r\n    classifier_optimizer_2,\r\n    losses=[classifier_result_2.loss]\r\n)\r\n\r\n\r\n\r\ninit = tf.initialize_all_variables()\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n##########################\r\n# Train First Classifier #\r\n##########################\r\n    classifier_training_loss = 0.0\r\n    for i in range(10):\r\n        training_images, training_labels = dataset.train.next_batch(\r\n            50\r\n        )\r\n        _, classifier_runtime_loss = sess.run(\r\n            fetches=[\r\n                classifier_train_op_1,\r\n                classifier_result_1.loss\r\n            ],\r\n            feed_dict={\r\n                classifier_images_placeholder_1: training_images,\r\n                classifier_labels_placeholder_1: training_labels\r\n            }\r\n        )\r\n        classifier_training_loss += classifier_runtime_loss\r\n    classifier_training_loss /= dataset.train.num_examples\r\n    print(classifier_training_loss)\r\n###########################\r\n# Train Second Classifier #\r\n###########################\r\n    classifier_training_loss = 0.0\r\n    for i in range(10):\r\n        training_images, training_labels = dataset.train.next_batch(\r\n            50\r\n        )\r\n        _, classifier_runtime_loss = sess.run(\r\n            fetches=[\r\n                classifier_train_op_2,\r\n                classifier_result_2.loss\r\n            ],\r\n            feed_dict={\r\n                classifier_images_placeholder_2: training_images,\r\n                classifier_labels_placeholder_2: training_labels\r\n            }\r\n        )\r\n        classifier_training_loss += classifier_runtime_loss\r\n    classifier_training_loss /= dataset.train.num_examples\r\n    print(classifier_training_loss)\r\n```\r\n\r\nError message is attached below\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:968] Invalid argument: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [50,28,28,1]\r\n    [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[50,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\nTraceback (most recent call last):\r\n  File \"stack_overflow.py\", line 134, in <module>\r\n    classifier_labels_placeholder_2: training_labels\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [50,28,28,1]\r\n     [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[50,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'Placeholder', defined at:\r\n  File \"stack_overflow.py\", line 44, in <module>\r\n    1\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1332, in placeholder\r\n    name=name)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1748, in _placeholder\r\n    name=name)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Users/user/Packages/anaconda/envs/tensorflow2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [50,28,28,1]\r\n     [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[50,28,28,1], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```", "comments": ["I think it's a bug in your code rather than in TensorFlow. \r\n(when you call `AdamOptimizer` the second time, it creates an op which optimizes variables of both classifiers, so both sets of placeholders need to be fed)", "Sorry. I didn't notice that.\r\n\r\nI set a variable scope for two classifiers and minimize the loss with regards to the variables in the scope. That works well. Thanks!\r\n", "I am trying a simple linear regression using tensorFlow. The first time when I run the program (after kernel starts), it works fine. But when I run it again it gives similar error."]}, {"number": 6167, "title": "Adding solution for pip error with Anaconda", "body": "Issue #6136.\r\nAs this is not a Windows specific problem I added the instructions on \"PIp Installation Issues\".", "comments": ["Can one of the admins verify this patch?", "@Carmezim thanks for adding this!", "@yifeif glad to do it!"]}, {"number": 6166, "title": "Build \"general\" shared library for Android to use with Android Make ", "body": "Luckily there is a very good introduction into programming with Tensorflow on an Android device provided here:\r\nhttps://github.com/miyosuda/TensorFlowAndroidDemo\r\n\r\nThe version of TF used in this project is 0.10 which doesn't work with my trained model coming from TF 0.12. I was wondering which is the best way to have a \"generalized\" shared library (or however this is called), built with bazel that I could use interchangeably like the one provided in the project mentioned above. \r\nI was able to build the TF-Android Example using Bazel. My Anrdoid-project, which uses Tensorflow, also needs OpenCV to work. Therefore I'm building my App with a make-file which worked quiet well so far. \r\n\r\nMaybe I'm completely wrong, as I'm not really a programmer - so far. Thank you for any advice. \r\n\r\n", "comments": ["I think I found the answer already. Should have had stumbled over it a bit earlier:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile ", "Several generalized targets exist currently (by generalized I basically mean not tensorflow/examples/android-specific). It just depends what your requirements are and the tools you want to use.\r\n\r\nYou can build the TF library suitable for dropping into your app with the Android inference JNI bindings defined in tensorflow/contrib/android/BUILD using Bazel as so:\r\n```\r\nbazel build //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n```\r\n\r\nThe Java src counterpart is found under tensorflow/contrib/android/java. See [TensorFlowImageClassifier.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowImageClassifier.java) for example using it.\r\n\r\nOtherwise, if you want to link TF into the rest of your native code, try tensorflow/core:android_tensorflow_lib (without inference bindings), or tensorflow/contrib/android: android_tensorflow_inference_jni (with inference bindings), which should give you a .a out.\r\n\r\nAdditionally, we have a makefile-based build equivalent to android_tensorflow_inference_jni under tensorflow/contrib/makefile, and a gradle-wrapper using cmake/make that eliminates the need for bazel under tensorflow/android/cmake.\r\n\r\nedit: corrected libtensorflow_inference.so target name", "A great! Will look into it! Thank you very much for this detailed answer. ", "May I still ask question regarding this topic eventhough it's closed? \r\nFor what ever reason, I can not build the solution above. It gives the following error:\r\n```\r\nUser$ bazel build //tensorflow/contrib/android:libtensorflow.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\nERROR: no such target '//tensorflow/contrib/android:libtensorflow.so': target 'libtensorflow.so' not declared in package 'tensorflow/contrib/android' defined by /Users/Bene/Tensorflow/tensorflow/contrib/android/BUILD.\r\nINFO: Elapsed time: 2,199s\r\n```\r\n\r\nHowever building the Android App using\r\n\r\n`bazel build //tensorflow/examples/android:tensorflow_demo`\r\n\r\nworked out quiet well I guess.", "It looks as if the target name has changed to //tensorflow/contrib/android:libtensorflow_inference.so @andrewharp can you confim (and update the comment above the target if so)?", "@michaelisard @beniroquai Correct, the comment didn't get updated when the target did. Thanks for catching that, fix sent.", "Thank you very much for your reply. I tested the code above and got the following error (Mac and Linux UBuntu 16.04 - all dependencies should have been met):\r\n\r\n```\r\n\r\n> INFO: Found 1 target...\r\n> ERROR: /home/useradmin/tensorflow/tensorflow/contrib/android/BUILD:71:1: Linking of rule '//tensorflow/contrib/android:libtensorflow_inference.so' failed: link_dynamic_library.sh failed: error executing command \r\n>   (cd /home/useradmin/.cache/bazel/_bazel_useradmin/b73e43c9f90fc31eeb7e686c21abd9f4/execroot/tensorflow && \\\r\n>   exec env - \\\r\n\r\n[.....]\r\n\r\n\r\n> bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo(quantized_activation_ops.o):quantized_activation_ops.cc:function Eigen::QInt32 tensorflow::FloatToQuantized<Eigen::QInt32>(float, float, float): error: undefined reference to 'round'\r\n> bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo(simple_graph_execution_state.o):simple_graph_execution_state.cc:function tensorflow::SimpleGraphExecutionState::Extend(tensorflow::GraphDef const&, std::unique_ptr<tensorflow::SimpleGraphExecutionState, std::default_delete<tensorflow::SimpleGraphExecutionState> >*) const: error: undefined reference to 'ceil'\r\n> bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo(unique_tensor_references.o):unique_tensor_references.cc:function tensorflow::UniqueTensorReferences::Add(tensorflow::Tensor const&): error: undefined reference to 'ceil'\r\n> bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/libandroid_tensorflow_lib_lite.lo(logging.o):logging.cc:function tensorflow::internal::LogMessage::GenerateLogMessage(): error: undefined reference to '__android_log_write'\r\n> collect2: error: ld returned 1 exit status\r\n> Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build\r\n> Use --verbose_failures to see the command lines of failed build steps.\r\n> INFO: Elapsed time: 97.991s, Critical Path: 85.88s\r\n> \r\n```\r\nThis is just a tiny snipped of the whole Warning/Error dialog. The Demo-App was built sucessully it says, but \r\n\r\n```\r\nbazel build //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n\r\n```\r\ndidn't work. I'm using TF 0.12.", "Ack, looks like `-llog` and `-lm` are missing from the linkopts. Add them and it should work; I'll send a fix too.\r\n\r\nThis time I've manually run the command myself and everything seems in order :)", "Thanks for the quick fix :) \r\nActually my programming skills are limited and I don't exactly know, what you mean by \" add the linkopts. Could you give a quick hint or a page which describes that? I've seen something like that in a `makefile` as well I guess.. Do I need to enter it in the `WORKSPACE` file somewhere?\r\nSorry for that ;) ", "You just need to find the definition of libtensorflow_inference.so in [tensorflow/contrib/android/BUILD](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/BUILD) and make it look like this:\r\n\r\n```\r\ncc_binary(\r\n    name = \"libtensorflow_inference.so\",\r\n    srcs = [],\r\n    copts = tf_copts(),\r\n    linkopts = [\r\n        \"-landroid\",\r\n        \"-llog\",\r\n        \"-lm\",\r\n        \"-z defs\",\r\n        \"-s\",\r\n        \"-Wl,--version-script\",  # This line must be directly followed by LINKER_SCRIPT.\r\n        LINKER_SCRIPT,\r\n    ],\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    tags = [\r\n        \"manual\",\r\n        \"notap\",\r\n    ],\r\n    deps = [\r\n        \":android_tensorflow_inference_jni\",\r\n        \"//tensorflow/core:android_tensorflow_lib\",\r\n        LINKER_SCRIPT,\r\n    ],\r\n)\r\n```", "Tadah! It worked! :) Let's see if I can proceed from now ^^ \r\n\r\nBut maybe - as stated somewhere here - a short tutorial in the Readme would be super great. Like \"how to integrate tensorflow libraries/binaries into a android studio project\" or something. ", "Not sure if this is useful, but am reporting for your info.\r\nI used the following command (as listed above):\r\n\r\n```\r\nbazel build //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n```\r\n\r\nand got the following error message (Running on MacOS with TF0.12 (Commit#412428)):\r\n\r\n```\r\nKevins-MacBook-Air-2:tensorflow kevin$ bazel build //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n>    --crosstool_top=//external:android/crosstool \\\r\n>    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n>    --cpu=armeabi-v7a\r\n..........................................................\r\nWARNING: Bazel Android NDK crosstools are based on Android NDK revision 12. The revision of the Android NDK given in android_ndk_repository rule 'androidndk' is '13.1.3345770'.\r\nINFO: Loading package: @local_config_cuda//cuda\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:avgpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:bounds_check.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:cwise_ops_gradients.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_activations.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_attention.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_cuboid_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_backward_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_cuboid_convolution.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_patch_3d.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_pooling.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_softmax.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:eigen_spatial_convolutions.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:maxpooling_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:ops_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:padding_fifo_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:pooling_ops_common.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_base.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:queue_op.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/kernels:typed_queue.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_entry.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_scorer.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_beam_search.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_decoder.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/ctc:ctc_loss_util.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:naming.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.cc' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nWARNING: /Users/kevin/tensorflow/tensorflow/core/BUILD:779:12: in srcs attribute of cc_library rule //tensorflow/core:android_tensorflow_lib_lite: please do not import '//tensorflow/core/util/tensor_bundle:tensor_bundle.h' directly. You should either move the file to this package or depend on an appropriate rule there.\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf/src/google/protobuf/util/time_util.cc [for host]:\r\nexternal/protobuf/src/google/protobuf/util/time_util.cc:52:18: warning: unused variable 'kMicrosPerMillisecond' [-Wunused-const-variable]\r\nstatic const int kMicrosPerMillisecond = 1000;\r\n                 ^\r\nexternal/protobuf/src/google/protobuf/util/time_util.cc:56:19: warning: unused variable 'kTimestampFormat' [-Wunused-const-variable]\r\nstatic const char kTimestampFormat[] = \"%E4Y-%m-%dT%H:%M:%S\";\r\n                  ^\r\n2 warnings generated.\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.cc:\r\nbazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/genfiles/external/protobuf/src: warning: directory does not exist.\r\nINFO: From Compiling tensorflow/core/kernels/split_v_op.cc:\r\ntensorflow/core/kernels/split_v_op.cc: In instantiation of 'void tensorflow::SplitVOpCPU<T, Tlen>::Compute(tensorflow::OpKernelContext*) [with T = float; Tlen = long long int]':\r\ntensorflow/core/kernels/split_v_op.cc:400:1:   required from here\r\ntensorflow/core/kernels/split_v_op.cc:212:63: warning: narrowing conversion of 'split_sizes_vec.std::vector<_Tp, _Alloc>::operator[]<long long int, std::allocator<long long int> >(((std::vector<long long int, std::allocator<long long int> >::size_type)i))' from '__gnu_cxx::__alloc_traits<std::allocator<long long int> >::value_type {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n           prefix_dim_size, split_sizes_vec[i], suffix_dim_size};\r\n                                                               ^\r\ntensorflow/core/kernels/split_v_op.cc: In instantiation of 'void tensorflow::SplitVOpCPU<T, Tlen>::Compute(tensorflow::OpKernelContext*) [with T = int; Tlen = long long int]':\r\ntensorflow/core/kernels/split_v_op.cc:400:1:   required from here\r\ntensorflow/core/kernels/split_v_op.cc:212:63: warning: narrowing conversion of 'split_sizes_vec.std::vector<_Tp, _Alloc>::operator[]<long long int, std::allocator<long long int> >(((std::vector<long long int, std::allocator<long long int> >::size_type)i))' from '__gnu_cxx::__alloc_traits<std::allocator<long long int> >::value_type {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\nINFO: From Compiling tensorflow/core/kernels/split_op.cc:\r\ntensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = Eigen::QUInt8]':\r\ntensorflow/core/kernels/split_op.cc:272:1:   required from here\r\ntensorflow/core/kernels/split_op.cc:156:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n         prefix_dim_size, split_dim_output_size, suffix_dim_size};\r\n                                                                ^\r\ntensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = float]':\r\ntensorflow/core/kernels/split_op.cc:272:1:   required from here\r\ntensorflow/core/kernels/split_op.cc:156:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\ntensorflow/core/kernels/split_op.cc: In instantiation of 'void tensorflow::SplitOpCPU<T>::Compute(tensorflow::OpKernelContext*) [with T = int]':\r\ntensorflow/core/kernels/split_op.cc:272:1:   required from here\r\ntensorflow/core/kernels/split_op.cc:156:64: warning: narrowing conversion of '(tensorflow::int64)split_dim_output_size' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\nINFO: From Compiling tensorflow/core/kernels/tensor_array_ops.cc:\r\ntensorflow/core/kernels/tensor_array_ops.cc: In instantiation of 'void tensorflow::TensorArraySplitOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\r\ntensorflow/core/kernels/tensor_array_ops.cc:1268:1:   required from here\r\ntensorflow/core/kernels/tensor_array_ops.cc:1131:72: warning: narrowing conversion of 'previous_length' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n       Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, previous_length, 0};\r\n                                                                        ^\r\ntensorflow/core/kernels/tensor_array_ops.cc:1133:65: warning: narrowing conversion of '(Scalar)tensor_lengths_t.Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::operator()<Eigen::Tensor<const long long int, 1, 1, int>, 16, Eigen::MakePointer>(i)' from 'Scalar {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n                                                 elements_per_row};\r\n                                                                 ^\r\ntensorflow/core/kernels/tensor_array_ops.cc: In instantiation of 'void tensorflow::TensorArraySplitOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = int]':\r\ntensorflow/core/kernels/tensor_array_ops.cc:1268:1:   required from here\r\ntensorflow/core/kernels/tensor_array_ops.cc:1131:72: warning: narrowing conversion of 'previous_length' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n       Eigen::DSizes<Eigen::DenseIndex, 3> indices{0, previous_length, 0};\r\n                                                                        ^\r\ntensorflow/core/kernels/tensor_array_ops.cc:1133:65: warning: narrowing conversion of '(Scalar)tensor_lengths_t.Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::operator()<Eigen::Tensor<const long long int, 1, 1, int>, 16, Eigen::MakePointer>(i)' from 'Scalar {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n                                                 elements_per_row};\r\n                                                                 ^\r\nINFO: From Compiling tensorflow/core/kernels/unpack_op.cc:\r\ntensorflow/core/kernels/unpack_op.cc: In instantiation of 'void tensorflow::UnpackOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = float]':\r\ntensorflow/core/kernels/unpack_op.cc:152:1:   required from here\r\ntensorflow/core/kernels/unpack_op.cc:109:75: warning: narrowing conversion of 'before_dim' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\n         Eigen::DSizes<Eigen::DenseIndex, 3> sizes{1, before_dim, after_dim};\r\n                                                                           ^\r\ntensorflow/core/kernels/unpack_op.cc: In instantiation of 'void tensorflow::UnpackOp<Device, T>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = int]':\r\ntensorflow/core/kernels/unpack_op.cc:152:1:   required from here\r\ntensorflow/core/kernels/unpack_op.cc:109:75: warning: narrowing conversion of 'before_dim' from 'tensorflow::int64 {aka long long int}' to 'int' inside { } [-Wnarrowing]\r\nTarget //tensorflow/contrib/android:libtensorflow_inference.so up-to-date:\r\n  bazel-bin/tensorflow/contrib/android/libtensorflow_inference.so\r\nINFO: Elapsed time: 2481.205s, Critical Path: 2358.75s\r\n\r\n```\r\nFurthermore, I can confirm that the BUILD file has the following section (with the required '-llog' an d '-lm' options): \r\n```\r\n# Build the native .so.\r\n# bazel build //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n#   --crosstool_top=//external:android/crosstool \\\r\n#   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n#   --cpu=armeabi-v7a\r\nLINKER_SCRIPT = \"//tensorflow/contrib/android:jni/version_script.lds\"\r\n\r\ncc_binary(\r\n    name = \"libtensorflow_inference.so\",\r\n    srcs = [],\r\n    copts = tf_copts(),\r\n    linkopts = if_android([\r\n        \"-landroid\",\r\n        \"-llog\",\r\n        \"-lm\",\r\n        \"-z defs\",\r\n        \"-s\",\r\n        \"-Wl,--version-script\",  # This line must be directly followed by LINKER_SCRIPT.\r\n        LINKER_SCRIPT,\r\n    ]),\r\n    linkshared = 1,\r\n    linkstatic = 1,\r\n    tags = [\r\n        \"manual\",\r\n        \"notap\",\r\n    ],\r\n    deps = [\r\n        \":android_tensorflow_inference_jni\",\r\n        \"//tensorflow/core:android_tensorflow_lib\",\r\n        LINKER_SCRIPT,\r\n    ],\r\n)\r\n```\r\nWhen I finally found the libtensorflow_inference.so file, it was only 177 bytes long.  Not sure it completed.", "@kevinashaw Can you try `bazel clean`, and then build again with Android NDK 12b (found [here](https://developer.android.com/ndk/downloads/older_releases.html#ndk-12b-downloads))?", "Closing; as I think the original issue is resolved now.\r\n\r\n@kevinashaw I was unable to reproduce your issue via the following steps:\r\n```\r\ngit clone --recurse-submodules https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngedit WORKSPACE # update ndk and sdk entries\r\nbazel build //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n```\r\nwhen using the 12b NDK. My guess would be that something was incorrect in your WORKSPACE file."]}, {"number": 6165, "title": "accelerate crf_log_norm", "body": "the base `crf_log_norm` function looks very cool, but it runs very slow.\r\nI run the experiment on cpu\r\n```\r\nmodel name  : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz\r\n```\r\n\r\nI use the configure\r\n```python\r\nbatch_size=128\r\nmax_seq_len=200\r\n```\r\nit runs `0.5s` per batch\r\n\r\nwhen I optimize the realization way\r\nit  runs `0.25s` per batch", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I don't know why, when I sign the cla, it occurs.\r\nA server error occurred, please try your request again.\r\n* required fields\r\n", "I don't know why, I can't sign in the google cla. I think there is some mistake with the google cla server.", "Check the email on your git commits matches the email you signed the CLA with.", "I meet problem when sign in google cla.\r\nit always\r\n\r\nA server error occurred, please try your request again.\r\n\r\nand when i try again, the error again.\r\nI don't know who can fix it.", "are you behind some sort of proxy ? ", "I signed it", "yes, Because I am in China, I must use proxy to visit Google.\r\nI fix it by change another net envirenment", "I signed it!"]}, {"number": 6164, "title": "TFRecords: DataLossError (see above for traceback): corrupted record at XXX", "body": "I convert large data from csv to tfrecords using tf.python_io.TFRecordWriter in hadoop\r\nthere're some error happens:\r\n* BUG 1) if I use zlib or gzip when create tfrecord writer , I can convert the csv successfully. But when I  loop all files to read , it's  stuck at some lines without any error.  when use these tfrecords to train model ,  errors \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\"\r\n(I use writer.close() when write done. so I maybe it's  a bug )\r\n* BUG 2) When I create tfrecord write without any compress option, the convert process goes well, and when I use the same program loop all the converted files , It all good . BUT when I use these tfrecords to train model in tensorflow , it's report \"TFRecords: DataLossError (see above for traceback): corrupted record at XXX\" again\uff08some step after, not at the begining, so maybe some part of the tfrecords error\uff09 . \r\n\r\nmy question:\r\n* (if solve these bugs are diffcult ) how to skip the corruted records? \r\nOR\r\n* how to solve these errors ?\r\n\r\n### Environment info\r\nOperating System:\r\ncentos 6 + hadoop\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nmy convert code:\r\n```\r\n          writer = tf.python_io.TFRecordWriter(pb_path)\r\n          example = tf.train.Example(features=tf.train.Features(feature={\r\n              \"label\":\r\n                  tf.train.Feature(float_list=tf.train.FloatList(value=[label])),\r\n              \"ids\":\r\n                  tf.train.Feature(int64_list=tf.train.Int64List(value=ids)),\r\n              \"values\":\r\n                  tf.train.Feature(float_list=tf.train.FloatList(value=values))\r\n          }))\r\n\r\n          writer.write(example.SerializeToString())\r\n          writer.close()\r\n```\r\n\r\nmy debug code (for loop all files):\r\n```\r\n  for f in files:\r\n   for serialized_example in tf.python_io.tf_record_iterator(f):\r\n      example = tf.train.Example()\r\n      example.ParseFromString(serialized_example)\r\n\r\n      # Read data in specified format\r\n      label = example.features.feature[\"label\"].float_list.value\r\n      ids = example.features.feature[\"ids\"].int64_list.value\r\n      values = example.features.feature[\"values\"].float_list.value\r\n```\r\n\r\nmy train code\r\n```\r\n# Read TFRecords files for training\r\nfilename_queue = tf.train.string_input_producer(\r\n    tf.train.match_filenames_once(FLAGS.train),\r\n    num_epochs=epoch_number)\r\nserialized_example = read_and_decode(filename_queue)\r\nbatch_serialized_example = tf.train.shuffle_batch(\r\n    [serialized_example],\r\n    batch_size=batch_size,\r\n    num_threads=thread_number,\r\n    capacity=capacity,\r\n    min_after_dequeue=min_after_dequeue)\r\nfeatures = tf.parse_example(\r\n    batch_serialized_example,\r\n    features={\r\n        \"label\": tf.FixedLenFeature([], tf.float32),\r\n        \"ids\": tf.VarLenFeature(tf.int64),\r\n        \"values\": tf.VarLenFeature(tf.float32),\r\n    })\r\nbatch_labels = features[\"label\"]\r\nbatch_ids = features[\"ids\"]\r\nbatch_values = features[\"values\"]\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\ncoord stopped\r\nTraceback (most recent call last):\r\n  File \"deepcake.py\", line 327, in <module>\r\n    coord.join(threads)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 2863006\r\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\r\n\r\nCaused by op u'ReaderRead', defined at:\r\n  File \"deepcake.py\", line 99, in <module>\r\n    serialized_example = read_and_decode(filename_queue)\r\n  File \"deepcake.py\", line 92, in read_and_decode\r\n    _, serialized_example = reader.read(filename_queue)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 265, in read\r\n    return gen_io_ops._reader_read(self._reader_ref, queue_ref, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 213, in _reader_read\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nDataLossError (see above for traceback): corrupted record at 2863006\r\n         [[Node: ReaderRead = ReaderRead[_class=[\"loc:@TFRecordReader\", \"loc:@input_producer\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReader, input_producer)]]\r\n```", "comments": ["@michaelisard  how to skip the corrupted record ?", "@ericyue  Is pb_path an HDFS path?\r\nBy the way, you can use [MapReduce or Spark](https://github.com/tensorflow/ecosystem/tree/master/hadoop) to make the conversion directly.", "thanks, @llhe   The pb_path is a local-system path. I am converting the csv to tfrecord in hadoop-streaming reduce process, and \"dfs -put\" the pb_path file to hdfs , then I get the files downloaded to someplace. \r\nI'll try your link in temporary , also wish to solve this ploblem :) ", "\"corrupted record at XXX\" means crc checksum failed. Could you make the repro just using the python scripts without hadoop-streaming?", "thanks @llhe \r\n1)  I think the crc check faild just happened a few lines , if so , how to skip the error record? can i just drop it?  how ?\r\n2) \"using the python scripts without hadoop-streaming?\"   you means run the streaming job in a local machine (single node) , just in pipe like cat xxx.dat |python bin/map.py|python bin/reduce.py ?\r\nI have try this , every thing ok, no error .   but the same program running in hadoop failed, I thinks maybe the tfrecords not work well in hdfs ?", "@michaelisard  is there any progress on this?", "@itsmeolivia may be able to comment on whether there's any way to track down issues with hdfs.", "hello, any progress?  :(  @itsmeolivia ", "@jhseu, Is this still an issue?\r\nCould you comment and/or close this issue?", "I haven't been able to reproduce this issue, so I'm closing it out. If you feel this is still an issue, please test it out in TF 1.2 and upload the full scripts you're using to https://gist.github.com", "This just happened to me too and seems the problem is that TFRecord does not recognize that the file is compressed (i.e. GZIP). You need to _explicitly_ configure it using the options, i.e:\r\n\r\n```\r\noptions = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\r\n```\r\n", "I kind of experienced the same error, I used the feature extractor from youtube-8M from google and tried to execute inference.py from google's starter code, and the following message appeared\r\n```\r\npython inference.py --output_file='testpredictions5.csv' --input_data_pattern='/home/estathop/Documents/testmodelfiles/testvideos.csv'  --train_dir='/home/estathop/Documents/features/youtube-8m/MyMoeModel2' --top_k=50\r\n2018-06-08 11:57:52.644482: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-06-08 11:57:52.712548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-06-08 11:57:52.712942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7845\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.93GiB freeMemory: 6.86GiB\r\n2018-06-08 11:57:52.712955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-06-08 11:57:52.894855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-06-08 11:57:52.894885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 \r\n2018-06-08 11:57:52.894891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N \r\n2018-06-08 11:57:52.895076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6611 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nINFO:tensorflow:number of input files: 1\r\nINFO:tensorflow:loading meta-graph: /home/estathop/Documents/features/youtube-8m/MyMoeModel2/inference_model.meta\r\nINFO:tensorflow:restoring variables from /home/estathop/Documents/features/youtube-8m/MyMoeModel2/inference_model\r\nINFO:tensorflow:Restoring parameters from /home/estathop/Documents/features/youtube-8m/MyMoeModel2/inference_model\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.DataLossError'>, corrupted record at 0\r\n\t [[Node: input/ReaderReadUpToV2 = ReaderReadUpToV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input/TFRecordReaderV2, input/input_producer, input/ReaderReadUpToV2/num_records)]]\r\n\t [[Node: input/ParseExample/ParseExample/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_32_input/ParseExample/ParseExample\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n2018-06-08 11:57:53.285454: W tensorflow/core/framework/op_kernel.cc:1318] OP_REQUIRES failed at example_parsing_ops.cc:144 : Invalid argument: Name: <unknown>, Feature: id (data type: string) is required but could not be found.\r\nINFO:tensorflow:Done with inference. The output file was written to testpredictions5.csv\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 227, in <module>\r\n    app.run()\r\n  File \"/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"inference.py\", line 223, in main\r\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\r\n  File \"inference.py\", line 182, in inference\r\n    coord.join(threads)\r\n  File \"/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/home/estathop/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0\r\n\t [[Node: input/ReaderReadUpToV2 = ReaderReadUpToV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](input/TFRecordReaderV2, input/input_producer, input/ReaderReadUpToV2/num_records)]]\r\n\t [[Node: input/ParseExample/ParseExample/_29 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_32_input/ParseExample/ParseExample\", tensor_type=DT_INT64, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n``\r\n```"]}, {"number": 6163, "title": "Opening http://projector.tensorflow.org/ shows a white screen in Chrome 55", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have search for open GitHub issues with the keyword `WebGL`, but none showed up.\r\n\r\n### Issue\r\n\r\nToday I opened http://projector.tensorflow.org/ in my browser, but got a complete white screen. The attached logs show the errors that are show in the console of the browser. Opening in Firefox on the same machine does succesfully launch the application and I was able to interact with it. \r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n```\r\nGoogle Chrome\t55.0.2883.75 (Official Build) (64-bit)\r\nRevision\t451c239c3b0722dc867b0f75839b959f729b756a-refs/branch-heads/2883@{#698}\r\nOS\tLinux \r\nJavaScript\tV8 5.5.372.29\r\nFlash\t23.0.0.207\r\nUser Agent\tMozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.75 Safari/537.36\r\n```\r\n\r\n\r\n### Logs or other output that would be helpful\r\n```\r\n(index):7671 THREE.WebGLRenderer 77\r\n(index):7671 THREE.WebGLRenderer: Error creating WebGL context.\r\nTHREE.WebGLRenderer\r\n(index):7722 Uncaught TypeError: Cannot read property 'getExtension' of null\r\n    at THREE.WebGLExtensions.get ((index):7722)\r\n    at new THREE.WebGLRenderer ((index):7672)\r\n    at new Gp ((index):8253)\r\n    at new cq ((index):8320)\r\n    at HTMLElement.q.setupUIControls ((index):8430)\r\n    at HTMLElement.q.ready ((index):8413)\r\n    at HTMLElement._invokeBehavior ((index):5501)\r\n    at HTMLElement._doBehavior ((index):5501)\r\n    at HTMLElement._readySelf ((index):5516)\r\n    at HTMLElement._ready ((index):5516)\r\n(index):8430 Uncaught TypeError: Cannot read property 'resize' of undefined\r\n    at (index):8430\r\n```\r\n", "comments": ["Same problem here on Chrome 55 on Win 10. Page runs fine on Edge.\r\n`THREE.WebGLRenderer: Error creating WebGL context`", "Same here.  Version 54.0.2840.98 (64-bit) on macOS (Sierra).  (Works in Safari.)", "Hi,\r\n\r\nThe problem is due to WebGL being disabled. To enable WebGL in Chrome, go to chrome://settings and make sure \"enable hardware acceleration\" is checked. However, this does not guarantee that WebGL will work. Some users reported success after reinstalling/updating their graphics drivers.\r\n\r\nWith commit 2166d20cf2308b4f0e1a895ac8b8aacaf04dd15f we are displaying an informative message instead of a blank screen when WebGL is not working.\r\n\r\nThank you for reporting this!", "Confirmed I get a nice modal now, thanks a lot for the speedy fix and explanation!"]}, {"number": 6162, "title": "Import error with skflow", "body": "Hello,\r\n\r\nI get an error when I try to import skflow with the command \r\n`from tensorflow.contrib import skflow`\r\nThe error is \"ImportError: cannot import name 'skflow\". I have googled the error came across this similar issue: https://github.com/tensorflow/tensorflow/issues/1931. Apparently I have a lower version of tensorflow (I had 0.12) and need a newer version, such as 0.80.\r\n\r\nI had done the installation by looking at this website (for cpu based): https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows. \r\n\r\nProbably only 0.12 is released for windows. Is there a way to get a higher version for Windows, or some workaround?\r\n\r\nAs a solution I have also separately installed skflow, but some of the functions are not there, as I see them in this tutorial: http://learningtensorflow.com/lesson8/, since it is an older version probably.\r\n\r\nI am now trying to build tensorflow on my computer with cmake and MSBuild, but it also does not run smoothly. You can see the issue with that here: https://github.com/tensorflow/tensorflow/issues/6160\r\n\r\nI have the following specs:\r\n- Windows 7, service pack 1, 64 bit\r\n- Python 3.5\r\n- Tensorflow 0.12\r\n\r\nThanks a lot,\r\nKerem", "comments": ["TensorFlow 0.8 is not available for Windows - note that it is actually an **older** version than TensorFlow 0.12 (the version numbers are not decimal fractions, so 0.80 != 0.8).\r\n\r\nThe `tf.contrib.skflow` package has been removed, and that tutorial is out of date. Please contact the author and ask them to update it to the latest APIs."]}, {"number": 6161, "title": "Improved support for OpenCL", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->", "GPU failure is just a known flake.\r\nCPU issue I think I have seen before, but I would rerun to make sure.\r\n\r\nWindows issue looks like a legit issue, but maybe it is also broken at head?\r\n@andrewharp I remember this windows problem was seen in the push from google. Do we have a fix ready for it?\r\nWho is working on the fix?"]}, {"number": 6160, "title": "Windows MSBuild fails", "body": "Hello,\r\nI am trying to build tensorflow for cpu on my Windows computer using cmake and MS Visual Studio. I do the steps as described in the link below, I came until step 4, however at step 4 however MSBuild fails. Since I don't need the test program, I am trying to dirrectly build the .whl file by building the tf_python_copy_scripts_to_destination.vcxproj.\r\n\r\nI am using the instructions from the link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md\r\n\r\n I have a service pack 1 Windows 7, 64 bit.\r\n\r\nIn terms of Prerequisites:\r\n- CMake version 3.5 up to 3.6 (I have 3.6.3)\r\n- Git (I have 2.10.0.windows.1)\r\n- SWIG (I have swigwin 3.0.10)\r\n\r\nAdditional pre-requisites for Microsoft Windows:\r\n- Visual Studio 2015 (I have VS 2015 and MSBuild 14.0)\r\n- Python 3.5 (I have Python 3.5)\r\n- NumPy 1.11.0 or later (I have 1.11.2)\r\n\r\nSince there are multiple MSBuilds, I set the path string to the MSBuild 14.0: C:\\Program Files (x86)\\MSBuild\\14.0\\Bin\\amd64. Also there are two versions, one directly under \\Bin and one under \\Bin\\amd64. I choose the latter one. \r\n\r\n\r\nThe error message I get is:\r\n============================================\r\n\r\nDone Building Project \"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default targe\r\nts) -- FAILED.\r\n\r\n\r\nBuild FAILED.\r\n\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj\" (default target) (3) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj\" (default target) (4) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default target) (5) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\proto_text.vcxproj\" (default target) (6) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj\" (default target) (7) ->\r\n(CustomBuild target) ->\r\n  cl : Command line warning D9002: ignoring unknown option '-std=c11' [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grp\r\nc\\src\\grpc\\grpc_unsecure.vcxproj] [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n  cl : Command line warning D9002: ignoring unknown option '-std=c11' [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grp\r\nc\\src\\grpc\\grpc_unsecure.vcxproj] [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n\r\n\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow.vcxproj\" (default target) (3) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_cpu.vcxproj\" (default target) (4) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\tf_core_framework.vcxproj\" (default target) (5) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\proto_text.vcxproj\" (default target) (6) ->\r\n\"C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj\" (default target) (7) ->\r\n(CustomBuild target) ->\r\n  C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\transport\\chttp2_transport.c\r\n(2319): error C2440: 'initializing' : cannot convert from 'gpr_slice' to 'gpr_slice_refcount *' [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorfl\r\now\\contrib\\cmake\\build\\grpc\\src\\grpc\\grpc_unsecure.vcxproj] [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n  C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc\\src\\grpc\\src\\core\\ext\\transport\\chttp2\\transport\\chttp2_transport.c\r\n(2319): error C2440: 'initializing' : cannot convert from 'gpr_slice' to 'gpr_slice_refcount *' [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorfl\r\now\\contrib\\cmake\\build\\grpc\\src\\grpc\\grpc_unsecure.vcxproj] [C:\\Users\\ktezcan\\temp_tensorflowbuildfiles\\tensorflow\\tensorflow\\contrib\\cmake\\build\\grpc.vcxproj]\r\n\r\n2 Warning(s)\r\n2 Error(s)\r\n\r\nTime Elapsed 00:00:58.06\r\n\r\n=================================\r\n\r\nAny help is appreciated. Thanks,\r\nKerem\r\n\r\n\r\n", "comments": ["Do you need to install TensorFlow on Windows specifically through CMake for some reason or just want to install it?\r\n\r\nIf the latter you can install with pip:\r\n`pip install tensorflow`\r\nfor gpu:\r\n`pip install tensorflow-gpu`", "I was basically trying to use skflow as shown in an online tutorial, which was apparently out of date.\r\nSince I was getting an error with the code from the tutorial, after some googleing I thought I had an old version of tensorflow and needed to update the version. But apparently that is not the case and the tutorial was the old one! You can see this discussion in issue: https://github.com/tensorflow/tensorflow/issues/6162. \r\n\r\nHence I will continue to use the windows release for now and find updated tutorials. On the other hand, it still does not solve my problem of building the code from source, but that is a problem for the future...\r\n\r\nThanks"]}, {"number": 6159, "title": "Fix issues caused by the library update", "body": "Fixes were made to eliminate deprecated references in the file caused by library update. Now the file compiles as expected. ", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it! ", "@tensorflow-jenkins test this please", "The tutorial test on ptb_world_lm still errored out:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"ptb_word_lm.py\", line 369, in <module>\r\n    tf.app.run()\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"ptb_word_lm.py\", line 331, in main\r\n    m = PTBModel(is_training=True, config=config, input_=train_input)\r\n  File \"ptb_word_lm.py\", line 141, in __init__\r\n    (cell_output, state) = cell(inputs[:, time_step, :], state)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 767, in __call__\r\n    cur_inp, new_state = cell(cur_inp, cur_state)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 292, in __call__\r\n    concat = _linear([inputs, h], 4 * self._num_units, True, scope=scope)\r\n  File \"/workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 846, in _linear\r\n    raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\r\nValueError: linear is expecting 2D arguments: [TensorShape(None), TensorShape([Dimension(20), Dimension(2)])]\r\n```\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/2931/console\r\n\r\n", "@yesanton looks like the fix did not work.\r\nCould you take another look?", "@gunan yes, I will check again. \r\n", "#6200 fixes the ptb_word_lm issue. Closing this pr."]}, {"number": 6158, "title": "installation issues on Ubuntu 16.04 GPU version", "body": "Dear developers:\r\nI encountered one error with fine setting configuration\r\nAll the settings is right, and it said that \r\n\r\n                                           all external dependencies fetch successful \r\n                                           configuration finished \r\n\r\nall the thing right until I generate the pip_package\r\n\r\n(bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package), \r\n\r\nprocess like below:\r\n\r\n`yuze@nlp:~/tensorflow$ ./configure \r\n~/tensorflow ~/tensorflow\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] n\r\nNo Google Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] n\r\nNo Hadoop File System support will be enabled for TensorFlow\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n\r\nUsing python library path: /usr/local/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] y\r\nOpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] y\r\nCUDA support will be enabled for TensorFlow\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n[Default is: \"3.5,5.2\"]: \r\nPlease specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: \r\nPlease specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]: \r\nPlease specify the location where ComputeCpp 1.2 is installed. Refer to README.md for more details. [Default is /usr/local/computecpp]: \r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n......\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n\r\n\r\nyuze@nlp:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Found 1 target...\r\nERROR: missing input file '@local_config_sycl//sycl:LICENSE.text'.\r\nERROR: /home/yuze/.cache/bazel/_bazel_yuze/540cd99ea4fee1092020a833f635df2b/external/farmhash_archive/BUILD:13:1: C++ compilation of rule '@farmhash_archive//:farmhash' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\r\nERROR: /home/yuze/tensorflow/tensorflow/tools/pip_package/BUILD:105:1: //tensorflow/tools/pip_package:build_pip_package: missing input file '@local_config_sycl//sycl:LICENSE.text'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/yuze/tensorflow/tensorflow/tools/pip_package/BUILD:105:1 1 input file(s) do not exist.\r\nINFO: Elapsed time: 3.485s, Critical Path: 1.17s\r\n`\r\ncan you kindly tell me where is wrong?\r\n\r\nThank you!", "comments": ["Yifei, I think this is a new license? Maybe a missing dependency? ", "I see the issue for when OpenCL support is enabled, will send a PR.", "I think I fixed it in my last set of OpenCL enhancements (https://github.com/tensorflow/tensorflow/pull/6161)", "I see. Looks good. Let me close my PR.", "Thank you, all lovely guys, the installation is OK now, //smile \r\n\r\nBut  When I finished installation and test it, it occurs an error saying Device mapping : no known device\r\n\r\nit works out like below:\r\n\r\n`\r\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\r\n......\r\nINFO: All external dependencies fetched successfully.\r\nConfiguration finished\r\n\r\nyuze@nlp:~/tensorflow$ bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Found 1 target...\r\nINFO: From Compiling external/protobuf/python/google/protobuf/pyext/message_factory.cc:\r\n\r\n...................................................................................................................................\r\n\r\nINFO: From Compiling tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc:\r\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\r\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1120.235s, Critical Path: 852.79s\r\n\r\nyuze@nlp:~/tensorflow$ cd ../\r\nyuze@nlp:~$ python\r\nPython 2.7.12 (default, Nov 19 2016, 06:48:10) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> with tf.device('/cpu:0'):\r\n...   a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\n...   b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\n...   c = tf.matmul(a, b)\r\n... \r\n>>> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\nDevice mapping: no known devices.\r\nI tensorflow/core/common_runtime/direct_session.cc:255] Device mapping:\r\n\r\n`\r\n\r\nAnd I am sure my CUDA and cuDNN is all right, and the path environment has no problem. But where may this error occur ?\r\n\r\nThanks again.\r\n", "1- In your logs, I do not see you installing the pip package you just built using `pip install`\r\n2- You also enabled \"OpenCL\" we have not tested the configuration where both opencl and cuda are enabled. It is quite possible they are interacting in an unexpected way.\r\n3- Also, what is the reason for not using the GPU pip packages we provide?\r\n\r\nAlso, I think the tail of your log is missing, it might contain more information.", "@gunan  Hi~ dear gunan, thank you very much, I didn't know the pip packages you provide, it is quite easy to setup. And now, my tensorflow with GPU is running.  //Applause  "]}, {"number": 6157, "title": "Fix Windows CPU CI link", "body": "The Windows build badge doesn't link to the CI Job page.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Can you change the URL to https? Thanks.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "PR Merged. Thanks, @bostelk !"]}, {"number": 6156, "title": "typo in the code for `tf.contrib.learn` tutorial", "body": "doing tutorial with copy-paste fails with \"accessing `test_data` before assignment\"", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@sandersk, could you take a look?"]}, {"number": 6155, "title": "Update tf.zeros_initializer method", "body": "The `tf.zeros_initializer` method has an unnecessary argument, asking for the tensor shape. See [tensorflow documentation](https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#zeros_initializer). The `tf.ones_initializer` method, which is essentialy equivalent in functionality, does not have this argument.\r\n\r\nAs a sidenote, the method `tf.constant_initializer` (see [documentation](https://www.tensorflow.org/versions/r0.12/api_docs/python/state_ops.html#constant_initializer)) also doesn't have this argument and serves the same purpose. In fact, if no keyword args are provided, it will behave as the `zeros_initializer` is intended to.", "comments": ["Yes, I try slim.overfeat net but occur error with tf.zeros_initializer .\r\nI resolved it using by constant_initializer.\r\nThanks."]}, {"number": 6154, "title": "dynamic variants of attention_decoder and embedding_attention_decoder", "body": "I've been following @alrojo's PR(#4686) for the `tf.contrib.seq2seq.dynamic_rnn_decoder`. I recently had to customize `tf.nn.seq2seq.embedding_attention_decoder` and `tf.nn.seq2seq.attention_decoder` with while loops for a personal project, so I'd be happy to help write `attention` and `embedding_attention` decoder functions for use with `tf.contrib.seq2seq.dynamic_rnn_decoder` if that would be helpful. I'm unclear on whether this is already being worked on, since I see discussion of a `rnn_decoder_attention` function in #4761, but it was a commit which followed the old structure of `tf.contrib.seq2seq`. ", "comments": ["Hi @ssampang \r\n\r\nI am currently working on finalizing a attention decoder function with the proper `kernel_tests`, I hope to submit it next week.", "Ah ok. Sounds good", "Hi @alrojo , is there any progress in the attention decoder? \r\nthank you"]}, {"number": 6153, "title": "Training on GPU and CPU with the same code differs significantly.", "body": "I am training a TF network on two machines. It is the same code, which is basically a modification of the MNIST example. In one case, I use GPU on Amazon Cloud, in the other I use a standard CPU Intel Xeon. I have the same version of TF, the same code, the same parameters, the same packages the and same Ubuntu distribution.\r\n\r\nIn one case on GPU the learning process is significantly different, and particularly slower. It reaches only 0.60 performance while CPU one reaches 0.90. Increasing learning rate by 10 I managed to get the GPU result to 0.90 too. Have you encountered this problem before? Is this intended?", "comments": ["This may be WAI depending on how fast your GPU implementation runs. It's easy to end up with scenarios where GPU version runs 10x slower. The thing to look at is the performance after same number of iterations", "@yaroslavvb, No I have a rather sophisticated validation stopping but in essence it boils down to iteration. So it is not the time I am concerned with it is really the computation. Could it be that there are different floating point standards leading to this, or the Adam optimizer behaves differently on GPU than on CPU. As I said raising the learning rate by 10 led to the same behavior. ", "@Mojusko Could it caused by random initialization for the variables?", "No, the same numpy seed leeds to the same behavior, and if even this is not the relevant seed, it is too consistent to be caused by random fluctuations. ", "@Mojusko I'm sorry, this is frustrating for you. Unfortunately it's hard to guess from this level of detail what the root cause might be. I'm afraid the best next step might be for you to divide and conquer to see where the computations are differing, i.e. fetch tensors from the middle of your model, and locate the first place where the CPU/GPU results start to differ substantially.\r\n\r\nOne place you might focus on is reductions, since the order of reduction is known to be very different on CPU/GPU. Is it possible that your model has tensors laid out in some way that CPU reduction order consistently leads to faster training? (If so, that in itself might be interesting to understand from a model perspective.)\r\n\r\nSorry not to have more concrete solutions.", "@michaelisard thank you for suggestions. It is not frustrating that much. I posted it here because could not find any similar answer on the net, and was wondering if someone else had experience with this (as I have specific machine in mind AWS p2 instance), and whether this is possibly a normal behavior. I will dive into further investigations.", "Closing this issue for now but please reopen if you get a more specific bug report.", "Hi all,\r\n\r\nI have exactly the **opposite** behaviour.\r\nMy net is quite simple:\r\n```python\r\nmodel = Sequential()\r\n    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Conv2D(32, (3, 3)))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Conv2D(64, (3, 3)))\r\n    model.add(Activation('relu'))\r\n    model.add(MaxPooling2D(pool_size=(2, 2)))\r\n    model.add(Flatten())\r\n    model.add(Dense(64))\r\n    model.add(Activation('relu'))\r\n    model.add(Dropout(0.5))\r\n    model.add(Dense(1))\r\n    model.add(Activation('sigmoid'))\r\n\r\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n```\r\nsame inputs, running on two systems, CPU (Intel Core i5, compiled from source with SIMD support) and GPU (Nvidia Tesla K20).\r\n\r\nGPU run is significantly faster (as expected, and I'm quite surprised to read this issue here); what is surprising is that after the first epoch on **CPU I have**\r\n```4951s 566ms/step - loss: 0.7037 - acc: 0.4903  - val_loss: 0.6931 - val_acc: 0.5000```\r\nwhile **on GPU**\r\n```2892s 331ms/step - loss: 0.8896 - acc: 0.7267  - val_loss: 0.2933 - val_acc: 0.8777```.\r\n\r\nHow can I investigate in this?", "Found this: https://stackoverflow.com/a/43264395/827818\r\nI added these lines to my code: \r\n```python\r\n    np.random.seed(1)\r\n    from tensorflow import set_random_seed\r\n    set_random_seed(2)\r\n```\r\nIn order to exclude randomness issues.", "@Mojusko : At the end, was the seed the problem? \r\nI am also seeing a similar problem (Ubuntu, python 3.5, TF1.8).\r\nSame code, same data, same machine, just switching between GPU and CPU execution by adding `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).\r\n\r\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a `tf.multiply` (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.\r\nUnfortunately, I am not able to reproduce the problem in a unit test. If I feed the same inputs to `tf.multiply` in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.\r\n\r\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.\r\n\r\nNote that the problem is very reproducible.\r\n\r\nIf I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.\r\n\r\nAny idea or debugging experiment suggestion is very welcome. ", "@annemenini I have also seen this problem, have you got any solutions?"]}, {"number": 6152, "title": "Fix static initialization based registry patterns with shared libraries.", "body": "We rely on static initializers of shared libraries of which no other\r\ncode is used to be run; thus, the shared libraries need to be linked\r\ninto the final binary. Some distributions default to -as-needed for\r\nlinking shared libraries into binaries, which discards libraries that\r\nare not actively referenced.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6151, "title": "Update comments for Tensor proto tensor_content field", "body": "After [previous change](https://github.com/tensorflow/tensorflow/pull/746/commits/e7f63369e74e6500e27cff451c84623386fd05a8), the current comment is still confusing. Add description about the purpose.\r\n", "comments": ["Can one of the admins verify this patch?"]}, {"number": 6150, "title": "summary name of the new `summary` module", "body": "With the new `tf.summary` module, the name shown in tensorboard has to be equal to the summary op in the graph.\r\nAs a result, the name shown in tensorboard cannot be the same as any existing op in the graph.\r\nThis can create a bit of problem. Before this change, I could have some op named `loss` and the summary of its output also named `loss`, which is very consistent and easy to play with -- I can just look for the name of the tensor in tensorboard.\r\nNow it looks like I have to add a prefix/suffix to everything because they cannot use the same name. Otherwise I got `loss_1` in tensorboard which is confusing especially to new users.\r\n\r\nIs there some changes that could be done to the new `summary` module that keeps the nice old feature? ", "comments": ["I would really like to have a way to customize the tag name shown in tensorboard. \r\nI could send a PR by simply adding an optional keyword argument to all the `tf.summary.something` functions. Does that sound like an acceptable solution or are there any design concerns?", "This request came up on SO [as well](http://stackoverflow.com/questions/41027247/tensorflow-0-12-0rc-tf-summary-scalar-error-using-placeholders), @danmane for comment", "We intend to add a \"display_name\" option to the summaries that controls how it displays in the frontend. \r\n\r\nA pull request would probably look like: add some metadata to the summary op that appears in the NodeDef in the graph, have the EventAccumulator read that metadata when it parses the graph definition, and then pass the metadata along with the summary info to the TensorBoard frontend. It's not trivial. @ppwwyyxx if you want to take this on we can discuss it in more depth.", "Great! If you have that on the TODO list maybe I should leave it to you.\r\nOne question: isn't this just the old \"tag\" field that was in Summary::Value that's used by the old summary module? Why not use that field?", "Is there any advancement on this? It is really annoying to get duplicated summaries in Tensorboard in the multi-GPU setting.", "Migrating this issue to tensorflow/tensorboard"]}, {"number": 6149, "title": "`ReLu` to `ReLU`", "body": "I think `ReLU` would be better.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 6148, "title": "Data Parallel, multi GPU (minimal) example?", "body": "I am trying to get a simple example of tensorflow that parallelises the data over multiple GPUs to train. I have looked at `cifar10_multi_gpu.py` and its associated input files etc. but it seems to be a bit beyond me at this stage. Especially since I'm not sure why multiple GPUs would speed up the process in that example. If I understand correctly in that example all GPUs get the same copy of the data and the model. The only advantage that I see from this is that you can search a more spread out weight/parameter space.\r\n\r\nWhat I expected was a line that did something similar to:\r\n```\r\nwith tf.Session() as sess:\r\n    for i in range(4): #for 4 gpus\r\n        with tf.device('/gpu:%d' % i):\r\n            tf.feed_dict({x: data[i*step:(i+1)*step], y: labels[i*step:(i+1)*step])\r\n```\r\nwith the emphasis of the feed dict feeding different data to the four different gpus. I haven't however thought how I would go about getting the gradients and averaging them yet. \r\n\r\nAny code examples would be highly appreciated. If not thoughts or any direction on how to proceed is also welcome.", "comments": ["In `cifar10_multi_gpu.py` the input function gets called once for each GPU therefore they get different data.", "http://usyiyi.cn/documents/effective-tf/10.html", "https://github.com/vahidk/EffectiveTensorflow#multi_gpu"]}, {"number": 6147, "title": "Update inception_v3_test.py", "body": "Easy fix.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6146, "title": "Fast Layer Normalization GPU kernel", "body": "Hi, I wrote a [custom CUDA op for layer normalization](https://github.com/MycChiu/fast-LayerNorm-TF), which is about 5-10x faster than the current `tf.contrib.layers.layer_norm`.\r\n\r\nAfter I posted the link on reddit, some people suggested that I should try to merge this kernel into the trunk, but I have never done this before, so I need some guidance on how this custom kernel could be integrated into TensorFlow's current code.\r\n\r\nWith my current understanding on how TensorFlow's code is organized, here are some of the problems I think should be solved before the custom op can be integrated into the trunk:\r\n1. It doesn't have CPU kernels.\r\n2. Since the kernel uses shuffle instructions, it only supports cards newer than Kepler.\r\n3. Current implementation has a restriction on the size of last dimension (cannot be larger than 5102)\r\n\r\nWithout solving these problems, the fastest way to merge this op into the trunk is probably by putting it under the `tf.contrib` folder as it is right now. I will fill out the CLA and make a pull request if you guys think this custom kernel is ready to be merged into the `tf.contrib` folder.", "comments": ["@martinwicke can you comment?", "In principle, there's no problem in registering a kernel only for GPU. Kepler (CC 3.0, I think) is the minimum for TF anyway, so no problem there either. We may even be able to dispatch to the kernel from contrib.layers.layer_norm, but that may be a bit of a hack. More probably, it'll be a separate op that you can call instead of the layer.\r\n\r\nI think you should try merging this into contrib, please send a PR!", "I changed the tags, please reference this issue from your PR.", "Thank you @martinwicke! As requested, #6205 ."]}]