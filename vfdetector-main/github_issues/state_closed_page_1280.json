[{"number": 14729, "title": "Unhelpful error for dynamic_rnn in version 1.4.0", "body": "This code:\r\n```python\r\nimport tensorflow as tf\r\n\r\nx = tf.constant([\r\n\t\t[[0,0],[5,0],[1,0],[1,0],[2,3],[4,0]],\r\n\t\t[[0,0],[0,0],[1,3],[2,0],[0,0],[0,0]]\r\n\t], dtype=tf.int32) #changing this to tf.float32 solves the problem\r\n\r\n\r\ncell = tf.nn.rnn_cell.LSTMCell(num_units=15) \r\ninitial_state = cell.zero_state(tf.shape(x)[0], dtype=tf.float32)\r\noutputs, state = tf.nn.dynamic_rnn(cell, x, initial_state=initial_state, dtype=tf.float32)\r\n\r\ninit_op = tf.group(tf.global_variables_initializer(),\r\n\t\ttf.local_variables_initializer())\r\n\r\nwith tf.Session() as sess:\r\n\tsess.run(init_op)\r\n\tprint(sess.run([outputs, state]))\r\n```\r\nDoes not work, because the inputs to the LSTM are integers and they need to be float. However, in version 1.4.0 I get this error:\r\n```\r\nValueError: Initializer for variable rnn/lstm_cell/kernel/ is from inside a control-flow construct, such as a loop or conditional. When creating a variable inside a loop or conditional, use a lambda as the initializer.\r\n```\r\nWhich has nothing to do with what is wrong with the code. Version 1.2.0 however, generates this error which correctly refers to the problem:\r\n```\r\nTypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int32, float32] that don't all match.\r\n```\r\n", "comments": ["Thank you very much, I encountered the exact same issue and couldn't figure out what the problem was: [https://stackoverflow.com/questions/47393356/how-to-use-tensorflows-dataset-api-iterator-as-an-input-of-a-recurrent-neural/47409487](https://stackoverflow.com/questions/47393356/how-to-use-tensorflows-dataset-api-iterator-as-an-input-of-a-recurrent-neural/47409487).\r\n\r\n(Python 3.6, tensorflow 1.4.0)", "@parsa-saadatpanah Thanks for the clear problem description!\r\n\r\n@ebrevdo can you suggest someone to look into a fix?\r\n\r\nContributions are definitely welcome!", "thank you!\r\nit helped a lot", "hey... I have the same problem ... Any solution ??", "@jayshah19949596 as @parsa-saadatpanah mentioned, you probably have a placeholder of dtype=int32. Change this to float32.", "Suppose your input tensorflow looks like where you are accepting int values\r\n\r\n`input_x=tf.placeholder(name='input_',shape=[4,3,2],dtype=tf.int32)\r\n`\r\nnow just use tf.cast method\r\n\r\n`input_x = tf.cast(input_x,tf.float32)\r\n`\r\nand feed it to rnn .", "Not that it is a very helpful advice, and not that I am proud of that fact that it is helpful to me. But: \r\n* Try restarting the console if you're developing using ipython \r\n\r\nNot sure what is it doing wrong, but after few runs with breaks via `ctrl+c` I encountered the issue. Clearing the scope does not help, but new instance does. \r\n", "@Demetrio92 : Your answer worked for me! Restarting the console fixed it. Weird.", "@mblouin02 well, apparently, we just discovered an instantiating bug, should've thought of it...", "Later versions of TensorFlow feature a much more informative error message for the code sample above:\r\n\r\n```\r\nValueError: RNN cell only supports floating point inputs, but saw dtype: <dtype: 'int32'>\r\n```\r\n\r\nClosing out this issue - feel free to open, if mistaken. \ud83d\ude42 "]}, {"number": 14728, "title": "Edit mnist data read in to match tutorial", "body": "The accompanying tutorial (https://www.tensorflow.org/get_started/mnist/beginners) and this file did not match. The proposed code change was copied from the tutorial and confirmed to run correctly.\r\n\r\nNote that this removes the ability to pass a custom data directory as an argument. However, since this is aimed at absolute beginners I believe clarity is more important than flexibility.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it", "Can one of the admins verify this patch?", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi @arinbjornk, thanks for pointing this out.\r\n\r\nIn general it isn't necessary for the tutorial to match the provided code exactly. In this case, since the relevant folder is just the location where the downloaded data is stored, I think using a location inside `/tmp` and having a modifiable flag is slightly preferable to just hard-coding a relative path.", "If you still want to make them consistent, a better change would be to modify the path specified in the tutorial: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/mnist/beginners.md#the-mnist-data"]}, {"number": 14727, "title": "New Feature: #12686 Softmaxcrossentropywithlogits gradient function", "body": "The test case is failing. Submitting a PR for review as I am unable to debug what's wrong with the code. I'll continue debugging but submitting the PR in the interim.", "comments": ["Hi @pbanavara, what's the status of this change? Are you still debugging some issues?", "Can one of the admins verify this patch?", "@suharshs Sorry I've been slammed with work this month. Won't have time to debug till new years. Is that ok ?", "Totally fine :) Just wanted to ensure I wasn't blocking you.\r\n\r\nHave a great end of the year!", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflowbutler Sorry I haven't had time to debug this and haven't updated the issue as well. I will put some closure on this by Sunday. Apologies.", "Quick update - There are some shape related errors in the test case that I am resolving. I should make some progress today and get this done.  It's hard to understand the shapes since the Output object has no shape attribute. So it's taking a while.", "@pbanavara great to see this is being worked on, hope it comes soon. \ud83d\ude04 ", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is there any way I could help getting this done? We really need this for our usage (not using Python).", "@nietras Thank you for offering to help. I am stuck getting the tests passed. I have detailed the problem in the most recent (last) comment here\r\nhttps://github.com/tensorflow/tensorflow/issues/12686\r\nPlease take a look at a look and let me know if anything obviously glaring comes at you.", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@pbanavara @nietras and @suharshs,\r\n\r\nI apologize for the long comment.\r\n\r\nI have made some (I think significant) progress on figuring out why @pbanavara test has shape failures. Which has lead to what I think is a passing test for SoftmaxCrossEntropyGrad. There are some steps in the process, so bear with me as we go through it.\r\n\r\n- _BroadcastMul in the original python is a function call to a function in https://github.com/tensorflow/tensorflow/blob/f16b50e388543d2c05698926aa1ef265090dda6b/tensorflow/python/ops/nn_grad.py#L431 which resizes the input vector into a matrix, so we need a C++ implementation of this.\r\n```c++\r\nOutput _BroadcastMul(const Scope& scope, Output vec, Output mat){\r\n  /* Multiply after broadcasting vec to match dimensions of mat.\r\n     Args:\r\n       vec: A 1-D tensor of dimension [D0]\r\n       mat: A 2-D tensor of dimesnion [D0, D1]\r\n\r\n    Returns:\r\n      A tensor of dimension [D0, D1], the result fo vec * mat\r\n      we use an element for element multiply here.\r\n  */\r\n  auto reshaped = ExpandDims(scope, vec, -1);\r\n  return Multiply(scope, reshaped, mat);\r\n}\r\n```\r\n- The python code that @nietras asked about \"math_ops.matmul(grad_grad[:, None, :], softmax[:, :,None]), axis=1)) * softmax)\" is inserting dimensions of size 1 into the respective tensors where the \"None\" is. We can use the C++ ExpandDims operator to accomplish the same thing.\r\n- Once we use our C++ _BroadcastMul, we find that the MatMul C++ operator only handles 2D tensors, however we have 3D so we need to use BatchMatMul.\r\n```c++\r\n  auto grad = _BroadcastMul(scope, grad_loss, softmax_grad);\r\n  if(!_isZero(scope, grad_grad)){\r\n    std::vector<int> axis;\r\n    auto logitsSoftmax = Softmax(scope, logits);\r\n\r\n    auto grad_gradExpand = ExpandDims(scope, grad_grad, 1);\r\n    auto logitsSoftMaxExpand = ExpandDims(scope, logitsSoftmax, 2);\r\n    auto matMulResult = BatchMatMul(scope, grad_gradExpand, logitsSoftMaxExpand);\r\n    axis.push_back(1);\r\n    auto squeezeResult = Squeeze(scope, matMulResult, Squeeze::Axis(axis));\r\n    auto subtractionResult = Subtract(scope, grad_grad, squeezeResult);\r\n    auto multiplyResult = Multiply(scope, subtractionResult, logitsSoftmax);\r\n    grad = Add(scope, grad, multiplyResult);\r\n  }\r\n```\r\n- Doing the isZero check in C++ is a little clunky and can't really be done as nicely, but we can at least deal with the edge cases of Zero and Zero like:\r\n```c++\r\nbool _isZero(const Scope& scope, Output grad){\r\n  std::array<std::string, 2> zeroOpTypeNames {{\"ZerosLike\", \"Zeros\"}};\r\n  string opTypeName = grad.op().node()->type_string();\r\n  for(auto& zeroOpTypeName : zeroOpTypeNames){\r\n    if(opTypeName == zeroOpTypeName){\r\n      return true;\r\n    }\r\n  }\r\n  //the Operation we were provided is not named something obvious\r\n  //we need to actually look at its contents.\r\n  //the original python code did this by calling a utility function called\r\n  //tensor_util.constant_value. When you dig into tensor_tuil.constant_value\r\n  //it is a large number of 'if' statements that measure certain edge cases\r\n  //where it is possible to get the value of the tensor without actually\r\n  //evaluating it. There are many kinds of tensors that can not have this\r\n  //done.\r\n  //There is no C++ equivalent to tensor_util.constant_value so we do nothing\r\n  //for the moment.\r\n  return false;\r\n}\r\n```\r\n- Please see my attached file [nn_grad_snip.cc.txt](https://github.com/tensorflow/tensorflow/files/1868452/nn_grad_snip.cc.txt)\r\n for full working code of the SoftmaxCrossEntropyWithLogitsGrad function and support functions.\r\n- Making the above changes causes the test that @pbanavara wrote to no longer complain about shape mismatches. Here is my version of the same test code:\r\n```c++\r\nTEST_F(NNGradTest, SoftmaxCrossEntropyWithLogitsGrad){\r\n  TensorShape logitsShape({5,3}); //batch size of 5, 3 possible labels (classes),\r\n                                  //logits is what is produced by a network\r\n                                  //they are compared to labels which are the truth\r\n  TensorShape lossShape({5}); //batch size of 5, 1 value for each entry in the batch\r\n                              //loss is the difference between logits and labels\r\n\r\n  TensorShape backPropShape({5,3}); //the docmentation says the backprop output\r\n                                    //will have batch size x num classes as its shape\r\n\r\n  auto logits = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(logitsShape)); //estimation\r\n  auto labels = Placeholder(scope_, DT_FLOAT, Placeholder::Shape(logitsShape)); //truth\r\n\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, logits, labels);\r\n\r\n  RunTest({logits, labels}, {logitsShape, logitsShape}, {y.loss, y.backprop}, {lossShape, backPropShape});\r\n}\r\n```\r\n- This is where my lack of knowledge of how Tensorflow is testing the gradient functions throws a monkey wrench into things. The test now fails with the error:\r\n```\r\n[ RUN      ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad\r\n2018-04-02 15:41:01.235423: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA\r\ntensorflow/cc/gradients/nn_grad_test.cc:72: Failure\r\nExpected: (max_error) < (1e-3), actual: 1 vs 0.001\r\n[  FAILED  ] NNGradTest.SoftmaxCrossEntropyWithLogitsGrad (90 ms)\r\n```\r\n- Tracking this error down took me a while and I am still not certain my solution is correct. The next step was to isolate both the Python and C++ implementations of SoftmaxCrossEntropyWithLogitsGrad and run them as \"standalone\" programs providing them the same known inputs and ensuring that the outputs were identical. The file \r\n[SoftmaxCrossEntropyWithLogitsGradTest.py.txt](https://github.com/tensorflow/tensorflow/files/1868506/SoftmaxCrossEntropyWithLogitsGradTest.py.txt) is the isolation of the Python gradient code and the file \r\n[softmaxcrossentropy-test.cc.txt](https://github.com/tensorflow/tensorflow/files/1868531/softmaxcrossentropy-test.cc.txt) is the isolated version of the C++.\r\n  * The isolated python code outputs \r\n```\r\n0\r\n[ 1.00194287  0.91191393]\r\n1\r\n[[-0.63283461  0.33222499  0.30060962]\r\n [ 0.29762709 -0.59824544  0.30061829]]\r\n2\r\n[ 0.02  0.03]\r\n3\r\n[[ 0.04  0.05  0.06]\r\n [ 0.07  0.08  0.09]]\r\n4\r\n[[-0.01608398  0.00686562  0.00921836]\r\n [ 0.00594364 -0.01795938  0.01201574]]\r\n5\r\n[[ 0.02003886  0.02203886  0.02403886]\r\n [ 0.03635742  0.02735742  0.03605742]]\r\n```\r\n- Note that the output order of the tensors is the same between the Python and C++\r\n  * The isolated C++ code outputs\r\n```\r\ny.loss: Tensor<type: float shape: [2] values: 1.00194287 0.911913931>\r\n 1.00194\r\n0.911914\r\ny.backprop: Tensor<type: float shape: [2,3] values: [-0.632834613 0.332225 0.300609618]...>\r\n-0.632835  0.332225   0.30061\r\n 0.297627 -0.598245  0.300618\r\ngrad_loss: Tensor<type: double shape: [2] values: 0.02 0.03>\r\n0.02\r\n0.03\r\ngrad_grad: Tensor<type: double shape: [2,3] values: [0.04 0.05 0.06]...>\r\n0.04 0.05 0.06\r\n0.07 0.08 0.09\r\ngrad_outputs[0]: Tensor<type: float shape: [2,3] values: [-0.0160839763 0.00686561596 0.00921836123]...>\r\n -0.016084 0.00686562 0.00921836\r\n0.00594364 -0.0179594  0.0120157\r\ngrad_outputs[1]: Tensor<type: float shape: [2,3] values: [0.0200388562 0.0220388565 0.0240388587]...>\r\n0.0200389 0.0220389 0.0240389\r\n0.0363574 0.0273574 0.0360574\r\n```\r\n - At this point I am certain that the C++ gradient code is a faithful port of the python code. I have changed the inputs several times and the C++ produces what the python produces. This leads me to think that the C++ test code is somehow wrong. If we look at the generated documentation for [tensorflow::ops::SoftmaxCrossEntropyWithLogits](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/softmax-cross-entropy-with-logits) we see that while the Outputs are in the order loss, backprop the auto generated public attributes are in the _opposite order_. Because of this I changed the C++ test.\r\n- From\r\n```c++\r\nRunTest({logits, labels}, {logitsShape, logitsShape}, {y.loss, y.backprop}, {lossShape, backPropShape});\r\n```\r\n- To\r\n```c++\r\nRunTest({logits, labels}, {logitsShape, logitsShape}, {y.backprop, y.loss}, {backPropShape, lossShape});\r\n```\r\n- Which seems to work the test will now pass... however I would like someone who knows what they are doing to verify my reasoning.\r\n```\r\nbazel test -c dbg //tensorflow/cc:gradients_nn_grad_test\r\n...\r\nINFO: Analysed target //tensorflow/cc:gradients_nn_grad_test (1 packages loaded).\r\nINFO: Found 1 test target...\r\nTarget //tensorflow/cc:gradients_nn_grad_test up-to-date:\r\n  bazel-bin/tensorflow/cc/gradients_nn_grad_test\r\nINFO: Elapsed time: 385.282s, Critical Path: 384.53s\r\nINFO: Build completed successfully, 4 total actions\r\n//tensorflow/cc:gradients_nn_grad_test                                   PASSED in 0.9s\r\n\r\nExecuted 1 out of 1 test: 1 test passes.\r\n```\r\nI hope this helps you and that someone with more knowledge can ensure this is correct because I too would like to be able to have C++ gradients for this operator (mostly so I can use it from Java).", "@rajha-korithrien Thanks a bunch. I'll wait for someone more knowledgable to clarify like you said. ", "Sorry, it seems this got buried in my inbox and i missed it.\r\n\r\nNice job digging into this, and great job getting a working solution! It indeed, looks correct, but I am unsure why the outputs y.loss and y.backprop are backwards :( This is very strange!\r\n\r\nFor now, i think it is reasonable to proceed with your working change (with the backwards outputs) and could you create a new issue assigned to me mentioning the backwards outputs issue for this test?\r\n\r\nThanks a bunch!\r\n\r\n", "@suharshs and @pbanavara Thanks for getting back to this!\r\nI see two ways forward.\r\n\r\n1. Pbanavara takes the code files from my comment and uses them to fix/update pbanavara's branch that is currently under review by this pull request (including fixing the conflicts with master)\r\n2. I could checkout the branch Pbanavara made fix it up and commit the results (also fixing the conflicts with master) assuming I had permissions to do so.\r\n\r\nI would slightly prefer (1) because this is @pbanavara's pull request after all... I just hijacked it :-)\r\nAlso I don't have a CLA in place yet (still waiting for my company to fix that) so if I officially commit to a branch would that mean it can't be merged in?\r\n\r\nAs @suharshs requested, I have made a new issue to track the apparently backwards y.loss and y.backprop public attributes of tensorflow::ops::SoftmaxCrossEntropyWithLogits #18734 but I am unable to assign it (I don't seem to see a place to assign tickets.. I could be blind). @suharshs can you please go assign it to yourself?\r\n\r\nThanks!", "@suharshs Thank you for verifying the corrections.\r\n@rajha-korithrien  - Man, You put in so much effort to fix this. So technically you should submit a PR Since you don't have a CLA your PR won't go through. Do you have any idea how long this CLA will take ?", "@pbanavara  I would not wait! Who knows how long \"the management\" takes on something. Please feel free to fix up your code and move ahead with the pull request.\r\nIf you don't have time to fix the code up, can you give me permissions to push to your branch and I can fix it up that way? Does that even work, or will my touching your branch make your pull request not go through? ", "@rajha-korithrien Ok I'm on this. Yes, if you don't have a CLA - I won't be able to submit a PR if you push your changes to my repo.", "@pbanavara Thanks so much for doing this! I think the last step is to pull in the current master and resolve the conflicts. At that point I hope it gets merged.", "@rajha-korithrien You are very welcome. I resolved the merge conflicts and now the test build is failing to recognize SoftmaxCrossEntropyWithLogits. Very weird. The build goes through but fails at test. I'll debug this soon and commit. \r\n\r\n`ERROR: /Users/pbanavara/dev/tensorflow/tensorflow/cc/BUILD:375:1: C++ compilation of rule '//tensorflow/cc:gradients_nn_grad_test' failed (Exit 1)\r\ntensorflow/cc/gradients/nn_grad_test.cc:113:12: error: use of undeclared identifier 'SoftmaxCrossEntropyWithLogits'\r\n  auto y = SoftmaxCrossEntropyWithLogits(scope_, logits, labels);\r\n           ^\r\n1 error generated.\r\nTarget //tensorflow/cc:gradients_nn_grad_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 228.321s, Critical Path: 46.34s\r\nFAILED: Build did NOT complete successfully`", "Thanks all, please ping this issue when everything is committed and ready for review.", "@suharshs Once I eliminate merge conflicts and build. The test is not able to recognize SoftmaxCrossEntropyWithLogits. I get this error in my test in nn_grad_test.cc: \r\n`error: use of undeclared identifier 'SoftmaxCrossEntropyWithLogits'\r\nauto y = SoftmaxCrossEntropyWithLogits(scope_, logits, labels);`\r\n\r\nThe build goes through fine. I don't get any error during the build. I get this error only when I execute \r\n`bazel test //tensorflow/cc:gradients_nn_grad_test`\r\n\r\nIs there anything in the most recent build that has altered the build path or something ? Sorry I have no clue how to resolve this. Even in debug mode I get only this message. I tried multiple iterations of build/test but no luck. Some help please. Thank you.", "Hmm, interesting. Is the current merged code what has been committed so that I can take a look?\r\nThanks!", "Yep the current commit is the latest merged code", "I am seeing a lot of other modified files, can you make sure that after merging and dealing with conflicts only files you changed for this issue are modified in your branch vs the master branch? ", "Ok, The reason for the other modified files is that I had not pulled from the TF master for a while.  I'll try to delete this commit and recommit only the file with my merged changes. ", "@pbanavara what's the current status of this? I was trying to do the same thing before I found your work. That's a very nice job you have done. Cannot wait to try it..", "@YijinLiu I need to get the test working. Like I mentioned in the last comment suddenly my tests are not able to find the SoftmaxCrossEntropyWithLogits function. I'll try to fix this at the earliest and post back.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "@suharshs I have fixed all errors and now the tests are passing. Can you please review ?", "@suharshs  Apologies, bumping this. Not sure why the CLA changed from Yes to No. Is it because i mentioned @rajha-korithrien in the commit ? Please let me know. Otherwise this PR is tested. Kindly review.", "there are still a few files that shouldn't be included, could you remove those?", "@suharshs Sorry, which files are not to be included. I re-verified that all the said files are removed. The only files remaining are nbproject/private/configurations.xml and nbproject/configurations.xml -  Should I remove those as well ?", "Yes please remove the XML file, only the c plus plus files should be\nincluded.\n\nOn Tue, Jun 12, 2018, 8:58 PM Pradeep Banavara <notifications@github.com>\nwrote:\n\n> @suharshs <https://github.com/suharshs> Sorry, which files are not to be\n> included. I re-verified that all the said files are removed. The only files\n> remaining are nbproject/private/configurations.xml and\n> nbproject/configurations.xml - Should I remove those as well ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14727#issuecomment-396805598>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABYidq84B1yhblqQmyD1tlvAqw8vpA7_ks5t8I3mgaJpZM4QkgA2>\n> .\n>\n", "@yifeif Do you know what is missing for the CLA test? ", "looks like the recent commits are not made with the same account that signed CLA?\r\n", "Thanks @yifeif \r\n\r\nAlmost there @pbanavara. Can you sign the CLA with the account used for the newer commits? See how the account in the recent commits has changed here? https://github.com/tensorflow/tensorflow/pull/14727/commits \r\n\r\nThanks!", "@suharshs @yifeif Looks like my recent commits have no email associated with the commit. My laptop had crashed and I had forgotten to explicitly state the user email in git config. I'll have to amend the commits and change the author name and email. I'll try it out. Thank you so much for all your support.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Ok , in response to the Google bot message I confirm that I have authored all commits on this PR. Hope this is sufficient to merge the PR. @suharshs please let me know if you need anything else from my end. ", "It seems there are still many conflicts and too many commits in this change that need to be resolved. Not sure where those came from :( If it proves too difficult to remove those commits and clean up this PR, perhaps just make a new PR from HEAD, adding only the two files you modified, using your CLA approved account and we can get this in.", "Ok I will submit a new PR with just the changed files from the head. Will get this done by Monday. ", "Nagging Assignee @suharshs: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing since your other PR worked. :)", "Could you please link to the other PR?\n\nOn Tue, Aug 7, 2018, 19:47 Suharsh Sivakumar <notifications@github.com>\nwrote:\n\n> Closed #14727 <https://github.com/tensorflow/tensorflow/pull/14727>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14727#event-1775897407>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKTG76Ppm4tqr7o6_hd4spdmT7Qy7zkkks5uOdK3gaJpZM4QkgA2>\n> .\n>\n", "Sure, https://github.com/tensorflow/tensorflow/pull/20763", "So after all  do we have this feature in some version of TF ?", "> So after all do we have this feature in some version of TF ?\r\n\r\nTo my understanding the pull request #20763 was merged into master on July 24th. So for certain a checkout of master will have this feature. I am not certain what the TF policy is for pulling master into releases. However I have looked at the 1.11 branch and @pbanavara changes are in that release, and I have checked it was also in 1.8 so I think 1.8 is the earliest version with these changes.", "> \r\n> \r\n> > So after all do we have this feature in some version of TF ?\r\n> \r\n> To my understanding the pull request #20763 was merged into master on July 24th. So for certain a checkout of master will have this feature. I am not certain what the TF policy is for pulling master into releases. However I have looked at the 1.11 branch and @pbanavara changes are in that release, and I have checked it was also in 1.8 so I think 1.8 is the earliest version with these changes.\r\n\r\nTF 1.9 :  No gradient defined for op: SoftmaxCrossEntropyWithLogits. \r\nI have not built the .dll myself. I use this .dll. https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.9.0/cpp 32bit version . ", "> > > So after all do we have this feature in some version of TF ?\r\n> > \r\n> > \r\n> > To my understanding the pull request #20763 was merged into master on July 24th. So for certain a checkout of master will have this feature. I am not certain what the TF policy is for pulling master into releases. However I have looked at the 1.11 branch and @pbanavara changes are in that release, and I have checked it was also in 1.8 so I think 1.8 is the earliest version with these changes.\r\n> \r\n> TF 1.9 : No gradient defined for op: SoftmaxCrossEntropyWithLogits.\r\n> I have not built the .dll myself. I use this .dll. https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.9.0/cpp 32bit version .\r\n\r\nI have no experience using tensorflow in windows. I can verify that a checkout and build of master does have this feature on Linux and Mac."]}, {"number": 14726, "title": "Make a copy of a model", "body": "Hi,  is there a canonical method in Tensorflow for this? For example, in Keras, we can use keras.models.clone_model for this purpose.  I though that model's copy would be such a nice feature, since copy.deepcopy does not work for me in Tensorflow. \r\nI want to copy weights from this model to another model of identical structure,  and I do not want to save a model then restore it to another instance for this situation. Specifically, the situation at every iteration we train model1 then make model2 as a copy of current model1, adding noise to model1 parameters and sample from model2 and then use these samples to update model1.\r\n```\r\nClass Model1(object):\r\n    def method1(self):\r\n        ....\r\n    def method2(self):\r\n        ....\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "just use `from copy import copy` and do `copy(model)` instead of deep copy.", "@Erichliu00 I'm not sure using copy works to solve the problem fully. If we use copy then we can't change the weights of the one model without it affecting the weights of the other model. For example:\r\n\r\n```\r\nW = tf.Variable(tf.constant(0.2))\r\nb= tf.Variable(tf.constant(0.5))\r\nx = tf.placeholder(tf.float32)\r\ny = W*x+b\r\n\r\ntrain = tf.assign(W, A2)\r\ny2 = copy(y)\r\n```\r\n\r\nthe train here is just to mock what happens if we have the variable changed.\r\n\r\n```\r\ninit = tf.global_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    print(sess.run([y, y2], feed_dict={x:1}))\r\n    sess.run(train)\r\n    print(sess.run([y, y2], feed_dict={x:1}))\r\n    y2 = copy(y)\r\n    print(sess.run([y, y2], feed_dict={x:1}))\r\n```\r\n\r\nPrints out\r\n```\r\n[0.69999999, 0.69999999]\r\n[1.0, 1.0]  -> This should be [1, 0.69999999] if a true copy was made\r\n[1.0, 1.0]\r\n```\r\n\r\nSo I don't know how to make a clone of a model I would love to know though... I currently have very awkward work arounds for it. If you do find out a way please let me know", "@dennismckinnon , \r\n\r\nSorry I did not fully test that,\r\n```\r\nimport tensorflow as tf\r\nfrom copy import copy\r\n\r\nv1 = tf.Variable(2.0)\r\nv_copy1 = tf.Variable(v1)\r\nv_copy2 = tf.identity(v1)\r\nv_copy3 = copy(v1)\r\nv_copy4 = v1\r\ninit = tf.Variable(0.0)\r\nv_copy5 = init.assign(v1)\r\n\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init)\r\na, b, c, d, e= sess.run([v_copy1, v_copy2, v_copy3, v_copy4, v_copy5])\r\nprint(a, b, c, d, e) # 2.0 2.0 2.0 2.0 2.0\r\n\r\nsess.run(tf.assign(v1, 3.0))\r\na, b, c, d, e= sess.run([v_copy1, v_copy2, v_copy3, v_copy4, v_copy5])\r\nprint(a, b, c, d, e) # 2.0 3.0 3.0 3.0 3.0\r\n```\r\nThus, for copying variable, one way is `v_copy1 = tf.Variable(v1)`, for copying model, you can create two instances and inistialize the copy model's variable using such way.", "@Erichliu00  Did you find the way to clone the model? or you stayed with the last solution you wrote up there, \"you can create two instances and initialize the copy model's variable using such way.\"?  I  have the same problem and I wanna know if there is a way to directly clone the model.", "I have tried it this way:\r\n\r\n```python\r\nvariables = tf.trainable_variables()\r\nfor var1 in variables:\r\n\tif \"pred/\" in var1.name:\r\n\t\ttrained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\r\n\t\tvalue = sess.run(trained_var)\r\n\t\tsess.run(tf.assign(var1, value))\r\n```\r\n\r\nFor this to work I put all variables of the target model in the scope \"pred\" and the source model was in the same wider scope. By getting the value of the source via sess.run() you get the hard copy of the values which can then be assigned to your target.", "This seems unnecessarily difficult. Is the expectation that everyone creates their own Keras-like wrapper to try to separate sessions and make models easy to copy? Keras is the most reasonable API to Tensorflow I have seen but it seems barely supported.\r\n\r\nIn any case, Tensorflow has otherwise been great work and I realize that wasn't easy so thank you!"]}, {"number": 14725, "title": "NameError: global name 'xrange' is not defined in Python 3 - and solution proposal", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 Docker image\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/boosted_trees/examples/boston.py\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nRunning the example code under python 3.6 in Docker environment I got xrange error: NameError: global name 'xrange' is not defined in Python 3\r\n\r\nI propose to change every xrange to range under the 3.x versions:\r\n# patch for xrange\r\nfind /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees -type f -print0 | xargs -0 sed -i 's/ xrange(/ range(/g'\r\n\r\n\r\n### Source code / logs\r\nNameError: global name 'xrange' is not defined in Python 3\r\n", "comments": ["I think we use `from six.moves import xrange` elsewhere in the codebase to handle this incompatibility, without the potential for a performance regression on Python 2.7. Any file that uses `xrange()` should have this import at the top.", "It didn't work for me, that's why I used this workaround.", "This issue has already been fixed (cc2030dfd4842bb8ecdd1f84a7f18ae65734a90f) but unfortunately didn't make it to 1.4. ", "oh, thank you then :)", "You can try to change the code line 44 in \r\n`for i in xrange(self.num_pages):`\r\nchange it to \r\n`for i in list(range(self.num_pages)):`\r\n\r\nI hope this should work and kindly restart your python before running the code.", "> I think we use `from six.moves import xrange` elsewhere in the codebase to handle this incompatibility, without the potential for a performance regression on Python 2.7. Any file that uses `xrange()` should have this import at the top.\r\n\r\nThanks it worked for Python 2.7", "> I think we use `from six.moves import xrange` elsewhere in the codebase to handle this incompatibility, without the potential for a performance regression on Python 2.7. Any file that uses `xrange()` should have this import at the top.\r\n\r\nThanks it worked for me :)"]}, {"number": 14724, "title": "Using GPU mnist_deep.py throws OOM when allocating tensor with shape...", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, I am using the mnist_deep.py with tensorflow 1.4.0\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 & Tensorflow 1.4.0 binary installation, Linux Ubuntu 16.04 & Tensorflow 1.4.0 built form source\r\n- **TensorFlow installed from (source or binary)**: Windows 10 installed with TF binary, Linux Ubuntu 16.04 TF built from source\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6.2 on Windows 10, 3.5.2 on Linux\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: CUDA 8.0, CuDNN 6.0 \r\n- **GPU model and memory**: For Windows 10: NVIDIA GeForce 940MX, For Linux: HW similar to NVIDIA Jetson TX2\r\n- **Exact command to reproduce**: python mnist_deep.py\r\n\r\n### Describe the problem\r\nThe mnist_deep.py sample given in Tensorflow examples/tutorials works fine when run on CPU. But when the same example is run using GPU, an OOM occurs when trying to allocate memory for tensor (specifically 10000) in both the cases. It does not matter if one increases/decreases the number of iterations to train the model, the OOM occurs even after a single iteration is executed.\r\n\r\nThe other examples like mnist.py, mnist_softmax.py, mnist_softmax_xla.py, etc. runs properly without any issues on the GPU. I have also tried to use the config_proto options but none of them seem to help.\r\n\r\n### Source code / logs\r\n************************************************\r\n#### Windows 10:\r\ntensor_name=\"edge_75_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'conv1/Conv2D', defined at:\r\n\u00a0 File \"mnist_deep.py\", line 176, in <module>\r\n\u00a0\u00a0\u00a0 tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\u00a0\u00a0\u00a0 _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\u00a0 File \"mnist_deep.py\", line 137, in main\r\n\u00a0\u00a0\u00a0 y_conv, keep_prob = deepnn(x)\r\n\u00a0 File \"mnist_deep.py\", line 63, in deepnn\r\n\u00a0\u00a0\u00a0 h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\n\u00a0 File \"mnist_deep.py\", line 105, in conv2d\r\n\u00a0\u00a0\u00a0 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 630, in conv2d\r\n\u00a0\u00a0\u00a0 data_format=data_format, name=name)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n\u00a0\u00a0\u00a0 op_def=op_def)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n\u00a0\u00a0\u00a0 op_def=op_def)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n\u00a0\u00a0\u00a0 self._traceback = self._graph._extract_stack()\u00a0 # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,32,28,28]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](reshape/Reshape, conv1/Variable/read)]]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [[Node: Mean_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n************************************************\r\n####Linux:\r\ntensor_name=\"edge_75_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'conv1/Conv2D', defined at:\r\n\u00a0 File \"mnist_deep.py\", line 176, in <module>\r\n\u00a0\u00a0\u00a0 tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\u00a0\u00a0\u00a0 _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\u00a0 File \"mnist_deep.py\", line 137, in main\r\n\u00a0\u00a0\u00a0 y_conv, keep_prob = deepnn(x)\r\n\u00a0 File \"mnist_deep.py\", line 63, in deepnn\r\n\u00a0\u00a0\u00a0 h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\r\n\u00a0 File \"mnist_deep.py\", line 105, in conv2d\r\n\u00a0\u00a0\u00a0 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 630, in conv2d\r\n\u00a0\u00a0\u00a0 data_format=data_format, name=name)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n\u00a0\u00a0\u00a0 op_def=op_def)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\r\n\u00a0\u00a0\u00a0 op_def=op_def)\r\n\u00a0 File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\r\n\u00a0\u00a0\u00a0 self._traceback = self._graph._extract_stack()\u00a0 # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[10000,32,28,28]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](reshape/Reshape, conv1/Variable/read)]]\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [[Node: Mean_1/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_75_Mean_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n************************************************\r\n\r\nFurther detailed logs can be attached if needed. Your help and pointers to solve this will be much appreciated.", "comments": ["It seems to be related to #136. I suggest you modify the batch size to fit into the memory first. ", "@qmick Thanks for replying. The issue does not seem to be the one as #136. The OOM occurs even on smaller memory allocation blocks. Can you point me to the location for modification of batch size? ", "See ChinChangYang's solution: https://github.com/tensorflow/tensorflow/issues/136#issuecomment-292701072", "Do you have open other tensorflow session ? I often get this, because I have multiple session up and running. ", "@qmick Thanks. The modifications by ChinChangYang worked according to his comment. Maybe that modification should be given as an implementation as mnist_cuda_deep.py. The documentation on TF talks about batch_size but does not show a quick way for newbies like me to implement it.\r\n\r\nThanks for your responses. I will close the issue. I can send a patch with the modified code as a PR to be included in TF if there is an agreement that we need such an example for CUDA. Thanks once again.", "It's nice that you want to contribute to TF, but according to #6764, such patch is thought to be unnecessary.", "Thanks @qmick. I will leave it to that.", "For the people stuck with this in models other than mnist.\r\nthe reason for this is the high amount of parameters (please check your `model.summary()`).\r\n\r\nA good method to drastically lower these parameters is to add:\r\n`subsample=(2, 2)` (careful it lowers the resolution of images/data) in all the Convolutional layers above that Flatten layer,\r\nif subsample doesn't work then it is `stride=(2, 2)`.", "reducing the batch size worked for me."]}, {"number": 14723, "title": "Allow GANEstimator get_hooks_fn to be set manually", "body": "", "comments": ["Can someone review this please. It is a small but very useful change\r\n", "Can one of the admins verify this patch?", "Done", "What is Ubuntu CC?", "@yifeif PTAL at this change and merge if it looks fine to you.\r\n\r\nThanks!", "@tensorflow-jenkins test this please", "Thanks for doing this, Julian! Sorry for the delay; we've fixed some\nprocess bugs, and I'll be more responsive to PRs moving forwards.\n\nOn Mon, Dec 11, 2017 at 3:41 AM, Shanqing Cai <notifications@github.com>\nwrote:\n\n> Merged #14723 <https://github.com/tensorflow/tensorflow/pull/14723>.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14723#event-1380692603>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFvffJvvcaj11RVrQMPqc2VbxH8QU50eks5s_KRigaJpZM4QkSTZ>\n> .\n>\n", "Happy to help :)"]}, {"number": 14722, "title": "Add a way to provide target nodes in Android", "body": "This is required when running some models as a step for initializing the graph etc.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Can one of the admins verify this patch?", "I signed it! (as part of JoyTunes)", "Please re-check the CLA agreement.", "CLAs look good, thanks!\n\n<!-- ok -->", "To explain the motivation for this PR better:\r\nIf I try to supply the \"init\" node in `outputNames` instead of adding it as a target node, I get the following error:\r\n\r\n    java.lang.IllegalArgumentException: Tried to fetch data for 'init:0', which produces no output.  To run to a node but not fetch any data, pass 'init:0' as an argument to the 'target_node_names' argument of the Session::Run API", "@theyonibomber did you address all comments?\r\n@psanketi good to go?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Yeah comment was addressed on a commit before this was reviewed...", "OK, @psanketi WDYT?", "@psanketi can you take another look, please?", "(Sorry I was OOO for the past 4 weeks). \r\nYeah, good to go as far as my comment is concerned. "]}, {"number": 14721, "title": "Add back whitespace", "body": "When `tfcompile_flags` was changed so it could be not just a string but also a list of strings, the initial white space was erroneously removed (probably a misunderstanding of str.join) meaning `--out_object=` would consume the first flag.\r\n\r\nE.g. this would no longer work:\r\n```\r\ntf_library(\r\n  tfcompile_flags = \"--target_cpu='core-avx2'\"\r\n)\r\n```\r\n\r\nWhile this would work:\r\n```\r\ntf_library(\r\n  tfcompile_flags = \" --target_cpu='core-avx2'\"\r\n)\r\n```", "comments": ["Can one of the admins verify this patch?", "@gunan, I wonder about the state of these tests. Shouldn't the CI have complained? It seems to be down at the moment so I can't check.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/f1a10992320df3dbeb935e2401fee76097b0ea99/tensorflow/compiler/aot/tests/BUILD#L135\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/d4ad9c73969c45d1a224ebfc43eb645b9860216b/tensorflow/compiler/tests/BUILD#L675", "The manual tags there prevent the CI systems from trying to build those rules. The issue snuck in because of those."]}, {"number": 14720, "title": "ImageUtils.convertYUV420SPToARGB8888 resulting distorted image", "body": "\r\n\r\n\r\nHi @andrewharp \r\n\r\nI did some modification in TF detect android app to detect person from loaded picture rather than clicking it by camera(Not using cameraActivity).\r\n\r\nEverything is working fine except the function named ImageUtils.convertYUV420SPToARGB8888. This function returns distorted image (please find attachment)\r\n\r\nactual image:\r\n![actual_image](https://user-images.githubusercontent.com/2331214/33017067-f6c5dd42-ce16-11e7-9cb0-7827cf490c8b.png)\r\n\r\nAfter converting image from YUV 4:2:0 to ARGB:\r\n![converted_image](https://user-images.githubusercontent.com/2331214/33017069-f6f60e2c-ce16-11e7-9772-89d0a7d22612.png)\r\n\r\nSharing part of code below:\r\n\r\n            newbitmap = Bitmap.createScaledBitmap(bitmap, 640, 480, false);\r\n            ByteArrayOutputStream output = new ByteArrayOutputStream();\r\n            newbitmap.compress(Bitmap.CompressFormat.JPEG, 100, output);\r\n            imgbytes = output.toByteArray();\r\n\r\n            imageConverter =\r\n                new Runnable() {\r\n                    @Override\r\n                    public void run() {\r\n                        ImageUtils.convertYUV420SPToARGB8888(imgbytes, previewWidth, previewHeight, rgbBytes);\r\n                    }\r\n                };", "comments": ["Resolved :)", "hello,@lavinachitara , i met similar problem now, how did you solve it ? "]}, {"number": 14719, "title": "Tensorflow Lite demo app with inception-v3/Mobilenet_v1 (float) model crashes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 14.04\r\n- **Python version**: 3.4.3\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n\r\n\r\n### Describe the problem\r\nDevice: Galaxy S8\r\nI downloaded the \"Inception V3 Slim 2016\" from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md . I pushed the imagenet_2015_label_strings.txt and the \"inceptionv3_non_slim_2015.tflite\" to the asset folder\r\n\r\nI edited the ImageClassifier.java of tflite demo app. The changes are the followings:\r\nprivate static final String MODEL_PATH = \"/inceptionv3_non_slim_2015.tflite\";\r\nstatic final int DIM_IMG_SIZE_X = 299;\r\nstatic final int DIM_IMG_SIZE_Y = 299;\r\n\r\nThe app hangs when it starts! (I could run the app with the default mobilenet quantized graph).\r\nSimilar is the case with mobilenet_v1_224_Float graph as well (the app hangs or crashes). I assume, the float model graph is not yet supported by TF Lite. However, in the documentation its written that it does support float for most operations. I am thinking the error is due to image pre-processing output and input size of float model grpah. The error log is stated below:\r\n\r\nThe Error log:\r\n11-21 14:31:43.034 2111-2416/android.example.com.tflitecamerademo E/AndroidRuntime: FATAL EXCEPTION: CameraBackground\r\n                                                                                    Process: android.example.com.tflitecamerademo, PID: 2111\r\n                                                                                    java.lang.IllegalArgumentException: Failed to get input dimensions. 0-th input should have 1072812 bytes, but found 268203 bytes.\r\n                                                                                        at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)\r\n                                                                                        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:82)\r\n                                                                                        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)\r\n                                                                                        at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)\r\n                                                                                        at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:112)\r\n                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)\r\n                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment.-wrap0(Camera2BasicFragment.java)\r\n                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment$4.run(Camera2BasicFragment.java:558)\r\n                                                                                        at android.os.Handler.handleCallback(Handler.java:751)\r\n                                                                                        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                                        at android.os.Looper.loop(Looper.java:154)\r\n                                                                                        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n\r\n\r\n### Additional Questions:\r\n1) On the app the the tensorflow lite graph format is \".tflite\". However, on the documentation https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md the format is written as \".lite\"\r\n\r\n\r\n", "comments": ["I met the same problem and I think I've found how to solve it.\r\nFirst of all, the demo app is designed to use the quantized model, in which the input type and parameters' type are both 8-bit. But for the float version, the input type should be 32-bit. You can see from your error log that the expected bytes is exactly 4 times of the actual bytes!\r\n\r\nSo I think you need to modify the input type of the demo app to fit the 32-bit input.", "Thank you for the reply. @pkurogerjs\r\nHowever, The image data is already converted to floating point in the TFLiteDemo ImageClassifier.java file\r\n\r\n\"/** Writes Image data into a {@code ByteBuffer}. */\r\n  private void convertBitmapToByteBuffer(Bitmap bitmap) {\r\n    if (imgData == null) {\r\n      return;\r\n    }\r\n    imgData.rewind();\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n    // Convert the image to floating point.\r\n    int pixel = 0;\r\n    long startTime = SystemClock.uptimeMillis();\r\n    for (int i = 0; i < DIM_IMG_SIZE_X; ++i) {\r\n      for (int j = 0; j < DIM_IMG_SIZE_Y; ++j) {\r\n        final int val = intValues[pixel++];\r\n        imgData.put((byte) ((val >> 16) & 0xFF));\r\n        imgData.put((byte) ((val >> 8) & 0xFF));\r\n        imgData.put((byte) (val & 0xFF));\r\n      }\r\n    }\r\n\" ", "Well, I don't agree with the comment in the code. It just extract the RGB value from a pixel into 8-bit bytes. \r\nYou can see in ImageClassifier.java that the line \"tflite.run(imgData, labelProbArray);\"  takes a 8-bit format input and a 8-bit format output tensors. You need to change both of them to 32-bit float arrays.\r\n\r\nI've successfully run the mobilenet_224 float version. the input is a 4-dim float array and the output put is a 1*num_of_label_types array.\r\n\r\nI found this problem by reading the source code in the directory org.tenslow.lite. \r\n\r\nHope this will help you.\r\n", "I think this exception is because your labelProbArray(line 77) is still a byte array. Change its type to float, please. The exception is thrown when calling something like input.copyTo(output) in Tensorflow Lite's java code, which requires that the type input and  output should be the same.\r\n\r\nBesides, it seems that for inception v3, the input image size should be 299*299.\r\nI ran this model, the recognition results seems right, but the output probabilities are larger than 1.0. I'm confused about that", "@pkurogerjs, Thankyou for the help. It works. I am initially more eager towards performance benchmarking between quantized vs float models. ", "Any difference between Slim and non-slim one? I have had also the output probabilities are larger than 1.0. and the recognition does work well. It does not return a right answer \r\n@atrah22 Could you share your moditication?\r\nI hvae changed \r\n imgData = new float[DIM_BATCH_SIZE][DIM_IMG_SIZE_X][DIM_IMG_SIZE_Y][DIM_PIXEL_SIZE];\r\n    labelProbArray = new float[1][labelList.size()];\r\n\r\nnew AbstractMap.SimpleEntry<>(labelList.get(i), (labelProbArray[0][i])));", "I was able to get it working with inception v3 non-slim 2015 by making @kidsung 's changes plus:\r\n\r\nreplacing the imgData.put's with:\r\n```\r\n imgData[0][i][j][0] = (float) ((val >> 16) & 0xFF);\r\n imgData[0][i][j][1] = (float) ((val >> 8) & 0xFF);\r\n imgData[0][i][j][2] = (float) (val  & 0xFF);\r\n```\r\ncommenting out \r\n```\r\nimgData.rewind();\r\n```\r\nand \r\n```\r\nimgData.order(ByteOrder.nativeOrder());\r\n```\r\n\r\nand modifying the declarations:\r\n```\r\nprivate float[][][][] imgData = null;\r\nprivate float[][] labelProbArray = null;\r\n```\r\n\r\nAdditionally, I received a tensor length error for the labels, the file had 1001 and (somewhere) it expected 1008, so I filled in 7 lines of foo1, foo2, etc. ", "Seems like the issue is resolved. Could you please close it if it is?", "After modifying ImageClassifier.java , I can run float model from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md;\r\ntest the Inception model(Inception V3 Slim 2016)and  Mobilenet model(Mobilenet 1.0 224 Float)\r\nthe inceptionv3_slim_2016.tflite model inference time is 2260ms,\r\nthe  mobilenet_v1_1.0_224.tflite model inference time is 500ms and mobilenet_quant_v1_224.tflite is 65ms.\r\n add:\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128.0f;\r\nchange:\r\n private byte[][] labelProbArray = null;\r\n.............................................................\r\n private float[][] labelProbArray = null;\r\n\r\nimgData = ByteBuffer.allocateDirect(\r\n            DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);\r\n................................................................\r\nimgData = ByteBuffer.allocateDirect(\r\n            4*DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);\r\n   \r\n labelProbArray = new byte[1][labelList.size()];\r\n.............................................................\r\n labelProbArray = new float[1][labelList.size()];\r\n\r\nimgData.put((byte) ((val >> 16) & 0xFF));\r\nimgData.put((byte) ((val >> 8) & 0xFF));\r\nimgData.put((byte) (val & 0xFF));\r\n.............................................................\r\n  imgData.putFloat((((val >> 16) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n  imgData.putFloat((((val >> 8) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n  imgData.putFloat(((val & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n\r\nif use the inception model,modifying\r\n static final int DIM_IMG_SIZE_X = 224;\r\n static final int DIM_IMG_SIZE_Y = 224;\r\n.............................................................\r\nstatic final int DIM_IMG_SIZE_X  =  299;\r\nstatic final int DIM_IMG_SIZE_Y  =  299;", "@atrah22  Is there any result about quantized mode vs. float model ? ", "@OdingdongO  \r\nI tried your code and got  an error.\r\n`    new AbstractMap.SimpleEntry<>(labelList.get(i), (labelProbArray[0][i] & 0xff) / 255.0f));`\r\nOperatpr '&' cannot be applied to 'float','int'\r\nI use two way to  fix .\r\n` sortedLabels.add(new AbstractMap.SimpleEntry<>(labelList.get(i), (labelProbArray[0][i])));`\r\nand \r\n`sortedLabels.add(new AbstractMap.SimpleEntry<>(labelList.get(i), ((int)labelProbArray[0][i] & 0xff) / 255.0f));`\r\nI can run inceptionv3_slim_2016 model, but the predication get a bad result. It has no original effect.\r\nCan you help me to solve the problem? Do you had a better prediction\uff1f\r\n", "I  can  run  inceptionv3_slim_2016.tflite model after change some code.  The inference time is about 1200ms on my Huawei kirin960 phone.  It' s  too slow!!\r\n\r\nhttps://stackoverflow.com/questions/47463204/tensorflow-lite-convert-error-for-the-quantized-graphdef\r\n", "Hi @OdingdongO, I have a question about the image mean and std value of the inceptionv3_slim_2016 model, why the mean and std value are 128? How did you get the value?", "@ghostsun89 The mean and std values are e.g. listed [in this script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py). They don't refer explicitly to the slim model though.\r\n\r\nI combined the results collected here and created a pr: #16833.\r\nThe inception confidence score is still unnormalized though. Has anybody here already managed to scale it to [0, 1]? Any further help is appreciated very much.", "it seems the output of the [inceptionv3 tflite model released by google](https://storage.googleapis.com/download.tensorflow.org/models/tflite/inception_v3_slim_2016_android_2017_11_10.zip) is problematic. I'll create another issue later. If you run [label_image for tflite](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/examples/label_image/label_image.md), you'll get\r\n\r\n```bash\r\n> ./label_image -m inceptionv3_slim_2016.tflite\r\nLoaded model inceptionv3_slim_2016.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 1009.48 ms \r\n8.06111: 653 military uniform\r\n6.19022: 668 mortarboard\r\n5.83456: 401 academic gown\r\n5.26993: 835 suit\r\n4.80701: 855 theater curtain\r\n```\r\n\r\nIf you convert InceptionV3 yourself, \r\n```bash\r\n> curl http://download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz \\\r\n | tar -C /tmp -xzf -\r\n> bazel run --config=opt   //tensorflow/contrib/lite/toco:toco --  \\\r\n--input_file=/tmp/inception_v3_2016_08_28_frozen.pb  \\\r\n--output_file=/tmp/inceptionv3.tflite   --input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,299,299,3 \\\r\n--input_array=input   --output_array=InceptionV3/Predictions/Reshape_1\r\n```\r\n\r\nand push the /tmp/inceptionv3.tflite to your android devices, then you can see expected results like\r\n\r\n```bash\r\n> ./label_image -m inceptionv3.tflite             \r\nLoaded model inceptionv3.tflite\r\nresolved reporter\r\ninvoked \r\naverage time: 1020.93 ms \r\n0.496246: 653 military uniform\r\n0.0764156: 668 mortarboard\r\n0.0535454: 401 academic gown\r\n0.030444: 835 suit\r\n0.0191629: 855 theater curtain\r\n```\r\n", "@ghostsun89 @OdingdongO I checked the mean and std values again and think the ones posted above are incorrect. They are correct for the usage of the \"old non-slim\" inception-v3 version. However, the discussed slim version requires `mean=0` and `std=255`. [See this](https://github.com/tensorflow/models/issues/3346) for some additional information.", "@atrah22 @drricksanchez321 @pkurogerjs @OdingdongO  I'm trying to modify [MainActivity.java](https://github.com/googlecodelabs/mlkit-android/blob/master/custom-model/final/app/src/main/java/com/google/firebase/codelab/mlkit_custommodel/MainActivity.java) at [googlecodelabs/mlkit-android](https://github.com/googlecodelabs/mlkit-android/tree/master/custom-model/final) and ran into a similar issue. The code was written for quantised models and I'm trying to use unquantised model. I followed the above modifications and I finally ran into **java.nio.BufferOverflowException** error.\r\nSee [this](https://github.com/googlecodelabs/mlkit-android/issues/5) for more info.", "Hello all @pkurogerjs @OdingdongO @aselle @atrah22 ,\r\n\r\nI am facing the same problem with `inference_type=FLOAT`. However, the model i am trying is ssd_mobilenet_v1_coco. I had successfully trained the model on my custom data sets and is working fine.\r\nBut when porting to android it is throwing error related to  \r\n\r\n> mismatched Tensor + input buffer size. The error reads as\r\n8-16 10:26:42.107 28242 28257 E AndroidRuntime: FATAL EXCEPTION: inference\r\n08-16 10:26:42.107 28242 28257 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 28242\r\n08-16 10:26:42.107 28242 28257 E AndroidRuntime: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.\r\n\r\nwhere should I change the input size for the float inference. I would appreciate your pain if you could share any detailed steps (i am new to tensorflow and tflite).\r\n\r\nThank you for your time!", "@alokranjan007 Go through the comments in both [question and answer](https://stackoverflow.com/questions/50923996/changes-required-for-using-non-quantized-tflite-files-in-mainactivity-java). It may help you a bit. ", "@duplex143 Thank you for you help! But I could not find any MainActivity.java file for SSD_Mobilenet_V1. Since I am using the detection app there is detectorActivity file. Nevertheless, there is no such line related to ByteBuffer like the one discussed above and the mentioned link by you.\r\nCould you help me to understand about the necessary changes to be made for SSD_Mobilenet for float inference.\r\n\r\nThank you,", "> After modifying ImageClassifier.java , I can run float model from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md;\r\n> test the Inception model(Inception V3 Slim 2016)and Mobilenet model(Mobilenet 1.0 224 Float)\r\n> the inceptionv3_slim_2016.tflite model inference time is 2260ms,\r\n> the mobilenet_v1_1.0_224.tflite model inference time is 500ms and mobilenet_quant_v1_224.tflite is 65ms.\r\n> add:\r\n> private static final int IMAGE_MEAN = 128;\r\n> private static final float IMAGE_STD = 128.0f;\r\n> change:\r\n> private byte[][] labelProbArray = null;\r\n> .............................................................\r\n> private float[][] labelProbArray = null;\r\n> \r\n> imgData = ByteBuffer.allocateDirect(\r\n> DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);\r\n> ................................................................\r\n> imgData = ByteBuffer.allocateDirect(\r\n> 4*DIM_BATCH_SIZE * DIM_IMG_SIZE_X * DIM_IMG_SIZE_Y * DIM_PIXEL_SIZE);\r\n> \r\n> labelProbArray = new byte[1][labelList.size()];\r\n> .............................................................\r\n> labelProbArray = new float[1][labelList.size()];\r\n> \r\n> imgData.put((byte) ((val >> 16) & 0xFF));\r\n> imgData.put((byte) ((val >> 8) & 0xFF));\r\n> imgData.put((byte) (val & 0xFF));\r\n> .............................................................\r\n> imgData.putFloat((((val >> 16) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n> imgData.putFloat((((val >> 8) & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n> imgData.putFloat(((val & 0xFF)-IMAGE_MEAN)/IMAGE_STD);\r\n> \r\n> if use the inception model,modifying\r\n> static final int DIM_IMG_SIZE_X = 224;\r\n> static final int DIM_IMG_SIZE_Y = 224;\r\n> .............................................................\r\n> static final int DIM_IMG_SIZE_X = 299;\r\n> static final int DIM_IMG_SIZE_Y = 299;\r\n\r\n**The above solutions is differentiating between byte type & float type Buffers configuration based on model, which ever it accepts., So for resizing your input tensor, you can check following link :**\r\n https://github.com/tensorflow/tensorflow/issues/23940#issuecomment-447380387", "Hello All,\r\n\r\nI am working on the similar issue.I want to use latest ssd_mobilenet_v3_small_coco_2019_08_14.\r\nI am getting error as TensorFlowLite buffer with 307200 bytes and a Java Buffer with 270000 bytes.\r\nI checked few of the mentioned  method above to solve this.Still no luck !!\r\nCould you help me to understand about the necessary changes to be made for float inference.\r\n\r\nThank you,\r\n", "@Ammu1991  307200 = 320x320x3 and 270000 = 300x300x3. It's quite obvious that the model is expecting 320x320 and the Java code assumes 300x300x3.", "@freedomtan Thank you for you help!.I know but what change I have to make in the code.\r\nI have replaced V1 model with V3 version.Since I do not have labelmap.txt file for v3. Can you guide me how to use V3 model in the project.\r\n\r\nThank you,", "Hello All,\r\n\r\nFor coco_ssd_mobilenet_v1_1.0_quant_2018_06_29 model in the sample example project.In\r\ndownload_model.gradle their is web link from where v1 model is getting downloaded.\r\n\r\nhttp://storage.googleapis.com/download.tensorflow.org/models/tflite/coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.zip\r\n\r\nAnybody knows similar link for latest ssd_mobilenet_v3_small_coco_2019_08_14 in download_model.gradle from where I can get complete repository with all corresponding file set\r\n\r\nThank you,"]}, {"number": 14718, "title": "tensorflow/contrib/lite/download_dependencies.sh does not finish without error!", "body": "for tensorflow/contrib/lite/download_dependencies.sh, I can not run successfully with commit 049a34d692095b7e137bca27d2445415314ceaf7.\r\nAnd I rollback to 4b4b51cdd9e8c3c748b76dd8649bcd5556e84d76, everything is good.", "comments": ["Hi @albushuang , could you post the error message here?", "Thanks for reporting. I'm having #14631 to fix it. ", "Fixed in #14734"]}, {"number": 14717, "title": "Coverage for NMT", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: NVIDIA TITAN X (12GB)\r\n- **Exact command to reproduce**:  NA\r\n\r\nModelling coverage is a very useful feature in NMT to reduce over-translations.\r\n\r\nRef.: \r\nhttps://www.aclweb.org/anthology/P/P16/P16-1008.pdf,\r\nhttps://arxiv.org/pdf/1704.04368.pdf\r\n\r\nIs this feature available right now or, if not, how can I hack the current attention mechanism (say, Bahadanau) to add this feature ? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Coverage will fix the internal bias defect in beamsearchdecode and strongly recommends implementation @tatatodd", "@tatatodd this WAS a feature request."]}, {"number": 14716, "title": "failed to convert model with \"FusedBatchNorm\" to TFLITE format", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nboth source and binary tried\r\n- **TensorFlow version (use command below)**:\r\n1.4\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n7\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.8\r\n- **CUDA/cuDNN version**:\r\n8.0-5.1\r\n- **GPU model and memory**:\r\ngtx1080-8G\r\n- **Exact command to reproduce**:\r\n\r\n\r\n###Problem###\r\nMy original model is with node \"FusedBatchNorm\", when I run the script  ```bazel build tensorflow/contrib/lite/toco:toco``` and ```bazel run --config=opt tensorflow/contrib/lite/toco:toco -- --input_file=/home/wz/Desktop/rt-mobilenet.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE --output_file=/home/wz/Desktop/tflite-pose.lite --inference_type=FLOAT --input_type=FLOAT --input_arrays=image --output_arrays=Openpose/concat_stage7 --input_shapes=1,368,368,3``` \r\nEverything seems fine, but the result file is empty. And I traced the code, founding that code returned at file 'resolve_constant_binary' 's function 'EvaluateBinaryOperatorOnConstantInputs'.\r\nMy model can be get [here](https://www.dropbox.com/s/09xivpuboecge56/mobilenet_0.75_0.50_model-388003.zip?dl=0).\r\nGod help me! Thanks a lot!\r\n", "comments": ["It looks like you only provided a checkpoint. Could you provide python source code for the model?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 46 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this one. Please reopen if you are still having problems."]}, {"number": 14715, "title": "where is mobile  ssd model?", "body": "where is mobile  ssd model? where is it.where  is  mobile net model??", "comments": ["Perhaps you're looking for:\r\n\r\nhttps://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html\r\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\n\r\nFor future reference, this type of question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14714, "title": "Update docs for using Docker with GPU, with nvidia-docker2", "body": "Update docs to reflect the current version of `nvidia-docker` (version 2).\r\n\r\nFor reference: https://github.com/NVIDIA/nvidia-docker/wiki/About-version-2.0", "comments": ["Can one of the admins verify this patch?", "On my installation, `nvidia-docker` still works. I think to reduce confusion, we should stick with `nvidia-docker`, and think of this switch later down the line where we have a higher adoption rate for nvidia-docker2", "Yes @gunan , I guess you probably have the old `nvidia-docker` version installed.\r\n\r\nThe disadvantages of V1 that I see are:\r\n\r\n* [That version is now deprecated](https://github.com/NVIDIA/nvidia-docker/wiki/About-version-2.0), so it probably won't receive any more support.\r\n* As the installation instructions by default are now for V2 in [`nvidia-docker`](https://github.com/NVIDIA/nvidia-docker), newcomers (that sometimes are the main users of docs) could see that the instructions differ and that they don't work as straightforward as expected.\r\n* In V1, it's not possible to support Docker Compose or Docker in Swarm mode (for a distributed system). Of course it won't work with any other orchestrator (Kubernetes, Rancher, etc), as they expect to use Docker directly. But in V2 it's possible to set `nvidia` as the default \"runtime\", and then any other system that manages Docker (Docker Compose, Swarm, Kubernetes) will be able to set up the containers and use the GPU.\r\n\r\n---\r\n\r\nIf you want to keep the docs as they are, let me propose you to create a \"note\" in the end of the section explaining that if users use V2 they should switch the initial command.", "No, I verified that after i purged the old nvidia-docker and installed the new version.\r\n@flx42 for verification if `nvidia-docker` should still work.", "Here is what the nvidia-docker2 package installs:\r\n```\r\n$ dpkg -L nvidia-docker2\r\n/.\r\n/etc\r\n/etc/docker\r\n/etc/docker/daemon.json\r\n/usr\r\n/usr/bin\r\n/usr/bin/nvidia-docker\r\n/usr/share\r\n/usr/share/lintian\r\n/usr/share/lintian/overrides\r\n/usr/share/lintian/overrides/nvidia-docker2\r\n/usr/share/doc\r\n/usr/share/doc/nvidia-docker2\r\n/usr/share/doc/nvidia-docker2/copyright\r\n/usr/share/doc/nvidia-docker2/changelog.Debian.gz\r\n```\r\nPlease see `/usr/bin/nvidia-docker` above. When I check its contents it is identical to this file: https://github.com/NVIDIA/nvidia-docker/blob/master/nvidia-docker\r\n\r\nAlso querying further:\r\n```\r\n$ sudo nvidia-docker version\r\nNVIDIA Docker: 2.0.0\r\nClient:\r\n Version:      17.09.0-ce\r\n API version:  1.32\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:42:18 2017\r\n OS/Arch:      linux/amd64\r\n\r\nServer:\r\n Version:      17.09.0-ce\r\n API version:  1.32 (minimum version 1.12)\r\n Go version:   go1.8.3\r\n Git commit:   afdb6d4\r\n Built:        Tue Sep 26 22:40:56 2017\r\n OS/Arch:      linux/amd64\r\n Experimental: false\r\n```\r\n\r\nplease check your installation of nvidia-docker. I will close this PR as it complicates our documentation.", "Yes, V1 will still work.\r\nAnd V2 provides a wrapper script for backward compatibility, so your instructions should still work."]}, {"number": 14713, "title": "Estimator API and transfer learning/fine-tuning", "body": "I've been using the Estimator API with the model_fn and input_fn as shown in the official examples (https://github.com/tensorflow/models/blob/master/official/resnet/cifar10_main.py for instance).\r\n\r\nThis all looks great and wonderful. However, I'm now facing an issue for going further with it. I'd like to use a model trained on a dataset and transfer it to another dataset. In practice, I would like to take the weights from the trained model up to the softmax layer and only initialize randomly this final layer. Then, I can do fine-tuning on the new dataset, which has different labels for instance.\r\n\r\nI haven't found a way to do what I want. Is it something missing in the interface? Can we have something  like a variable list to restore from a checkpoint and some other not? Ideally, it would be also good to specify variables to be frozen. Does that all make sense?", "comments": ["I'have been able to restore weights via a custom tf.train.SessionRunHook. I customize the `after_create_session` method to run an init function generated with `tf.contrib.framework.assign_from_checkpoint_fn`. You can give it a list of variables you wish to restore. You can then return the hook with your `EstimatorSpec` structure.\r\n``` python\r\nclass RestoreHook(tf.train.SessionRunHook):\r\n    def __init__(self, init_fn):\r\n        self.init_fn = init_fn\r\n\r\n    def after_create_session(self, session, coord=None):\r\n        if session.run(tf.train.get_or_create_global_step()) == 0:\r\n            self.init_fn(session)\r\n\r\ndef model_fn():\r\n    ...\r\n    init_fn = assign_from_checkpoint_fn(model_path, var_list, ignore_missing_vars=True)\r\n    ...\r\n    return EstimatorSpec(..., training_hooks=[RestoreHook(init_fn)])\r\n```\r\nAbout generating the variables to restore list, `tf.contrib.framework` has some useful functions like `get_variables_to_restore`, `get_variables_by_name`, etc.\r\n", "Thanks for the hint, I'll try it as soon as possible. What I just implemented:\r\n\r\ndef model_fn():\r\n    ....\r\n    var_to_restore = []\r\n    for var in tf.trainable_variables():\r\n        if var.name.startswith('my_var'):\r\n            var_to_restore.append(var)\r\n\r\n    checkpoint_state = tf.train.get_checkpoint_state('pretrained')\r\n    input_checkpoint = checkpoint_state.model_checkpoint_path\r\n    pretrain_saver = tf.train.Saver(var_to_restore)\r\n\r\n    def init_fn(scaffold, session):\r\n        pretrain_saver.restore(session, input_checkpoint)\r\n\r\n    scaffold = tf.train.Scaffold(init_fn=init_fn)\r\n\r\n    return tf.estimator.EstimatorSpec(\r\n        mode=mode,\r\n        predictions=predictions,\r\n        loss=loss,\r\n        train_op=train_op,\r\n        eval_metric_ops=metrics,\r\n        scaffold=scaffold)\r\n\r\nLet's see whether this works too. But anyway, this requires more documentation or examples. Even a simpler interface, because this seems to me like a basic use.", "My method seems to work.\r\n\r\nI leave this issue open if a member from the tensorflow development team want to jump in or tag some enhancement.", "Glad you worked out your issue.  But for future reference, \r\n\r\n-----\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.", "@tatatodd I don't agree to your post. The issue I've posted here is to be understood as a feature request, either on the documentation side or on the code side. Typically, I would love to see a parameter in the constructor of the Estimator that lets us use pre-trained models, with variable selector. It is a very common use case and I find it ugly the way it has to be done.\r\nAnother remark, I spent quite some time digging a solution in Google search, stack overflow, etc. Strangely, I've found many users having the same question as I have, but no answer so far. So, I think my answer here should benefit the whole community.\r\n\r\n\r\n\r\n", "@jmaye My apologies, it wasn't clear to me that you were requesting a documentation or code feature.  And note that closing an issue doesn't make it less beneficial to the community, it just helps us keep track of what issues still need to be resolved.\r\n\r\nPerhaps @ispirmustafa can comment on your proposed changes to Estimator.", "@jmaye best way to do transfer learning for now is using [tf.train.init_from_checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/init_from_checkpoint). Example:\r\n\r\n```\r\nwith tf.variable_scope('transferred'):\r\n  # build transferred part of the graph\r\ntf.train.init_from_checkpoint(checkpoint_path, {'old_scope': 'transferred'})\r\n# rest should work seamless. \r\n```", "FYI, we're working on better solutions.", "@ispirmustafa Any other reference to track this?", "I think that we could reopen this untill we have a new issue or a PR to track this feature.", "Hi @bhack \r\nI've closed it since I think answer with current utilities is sufficient. If you think tf.train.init_from_checkpoint does not solve your problem, please re open it.", "I've was just referring to your [last comment ](https://github.com/tensorflow/tensorflow/issues/14713#issuecomment-349477199). So is there a follow-up?", "Also vaguely related https://github.com/tensorflow/tensorflow/issues/10155 ", "@jmaye I had the exact same use-case and tried to figure out how to use init_from_checkpoint as suggested (and it's really not as trivial as it sounded).\r\nEventually I got it working, see my answer here: https://stackoverflow.com/q/47867748/3214872", "There is an incoming solution that should be more easily configurable than init_from_checkpoint.  It will work for canned Estimators and be extensible to custom Estimators / model_fns. ", "@eddie-zhou Will it cover estimators generated with https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator?", "Sorry for the long delay -- yes, there will be support for that as well.", "Will it be in 1.5?", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Hi, I used method in my model_fn provided by @GPhilo \r\nThe problem is that it will run init_from_checkpoint every time whether in TRAIN mode or EVA mode.\r\n\r\nIs there any way to run init_from_checpoint once at the very beginning before the first train_op start?", "In 1.6, all Estimators will have native warm-start capabilities (only called in TRAIN).  See here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L102\r\n", "Can we close this issue?", "Fine for me, I'm still on 1.5 with the aforementioned code. Not urgent as it works as it is. I'll update the code as soon as I switch to 1.6. Thanks", "Can we have an example of how to use `tf.estimator.WarmStartSettings` when we want to warn start all variables under a given scope. The current documentation says that we have to set the `var_name_to_prev_var_name` but this dictionary is not easy to create when the model is large (e.g a resnet). It would be better if we could do the same thing as with `init_from_checkpoint(ckpt, {'/': 'resnet'}`)", "Can you clarify exactly how your variables are named, both in your model and in the checkpoint you're trying to warm-start from?", "Hello, everyone!\r\nI'm new to TensorFlow and meet these problem too. I build VGG16 Network for [CUB-200](http://www.vision.caltech.edu/visipedia/CUB-200.html) and try to fine-tuning it with `.npy` weights from caffe(maybe).\r\nNow, I try like that:\r\n```\r\ndef load_weights(self, weight_file, sess):\r\n        weights = np.load(weight_file)\r\n        keys = sorted(weights.keys())\r\n        for i, k in enumerate(keys):\r\n            print (i, k, np.shape(weights[k]))\r\n            if self.num_output != 1000 and (k == 'fc8_W' or k =='fc8_b'):\r\n                print('pass layer', k)\r\n                continue\r\n            sess.run(self.parameters[i].assign(weights[k]))\r\n            print (self.parameters[i].name, self.parameters[i].shape)\r\n```\r\n\r\nBut I will need a session to do the parameters assignment. I'm confused about how to get the **same**  session with `tf.estimator`.\r\n", "@Bowenwu1 that question belongs to stackoverflow, you won't get a reply here, since your problem has nothing to do with this specific issue.\r\nI suggest you to open a question on SO, it's usually rather fast to get an answer there for these questions ;)", "How about loading weights from checkpoints in the `slim` or `object detection` zoo.?\r\n\r\nI'm working on shifting most of my networks from `tf.contrib.slim` to `Estimators+Layers` and this would be really useful.\r\n\r\n", "For transfer learning with Estimator, I recommend to checkout following two options:\r\n* [Tensorflow Hub](https://github.com/tensorflow/hub/blob/master/tensorflow_hub) either directly hub layers or hub feature columns\r\n* [warm_start_from](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L109) argument of `Estimator`.", "@ispirmustafa I think both don't support estimators coming from tf.keras models (model_to_estimator).", "then one path would be using keras utilities before running model_to_estimator. @fchollet to comment about transfer learning functionalities you can use with tf.keras.Model.\r\n  ", "Or take more care on what is supported or not in model_to_estimator: https://github.com/tensorflow/tensorflow/issues/19295 cause `model_to_estimator` is becoming to be a sort of \"second class\" estimator.", "@GPhilo \r\nThank you for you reply. I still think TensorFlow should have more doc on **Transfer Learning**. Since seldom people train model from scratch, it's a quite common request.", "Hello!\r\n\r\nI've been trying to use warm start to load my model and fine-tune it on some other dataset. It works fine, except when I want to \"freeze\" layers in the model by making the relevant variables non-trainable, and train only the other unfrozen layers. In this case, the non-trainable variables do not get warm-started, even when the variable names match. It also gives an error when I specify those variables manually in the dictionary for `var_name_to_prev_var_name` argument.\r\n\r\nIs there any way to warm-start the non-trainable variables?", "You can explicitly pass in a list of Variables or list of strings if you want to avoid regexp + TRAINABLE_VARIABLES ([docstring](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/warm_starting_util.py#L315)).", "@eddie-zhou This feature does not seem to be available in version 1.8.0, which is the latest version of tensorflow-gpu on PyPI. It worked when I downloaded the file `tensorflow/python/training/warm_starting_util.py`, but now I'm getting errors with importing trainable variables. Now, it gives me the following error:\r\n```python\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmp/tmpzrdp8yhb/', vars_to_warm_start=['0_5_two/conv_w', 'batch_normalization_21/gamma', '1_8_one/conv_hebb', 'batch_normalization_50/moving_mean', 'batch_normalization_21/moving_mean', 'batch_normalization_14/moving_variance', '2_5_two/conv_b', '1_7_two/conv_w', '1_6_two/conv_hebb', '2_8_two/conv_b', 'batch_normalization_8/moving_mean', '2_3_two/conv_alpha', 'batch_normalization_24/gamma', 'batch_normalization_3/gamma', '0_1_one/conv_hebb', 'batch_normalization_4/moving_variance', 'batch_normalization_52/gamma', '0_4_two/conv_alpha', '0_2_one/conv_w', 'batch_normalization_45/beta', 'batch_normalization_4/gamma', 'batch_normalization_2/gamma', '2_1_two/conv_alpha', '2_8_two/conv_w', 'batch_normalization_24/moving_variance', '2_6_one/conv_w', '1_6_one/conv_alpha', 'batch_normalization_5/gamma', '1_2_two/conv_b', 'batch_normalization_44/moving_variance', '2_6_two/conv_b', '1_0_two/conv_b', 'batch_normalization_31/moving_mean', 'batch_normalization_10/moving_mean', 'batch_normalization_45/gamma', '0_7_two/conv_w', '2_2_one/conv_w', 'batch_normalization_50/beta', 'batch_normalization_4/moving_mean', 'batch_normalization_17/beta', '2_4_two/conv_alpha', '0_2_one/conv_alpha', '1_0_one/conv_alpha', 'batch_normalization_33/gamma', 'batch_normalization_35/moving_variance', '0_6_one/conv_alpha', '2_3_two/conv_w', '0_8_two/conv_alpha', 'batch_normalization_4/beta', '2_4_one/conv_b', 'batch_normalization_49/moving_mean', '1_7_one/conv_b', 'batch_normalization_35/beta', '2_2_one/conv_b', 'batch_normalization_22/moving_mean', '2_2_two/conv_alpha', 'batch_normalization_34/moving_variance', 'batch_normalization_53/moving_mean', '2_6_one/conv_b', 'batch_normalization_9/gamma', '1_0_two/conv_w', '2_3_one/conv_alpha', 'batch_normalization_26/moving_variance', 'batch_normalization_20/moving_variance', 'batch_normalization_17/gamma', '1_2_one/conv_alpha', 'batch_normalization_33/moving_mean', 'batch_normalization_39/beta', 'batch_normalization_23/moving_mean', 'batch_normalization_6/gamma', 'batch_normalization_17/moving_mean', 'Variable_3', '1_8_one/conv_alpha', 'batch_normalization_2/moving_mean', '1_1_one/conv_b', '0_4_one/conv_hebb', 'batch_normalization_53/gamma', 'batch_normalization_7/gamma', 'batch_normalization/moving_mean', '2_2_one/conv_hebb', 'batch_normalization_32/moving_mean', 'batch_normalization_13/beta', 'batch_normalization_35/moving_mean', '0_6_two/conv_w', '1_0_one/conv_hebb', 'batch_normalization_52/beta', 'batch_normalization_31/beta', '1_1_one/conv_alpha', 'batch_normalization_26/gamma', 'batch_normalization_18/gamma', '1_7_two/conv_b', 'batch_normalization_15/beta', '1_3_one/conv_alpha', '0_7_one/conv_hebb', '0_0_two/conv_w', 'batch_normalization_47/gamma', '0_3_one/conv_b', '0_2_one/conv_b', 'batch_normalization_24/beta', '2_7_one/conv_hebb', 'batch_normalization_5/beta', 'batch_normalization_10/beta', 'batch_normalization_34/beta', '1_0_one/conv_w', 'batch_normalization_51/moving_mean', '2_7_one/conv_b', '2_8_two/conv_hebb', '0_8_one/conv_b', '0_6_two/conv_b', 'batch_normalization_5/moving_mean', '2_4_one/conv_hebb', 'batch_normalization_19/moving_mean', '1_1_two/conv_b', '1_4_one/conv_w', '0_5_two/conv_hebb', 'beta2_power', 'batch_normalization_23/moving_variance', '2_0_three/conv_b', '0_0_one/conv_alpha', 'batch_normalization_6/moving_variance', '0_7_one/conv_w', '2_0_one/conv_alpha', 'first/conv_b', 'Variable_4/Adam', '1_1_one/conv_w', 'batch_normalization_25/beta', '0_4_one/conv_b', 'batch_normalization_20/moving_mean', '1_0_three/conv_w', '0_0_one/conv_hebb', 'batch_normalization_48/beta', '0_3_two/conv_w', '0_1_two/conv_w', 'batch_normalization_37/beta', 'batch_normalization_47/moving_mean', '0_0_two/conv_hebb', 'batch_normalization_28/moving_variance', '2_0_one/conv_b', '2_8_one/conv_alpha', 'Variable_2/Adam_1', 'batch_normalization_44/moving_mean', 'batch_normalization_44/gamma', 'batch_normalization_42/beta', '0_2_two/conv_w', '0_5_one/conv_b', 'batch_normalization_36/beta', 'batch_normalization_54/moving_mean', 'batch_normalization_42/gamma', 'batch_normalization_15/gamma', '1_8_two/conv_alpha', '0_4_two/conv_b', '2_7_one/conv_w', 'batch_normalization_23/beta', 'batch_normalization_33/beta', 'batch_normalization_49/beta', '2_0_one/conv_w', '2_8_one/conv_b', 'batch_normalization_54/beta', 'batch_normalization_29/moving_variance', '1_0_two/conv_alpha', 'first/conv_w', 'batch_normalization_13/gamma', 'batch_normalization_41/beta', 'Variable_4', 'batch_normalization_48/moving_variance', '0_1_one/conv_w', 'batch_normalization_41/gamma', 'batch_normalization_10/moving_variance', '1_7_two/conv_alpha', 'batch_normalization_29/moving_mean', '1_5_one/conv_b', 'batch_normalization_47/moving_variance', '1_6_one/conv_hebb', 'batch_normalization_13/moving_mean', 'batch_normalization_50/gamma', 'batch_normalization_18/moving_variance', '2_7_two/conv_hebb', 'batch_normalization_25/moving_variance', 'batch_normalization_37/moving_variance', 'batch_normalization_5/moving_variance', 'batch_normalization_46/moving_variance', '1_3_one/conv_w', 'batch_normalization_25/moving_mean', 'batch_normalization_20/gamma', '0_6_one/conv_hebb', '2_0_two/conv_hebb', '2_8_two/conv_alpha', 'batch_normalization_31/moving_variance', 'batch_normalization_7/moving_variance', '0_3_one/conv_hebb', '2_0_one/conv_hebb', 'batch_normalization_51/gamma', 'first/conv_hebb', '0_7_two/conv_alpha', 'batch_normalization_6/moving_mean', 'batch_normalization_32/beta', '1_3_two/conv_alpha', 'batch_normalization_1/moving_mean', '1_2_one/conv_w', '2_6_two/conv_w', '1_7_two/conv_hebb', 'batch_normalization_29/gamma', '2_6_one/conv_alpha', '1_6_one/conv_b', '0_2_two/conv_alpha', 'batch_normalization/moving_variance', '0_4_one/conv_alpha', '0_5_one/conv_w', 'batch_normalization_54/gamma', 'batch_normalization_41/moving_variance', 'batch_normalization_43/moving_variance', 'batch_normalization_22/gamma', '0_8_one/conv_alpha', '0_8_two/conv_hebb', 'batch_normalization_48/moving_mean', '1_4_two/conv_alpha', '0_3_one/conv_alpha', '2_7_one/conv_alpha', 'batch_normalization_12/beta', '2_0_two/conv_alpha', '1_8_two/conv_w', '2_5_one/conv_w', '1_3_one/conv_hebb', '0_4_one/conv_w', 'batch_normalization_54/moving_variance', 'batch_normalization_3/moving_variance', '0_5_one/conv_hebb', 'batch_normalization_9/beta', '2_4_two/conv_b', 'batch_normalization_42/moving_mean', '1_8_two/conv_hebb', '1_6_two/conv_b', 'batch_normalization_9/moving_variance', 'batch_normalization_39/gamma', 'batch_normalization_6/beta', 'batch_normalization_38/gamma', 'batch_normalization_46/beta', '0_8_two/conv_b', '2_2_two/conv_hebb', 'batch_normalization_40/moving_variance', '1_1_one/conv_hebb', 'batch_normalization_8/gamma', 'batch_normalization_27/moving_mean', '0_5_two/conv_alpha', '2_8_one/conv_w', '0_3_one/conv_w', 'batch_normalization_46/moving_mean', '1_6_one/conv_w', 'batch_normalization_38/beta', '2_2_two/conv_w', '0_7_one/conv_b', 'batch_normalization_28/gamma', 'batch_normalization_45/moving_variance', 'batch_normalization/beta', '0_1_one/conv_b', '2_6_two/conv_hebb', '2_3_one/conv_b', '1_1_two/conv_hebb', 'batch_normalization_16/moving_variance', '1_3_two/conv_b', 'batch_normalization_44/beta', 'batch_normalization_22/moving_variance', 'batch_normalization_11/beta', '1_5_one/conv_alpha', '2_7_two/conv_alpha', 'batch_normalization_16/gamma', 'batch_normalization_7/beta', '1_1_two/conv_alpha', 'batch_normalization_24/moving_mean', '0_1_two/conv_b', 'batch_normalization_37/moving_mean', 'Variable_2/Adam', 'Variable_1/Adam', 'batch_normalization_8/beta', '0_0_two/conv_alpha', '2_3_one/conv_hebb', 'batch_normalization_49/moving_variance', '1_2_one/conv_b', '2_3_one/conv_w', '0_7_one/conv_alpha', '2_4_one/conv_w', '0_1_one/conv_alpha', 'batch_normalization_30/beta', '1_0_three/conv_b', 'batch_normalization_32/gamma', '1_4_two/conv_w', 'batch_normalization_52/moving_variance', '2_1_two/conv_b', '2_1_one/conv_b', '2_5_two/conv_alpha', '1_0_two/conv_hebb', '1_4_one/conv_b', '1_1_two/conv_w', 'batch_normalization_43/moving_mean', '2_0_two/conv_b', '2_3_two/conv_hebb', 'Variable', 'batch_normalization_53/beta', 'batch_normalization_25/gamma', '1_2_two/conv_hebb', 'beta1_power', 'batch_normalization_38/moving_variance', 'batch_normalization_36/moving_variance', 'batch_normalization_2/moving_variance', '2_4_one/conv_alpha', 'batch_normalization_45/moving_mean', '0_3_two/conv_b', '0_1_two/conv_alpha', '1_8_one/conv_b', '0_0_one/conv_b', 'batch_normalization_2/beta', 'batch_normalization_12/moving_variance', '2_5_one/conv_hebb', 'batch_normalization_19/moving_variance', 'batch_normalization_3/moving_mean', '0_8_one/conv_hebb', 'batch_normalization_26/beta', '2_4_two/conv_w', 'batch_normalization_36/moving_mean', '1_3_two/conv_hebb', '2_0_three/conv_w', '1_5_two/conv_b', '0_8_one/conv_w', 'batch_normalization_12/gamma', '2_0_three/conv_alpha', '2_6_two/conv_alpha', '1_5_one/conv_hebb', 'batch_normalization/gamma', 'batch_normalization_1/gamma', 'batch_normalization_26/moving_mean', '2_2_two/conv_b', 'batch_normalization_14/beta', '1_3_two/conv_w', 'batch_normalization_28/moving_mean', 'batch_normalization_14/moving_mean', '0_4_two/conv_w', 'batch_normalization_11/gamma', '1_8_one/conv_w', '0_3_two/conv_hebb', 'batch_normalization_34/gamma', '1_5_two/conv_alpha', '2_5_two/conv_w', 'batch_normalization_31/gamma', '0_8_two/conv_w', 'batch_normalization_18/moving_mean', '1_4_one/conv_hebb', 'batch_normalization_1/moving_variance', '2_1_one/conv_w', 'batch_normalization_15/moving_variance', 'batch_normalization_40/gamma', '2_1_one/conv_alpha', '2_1_one/conv_hebb', '2_2_one/conv_alpha', 'batch_normalization_37/gamma', 'batch_normalization_19/beta', '2_0_three/conv_hebb', '0_3_two/conv_alpha', '2_1_two/conv_w', 'first/conv_alpha', 'batch_normalization_11/moving_variance', '0_6_one/conv_b', 'batch_normalization_43/beta', 'batch_normalization_3/beta', 'batch_normalization_51/moving_variance', 'batch_normalization_27/moving_variance', 'batch_normalization_11/moving_mean', 'batch_normalization_33/moving_variance', '2_5_one/conv_b', 'batch_normalization_16/moving_mean', 'batch_normalization_51/beta', '0_7_two/conv_b', '0_1_two/conv_hebb', '1_6_two/conv_w', '2_5_one/conv_alpha', '0_0_one/conv_w', 'batch_normalization_18/beta', 'batch_normalization_10/gamma', 'batch_normalization_30/gamma', 'batch_normalization_20/beta', 'Variable_2', 'batch_normalization_34/moving_mean', 'batch_normalization_19/gamma', 'batch_normalization_14/gamma', 'batch_normalization_8/moving_variance', 'batch_normalization_13/moving_variance', '1_8_two/conv_b', 'Variable_4/Adam_1', 'batch_normalization_27/gamma', 'batch_normalization_38/moving_mean', 'Variable_1/Adam_1', '1_3_one/conv_b', 'batch_normalization_16/beta', 'batch_normalization_53/moving_variance', '1_2_one/conv_hebb', 'batch_normalization_12/moving_mean', 'batch_normalization_7/moving_mean', '1_4_two/conv_hebb', '1_4_one/conv_alpha', '2_7_two/conv_b', 'batch_normalization_23/gamma', '0_6_two/conv_hebb', '0_4_two/conv_hebb', '0_7_two/conv_hebb', 'batch_normalization_36/gamma', 'batch_normalization_35/gamma', 'batch_normalization_43/gamma', 'batch_normalization_21/beta', '1_4_two/conv_b', 'batch_normalization_48/gamma', 'batch_normalization_41/moving_mean', '1_0_one/conv_b', '0_5_one/conv_alpha', 'batch_normalization_22/beta', 'batch_normalization_29/beta', 'batch_normalization_40/moving_mean', '0_2_two/conv_b', 'batch_normalization_21/moving_variance', 'batch_normalization_32/moving_variance', 'batch_normalization_1/beta', 'batch_normalization_46/gamma', 'batch_normalization_9/moving_mean', 'batch_normalization_42/moving_variance', '1_7_one/conv_hebb', 'batch_normalization_40/beta', '2_6_one/conv_hebb', '1_0_three/conv_alpha', '1_7_one/conv_w', '0_0_two/conv_b', 'batch_normalization_15/moving_mean', '1_5_one/conv_w', '0_2_two/conv_hebb', '1_5_two/conv_hebb', 'batch_normalization_50/moving_variance', 'batch_normalization_52/moving_mean', '1_7_one/conv_alpha', 'batch_normalization_28/beta', '2_1_two/conv_hebb', '2_7_two/conv_w', 'batch_normalization_17/moving_variance', 'batch_normalization_27/beta', '0_2_one/conv_hebb', 'batch_normalization_30/moving_mean', '0_5_two/conv_b', 'batch_normalization_39/moving_variance', '1_0_three/conv_hebb', '0_6_one/conv_w', '1_5_two/conv_w', 'batch_normalization_49/gamma', 'batch_normalization_30/moving_variance', '1_6_two/conv_alpha', 'Variable_1', 'batch_normalization_39/moving_mean', '2_5_two/conv_hebb', '2_0_two/conv_w', 'batch_normalization_47/beta', '1_2_two/conv_alpha', '2_4_two/conv_hebb', '1_2_two/conv_w', '2_8_one/conv_hebb', '2_3_two/conv_b', '0_6_two/conv_alpha'], var_name_to_vocab_info={}, var_name_to_prev_var_name={})\r\n\r\nINFO:tensorflow:Warm-starting from: ('/tmp/tmpzrdp8yhb/',)\r\nINFO:tensorflow:Warm-starting variable: 0_5_two/conv_w; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_5_two/conv_w:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_5_two/conv_w\r\nINFO:tensorflow:Warm-starting variable: 2_0_one/conv_w/Adam; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_0_one/conv_w/Adam:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_0_one/conv_w/Adam\r\nINFO:tensorflow:Warm-starting variable: 2_3_two/conv_b/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_3_two/conv_b/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_3_two/conv_b/Adam_1\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_9/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_9/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_9/moving_variance\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_21/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_21/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_21/gamma\r\nINFO:tensorflow:Warm-starting variable: 1_8_one/conv_hebb; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_8_one/conv_hebb:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_8_one/conv_hebb\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_6/beta; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_6/beta:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_6/beta\r\nINFO:tensorflow:Warm-starting variable: 2_0_three/conv_b/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_0_three/conv_b/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_0_three/conv_b/Adam_1\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_38/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_38/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_38/gamma\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_53/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_53/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_53/moving_variance\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_50/moving_mean; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_50/moving_mean:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_50/moving_mean\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_46/beta; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_46/beta:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_46/beta\r\nINFO:tensorflow:Warm-starting variable: 0_8_two/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_8_two/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_8_two/conv_b\r\nINFO:tensorflow:Warm-starting variable: 0_7_two/conv_w; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_7_two/conv_w:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_7_two/conv_w\r\nINFO:tensorflow:Warm-starting variable: 2_6_one/conv_alpha/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_6_one/conv_alpha/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_6_one/conv_alpha/Adam_1\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_21/moving_mean; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_21/moving_mean:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_21/moving_mean\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_15/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_15/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_15/moving_variance\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_3/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_3/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_3/gamma\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_48/gamma/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_48/gamma/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_48/gamma/Adam_1\r\nINFO:tensorflow:Warm-starting variable: 2_5_one/conv_hebb; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_5_one/conv_hebb:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_5_one/conv_hebb\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_36/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_36/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_36/gamma\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_23/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_23/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_23/moving_variance\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_14/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_14/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_14/moving_variance\r\nINFO:tensorflow:Warm-starting variable: 2_5_two/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_5_two/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_5_two/conv_b\r\nINFO:tensorflow:Warm-starting variable: 1_7_two/conv_w; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_7_two/conv_w:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_7_two/conv_w\r\nINFO:tensorflow:Warm-starting variable: 1_5_two/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_5_two/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_5_two/conv_b\r\nINFO:tensorflow:Warm-starting variable: 2_7_two/conv_w/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_7_two/conv_w/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_7_two/conv_w/Adam_1\r\nINFO:tensorflow:Warm-starting variable: 1_1_one/conv_hebb; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_1_one/conv_hebb:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_1_one/conv_hebb\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_39/gamma/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_39/gamma/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_39/gamma/Adam_1\r\nINFO:tensorflow:Warm-starting variable: 1_6_two/conv_hebb; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_6_two/conv_hebb:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_6_two/conv_hebb\r\nINFO:tensorflow:Warm-starting variable: 2_8_two/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_8_two/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_8_two/conv_b\r\nINFO:tensorflow:Warm-starting variable: 1_0_three/conv_alpha; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_0_three/conv_alpha:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_0_three/conv_alpha\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_27/moving_mean; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_27/moving_mean:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_27/moving_mean\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_39/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_39/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_39/gamma\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_8/moving_mean; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_8/moving_mean:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_8/moving_mean\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_46/beta/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_46/beta/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_46/beta/Adam_1\r\nINFO:tensorflow:Warm-starting variable: 2_8_one/conv_w; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_8_one/conv_w:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_8_one/conv_w\r\n\r\nINFO:tensorflow:Warm-starting variable: 0_0_two/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_0_two/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_0_two/conv_b\r\nINFO:tensorflow:Warm-starting variable: 2_5_one/conv_alpha/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_5_one/conv_alpha/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_5_one/conv_alpha/Adam_1\r\nINFO:tensorflow:Warm-starting variable: 0_3_one/conv_w; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_3_one/conv_w:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_3_one/conv_w\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_46/gamma/Adam; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_46/gamma/Adam:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_46/gamma/Adam\r\nINFO:tensorflow:Warm-starting variable: 2_3_two/conv_alpha; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_3_two/conv_alpha:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_3_two/conv_alpha\r\nINFO:tensorflow:Warm-starting variable: 2_0_two/conv_alpha/Adam; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_0_two/conv_alpha/Adam:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_0_two/conv_alpha/Adam\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_24/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_24/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_24/gamma\r\nINFO:tensorflow:Warm-starting variable: 2_1_two/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_1_two/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_1_two/conv_b\r\nINFO:tensorflow:Warm-starting variable: 2_0_two/conv_b/Adam; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_0_two/conv_b/Adam:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_0_two/conv_b/Adam\r\nINFO:tensorflow:Warm-starting variable: 2_8_two/conv_w/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_8_two/conv_w/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_8_two/conv_w/Adam_1\r\nINFO:tensorflow:Warm-starting variable: 0_1_one/conv_hebb; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_1_one/conv_hebb:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_1_one/conv_hebb\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_4/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_4/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_4/moving_variance\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_28/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_28/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_28/gamma\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_45/moving_variance; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_45/moving_variance:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_45/moving_variance\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_42/gamma/Adam_1; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_42/gamma/Adam_1:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_42/gamma/Adam_1\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_15/moving_mean; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_15/moving_mean:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_15/moving_mean\r\nINFO:tensorflow:Warm-starting variable: 0_4_two/conv_alpha; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_4_two/conv_alpha:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_4_two/conv_alpha\r\nINFO:tensorflow:Warm-starting variable: 1_0_two/conv_hebb; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_0_two/conv_hebb:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_0_two/conv_hebb\r\nINFO:tensorflow:Warm-starting variable: batch_normalization/beta; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization/beta:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization/beta\r\nINFO:tensorflow:Warm-starting variable: 0_2_one/conv_w; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 0_2_one/conv_w:0 from checkpoint /tmp/tmpzrdp8yhb/ with 0_2_one/conv_w\r\nINFO:tensorflow:Warm-starting variable: 1_6_two/conv_alpha; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 1_6_two/conv_alpha:0 from checkpoint /tmp/tmpzrdp8yhb/ with 1_6_two/conv_alpha\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_45/beta; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_45/beta:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_45/beta\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_4/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_4/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_4/gamma\r\nINFO:tensorflow:Warm-starting variable: 2_2_two/conv_alpha/Adam; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_2_two/conv_alpha/Adam:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_2_two/conv_alpha/Adam\r\nINFO:tensorflow:Warm-starting variable: 2_3_one/conv_b; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_3_one/conv_b:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_3_one/conv_b\r\nINFO:tensorflow:Warm-starting variable: 2_7_two/conv_b/Adam; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable 2_7_two/conv_b/Adam:0 from checkpoint /tmp/tmpzrdp8yhb/ with 2_7_two/conv_b/Adam\r\nINFO:tensorflow:Warm-starting variable: batch_normalization_2/gamma; prev_var_name: Unchanged\r\nINFO:tensorflow:Initialize variable batch_normalization_2/gamma:0 from checkpoint /tmp/tmpzrdp8yhb/ with batch_normalization_2/gamma\r\nINFO:tensorflow:Warm-starting variable: Variable_2; prev_var_name: Unchanged\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-a56b5d459051> in <module>()\r\n----> 1 model.train(input_fn=lambda:svhn_input_fn(test_mat), hooks=[logging_hook], max_steps=1)\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    361 \r\n    362     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 363     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    364     logging.info('Loss for final step: %s.', loss)\r\n    365     return self\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n    841       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n    842     else:\r\n--> 843       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n    844 \r\n    845   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n    857       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n    858                                              hooks, global_step_tensor,\r\n--> 859                                              saving_listeners)\r\n    860 \r\n    861   def _train_model_distributed(self, input_fn, hooks, saving_listeners):\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/estimator/estimator.py in _train_with_estimator_spec(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\r\n    984                    (self._warm_start_settings,))\r\n    985       # pylint: disable=protected-access\r\n--> 986       warm_starting_util.warm_start(*self._warm_start_settings)\r\n    987       # pylint: enable=protected-access\r\n    988     # Check if the user created a loss summary, and add one if they didn't.\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/training/warm_starting_util.py in warm_start(ckpt_to_initialize_from, vars_to_warm_start, var_name_to_vocab_info, var_name_to_prev_var_name)\r\n    391         if len(variable) == 1:\r\n    392           variable = variable[0]\r\n--> 393         _warm_start_var(variable, ckpt_to_initialize_from, prev_var_name)\r\n    394 \r\n    395   prev_var_name_not_used = set(\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/training/warm_starting_util.py in _warm_start_var(var, prev_ckpt, prev_tensor_name)\r\n    130     current_var_name = _infer_var_name([var])\r\n    131   elif isinstance(var, list) and all(_is_variable(v) for v in var):\r\n--> 132     current_var_name = _infer_var_name(var)\r\n    133   elif isinstance(var, variables_lib.PartitionedVariable):\r\n    134     current_var_name = _infer_var_name([var])\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/training/warm_starting_util.py in _infer_var_name(var)\r\n    103     Name of the `var`\r\n    104   \"\"\"\r\n--> 105   name_to_var_dict = saver.BaseSaverBuilder.OpListToDict(var)\r\n    106   if len(name_to_var_dict) > 1:\r\n    107     raise TypeError(\"`var` = %s passed as arg violates the constraints.  \"\r\n\r\n~/resnet_venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py in OpListToDict(op_list, convert_variable_to_tensor)\r\n    636           if name in names_to_saveables:\r\n    637             raise ValueError(\"At least two variables have the same name: %s\" %\r\n--> 638                              name)\r\n    639           names_to_saveables[name] = var\r\n    640 \r\n\r\nValueError: At least two variables have the same name: Variable_2\r\n```\r\nThe error occurs everytime I re-run the kernel in the notebook, with the error thrown for different variables everytime (all defined with tf.Variable). This happens when I'm trying to train an estimator just after defining the estimator with warm starting.", "Generally, steer towards using `tf.get_variable` instead of `tf.Variable` -- I suspect that's where your problems with duplicate names are coming from.", "The issue is sort-of solved. It still existed even after changing all instances of `tf.Variable` in the transfer learning model to `tf.get_variable`. Also, it only seemed to occur with trainable variables in the graph.\r\n\r\nTherefore I simply did not specify the trainable variables in the list passed to the `vars_to_warm_start` parameter. Now, even though they aren't in the list, they're still being warm-started. This is exactly what I had intended, but it still wouldn't allow me to warm-start non-trainable variables and ignore warm-starting for trainable ones.\r\n\r\nAlthough my issue has been solved, is there a way of getting the trainable variables to not warm-start automatically? Or is it probably due to the use of `tf.Variable` also in the checkpoint from where variables are warm-started?", "I have the same problem with you@Xyza1972 , when using \"tf.train.init_from_checkpoint\"  I found that the ckpt file has two types ,if the ckpt file is only one file like xxx.ckpt ,it can work well . But when the ckpt split to three files like \"model.ckpt-xxx.data\u3001model.ckpt-xxx.index\u3001model.ckpt-xxx.meta\", the problem would appear, for detail , it will restore the ckpt weights over and over again when a new epoch starts, which can not lead to a right way to get a perfect accuracy . Is that a bug or i missed something ?? Thanks for helping me .", "> I have the same problem with you@Xyza1972 , when using \"tf.train.init_from_checkpoint\" I found that the ckpt file has two types ,if the ckpt file is only one file like xxx.ckpt ,it can work well . But when the ckpt split to three files like \"model.ckpt-xxx.data\u3001model.ckpt-xxx.index\u3001model.ckpt-xxx.meta\", the problem would appear, for detail , it will restore the ckpt weights over and over again when a new epoch starts, which can not lead to a right way to get a perfect accuracy . Is that a bug or i missed something ?? Thanks for helping me .\r\n\r\nHello, I meet the same problem. Did you find the solution? Thanks", "@thalitadru Hi, what's your version of tensorflow? I restore variables like what you do leveraging `tf.train.SessionRunHook` with tensorflow 1.15, but it throws the error that key `h3/bias` not in the checkpoint, key `h3/bias` is the variable in my new network which is not in the checkpoint. Anyone has the same problem?", "So is warmstartsettings or init_from_checkpoint recommended for TL? "]}, {"number": 14712, "title": "cmake error on MacOS", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **Python version**: python 3.6.2\r\n- **TensorFlow version**: master\r\n- **Bazel version**: 0.7.0-homebrew. Build timestamp: 1510456291\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.38)\r\n- **Exact command to reproduce**:\r\n```\r\n# in tensorflow directory\r\ncd tensorflow/contrib/cmake\r\nmkdir build\r\ncd build\r\ncmake .. -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=/usr/local/bin/python3\r\nmake tf_tutorials_example_trainer\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI'm following the instructions [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) to build using cmake on Mac. However during make, the following error is thrown.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n# ...\r\nScanning dependencies of target tf_tutorials_example_trainer\r\n[100%] Building CXX object CMakeFiles/tf_tutorials_example_trainer.dir/Users/kevenwang/VirtualBoxShared/another_tf/tensorflow/cc/tutorials/example_trainer.cc.o\r\n[100%] Linking CXX executable tf_tutorials_example_trainer\r\nUndefined symbols for architecture x86_64:\r\n  \"_ares_cancel\", referenced from:\r\n      on_readable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n      on_writable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n  \"_ares_destroy\", referenced from:\r\n      grpc_ares_ev_driver_unref(grpc_ares_ev_driver*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n  \"_ares_free_data\", referenced from:\r\n      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      on_txt_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_gethostbyname\", referenced from:\r\n      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_getsock\", referenced from:\r\n      grpc_ares_notify_on_event_locked(grpc_exec_ctx*, grpc_ares_ev_driver*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n  \"_ares_inet_ntop\", referenced from:\r\n      on_hostbyname_done_cb(void*, int, int, hostent*) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_init\", referenced from:\r\n      _grpc_ares_ev_driver_create in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n     (maybe you meant: _grpc_ares_init, _grpc_resolver_dns_ares_init )\r\n  \"_ares_library_cleanup\", referenced from:\r\n      _grpc_ares_cleanup in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_library_init\", referenced from:\r\n      _grpc_ares_init in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_parse_srv_reply\", referenced from:\r\n      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_parse_txt_reply_ext\", referenced from:\r\n      on_txt_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_process_fd\", referenced from:\r\n      on_readable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n      on_writable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\n  \"_ares_query\", referenced from:\r\n      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_search\", referenced from:\r\n      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_set_servers_ports\", referenced from:\r\n      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n  \"_ares_strerror\", referenced from:\r\n      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      _grpc_ares_init in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      on_hostbyname_done_cb(void*, int, int, hostent*) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      on_txt_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)\r\n      _grpc_ares_ev_driver_create in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[3]: *** [tf_tutorials_example_trainer] Error 1\r\nmake[2]: *** [CMakeFiles/tf_tutorials_example_trainer.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/tf_tutorials_example_trainer.dir/rule] Error 2\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "updated original post", "CMake files are under contrib, and just like the rest of contrib, they are maintained as best effort (on windows we have a different support level for them).\r\nIs there a reason you are not using bazel?", "I intend to build on windows. However, I develop on Mac OS. So ideally I hope to cmake it on Mac. \r\n\r\nI have found MSBuild (configured with cmake) to be very slow on windows. Usually taking 10+ minutes for a minimal file change.", "This issue can be resolved by PR #17581 "]}, {"number": 14711, "title": "//tensorflow/python:session_list_devices_test fails on X86", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 16.04 \r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source\r\n- **TensorFlow version (use command below)**:\r\n      TF-1.3.1\r\n- **Python version**:\r\n      Python 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n      Bazel - 0.5.4\r\n- **CUDA/cuDNN version**:\r\n      NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n     ` bazel test --config=opt  //tensorflow/python:session_list_devices_test `\r\n\r\n### Describe the problem\r\nFollowing 3 sub-tests are failing on Ubuntu:16.04 (x86) with the assertion errors\r\n1) FAIL: testListDevices (__main__.SessionListDevicesWithCApiTest)\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/python/client/session_list_devices_test.py#L39\r\n `self.assertGreaterEqual(1, len(devices), devices)` .....# Getting AssertionError due to: \"1\" unexpectedly not greater than or equal to \"3\"  \r\n2) FAIL: testListDevicesGrpcSession (__main__.SessionListDevicesWithCApiTest)\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/python/client/session_list_devices_test.py#L47\r\n `self.assertGreaterEqual(1, len(devices), devices`) .....#  Getting AssertionError due to: \"1\" unexpectedly not greater than or equal to \"3\"  \r\n3) FAIL: testListDevicesClusterSpecPropagation (__main__.SessionListDevicesWithCApiTest)\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/python/client/session_list_devices_test.py#L66\r\n `self.assertGreaterEqual(2, len(devices), devices)` ..... #  Getting AssertionError due to: \"2\" unexpectedly not greater than or equal to \"6\"\r\n\r\nIs this is a known failure (can we ignore ) or I am missing something here ?. Please provide your comments on this.Thanks!\r\n### Source code / logs\r\n```\r\n\r\n$  bazel test --config=opt  //tensorflow/python:session_list_devices_test\r\n\r\n2017-11-20 08:35:46.791667: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:343] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\n2017-11-20 08:35:46.795931: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 026f646fb0c59742 with config:\r\nF.\r\n======================================================================\r\nFAIL: testListDevices (__main__.SessionListDevicesTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py\", line 39, in testListDevices\r\n    self.assertGreaterEqual(1, len(devices), devices)\r\nAssertionError: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 42115744), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42116496), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42116608)]\r\n\r\n======================================================================\r\nFAIL: testListDevicesClusterSpecPropagation (__main__.SessionListDevicesTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py\", line 66, in testListDevicesClusterSpecPropagation\r\n    self.assertGreaterEqual(2, len(devices), devices)\r\nAssertionError: [_DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 42238272), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42238864), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42238896), _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, 42238928), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_EXEC:0, XLA_EXEC, 42238960), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 42238992)]\r\n\r\n======================================================================\r\nFAIL: testListDevicesGrpcSession (__main__.SessionListDevicesTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py\", line 47, in testListDevicesGrpcSession\r\n    self.assertGreaterEqual(1, len(devices), devices)\r\nAssertionError: [_DeviceAttributes(/job:local/replica:0/task:0/device:CPU:0, CPU, 42226672), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 41891296), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 41891328)]\r\n\r\n======================================================================\r\nFAIL: testListDevices (__main__.SessionListDevicesWithCApiTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py\", line 39, in testListDevices\r\n    self.assertGreaterEqual(1, len(devices), devices)\r\nAssertionError: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 42268048), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42268800), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42268912)]\r\n\r\n======================================================================\r\nFAIL: testListDevicesClusterSpecPropagation (__main__.SessionListDevicesWithCApiTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py\", line 66, in testListDevicesClusterSpecPropagation\r\n    self.assertGreaterEqual(2, len(devices), devices)\r\nAssertionError: [_DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 42313280), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42313872), _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42313904), _DeviceAttributes(/job:worker/replica:0/task:1/device:CPU:0, CPU, 42313936), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_EXEC:0, XLA_EXEC, 42313968), _DeviceAttributes(/job:worker/replica:0/task:1/device:XLA_CPU:0, XLA_CPU, 42314000)]\r\n\r\n======================================================================\r\nFAIL: testListDevicesGrpcSession (__main__.SessionListDevicesWithCApiTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/0cb601a163d4c4c6c065cdaa9629611b/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/session_list_devices_test.runfiles/org_tensorflow/tensorflow/python/client/session_list_devices_test.py\", line 47, in testListDevicesGrpcSession\r\n    self.assertGreaterEqual(1, len(devices), devices)\r\nAssertionError: [_DeviceAttributes(/job:local/replica:0/task:0/device:CPU:0, CPU, 42260656), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_EXEC:0, XLA_EXEC, 42260688), _DeviceAttributes(/job:local/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 42331824)]\r\n\r\n----------------------------------------------------------------------\r\nRan 8 tests in 1.857s\r\n\r\nFAILED (failures=6)\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\n\r\n```", "comments": ["@sandipmgiri Thanks for filing the issue!\r\n\r\nIt looks like the test is only expecting to see the CPU device, but it's getting confused by the extra XLA_EXEC and XLA_CPU devices.  I'm assuming you built with XLA support enabled?\r\n\r\nIn the short-term it's safe to just ignore these failures, since I believe it's a problem with the tests themselves, not with the system.\r\n\r\nFYI @saeta who added these tests.  Perhaps the `assertGreaterEqual` should have been `assertLessEqual`?", "Hi @saeta , Please provide your comments on this.Thanks!"]}, {"number": 14710, "title": "Change ndimage.imread to imageio.imread.", "body": "Scipy will not support imread from 1.0.0 as its document says:\r\nhttps://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.misc.imread.html\r\n\r\nChange to imageio.imread and add its correspond exception.", "comments": ["Can one of the admins verify this patch?", "Thanks @zxcqwe4906!"]}, {"number": 14709, "title": "Include _solib_local for MKL-DNN libs", "body": "`_solib_local/libmklml_intel.so` is not getting included in the package.\r\n`build_pip_package.sh` has been updated to copy `*solib*` instead of just `_solib_k8`, so just update `setup.py` to include it.\r\nSee https://github.com/tensorflow/tensorflow/issues/13711 for details.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Change looks good, but could you take care of the CLA?", "u need to pass cla ~ i was there, now i passed , just follow me:\r\n#14867", "Hi, I tried your code and it didn't solve the 'libmklml_intel.so: cannot open...' issue. After having a look at the find_files(), I found out that this function's second argument don't support filename pattern matching like bash script do.  And I my case I just need to change '_solib_k8' to '_solib_local', haven't figured out a general way to fix it though.:confused:", "@ic0n You are completely right. I fixed it locally, but forgot to update this PR.", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 14708, "title": "android library not loading saved model due to invalid graphdef error.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.x\r\n- **TensorFlow installed from (source or binary)**:\r\nUsing VirtualEnv as described on website\r\n- **TensorFlow version (use command below)**:\r\n1.4 (also tried 1.0, and 1.2)\r\n- **Python version**: \r\n2.7.1.2\r\n- **Bazel version (if compiling from source)**:\r\nNA\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNA\r\n- **CUDA/cuDNN version**:\r\nNA\r\n- **GPU model and memory**:\r\nNA\r\n- **Exact command to reproduce**:\r\n\r\nI have a bug where a frozen model will not load in an android app. The code is at\r\n\r\nhttps://github.com/cooledge/Chatbot\r\n\r\nIn ChatActivity on line 39 I load an model that is trivial and built by running \"cd nn; python ./train_noop.py --epochs 5 --save_words --freeze\". That loads thus showing the Android app is setup correctly for loading models. On line 40 I load a model that I got from your tensorflow demo. That also loads showing that complex models can be loaded. On line 41 I load the model that I actually want to load and that fails with the error invalid GraphDef. That model is build by running 'cd nn; python ./train.py --epochs 5 --save_words --freeze\". Why is that not loading. It loads under python on ubuntu and can generate samples. The bug is repoducable by running the app after syncing you don't need to run the training.\r\n\r\n", "comments": []}, {"number": 14707, "title": "Fix docstring of variable_scope()", "body": "Add missing \"```\" to docstring.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14706, "title": "failed to bazel build tensorflow lite ", "body": "build tensorflow lite  demo with \"bazel --output_base=/data/wjx/bazel/tensorflow/output --output_user_root=/data/wjx/bazel/tensorflow build --cxxopt='--std=c++11'  //tensorflow/contrib/\r\nlite/java/demo/app/src/main:TfLiteCameraDemo\"\r\n\r\nEnvironment:\r\nOS: ubuntu 16.04\r\ntf version: tensorflow 1.4 master\r\npython:2.7.12\r\nAndroidSDK: 27 BuildToolsVersion: 27.0.1\r\nNDK: android-ndk-r14e\r\n\r\nERROR: /data/wjx/bazel/tensorflow/output/external/androidsdk/com.android.support/BUILD:4277:1: Merging Android resources for @androidsdk//com.android.support:support-compat-25.2.0 failed (Exit 1)\r\nNov 20, 2017 1:31:06 AM com.google.devtools.build.android.AndroidResourceMergingAction main\r\nSEVERE: Unexpected\r\njava.io.IOException: Mount point not found\r\n\tat sun.nio.fs.LinuxFileStore.findMountEntry(LinuxFileStore.java:91)\r\n\tat sun.nio.fs.UnixFileStore.<init>(UnixFileStore.java:65)\r\n\tat sun.nio.fs.LinuxFileStore.<init>(LinuxFileStore.java:44)\r\n\tat sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:51)\r\n\tat sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:39)\r\n\tat sun.nio.fs.UnixFileSystemProvider.getFileStore(UnixFileSystemProvider.java:368)\r\n\tat java.nio.file.Files.getFileStore(Files.java:1461)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.makeWritable(ScopedTemporaryDirectory.java:59)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:83)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:36)\r\n\tat java.nio.file.Files.walkFileTree(Files.java:2670)\r\n\tat java.nio.file.Files.walkFileTree(Files.java:2742)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.close(ScopedTemporaryDirectory.java:96)\r\n\tat com.google.devtools.build.android.AndroidResourceMergingAction.main(AndroidResourceMergingAction.java:289)\r\n\tat com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$7.call(ResourceProcessorBusyBox.java:91)\r\n\tat com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:172)\r\n\r\nException in thread \"main\" java.io.IOException: Mount point not found\r\n\tat sun.nio.fs.LinuxFileStore.findMountEntry(LinuxFileStore.java:91)\r\n\tat sun.nio.fs.UnixFileStore.<init>(UnixFileStore.java:65)\r\n\tat sun.nio.fs.LinuxFileStore.<init>(LinuxFileStore.java:44)\r\n\tat sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:51)\r\n\tat sun.nio.fs.LinuxFileSystemProvider.getFileStore(LinuxFileSystemProvider.java:39)\r\n\tat sun.nio.fs.UnixFileSystemProvider.getFileStore(UnixFileSystemProvider.java:368)\r\n\tat java.nio.file.Files.getFileStore(Files.java:1461)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.makeWritable(ScopedTemporaryDirectory.java:59)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:83)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.visitFile(ScopedTemporaryDirectory.java:36)\r\n\tat java.nio.file.Files.walkFileTree(Files.java:2670)\r\n\tat java.nio.file.Files.walkFileTree(Files.java:2742)\r\n\tat com.google.devtools.build.android.ScopedTemporaryDirectory.close(ScopedTemporaryDirectory.java:96)\r\n\tat com.google.devtools.build.android.AndroidResourceMergingAction.main(AndroidResourceMergingAction.java:289)\r\n\tat com.google.devtools.build.android.ResourceProcessorBusyBox$Tool$7.call(ResourceProcessorBusyBox.java:91)\r\n\tat com.google.devtools.build.android.ResourceProcessorBusyBox.main(ResourceProcessorBusyBox.java:172)\r\nTarget //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 16.750s, Critical Path: 3.39s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\ncan anyone help me?", "comments": ["I've discovered this too, running builds in Docker. I'm surprised, because a very similar environment didn't have this issue.", "@wujsy, are you using Docker for this?\r\n\r\nAlso, what Bazel version are you using? Can you add all of the requested information from the new issue template?", "This is a Bazel version problem. You're probably using Bazel 0.5.4 -- switching from that version to the latest Bazel version fixes the problem when I saw it in my builds.", "@angersson Thanks for your reply, Yes, I use Docker for building, my bazel version 0.7.0, but I can build sucessfully if do not use a Docker. So I doubt that is it a Docker problem?\r\nHere is my Docker system info:\r\n### System information\r\n- **Have I written custom code: No\r\n- **OS Platform and Distribution : Linux Ubuntu 16.04\r\n- **TensorFlow installed from: Source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: GPU K80 12G\r\n"]}, {"number": 14705, "title": "Complie error with cuda9.0+cudnn7.0+tensorflow-r1.4 on Ubuntu 16.04LTS", "body": "```\r\nERROR: /home/wavy/Ten/tensorflow/tensorflow/contrib/factorization/BUILD:116:1: Linking of rule '//tensorflow/contrib/factorization:gen_gen_factorization_ops_py_wrappers_cc' failed (Exit 1)\r\n```\r\nThis is the error output.\r\n\r\nThanks!\r\n", "comments": ["tf-1.4 did't support cudnn7.0. Only for cudnn6.0.", "> tf-1.4 did't support cudnn7.0. Only for cudnn6.0.\r\n\r\nI don't think this is the case as I built tensorflow 1.4.0 with CUDA 9.0, cuDNN v7 on Ubuntu 16.04 LTS without any issues (specified CUDA and cuDNN versions when running the `configure` script, then ran `bazel build -c opt --action_env PATH=\"$PATH\" //tensorflow/tools/pip_package:build_pip_package`).", "I didn't try cudnn7.0  but in the official release claim that tf didn't supprot for cudnn7.0.    ", "It actually says that the precompiled binaries were released with CUDA 8, it doesn't mention the issue of supporting it. The fix seemed to have been merged a while back, but it's still not clear if there is support for it or not. \r\n@MikulasZelinka - did you run it through some testing to see if it actually performs as expected?\r\nCan someone from the TF team respond and say if it should work properly when compiled against CUDA 9 and cuDNN 7?\r\nThanks \ud83d\udc4d ", "I had compiled successfully with cuda9.0+cudnn7.0 several days ago\uff0chowerver it had some warning, when I tested it with the simple code. I didn't success when I tried two days ago...", "@ofirbb I only ran some of my experiments (mostly with RNNs) and the results were identical to CUDA 8.0 and cuDNN v6. Which tensorflow tests should I run and how?", "@MikulasZelinka I meant something in the form of [running the TF unit tests](https://stackoverflow.com/questions/34204551/run-tensorflow-unit-tests)", "TensorFlow 1.4 supports CUDA 8, cuDNN 6.  Here are the configurations we support:\r\nhttps://www.tensorflow.org/install/install_sources#tested_source_configurations\r\n\r\nWe anticipate releasing TensorFlow 1.5 with CUDA 9 and cuDNN 7:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/RELEASE.md\r\n\r\nAlso note that this is a duplicate of #14244"]}, {"number": 14704, "title": "tf.data.Dataset.padded_batch() doesn't work with nested elements", "body": "### System information\r\n\r\nTF 1.4 (pip install)\r\nPython version 3.5.2 (Anaconda)\r\n\r\n### Problem description\r\n\r\n`tf.data.Dataset.padded_batch()` fails if a dataset element has some nested structure instead of being a tensor. Dataset API is supposed to work with Estimator's input_fn functionality which should return\r\nfeatures and labels as separate python objects and it is very inconvenient to merge everything into a single tensor, make a batch and then split.\r\n\r\n### Source\r\n\r\n    import tensorflow as tf\r\n    print(tf.__version__)    \r\n\r\n    dataset = tf.data.Dataset.range(100)\r\n    dataset = dataset.map(lambda x: {'x': tf.fill([tf.cast(x, tf.int32)], x),\r\n                                                           'y': tf.fill([tf.cast(x, tf.int32)], x)})\r\n    dataset = dataset.padded_batch(4, padded_shapes=[None])\r\n\r\n    iterator = dataset.make_one_shot_iterator()\r\n    next_element = iterator.get_next()\r\n\r\n    with tf.train.MonitoredSession() as sess:\r\n        print(sess.run(next_element))\r\n        print(sess.run(next_element))\r\n\r\n### Actual\r\n\r\n    ---------------------------------------------------------------------------\r\n    TypeError                                 Traceback (most recent call last)\r\n    <ipython-input-38-bb9f335976ed> in <module>()\r\n          2 dataset = dataset.map(lambda x: {'x': tf.fill([tf.cast(x, tf.int32)], x),\r\n          3                                  'y': tf.fill([tf.cast(x, tf.int32)], x)})\r\n    ----> 4 dataset = dataset.padded_batch(4, padded_shapes=[None])\r\n          5 \r\n          6 iterator = dataset.make_one_shot_iterator()\r\n\r\n    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py in padded_batch(self, batch_size, padded_shapes, padding_values)\r\n        693       A `Dataset`.\r\n        694     \"\"\"\r\n    --> 695     return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)\r\n        696 \r\n        697   def map(self, map_func, num_parallel_calls=None):\r\n\r\n    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, batch_size, padded_shapes, padding_values)\r\n       1290                       self._default_padding(input_dataset))\r\n       1291     self._padded_shapes = nest.map_structure_up_to(\r\n    -> 1292         input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)\r\n       1293     self._padding_values = nest.map_structure_up_to(\r\n       1294         input_dataset.output_shapes, _padding_value_to_tensor, padding_values,\r\n\r\n    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/util/nest.py in map_structure_up_to(shallow_tree, func, *inputs)\r\n        510     raise ValueError(\"Cannot map over no sequences\")\r\n        511   for input_tree in inputs:\r\n    --> 512     assert_shallow_structure(shallow_tree, input_tree)\r\n        513 \r\n        514   # Flatten each input separately, apply the function to corresponding elements,\r\n\r\n    ~/anaconda3/lib/python3.5/site-packages/tensorflow/python/data/util/nest.py in assert_shallow_structure(shallow_tree, input_tree, check_types)\r\n        354       raise TypeError(\r\n        355           \"If shallow structure is a sequence, input must also be a sequence. \"\r\n    --> 356           \"Input has type: %s.\" % type(input_tree))\r\n        357 \r\n        358     if check_types and not isinstance(input_tree, type(shallow_tree)):\r\n\r\n    TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <class 'list'>.\r\n\r\n### Expected\r\n\r\nIt should produce dictionary, where x and y values are batch tensors with proper paddings.\r\n", "comments": ["In fact, it does. The `padded_shapes=[None]` argument in your code snippet is wrong: that argument must match the structure of the input dataset, which means in this case that it must be a dictionary mapping `'x'` and `'y'` to the appropriately padded shapes. The following replacement works:\r\n\r\n```python\r\ndataset = dataset.padded_batch(4, padded_shapes={'x': [None], 'y': [None]})\r\n```", "@mrry, I'd really appreciate if you can help me with a query along this. I have a dataset having the element of the form **(features, targets)** where the features is itself a dictionary, say `features={'input1': Tensor, 'input2': Tensor}` and **targets is a tensor object**. How should my padded_shapes argument look like? \r\n\r\nThanks. ", "@mrry , [https://github.com/tensorflow/tensorflow/issues/21259](https://github.com/tensorflow/tensorflow/issues/21259) helped me figure this out. Thanks again. ", "Hi I'm [trying to figure this out](https://stackoverflow.com/questions/61642076/not-sure-why-im-getting-an-error-of-error-in-py-get-attr-implx-name-silent) in an R example. I'm getting the same issue and wondering how to implement the mapping. \r\n\r\n@mrry ", "In addtition, for fixed size of input and dynamic output like in detection, providing this format instead of single None, because the dimension has changed.\r\n`batch_size=batch_size, padded_shapes=([800, 800, 3],[None,4],[None]),padding_values=(0.0, 1e-8, -1), drop_remainder=True`"]}, {"number": 14703, "title": "tf.layers uses wrong variable scope", "body": "TF 1.4.\r\n```python\r\nimport tensorflow as tf\r\ndef f(x):\r\n    return tf.layers.conv2d(x, 30, 3)\r\n\r\nx = tf.zeros([3, 20, 20, 1])\r\n\r\nwith tf.variable_scope('a'):\r\n    print(f(x))\r\nwith tf.variable_scope('a', reuse=True):\r\n    print(f(x)) # works\r\n\r\nprint(f(x))\r\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\r\n    print(f(x)) # failed with:\r\n\"\"\"\r\nValueError: Variable conv2d_1/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\"\"\"\r\n```\r\n\r\nFrom what I can see, tf.layers is trying to create the variables under a wrong variable scope name, making variable sharing impossible if used under the root scope.\r\n\r\nI found that this works:\r\n```python\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n    print(f(x))\r\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\r\n    print(f(x)) # works\r\n```\r\nBut the first line seems redundant and counter-intuitive.\r\n\r\n//UPDATE:\r\nIt also works if I don't use tf.layers:\r\n```python\r\ndef f(x):\r\n    W = tf.get_variable('w', shape=[1])\r\n    return x + W\r\n```", "comments": ["Yes, I think the bug will be fixed when my PR  #14390 is merged, see the commit:  https://github.com/tensorflow/tensorflow/pull/14390/commits/c59e89ad2a523bfaf0143a945ccc0754a7975aa2", "I'm not sure I understand how they are related. Just to clarify, in my case I don't care what name scope is used inside the variable scope -- it can be either a new name scope or the original one. I just want the correct variable name, which ideally should be independent of the surrounding name scope.", "Oh, my bad. The commit mentioned above is to resolve `original_name_scope` bug with root scope. Perhaps it's unrelated.", "I don't think this is a bug. You use variable_scope in a wrong way.", "@x10000year Which part is wrong?", "Let's clarify it: do you expect that `layer_2` is `layer_1`?\r\n\r\n```python\r\nlayer_1 = tf.layers.conv2d(x, 30, 3)\r\nwith tf.variable_scope(tf.get_variable_scope(), reuse=True):\r\n  layer_2 = tf.layers.conv2d(x, 30, 3)\r\n```    ", "I thought the expectation is obvious. To clarify, if I write the following __naive__ \"add\" layer:\r\n```python\r\ndef f(x):\r\n    W = tf.get_variable('w', shape=[1])\r\n    return x + W\r\n```\r\nThen my snippet can run with my expected variable sharing behavior. But tf.layers is doing something different now.", "I think tf.layer should act like a class, instead of a function. I mean that tf.layer should maintain its own variable scope and name scope, which won't be effected by outside. So layer_1 and layer_2 should be different objects in my opinion. I believe @fchollet can give us the answer. ", "Yes perhaps @fchollet can describe what the expected behavior is.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "ping @fchollet .", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "The definitive answer on this issue is:\r\n\r\n- Yes, this is an issue.\r\n- We cannot fix it without breaking backwards compatibility.\r\n- We will fix it soon in `tf.keras`. It will never be fixed in `tf.layers` (since it is necessary to maintain existing code working -- which is not the case in Keras). `tf.layers` will eventually be removed (in TF 2.0).", "If tf.layers would be removed later, what should I use in the future? Is there any Tensorflow based middle level package like slim but supported officially by Google in later versions? I mean \"pure tensorflow package\", but not Keras.", "@ybsave If I'm not wrong, `tf.layers` will be replaced by `tf.keras.layers` in the future. ", "@facaiy Thank you for your quick response. Are there any official Tensorflow documents about using the Tensorboard under tf.keras? So far, I only found lots of instructions about using Tensorboard by Keras in a pre-defined way. But I want to personalized my own Tensorboard data, e.g., my own intermediate results, my own defined scalars or histograms, excluding some Keras pre-defined histograms; so far, I do not find any instructions about that. If there are not any, will Tensorflow people update the guidance later? Thank you.", "I think you can use tf.keras.layer like tf.layer as before. It's not necessary to combine them with tf.keras.Model or Sequence. ", "Thank you for you response. For the tf.keras.layers, is there any way of sharing initialization parameters like the \"arg_scope\" in slim?", "@facaiy Does tf.keras.layers will work with tf.variable_scope?", " tf.variable_scope will be removed in tf 2.0. You can find more details here: https://github.com/tensorflow/community/pull/11  "]}, {"number": 14702, "title": "fix misspellings", "body": "Fixed some typos. :)", "comments": ["Can one of the admins verify this patch?"]}, {"number": 14701, "title": "startup time (_make_train_function()) very slow on Tesla V100-SXM2-16GB GPU, compared to less powerful GPU", "body": "cross posted on keras: https://github.com/fchollet/keras/issues/8537\r\n\r\nRunning mnist_cnn.py (slightly modified - mainly adding logging) from tensorflow 1.4\r\nrunning was done using a prebuilt docker image: tensorflow/tensorflow:1.4.0-gpu-py3\r\non a p2.xlarge aws machine (that has a Tesla K80 GPU) performance is good, the 1st batch (which is dominated by the call to _make_train_function) takes about 2 seconds: (see time stamp for begin batch and end batch)\r\n\r\n```\r\n2017-11-19 08:26:26,172 : INFO : fit\r\n\r\n2017-11-19 08:26:26,637 : INFO : begin batch\r\n2017-11-19 08:26:26.638409: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-19 08:26:26.760940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-19 08:26:26.761478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-19 08:26:26.761506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n\r\n2017-11-19 08:26:28,135 : INFO : end batch\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/1\r\n60000/60000 [==============================] - 12s - loss: 0.3526 - acc: 0.8920 - val_loss: 0.0818 - val_acc: 0.9755\r\nTest loss: 0.081773182778\r\nTest accuracy: 0.9755\r\n```\r\n\r\non a p3.2xlarge machine (with a Tesla V100-SXM2-16GB GPU) the same part takes about 10 minutes\r\n\r\n```\r\n2017-11-19 08:26:44,120 : INFO : fit\r\n\r\n2017-11-19 08:26:44,715 : INFO : begin batch\r\n2017-11-19 08:26:44.716680: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-19 08:26:46.108295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-19 08:26:46.108775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 15.77GiB freeMemory: 15.36GiB\r\n2017-11-19 08:26:46.108815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\n\r\n2017-11-19 08:36:16,552 : INFO : end batch\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/1\r\n60000/60000 [==============================] - 576s - loss: 0.3418 - acc: 0.8949 - val_loss: 0.0769 - val_acc: 0.9772\r\nTest loss: 0.0769035610346\r\nTest accuracy: 0.9772\r\n```\r\n\r\nthe code that was used:\r\n```\r\n#!/usr/bin/env python\r\n'''Trains a simple convnet on the MNIST dataset.\r\n\r\nGets to 99.25% test accuracy after 12 epochs\r\n(there is still a lot of margin for parameter tuning).\r\n16 seconds per epoch on a GRID K520 GPU.\r\n'''\r\n\r\nfrom __future__ import print_function\r\nimport cProfile\r\nimport os\r\nfrom tensorflow.contrib import keras\r\nfrom tensorflow.contrib.keras import backend as K\r\nimport logging\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\nlogging.basicConfig(level=logging.INFO, format='\\n%(asctime)s : %(levelname)s : %(message)s')\r\n\r\nclass callback(keras.callbacks.Callback):\r\n    def on_batch_begin(self, batch, logs=None):\r\n      if batch <= 1:\r\n            logger.info('begin batch')\r\n\r\nclass callback(keras.callbacks.Callback):\r\n    def on_batch_end(self, batch, logs=None):\r\n        if batch <= 1:\r\n            logger.info('end batch')\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 1\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, shuffled and split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(keras.layers.Dropout(0.25))\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(128, activation='relu'))\r\nmodel.add(keras.layers.Dropout(0.5))\r\nmodel.add(keras.layers.Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\nprofiler = cProfile.Profile()\r\nprofiler.enable()\r\nlogger.info('fit')\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test), callbacks=[callback()])\r\nprofiler.dump_stats(os.path.expanduser('~/profiler.pstats'))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\n\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n\r\n```\r\n", "comments": ["using a version of tensorflow that was build with CUDA 9 appears to solve this\r\n\r\nhttps://github.com/mind/wheels/releases/tag/tf1.4-gpu-cuda9", "Same issue on Tesla V100, I think the new GPU is not working very well with CUDA8.0\r\n", "@tfboyd might have some suggestions about why Volta+CUDA8 performs so poorly.", "I do not think it is worth looking into CUDA 8 issues with V100.  CUDA 9 performs much better and is the intended version of CUDA for Volta.  Closing.  I know this might be frustrating but having someone look at this means not looking at something else and I would not suggest CUDA 8 with Volta.  There are a few fixes we are getting for Windows and then we can roll out 1.5 with CUDA 9 in the binary.  ", "I run into the same issue when using V100 to do tensorflow benchmark in a tensorflow1.8 container with cuda9 and cudnn7. Test on host also needs much more warm ups more than P100, but the most in container. I also asked this question on nvidia dev forum here.\r\nhttps://devtalk.nvidia.com/default/topic/1036474/container-tensorflow/slower-performance-in-container-when-using-v100/"]}, {"number": 14700, "title": "when validate my tensorflow installation using \"hello tensorflow\": *** stack smashing detected ***: python terminated Aborted (core dumped)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: installed from native pip3, for Python 3.5, GPU support\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**:  Python 3.5.2\r\n- **Bazel version (if compiling from source)**: GCC 4.4.7 20120313 (Red Hat 4.4.7-1)\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 9.0, V9.0.176\r\n- **GPU model and memory**: name: GeForce GTX 1080 Ti, totalMemory: 10.91GiB, freeMemory: 10.51GiB\r\n\r\n### Describe the problem\r\n\r\nwhen validate my tensorflow installation using \"hello tensorflow\": *** stack smashing detected ***: python terminated Aborted (core dumped)\r\n\r\n### Source code / logs\r\npython: \r\nimport tensorflow as tf\r\nhello = tf.constant('Hello, TensorFlow!')\r\nsess = tf.Session()\r\nprint(sess.run(hello))\r\n\r\n\r\n![screenshot from 2017-11-19 15-21-37](https://user-images.githubusercontent.com/25097258/32988464-59c8e696-cd40-11e7-97a5-a56b8f438f9c.png)\r\n\r\n\r\n", "comments": ["I find the solution.\r\nIt's a problem caused by version of CUDA. \r\n\r\nIn the problem above, my CUDA version is V9.0.176. I uninstalled it and installed V8.0.61instead. \r\nAnd cuDNN is V7.\r\n\r\nThen the problem was solved. "]}]