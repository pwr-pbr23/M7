[{"number": 8058, "title": "Made sequence_length optional in bidirectional dynamic rnn", "body": "Closes #5588 ", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@persiyanov FYI, you need to resolve conflicts in rnn.py first.", "I just pushed this fix internally?\n\nOn Mar 4, 2017 8:29 PM, \"Shanqing Cai\" <notifications@github.com> wrote:\n\n> @persiyanov <https://github.com/persiyanov> FYI, you need to resolve\n> conflicts in rnn.py first.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8058#issuecomment-284204699>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_XfRPZdO9FBo52JtZ4FC0k4l0U8ks5rijo0gaJpZM4MSoYa>\n> .\n>\n", "In that case I will close this PR. Thank you for sending this PR all the same, @persiyanov "]}, {"number": 8057, "title": "AdamOptimizer's slots \"beta1_power\" and \"beta2_power\" are not available via \"get_slot()\" and \"get_slot_names()\"", "body": "**Preamble.** \r\nI want to explicitly pass list of variables to `tf.variables_initializer()`. I do something like this:\r\n``` python\r\nmodel_variables = my_model.get_variables_list()\r\noptimizer_slots = [\r\n    optimizer.get_slot(var, name)\r\n    for name in optimizer.get_slot_names()\r\n    for var in model_variables\r\n]\r\nall_variables = [\r\n    *model_variables,\r\n    *optimizer_slots,\r\n    global_step,\r\n]\r\ninit_op = tf.variables_initializer(all_variables)\r\n```\r\n\r\nWhen I used the `AdamOptimizer`, I got such exception:\r\n```\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value beta2_power\r\n```\r\n\r\n**The problem.**\r\nAfter digging into the TensorFlow sources, I found that AdamOptimizer overrides its `_create_slots()` in that way:\r\n``` python\r\n  def _create_slots(self, var_list):\r\n    # Create the beta1 and beta2 accumulators on the same device as the first\r\n    # variable.\r\n    if (self._beta1_power is None or\r\n        self._beta1_power.graph is not var_list[0].graph):\r\n      with ops.colocate_with(var_list[0]):\r\n        self._beta1_power = variables.Variable(self._beta1,\r\n                                               name=\"beta1_power\",\r\n                                               trainable=False)\r\n        self._beta2_power = variables.Variable(self._beta2,\r\n                                               name=\"beta2_power\",\r\n                                               trainable=False)\r\n    # Create slots for the first and second moments.\r\n    for v in var_list:\r\n      self._zeros_slot(v, \"m\", self._name)\r\n      self._zeros_slot(v, \"v\", self._name)\r\n```\r\nIt creates two `Variables` and does not store them into `self._slots`, therefore I can not access them using public interface.\r\n\r\nThis problem refers to library's API design.\r\n\r\nI see that *beta1_power* and *beta2_power* slots are not subject to any variable while `self._slots` implies that each slot have both name and related variable, so there is no obvious solution... It may be reasonable to extend public API to cover such cases.", "comments": ["`AdamOptimizer` is the one built-in optimizer with such problem. So the simplest fast-and-dirty workaround is to do so:\r\n``` python\r\nmodel_variables = my_model.get_variables_list()\r\noptimizer_slots = [\r\n    optimizer.get_slot(var, name)\r\n    for name in optimizer.get_slot_names()\r\n    for var in model_variables\r\n]\r\nif isinstance(optimizer, tf.train.AdamOptimizer):\r\n    optimizer_slots.extend([\r\n        optimizer._beta1_power, optimizer._beta2_power\r\n    ])\r\nall_variables = [\r\n    *model_variables,\r\n    *optimizer_slots,\r\n    global_step,\r\n]\r\ninit_op = tf.variables_initializer(all_variables)\r\n```", "just had the same problem. diggin thru Adam's source, the author did provide an undocumented method to acquire beta#_power:\r\n\r\n```py\r\ndef _get_beta_accumulators(self):\r\n    return self._beta1_power, self._beta2_power\r\n```", "@alextp I think you've looked at this code recently.  Do you have any suggestions?", "The private method is sad because it can return wrong results if the optimizer is reused across many graphs (and, indeed, the optimizer itself is kinda wrong for that use-case).\r\n\r\nI think we can make the slot API better by allowing variable-free slots (keyed by a graph, or, if None, the current default graph) and having Adam use those. Something called `graph_slot`, or `free_slot`?", "I started working on the patch for this issue. If no one mind, I'll submit a pull request soon.", "@faddey-w any update on this? Is this still an issue or has it been resolved?", "Yes, the problem still exists.\r\nSorry for such delay, the coursework at my university took all my free time. :) Fortunately, I've finished it just a few days ago, so I can continue working on this issue.\r\nThere is not much work to do, so I believe, I'll send a pull request in next 2-3 days.", "I understood here such a thing: variables `beta1_power` and `beta2_power` are specific to each call to `apply_gradients`, but not to the whole graph. So if we want to call `apply_gradients` twice, two separate pairs of beta accumulators should be created, even if both calls are made within the single graph. This does not fit into the concept of \"graph slots\". Definitely, we should separate these slots by graphs, but we cannot simply key these slots by graphs only.\r\n\r\nIn general, the concept of slots is not suitable for cases when we need a separate variable for each application of gradients, as here. Keeping in mind that the original problem was the impossibility to retrieve beta accumulators without touching undocumented/protected API, I think that optimizers should add such \"per-application\" variables to some internal lists. But this seems to be quite ugly solution. Do anyone have better suggestion? \r\n\r\nMaybe it will be better simply to make `_get_beta_accumulators` public and documented? This is the only special case, therefore it is short-sighted and unjustified to invent a complex API for it.", "I'm receiving the same error (`FailedPreconditionError (see above for traceback): Attempting to use uninitialized value beta1_power`) when I try something as simple as \r\n\r\n`optimizer = tf.train.AdamOptimizer()`\r\n\r\nfollowed by\r\n\r\n`sess.run(tf.global_variables_initializer())`\r\n\r\nwhile trying to train my sequence to sequence model. By contrast, `optimizer = tf.train.GradientDescentOptimizer(learning_rate=initial_learning_rate)` works fine. Am I doing something wrong, or is this currently a bug with TensorFlow?", "Certainly there is some undocumented behavior with beta#_power variables, but it should not produce an error if you run `global_variables_initializer`. Could you post all your code?", "I'm happy to post all my code, but tell me if this is sufficient. Here's my main function, where execution starts.\r\n\r\n```\r\ndef main(_):\r\n    x_minibatch, y_minibatch, y_lengths_minibatch = construct_data_pipeline()\r\n    model = import_model()\r\n    train(model=model, x_minibatch=x_minibatch, y_minibatch=y_minibatch, y_lengths_minibatch=y_lengths_minibatch)\r\n```\r\n\r\n`import_model()` imports my `Model` class, which has the following constructor:\r\n\r\n```\r\nclass Model(object):\r\n    def __init__(self, batch_size, initial_learning_rate):\r\n        self.x, self.y_actual, self.y_actual_lengths = self._define_inputs(batch_size)\r\n        self.encoder_outputs, self.encoder_final_states, self.decoder_outputs, self.y_predicted_logits = self._define_model()\r\n        self.loss = self._define_loss()\r\n        self.metrics = self._define_metrics()\r\n        self.optimizer = self._define_optimizer(initial_learning_rate)\r\n```\r\n\r\n`_define_optimizer(initial_learning_rate)` is defined as follows:\r\n\r\n```\r\n    @staticmethod\r\n    def _define_optimizer(initial_learning_rate):\r\n        with tf.name_scope('define_optimizer'):\r\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=initial_learning_rate)\r\n            # optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate)\r\n        return optimizer\r\n```\r\n\r\nThen, `main` calls `train()`, which is defined as follows:\r\n\r\n```\r\ndef train(model, x_minibatch, y_minibatch, y_lengths_minibatch):\r\n    with tf.Session() as sess:\r\n\r\n        # create coordinator to handle threading\r\n        coord = tf.train.Coordinator()\r\n\r\n        # start threads to enqueue input minibatches for training\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n        # initialize all variables and ops\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        start_time = time.time()\r\n\r\n        # train\r\n        for step in range(tf.app.flags.FLAGS.training_steps):\r\n\r\n            train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch)\r\n\r\n            # every 100 steps, evaluate model metrics and write summaries to disk\r\n            if (step + 1) % 10 == 0 or step == tf.app.flags.FLAGS.training_steps - 1:\r\n                eval_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch, step, start_time)\r\n                start_time = time.time()\r\n\r\n        # when done, ask the threads to stop\r\n        coord.request_stop()\r\n\r\n        # wait for threads to finish\r\n        coord.join(threads)\r\n```\r\n\r\nMy `train_op` is defined as follows:\r\n\r\n```\r\ndef train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch):\r\n    x_values, y_values, y_lengths_values = sess.run([x_minibatch, y_minibatch, y_lengths_minibatch])\r\n\r\n    # minimize loss\r\n    sess.run([model.optimizer.minimize(model.loss), model.y_predicted_logits],\r\n                                     feed_dict={model.x: x_values,\r\n                                                model.y_actual: y_values,\r\n                                                model.y_actual_lengths: y_lengths_values})\r\n```\r\n\r\nAs you can see, the model is constructed before I call `sess.run(tf.global_variables_initializer())` and isn't touched afterwards.", "The problem is that you call `model.optimizer.minimize` too late. This methods creates additional tensors within your graph, so calling it within a loop is bad idea - it is something similar to a memory leak. Also, in case of stateful optimizers (such as AdamOptimizer) `minimize` creates additional variables. That's why you get exception you described - your initializer runs before you create them. The solution for you will be to place call to `model.optimizer.minimize` within the model class itself, and store its result in model`s attribute. \r\n\r\nSo, your problem does not refer to this issue.", "@faddey-w , thank you! That did it \ud83d\udde1 \r\n\r\nIt also fixed my problem over here. https://stackoverflow.com/questions/44706150/tensorflow-seq2seq-training-time-per-minibatch-monotonically-increases\r\n\r\nI'm going to credit you there.", "I'm working with the C++ API and I cannot find any default values nor recommendations for the parameters values of the tensorflow::ApplyAdam class.\r\nWhat are the current default values for beta#_power that are passed from the python class to the lower level API?\r\n", "They have to be variables, and by default are initalized such that beta1=0.9 and beta2=0.999", "What are the defaults of beta#_power?", "beta1_power is initialized to beta1, beta2_power is initialized to beta2. Then, after each training step beta#_powers are multiplied by beta1 and beta2, respectively.", "I'm closing this issue since technically these values are not slots (as there is not one value per variable in the model).", "I do not know why this issue is closed, but for TF1.8.0 this issue is still alive. \r\nThose 'beta1_power' and 'beta2_power' are not within the variable_scope unlike other variables.\r\n\r\nFor some cases, \"The solution for you will be to place call to model.optimizer.minimize within the model class itself, and store its result in model`s attribute.\" does not help because you may want to separate(*) the part where you define the 'network model' with the one you define 'training method'.\r\n\r\n(*) If you wanna build modular programming having a catalog of learning algorithms, then this is the case to separate 'network builder' from 'network trainer'\r\n\r\nMy take on this is to set two variable_scope context managers(a.k.a with) for each '_build' and '_train' parts for each with a same word 'network' starting with, and then use the word to aggregate whole variables within the two scopes:\r\nex) \r\nwith tf.variable_scope('network_build') as net_build:\r\n       build_network()\r\n\r\n...\r\nwith tf.variable_scope('network_train') as net_train:\r\n      define_training_method()\r\n\r\n...\r\ntf.Session.run(tf.variables_initializer(var_list=tf.global_variables(scope='network')))\r\n", "tf.contrib.optimizer_v2.AdamOptimizer fixes this issue:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/optimizer_v2/adam.py\n\nOn Fri, May 25, 2018 at 1:32 AM sdr2002 <notifications@github.com> wrote:\n\n> I do not know why this issue is closed, but for TF1.8.0 this issue still\n> alive.\n> Those 'beta1_power' and 'beta2_power' are not within the variable_scope\n> unlike other variables.\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8057#issuecomment-391981229>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdWcxdjbZrLxAizLPyxEgumfdiJ5ks5t18GPgaJpZM4MScRa>\n> .\n>\n\n\n-- \n - Alex\n", "Issue still present in 1.13.1. tf.contrib.optimizer_v2.AdamOptimizer does not help as well. Better to move to tf2.0.", "There is another method which was added recently: https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/train/Optimizer#variables\r\nIt just returns all variables that optimizer have, including non-slot ones. If I remember correctly it is present in 1.13.1. If you want just to get all variables and do something with the whole set (initialize, for example), and don't need to get some specific variable, it should help you then."]}, {"number": 8056, "title": "error installing tenserflow", "body": "Hello Gents,\r\n\r\nI am trying to install the TenserFlow  but when i reach and type this command:\r\n\r\n\"bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\"\r\n\r\nafter following all instruction in the following link:\r\n\"https://alliseesolutions.wordpress.com/2016/07/05/how-to-install-gpu-tensorflow-0-9-from-sources-ubuntu-14-04/\" \r\n\r\ni get the following error (I managed to reduced errors from 5 to only this one):\r\n\r\nNOTE: i am a beginner in Linux  (Ubuntu 14.04) and thus i require step by step instructions if possible.\r\n\r\n\r\n\r\nERROR: /home/maher/tensorflow/tensorflow/compiler/tf2xla/BUILD:23:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:xla_compiler' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 180 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::CheckSignature(const DataTypeVector&, const std::vector<tensorflow::XlaCompiler::Argument>&)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:48:34: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < types.size(); ++i) {\r\n                                  ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::BuildExecutable(const tensorflow::XlaCompiler::CompilationResult&, std::unique_ptr<xla::LocalExecutable>*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:139:52: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < result.xla_input_shapes.size(); ++i) {\r\n                                                    ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::BuildArguments(const std::vector<tensorflow::XlaCompiler::Argument>&, bool, xla::ComputationBuilder*, std::vector<tensorflow::XlaContext::Argument>*, std::vector<int>*, std::vector<xla::Shape, std::allocator<xla::Shape> >*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:228:8: error: 'size_typ' is not a member of 'std::vector<tensorflow::XlaCompiler::Argument>'\r\n   for (std::vector<XlaCompiler::Argument>::size_typ i = 0;\r\n        ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:228:53: error: expected ';' before 'i'\r\n   for (std::vector<XlaCompiler::Argument>::size_typ i = 0;\r\n                                                     ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:229:8: error: 'i' was not declared in this scope\r\n        i < args.size(); ++i) {\r\n        ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::CompileGraph(const string&, std::unique_ptr<tensorflow::Graph>, tensorflow::FunctionLibraryRuntime*, const std::vector<tensorflow::XlaCompiler::Argument>&, tensorflow::XlaCompiler::CompilationResult*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                    ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 1 is invalid\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                                              ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 2 is invalid\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: invalid type in declaration before 'i'\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                                                            ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: expected ';' before 'i'\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: 'i' was not declared in this scope\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:481:43: error: expected ')' before ';' token\r\n        i < result->variable_updates.size(); ++i) {\r\n                                           ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:49: warning: unused variable 'size_type' [-Wunused-variable]\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                                                 ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:481:47: error: 'i' was not declared in this scope\r\n        i < result->variable_updates.size(); ++i) {\r\n                                               ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:481:48: error: expected ';' before ')' token\r\n        i < result->variable_updates.size(); ++i) {\r\n                                                ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:508:1: error: expected '}' at end of input\r\n }  // namespace tensorflow\r\n ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: At global scope:\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:508:1: error: expected '}' at end of input\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::CompileGraph(const string&, std::unique_ptr<tensorflow::Graph>, tensorflow::FunctionLibraryRuntime*, const std::vector<tensorflow::XlaCompiler::Argument>&, tensorflow::XlaCompiler::CompilationResult*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:508:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }  // namespace tensorflow\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.", "comments": ["How about you follow this tutorial instead?\r\n\r\nhttps://www.tensorflow.org/install/install_linux\r\n\r\nfrom the command line, just:\r\n\r\npip install tensorflow", "If you are a beginner on linux, we recommend starting with the command @MikeTam1021 suggested. Since GPU version requires a lot of non-trivial installation of CUDA, cuDNN, and manipulation of environment variables, you should definitely start with the CPU version.\r\n\r\nIf you are using ubuntu 14.04, you may need to run the pip install command with the URL as follows.\r\n```\r\npip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl\r\n```", "@mahernadar let us how it goes!  And I agree with the advice - using a binary install via pip will be much simpler.\r\n\r\nIf you're curious, the particular compilation error you're encountering is being fixed by #8039 ", "Dear Gunan,\n\nThanks allot for your response. That's a great suggestion, but i need to\nlearn fast because i will need to work on a real project with real pictures\nand try to implement a classification according to project requirements.\n\nYou think CPU is good enough for that? Note that i have an Alienware laptop\nwith Nvidia 770M.\n\nAgain, thanks allot for your input :)\n\nSincerely,\n\nMaher A. Nadar\n\nOn Fri, Mar 3, 2017 at 11:52 PM, gunan <notifications@github.com> wrote:\n\n> If you are a beginner on linux, we recommend starting with the command\n> @MikeTam1021 <https://github.com/MikeTam1021> suggested. Since GPU\n> version requires a lot of non-trivial installation of CUDA, cuDNN, and\n> manipulation of environment variables, you should definitely start with the\n> CPU version.\n>\n> If you are using ubuntu 14.04, you may need to run the pip install command\n> with the URL as follows.\n>\n> pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp27-none-linux_x86_64.whl\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8056#issuecomment-284092573>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AY86zbRKafxVGZ0gd9n84rRhh4Ic8Fsoks5riJmxgaJpZM4MSbbw>\n> .\n>\n", "I recommend using the CPU-pip version for a few reasons:\r\n1-Installation of CPU pip version is effortless. You can start using tensorflow in 5-10 minutes.\r\n2-If you have a single GPU, that will be used for both display and computation. This is not recommended. When running large training graphs you will almost certainly run into issues due to GPU also being used for display, and this will set you back even further.", "Closing this issue since it appears to be due to following a tutorial which advocates building from live head of the git repo (despite the URL saying `how-to-install-gpu-tensorflow-0-9-from-sources-ubuntu-14-04`)\r\n\r\nThis command will pick up the current state of the github repository, rather than a specific known working version:\r\n`git clone https://github.com/tensorflow/tensorflow`\r\n\r\nThough we make every effort to test each commit, some issues will inevitable slip through, so it is usually much safer to install a working binary package or at least build from source using a specific tagged version.\r\n \r\nAs @tatatodd says, a fix for the particular error is in progress here https://github.com/tensorflow/tensorflow/pull/8039\r\n\r\n\r\n "]}, {"number": 8055, "title": "TensorBoard: Can't load >5 Audio summaries, Broken pipe error", "body": "*Possibly related:* https://github.com/tensorflow/tensorflow/issues/4207\r\n*Setup:* OSX 10.10, TF nightly build of 2017-03-02 without GPU, Python 2.7.13. Affects both Chrome 56.0 and Safari 10.0. I think the official 1.0.0 release also had this issue.\r\n\r\nMy model logs relatively long audio summaries (~3 minutes long at 16 kHz), and created around 60 of them overnight. The issue also appears with fewer summaries created, at least on Chrome.\r\n\r\nWhen using Chrome, they only load upon playback. Only the first 5 or so that I play will work; after that (even if I pause them) I can't get the others to play unless I restart TensorBoard. When using Safari, all of them try to load at once and nothing works.\r\n\r\nOn the TensorBoard side, I see a bunch of `error: [Errno 32] Broken pipe`, which could mean the browser tries to manage the incoming streams and TensorBoard doesn't know how to defer the transfers (see [this post](http://stackoverflow.com/questions/14207708/ioerror-errno-32-broken-pipe-python)).\r\n\r\nFor Chrome, there is a [limit of 6 concurrent audio downloads](https://bugs.chromium.org/p/chromium/issues/detail?id=162627). I think in my case the audio keeps the connection open even after I pause it, and instead of downloading the whole file it streams it, so it never closes and I can't play other files. I may need to restart TensorBoard to clear the browser cache, or maybe TensorBoard itself has a limit on the concurrent streams and they don't close even after a page reload. For Safari, I think it doesn't wait until playback to download the files, so all 60 players start the download concurrently and only the first ones survive.\r\n\r\nIf it's not a bug on the TensorBoard server and the summaries can't be prevented from keeping the connection open on pause or stop, I guess a possible solution could be to have a global player and load the audio files to it on-demand when requesting a summary, rather than each summary having its own player.", "comments": ["I think @dandelionmane and @jart might have some ideas here.", "@rryan Can you take a look?", "hey @lemonzi! could you share an example events file that produces the problem?", "Hey @rryan, long time! This should do it (the audio is quite horrible though): https://dl.dropboxusercontent.com/u/8226735/events.out.tfevents.1488843003.iAirLemon", "Update: when using an Incognito Tab on Chrome, the audios pre-fetch for some reason, as with Safari, and therefore clicking the Audio link means that the other summaries will no longer update and the tab won't reload. I have to open a new tab to create a brand new connection with the server.", "Update: the Dropbox link broke (see https://www.dropbox.com/help/16), use this one instead: https://www.dropbox.com/s/ow3sz2w3qurdm8y/events.out.tfevents.1488843003.iAirLemon", "It got worse with the latest Tensorboard release; now only the 6 first audio summaries load, the rest hang forever. Only works on Incognito mode, when browsing with cache nothing loads at all.", "Migrated to https://github.com/tensorflow/tensorboard/issues/35"]}, {"number": 8054, "title": "summarize_graph doesn't recognize VariableV2 ", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n### Environment info\r\nOperating System:\r\nWindows 10\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nNone\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n8cac382a5425d64f3083cb5adec525baa163e18e\r\n2. The output of `bazel version`\r\ncompiled by cmake\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n```\r\n--- a/tensorflow/tools/graph_transforms/summarize_graph_main.cc\r\n+++ b/tensorflow/tools/graph_transforms/summarize_graph_main.cc\r\n@@ -109,7 +109,7 @@ Status SummarizeGraph(const GraphDef& graph, const string& graph_path) {\r\n     if (node.op() == \"Placeholder\") {\r\n       placeholders.push_back(&node);\r\n     }\r\n-    if (node.op() == \"Variable\") {\r\n+    if (node.op() == \"Variable\" || node.op() == \"VariableV2\") {\r\n       variables.push_back(&node);\r\n     }\r\n   }\r\n@@ -168,7 +168,7 @@ Status SummarizeGraph(const GraphDef& graph, const string& graph_path) {\r\n     if (node.device() != \"\") {\r\n       ++device_counts[node.device()];\r\n     }\r\n-    if ((node.op() == \"Const\") || (node.op() == \"Variable\")) {\r\n+    if ((node.op() == \"Const\") || (node.op() == \"Variable\") || (node.op() == \"VariableV2\")) {\r\n       Tensor tensor;\r\n       if (node.attr().count(\"value\") &&\r\n           tensor.FromProto(node.attr().at(\"value\").tensor())) {\r\n```", "comments": ["Thanks for filing the bug @snnn , and the suggested fix.  Would you like to submit a PR to fix this?\r\n\r\nI think @petewarden should know whether this is the correct fix; it looks good to me.", "This does look like a good fix, thanks @tatatodd !", "I'll submit a pull request tmr. ", "Indeed, I haven't understand the code fully, and I do not know how to use it.  Why all variables are marked as input? Should it only work with frozen graph?"]}, {"number": 8053, "title": "per_process_gpu_memory_fraction causes memory error", "body": "I wrote an LSTM model in tensorflow and use a `Saver` to create a checkpoint after each epoch. So far so good. However, when I add the option `per_process_gpu_memory_fraction=0.9` to the session, it runs out of memory when I save the model right after the first epoch. I have a GeForce Titan X with 12GB of memory; tf uses a bit more than half of it (see below).\r\n\r\nInterestingly, if I do not save the model, it does not crash (at least not after the first epoch). Also,\r\n\r\n| per_process_gpu_memory_fraction | memory used | crashes |\r\n| ------------------------------------------- | ----------------- | --------- |\r\n| 0.9 | 7377MiB | yes |\r\n| 0.8 | 7288MiB | yes |\r\n| 0.5 | 6272MiB | no |\r\n| not set | 6955MiB | no |\r\n\r\nI forgot with what parameters, but I also saw the program survive the first epoch, only to crash after the fifth.\r\n\r\nI have modified the [ptf_word_lm.py](https://github.com/DavidNemeskey/models/blob/saver_memory_error/tutorials/rnn/ptb/ptb_word_lm.py) in tensorflow/models, so that it can be used to reproduce the error. It does not crash with `per_process_gpu_memory_fraction=0.8`, as my model is a bit different, but it does at `0.9`. Please call the script with `--model medium`.\r\n\r\nOn the surface, I only see a `Dst tensor is not initialized` error, but as described in #7025, it is a sign of a memory error. Also, in tf 0.11, it returned with a proper out of memory error, but I have updated to 1.0 since, as I hoped this bug had been fixed there. Apparently not.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI couldn't find anything similar.\r\n\r\n### Environment info\r\n[environment.txt](https://github.com/tensorflow/tensorflow/files/816786/environment.txt)\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nSee `ptb_word_lm.py` in [this branch](https://github.com/DavidNemeskey/models/tree/saver_memory_error)\r\n\r\n### What other attempted solutions have you tried?\r\n--\r\n\r\n### Logs or other output that would be helpful\r\n[log_err.txt](https://github.com/tensorflow/tensorflow/files/816796/log_err.txt)\r\n", "comments": ["@vrv might have some thoughts here.  It certainly sounds like there's a bug lurking somewhere.", "@vrv Spent some time analyzing this but didn't find any solutions.  Closing since I don't think we'll solve this without more information or insight.", "The only thing I could think of was that somehow the gpu allocation algorithm performs slightly differently when it has less memory because of the buddy allocation system, and it might be hitting a weird corner case where having more memory leads to more unreclaimable fragmentation, and thus an eventual OOM.  But we don't really have good enough tools to understand whether this is actually happening :("]}, {"number": 8052, "title": "install_linux.md: add missing closing <tt> tag", "body": "There is a simple formatting error in install_linux.md which leads to the lower section of https://www.tensorflow.org/install/install_linux#run_a_short_tensorflow_program  being rendered entirely in monospaced font. Add a closing ``<tt>`` tag to correct formatting.", "comments": ["Can one of the admins verify this patch?", "Thank you very much for bringing this to our attention!\r\nWe had a bunch of fixes to this file internally. We are also incorporating this change into it, and will merge it internally first.\r\nIt will also change the tag being used here. Therefore, I will close this PR.\r\nAgain, thank you very much for your help!"]}, {"number": 8051, "title": "Make tf.Tensor's magic functions (e.g., `__add__()`) play nice with unknown types", "body": "`tf.Tensor`'s magic functions should return `NotImplemented` rather than raising an exception when they are given an object of an unsupported type (i.e., an object that TensorFlow cannot `convert_to_tensor()`). This would give that object a chance to perform the operation with its reverse magic function (e.g., `__radd__()`). See the code example below. Note that this is what NumPy arrays do.\r\n\r\n### Environment info\r\nOperating System: **Ubuntu 16.10**\r\nInstalled version of CUDA and cuDNN:  **N/A**\r\nInstalled pip package:\r\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.0.0-cp35-cp35m-linux_x86_64.whl\r\nTensorFlow version: **1.0.0**\r\n\r\n### Reproducible example\r\n\r\n```\r\n>>> class AmazingMatrix(object):\r\n...   def __add__(self, b):\r\n...     print(\"I know how to handle TensorFlow Tensors!\")\r\n...   def __radd__(self, a):\r\n...     print(\"I know how to handle TensorFlow Tensors!\")\r\n... \r\n>>> am = AmazingMatrix()\r\n>>> t = tf.constant([[0.]])\r\n>>> am + t\r\nI know how to handle TensorFlow Tensors!\r\n>>> t + am\r\nTraceback (most recent call last):\r\n[...]/python/ops/math_ops.py\", line 799, in binary_op_wrapper\r\n    y = ops.convert_to_tensor(y, dtype=x.dtype.base_dtype, name=\"y\")\r\n[...]\r\nTypeError: Expected float32, got <__main__.AmazingMatrix object at 0x7f5999a73518> of type 'AmazingMatrix' instead.\r\n```\r\n\r\n### Related GitHub issues or StackOverflow threads\r\nhttp://stackoverflow.com/questions/38599637/overloading-tensorflow-tensor-magic-method-priority-similar-to-numpys-array\r\n\r\n### What other attempted solutions have you tried?\r\nNone. I guess one could create Tensor wrappers, with the appropriate behavior, but it would be quite cumbersome to use.\r\n\r\n### Logs or other output that would be helpful\r\nNone\r\n", "comments": ["Seems reasonable.\r\n\r\nCan we ensure this doesn't increase probability of bugs such as one fixed by https://github.com/tensorflow/tensorflow/commit/a8c3de3bddf01b4b80c986b3bb81d2a1658be3c8 ?\r\n\r\n\r\nIn that case, \"numpy_array+tensorflow_variable\" delegated addition to numpy, which used the fact that \"variable\" supports `__getitem__` to run expressions of the form\r\n\r\n```\r\nnumpy_array[0]+tensorflow_variable[0] \r\nnumpy_array[1]+tensorflow_variable[1]\r\n...\r\n```\r\n\r\nThis added a bunch of `Slice` ops to the default graph", "@yaroslavvb That's a great question. I propose the following alternative solution, to avoid any risk.\r\n\r\nWhen a tensor's LHS magic function is called, then before doing anything we check the following conditions (for example for `Tensor.__add__()`):\r\n* `type(RHS)` has `__radd__`\r\n* `RHS.__tensor_priority__` is defined and greater than `LHS.__tensor_priority__` (which defaults to 0).\r\n\r\nIf both conditions apply, then delegate to the RHS (by returning `NotImplemented`). Or else, proceed as usual, and if an error occurs when calling `convert_to_tensor()` and if condition 1 applies, then display a friendly error message explaining the `__tensor_priority__` logic.\r\n\r\nThe advantages are:\r\n* This gives full control to developers who want to build libraries on top of TensorFlow. If they set `__tensor_priority__`, they can have full control even when tensors are on the LHS.\r\n* This won't break any existing code since nothing changes unless `__tensor_priority__` is defined by the RHS.\r\n* It's consistent with NumPy's approach.\r\n* We don't use `__array_priority__` which may lead to conflicts.\r\n* Compared to the first approach I proposed, this only requires the developer to define `__tensor_priority__` and the error message will explain what to do.\r\n\r\nWdyt?", "Seems reasonable to me too, but I'm not a python expert.  @aselle might have some thoughts on this", "FYI, I submitted a Pull Request for the proposed `__tensor_priority__` implementation, please comment.", "Let me try to provide some context around `__array_priority__` as a NumPy developer. (There has actually been extensive discussion about [removing it entirely](https://github.com/numpy/numpy/issues/5844) going forward, but that's a whole different matter that you probably don't want to get into.)\r\n\r\nIt exists because NumPy has two different ways of interacting with objects of an unknown type. For example, suppose `array = np.array([1, 2, 3])` and `other = SomeType()`. What should `array + other` give?\r\n\r\n1. If `__array_priority__` is unset or less than 1: treat the unknown type as a scalar, and apply the operation in an element-wise fashion over each element of the array, e.g., `array + other` -> `np.array([1 + other, 2 + other, 3 + other])`.\r\n2. If `__array_priority__` is greater than 1: treat the unknown type as an array, and let it try to handle the entire NumPy array, e.g., `array + other` -> `other.__radd__(array)`.\r\n\r\nI don't think TensorFlow *ever* wants behavior (1). Hence we don't need `__tensor_priority__`. TensorFlow should stick to Python's default system for overriding arithmetic operations.\r\n\r\nSomething like one of these solutions seems like a good place to start:\r\n\r\n\"Try converting everything to a tensor\"\r\n```python\r\n# This is possibly not flexible enough, because you might, for example,\r\n# want to define a type (e.g., a tensor with physical units) that is convertible\r\n# to a tensor but still wants to override arithmetic.\r\ndef __add__(self, other):\r\n  try:\r\n    other = ops.convert_to_tensor(other)\r\n  except TypeError:\r\n    return NotImplemented\r\n  return add_impl(self, other)\r\n```\r\n\r\nOr \"Let the other type try to override the operation, otherwise convert to tensor\" (I like this one better)\r\n```python\r\n# needs to at least include tf.Tensor, but what other types to include would\r\n# depend on internal implementation details.\r\nKNOWN_TYPES = (tf.Tensor, tf.Variable, ...)\r\n\r\ndef __add__(self, other):\r\n  # We need exclude known types (including type(self)) to avoid an\r\n  # infinite loop due to similar code in the implementation of Tensor.__radd__\r\n  if not isinstance(other, KNOWN_TYPES):\r\n    # defer to the other type, if it provides an implementation\r\n    result = other.__radd__(self)\r\n    if result is not NotImplemented:\r\n      return result\r\n  # other is known or does not provide an implementation\r\n  other = ops.convert_to_tensor(other)\r\n  return add_impl(self, other)\r\n```\r\n", "Thanks Stephan, awesome insights.  I initially proposed the first solution you suggest. The only problem I see is that the user cannot easily get control in the case where `other` is a subtype of a supported type (the only option would be to somehow force `convert_to_tensor(other)` to raise a `TypeError`, not sure how.  The same is true for the second option you suggest.\r\n\r\nPerhaps it's not such a big deal.\r\nAlso, I'm wondering, what is the point of trying `convert_to_tensor(other)` if `other` is not a known type? Perhaps we should just return `NotImplemented` systematically in that case?\r\n", "Since there's a pull request in progress, I'm goign to label it as 'contributions welcome' so that this issue doesn't keep needing re-triaging every time somebody comments!", "> The only problem I see is that the user cannot easily get control in the case where other is a subtype of a supported type (the only option would be to somehow force convert_to_tensor(other) to raise a TypeError, not sure how. \r\n\r\nThe simple solution is to switch `isinstance(obj, cls)` checks to exact type checks `type(obj) is cls`. But I'm not sure it really makes sense to even worry about subclasses of normal TensorFlow types. They aren't really meant for subclassing AFAICT, but python doesn't let you declare a type as \"final\" so there's not much we can do to stop.", "> Also, I'm wondering, what is the point of trying convert_to_tensor(other) if other is not a known type? Perhaps we should just return NotImplemented systematically in that case\r\n\r\nHere I was trying to handle other types that can be converted into Tensors but don't know anything about TensorFlow, like built-in Python number and NumPy arrays.\r\n\r\nMaybe explicitly listing types to convert is a better idea, e.g.,\r\n```python\r\nTYPES_TO_CONVERT = (tf.Variable, number.Number, numpy.ndarray)\r\n\r\ndef __add__(self, other):\r\n  if isinstance(other, TYPES_TO_CONVERT):\r\n    other = ops.convert_to_tensor(other)\r\n  elif type(other) is not type(self):\r\n    return NotImplemented\r\n  return add_impl(self, other)\r\n```", "Apparently there's already a `_tensor_conversion_func_registry` containing various base types and their corresponding conversion functions to Tensors, located in `tensorflow.python.framework`.\r\nNew conversion functions can be registered using `register_tensor_conversion_function()`, which is part of the [public API](https://www.tensorflow.org/api_docs/python/tf/register_tensor_conversion_function).\r\nSo I guess there's already quite a lot of flexibility. The only thing missing is a way to delegate binary ops to the RHS. Perhaps the first option would be sufficient after all: just return `NotImplemented` if a `TypeError` is raised. I'll make the change in the Pull Request."]}, {"number": 8050, "title": "[BACKPORT-r0.12] Fix loading of libhdfs.so when installed in non-standard directory", "body": "This PR is to backport #5911 to branch r0.12", "comments": ["Can one of the admins verify this patch?", "I am not sure how severe this issue is, but we probably wont release a new patch release for 0.12.\r\nSo Im not sure about accepting this patch into this release branch.\r\n\r\n@jhseu WDYT?", "Yeah, let's not patch 0.12. Please upgrade to 1.0 :)"]}, {"number": 8049, "title": "AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'", "body": "on `win7 x64`\r\n\r\npython version is `3.5`\r\ntensorflow's version is `1.0.0`, and it is installed by `pip install tensorflow-gpu`\r\n\r\nand, it seems I can replace `tf.nn.rnn_cell with tf.contrib.rnn`,...", "comments": ["@jonygli Please fill out the following information, which was part of the \"New issue\" template.  Note that the RNN operations were moved from `tf.contrib.rnn` to `tf.nn.rnn_cell` in TensorFlow 1.0, so I suspect you're not actually running TensorFlow 1.0.\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n", "Got the same error when testing tensorbox\r\n \r\n`bazel version\r\nExtracting Bazel installation...\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261`\r\n\r\n` python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n1.0.0\r\n`\r\n`python3 train.py --hypes hypes/overfeat_rezoom.json --gpu 0 --logdir output\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 16, in <module>\r\n    from tensorflow.models.rnn import rnn_cell\r\nImportError: No module named 'tensorflow.models'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 18, in <module>\r\n    rnn_cell = tf.nn.rnn_cell\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'`\r\n\r\n", "on win7 x64\r\npython version is 3.5\r\ntensorflow's version is 1.0.0, and it is installed by pip install tensorflow\r\n\r\n```\r\n# highway layer that borrowed from https://github.com/carpedm20/lstm-char-cnn-tensorflow\r\ndef highway(input_, size, layer_size=1, bias=-2, f=tf.nn.relu):\r\n    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\r\n\r\n    t = sigmoid(Wy + b)\r\n    z = t * g(Wy + b) + (1 - t) * y\r\n    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\r\n    \"\"\"\r\n    output = input_\r\n    for idx in range(layer_size):\r\n        output = f(tf.nn.rnn_cell._linear(output, size, 0, scope='output_lin_%d' % idx)) #tf.contrib.layers.linear instad doesn't work either.\r\n        transform_gate = tf.sigmoid(tf.nn.rnn_cell._linear(input_, size, 0, scope='transform_lin_%d' % idx) + bias)\r\n        carry_gate = 1. - transform_gate\r\n\r\n        output = transform_gate * output + carry_gate * input_\r\n\r\n    return output\r\n```\r\n\r\nand got error \r\nline 15, in highway\r\n    output = f(tf.nn.rnn_cell._linear(output, size, 0, scope='output_lin_%d' % idx))\r\nAttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell'\r\n\r\nI'm not familiar with tensorflow, but I have to use highway, I've try some method\r\nhttp://stackoverflow.com/questions/42552540/tensorflow-tf-contrib-rnn-module-object-is-not-callable\r\nhttp://stackoverflow.com/questions/42311007/attributeerror-tensorflow-python-ops-rnn-has-no-attribute-rnn\r\nbut all failed.", "@tatatodd it doesn't need any step to reproduce, just after installing the specified version, then you will find that there is no such rnn_cell.py file under nn, while there is a rnn_cell.py under contrib!\r\n\r\nit seems rnn_cell is refactored to contrib in this version!!", "Andrew can you update the Python code rewriter?", "Just use \r\nfrom tensorflow.contrib import rnn \r\nlstm_cell = rnn.BasicLSTMCell(rnn_size) \r\noutputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\r\n\r\ninstead of \r\nfrom tensorflow.python.ops import rnn, rnn_cell \r\nlstm_cell = rnn_cell.BasicLSTMCell(rnn_size,state_is_tuple=True) \r\noutputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\ufeff", "Closing this. @aselle feel free to reopen if upgrade script needs to be updated.", "still not working @wangyizhen \r\nit says: AttributeError: module 'tensorflow.contrib.rnn' has no attribute 'BasicLSTMCell'\r\n"]}, {"number": 8048, "title": "Do not build nccl on windows.", "body": "", "comments": ["http://ci.tensorflow.org/job/tf-pr-win-bzl/12/\r\nWindows bzl build.\r\n\r\nI expect a lot of failures in this build, as it has been failing for ages now.\r\nI will only check for the nccl related failure to be resolved, we can address the rest later.", "2nd try:\r\nhttp://ci.tensorflow.org/job/tf-pr-win-bzl/13/"]}, {"number": 8047, "title": "Error in tensorflow debugger (tfdbg) while executing session run call in child thread", "body": "I ran tensorflow debugger using the command \"python -m <PythonModule> --debug\" but got the following error (i,e Signal only works in main thread):\r\n\r\n![screenshot3png](https://cloud.githubusercontent.com/assets/21690396/23540767/24df69c6-ff98-11e6-96bf-5ee98a439f77.PNG)\r\n\r\nThe error is thrown when a session run call is executed in a child thread (spawned from main thread). Is tensorflow debugger only supported for single threaded applications?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: CentOS 7.2.1511\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n![screenshot](https://cloud.githubusercontent.com/assets/21690396/23540624/38732a46-ff97-11e6-9749-a43b8af6c5aa.PNG)\r\n\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n![screenshot2](https://cloud.githubusercontent.com/assets/21690396/23540694/b42648f8-ff97-11e6-93ff-4cabc2d2d199.PNG)\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@agupta74 tfdbg's CLI class attempts to register signal handlers even when not running on the main threads, which is the reason for this error. We will send a bug fix to the master branch.\r\n\r\nAs a workaround until the bug is fixed, you can pretend that the child process is a \"remote process\" and use the approach outlined here:\r\nhttps://www.tensorflow.org/programmers_guide/debugger#offline_debugging_of_remotely-running_sessions\r\n\r\n```\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\n# ... Code where your session and graph are set up...\r\n\r\nrun_options = tf.RunOptions()\r\ntf_debug.watch_graph(\r\n      run_options,\r\n      session.graph,\r\n      debug_urls=[\"file:///shared/storage/location/tfdbg_dumps_1\"])\r\n# Be sure to use different directories for different run() calls.\r\nsession.run(fetches, feed_dict=feeds, options=run_options)\r\n```\r\n\r\nLater, in an environment that you have terminal access to, you can load and inspect the data in the dump directory on the shared storage by using the offline_analyzer of tfdbg. For example:\r\n\r\n```\r\npython -m tensorflow.python.debug.cli.offline_analyzer \\\r\n    --dump_dir=/shared/storage/location/tfdbg_dumps_1\r\n```\r\n", "Thank you for reporting this bug, @agupta74!", "@caisq Any updates regarding this issue? I've tried using `offline_analyzer` per your suggestion; but, keep running into `ValueError: Duplicate node name` as in #7051. Unlike #7051, I'm running locally on CPU. The problem seems to stem from repeated calls to `sess.run()`, where `sess` is an instance of `DumpingDebugWrapperSession`; but, I'm not sure how to get around this issue when using `tf.get_variable()` given that a call to an initializer seems necessary. Included below is a code snippet.\r\n\r\n\r\n```\r\n# Dependencies\r\nimport subprocess\r\nimport tensorflow as tf\r\nfrom tempfile import TemporaryDirectory\r\nfrom tensorflow.python import debug as tf_debug\r\n\r\ndump_dir = TemporaryDirectory() #temp. directory for demo\r\nwith tf.Session() as _sess:\r\n  # Create 'DumpingDebugWrapperSession' wrapper\r\n  def watch_fn(fetches, feed_dict):\r\n    node_name_regex_whitelist = '(watch_[0-9]+)'\r\n    debug_ops, op_type_regex_whitelist = None, None\r\n    return debug_ops, node_name_regex_whitelist, op_type_regex_whitelist\r\n  sess = tf_debug.DumpingDebugWrapperSession(_sess,\r\n                  dump_dir.name, watch_fn=watch_fn)\r\n\r\n  # Declare TensorFlow variables\r\n  tf_vars = [tf.get_variable('watch_0', shape=[32], dtype='float32'),\r\n             tf.get_variable('ignore_0', shape=[32], dtype='float32')]\r\n\r\n  # Calls sess.run()\r\n  init_op = tf.global_variables_initializer()\r\n  sess.run(init_op)\r\n  # <stuff goes here>\r\n  fetches = sess.run(tf_vars, feed_dict={})\r\n\r\n# Call to offline_analyzer\r\nargv = ['python3.5', '-m', 'tensorflow.python.debug.cli.offline_analyzer',\r\n        '--ui_type', 'readline', '--dump_dir', dump_dir.name]\r\ncmdline = \" \".join(argv)\r\ntry:\r\n  proc = subprocess.Popen(cmdline, shell=True)\r\n  proc.wait()\r\nfinally:\r\n  dump_dir.cleanup()\r\n```", "@j-wilson With regard to the duplicate node name error you are getting, are you using the same file:// debug URL (i.e., the same directory) for different session.run() calls? If that's the case, can you try using a unique different URL (i.e. ,directory) for each session.run() call?\r\n\r\n", "@caisq The script runs if modified s.t. a new wrapper/directory is used for each call to `sess.run()`; however, none of the tensors show up inside the debugger CLI, i.e. `0 dumped tensor(s):`.\r\n\r\nAs a separate comment, is there a reason why separate wrappers/directories are needed for each call to `sess.run()`? The wrapper creates subdirectories `run_<run_id>`, suggesting that reuse of a single wrapper might be possible --- which would help with usability.\r\n\r\n", "It should work with a single wrapper.\r\n\r\nAs for the reason why you see 0 dumped tensor(s), it might be that\r\n1) the node_name_regex_whitelist value returned from your watch_fn hits no nodes in the graph? What if you use watch_fn=None, so the default behavior (i.e., watch all nodes) can be used?\r\n2) ops are constant folded during graph optimization (less likely)?"]}, {"number": 8046, "title": "Windows: Fix error in release script for C library", "body": "Bug in 8777d9a15614b65271d25401f488b70b6b3dca1e", "comments": []}, {"number": 8045, "title": "Update mnist_replica.py", "body": "Adding elif statement", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "It failed pylint. Please fix.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/3419/console", "Closing PR on six days of no response. If you'd like to work on this PR more, feel free to re-open it."]}, {"number": 8044, "title": "Broken link in Adding a New Op to zero_out_op_1.py", "body": "The link in [Adding a New Op](https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/docs_src/extend/adding_an_op.md#L244) to `zero_out_op_1.py` is broken. It points to: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/g3doc/how_tos/adding_an_op/zero_out_op_1.py.", "comments": ["Hi @cancan101, I briefly mentioned this broken link in #8029 as an example of the many broken g3doc links.  It looks like all of the broken links on the Adding a New Op page have been fixed by #8066, which has just been merged.", "I will then close this issue, since it looks like it is resolved.\r\nPlease comment if it is not, then I can reopen it right away.", "My mistake, the broken links on Adding a New Op are not fixed by the pull request.  The code files are where they should be, but the links weren't changed in [adding_an_op.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md).", "reopened :)", "It looks like all that is required is a quick find-replace on the [adding_an_op.md](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md) file:  g3doc/how_tos &#x27F6; examples", "@josh11b @dr4b  Any ideas what's still broken?", "we re-added these files to the repo here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/adding_an_op\r\n\r\nbut the links have to be fixed, I think.", "Links are fixed in #8104."]}, {"number": 8043, "title": "DNNLinearCombinedClassifier weight_column_name doesn't work in v1.0", "body": "After migrating to tensorflow v1.0 of my sources, weight_column_name in estimators (tested in tf.contrib.learn.DNNLinearCombinedClassifier) seems to not working now.\r\nScripts now outputs this error:\r\n`ERROR:tensorflow:Could not create metric ops for MetricSpec(metric_fn=_predictions_streaming_mean, prediction_key=logistic, label_key=None, weight_key=class_weights)`\r\n\r\n### Environment info\r\nOperating System: \r\nCentOS Linux release 7.3.1611 (Core)\r\n\r\nDocker image within TF package:\r\n1. Docker latest image: tensorflow/tensorflow\r\nCreated container:\r\n`docker run -it -p 8888:8888 -v ..... --memory=12g tensorflow/tensorflow`\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`:\r\n`1.0.0`\r\n\r\n### Reproducing\r\n\r\n1. Download wide_n_deep_tutorial.py: [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py](url)\r\n(This solely works fine)\r\n2. Then add weightings (insert lines at):\r\nline 26 `import numpy as np`\r\nline 130 `weight_column_name='class_weights',`\r\nline 153 `feature_cols['class_weights'] = tf.constant(np.ones(np.shape(df[LABEL_COLUMN].values)))`\r\n3. Code executes with error:\r\n`ERROR:tensorflow:Could not create metric ops for MetricSpec(metric_fn=_predictions_streaming_mean, prediction_key=logistic, label_key=None, weight_key=class_weights)`\r\nFull trace of error is below.\r\n\r\nIf we repeat this procedures with version 0.10, code works well.\r\n\r\nI tried to reshape weight feature col to [2, n], to [n, 2] - also, doesn't help.\r\n\r\n### Trace\r\n```\r\nERROR:tensorflow:Could not create metric ops for MetricSpec(metric_fn=_predictions_streaming_mean, prediction_key=logistic, label_key=None, weight_key=class_weights).\r\nTraceback (most recent call last):\r\n  File \"wide_n_deep_tutorial.py\", line 236, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"wide_n_deep_tutorial.py\", line 199, in main\r\n    FLAGS.train_data, FLAGS.test_data)\r\n  File \"wide_n_deep_tutorial.py\", line 188, in train_and_eval\r\n    m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 426, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 934, in _train_model\r\n    model_fn_ops = self._call_legacy_get_train_ops(features, labels)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1003, in _call_legacy_get_train_ops\r\n    train_ops = self._get_train_ops(features, labels)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1162, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 312, in _dnn_linear_combined_model_fn\r\n    features, labels, mode, _make_training_op, logits=logits)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 531, in head_ops\r\n    labels, predictions)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1390, in _eval_metric_ops\r\n    return estimator._make_metrics_ops(metrics, features, labels, predictions)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 258, in _make_metrics_ops\r\n    result[name] = metric.create_metric_ops(features, labels, predictions)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/metric_spec.py\", line 220, in create_metric_ops\r\n    weights=inputs[self.weight_key])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1440, in _predictions_streaming_mean\r\n    return metrics_lib.streaming_mean(predictions, weights=weights)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/metrics/python/ops/metric_ops.py\", line 392, in streaming_mean\r\n    updates_collections=updates_collections, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/metrics_impl.py\", line 262, in mean\r\n    math_ops.to_float(weights), values)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 166, in broadcast_weights\r\n    with ops.control_dependencies((assert_broadcastable(weights, values),)):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 102, in assert_broadcastable\r\n    weights_rank_static))\r\n```\r\n### Update 3/10/2017\r\nSomehow in process of editing/formatting this post, forgot one line at the end of traceback:\r\n`ValueError: weights can not be broadcast to values. values.rank=2. weights.rank=1.`", "comments": ["Thanks for filing the issue @erko \r\n\r\n@martinwicke might know about this, or might know who can investigate why `tf.contrib.learn.DNNLinearCombinedClassifier(weight_column_name)` isn't working correctly.", "This is a bug. Workaround is to reshape weights to (batch, 1).\r\n\r\ne.g.,\r\nfeature_cols[\"class_weights\"] = tf.reshape(\r\n      tf.ones_like(df[LABEL_COLUMN].values), shape=(-1, 1))", "@honkentuber Thanks for help,\r\nbut not works this way too:\r\nwithout reshape, error message: `ValueError: weights can not be broadcast to values. values.rank=2. weights.rank=1.`\r\nwith reshape: `ValueError: weights can not be broadcast to values. values.rank=1. weights.rank=2.`\r\nTried exactly your line of code, and some variations, results are same. Looks like problem is where weights.rank is adapting to values.rank\r\n", "Sorry, I wasn't running against the right version. Also, add this line just after `label` is created in `input_fn`:\r\nlabel = tf.reshape(label, shape=(-1, 1))", "@honkentuber thanks, you absolutely right. Reshaping label too (w class_weight) helps to solve this problem. \r\n\r\n", "Hi ! @honkentuber  @ispirmustafa   ( i don' mean to bother tagging you.. ) \r\n\r\ni'm having the same issue. but changing the shape of the label tensor didn't work . i define my metrics as:\r\n```\r\nMETRICS = {\r\n    'loss' : metric_spec.MetricSpec (\r\n        metric_fn = metric_ops.streaming_mean , \r\n        prediction_key = 'loss' ,\r\n        weight_key = None ,\r\n        label_key = None \r\n    )\r\n}\r\n```\r\nloss is a tensor holding my loss function \r\n\r\nand pass it to the eval_metrics argument, creating a Experiment object. but i i run it i get the same error, \r\n**ValueError: weights can not be broadcast to values. values.rank=0. weights.rank=2**\r\n\r\ni'm using a batch size of 2 ( for debuging ) . But i find it really weird because i specify **weight_key = None**, and by checking the code, it says that in that case the weights arre just 1.  Some how a \"weights\" parameter is reaching streaming_mean , even if i said that i dont want it to be weigthed. \r\nThe output of my  **input_fn** is a list   [ images , labels   ]\r\n\r\nSo.. what i'm doing wrong? \r\nThanks  in advance :) ", "I believe the problem here is that the streaming_mean function doesn't take\r\na label argument, so it's mistaking labels for weights. You can\r\nreplace metric_ops.streaming_mean\r\nwith:\r\n\r\ndef _predictions_streaming_mean(labels, predictions, weights):\r\n  return metric_ops.streaming_mean(values=predictions, weights=weights)\r\n\r\nOn Fri, Mar 24, 2017 at 11:24 PM, Andr\u00e9s Huertas Suarez <\r\nnotifications@github.com> wrote:\r\n\r\n> Hi !\r\n>\r\n> i'm having the same issue. but changing the shape of the label tensor\r\n> didn't work . i define my metrics as:\r\n>\r\n> METRICS = {\r\n>     'loss' : metric_spec.MetricSpec (\r\n>         metric_fn = metric_ops.streaming_mean ,\r\n>         prediction_key = 'loss' ,\r\n>         weight_key = None ,\r\n>         label_key = None\r\n>     )\r\n> }\r\n>\r\n> and pass it to the eval_metrics argument, creating a Experiment object.\r\n> but i i run it i get the same error,\r\n> *ValueError: weights can not be broadcast to values. values.rank=0.\r\n> weights.rank=2*\r\n>\r\n> i'm using a batch size of 2 ( for debuging ) . But i find it really weird\r\n> because i specify *weight_key = None*, and by checking the code, it says\r\n> that in that case the weights arre just 1. Some how a \"weights\" parameter\r\n> is reaching streaming_mean , even if i said that i dont want it to be\r\n> weigthed.\r\n> The output of my *input_fn* is a list [ images , labels ]\r\n>\r\n> So.. what i'm doing wrong?\r\n> Thanks in advance :)\r\n>\r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/8043#issuecomment-289192348>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/AS8QxfM22FXVRRT_MZ2hetHFbhmze1NUks5rpLM3gaJpZM4MR4Ij>\r\n> .\r\n>\r\n", "Yeah \r\n\r\ni ended up solving it by doing something similar, \"masking\" the streaming_mean function, and passint to it weights  = tf.ones( shape= () , dtype=tf.float32 )\r\n\r\nthat did the trink ;) thanks.", "FYI, this should be fixed in 1.1.\n\n\nPhilip Tucker |  Software Engineer |  ptucker@google.com |\n\nOn Mon, Mar 27, 2017 at 9:57 AM, Andr\u00e9s Huertas Suarez <\nnotifications@github.com> wrote:\n\n> Yeah\n>\n> i ended up solving it by doing something similar, \"masking\" the\n> streaming_mean function, and passint to it weights = tf.ones( shape= () ,\n> dtype=tf.float32 )\n>\n> that did the trink ;) thanks.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8043#issuecomment-289515253>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AS8QxZwiwyDdCdghtePh8C_PZu9faGMvks5rp-p4gaJpZM4MR4Ij>\n> .\n>\n", "Hello... @honkentuber \r\n\r\nTensorflow version: 1.2.1\r\nAnaconda version: 1.6.3\r\nOS: Windows Server R2 2008 Datacenter\r\n\r\nI also run into the same problem with DNNClassifer: \r\n\r\n* Logging error:\r\n\r\n> Message: 'Weights {} has shape {}, expanding to make it 2d.'\r\n> Arguments: ('sample_weight', TensorShape([Dimension(60480)]))\r\n\r\n* ValueError:\r\n\r\n> ValueError: weights can not be broadcast to values. values.rank=1. weights.rank=2. values.shape=(60480,). weights.shape=(60480, 1).\r\n\r\nI have fixed according to your instruction but later I get this error (which I believe this is due to the fact that 'sample_weight' was forced to the shape(60480,1)):\r\n\r\n> ValueError: Features are incompatible with given information. Given features: {'is_signed_out': <tf.Tensor 'Const:0' shape=(60480,) dtype=bool>, 'timesend_start_mapping_hour': <tf.Tensor 'Const_3:0' shape=(60480,) dtype=int64>, 'urls_per_cookie': <tf.Tensor 'Const_8:0' shape=(60480,) dtype=int64>, 'timesend_start_mapping_minute': <tf.Tensor 'Const_4:0' shape=(60480,) dtype=int64>, 'geoip_lon': <tf.Tensor 'Const_2:0' shape=(60480,) dtype=float64>, 'res_width': <tf.Tensor 'Const_5:0' shape=(60480,) dtype=float64>, 'sample_weight': <tf.Tensor 'Reshape_1:0' shape=(60480, 1) dtype=float64>, 'ua_os_family': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x000000000C7670B8>, 'geoip_lat': <tf.Tensor 'Const_1:0' shape=(60480,) dtype=float64>, 'res_height': <tf.Tensor 'Const_6:0' shape=(60480,) dtype=float64>, 'urlrefs_per_cookie': <tf.Tensor 'Const_7:0' shape=(60480,) dtype=float64>}, required signatures: {'is_signed_out': TensorSignature(dtype=tf.bool, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'timesend_start_mapping_hour': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'geoip_lon': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'res_width': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'timesend_start_mapping_minute': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'ua_os_family': TensorSignature(dtype=tf.string, shape=None, is_sparse=True), 'geoip_lat': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'sample_weight': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'res_height': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'urls_per_cookie': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(60480)]), is_sparse=False), 'urlrefs_per_cookie': TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(60480)]), is_sparse=False)}.\r\n\r\nWhat should I do now?\r\n\r\nThank you very much in advance!", "This is still not fixed in TensorFlow 1.3.0.\r\n\r\nI'm using the high-level API only, and define the weights column in exactly the same fashion as I define other feature columns:\r\n\r\n`tf.contrib.layers.real_valued_column(self.schema.label_weight_column)`\r\n\r\n... where I have tests that prove that `self.schema.label_weight_column` is the string naming my weight column `\"outcome_class_weight\"`\r\n\r\nI then define the model as follows, where `deep_feature_columns` includes `tf.contrib.layers.real_valued_column(self.schema.label_weight_column)`:\r\n\r\n    tf.contrib.learn.DNNClassifier(\r\n      model_dir=self.schema.model_dir,\r\n      feature_columns=deep_feature_columns,\r\n      weight_column_name=self.schema.label_weight_column,\r\n      hidden_units=self.schema.hidden_units,\r\n      activation_fn=self.schema.activation_fn,\r\n      dropout=self.schema.dropout_probability,\r\n      gradient_clip_norm=self.schema.gradient_clip_norm_ratio,\r\n      enable_centered_bias=self.schema.enable_centered_bias,\r\n      optimizer=optimizer,\r\n      config=run_config\r\n    )\r\n\r\nFinally, upon running, I see the weights broadcast Exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"runner.py\", line 89, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"runner.py\", line 40, in main\r\n    metrics = trainer.train_and_test()\r\n  File \"/Users/aaron/git/bb_tensorflow/domain/model_trainer.py\", line 93, in train_and_test\r\n    monitors=[self.__build_monitor__()]\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 296, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 458, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 958, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1165, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1136, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 209, in _dnn_model_fn\r\n    logits=logits)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 854, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 651, in _create_model_fn_ops\r\n    weighted_average_loss, predictions, labels, weight_tensor)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 908, in _metrics\r\n    _indicator_labels_streaming_mean(labels, weights))\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1937, in _indicator_labels_streaming_mean\r\n    weights = weights_broadcast_ops.broadcast_weights(weights, labels)\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 167, in broadcast_weights\r\n    with ops.control_dependencies((assert_broadcastable(weights, values),)):\r\n  File \"/Users/aaron/git/bb_tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/weights_broadcast_ops.py\", line 103, in assert_broadcastable\r\n    weights_rank_static, values.shape, weights.shape))\r\nValueError: weights can not be broadcast to values. values.rank=1. weights.rank=2. values.shape=(18018,). weights.shape=(18018, 1).\r\n```\r\n\r\nI'm going to attempt @honkentuber 's proposed fix, but I strongly suspect I'll encounter @ntvy95 's exception after doing so.  (Will update if not.)\r\n\r\nAlso, apologies for being forward, but why was this ticket closed when the exception still occurs in the most recent stable version of TensorFlow?", "Is this a problem with `tf.estimator.Estimator`? I hope not, and I believe that that is why this issue was closed.\r\n\r\nFYI @ispirmustafa ", "contrib DNNClassifier doesn't support a weight column. it expects that weight is processed within the input_fn properly.\r\ncore DNNClassifier (tf.estimator.DNNClassifier) accepts a weight column. Please use that instead.", "I was able to get this working by riffing off the suggested fix, and changing only the label definition provided by the input function for all my models as per this git diff:\r\n\r\n```\r\n-    label = tf.constant(\r\n-      df[self.schema.label_column].values,\r\n-      dtype=tf.bool\r\n+    label = tf.reshape(\r\n+      tf.constant(\r\n+        df[self.schema.label_column].values,\r\n+        dtype=tf.bool\r\n+      ),\r\n+      [-1, 1]\r\n     )\r\n```\r\n\r\nSo, while I would hope that the Estimator would take care of this for me, it's not clear whether or not its responsibility is to take care of it for me. :-) \r\n\r\nIOW, I'm lucky I was able to suss this relatively quickly based upon @honkentuber 's comments: others might not be so fortunate, so some fix would be cool.  Just not sure where that would be.\r\n\r\nFinally, while I have you ;-) , two quick questions that you can send me to Stack Overflow for if you don't want to deal:\r\n\r\nI stupidly created my dev/test sets with the weight column.  But, obviously, for tuning and testing, I don't want the weight column in my features or defined by the model.  (I tried it, and the presence of the weight column garnered an accuracy of 99.4% because the weight column is completely correlated with the label). And, clearly, when making predictions, defining that column is right out.\r\n\r\nSo, the first question is: Will it be possible to load a trained model (via its `model_dir`) into an Estimator definition that doesn't define the weight column in the features or as the `weight_column_name` param?  (If not, then what exactly is the utility of the `weight_column_name` param?)\r\n\r\nI'm going to try that soon, but can't immediately due to the the following question:\r\n\r\nI'm using the (deprecated) Early Stopping ValidationMonitor feature described in \"Logging and Monitoring\" tutorial ... that appears to have been taken down recently. (?!)  The problem is that the Early Stopping ValidationMonitor is passed into the model during training, and thus the input function it uses for test data may (?) need to provide the same features for the dev/test set as it does for the training set.  \r\n\r\nThe question then is: does its input function need to match the signature of the training input function?  \r\n\r\nIf so, I'll (tragically!) need to stop using this feature, which I love because it narrows the wide degree of freedom of manually tuning `training_steps`.  IOW, tuning `early_stopping_steps` can occur over a much narrower range of values than tuning `training_steps`, which I dig.", "@ispirmustafa - Ok, I'll try that, but according top https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNClassifier#__init__ the `weight_column_name` field is supported.\r\n\r\nAlso, porting-over would require some considerable reworking.  \r\n\r\nShort of that, how would I go about \"processing the weight in the input_fn\" if I just wanted to keep the contrib Estimator?\r\n\r\nAgain, feel free to banish me to StackOverflow if it's not appropriate for me to bug you on the topic.", "I agree `weight_column_name` is misleading. it is asking for a name actually. we fixed that in core estimators.", "Yup, I'm providing it with a column name, and reshaping my label solved the crash I was getting.  \r\n\r\nAs described in my comments above, assuming I stay with the contrib Estimator, the problem is now that I need to figure out how to test a trained model without the weight feature being present, and also how to use a trained model for predictions without that weight feature being present.\r\n\r\nOr, are you saying that the contrib DNNClassifier `weight_column_name` param just doesn't work at all?\r\n\r\nBasically, I think you described the correct approach to solving the problem by stating that the weight should be processed in the input function.  That way, it never ends up in the features, and the model doesn't need to be told about it.\r\n\r\nI just don't know how to do that ... yet. :-)  Any hints would be very much appreciated.\r\n", "the input_fn you send to predict or export should not have 'weight' in features dict. ", "I tried that, though with `tf.contrib.learn.DNNClassifier` rather than `tf.estimator.DNNClassifier`. (I don't think it matters which I use because both the `weight_column_name` of the former and the `weight_column` of the latter expect the column referenced to be a feature column.)\r\n\r\nSpecifically, if I:\r\n\r\n1. Construct a `tf.contrib.learn.DNNClassifier`, and specify `weight_column_name` and include that column among the `feature_columns`.\r\n\r\n2. Call its `fit` method with an `input_fn` that includes the weight column among the feature_columns.\r\n\r\n3. Call its `evaluate` method with an `input_fn` that does **not** supply the weight column among the feature_columns.\r\n\r\n... then `evaluate` crashes with an error that the weight column is missing.\r\n\r\nIf, instead, I try this approach:\r\n\r\n1. Construct a `tf.contrib.learn.DNNClassifier`, and specify `weight_column_name` and include that column among the `feature_columns`.\r\n\r\n2. Call its `fit` method with an `input_fn` that includes the weight column among the feature columns.\r\n\r\n3. Construct a new `tf.contrib.learn.DNNClassifier` that uses the same `model_dir` as the classifier created in step 1, but that does not specify the `weight_column_name` param and does not include the weight column among the `feature_columns`.\r\n\r\n4. Call the `evaluate` method of the new classifier with an `input_fn` that does **not** include the weight column among the feature columns.\r\n\r\n... then it crashes, as I described in this [StackOverflow](https://stackoverflow.com/a/46249103/915989) issue.\r\n\r\nIt seems to me that the requirement that the class weights column be one of the model's feature columns renders this feature entirely useless for remedying class imbalance.\r\n\r\n(It's undoubtedly useful for other use cases; I'm just not currently interested in any of them.)  :-)  :-)", "you should not include `weight_column`(`_name`) as part of `feature_columns`. Providing that as part of `feature_columns` is saying that you're using it as a feature which is wrong.", "If I don't provide it as a feature column, I get an error that it needs to be provided as a feature column.\r\n\r\nAlso, here's the doc for the param of `tf.contrib.learn.DNNClassifier`, with emphasis added:\r\n\r\n```\r\nweight_column_name: A string defining **feature column** name representing weights. \r\nIt is used to down weight or boost examples during training. It will be multiplied by the loss of the example.\r\n```\r\n\r\n... and, here's the doc for the param of `tf.estimator.DNNClassifier`, with bolding added:\r\n\r\n```\r\nweight_column: A string or a _NumericColumn created by tf.feature_column.numeric_column defining **feature column** representing weights. \r\nIt is used to down weight or boost examples during training. \r\nIt will be multiplied by the loss of the example. \r\nIf it is a string, it is used as a key to fetch weight tensor **from the features**. \r\nIf it is a _NumericColumn, raw tensor is fetched by key weight_column.key, then weight_column.normalizer_fn is applied on it to get weight tensor.\r\n```\r\n\r\nI tried not defining it as a feature column, hoping that documentation was in error.  But, it's not.", "it would be great if you can create a small example where you see the problem. then we can discuss the code of that example.", "did someone find the solution to this? :-) \r\n@brightbytes-dude you maybe? I have exactly the problem you describe up here with API 1.4", "@ednaruiz - I still owe the thread some failing `pytest`s to demonstrate the issues with various attempted workarounds. (I haven't found the time to yet, and sadly am now working on another project for a few weeks.)\r\n\r\nUltimately though, the only solutions I could come up with are:\r\n\r\n1. Over-sample the under-represented class's examples.  \r\n\r\n2. Use the low-level API to define the loss function by hand such that it uses the contents of the weight column when determining loss, and do not define the weight column as a feature column.  (I'm not sure how to do this yet, but there seems enough data on the web that I think I could figure it out.)\r\n\r\nTo reiterate the problem: if one attempts to solve the Class Imbalance problem by defining a weight column as a feature column when the class weights correspond to the class, then the network can trivially attain perfect accuracy simply by 100% correlating the weight to the outcome.\r\n\r\nFor example, say I'm building a dog classifier and I have 100 pictures, 95 of which are **not** dogs, and the remaining 5 are dogs.\r\n\r\nI then add a weight column to my dataset, where the not-dog pictures have a class weight of 1/95 and the dog pictures have a weight of 1/5, and I specify the weight column when constructing my model.\r\n\r\nBy doing so, the weight column becomes a feature column.  Because it's a feature column, my input function needs to supply values for it, and my model will use it to \"guess\" the correct class.  \r\n\r\nI then train my model, and find that no matter how bad the dog pictures are, every training run always achieves 100% accuracy.  \r\n\r\nThat is, because the weight column is a feature column, the model trivially learns that if the class weight is 1/5, it's a dog, and if the class weight is 1/95, it's not a dog. Viva la ML!! :-)\r\n\r\nFurthermore, since my test runs and prediction runs require that the column be present (because it's a feature column of the model), I'm stuck specifying **something** when testing and predicting.  \r\n\r\nFor my actual model, hoping I could work around this, I tried setting the weight column value for testing and predicting to first the mean and then the midrange, but in both cases the trained models had worse accuracy than if I'd never used the column weights param.  So, I gave up.\r\n\r\nMy conclusion is that one can only use the `weight_column_name` parameter when the values in that column are **not** correlated with the outcome.  \r\n\r\nHowever, to solve the Class Imbalance problem, the `weight_column_name` must be correlated with the outcome.  So, I can't use this param in my modeling.", "@brightbytes-dude [testLossWithWeights in dnn_test.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/dnn_test.py#L632) is testing the case you're describing. Check it out to see how you can use weight column in your input_fn without adding it to DNNClassifier feature_columns. You mentioned that you've tried exactly that but it seems there may be just a simple bug in your code. Hope this helps locating the issue.", "@brightbytes-dude I think you are experiencing the same exact issue I am https://stackoverflow.com/questions/48460913/how-do-i-properly-use-the-weight-column-when-working-with-tf-estimator-dnnclassi\r\n\r\nDid you ever figure out a solution to it?", "@tilarids - Sorry for the (very!) long delay: I wasn't able to get back to the project until recently.\r\n\r\nThe test case you indicated allowed me to get this working on my project: thank you very much for referencing it!!\r\n\r\nMy mistake was to take the documentation literally when it says `weight_column_name: A string defining feature column name representing weights.`.  I assumed that meant that `weight_column_name` - `w` in the test - **must** be one of the model features.  Whereas, the test case clearly shows that it absolutely should not be.  (And, when not including it among the model features didn't work the first time, I assumed it was because the feature was broken, rather than what must have been the case: that something else in my code was broken.)\r\n\r\nFurthermore, using weights had exactly the effect I'd hoped: it improved training & test Recall at 50% threshold in my binary classification problem, though of course at the expense of reducing Accuracy.  But AUC remains the same whether or not weights are used, demonstrating to my satisfaction that the TF implementation of case weighting is correct.\r\n\r\nFinally, I have one question about that test case: in that example, one of the the prediction cases for evaluation has a different weight than the rest.  What is the actual use case for having a batch of predictions where the individual cases have different weights associated with them?\r\n\r\nI ask because in my domain, all cases to be labelled with a prediction matter equally.  So, I was planning on just passing in the exact same weight for every single one of them, for every batch.  Thus, the second question: does it matter what constant weight I choose for the cases to be evaluated?\r\n\r\nApologies in advance if I should be asking these questions elsewhere ..."]}, {"number": 8042, "title": "Estimator Tensor not from same graph for predict()", "body": "I run Tensorflow 1.0.\r\nI use Mac OSX Sierra.\r\nI use the `Estimator` and `Layers` api.\r\n\r\nThis is the first part of my `model_fn` which I pass to `.Estimator(model_fn=...)`.\r\n```\r\nclass Model(object):\r\n\r\n  def __init__(self):\r\n    ...\r\n    self._estimator = learn.Estimator(\r\n        model_fn=self.model_fn,\r\n        model_dir=modeldir,\r\n        params={\r\n            'width': width,\r\n            'height': height,\r\n            'classes': classes\r\n        },\r\n        config=RunConfig(save_checkpoints_secs=30, gpu_memory_fraction=0.75)\r\n    )\r\n\r\n  def model_fn(self, features, labels, mode, params):\r\n    input_layer = tf.reshape(features, [-1, width, height, 1], name='fitinput')\r\n\r\n    conv1 = tf.layers.conv2d(\r\n        inputs=input_layer,\r\n        filters=32,\r\n        kernel_size=[5, 5],\r\n        padding=\"same\",\r\n        activation=tf.nn.relu,\r\n        name=\"conv1\"\r\n    )\r\n...\r\n```\r\nI only use `input_fn` to pass data to the individual steps (images, labels).\r\nI train with `estimator.fit(...)` and evaluate with `estimator.evaluate(...)`. Both work fine and create summaries and checkpoints in my model directory.\r\nAfter that I run `estimator.predict(...)` with the same data that I trained on (just to troubleshoot this problem). For `predict` I only pass images.\r\nCalling `predict` results in the following error:\r\n```\r\n...\r\n    estimator.predict(input_fn=predict_data_fn)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 280, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 561, in predict\r\n    as_iterable=as_iterable)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 865, in _infer_model\r\n    infer_ops = self._call_legacy_get_predict_ops(features)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 996, in _call_legacy_get_predict_ops\r\n    infer_ops = self._get_predict_ops(features)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1217, in _get_predict_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.INFER)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1133, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"ocrcnn.py\", line 197, in cnn_model_fn\r\n    name=\"conv1\"\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 509, in conv2d\r\n    return layer.apply(inputs)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 303, in apply\r\n    return self.__call__(inputs, **kwargs)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 273, in __call__\r\n    outputs = self.call(inputs, **kwargs)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 156, in call\r\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 586, in convolution\r\n    with ops.name_scope(name, \"convolution\", [input, filter]) as name:\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/contextlib.py\", line 59, in __enter__\r\n    return next(self.gen)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 4219, in name_scope\r\n    g = _get_graph_from_inputs(values)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3968, in _get_graph_from_inputs\r\n    _assert_same_graph(original_graph_element, graph_element)\r\n  File \"/users/anaconda/envs/ocnncr/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3907, in _assert_same_graph\r\n    \"%s must be from the same graph as %s.\" % (item, original_item))\r\nValueError: Tensor(\"conv1/kernel:0\", shape=(5, 5, 1, 32), dtype=float32_ref) must be from the same graph as Tensor(\"fitinput:0\", shape=(50, 28, 28, 1), dtype=float32).\r\n```\r\n\r\nI have no idea if this is a bug or I did something wrong. So I am sorry if this should have been a Stackoverflow issue instead.", "comments": ["I did some more testing now. When I move the code out of the `class` then I can call `predict` without any problem. But when calling the estimator from outside the class like so:\r\n```\r\nmodel = Model()\r\nmodel._estimator.predict(input_fn=input_fn)\r\n```\r\nThen I get the above mentioned error.", "I found this issue https://github.com/tensorflow/tensorflow/issues/4026 which solved my problem ... Maybe it is just me being stupid, but it would be great if you mention that the tensors and ops all have to be inside the input_fn somewhere in the documentation.", "@sleighsoft Could you please post the edited the code that works. I'm facing the same issue when trying to export a model using `estimator`. The method `estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)` requires an input receiver function. Where should the placeholder for feeding inputs during inference be placed.  How is that placeholder connected to the input pipeline. I'm using `tf.data.Dataset` for input pipeline.", "@deepaksuresh  I'm facing the same problem.\r\nI've written an input_fn to export a saved from a Estimator-based model that for training receives data from tf.data.Dataset. It seams that another graph is created when doing this.", "@rodrigofp-cit Could you please tell me what exactly you're trying to do?\r\nAre you trying to load the saved model and feed it data?", "@deepaksuresh Now I realized what I was doing wrong.\r\n\r\n**First**: I was not passing **export_outputs** argument to [tf.estimator.EstimatorSpec](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec). Now I'm doing this on my model_fn\r\n\r\n```python\r\nreturn tf.estimator.EstimatorSpec(\r\n                mode=mode,\r\n                predictions=prediction_dict,\r\n                loss=total_loss,\r\n                train_op=train_op,\r\n                eval_metric_ops=eval_metric_ops,\r\n                export_outputs={'predict':tf.estimator.export.PredictOutput(outputs=prediction)})\r\n```\r\n\r\n**Second**: I was trying to create a serving function that not only map what is received to a FixedLenFeature, but also decoded it to an image (in my case). This was causing trouble for me, so I decided to just make the mapping. This is my code;\r\n\r\n```python\r\ndef get_serving_fn():        \r\n        feature_spec = {'raw_bytes':tf.FixedLenFeature(dtype=tf.string,default_value=\"\",shape=())}       \r\n        return  tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec=feature_spec)\r\n```\r\n\r\nAnd on my model_fn I check whether what is received as **features** is a dict(export mode) or just a Tensor(eval or train)\r\n\r\n```python\r\n def model_fn(features, labels, mode, params):            \r\n            if mode != tf.estimator.ModeKeys.PREDICT:\r\n                # unfortunately, when exporting a saved model, features is a dict of Tensors, instead of a Tensor itself                                \r\n                image_batch = tf.map_fn(self.preproc_fn,features,dtype=tf.float32)\r\n                tf.logging.info('model_fn called for training, evaluation or prediction, not export')\r\n                tf.logging.info('Input shape not export: {}'.format(features.get_shape()))\r\n            else:\r\n                image_batch = tf.map_fn(self.preproc_fn,features['raw_bytes'],dtype=tf.float32)\r\n                tf.logging.info('model_fn called to export saved_model')\r\n                tf.logging.info('Input shape export: {}'.format(image_batch.get_shape()))\r\n```\r\n\r\nAnd export the saved model with\r\n\r\n```python\r\nestimator.export_savedmodel(export_dir_base=output_dir,\r\n                                serving_input_receiver_fn=get_serving_fn())\r\n```\r\n\r\nWell, to conclude. I've fixed the bug."]}, {"number": 8041, "title": "broken link for flower photos in the official tutorial about transfer learning", "body": "In official tutorial \"How to Retrain Inception's Final Layer for New Categories\", link for downloading flower photos is broken:\r\n`curl -O http://download.tensorflow.org/example_../images/flower_photos.tgz`\r\n", "comments": ["Thanks for filing the issue @zakizhou !  I've fixed this internally, and the site will be updated at some point.\r\n\r\nFYI the correct link is this:\r\n`curl -O http://download.tensorflow.org/example_images/flower_photos.tgz`", "curl -O http://download.tensorflow.org/example_images/flower_photos.tgz This link is working today. Thanks."]}, {"number": 8040, "title": "Is it able to set `allow_growth=True` to environment variables?", "body": "Hi all, we a device with 3 gpus, and there are several people who are learning deep-learning.\r\nHowever, while using tensorflow, all gpus will be used, which causes others not able to calculate.\r\n\r\nWe know setting `config.gpu_options.allow_growth = True` in python code can avoid this, but there are always some people who forget to do this.\r\n\r\nSo I wonder if we can set `config.gpu_options.allow_growth = True` to default (such as environment variables)? Thanks!", "comments": ["A PR to add this might take some work, but maybe there's a work-around using sitecustomize. IE, you can use sitecustomize to force Python to always run some code, and this code could be used to load tensorflow, and then monkey session init to use custom config. \r\n\r\nIE something like this:\r\n```\r\noldinit = tf.Session.__init__\r\ndef myinit(session_object, target='', graph=None, config=None):\r\n    print(\"Intercepted!\")\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\ntf.Session.__init__ = myinit\r\n\r\n```\r\nThis is more flexible than env-var on the TensorFlow side because you can do arbitrary configuration. For instance, you could run some code that automatically selects least-used GPU and restricts TensorFlow to use it through `CUDA_VISIBLE_DEVICES` as I'm doing in [notebook_util](https://github.com/yaroslavvb/stuff/blob/master/notebook_util.py#L46)", "Thanks for the suggestion @yaroslavvb !\r\n\r\n@ckcz123 There isn't such a mechanism to set session config defaults based on an environment variable.  Although it would be convenient in the scenario you're describing, such a mechanism would also make it harder to reason about what a particular snippet of code is expected to do.  In general we'd like to keep the configuration codepath reasonably straightforward, so I don't think we'll be adding such a mechanism.\r\n\r\nHope that makes sense.  I'm closing this out, but if you feel I'm mistaken, just respond to this issue and I'll re-open it.  Thanks!", "I think this specific option (allow growth) should exist as an env-var flag. Consider the following common scenario:\r\n\r\nI only own one machine, which has only one GPU. I sometimes want to run some random person's training code I got on github without it eating all my GPU memory. Because a little later, I want to start a notebook while the training is still running and still be able to do some tensorflow prototyping in it. While your monkey-patching trick is pretty neat, I'd like not to have to paste it into every code that I download.\r\n\r\nMy point being that whether to consume all GPU memory or not is much more a run-site setting than a code-site setting IMO.", "I guess this is a bit of a mismatch between google use-case and open-source use-case -- Google runs single TensorFlow process per GPU.\r\n\r\nTo avoid having to copy paste this into every snippet, you could do something like this:\r\n\r\n`alias python=\"python -m myconfig\"`\r\nand then put the snippet into `myconfig.py`\r\n\r\nThe general issue about global modifications like environment variables or monkey-patching is that it makes bug reporting harder. You have to make sure users provide copy of code, and also all values of environment variables they used. Then people forget about their environment variables, etc\r\n\r\n\r\n"]}, {"number": 8039, "title": "Fixing two small typos that were preventing the XLA compiler from building", "body": "Resolves #8031", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it! \r\n\r\nCompany Name: Cognitive Robotics, LLC", "@skycoop can you sign the CLA please?", "@tensorflow-jenkins test this please", "@skycoop Can you sign the CLA please?", "@caisq I have signed the CLA. One of the emails attached to this Github account is cooper@ctxrobotics.com, which is a member of the group associated with the CLA signed by Cognitive Robotics, LLC.", "Just realized the author info on my commit was using my personal email. Just amended the commit with the correct email address", "@skycoop, could you resolve conflicts in your PR, and comment \"I signed it!\" for CLA bot to force a recheck?", "Turns out the conflicts were caused by someone else solving the problem in a slightly different way. I'm going to close this PR since it now builds with those changes.", "how to resolve this issue?"]}, {"number": 8038, "title": "Allow setting name on accuracy op", "body": "Closes https://github.com/tensorflow/tensorflow/issues/7986", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "PR merged. Thank you for the contribution, @cancan101 !"]}, {"number": 8037, "title": "How to compile tensorflow using SSE4.1, SSE4.2, and AVX. ", "body": "Just got tensorflow running. Now running into this error. \r\n\r\nCurrently using Mac Yosemite, downloaded tensorflow using pip3 through anaconda, using python 3.5. \r\n\r\n`W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.`\r\n\r\n`W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.`\r\n\r\n`W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.`\r\n\r\nSo since anaconda has a special set of commands, how do you get tensorflow to run on SSE4.1, SSE4.2, and AVX via the anaconda command system ?  I am really confused how to go about this. \r\n", "comments": ["This isn't an error, just warnings saying if you build TensorFlow from source it can be faster on your machine.\r\n\r\nSO question about this: http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions\r\nTensorFlow guide to build from source: https://www.tensorflow.org/install/install_sources", "Just as @Carmezim stated these are simply warning messages.\r\nFor each of your programs, you will only see them once.\r\nAnd just like the warnings say, you should only compile TF with these flags if you need to make TF faster.\r\n\r\nYou can follow our guide to install TensorFlow from sources to compile TF with support for SIMD instruction sets.", "Ok, thanks. I get it.", "Is there a way we can silence this? ", "The only way to silence these warning messages is to build from sources, using `--config opt` option.", "A sort of \"workaround\" (albeit imperfect) that redirects the messages on Unix/Linux/OSX:\r\npython myscript.py 2>/dev/null\r\n", "@CGTheLegend @ocampesato you can use TF environment variable `TF_CPP_MIN_LOG_LEVEL` and it works as follows: \r\n- It defaults to **0**, displaying all logs\r\n- To filter out `INFO` logs set it to **1**\r\n- `WARNINGS` additionally, **2**\r\n- and to additionally filter out `ERROR` logs set it to **3**\r\n\r\nSo you can do the following to silence the warnings:\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n@gunan @mrry I've seen many folks interested in silencing the warnings, would there be interest in adding this kind of info to the docs?", "I install from tensorflow install guide, also got this warning.\r\n\r\n`pip3 install --upgrade tensorflow`", "@jadeydi  Instead of compiling from source, \"pip\" just install the binary as well, so that you'll still got these warnings.", "I just compiled tensorflow with support for SSE4.1 SSE4.2 AVX AVX2 and FMA. The build is available here: https://github.com/lakshayg/tensorflow-build . I hope this is useful.", "Hi @lakshayg, thanks for sharing. You might want to check https://github.com/yaroslavvb/tensorflow-community-wheels", "Approximately much faster is the build compared to the standard `pip install tensorflow-gpu` on Ubuntu?  Is it only faster for CPU computations, or is there any benefit to GPU computations?", "http://www.anandtech.com/show/2362/5\r\n\r\nThis came up on google and has some decent technical details.\r\n\r\n> test is a DivX encode using VirtualDub 1.7.6 and DivX 6.7. SSE4 comes in if you choose to enable a new full search algorithm for motion estimation, which is accelerated by two SSE4 instructions: `MPSADBW` and `PHMINPOSUW`. The idea is that motion estimation (figuring out what will happen in subsequent frames of video) requires a lot of computation of sums of absolute differences, as well as finding the minimum values of the results of those computations. The SSE2 instruction `PSADBW` can compute two sums of differences from a pair of 16B unsigned integers; the SSE4 instruction `MPSADBW` can do eight.\r\n\r\n...\r\n\r\n> On our QX9650, the full search with SSE4 enabled runs about 45% faster than with SSE2 only\r\n\r\nNow sure what functions tensorflow is using, but might be worth the effort.", "Sorry but this is a ridiculous thing to have output in all TF scripts by default. Most people probably aren't compiling TF from source nor want to.", "@TomAshley303, this is a pretty awesome info to get! I don't plan to recompile from source. I don't want to. But the info tells me what to do if my model becomes big and slow and will need a performance boost. It's usually cheaper to recompile with extensions than to buy new hardware, given that having good walkthroughts (which we do have) minimizes the labour cost of recompiling (CPU time does not matter, can run overnight).", "I went through the process... Was straight-forward and took no time at all. Not your usual cmake C++ kinda nightmare.", "I have a small bash script to compile TF under MacOS/Linux. It dynamically calculates CPU features and put them as the build parameters. I was thinking to create a PR but didn't find a folder with scripts (helpers) for local builds, only ci_build. If it makes sense I will do it\r\n\r\ngist\r\nhttps://gist.github.com/venik/9ba962c8b301b0e21f99884cbd35082f", "A note to @gunan\r\n\r\nI've encountered this issue when I was installing TensorFlow for the first time. Now I am having to figure out how to resolve it again because I'm installing TensorFlow on a new machine. It's a pain in the neck, and the documentation you've provided is not clear at all.\r\n\r\nThe fact that I have to do it on my end is ridiculous and infuriating. It's no good making something available from pip/pip3 if it then just throws warnings at you all day.\r\n\r\nAt the very least, you should edit [https://www.tensorflow.org/install/install_sources](https://www.tensorflow.org/install/install_sources) and explicitly explain how to compile it with SSE / AVX\r\n\r\nThe solution that worked for me: input \"-mavx -msse4.1 -msse4.2\" when prompted during the configuration process (when you run ./configure).\r\n\r\nIs it that hard to add this to your installation instructions? ", "Thank you, according to @Carmezim answer, I get the cpu speed up version based on avx and sse. I'v tested faster-rcnn(resnet-101) on Intel. Cost time speeds up about 30%, it is truly useful. ", "You can silence the warnings.\r\nJust add these codes at the top.\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\nimport tensorflow as tf\r\nAs mentioned here: https://stackoverflow.com/a/44984610", "you could easily add a user variable in System Environment Variable:\r\nTF_CPP_MIN_LOG_LEVEL, value = 2. Then restart your IDE", "@mikalyoung improvements for GPU computations cannot be expected, since those instructions set are CPU only, and they allow for vectorized operations.\r\nSo if you compare two codes running (ideally) 100% on GPUs, one on a Tensorflow instance compiled with SIMD support and one without, you should get the same results in terms of speed (and hopefully numerically also).", "I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n\r\nAs You can see the warning is with my system also but In that, I am not able to understand 'I' in the starting of the Warning so someone can help me in that case", "\"I\" there just is a shorthand for \"INFO\". The other letters you can see there are E for error, or F for fatal.", "So I installed using conda.  If I wish now to compile from source instead to take advantage of any speed boost, do I need to do anything to remove my conda install of tensorflow? Or is it in its own little container and I can separately compile from source?  ", "I had installed DeepSpeech and also a DeepSpeech server. Went to start the server and got an error message - \"2018-01-17 08:21:49.120154: F tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.\r\nAborted (core dumped)\"\r\n\r\nApparently I need to compile TensorFlow on the same computer. Is there a list somewhere to match Kubuntu 17.10.1 and a HP Probook 4330S please ?", "Why are there no windows compiles? I am having the same issues, but instead of muting the warnings I would like to use my GPU, I also have an and graphics card and not Nvidia what do I do?", "*I Do not have a Nvidia graphics card, I have an and one what do I do?", "*AMD graphics card.. autocorrect ", "These are not merely warnings as it kills the process on my test boxes. Since I also use AMD GPUs I spun up a Digital Ocean tensorflow box to give this a go, but it seems there is no GPU support there either, and it's failing miserably. \r\n\r\n`# Job id 0\r\n# Loading hparams from /home/science/tf-demo/models/nmt-chatbot/model/hparams\r\n  saving hparams to /home/science/tf-demo/models/nmt-chatbot/model/hparams\r\n  saving hparams to /home/science/tf-demo/models/nmt-chatbot/model/best_bleu/hparams\r\n  attention=scaled_luong\r\n  attention_architecture=standard\r\n  batch_size=128\r\n  beam_width=10\r\n  best_bleu=0\r\n  best_bleu_dir=/home/science/tf-demo/models/nmt-chatbot/model/best_bleu\r\n  check_special_token=True\r\n  colocate_gradients_with_ops=True\r\n  decay_factor=1.0\r\n  decay_steps=10000\r\n  dev_prefix=/home/science/tf-demo/models/nmt-chatbot/data/tst2012\r\n  dropout=0.2\r\n  encoder_type=bi\r\n  eos=</s>\r\n  epoch_step=0\r\n  forget_bias=1.0\r\n  infer_batch_size=32\r\n  init_op=uniform\r\n  init_weight=0.1\r\n  learning_rate=0.001\r\n  learning_rate_decay_scheme=\r\n  length_penalty_weight=1.0\r\n  log_device_placement=False\r\n  max_gradient_norm=5.0\r\n  max_train=0\r\n  metrics=['bleu']\r\n  num_buckets=5\r\n  num_embeddings_partitions=0\r\n  num_gpus=1\r\n  num_layers=2\r\n  num_residual_layers=0\r\n  num_train_steps=500000\r\n  num_translations_per_input=10\r\n  num_units=512\r\n  optimizer=adam\r\n  out_dir=/home/science/tf-demo/models/nmt-chatbot/model\r\n  output_attention=True\r\n  override_loaded_hparams=True\r\n  pass_hidden_state=True\r\n  random_seed=None\r\n  residual=False\r\n  share_vocab=False\r\n  sos=\r\n  source_reverse=False\r\n  src=from\r\n  src_max_len=50\r\n  src_max_len_infer=None\r\n  src_vocab_file=/home/science/tf-demo/models/nmt-chatbot/data/vocab.from\r\n  src_vocab_size=15003\r\n  start_decay_step=0\r\n  steps_per_external_eval=None\r\n  steps_per_stats=100\r\n  subword_option=\r\n  test_prefix=/home/science/tf-demo/models/nmt-chatbot/data/tst2013\r\n  tgt=to\r\n  tgt_max_len=50\r\n  tgt_max_len_infer=None\r\n  tgt_vocab_file=/home/science/tf-demo/models/nmt-chatbot/data/vocab.to\r\n  tgt_vocab_size=15003\r\n  time_major=True\r\n  train_prefix=/home/science/tf-demo/models/nmt-chatbot/data/train\r\n  unit_type=lstm\r\n  vocab_prefix=/home/science/tf-demo/models/nmt-chatbot/data/vocab\r\n  warmup_scheme=t2t\r\n  warmup_steps=0\r\n# creating train graph ...\r\n  num_bi_layers = 1, num_bi_residual_layers=0\r\n  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\r\n  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\r\n  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\r\n  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\r\n  learning_rate=0.001, warmup_steps=0, warmup_scheme=t2t\r\n  decay_scheme=, start_decay_step=0, decay_steps 10000, decay_factor 1\r\n# Trainable variables\r\n  embeddings/encoder/embedding_encoder:0, (15003, 512),\r\n  embeddings/decoder/embedding_decoder:0, (15003, 512),\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/memory_layer/kernel:0, (1024, 512),\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (1536, 2048), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (1536, 512), /device:GPU:0\r\n  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 15003), /device:GPU:0\r\n# creating eval graph ...\r\n  num_bi_layers = 1, num_bi_residual_layers=0\r\n  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n# Trainable variables\r\n  embeddings/encoder/embedding_encoder:0, (15003, 512),\r\n  embeddings/decoder/embedding_decoder:0, (15003, 512),\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/memory_layer/kernel:0, (1024, 512),\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (1536, 2048), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (1536, 512), /device:GPU:0\r\n  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 15003), /device:GPU:0\r\n# creating infer graph ...\r\n  num_bi_layers = 1, num_bi_residual_layers=0\r\n  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\r\n# Trainable variables\r\n  embeddings/encoder/embedding_encoder:0, (15003, 512),\r\n  embeddings/decoder/embedding_decoder:0, (15003, 512),\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/memory_layer/kernel:0, (1024, 512),\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (1536, 2048), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (1024, 2048), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (2048,), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\r\n  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (1536, 512), /device:GPU:0\r\n  dynamic_seq2seq/decoder/output_projection/kernel:0, (512, 15003),\r\n# log_file=/home/science/tf-demo/models/nmt-chatbot/model/log_1519669184\r\n2018-02-26 18:19:44.862736: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nKilled`\r\n\r\n", "what command needs to run and where to run these commands and how please tell.  I desperately need help.", "But does it mean that the system is not using GPU for the process?", "Well you need to resolve this if you are building tensorflow in an acceleration environment such as using k-fold in the KerasClassifier. \r\nTo resolve this you will need to build tensorflow from source just as everyone recommends.\r\nTo build tensorflow from source you will need to have the following tool\r\n1. Install git on you machine if you haven't down so already - on ubuntu machine just type \"sudo apt-get install git\r\n2. You will need to install bazel. It is highly recommended to use the custom APT repository. Follow the instruction on this link to install bazel  https://docs.bazel.build/versions/master/install-ubuntu.html.\r\n3. You need to the following python dependencies... using the command below\r\n       numpy, dev, and wheel\r\n sudo apt-get install python-numpy python-dev python-pip python-wheel\r\n4.Once you have all the dependencies installed, clone the tensorflow github to your local drive\r\n git clone https://github.com/tensorflow/tensorflow \r\n5. Go to the location to clone tensorflow and cd to the tensorflow file and run the config file\r\ncd tensor\r\n./configure\r\n\r\nJust follow the instruction on the screen to complete tensorflow installation.\r\nI will highly recommend to update your machine once tensorflow is installed\r\nsudo apt-get update\r\n\r\nGood luck and enjoy...\r\n\r\n    ", "Just chiming in on this thread that you shouldn't just silence these warnings - I'm getting about 43% faster training time by building from source, I think it's worth the effort.\r\n\r\n- Tensorflow's [instructions on building from source](https://www.tensorflow.org/install/install_sources) are pretty clear...\r\n- ...but they don't actually explain how to turn on SSE/AVX/FMA etc - so use [this thread](https://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions) to get an idea of how to set your Bazel build flags", "how to install tensorflow using this file\" tensorflow-1.6.0-cp36-cp36m-win_amd64.whl\"\r\n", "@anozele `pip3 install --upgrade *path to wheel file*`", "@gunan --config=opt is not enough, you should also add, e.g., --copt=\"-msse4.2\", when you build TensorFlow from source.", "According to Intel, https://software.intel.com/en-us/articles/intel-optimization-for-tensorflow-installation-guide, If you use intel built Tensorflow, you can ignore those warning since all available instruction set would be used by the backend MKL. Can anyone from Tensorflow confirm this?", "> This isn't an error, just warnings saying if you build TensorFlow from source it can be faster on your machine.\r\n> \r\n> SO question about this: http://stackoverflow.com/questions/41293077/how-to-compile-tensorflow-with-sse4-2-and-avx-instructions\r\n> TensorFlow guide to build from source: https://www.tensorflow.org/install/install_sources\r\n\r\nHowever,it is not faster than i do not use -FMA -AVX -SSE [https://stackoverflow.com/questions/57197854/fma-avx-sse-flags-did-not-bring-me-good-performance](url)", "Hi. Sorry if I am beating a dead horse. Just wondering why is the default pip wheel not the binaries compiled with advance instructions?", "> Hi. Sorry if I am beating a dead horse. Just wondering why is the default pip wheel not the binaries compiled with advance instructions?\r\n\r\nThis is because old cpu architectures don't support advanced instruction set. See [wiki](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) for the detailed list of cpus supporting  AVX, AVX2 or AVX512. If the default pip binary is compiled with these instruction sets then tensorflow cannot work on old CPUs.", "> But does it mean that the system is not using GPU for the process?\r\n\r\nNope, It shows even if you are using GPU, if you haven`t silenced the messages you should also see Tensorflor loading your GPU device in command prompt.", "If you check with this repo:\r\n\u8bf7\u67e5\u770b\u4e0b\u9762\u4ee3\u7801\uff1a\r\n\r\nhttps://github.com/fo40225/tensorflow-windows-wheel\r\n\r\nHe has compiled almost all version of TF with SSE and AVX\r\n\u4ed6\u5df2\u7ecf\u5c06\u51e0\u4e4e\u6240\u6709TF\u7248\u672c\u7f16\u8bd1\u597d\u4e86\uff01\r\n", "This article was a good tutorial on how to build from source including the flags\r\nhttps://medium.com/@pierreontech/setup-a-high-performance-conda-tensorflow-environment-976995158cb1\r\n\r\n> try forcing the inclusion of the appropriate extensions using additional bazel options like `--copt=-mavx --copt=-msse4.1 --copt=-msse4.2`"]}, {"number": 8036, "title": "Could LSTM model be used to predict from multivariate variables", "body": "I am doing a project on water content predictions. The input variables on time series include temperature, rainfall and so on. I do not know whether LSTM can predict from multiple variables or not. And I do not know how to do it? Could anybody please help me and provide such a related example? Thank you very much.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8035, "title": "obsfucate_names spelled wrong", "body": "I assume this isn't a pun...\r\n\r\nhttps://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=obsfucate_names", "comments": ["FYI @petewarden ", "Looks like this was fixed."]}, {"number": 8034, "title": "SparseTensor constructor change not noted as a breaking change in 1.0", "body": "The `shape` keyword argument of the `SparseTensor` constructor changes its name to `dense_shape` between Tensorflow 0.12 ([relevant API document](https://www.tensorflow.org/versions/r0.12/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor)) and Tensorflow 1.0 ([relevant API document](https://www.tensorflow.org/versions/r0.11/api_docs/python/sparse_ops/sparse_tensor_representation#SparseTensor)).  However, this is neither documented on [the migration page](https://www.tensorflow.org/install/migration) nor handled by the migration script.\r\n\r\n", "comments": ["@concretevitamin can you comment on this?", "Adding @ebrevdo @aselle @martinwicke.", "We should change the docs in the updates md AND update the auto TF\nrewriting tool to make this fix for the user.\n\nOn Fri, Mar 3, 2017 at 9:47 AM, Zongheng Yang <notifications@github.com>\nwrote:\n\n> Adding @ebrevdo <https://github.com/ebrevdo> @aselle\n> <https://github.com/aselle> @martinwicke <https://github.com/martinwicke>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8034#issuecomment-284021761>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-faaq2D3eLiWJAxdK-hKYYSvrA7ks5riFI3gaJpZM4MRp_t>\n> .\n>\n", "@gunan, could you update the release notes, and I'll update the tool", "Release.md is updated. @aselle, is the tool updated?\r\nPlease feel free to close once the tool is ready.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing as obsolete. "]}, {"number": 8033, "title": "Direct access to Tensor Buffers in C++ interface", "body": "I would like to request direct access to the tensorflow buffers through the C++ interface.  I have commented previously about this on [StackOverflow](http://stackoverflow.com/questions/39379747/import-opencv-mat-into-c-tensorflow-without-copying), and have gotten it to work by exposing the `TensorCApi` class.  This required some fiddling around with the source code.\r\n\r\nMy specific use case is using tensorflow from a [ROS](http://www.ros.org/) (robot operating system) node.  ROS has its own build system, so I have to link to the tensorflow library (libtensorflow.so) externally.  The C++ interface is much more convenient than the C interface, as I only load and do forward inference on static graphs.\r\n\r\nBecause I get an OpenCV array as input, my only other option is to iterate the entire array and copy it to a newly allocated tensor buffer as suggested in [this post](http://stackoverflow.com/questions/36044197/how-do-i-pass-an-opencv-mat-into-a-c-tensorflow-graph).  In my case, copying the buffer by iterating it can take 25-75ms, whereas simply pointing to the memory already allocated by OpenCV incurs almost no overhead.  I am doing all of this in real time as part of a tight control loop, so this extra time is absolutely critical.\r\n\r\nI realize this patch is probably not the right way to expose the interface, and my example code provides no memory checking or safeguards, but it is an example (diff against the r1.0 tensorflow tag):\r\n\r\n```\r\ndiff --git a/tensorflow/c/c_api.cc b/tensorflow/c/c_api.cc\r\nindex 83ce3e2..34ae2b4 100644\r\n--- a/tensorflow/c/c_api.cc\r\n+++ b/tensorflow/c/c_api.cc\r\n@@ -464,14 +464,11 @@ TF_Tensor* TF_Tensor_EncodeStrings(const Tensor& src) {\r\n                       dimvec.size(), base, size, DeleteArray, base);\r\n }\r\n \r\n-class TensorCApi {\r\n- public:\r\n-  static TensorBuffer* Buffer(const Tensor& tensor) { return tensor.buf_; }\r\n-  static Tensor MakeTensor(TF_DataType type, const TensorShape& shape,\r\n-                           TensorBuffer* buf) {\r\n-    return Tensor(static_cast<DataType>(type), shape, buf);\r\n-  }\r\n-};\r\n+TensorBuffer* TensorCApi::Buffer(const Tensor& tensor) { return tensor.buf_; }\r\n+Tensor TensorCApi::MakeTensor(TF_DataType type, const TensorShape& shape,\r\n+                         TensorBuffer* buf) {\r\n+  return Tensor(static_cast<DataType>(type), shape, buf);\r\n+}\r\n \r\n // Create an empty tensor of type 'dtype'. 'shape' can be arbitrary, but has to\r\n // result in a zero-sized tensor.\r\ndiff --git a/tensorflow/c/c_api.h b/tensorflow/c/c_api.h\r\nindex e625d65..7479a1f 100644\r\n--- a/tensorflow/c/c_api.h\r\n+++ b/tensorflow/c/c_api.h\r\n@@ -18,6 +18,7 @@ limitations under the License.\r\n \r\n #include <stddef.h>\r\n #include <stdint.h>\r\n+#include \"tensorflow/core/framework/tensor.h\"\r\n \r\n // --------------------------------------------------------------------------\r\n // C API for TensorFlow.\r\n@@ -64,6 +65,10 @@ limitations under the License.\r\n //   and the API just provides high level controls over the number of\r\n //   devices of each type.\r\n \r\n+using tensorflow::Tensor;\r\n+using tensorflow::TensorBuffer;\r\n+using tensorflow::TensorShape;\r\n+\r\n #ifdef __cplusplus\r\n extern \"C\" {\r\n #endif\r\n@@ -1030,6 +1035,15 @@ extern void TF_DeleteLibraryHandle(TF_Library* lib_handle);\r\n // in this address space.\r\n extern TF_Buffer* TF_GetAllOpList();\r\n \r\n+namespace tensorflow{\r\n+class TensorCApi {\r\n+public:\r\n+ static TensorBuffer* Buffer(const Tensor& tensor);\r\n+ static Tensor MakeTensor(TF_DataType type, const TensorShape& shape,\r\n+                          TensorBuffer* buf);\r\n+};\r\n+};\r\n+\r\n #ifdef __cplusplus\r\n } /* end extern \"C\" */\r\n #endif\r\ndiff --git a/tensorflow/core/framework/tensor.h b/tensorflow/core/framework/tensor.h\r\nindex c2a1c3d..14e9881 100644\r\n--- a/tensorflow/core/framework/tensor.h\r\n+++ b/tensorflow/core/framework/tensor.h\r\n@@ -36,7 +36,7 @@ limitations under the License.\r\n namespace tensorflow {\r\n \r\n class TensorBuffer;  // Forward declaration.\r\n-class TensorCApi;\r\n+// class TensorCApi;\r\n \r\n /// @ingroup core\r\n /// Represents an n-dimensional array of values.\r\n```\r\n\r\nand a small snippet of code to use it:\r\n```\r\n// Put an image in the cameraImg mat\r\ncv::resize(image->image, cameraImg, cv::Size(inputwidth, inputheight), 0, 0, cv::INTER_AREA);\r\n// Create a new tensor pointing to that memory:\r\nconst int64_t tensorDims[4] = {1,inputheight,inputwidth,3};\r\nint *imNumPt = new int(1);\r\nTF_Tensor* tftensor = TF_NewTensor(TF_DataType::TF_UINT8, tensorDims, 4,\r\n                           cameraImg.data, inputheight * inputwidth * 3,\r\n                           NULL, imNumPt);\r\nTensor inputImg = tensorflow::TensorCApi::MakeTensor(tftensor->dtype, tftensor->shape, tftensor->buffer);\r\n```", "comments": ["/cc @keveman, who was also interested in doing this today (and came up with a slightly different solution...).", "The C++ interface seems much more concise for this application, but it may be better to use the C interface.  The c interface header doesn't depend on any other headers, so building outside of the tensorflow source tree becomes much easier (a requirement for my project).  However, I cannot find any examples of loading and running a graph using the C api.  \r\n\r\nIf there are concise resources for the C api, I can investigate this pathway. ", "@josh11b Can you share your thoughts here?", "@pdrews I tried your approach, but I got the following error: \"undefined reference to `TF_NewTensor'\" -- Any pointers?", "@HossamAmer12 This should be in the `c_api.h` header in tensorflow.  That code snippet is not complete, so won't compile on its own. I had to add several headers from the c++ interface as well.  I added a line at the bottom of the code snippet that is required to use this new C tensor from the C++ interface.", "@pdrews Is there any chance that you can provide the steps anywhere (here, tutorial or anything)? \r\n\r\nWould be highly appreciated!", "@pdrews I would recommend using the C API for now. As you mentioned, it's more verbose but might make your project build simpler, and most importantly it already exposes the functionality you need :) Unfortunately we lack documentation on it so here's some info to get you started:\r\n\r\n_Building:_ https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.0.0.tar.gz is a pre-built Linux binary (CPU-only, I'm assuming your robot doesn't have a GPU). I believe this should work if ROS is built on top of a standard Linux distro. Otherwise it sounds like you have already figured out how to build from source.\r\n\r\n_Usage_: Here's an example on how to load and run a graph:\r\n```c++\r\nTF_Status* s = TF_NewStatus();\r\nTF_Graph* graph = TF_NewGraph();\r\n\r\nconst char* graph_def_data; // <-- your serialized GraphDef here\r\nTF_Buffer graph_def = {graph_def_data, strlen(graph_def_data), nullptr};\r\n\r\n// Import `graph_def` into `graph`\r\nTF_ImportGraphDefOptions* import_opts = TF_NewImportGraphDefOptions();\r\nTF_ImportGraphDefOptionsSetPrefix(import_opts, \"import\");\r\nTF_GraphImportGraphDef(graph, &graph_def, import_opts, s);\r\nassert(TF_GetCode(s) == TF_OK);\r\n\r\n// Setup graph inputs\r\nstd::vector<TF_Output> inputs;\r\nstd::vector<TF_Tensor*> input_values;\r\n// Add the placeholders you would like to feed, e.g.:\r\nTF_Operation* placeholder = TF_GraphOperationByName(graph, \"import/my_placeholder\");\r\ninputs.push_back({placeholder, 0});\r\nTF_Tensor* tensor = TF_NewTensor(/*...*/);\r\ninput_values.push_back(tensor);\r\n\r\n// Setup graph outputs\r\nstd::vector<TF_Output> outputs;\r\n// Add the node outputs you would like to fetch, e.g.:\r\nTF_Operation* output_op = TF_GraphOperationByName(graph, \"import/my_output\");\r\noutputs.push_back({output_op, 0});\r\nstd::vector<TF_Tensor*> output_values(outputs.size(), nullptr);\r\n\r\n// Run `graph`\r\nTF_SessionOptions* sess_opts = TF_NewSessionOptions();\r\nTF_Session* session = TF_NewSession(graph, sess_opts, s);\r\nassert(TF_GetCode(s) == TF_OK);\r\nTF_SessionRun(session, nullptr,\r\n              &inputs[0], &input_values[0], inputs.size(),\r\n              &outputs[0], &output_values[0], outputs.size(),\r\n              nullptr, 0, nullptr, s);\r\n\r\nvoid* output_data = TF_TensorData(output_values[0]);\r\nassert(TF_GetCode(s) == TF_OK);\r\n\r\n// If you have a more complicated workflow, I suggest making scoped wrapper\r\n// classes that call these in their destructors\r\nfor (int i = 0; i < inputs.size(); ++i) TF_DeleteTensor(input_values[i]);\r\nfor (int i = 0; i < outputs.size(); ++i) TF_DeleteTensor(output_values[i]);\r\nTF_CloseSession(session, s);\r\nTF_DeleteSession(session, s);\r\nTF_DeleteSessionOptions(sess_opts);\r\nTF_DeleteImportGraphDefOptions(import_opts);\r\nTF_DeleteGraph(graph);\r\nTF_DeleteStatus(s);\r\n```\r\nFull disclosure, I didn't actually run this code, so please let me know if you run into problems. For details please refer to the [C API header](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/c/c_api.h). The [C API test](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/c/c_api_test.cc) has more examples for how to use the code.\r\n\r\nIf using the C API doesn't work for you, using the C++ API is still an option. I'm currently working on making the C++ API more user-friendly (e.g. distributing binaries so you don't have to build from source), and I'll be sure to include this feature request, but it might not be available for a while.", "@HossamAmer12 I suggest using the instructions I provided above to use the C API. If you'd like to try modifying the C++ API, please refer to [Installing TensorFlow from Source](https://www.tensorflow.org/install/install_sources). Once you get that working, you can use `bazel build -c opt //tensorflow:libtensorflow.so`, which will build the libtensorflow.so in bazel-bin/tensorflow (relative to your TF checkout root). I'd like to keep this issue focused on direct access to Tensor data, so if you have any further build problems please report them on [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow) (we try to reserve GitHub issues for bugs, feature requests, etc.).", "@skye Thank you for your reply!\r\n\r\nI have already built TF from source and it is all set up, yet I do have the same issue where I use the TF's C++ API in a project that requires manually feeding data (char*) into tensors; to avoid this, I tried @pdrews's approach and I got the compile error that I've mentioned. \r\n\r\nBecause I still use the C++ API, it seems to me that I should still keep copying the data in order to run my NN -- Correct?", "Yes, if you have it working acceptably with copying, I'd say you should continue using that. Premature optimization is the root of all evil ;)\r\n\r\nThat said, I realize you probably need to build with `bazel build -c opt //tensorflow/tools/lib_package:libtensorflow` and look at bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz (which includes the header and .so you need). Linking that should expose the C API.", "@skye I understand :)  I'm working on a project where optimising execution time is a big deal and copying data is expensive, so I'd appreciate it if you can come up with the C++ feature as soon as you can. I'm actually thinking of excluding this copy time for my calculated total time.\r\n\r\nAs for your C API suggestion, would it work normally with C++ code? (I guess yes)", "@skye That looks great!  An example of the C interface is exactly what I have been missing.  I will look into transitioning to that over the next week.  I am already able to compile libtensorflow.so from source (we do have a GPU on our robot, so we need GPU acceleration).  I assume the graphdef char array can just be read in from a .pb file created from the freeze_graph.py script?", "@HossamAmer12 Yup, you can use the C API in a C++ application (I usually end up writing little C++ wrappers to make it easier to use, like scoped wrapper classes).\r\n\r\n@pdrews I'm not super familiar with freeze_graph.py but it looks like that should work (the input to TF_ImportGraphDef is a serialized GraphDef proto, which it looks like freeze_graph.py emits).\r\n\r\nAlso, here's the link to the GPU-enabled version: https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.0.0.tar.gz\r\n\r\nFor future reference, these links and some more instructions are inconveniently located here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md. I'll look into creating some more documentation for the C API.", "Might be a little be too hacky, but one way not to changing TF C/C++ API is to allocate an empty `Tensor` before you do any other computation, and use the buffer (a direct pointer could be obtained from `StringPiece`) of that `Tensor` as the underlaying storage for data structure in, say, OpenCV. \r\n\r\nLifetime issue could be tricky though.", "@skye with your help I have an 'almost' working program using the C API libtensorflow.so. \r\n\r\nI run into the following problem. I get this error message:\r\nW tensorflow/core/common_runtime/device_mgr.cc:94] Unknown device: /job:localhost/replica:0/task:0/cpu:0 all devices: \r\n/job:localhost/replica:0/task:0/cpu:0 unknown device.\r\n\r\nI've compiled the library like so:\r\nbazel build -c opt //tensorflow/tools/lib_package:libtensorflow -c dbg --strip=never --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --copt=-Wno-error=stringop-overflow\r\n\r\nI've also tried without any of the optimization or debug symbols.  I've followed your example above for loading a graph definition and running the session. I can iterate through the operations and print their names, so I know that part is working. I get the above error with a != TF_OK status, when trying to run the session.\r\n\r\nAny ideas what I'm doing wrong?\r\n\r\nthanks\r\n\r\nPS: I should add that I'm not able to use the pre-built  library linked above, because of a version conflict with my system's protobuf. I had to statically link OpenCV for the same reason.\r\nI should also add that I'm on Ubuntu 17.10 beta, with gcc 7.2, on VMWare 12 Player, under windows 10.", "@pdrews In your case you can allocate a Tensor with the C++ API then use flat to get a pointer to the memory for that particular tensor.\r\nWith that pointer you can use cv::Mat(rows, cols, type, pointer) constructor to fake a cv::Mat with the memory from the Tensor.\r\n\r\nThen just use your cv::Mat.\r\n\r\nUntested code:\r\n```\r\n// allocate a Tensor\r\nTensor inputImg(DT_FLOAT, TensorShape({1,inputHeight,inputWidth,3}));\r\n\r\n// get pointer to memory for that Tensor\r\nfloat *p = inputImg.flat<float>().data();\r\n// create a \"fake\" cv::Mat from it \r\ncv::Mat cameraImg(inputHeight, inputWidth, CV_32FC3, p);\r\n\r\n// use it here as a destination\r\ncv::Mat imagePixels = ...; // get data from your video pipeline\r\nimagePixels.convertTo(cameraImg, CV_32FC3);\r\n```\r\nHope this helps", "@guillaume-michel how would you use the (converted) `cameraImg` in the C++ API's run function which accepts a Tensor?  In other words, once `cameraImg` has the image data, how do you get it back to a Tensor? \r\n\r\nEDIT: Ah, after looking more closely at the `cv::Mat` constructor, the cv::Mat and the tensor share the same data in memory.", "@guillaume-michel Any way to do this for a tensor which has a batch of images? \r\nEg.\r\n`Tensor inputImg(DT_FLOAT, TensorShape({4,inputHeight,inputWidth,3}));`\r\n\r\nSince tensors aren't subscriptable, I'd imagine you would create tensors for each image, load the `cv::Mat` into it, and combine them somehow to forrn `inputImg`. If so, is there a way to combine tensors?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "I'm gonna close this as we're no longer actively working on the C++ API. Feel free to continue this discussion though.", "@skye Does \"no longer actively working on the C++ API\" also mean that the C++ API would eventually be deprecated/abandoned? \r\n\r\nThe TF1.4 doc gives no indication to that.\r\n\r\n https://www.tensorflow.org/api_guides/cc/guide", "No, I should have been more specific. I meant that I'm no longer working on improving C++ API usability, in particular the Tensor interface, meaning there's nothing to do for this issue anymore (this functionality is currently possible in both the C API and using the Tensor C++ interface directly).", "> @guillaume-michel Any way to do this for a tensor which has a batch of images?\r\n> Eg.\r\n> `Tensor inputImg(DT_FLOAT, TensorShape({4,inputHeight,inputWidth,3}));`\r\n> \r\n> Since tensors aren't subscriptable, I'd imagine you would create tensors for each image, load the `cv::Mat` into it, and combine them somehow to forrn `inputImg`. If so, is there a way to combine tensors?\r\n\r\n@jimaldon Did you find any solution for this?", "@ibadami  Yes! memcpy to the rescue\r\n\r\n```\r\n  float*   imgTensorFlat = inputImg.flat<float>().data();\r\n  int      arrSize       = imageWidth * imageHeight * imageChannels;\r\n\r\n  cv::Mat temp;\r\n  for (int z = 0; z < imagesInBatch; ++z)\r\n  {   \r\n    cv::Mat inputImg = cv::imread(aInputBatch[z], CV_32F);\r\n    inputImg.convertTo(temp, CV_32F);\r\n    float* p = temp.ptr<float>();\r\n    std::copy(p, p + arrSize, imgTensorFlat + z * arrSize);\r\n  }   \r\n```", "I'm following @guillaume-michel method to convert my image to tensor as input for my Object detection model (SSD).\r\n\r\nHowever, if I keep the original size of the image (1280x720), it takes long time to finish the conversion (~0.3s per image).\r\n\r\nSo I would resize the image to 300x300 before converting, it can help significantly decrease the conversion time.\r\nBut does it affect to the object detection result? (When I train the SSD model, I used the config:\r\nfixed_shape_resizer {\r\n        height: 300\r\n        width: 300\r\n      }\r\n)"]}, {"number": 8032, "title": "Add support for the matmul (@) operator.", "body": "This PR closes #1062.\r\nAssuming `a` and `b` are tensors, `a @ b`  is equivalent to `tf.matmul(a, b)`.\r\n\r\nNote that either `a` or `b` can be a NumPy array or a regular python array, for example the following expressions are valid:\r\n* `a @ [[1.], [2.]]`\r\n* `[[1., 2.]] @ a`\r\n* `a @ np.array([[1.], [2.]])`\r\n* `np.array([[1., 2.]]) @ a`\r\n", "comments": ["Can one of the admins verify this patch?", "cc @ibab ", "@tensorflow-jenkins test this please", "@caisq That's odd, 2 tests fail with a weird error: \"Unable to get pull request builder trigger\".  I'm guessing this is a temporary error? Perhaps just retry? How can I help?", "@ageron. I don't see the errors now. Maybe those were just transient. The //tensorflow/python:localhost_cluster_performance_test failure in Linux CPU Test doens't look related. So I guess the tests are all good for now.", "Can one of the admins verify this patch?", "@girving Thanks for your review. Sorry for the merge commit during review, I'll remove it.  I'll also add the separate test file for the @ operator.", "Can one of the admins verify this patch?", "@girving As you requested, I moved the test code from cwise_ops_test.py to  matmul_op_test.py, I removed the mention of TensorFlow 1.1 from the documentation, and I removed the merge commit.", "@girving I added the test you requested.", "@shoyer Great point, thanks. I replaced `eval(\"x @ y\")` by `operator.matmul`.", "Jenkins, test this please.", "@gunan What does this sort of build abort mean?\r\n\r\n    Build was aborted\r\n    Aborted by unknown\r\n    Unable to get pull request builder trigger!!\r\n    Setting status of 0c19220e483aabfd25e5453fed9f7ef896c9b4aa to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3951/ and message: 'FAILURE\r\n     '\r\n    Using context: Linux CPU Tests\r\n    Finished: ABORTED", "Bad indentations in matmul_op_test.py caused pylint check failures. See:\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/3421/console", "Ah, right.  Apologies for forgetting to look at the sanity checks again.\r\n\r\n@ageron Can you fix these indentation errors?  @caisq Can @ageron check such things locally?", "@girving, @ageron can use the following command to run the sanity build locally (requires docker):\r\n```\r\ntensorflow/tools/ci_build/ci_build.sh CPU tensorflow/tools/ci_build/ci_sanity.sh\r\n```", "@girving @caisq Sorry about that, I'm on it.", "@girving @caisq Your tool worked, thanks. I fixed the indentation, everything should be green now.", "@tensorflow-jenkins test this please", "@caisq Oh noooo, what's wrong now? Apparently a performance test failed, correct? Perhaps the performance tests need to be adjusted to be la bit less strict. Wdyt?", "The errors look like\r\n\r\n        class MatMulInfixOperatorTest(test.TestCase):\r\n    NameError: name 'test' is not defined", "I think it should be `test_lib.TestCase`.", "Did you try running the tests locally?", "@ageron , there are some remaining problems. \r\n\r\nSee https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/3955/consoleFull\r\n\r\n```\r\n    class MatMulInfixOperatorTest(test.TestCase):\r\nNameError: name 'test' is not defined\r\n```\r\n\r\nPlease use test_util.TensorFlowTestCase instead. There are plenty of examples in other py test files.\r\n\r\nFor the Linux CPU test, you can run the tests locally by using\r\n\r\n```\r\nexport TF_BUILD_CONTAINER_TYPE=CPU\r\nexport TF_BUILD_PYTHON_VERSION=PYTHON2\r\nexport TF_BUILD_IS_OPT=OPT\r\nexport TF_BUILD_IS_PIP=PIP\r\nexport TF_BUILD_APPEND_CI_DOCKER_EXTRA_PARAMS=\"-e TF_BUILD_FILTER_INSTALL_TESTS_BY_TAG=-manual\"\r\ntensorflow/tools/ci_build/ci_parameterized_build.sh\r\n```\r\n", "@caisq @girving Yes, I fully tested on my machine using the command you provided, and everything worked fine.  The problem is the test file was updated in the meantime by other people (among other things, test was renamed to test_lib), and I need to merge their changes.  But earlier, Geoffrey told me *not* to merge during a code review, so I'm a bit stuck.  What should I do? Merge master despite the ongoing code review, test locally and commit? Or...? I see no other option.", "@caisq @girving Ok, I replaced \"test.TestCase\" with \"test_lib.TestCase\". Please try again... I hope this will be it.  I ran the tests on my machine, everything worked fine:\r\n\r\n```\r\n[...]\r\n//bazel_pip/tensorflow/tensorboard/lib/python:http_util_test             PASSED in 3.3s\r\n//bazel_pip/tensorflow/tensorboard/lib/python:json_util_test             PASSED in 3.3s\r\n//bazel_pip/tensorflow/tensorboard/plugins/debugger:debugger_plugin_test PASSED in 3.7s\r\n//bazel_pip/tensorflow/tensorboard/plugins/projector:projector_plugin_test PASSED in 5.1s\r\n\r\nExecuted 570 out of 570 tests: 570 tests pass.\r\n\r\n=== Testing user ops ===\r\nPYTHON_BIN_PATH: /workspace/pip_test/venv/bin/python\r\n/tmp/tmp.UxvZtfODLd /workspace\r\nTensorFlow include path: /workspace/pip_test/venv/local/lib/python2.7/site-packages/tensorflow/include\r\n\r\ng++ version:\r\nUsing built-in specs.\r\nCOLLECT_GCC=g++\r\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/5/lto-wrapper\r\nTarget: x86_64-linux-gnu\r\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 5.4.0-6ubuntu1~16.04.4' --with-bugurl=file:///usr/share/doc/gcc-5/README.Bugs --enable-languages=c,ada,c++,java,go,d,fortran,objc,obj-c++ --prefix=/usr --program-suffix=-5 --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-libmpx --enable-plugin --with-system-zlib --disable-browser-plugin --enable-java-awt=gtk --enable-gtk-cairo --with-java-home=/usr/lib/jvm/java-1.5.0-gcj-5-amd64/jre --enable-java-home --with-jvm-root-dir=/usr/lib/jvm/java-1.5.0-gcj-5-amd64 --with-jvm-jar-dir=/usr/lib/jvm-exports/java-1.5.0-gcj-5-amd64 --with-arch-directory=amd64 --with-ecj-jar=/usr/share/java/eclipse-ecj.jar --enable-objc-gc --enable-multiarch --disable-werror --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu\r\nThread model: posix\r\ngcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) \r\n\r\nExtra GPP flag: \r\nTesting user ops in CPU environment\r\nCompiling user op C++ source file zero_out_op_kernel_1.cc\r\nInvoking user op zero_out defined in file zero_out.so via pip installation\r\n2017-03-07 10:07:46.865252: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-07 10:07:46.865286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-07 10:07:46.865298: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-07 10:07:46.865309: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-03-07 10:07:46.865321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nOutput from user op ([42,0,0]) matches expected output\r\n/workspace\r\n\r\nSUCCESS: Testing of user ops PASSED\r\n\r\nParameterized build ends with SUCCESS at: mardi 7 mars 2017, 11:07:47 (UTC+0100) (Elapsed time: 2794 s)\r\n\r\n```", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "@ageron All tests passed. Yay! \r\n@girving Should be okay to merge unless @shoyer has more comments?", "Joy! Happiness! The tests pass at last, thanks @caisq.", "Merged.  Thank you for the contribution!", "Awesome, thanks Geoffrey and Shanqing."]}, {"number": 8031, "title": "Can't Build XLA compiler for Tensorflow", "body": "I am trying to compiler tensorflow using bazel following the steps in https://www.tensorflow.org/install/install_sources#ConfigureInstallation.\r\n\r\nMy system is Ubuntu 16.04\r\ngcc -v 5.4\r\nI want to use XLA compiler so set it in the ./configuration file.\r\nI am using CPU only for now.\r\nI tried with pip and pip3\r\n\r\nI get the following Error:\r\n\r\n```\r\nERROR: /home/achang/tensorflow/tensorflow/compiler/tf2xla/BUILD:23:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:xla_compiler' failed: gcc failed: error executing command \r\n  (cd /home/achang/.cache/bazel/_bazel_root/6c98a6c54aeed71fa731445a9f51836b/execroot/tensorflow && \\\r\n  exec env - \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' '-march=native' -MD -MF bazel-out/local-opt/bin/tensorflow/compiler/tf2xla/_objs/xla_compiler/tensorflow/compiler/tf2xla/xla_compiler.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/compiler/tf2xla/_objs/xla_compiler/tensorflow/compiler/tf2xla/xla_compiler.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D_DEBUG -DLLVM_BUILD_GLOBAL_ISEL -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local-opt/genfiles/external/local_config_cuda -iquote external/llvm -iquote bazel-out/local-opt/genfiles/external/llvm -isystem external/jemalloc/include -isystem bazel-out/local-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local-opt/genfiles/external/local_config_cuda/cuda/include -isystem external/llvm/include -isystem bazel-out/local-opt/genfiles/external/llvm/include -isystem external/llvm/lib/IR -isystem bazel-out/local-opt/genfiles/external/llvm/lib/IR -isystem external/llvm/include/llvm/IR -isystem bazel-out/local-opt/genfiles/external/llvm/include/llvm/IR -isystem external/llvm/lib/Target/PowerPC -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/PowerPC -isystem external/llvm/lib/Target/X86 -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/X86 -isystem external/llvm/lib/Target/AArch64 -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/AArch64 -isystem external/llvm/lib/Target/ARM -isystem bazel-out/local-opt/genfiles/external/llvm/lib/Target/ARM -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/compiler/tf2xla/xla_compiler.cc -o bazel-out/local-opt/bin/tensorflow/compiler/tf2xla/_objs/xla_compiler/tensorflow/compiler/tf2xla/xla_compiler.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::CheckSignature(const DataTypeVector&, const std::vector<tensorflow::XlaCompiler::Argument>&)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:48:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < types.size(); ++i) {\r\n                     ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::BuildExecutable(const tensorflow::XlaCompiler::CompilationResult&, std::unique_ptr<xla::LocalExecutable>*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:139:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n   for (int i = 0; i < result.xla_input_shapes.size(); ++i) {\r\n                     ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In function 'tensorflow::Status tensorflow::{anonymous}::BuildArguments(const std::vector<tensorflow::XlaCompiler::Argument>&, bool, xla::ComputationBuilder*, std::vector<tensorflow::XlaContext::Argument>*, std::vector<int>*, std::vector<xla::Shape, std::allocator<xla::Shape> >*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:228:8: error: 'size_typ' is not a member of 'std::vector<tensorflow::XlaCompiler::Argument>'\r\n   for (std::vector<XlaCompiler::Argument>::size_typ i = 0;\r\n        ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:229:8: error: 'i' was not declared in this scope\r\n        i < args.size(); ++i) {\r\n        ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc: In member function 'tensorflow::Status tensorflow::XlaCompiler::CompileGraph(const string&, std::unique_ptr<tensorflow::Graph>, tensorflow::FunctionLibraryRuntime*, const std::vector<tensorflow::XlaCompiler::Argument>&, tensorflow::XlaCompiler::CompilationResult*)':\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                    ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:20: error: 'VariableWrite' is not a member of 'tensorflow::XlaCompiler'\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 1 is invalid\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                                              ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:46: error: template argument 2 is invalid\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: expected ';' before 'i'\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                                                            ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:60: error: 'i' was not declared in this scope\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:481:43: error: expected ')' before ';' token\r\n        i < result->variable_updates.size(); ++i) {\r\n                                           ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:480:49: warning: unused variable 'size_type' [-Wunused-variable]\r\n   for (std::vector<XlaCompiler::VariableWrite>::size_type  i = 0;\r\n                                                 ^\r\ntensorflow/compiler/tf2xla/xla_compiler.cc:481:47: error: 'i' was not declared in this scope\r\n        i < result->variable_updates.size(); ++i) {\r\n                                               ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1162.629s, Critical Path: 913.22s\r\n```\r\n", "comments": ["FYI this breakage was introduced by PR #7752 \r\n\r\nLooks like the referenced #8039 will fix this.  Closing this out.  Thanks for filing the issue @Andrechang !"]}, {"number": 8030, "title": "TensorBoard not finding files to display, suggested solution?", "body": "It seems that the default location for the log files, for most tutorials is in the system root tmp directory.. eg /tmp. This directory cannot normally be found by starting the tensorboard program according to instructions, nor do the debug instructions on the readme page for Tensorboard help at all.. Since they: Grep/ starting in debug Mode etc.. also depend on seeing directories within the user space.\r\n\r\nI'd suggest simply modifying the tutorials to add the ./tmp so that the files end up in user space and also that the tensorBoard start up default directory be configured to match, only requiring configuration when needed for larger and more complex installs.\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@jart might have some suggestions here", "I don't really understand what you're asking. TensorBoard works just fine with logs stored in `/tmp/`; that's where I put essentially everything. It also works fine with `~/tmp`, but I don't see any reason to change the existing tutorials to use this instead. Indeed, I'd argue that `/tmp/` is better because it's cleared when you restart the machine.\r\n\r\nIf you're still having issues, try running `tensorboard --logdir /tmp/mnist_tutorial` or whatever your directory is. If that doesn't work, please feel free to report it to us by creating an issue in our new repository at https://github.com/tensorflow/tensorboard/issues. Thanks!"]}, {"number": 8029, "title": "Multiple broken g3doc links", "body": "There are, by my count, at least 20 broken g3doc links scattered among documentation, code, and build scripts caused by [this commit](https://github.com/tensorflow/tensorflow/commit/1c707ac780313f48a6733dc3beedf4b8a2b3df77) migrating g3docs to the docs_src + source code documentation generating system.  So far, in relation to this, the following issues have been opened: #8014, #7999, #7989, and the following pull requests have been made: #8018, #7990, #7965.  Two of the pull requests, #8018 and #7990, take care of the two broken links on the core TensorFlow README and are waiting on the signing of CLAs.\r\n\r\nDue to the restructuring that took place in the transition from g3doc to docs_src, a simple find-replace won't work.  For example, in reference to [this line in fully-connected-reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py#L20), `tensorflow/g3doc/how_tos/reading_data.md#reading-from-files` would have to be changed to `tensorflow/docs_src/programmers_guide/reading_data.md#reading-from-files`.  In fact, according to the new reference syntax, it should be changed to `@{$reading_data#reading-from-files}`.\r\n\r\nMoreover, some g3doc files that are still linked to by existing documentation no longer exist in this repository.  E.g., the [adding an op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/extend/adding_an_op.md) extension guide links to many of the Python files contained in [this g3doc folder](https://github.com/tensorflow/tensorflow/tree/8120e2a270c28e0a62b9f522164b196a90f113b7/tensorflow/g3doc/how_tos/adding_an_op) (which has now been deleted.)  Although these Python files should probably be reinstated somewhere, this appears to be the only collection of Python files in g3doc that were deleted and are still needed.\r\n\r\nSo, how best to go about fixing these issues?  Manually fix each broken link in a single pull request?  And where should the [Python files](https://github.com/tensorflow/tensorflow/tree/8120e2a270c28e0a62b9f522164b196a90f113b7/tensorflow/g3doc/how_tos/adding_an_op) pertaining to the [adding an op](https://www.tensorflow.org/extend/adding_an_op) extension guide be placed?", "comments": ["Thanks for filing the detailed issue @ClarkZinzow !\r\n\r\n@xmbrst @dr4b Can you guys take a look?", "Hi!  We are still updating the documentation about how to contribute documentation, but for now:\r\n\r\n1. To fix docs in docs_src, a set of PRs of reasonable sizes that contains a few related files would be good---one PR per error is probably overkill, but one huge PR might be hard to review.\r\n\r\n2. The support code files for Adding an Op should be re-added, but we will add it to the tensorflow/examples directory and update the links.  We will do this.\r\n\r\nThanks!", "Thanks @tatatodd for the labeling and assigning, and thanks @wolffg for the info.  For docs_src fixes, I'll limit the domain of each pull request to sibling files (should provide the right level of granularity for the pull requests; let me know if you think this is too coarse-grained), and I'll let the Adding an Op code files be.  It appears that the latter is starting to get attention: #8044.\r\n\r\nI'm hoping to provide consistent useful contributions to TensorFlow, both via bug fixes and feature additions, once I feel very comfortable with the code base.  In the meantime, I thought that fixing documentation errors and other minor issues would be a good way to help out!\r\n\r\nEdit:  It looks like the Adding an Op code has been taken care of via #8066, thanks @jhseu!", "BTW, is this bug still relevant?  I think a lot of these issues got fixed over the months as we solidified the parameterized link process?"]}]