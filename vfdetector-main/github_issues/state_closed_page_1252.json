[{"number": 15588, "title": "Tensorflow crashes on AVX512 systems (Core i9 / Xeon etc)", "body": "(for now, placeholder for notes while investigating)\r\nusing the r1.5 branch (but 1.4 and earlier have the same issues)\r\n\r\nTensorflow crashes when compiled for AVX512. The cause is a misalignment exception; AVX512 has a 64 byte natural alignment, and Eigen and others often use instructions that explicitly fault on misaligned loads.\r\n\r\ntensorflow/core/framework/allocator.h has this code\r\nclass Allocator {\r\n public:\r\n#ifdef EIGEN_VECTORIZE_AVX512\r\n  // Align to 64 byte boundary.\r\n  static constexpr size_t kAllocatorAlignment = 64;\r\n#else\r\n  // Align to 32 byte boundary.\r\n  static constexpr size_t kAllocatorAlignment = 32;\r\n#endif\r\n\r\n\r\nwhich implies that at least this allocator tries to do the right thing, however it seems EIGEN_VECTORIZE_AVX512 is not properly defined (in cpp meaning) at this place, because when adding an #error in the 32 byte alignment case, the #error hits, so the 64 bit alignment case is not used.\r\n\r\nThis is not the only case; brute forcing the 32 to be 64 still faults in misalignment... just later on in a different place.\r\n\r\n", "comments": ["so far root cause is pointing to the compiler flags not being consistently passed; specifically the -march is getting stripped from the build of allocator.cc which then leads the eigen includes to not set EIGEN_VECTORIZE_AVX512 which then causes.. well disaster.\r\nThis is not limited to allocator.cc but it seems to be a broader generic problem of cflags", "We probably need to add copts=tf_copts() to //tensorflow/core:framework_internal for framework/allocator.h, since that's not transitive (may have been another casualty of the RTLD_GLOBAL removal). ", "Although if we need it on framework_internal then we also need it on framework, and that hasn't ever had a copts as far as I can tell.", "I have no evidence to suggest that avx512 in this way has ever worked.", "as a broader observation; really anything that ends up including ./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h and its friends needs to be consistent about its compiler flags; you get strange results if that mixes-and-matches", "https://github.com/tensorflow/tensorflow/pull/15606", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rmlarsen: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "the work is being done in\nhttps://github.com/tensorflow/tensorflow/pull/19121\n\nOn Wed, May 9, 2018 at 6:06 PM, Alfred Sorten Wolf <notifications@github.com\n> wrote:\n\n> Nagging Assignee @rmlarsen <https://github.com/rmlarsen>: It has been 15\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15588#issuecomment-387919225>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABPeFcDgB0qW8xXu_1Rb0AOEaAc1sI-6ks5tw5J4gaJpZM4RLHPW>\n> .\n>\n"]}, {"number": 15587, "title": "lite model file size", "body": "how to reduce my model size(my own trained tensorflow model )\r\ni want to use tensorflow lite converter get a smaller tflite file, but the tflite file has the same size with my trained model,\r\n-------------------------------------------------------------------------\r\n\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --allow_custom_ops \\\r\n  --input_file=/data/log/frozen_model.pb --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \\\r\n  --output_file=/data/log//mobilenet.tflite --inference_type=FLOAT \\\r\n  --input_data_types=FLOAT --input_arrays=input \\\r\n  --output_arrays=output --input_shapes=1,224,224,3\r\n\r\n--------------------------------------------------------------------------", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 15586, "title": "WIP: Remove invalid merge_repeated option from CTC beam decoder", "body": "The CTC beam decoding implicitly collapses repeated characters as part of calculating the optimal path (i.e. 'AAA' will contribute probability mass through the path 'A').\r\n\r\nSo the correct CTC decoding behavior occurs when merge_repeated=False. In this case, it DOES merge repeated characters. The merge_repeated flag, when true, will merge repeated characters after characters have already been merged/blank symbols removed. As it stands now, the behavior is extremely misleading.\r\n\r\nThis PR removes the misleading parameter from the ctc beam search op.\r\n\r\nCloses #9550.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Bueller?", "This breaks backwards compatibility (and all users who currently have an old graph built using the ctc loss, regardless of the argument's value), so we definitely cannot accept this PR as is!\r\n\r\nCan you instead update the documentation, making it very clear what the arguments mean - and adding a warning about use cases?  Julian's table looks like a very good item to add to the docs.", "Can you enumerate a use case where `merge_repeated=True` is actually valid? I recognize it's a breaking change, but the current behavior, IMO, is a bug.", "Happy to update docs in the interim.", "Regardless of the current behavior; this PR, as is, introduces a worse bug:\nnamely that existing graphs will crash in the new version of the C++\nruntime.  That's against TF's backwards compatibility guarantees.\n\nYou are welcome to add a deprecation warning if you strongly believe this\nargument to be a bug -- we will review it.  There's even a decorator that\nallows deprecating function arguments in TF.\n\nOn Thu, Jan 4, 2018 at 9:57 AM, Ryan Leary <notifications@github.com> wrote:\n\n> Can you enumerate a use case where merge_repeated=True is actually valid?\n> I recognize it's a breaking change, but the current behavior, IMO, is a bug.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15586#issuecomment-355352511>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim-icBzghi8_QylkasDC_Rfnd-h2sks5tHREEgaJpZM4RLB9L>\n> .\n>\n", "Got it. Definitely agree that this PR is a WIP, but wanted to get some feedback. I do believe that the argument shouldn't exist (and would like to encourage feedback/discussion on that contention).\r\n\r\nIf there's a more appropriate way to deprecate the kwarg (and apparently there is), I'll look into doing using that mechanism for achieving the same end goal.", "@ryanleary do you intend to continue working on this PR or can we close it?", "@ryanleary any update on this PR?", "@rmlarsen Could you send me a pointer to an appropriate way to deprecate a kwarg? I think the argument really should be removed in future versions.", "Re @ryanleary  Check this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/gru_ops.py#L135", "yes; feel free to add the deprecation decorator!", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 58 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "I'm about to push a v2 with this argument removed."]}, {"number": 15585, "title": "FP16 slower than FP32", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nPartly\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRHEL 7\r\n- **TensorFlow installed from (source or binary)**:\r\nunknown\r\n- **TensorFlow version (use command below)**:\r\n('unknown', '1.4.0')\r\n- **Python version**: \r\npython2.7\r\n- **Bazel version (if compiling from source)**:\r\nunknown\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 4.8\r\n- **CUDA/cuDNN version**:\r\n9.0 / 7.0\r\n- **GPU model and memory**:\r\nPascal P-100\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI tried running the tutorial/rnn/ptb and tutorial/images/mnist examples with the --use_fp16 option, to see if speedup was achievable by utilizing the new half-precision features.\r\nIt turned out that a single training step for MNIST with FP32 took 3.3ms, with FP16 it was 4ms. For PTB small (I had to use lstm_cell=basic, because other types are not yet supported in FP16), the WPS dropped from 24000 to 22000 when switching to FP16.\r\n\r\nSo the performance **decreases** when using FP16 in real-world examples.\r\n\r\nTo check this, I've created a small benchmark for myself (since I can't get the nightly tf build on my machine, I can't run the official benchmarks), which is basically one big matrix multiplication in Tensorflow.\r\n\r\nI've run it with square matrices with a width of 8k, 16k and 32k. In each case, FP16 and FP32 yielded nearly the same runtime. Profiling with nvprof I found out that the used CUDA function is **maxwell_fp16_segmemm_fp16_128x128_nn** for FP16 and **sgemm_128x128x8_NN_vec** for FP32.\r\n\r\nSince I wanted to double check if matrix multiplication in FP16 is really slower than in FP32 on my GPU, I tried to directly benchmark the GPU using cuBlas with a similar operation. It turns out that here, FP16 is nearly twice as fast as FP32. CuBlas internally uses **maxwell_hgemm_256x128_nn** for matrix multiplication of 16k x 16k square matrices in FP16. (again  according to the nvprof profiler)\r\n\r\nSo I'm wondering why Tensorflow is unable to achieve similar results in terms of speed, or if I'm doing something wrong in my tests.\r\n\r\n### Source code\r\n\r\nTensorflow Code snippet:\r\n```\r\n    graph = tf.Graph()\r\n        with graph.as_default():\r\n          tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n          tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n          tf_output = tf.matmul(tf_input1, tf_input2)\r\n  \r\n      with tf.Session(graph=graph) as session:\r\n          tf.global_variables_initializer().run()\r\n          print(\"Initialized\")\r\n          for i in range(FLAGS.times):\r\n              out = session.run([tf_output])#, feed_dict=feed_dict)\r\n          print(\"Done\")\r\n```\r\n\r\n\r\ncuBlas Code FP16 (Snippet):\r\n```\r\n        uint16_t *d_a;          // d_a - a on the device\r\n        uint16_t *d_b;          // d_b - b on the device\r\n        uint16_t *d_c;          // d_c - c on the device\r\n        cudaStat = cudaMalloc ((void **) &d_a, m * k * sizeof (*a));    // device memory alloc for a\r\n        cudaStat = cudaMalloc ((void **) &d_b, k * n * sizeof (*b));    // device memory alloc for b\r\n        cudaStat = cudaMalloc ((void **) &d_c, m * n * sizeof (*c));    // device memory alloc for c\r\n        stat = cublasCreate (&handle);  // initialize CUBLAS context\r\n        // copy matrices from the host to the device\r\n        stat = cublasSetMatrix (m, k, sizeof (*a), a, m, d_a, m);   //a -> d_a\r\n        stat = cublasSetMatrix (k, n, sizeof (*b), b, k, d_b, k);   //b -> d_b\r\n        stat = cublasSetMatrix (m, n, sizeof (*c), c, m, d_c, m);   //c -> d_c\r\n        uint16_t al = FP_16_ONE;       // al = 1 \r\n        uint16_t bet = FP_16_ONE;      // bet =1\r\n        // matrix - matrix multiplication : d_c = al*d_a *d_b + bet *d_c\r\n        // d_a -mxk matrix , d_b -kxn matrix , d_c -mxn matrix ;\r\n        // al ,bet -scalars\r\n\r\n        stat = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &al, d_a, CUDA_R_16F, m, d_b, CUDA_R_16F, k, &bet, d_c, CUDA_R_16F, m, CUDA_R_16F, CUBLAS_GEMM_DEFAULT); \r\n\r\n        stat = cublasGetMatrix (m, n, sizeof (*c), d_c, m, c, m);   // cp d_c - >c\r\n```\r\n  ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "**Exact command to reproduce**\r\nN/A", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes it's still an issue", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "In general, fp16 on Pascal GPUs (like your P100) will not be much faster, if faster at all. In your cuBlas example, you pass CUDA_R_16F as the second-to-last parameter, `computeType`, to `cublasGemmEx()`. In TensorFlow, we use fp32 as a compute type, since models do not work well in practice if a lower precision is used as the compute type. This can be changed by setting the evironmental variables [TF_FP16_CONV_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/bffa3e10bf4886f03a68f7e93ba39c91d447f101/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2236) and [TF_FP16_MATMUL_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/de0904e90669d345e0feaa89d1e99b7618fb6677/tensorflow/core/util/matmul_autotune.cc#L44) to 0. But if you do that, the models will probably not train as well.\r\n\r\nNote everything I said applies to Pascal. On Volta, fp16 is significantly faster.\r\n\r\nAlso, I'm not sure where this tutorial/images/mnist example is. Can you clarify what the example is so I can reproduce? Additionally, I cannot run the TensorFlow example you provided, since FLAGs and get_dtype are undefined. Can you post a complete example?", "@reedwm Thank you for your response.\r\n\r\nI did try to set the environmental variables, but it didn't change anything. The called CUDA functions remained the same, and it had no effect on timing.\r\n\r\nThe tutorials/image/mnist example can be found here: https://github.com/tensorflow/models/tree/master/tutorials/image/mnist\r\nI just used it for trying a prebuilt model with fp16 vs fp32. (again: running with or without the environment variables set didn't change anything)\r\n\r\nMy code:\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\n\r\nFLAGS = None\r\n\r\ndef get_dtype():\r\n  return tf.float16 if FLAGS.use_fp16 else tf.float32\r\n\r\ndef benchmark():\r\n  graph = tf.Graph()\r\n  with graph.as_default():\r\n    tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n    tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n    tf_output = tf.matmul(tf_input1, tf_input2)\r\n\r\n  with tf.Session(graph=graph) as session:\r\n    tf.global_variables_initializer().run()\r\n    print(\"Initialized\")\r\n    for i in range(FLAGS.times):\r\n      out = session.run([tf_output])\r\n    print(\"Done\")\r\n\r\ndef parse():\r\n  global FLAGS\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument('--use_fp16', default=False, help='use FP16 for benchmarking', action='store_true')\r\n  parser.add_argument('--size', default=4096, type=int, help='size of matrices to multiply')\r\n  parser.add_argument('--times', default=10, type=int, help='amount of multiplications')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n\r\nif __name__ == \"__main__\":\r\n  parse()\r\n  benchmark()\r\n```\r\n\r\nWhat I did:\r\n```\r\nexport TF_FP16_CONV_USE_FP32_COMPUTE=0\r\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=0\r\nnvprof ./Matrixmul.py --use_fp16 --size 16000\r\n```\r\nOutput:\r\n```\r\n[...]\r\n==56407== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\r\n==56407== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   96.94%  8.16365s        10  816.37ms  814.54ms  819.59ms  maxwell_fp16_sgemm_fp16_128x128_nn\r\n```\r\n\r\n\r\nWithout the environment variables:\r\n```\r\nexport TF_FP16_CONV_USE_FP32_COMPUTE=1\r\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=1\r\nnvprof ./Matrixmul.py --use_fp16 --size 16000\r\n```\r\nOutput:\r\n```\r\n==70743== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\r\n==70743== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   96.49%  8.17353s        10  817.35ms  814.50ms  824.69ms  maxwell_fp16_sgemm_fp16_128x128_nn\r\n```\r\n\r\n\r\nWith FP32:\r\n```\r\nnvprof ./Matrixmul.py --size 16000\r\n```\r\nOutput:\r\n```\r\n[...]\r\n==89305== Profiling application: python ./Matrixmul.py --size 16000\r\n==89305== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   93.54%  8.14831s        10  814.83ms  810.68ms  818.11ms  sgemm_128x128x8_NN_vec\r\n```\r\n\r\nNote that **maxwell_fp16_sgmemm_fp16_128x128_nn** uses only FP16 storage but computes in FP32. So why is this function called when explicitly setting the TF_FP16_MATMUL_USE_FP32_COMPUTE to 0?", "Looking at [matmul_op.cc](https://github.com/tensorflow/tensorflow/blob/de0904e90669d345e0feaa89d1e99b7618fb6677/tensorflow/core/kernels/matmul_op.cc#L388), it looks like we sometimes do not pass the compute type from `TF_FP16_MATMUL_USE_FP32_COMPUTE` to cuBLAS. Adding a log statement, this seems to be occurring. /CC @yzhwang can you comment?\r\n\r\nLooking at tutorials/image/mnist, I'm not sure why fp16 is slower. When I generate Timelines for that example, the overhead of generating a timeline causes both to take the same amount of time, so it's hard to debug this. Since the example is so simple and fp16 has little performance benefit from P100s anywhere, it's not worth spending more time to debug this.", "Note that to call cublasGemmEx, one needs to enable autotune for matmul, which can be done by setting this environment var:\r\n`export TF_MATMUL_AUTOTUNE_ENABLE = 1`\r\nBy default the autotune for matmul is disabled, \r\n\r\nSo first you can try turning on the autotune for matmul.\r\n\r\n@reedwm For your question, I think the computation_type is the param that is sent in. I do not see any possibility that it failed to get sent to cublas. I'd set it with:\r\n`os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE']='1'`\r\nin python just to make sure it gets set.", "@yzhwang \r\nsetting TF_MATMUL_AUTOTUNE_ENABLE also does not change the behavior.\r\nCode:\r\n```\r\n$ export TF_MATMUL_AUTOTUNE_ENABLE=1\r\n$ export TF_FP16_MATMUL_USE_FP32_COMPUTE=0\r\n$ python -c \"import os; print os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE']\"\r\n0\r\n$ python -c \"import os; print os.environ['TF_MATMUL_AUTOTUNE_ENABLE']\"\r\n1\r\n$ nvprof ./Matrixmul.py --size 16000 --use_fp16\r\n```\r\nOutput:\r\n```\r\n[...]\r\n==126141== Profiling application: python ./Matrixmul.py --size 16000 --use_fp16\r\n==126141== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   96.77%  8.17152s        10  817.15ms  816.77ms  817.74ms  maxwell_fp16_sgemm_fp16_128x128_nn\r\n[...]\r\n```", "@yzhwang \r\n\r\nAccording to matmul_op.cc it doesn't seem to matter anyways, because autotune is not supported for halfs:\r\nhttps://github.com/tensorflow/tensorflow/blob/b06326cadc7b45078291e8fb3da0b7e941978540/tensorflow/core/kernels/matmul_op.cc#L235\r\n\r\nThat's why you don't autotune here https://github.com/tensorflow/tensorflow/blob/b06326cadc7b45078291e8fb3da0b7e941978540/tensorflow/core/kernels/matmul_op.cc#L299\r\n\r\nAnd launch the normal Gemm (without Ex) here: https://github.com/tensorflow/tensorflow/blob/b06326cadc7b45078291e8fb3da0b7e941978540/tensorflow/core/kernels/matmul_op.cc#L398", "@KillPinguin Thanks for pointing out. This was a change I made a couple of months ago due to a cublas bug back then. I will try to fix the issue in two steps:\r\n1) enable autotune support for fp16. I need to do some tests to make sure that no bug shows up.\r\n2) enable autotune for matmul by default in general. Last time I tried to enable it by default I got mixed performance results (for some shapes autotune helps, for some shapes it hurts the performance).\r\n\r\nBoth step involves simple change but extensive tests/benchmarks, so do not expect the change to happen within weeks.\r\n\r\nAlso, just so you know, as long as the first step is done, you should see a difference:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_blas.cc#L1863\r\nsince matmul for half is calling SgemmEx() now, just the compute type is still FP32.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi @yzhwang & @KillPinguin - we've been noticing the same issues. We have been running on Volta cards (a V100 and a Titan V) and see no performance improvement with FP 16. Happy to provide benchmarking code, etc.\r\n\r\nWe were wondering if you had any new thoughts or advice on this? Thanks in advance.", "Nagging Assignee @yzhwang: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Any news?", "@cpandar we now also tried it on a V100 lately, which also was slower with FP16 than FP32.\r\n\r\nUnfortunately I have switched positions since then and am no longer involved into the project, so I can't  conduct any more tests.", "/cc @tfboyd any coverage from the benchmark team?", "This appears to be a duplicate of #5592. Please follow that issue. If you feel this issue should be reopened, please provide new evidence and I'll do that.", "> This appears to be a duplicate of #5592. Please follow that issue. If you feel this issue should be reopened, please provide new evidence and I'll do that.\r\n\r\nThat issue is closed but this problem was never fixed. From simple test programs carrying out a few ops on a graph to several tf.keras layers, anything with float16 I've tried is significantly slower than 32-bit fp on a Turing GPU. ", "With the new FP16 we are seeing speedups with Keras us TF 2.0. It is still\nexperimental.  With a single gpu and synthetic data we are pushing 1,300\nimages per second.  More to come and working on the input pipeline (real\ndata).\n\nOn Mon, Mar 11, 2019, 4:55 PM Alvaro S. <notifications@github.com> wrote:\n\n> This appears to be a duplicate of #5592\n> <https://github.com/tensorflow/tensorflow/issues/5592>. Please follow\n> that issue. If you feel this issue should be reopened, please provide new\n> evidence and I'll do that.\n>\n> That issue is closed but this problem was never fixed. From simple test\n> programs carrying out a few ops on a graph to several tf.keras layers,\n> anything with float16 I've tried is significantly slower than 32-bit fp on\n> a Turing GPU.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15585#issuecomment-471787218>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWZesuMgLyFoNvGEC8aSZrQcF9r8famhks5vVuzugaJpZM4RK94i>\n> .\n>\n", "@tfboyd \r\nHello, Would you give more specific information about the experiment?\r\nI am comparing performance of FP32 and FP16 on the simple 2-layer LSTM model.\r\nI have built the latest version of tensorflow + keras and implemented the application on that.\r\n\r\ni couldn't see any acceleration with FP16, yet.\r\nAnd i made a issue thread regarding this at #26500 \r\n", "> @tfboyd\r\n> Hello, Would you give more specific information about the experiment?\r\n> I am comparing performance of FP32 and FP16 on the simple 2-layer LSTM model.\r\n> I have built the latest version of tensorflow + keras and implemented the application on that.\r\n> \r\n> i couldn't see any acceleration with FP16, yet.\r\n> And i made a issue thread regarding this at #26500\r\n\r\nSame here, lstm inference at least doesn't seem to improve. I did manage to run the tf cnn benchmark (after patching it, because it tries to include some stuff that's no longer there under the same name) and it does perform as expected. But it seems to me that fp16 support is spotty outside cnns\r\nOh, by the way, you should try CuDNNLSTM instead of LSTM, which is awfully slow.", "I failed to read the entire thread, sorry about that.  I do not have any LSTM examples FP16 yet.  We are just finishing up easy FP16 with NVIDIA to be released in a few weeks and per usual the first focus is on convs.  ", "@protoget do you know if float16 should be significantly faster on LSTM inference?", "meet the same problem on v100: fp16 is very slow than fp32", "I faces the same issue. \r\nI converted model to pure DT_HALF, run Google Cloud with Nvidia Tesla T4 and model with DT_HALF is slower in about 8-9 times than DT_FLOAT.\r\n\r\nIs this normal behaviour? Did anyone succeed to run FP16 with Tensorflow at least somewhere?\r\n\r\nMaybe FP16 can be used with Tensorflow TensorRT binding?", "meet the same problem with v100 on aws p3 instance,fp16 is 3x slower than fp32,I have not found the solution", "how to check if I am using fp32 or fp16 on v100?"]}, {"number": 15584, "title": "explained function generate_batch in Deep Learning Assignment 5 word2vec", "body": "please explain formula : data_index = (data_index + 1) % len(data) in function generate_batch\r\nthank you so so much", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15583, "title": "tensorflow logging glog redefined warning", "body": "I just integrated a tensorflow module to my existing c++ project. I was already using glog for logging in my existing project, logging to files.\r\n\r\nNow when I build the new project it throws warnings like:\r\n`/usr/local/include/google/tensorflow/tensorflow/core/platform/default/logging.h:254:0: warning: \"CHECK_GT\" redefined\r\n#define CHECK_GT(val1, val2) CHECK_OP(Check_GT, >, val1, val2)\r\n/usr/local/include/glog/logging.h:793:0: note: this is the location of the previous definition\r\n#define CHECK_GT(val1, val2) CHECK_OP(_GT, > , val1, val2)`\r\n\r\nIs this behavior safe? I am not concerned that much with the logs that tensorflow generates, I need my existing modules to continue logging to files.\r\n\r\nIf this behavior is unsafe, this could be a good feature request to have glog logging not affected by tensorflow logging.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "These may not be relevant here, nevertheless, here you go.\r\n\r\nHave I written custom code - yes\r\nOS Platform and Distribution - Ubuntu 16.04\r\nTensorFlow installed from - pip install\r\nTensorFlow version - 1.4\r\nBazel version - Using CMake\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 15582, "title": "upgrade tensorflow 1.4 from 1.2\uff0cget undefined symbol:error", "body": "I import c++ api of concat op\r\nand  add  \"//tensorflow/core/kernels:concat_lib_hdrs\", to the deps\r\nmy code work ok based on tf1.2 .\r\nwhen I upgrade tf to 1.4 ,compile and install is ok.\r\nwhen I run my model ,I got the error as :\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/python/ops/own_concat_ops_test.py\", line 5, in <module>\r\n    from tensorflow.contrib import own_concat\r\n  File \"/home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/__init__.py\", line 19, in <module>\r\n    from tensorflow.contrib.own_concat.python.ops.own_concat_ops import *\r\n  File \"/home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/python/ops/own_concat_ops.py\", line 11, in <module>\r\n    resource_loader.get_path_to_datafile('_own_concat_ops.so'))\r\n  File \"/home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/util/loader.py\", line 55, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"/home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/load_library.py\", line 56, in load_op_library\r\n    lib_handle = py_tf.TF_LoadLibrary(library_filename, status)\r\n  File \"/home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/my_work/.cache/bazel/_bazel_my_work/ce8941cb5767b65fdf1825ce866fc372/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/python/ops/_own_concat_ops.so: undefined symbol: _ZN10tensorflow9ConcatCPUIfEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2ElE11ConstMatrixESt14default_deleteIS8_EESaISB_EEPNS7_6MatrixE", "comments": ["Not totally clear to me what you're doing, but custom ops need to be linked again libtensorflow_framework.so in 1.4. See the [updated custom op instructions](https://www.tensorflow.org/versions/master/extend/adding_an_op#build_the_op_library). If you're building the custom op with Bazel, use tf_custom_op_library/tf_kernel_library/tf_cc_binary instead of the default Bazel rules (take a look at [contrib/boosted_trees](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/BUILD) for example).", "thanks for your reply ,I build my custom op with Bazel\r\n here is my BUILD\r\n\r\ntf_custom_op_library(\r\n    name = \"python/ops/_ddn_ops.so\",\r\n    srcs =  glob([\r\n        \"kernels/own_concat.cc\",\r\n        \"ops/own_concat.cc\",\r\n        ],\r\n        exclude = [\r\n        \"**/*test.cc\",\r\n        ],\r\n    ),\r\n    deps = [\r\n        \"//tensorflow/core:protos_all_cc\",\r\n        \"//tensorflow/core/kernels:ops_util_hdrs\",\r\n        \"//tensorflow/core/kernels:split_lib_hdrs\",\r\n        \"//tensorflow/core/kernels:concat_lib_hdrs\",\r\n    ],\r\n)\r\nand I\r\n`#include \"tensorflow/core/kernels/concat_lib.h\"\r\nnamespace tensorflow {\r\ntemplate <typename T>\r\nvoid Concat(OpKernelContext* c, const gtl::ArraySlice<Tensor>& inputs, int32 concat_dim,Tensor* output) {\r\n*******some preprocess code******\r\nConcatCPU<T>(c->device(), inputs_flat, &output_flat);\r\n};\r\n}`\r\n\r\nit works based on tensorflow 1.2,and I can use own_concat op api rightlly\r\nwhen I upgrade tenforflow version to 1.4 I get the error undefined symbol:error\r\nsomething of tensorflow BUILD or custom op BUILD changed ?\\\r\nHowever I compare the source codes of two version,I don't find any clues\r\n", "ldd my _own_concat_ops.so output is different between 1.2 and 1.4:\r\n1.4:\r\n`ldd  bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/python/ops/_own_concat_ops.so\r\n        linux-vdso.so.1 =>  (0x00007fffb29fe000)\r\n        libachk.so => /lib/libachk.so (0x00007f6cfd1cf000)\r\n        libtensorflow_framework.so => /home/yogur/new_TF/tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/python/ops/../../../../../_solib_k8/_U_S_Stensorflow_Scontrib_Sown_concat_Cpython_Sops_S_Uown_concat_Uops.so___Utensorflow/libtensorflow_framework.so (0x00007f6cfc5d8000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6cfc2d2000)\r\n        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6cfbfce000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6cfbdb8000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6cfb9f2000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007f6cfd331000)\r\n        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6cfb7ee000)\r\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6cfb5d0000)`\r\n+++++++++++++++++++++++++++++++++++++++\r\n1.2:\r\n`ldd bazel-out/local-opt/bin/tensorflow/contrib/own_concat/own_concat_ops_test.runfiles/org_tensorflow/tensorflow/contrib/own_concat/python/ops/_own_concat_ops.so\r\n        linux-vdso.so.1 =>  (0x00007fff513fe000)\r\n        libachk.so => /lib/libachk.so (0x00007f12a467f000)\r\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f12a4379000)\r\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f12a415b000)\r\n        libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f12a3e57000)\r\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f12a3c41000)\r\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f12a387b000)\r\n        /lib64/ld-linux-x86-64.so.2 (0x00007f12a4d1a000)`", "Oh I see, you're relying on symbols which are now hidden from custom ops (we moved core kernels to the language bindings). You can build with --config=monolithic to get the old behavior. You could also explicitly link against _pywrap_tensorflow_internal.so for that symbol.\r\n\r\nOr just copy the code you need from concat_lib_cpu.cc? It looks like there are only a few lines which call into other libraries. You'd just need to avoid the kernel registrations.", ":+1: \r\nWhen I get your latest reply ,I have tried copy the source code from concat_lib_cpu.cc to my own code\uff0c\r\nit do works~\r\nAfter I see your reply\uff0cI tried build with  --config=monolithic \uff0cit also works~\r\nI ended up choosing the copy code  method to prevent unknown changes of the dependent code\r\n\r\nthank you very much\uff01\uff01\r\n", "Awesome, glad it worked. "]}, {"number": 15581, "title": "TfLiteCameraDemo only contains 32-bit libtensorflowlite_jni.so", "body": "I strictly followed the steps on https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/ to\r\nbuild the TfLiteCameraDemo. But I found only 32-bit libtensorflowlite_jni.so was contained in the final APK.\r\n\r\nI modified the source code of libneuralnetworks.so to test my employer's NN accelerator, which needed the 64-bit libtensorflowlite_jni.so.\r\n\r\neven I configed bazel with\r\n./configure --config=android_arm64\r\n\r\nand built with \r\nbazel build --cxxopt=--std=c++11 --cpu=arm64-v8a //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo\r\n\r\ncould not work.\r\n\r\nWhere was my mistake? Or how can I pack the 64-bit  libtensorflowlite_jni.so into the final APK?\r\n", "comments": ["BTW:\r\n1. I made many changes in both java and c, and they run normally with the 32-bit libtensorlite_jni.so/libneuralnetworks.so\r\n2. I did an extreme attempt,\r\ncd /home/benshi001\r\ngrep armeabi-v7a * -r\r\nthen remove all lines with armeabi-v7a\r\nbut the bazel build still invoke armeabi-v7a build procedure\r\n\r\n", "@aselle ", "@angersson ,could you please take a look?", "I confirmed this by building with `bazel --config=android_arm64`, unpacking the .APK, and using the `file` command (note the armeabi-v7a directory):\r\n\r\n```\r\nlib/armeabi-v7a/libtensorflowlite_jni.so: ELF 32-bit LSB  shared object, ARM, EABI5 version 1 (SYSV), dynamically linked, stripped\r\n```\r\n\r\nThe internal version of the build with the same commands contains `arm64-v8a/libtensorflowlite_jni.so`, a 64-bit ELF file.\r\n\r\nI'll check to see if there's an easy fix. ", "From #9060, you can build the correct 64-bit library by setting the `--fat_apk_cpu=arm64-v8a` flag. \r\n\r\n```shell\r\nbazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n```", "I've submitted an internal change that adds --fat_apk_cpu=... when using --config=android_arm{,64}. Once it gets synced, we shouldn't see this particular problem much any more.", "Thank you so much! "]}, {"number": 15580, "title": "there is no gen_summary_ops", "body": "when i \r\nfrom tensorflow.contrib.summary import summary_ops\r\n\r\nit raises an error\r\n---> 29 from tensorflow.contrib.summary import gen_summary_ops\r\n     30 from tensorflow.core.framework import graph_pb2\r\n     31 from tensorflow.python.eager import context\r\n\r\nImportError: cannot import name 'gen_summary_ops'", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thank you for your reply.\r\nIn fact , I meet this problem when I tried to run the sample of multimodel in the tensor2tensor\uff0cthe official sample.And it told me that there is no module tensorflow.contrib.eager. So I checked my tensorflow lib and found there was no eager module in the directory contrib. Then I downloaded the whole directory contrib from /tensorflow/tensorflow/contrib in the github and replaced the local one.Then the problem appered.\r\nMy tensorflow version is tensorflow-gpu 1.4.1. And I find that it is the latest version on the pypi .And all the other relevant libs work well.Acturally, it works well except running the scripts from tensor2tensor.\r\nI guessed that maybe your codes in the contrib are not up-to-date", "@wqh17101 The \"gen\" part of `gen_summary_ops` is there because the code for these operations is generated during the build process. If you only take the raw code from github and don't build, you will end up with no generated classes.\r\n", "So,how can I get the module correctly?\n", "I have no experience with the GPU version of the Python libraries - so don't close this issue as it appears from what you've reported that there is some problem with the 1.4.1 GPU release.\r\n\r\nTo address your situation now, perhaps you could build your own Python wheel: https://www.tensorflow.org/install/install_sources\r\n", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I have tried ,but it did not work. My os is windows10 .So i hope i can solve this problem by update the tensorflow. But It does not right work .\r\n I do not know whether the Tensor2Tensor used the same Tensorflow which is released", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "I think your best option is to install the prepackaged binary via Anaconda/pip as per our [doc](https://www.tensorflow.org/install/install_windows). Let us know if that worked for you.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 15579, "title": "[Bazel/MSVC] Fix build error since aae439c", "body": "#15213", "comments": ["Can one of the admins verify this patch?", "I'd prefer to put the change in build_config.bzl\r\n\r\n", "@snnn Like https://github.com/rongjiecomputer/tensorflow/commit/924696e47d36ee644687093f83852bdc3d58f33e ?", "Yes. Just my personal suggestion.", "Jenkins, test this please."]}, {"number": 15578, "title": "Portability fix for StrAppend", "body": "#15557", "comments": ["Can one of the admins verify this patch?", "I am not sure this is correct. If the initial string is empty, not append?", "Here the size of the \"result\" string is after expending? empty() is a shortcut for size()==0 ? ", "If you do `string x; StrAppend(&x, 1, 2)` then you're supposed to get `\"12\"`.  I think your check will not result in the correct string.", "Ok... Let me check it. Sorry, I'm in vacation right now. I'll do it in this weekend.", "Close it as you will migrate these GTL containers to ABSL. "]}, {"number": 15577, "title": "update README.md", "body": "Added whl file downloading links of Python 3.6 for Linux with CPU only and GPU both :)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it.\n\nOn 22-Dec-2017 1:50 pm, \"googlebot\" <notifications@github.com> wrote:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n>\n> \ud83d\udcdd *Please visit https://cla.developers.google.com/\n> <https://cla.developers.google.com/> to sign.*\n>\n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> verify. Thanks.\n> ------------------------------\n>\n>    - If you've already signed a CLA, it's possible we don't have your\n>    GitHub username or you're using a different email address on your commit.\n>    Check your existing CLA data <https://cla.developers.google.com/clas>\n>    and verify that your email is set on your git commits\n>    <https://help.github.com/articles/setting-your-email-in-git/>.\n>    - If your company signed a CLA, they designated a Point of Contact who\n>    decides which employees are authorized to participate. You may need to\n>    contact the Point of Contact for your company and ask to be added to the\n>    group of authorized contributors. If you don't know who your Point of\n>    Contact is, direct the project maintainer to go/cla#troubleshoot. The email\n>    used to register you as an authorized contributor must be the email used\n>    for the Git commit.\n>    - In order to pass this check, please resolve this problem and have\n>    the pull request author add another comment and the bot will run again. If\n>    the bot doesn't comment, it means it doesn't think anything has changed.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/15577#issuecomment-353546185>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT6G0MHHhTcRob-r2Yl6S86Zz9bbbgL6ks5tC2ZGgaJpZM4RKw6Y>\n> .\n>\n", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks @im-minion. You're correct in that the readme does not have updated links to 3.6 whl files. ", "Closing: #15666"]}, {"number": 15576, "title": "Feature request: Possible to introduce tf.complex32 datatype", "body": "Just recently I figured out, that our GPU (TitanX 12GB) is not big enough of its memory capacities to process our data. I was wondering whether there are attempts to \"quantize\" the complex datatypes as well. \r\n\r\nI've seen that there are approaches to quantize float32 datatypes for example and was wondering if this is possible for real/imag types as well. \r\n\r\nThanks a lot! ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Nope, there aren't any active efforts towards this AFAIK.\r\n\r\nClosing this out since the feature request itself is a bit broad and there is nobody actively working on this.", "I know this issue has been closed for being too broad, but I am still wondering whether it should be considered for people trying to optimize their model.\r\nIn particular, [this section](https://www.tensorflow.org/guide/profiler#improve_device_performance) specifies that you should use half-precision to improve device performance, but it isn't possible with complex dtypes (which are needed for Fourier transforms for example).\r\n\r\nI haven't looked into [mixed precision](https://www.tensorflow.org/guide/keras/mixed_precision) yet (to have all other operations performed with half-precision), but a complex32 dtype would simplify things. "]}, {"number": 15575, "title": "TensorFlow automatically modify variable scope name", "body": "check the following code:\r\n\r\n```python\r\nwith tf.variable_scope('test'):\r\n      v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')\r\n\r\nwith tf.variable_scope('test'):\r\n      v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')\r\n\r\nprint(v1)\r\nprint(v2)\r\n```\r\n\r\n\r\nthe code output:\r\n\r\n ```python\r\nTensor(\"test/v1:0\", shape=(10, 10), dtype=float32)\r\nTensor(\"test_1/v2:0\", shape=(5, 5), dtype=float32)\r\n```\r\n\r\nIt looks ridiculous for TF modfiy variable scope name 'test' into 'test_1' while my new placeholder named 'v2' is different from 'v1'. However, if i add variables, everything is going to be normal. code like:\r\n\r\n```python\r\n    with tf.variable_scope('test'):\r\n        v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')\r\n        w1 = tf.get_variable('w1', shape=(2,2))\r\n\r\n    with tf.variable_scope('test'):\r\n        v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')\r\n        w2 = tf.get_variable('w2', shape=(3, 3))\r\n    print(v1)\r\n    print(v2)\r\n    print(w1)\r\n    print(w2)\r\n```\r\noutputs:\r\n```python\r\nTensor(\"test/v1:0\", shape=(10, 10), dtype=float32)\r\nTensor(\"test_1/v2:0\", shape=(5, 5), dtype=float32)\r\n<tf.Variable 'test/w1:0' shape=(2, 2) dtype=float32_ref>\r\n<tf.Variable 'test/w2:0' shape=(3, 3) dtype=float32_ref>\r\n```\r\n\r\nI dont know why TF **just modify the variable scope name** while i try to add a new placeholder into the existed scope.Is it a BUG?\r\n", "comments": ["The behavior is what we expect, not a bug. To understand it, you have to figure out two conceptions: `tf.name_scoep` and `tf.variable_scope`.\r\n\r\nThese posts below perhaps are useful for you:\r\n+ [What's the difference of name scope and a variable scope in tensorflow?](https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow)\r\n+ [ Sharing Variables ](https://www.tensorflow.org/versions/r0.12/how_tos/variable_scope/)\r\n+ [ TensorBoard: Graph Visualization ](https://www.tensorflow.org/get_started/graph_viz)\r\n\r\nIf you want to reenter a variable scope, use \r\n```python\r\nwith tf.variable_scope('test') as scope:\r\n    v1 = tf.placeholder(tf.float32, shape=(10,10), name='v1')\r\n    w1 = tf.get_variable('w1', shape=(2,2))\r\n\r\nwith tf.variable_scope(scope) as scope2:\r\n    with tf.name_scope(scope2.original_name_scope):\r\n         v2 = tf.placeholder(tf.float32, shape=(5, 5), name='v2')\r\n         w2 = tf.get_variable('w2', shape=(3, 3))\r\n```", "Well, it does work! Thanks for you help and answer", "What if i need to do this in the case where i have a scope inside a scope? `Your method works for the outermost scope but TF still automatically renames the variables for the scopes inside."]}, {"number": 15574, "title": "Fix adding elements to collections.deque.", "body": "In some cases, [word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L123) will fail at L123 and throw the following error:\r\n```\r\nTypeError: sequence index must be integer, not 'slice'\r\n```\r\nIt seams the ```text8``` dataset and parameters will not touch that line code, but I hit this issue when I run this script against my own dataset. This is caused by Python ```collections.deque``` doesn't support index by slice:\r\n```\r\n>>> from collections import deque\r\n>>> d = deque('tensorflow')\r\n>>> d[:]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: sequence index must be integer, not 'slice'\r\n```\r\nThis PR fix this issue by using ```deque.extend```, and this is consistent with L114.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please."]}, {"number": 15573, "title": "'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created", "body": "Hi i'm running the tensorflow from the source and after building the tensorflow using bazel using below command \r\n\r\n**bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package** \r\n\r\n**The error message pasted below**\r\n\r\n10 errors detected in the compilation of \"/home/lb/.cache/bazel/_bazel_lb/144f5944ef8ff562c57e67fdaa41565f/execroot/org_tensorflow/tmpae8_5f0170552fd082f4/tmpxft_0000245c_00000000-6_reduce_slice_ops_gpu.cu.cpp1.ii\".\r\nERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: output 'tensorflow/contrib/reduce_slice_ops/_objs/python/ops/_reduce_slice_ops_gpu/tensorflow/contrib/reduce_slice_ops/kernels/reduce_slice_ops_gpu.cu.pic.o' was not created\r\nERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/reduce_slice_ops/BUILD:14:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 74.030s, Critical Path: 8.38s\r\nFAILED: Build did NOT complete successfully\r\n", "comments": ["Don't you have other error messages above those? Can you post them as well?", "No i only have this much \r\nand i'm using \r\nCUDA 9.1 with GCC 5.4 version", "Even the posted error message mentions '10 errors', try to find them :) I had today 10 errors as well, that was with Eigen...", "ERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/contrib/rnn/BUILD:218:1: error while parsing .d file: /home/lb/.cache/bazel/_bazel_lb/144f5944ef8ff562c57e67fdaa41565f/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/rnn/_objs/python/ops/_lstm_ops_gpu/tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.pic.d (No such file or directory)\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:14:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from ./tensorflow/contrib/rnn/kernels/lstm_ops.h:19,\r\n                 from tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc:20:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/Core:59:34: fatal error: math_functions.hpp: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1804.018s, Critical Path: 117.24s\r\nFAILED: Build did NOT complete successfully\r\n", "How to resolve this even i'm getting the same error with Eigen", "Check these @Avinash01111992, a symlink will solve it:\r\nhttps://github.com/tensorflow/tensorflow/issues/15389\r\nhttps://stackoverflow.com/a/47807106", "INFO: From Compiling tensorflow/core/kernels/bincount_op_gpu.cu.cc:\r\n./tensorflow/core/util/cuda_kernel_helper.h(116): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(116): error: expected a \")\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(117): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(120): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(120): error: expected a \";\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(126): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(126): error: expected a \")\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(127): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(130): error: namespace \"Eigen::half_impl\" has no member \"__half_raw\"\r\n\r\n./tensorflow/core/util/cuda_kernel_helper.h(130): error: expected a \";\"\r\n\r\n10 errors detected in the compilation of \"/home/lb/.cache/bazel/_bazel_lb/144f5944ef8ff562c57e67fdaa41565f/execroot/org_tensorflow/tmp557_797e0d9b2e6c8ac1/tmpxft_0000709d_00000000-6_bincount_op_gpu.cu.cpp1.ii\".\r\nERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/core/kernels/BUILD:3236:1: output 'tensorflow/core/kernels/_objs/bincount_op_gpu/tensorflow/core/kernels/bincount_op_gpu.cu.pic.o' was not created\r\nERROR: /home/lb/WorkSpace/AI-WORK/tensorflow/tensorflow/core/kernels/BUILD:3236:1: not all outputs were created or valid\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 688.604s, Critical Path: 79.75s\r\nFAILED: Build did NOT complete successfully\r\n", "@gorogm  HI i did this Symlink stuff still i'm facing the same issue ", "If your message is still\r\n`math_functions.hpp: No such file or directory`\r\nthan work a bit more on the symlink.\r\n\r\nFor the solution of Eigen::half_impls I'm afraid we have to wait a bit, I found a github-issue for this as well some hours ago:\r\nhttps://github.com/tensorflow/tensorflow/issues/15108", "I done with symlink \r\n\r\n**ln: failed to create symbolic link '/usr/local/cuda/include/math_functions.hpp': File exists**\r\n\r\nAre you getting same issue even after symlink ?\r\n**Eigen::half_impls** \r\nIS this github-issues ?\r\n\r\nHope this issue resolved once the new push is done ?\r\n", "@gorogm  did you got the solution for  Eigen ", "@Avinash01111992 I'm using an older commit, which works.", "@gorogm  Ok thanks , I'll try with older commit and will let you know if i get any issues and please do intimate me if this issue resolved in master branch.", "@Avinash01111992 I'll try my best, however I'm not a developer of tensorflow, just an other user :) So I won't have any private information, I can only suggest you to subscribe to the mentioned issues, and you'll be notified there. Have a nice day!", "@gorogm  Hi , After checkout with previous commit  I'm getting the below error  \r\n\r\nERROR: /home/lb/ai@work/tensorflow/tensorflow/contrib/verbs/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/verbs:grpc_verbs_service' failed (Exit 1)\r\nIn file included from ./tensorflow/contrib/verbs/rdma_mgr.h:24:0,\r\n                 from ./tensorflow/contrib/verbs/grpc_verbs_service.h:22,\r\n                 from tensorflow/contrib/verbs/grpc_verbs_service.cc:22:\r\n./tensorflow/contrib/verbs/rdma.h:21:30: fatal error: infiniband/verbs.h: No such file or directory\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1615.770s, Critical Path: 93.59s\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Do you have any idea on this error ?**\r\nThanks and have a nice day ", "This this sounds like a duplicate of the aforementioned issue. Please click subscribe on that and thank you for helping our friend @gorogm."]}, {"number": 15572, "title": "layers.Conv3DTranspose doesn't work with unspecified DWH dimensions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (n/a)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 (All affected)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\ntensorflow.layers.Conv3DTranspose doesn't work when the input dimensions are not specified and bias is added.  This occurs because the layer output is reshaped from five dimensions to 4 by combining the depth and height dimensions:  None * None throws an exception.  The code for this is in tensorflow/python/layers/convolutional.py, line 1608.\r\n\r\nIt appears that this reshaping is done because nn.bias.add can't handle 5 dimensional inputs.  This seems like it should be an easy fix since nn.bias.add just broadcasts across all but the channel dimension anyway.  The *Transpose layers should really all use the same general _ConvTranspose style that the non-transposed convolution layers use, to avoid code duplication.\r\n\r\nIn the meantime, tensorflow.layers.Conv3D DOES work with unspecified input dimensions and channel-last ordering.  It turns out that Conv3D and Conv3DTranspose use different code for adding the bias.  In lieu of a nn.bias.add fix, this problem can be alleviated by using the Conv3D bias code for Conv3DTranspose.\r\n\r\n### Suggested fix (tested)\r\n\r\nIn tensorflow/python/layers/convolutional.py, replace lines 1609-1625 with (modified from lines 169-189): \r\n\r\n        if self.data_format == 'channels_first':\r\n              outputs_shape = outputs.shape.as_list()\r\n              outputs_4d = array_ops.reshape(outputs,\r\n                                             [outputs_shape[0], outputs_shape[1],\r\n                                              outputs_shape[2] * outputs_shape[3],\r\n                                              outputs_shape[4]])\r\n              outputs_4d = nn.bias_add(outputs_4d, self.bias, data_format='NCHW')\r\n              outputs = array_ops.reshape(outputs_4d, outputs_shape)\r\n          else:\r\n            outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\r\n", "comments": ["If you'd like, I think tensorflowers would welcome your contribution to fix the question. ", "@rmlarsen do you agree this seems like a good fix? If so @robb-brown would you make a PR? Thanks!", "Pull request #15595.", "Nagging Assignee @rmlarsen: It has been 259 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 15571, "title": "why keras run slower and slower", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Optimization is a challenging problem that the team takes very seriously, and we appreciate the feedback. However I can't keep this issue open due to information."]}, {"number": 15570, "title": "Using grid_rnn_cell.Grid1LSTMCell makes stack_bidirectional_dynamic_rnn Throw different shape errors", "body": "I encountered a problem using grid_rnn_cell.Grid1LSTMCell. I have no problems using it on contrib static_rnn and contrib static_bidirectional_rnn.\r\n\r\n```\r\nclass GridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.static_rnn(self.cell, self.input_layer, dtype=tf.float32)\r\n\r\nclass BidirectionalGridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cell_fw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n        self.cell_bw = grid_rnn_cell.Grid1LSTMCell(num_units=8)\r\n\r\n    def test_simple_bidirectional_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.static_bidirectional_rnn(self.cell_fw, self.cell_bw, self.input_layer, dtype=tf.float32)\r\n```\r\n\r\nHowever, when I test it on contrib stack_bidirectional_dynamic_rnn:\r\n\r\n```\r\nclass StackBidirectionalGridRNNTest(tf.test.TestCase):\r\n    def setUp(self):\r\n        self.num_features = 1\r\n        self.time_steps = 1\r\n        self.batch_size = 1\r\n        tf.reset_default_graph()\r\n        self.input_layer = tf.placeholder(tf.float32, [self.batch_size, self.time_steps, self.num_features])\r\n        self.cells_fw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]\r\n        self.cells_bw = [grid_rnn_cell.Grid1LSTMCell(num_units=8) for _ in range(2)]\r\n\r\n    def test_stack_bidirectional_grid_rnn(self):\r\n        self.input_layer = tf.unstack(self.input_layer, self.time_steps, 1)\r\n        rnn.stack_bidirectional_dynamic_rnn(self.cells_fw, self.cells_fw, self.input_layer, dtype=tf.float32)\r\n```\r\n\r\nI encounter these shape errors:\r\n\r\n(1) When the input_layer is unstacked: `Shape (1, 1) must have rank at least 3`\r\n(2) When the input_layer is not unstacked: Shape `(1, 2, 8) must have rank 2`\r\n\r\nWhich are, apparently,  conflicting with each other.", "comments": ["While I'm at it, when I test it on contrib stack_bidirectional_rnn (simply replace the above stack_bidirectional_dynamic_rnn), I encounter this error:\r\n\r\n`'tuple' object has no attribute 'dtype'`", "A few more things I've observed\r\n\r\n-Apparently, setting the `batch_size` to `None` will cause this error: `Cannot convert a partially known TensorShape to a Tensor: (?, 0)`. Seems that the batch size has to be known before the inputs are fed into the grid lstm.\r\n-There is no problem in setting `time_steps` to None.\r\n", "@phvu I'm doing some learning tests to be able to use grid_rnn and above are my recent findings.", "I'll defer to @phvu on this one.  @michaelisard are we able to assign users not in the TF org?", "@ebrevdo I don't think so unfortunately.", "I don't think 1-LSTM is designed to be stacked (in the way you are trying to do). You should probably try 2-LSTM cells, and read the paper for more information.", "Thanks @phvu I'll read the paper just a bit more."]}, {"number": 15569, "title": "Dataset shuffle operation is not deterministic", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:elementary OS 0.4.1 Loki\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Not related\r\n- **GPU model and memory**: Not related \r\n- **Exact command to reproduce**: mentioned below\r\n\r\n\r\n### Describe the problem\r\nDataset Shuffle operation is not determenastic \r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef test():\r\n  np.random.seed(42)\r\n  tf.set_random_seed(42)\r\n  numbers = np.arange(1,100)\r\n\r\n  def get_data():\r\n    dataset = tf.data.Dataset.from_tensor_slices(numbers)  # some initial dataset\r\n    dataset =  dataset.shuffle(100)\r\n    x = dataset.make_one_shot_iterator().get_next()\r\n    return x\r\n\r\n  # execution 1\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch1 = sess.run(x)\r\n    print(x_batch1)\r\n  # clear out everything\r\n  tf.reset_default_graph()\r\n\r\n  # execution 2\r\n  x = get_data()\r\n  with tf.Session() as sess:\r\n    x_batch2 = sess.run(x)\r\n    print(x_batch2)\r\n  # results should be equivalent\r\n  assert np.allclose(x_batch1, x_batch2)\r\ntest()\r\n```\r\ntest code sample taken from @dusenberrymw", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "When you call `tf.reset_default_graph()`, it resets the seed. If you call `tf.set_random_seed(42` after `tf.reset_default_graph()` the result is deterministic as expected."]}, {"number": 15568, "title": "InvalidArgumentError: slice index 0 of dimension 0 out of bounds.", "body": "Hi,\r\n\r\n### Describe the problem\r\nI am trying to encode and decode an image using tf.image.decode_jpeg and b64 encoding. The code is so simple but it seems I have an error regarding StrideSlice. I am not sure if the problem coming from the undefined shape of input placeholder or related to jpeg_decode.  Thanks. \r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport base64\r\nimport functools\r\n\r\n\r\ndef build_graph(input_len=None):\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        image_bytes = tf.placeholder(tf.string, name='input_node')\r\n        images = tf.map_fn(\r\n            functools.partial(tf.image.decode_jpeg, channels=3),\r\n            image_bytes,\r\n            dtype=tf.uint8\r\n        )\r\n        image_floats = tf.cast(images, tf.float32, name=\"output_node\") / 255.0\r\n\r\n    return graph\r\n\r\n\r\nif __name__ == '__main__':\r\n    file_name = \"test_1.jpg\"\r\n    with open(file_name, \"rb\") as imageFile:\r\n        image_string = imageFile.read()\r\n        image_encode = base64.b64encode(image_string).decode(\"utf-8\")\r\n\r\n    graph = build_graph(input_len=len(image_encode))\r\n\r\n    with tf.Session(graph=graph) as session:\r\n        init_op = tf.group(tf.global_variables_initializer(),\r\n                           tf.local_variables_initializer())\r\n        session.run(init_op)\r\n        input_node = graph.get_tensor_by_name(\"input_node:0\")\r\n        output_node = graph.get_tensor_by_name(\"output_node:0\")\r\n        image_out = session.run(output_node, feed_dict={input_node: image_encode})\r\n```\r\n\r\nThese is the traceback:\r\n\r\n```\r\nCaused by op u'map/TensorArrayUnstack/strided_slice', defined at:\r\n  File \"/.../Simple_Graph_Design/ImageTest.py\", line 26, in <module>\r\n    graph = build_graph(input_len=len(image_encode))\r\n  File \"/.../Simple_Graph_Design/ImageTest.py\", line 13, in build_graph\r\n    dtype=tf.uint8\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/functional_ops.py\", line 354, in map_fn\r\n    elem_ta.unstack(elem) for elem_ta, elem in zip(elems_ta, elems_flat)]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 107, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 412, in unstack\r\n    num_elements = array_ops.shape(value)[0]\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 5430, in strided_slice\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): slice index 0 of dimension 0 out of bounds.\r\n\t [[Node: map/TensorArrayUnstack/strided_slice = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](map/TensorArrayUnstack/Shape, map/TensorArrayUnstack/strided_slice/stack, map/TensorArrayUnstack/strided_slice/stack_1, map/TensorArrayUnstack/strided_slice/stack_1)]]\r\n\r\n```\r\n### System information\r\n- TF version: 1.4.0\r\n- Linux Ubuntu 16.04\r\n- Python version: 2.7\r\n- No CUDA or GPU \r\n\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Not really!\r\nOS Platform and Distribution: Linux, Ubuntu 16.04\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.4.0\r\nBazel version: Don't have\r\nCUDA/cuDNN version: Don't have\r\nGPU model and memory: Don't have\r\nExact command to reproduce: see above", "It seems to me that there are two changes that need to be made to the code for it to work.\r\n\r\n1. Currently it gets a single string as the input of `tf.map_fn`, which should get a 1-D Tensor of strings instead (that is where the error is coming from). This could be fixed by losing `map_fn` altogether or passing a list of strings (instead of a single string) as `input_node` in the `feed_dict`.\r\n\r\n2. To the best of my knowledge, `tf.image.decode_jpeg` does not decode `base64`-encoded jpeg strings. They need to be decoded first. That could be done with the function `base64.b64decode`. `bytes.decode` does not decode base64 strings either, as far as I know.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Solved! @IanTayler Thanks. :) ", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "@AziziShekoofeh Could you please tell me how you resolved the issue? I have the same error when I send a string input in a client-server program.", "@R-Miner \r\nI was hitting the error if the Feed the data as below\r\n```\r\nbase64_decoded_output_data = sess.run(dense, feed_dict={input_data: image['base64'] })\r\n```\r\nThen as suggested, I have passed the data in a list format, which resolved the issue\r\n```\r\nbase64_decoded_output_data = sess.run(dense, feed_dict={input_data: [image['base64'] ]})\r\n```", "I got the same error when I failed to use tf.map_fn the right way, the following code may work~\r\n```\r\ndef process(image):\r\n    # u process func\r\n    # eg: return tf.image.decode_jpeg(image, channel=3)\r\n\r\nfilenames = [image1, image2, image3]\r\n\r\nwith tf.Session() as sess:\r\n    filenames = tf.constant(filenames)\r\n    processed_images = tf.map_fn(process, filenames, dtype=tf.float32)\r\n    # if necessary:\r\n    # processed_images = tf.concat(processed_images, axis=0)\r\n    images = sess.run(processed_images)\r\n```", "I got the same error with C++ code:\r\n```\r\ntensorflow::Tensor imgBGRTF(tensorflow::DT_STRING, tensorflow::TensorShape());\r\nimgBGRTF.scalar<tensorflow::string>()() = imgStr;\r\n```\r\nand i changed to:\r\n```\r\ntensorflow::Tensor imgBGRTF(tensorflow::DT_STRING, tensorflow::TensorShape({1}));\r\nimgBGRTF.scalar<tensorflow::string>()(0) = imgStr;\r\n```\r\nHowever, i got: \r\nCheck failed: NDIMS == dims() (3 vs. 4)Asking for tensor of 3 dimensions from a tensor of 4 dimensions\r\n\r\nWhat should i do?", "i found the error in my code, \r\nCheck the x_train shape, in my case was (21000, 0, 1), try to change the 0 for some number grater than 0, "]}, {"number": 15567, "title": "Recognize more environments as interactive", "body": "... and log to stdout in those cases, and show INFO messages by default.\r\n\r\nFixes #6438.", "comments": ["Jenkins, test this please.", "Sorry Martin, It looks like I made an error verifying that bug.\r\n\r\nAs far as I can tell `tf.logging` in a `jupyter` notebook is working already."]}, {"number": 15566, "title": "MKL: Reverting PR #14478, which breaks Inception v3 with MKL.", "body": "PR #14478 https://github.com/tensorflow/tensorflow/pull/14478 (based on commit e7b69e4) removed line 276 from  #ifdef INTEL_MKL section and breaks Inception v3 with MKL. Perhaps someone meant to remove line 202?", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 15565, "title": "Linux GPU build failing (__CUDACC_VER__ is no longer supported)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10, kernel 4.14.8\r\n- **TensorFlow installed from (source or binary)**:\r\nSource (failed build)\r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.9\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0.4\r\n- **GPU model and memory**:\r\nTwo 1080 Ti (11Gb each)\r\n- **Exact command to reproduce**:\r\nbazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n### Describe the problem\r\nI note the build bot is failing too for Linux GPU, since the latest merge in the past 1 hour.\r\n\r\n### Source code / logs\r\nERROR: /home/daniel/build/tensorflow/tensorflow/contrib/image/BUILD:106:1: error while parsing .d file: /home/daniel/.cache/bazel/_bazel_daniel/a40ff47569db15fec953d4e5b5812083/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/image/_objs/python/ops/_distort_image_ops_gpu/tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.pic.d (No such file or directory)\r\nIn file included from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/common_functions.h:50:0,\r\n                 from /usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/cuda_runtime.h:115,\r\n                 from <command-line>:0:\r\n/usr/local/cuda-9.0/bin/../targets/x86_64-linux/include/crt/common_functions.h:64:24: error: token \"\"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\" is not valid in preprocessor expressions\r\n #define __CUDACC_VER__ \"__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.\"\r\n                        ^\r\n./tensorflow/core/util/cuda_device_functions.h:37:7: note: in expansion of macro '__CUDACC_VER__'\r\n #elif __CUDACC_VER__ >= 7050\r\n       ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 237.382s, Critical Path: 22.79s\r\n", "comments": ["This was just fixed internally. It will be fixed in GitHub when the changes are pushed, which probably will occur by the end of the week.", "If it's already committed internally then I'm going to close this out. If the issue continues to persist at head a week from now, let me know and I'll re-open.", "This still occurs if I build from source...\r\n\r\nexecroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/contrib/image/_objs/python/ops/_distort_i\r\nmage_ops_gpu/tensorflow/contrib/image/kernels/adjust_hsv_in_yiq_op_gpu.cu.pic.d\r\n\r\nafter running:\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures"]}, {"number": 15564, "title": "sparsemax implementation is not infinite-safe", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, but I have a simple reproducer below\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Fedora 27\r\n- **TensorFlow installed from (source or binary)**: pip3 install --user tensorflow\r\n- **TensorFlow version (use command below)**:  1.4.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: no\r\n- **GCC/Compiler version (if compiling from source)**: no\r\n- **CUDA/cuDNN version**: no\r\n- **GPU model and memory**: no (x86_64 CPU)\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\ntf.contrib.sparsemax.sparsemax does not produce the correctly expected results when infinities are present in the input.\r\n\r\nMathematically, a negative infinity should be treated the same as a very large negative number, and produce a 0 in the output.\r\n\r\nThis is a real-world problem when sparsemax is combined with contrib.seq2seq.LuongAttention (as suggested in its documentation, and also as suggested in the sparsemax paper), because entries past the input length in the batch will have a score of -inf by default.\r\n\r\n### Source code / logs\r\n\r\nReproducer (assume tensorflow as tf)\r\n```\r\n>>> tf.contrib.sparsemax.sparsemax([[1., -1e+5]]).eval(session=sess)\r\narray([[ 1.,  0.]], dtype=float32)\r\n>>> tf.contrib.sparsemax.sparsemax([[1., float('-inf')]]).eval(session=sess)\r\n2017-12-21 22:13:55.697302: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\n2017-12-21 22:13:55.697997: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\n2017-12-21 22:13:55.699634: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\n2017-12-21 22:13:55.699980: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\nTraceback (most recent call last):\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\n\t [[Node: sparsemax_7/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](sparsemax_7/sub, sparsemax_7/stack)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 570, in eval\r\n    return _eval_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 4455, in _eval_using_default_session\r\n    return session.run(tensors, feed_dict)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\n\t [[Node: sparsemax_7/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](sparsemax_7/sub, sparsemax_7/stack)]]\r\n\r\nCaused by op 'sparsemax_7/GatherNd', defined at:\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/contrib/sparsemax/python/ops/sparsemax.py\", line 69, in sparsemax\r\n    tau_sum = array_ops.gather_nd(z_cumsum, indices)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1971, in gather_nd\r\n    \"GatherNd\", params=params, indices=indices, name=name)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/home/redacted/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): flat indices[0, :] = [0, -1] does not index into param (shape: [1,2]).\r\n\t [[Node: sparsemax_7/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](sparsemax_7/sub, sparsemax_7/stack)]]\r\n```\r\n\r\n", "comments": ["@AndreasMadsen am I right that you contributed this? Any thoughts?", "According to the [paper](https://arxiv.org/pdf/1602.02068.pdf):\r\n<img width=\"439\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1112263/34284021-fc2b3a64-e709-11e7-9469-984e7c632a29.png\">\r\n\r\nI agree that `-inf` should return 0 in common, however we need take care of it when all elements are `-inf`. In that case, both `-inf + 1 > -inf` and `-inf / -inf` are undefined if I understand it correctly.", "@michaelisard you are correct.\r\n\r\nI would say if all values are `-inf` they should be assigned an equal probability.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Yes, this is still an issue (not sure what \"awaiting response\" is for, is it like needinfo in bugzilla? if so, not sure what response you expect...)\r\nAdditionally, I observed that large positive or negative numbers also gives invalid results, because adding 1 has no effect.\r\nIn turn, this results in a sparsemax value outside of [0, 1], which breaks the math of the attention function later on.", "The original poster has replied to this issue after the stat:awaiting response label was applied.", "So @AndreasMadsen what do you think of @facaiy comment? That it's undefined? If so, then we can either 1) not promise any value, so it's not a bug, or 2) define an expected behavior. In the latter case, then we either set to the value that @gcampax suggested, or leave it as is, but specify it in the documentation.", "@drpngx I think the behavior should be the same as with softmax. Although, I don't remember what that is.\r\n\r\nSoftmax and Sparsemax share many properties and the use case it the same.", "Right. Can you send a PR?", "Also, it's always a good idea to check what numpy does. We don't have to be compatible but we should clearly indicate when we are", "Right now I'm a bit busy, but I should be able to work on this from Febuary 19th as I will be moved to our Open Source Data Science Team.", "Dear guys,\r\n\r\nMy name is Kien. Would you mind telling me whether the infinite-safe problem with sparsemax has been resolved or not? I am using TF 1.3 and still encounter this problem. Thank you.", "@clarken92 It hasn't been solved, as far as I know. I would really like to solve this, but I'm having a hard time allocating the time and I'm on vacation right now.", "@AndreasMadsen Thank you for replying. I have checked the code from tensorflow, it not difficult to understand. I think based on the detailed algorithm from the paper, I could fix this and will report within 1 or 2 days.\r\nI have to say that I really love this paper. Moving from softmax to sparsemax change a lot of concepts: soft => hard, nonlinear => linear. I hope that in the future, it can be combined with relational deep learning and stochastic discrete model like REBAR and RELAX to enable reasoning over discrete structures.", "Hi guys,\r\n\r\nI think I have solved the problem of sparsemax. The main problem is that the code for normalizing the logits in sparsemax() function substracts the **mean** instead of **max**. mean is very sensitive to noise (very positive or very negative numbers) since it includes all elements. For example, if we have 3 elements, one of them is -1e30 while the others 2 are 0.1 and 0.2, subtracting the mean will result in all three values around 1e29, causing a precision problem and other problem like division of a very large number by a very large number. Therefore, I replaced \r\n`z = logits - math_ops.reduce_mean(logits, axis=1)[:, array_ops.newaxis]`\r\nwith \r\n`z = logits - tf.reduce_max(logits, axis=1, keep_dims=True)` \r\nand the code works well. This is similar to the trick for robust computation in softmax.\r\n\r\nAnother problem I discovered is that the maximum range that we can detect small change is about [-1e6, 1e6]. So if you have 2 values like 1e7 and 1e7 + 0.5, they will be treated equally. I show this phenomenon in my unit test. To prevent this, it added in the code of sparsemax() a control_dependency that checks whether there is at least one value within this range or not. If not, we should not compute sparsemax since it will cause a precision error later.\r\n\r\nI attatch here my code and my unit tests. Could you please check whether it is OK or not. Thank you.\r\n[my_sparsemax.zip](https://github.com/tensorflow/tensorflow/files/2119313/my_sparsemax.zip)\r\n\r\n  ", "One more thing I forget to say, I also add the option for computing sparsemax in case of 2 classes where only one logit value is available. This behavior is similar to sigmoid function. In this case, I assume that the logit for class 0 is 0 and the logit for class 1 is the logit itself.", "@clarken92 I don't get how `max` makes things better. Yes, you avoid the issue of `mean` becoming `-inf`. But you still have an issue with `inf` and `nan`.\r\n\r\nRegarding, the stability of `mean` I don't think your solution fixes that much as it will still cause issues if a value is `1e30`. When comparing with `softmax` you should be aware that the numerical stability comes from very different places. In `sparsemax` the numerical instability comes from the `cumsum` in `softmax` it comes from `exp`. Hence the solution to prevent this should be different.\r\n\r\nIn the end, I don't see how this addresses the issue of `inf` and `nan`. To me, your solution just addresses possible issues regarding numerical stability.\r\n\r\n--- \r\n\r\nI finally got time to make a fix for this, for now it is here: https://github.com/AndreasMadsen/tensorflow/commit/7e25025409e7921f2e7816dbc8ba7fe3a532428b. I will open a PR as soon as I have tested it properly.", "PR with a nan an inf safety fix: https://github.com/tensorflow/tensorflow/pull/21183"]}, {"number": 15563, "title": "Cannot statically link against Tensorflow library in Golang", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: `go build -a -v -o hellotf --ldflags '-linkmode external -extldflags \"-static -L /usr/local/lib\"' -x <GITHUB_PROJ_NAME>/hellotf`\r\n\r\n### Describe the problem\r\nI'm trying to compile a **statically-linked** binary of the hello world app against the TF 1.4.0 binary per the [golang install & hello-world instructions](https://www.tensorflow.org/install/install_go) and am not able to successfully link against the TF library as it reports that `ltensorflow` cannot be found by `/usr/bin/ld` in the `go build`.\r\n\r\nThe command used is:\r\n`go build -a -v -o hellotf --ldflags '-linkmode external -extldflags \"-static -L /usr/local/lib\"' -x <GITHUB_PROJ_NAME>/hellotf`\r\n\r\nI've tried linking against `/usr/local/lib` (where libtensorflow lives) using `-extldflags`, CGO_LDFLAGS, LDFLAGS etc, done `sudo ldconfig` and the env setup with LD_LIBRARY_PATH and LIBRARY_PATH, but cannot move past this step and always get the error status:\r\n\r\n```\r\n/.gvm/gos/go1.8.3/pkg/tool/linux_amd64/link: running gcc failed: exit status 1\r\n/usr/bin/ld: cannot find -ltensorflow\r\n```\r\n\r\nThe only related issue I've encountered for this is: https://stackoverflow.com/questions/44428816/tensorflow-for-go-demo-example-run-failed, but that didn't help much either.\r\n\r\nThat being said, I can dynamically link and build the helloworld go code e.g. `go build hello_tf.go` and `go test github.com/tensorflow/tensorflow/tensorflow/go` all work successfully; the issue arises when I try to statically link and cannot link to libtensorflow after it compiles, no matter what settings I try using.\r\n\r\nAny help or advice would be greatly appreciated. Thanks!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "TensorFlow for Go depends on the `libtensorflow` library for the TensorFlow C API. This is currently built and distributed as a shared library (`libtensorflow.so`) only, not a static library (`libtensorflow.a`). So, fully statically linked binaries are not yet possible from the binaries we distribute with releases.\r\n\r\nOnce https://github.com/bazelbuild/bazel/issues/1920 is resolved, we should be able to build static libraries as part of our release and hopefully that will help.\r\n\r\nTill then, you'd have to either live with dynamic linking or figure out how to build a static `libtensorflow.a` using some other build process (e.g., via the CMake build or something)\r\n\r\nHope that helps."]}, {"number": 15562, "title": "Fix typos", "body": "fix typos", "comments": ["Can one of the admins verify this patch?"]}, {"number": 15561, "title": "_Assert3DImage that adds a control dependency for the shape check.", "body": "I noticed that the `_Check3DImage` was always used in exactly the same way and extracted this into its own convenience function. \r\n\r\nThe only remaining use of `_Check3DImage` was an import in `gen_image_ops.py` saying\r\n\"# TODO(drpng): remove these once internal use has discontinued.\", so maybe it could be removed entirely.", "comments": ["Can one of the admins verify this patch?", "@ngc92 could you rebase and push again?", "I think I screwed up the rebase somewhere. I created a new branch with the same content, now cleaned up under #15926."]}, {"number": 15560, "title": "Branch 179822007", "body": "", "comments": ["cmake rerun http://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/5958/", "cc: @angersson, @ekelsen "]}, {"number": 15559, "title": "Show CCI badge in README", "body": "Fixes #7645.", "comments": []}]