[{"number": 3528, "title": "rnn.bidirectional_dynamic_rnn doesn't really take sequence_length of int32s", "body": "The docstring for rnn.bidirectional_dynamic_rnn indicates that the sequence_length argument can be an int32/int64 vector, but array_ops.reverse_sequence only accepts an int64 vector as its seq_lengths argument.  I'll leave it to you whether the solution is to modify reverse_sequence or just amend the docstring.\n", "comments": ["@ebrevdo would you make the call?\n", "We can also cast up to int64.\n", "Any preference?\n", "Personally, I'm using the same sequence_length tensor in a ctc_ops call down the line which requires int32, so it'd be nice to have those consistent.\n", "They will be consistent; ctc_ops can continue working with the int32\nversion.\n\nUnless you have sequence lengths greater than 2**31-1, the values are the\nsame.\n\nOn Fri, Jul 29, 2016 at 10:22 AM, jonrein notifications@github.com wrote:\n\n> Personally, I'm using the same sequence_length tensor in a ctc_ops call\n> down the line which requires int32, so it'd be nice to have those\n> consistent.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3528#issuecomment-236240405,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim9umYSJxu4mo33sIuBcfz4tj4c7fks5qajbrgaJpZM4JWJ4g\n> .\n", "@ebrevdo  Can I suggest another fix? If sequence_length = None, array_ops.reverse_sequence will not work. I think the patch is trivial: \nif sequence_length is not None:\n  inputs_reverse = array_ops.reverse_sequence(\n        input=inputs, seq_lengths=sequence_length,\n        seq_dim=time_dim, batch_dim=batch_dim)\nelse:\n  dims = [False]*tf.rank(inputs)\n  dims[time_dim] = True\n  inputs_reverse = array_ops.reverse(inputs, dims=dims)\n", "And I agree with @jonrein, it will be easier to have possibility to use int32 too.\n", "This is pretty easy to do by adding a few lines to the reverse sequence op declaration under core/ops/ and instantiating some additional tempaltes in core/kernels/.  Contributions welcome.\n", "in dynamic_rnn, seq_length will first be cast to int32, [dynamic_rnn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L787)\n", "@ebrevdo If this bug haven't been fixed , I want to work on it.\n", "Contributions welcome\n\nOn Oct 8, 2016 6:04 AM, \"\u90ed\u540cjet\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo If this bug haven't been fixed , I\n> want to working on it.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3528#issuecomment-252423558,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim_yl4qBAsQ3KSLt7vYdiKSRVOfL5ks5qx5TsgaJpZM4JWJ4g\n> .\n", "It seems suiyuan2009 fixed it.\n", "I think this issue could be closed as PR #3774 adds the int32 support?", "Thanks!"]}, {"number": 3527, "title": "//tensorflow/python:Sparse_split_op_test  is failing on Big Endian", "body": "### Environment info\n\nOperating System: Ubuntu/Red Hat\n\nInstalled version of CUDA and cuDNN:  Not installed \n\nThe output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. **0.8.0**\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) [ [4b7bc31](https://github.com/tensorflow/tensorflow/commit/4b7bc3174ed67b4a0eb1803537c9d00f132e9ae7)]\n2. The output of `bazel version`  **0.3.0**\n### Steps to reproduce\n1.  Run `bazel test //tensorflow/python:Sparse_split_op_test`\n### Logs or other output that would be helpful\n\nIt was observed that this particular test consists of row-wise and column-wise split of tensor.\nTest is failing on execution of column wise tensor split in case of Big Endian. \nPlease check attached stack-trace. \n\n[sparse_split_test_log.txt](https://github.com/tensorflow/tensorflow/files/385847/sparse_split_test_log.txt)\n\nI checked the test is passing in case of  Little Endian (Ubuntu, Red Hat)\n", "comments": ["@girving this looks like a bug but the code author isn't in the tf org. Would you assign appropriately?\n", "@jparinita Is that the only test that fails?  I'm particularly curious whether any simpler tests exercising TensorShape might fail.\n", "@girving  \n\nThere were many tests failing from //tensorflow/... test suite. \nLooks like  tensorflow is not supporting Big Endian as of now. \nWe have made changes to source code to add big endian support ( changes for const bool kLittleEndian variable) \n\nWith code changes, most of the tests are passing like \n\n```\n//tensorflow/contrib/framework:ops_test \n//tensorflow/core:framework_op_kernel_test          \n//tensorflow/core:util_sparse_sparse_tensor_test \n```\n\nHowever, we still couldn't figure out why Sparse_split_op_test is failing. \n", "@jparinita That you for the fixes so far!  Would you be interested in sending them as a PR so I can see how TensorFlow is currently broken (and make it slightly less broken)?  My best guess is still that it's a bug in TensorShape, but I don't know where.\n\nNote that TensorShape has been heavily optimized to avoid memory allocations, which involved quite a lot of case analysis.  It's possible that some of these cases are now correct in big endian and others are not.\n", "@girving  \n\nCould you please let us know the test \"Sparse_split_op_test\" which is failing can be ignored? Or is it used to test some complex functionality of TensorFlow?\n\nAbout PR for Tensorflow,  I have built TensorFlow version 0.8.0 for big endian and also executed test cases for same version. I made few code changes related to big endian in the source code that I git cloned from tag 0.8.0.\n\nNow, I am trying to build TensorFlow master on big endian and I am facing few build issues. \nIssues like: \n1. has a dependency on protobuf and need changes here.\n2. Facing issue as 'Packet4f' does not name a type \nThis one is related to EIGEN_VECTORIZE_SSE2. \n\nI haven't faced them on v0.8.0 before. \nI am looking into above issues. \nOnce I could fix these issues and able to build master, I will create a PR .\n", "@jparinita `sparse_split_op_test` mostly just tests that one op, but there's a good chance that the failure is in core functionality rather than that op so it's worth debugging.\n", "Removing my assignment since I can't debug this personally.  Still happy to help with advice and code reviews.\n", "@girving \n\nSince TensorFlow v0.9.0 was released, I have built v0.9.0 on big endian platform and currently executing tests.\n`sparse_split_op_test` is failing on v0.9.0 as well.  Additionally, there are few more test failures seen such as \n\n```\n//tensorflow/python:sparse_matmul_op_test\n//tensorflow/python:string_to_hash_bucket_op_test\n//tensorflow/python:conv_ops_test\n//tensorflow/core/kernels:sparse_matmul_op_test_gpu\n//tensorflow/python:determinant_op_test\n```\n\nI am debugging the code to find the root cause but no luck yet. \nAbove mentioned tests seem to be related to ops functionality (`op_def_library.apply_op()`). \nAny suggestions would be helpful. \n", "@jparinita I'd still suggest preparing a pull request of your changes so far.  It's hard for me to help without some idea what kind of changes need to be made.\n", "@girving \nWe have created a PR  #4508 for TensorFlow with big endian changes. \nCould you please have a look? \nAny suggestions would be helpful to fix failing test cases mentioned above . \n", "Cc @aselle for triage, since I'm OOO.\n", "@aselle  Could you please have a look? \n", "I will look at this first thing tomorrow.\n", "I looked at the code you changed, and it seems plausible. I also Could you post the test logs from the failing tests. I checked your code, and it looks like what you are doing is plausible. Obvious ways to check for more areas that need changing is grepping for reinterpret_cast's and uses of unions. Also, you might want to try compiling with -fno-strict-aliasing to see if a you are hitting some strict aliasing problems. \n\nI would like to help more, but I can't reproduce your big endian test, because I have no easy access to a   machine that is big endian. Do you know of an easy environment to build/run big endian code?\n", "More notes,\n\nAlso, in terms of looking at TensorShape which Geoffrey thought might be an issue. It looks like things would mostly be ok. One question is whether your architecture has alignment restrictions for uint16 for uint8 and uint32's. It looks like we aren't doing manipulations as anything but the word size and bytes, which probably is safe. You should insert  \n\n```\nstatic_assert(sizeof(Rep16)==12, \"rep16 not 12 bytes\");\nstatic_assert(sizeof(Rep32)==12, \"rep32 not 12 bytes\");\n```\n\ninto `TensorShape` inside `tensor_shape.h`. \n\nIf you have valgrind, you might try running that to see if any buffers are being run-off.\n", "@aselle \nThank you for your comments.\nWe are working on your suggestions for big endian.\n\nDo you know of an easy environment to build/run big endian code?\n==> Will it be ok if we provide our big endian vm as an extension to TensorFlow CI?\n", "Can you describe the endian VM? How much slower is it than a native system? Is the emulator and software open source code? I'll look into how hard it would be to put that in our testing infrastructure. @gunan and @martinwicke  can also comment.\n", "For now, I think it is possible to hook it up to our test infra for daily tests.\nBut in the near future we will transfer management of our test infra, and in that case we will be quite limited.\n", "@aselle   @gunan \nCan you please provide more details on how the current CI infrastructure is ?\nAlso, I need below details in order to setup the CI infrastructure on z Systems (big endian):\n1. How frequent the CI runs on average?\n2. What is the recommended H/W and configuration requirement?\n3. Special S/W requirements (if any).\n", "@aselle  @gunan Could you please provide details on the current CI infrastructure ?\n", "We have attached test logs for the below failing tests:\n1.  //tensorflow/python:string_to_hash_bucket_op_test \n2. //tensorflow/python:sparse_split_op_test\n\n[string_to_hash_bucket_op_test.log.txt](https://github.com/tensorflow/tensorflow/files/533415/string_to_hash_bucket_op_test.log.txt)\n[sparse_split_op_test.txt](https://github.com/tensorflow/tensorflow/files/533436/sparse_split_op_test.txt)\n\nCould you please have a look at logs?\n", "About our CI:\nLooking back at our future plans, we will be retiring our self maintained CI. Therefore, we ourselves would like to avoid adding more machines/setups to that to avoid making our final move more complicated.\n\nTherefore, we decided against running a big endian machine/VM in our CI. I would be happy to help you if you wanted to setup a CI with your machines for this. And we will be happy to accept any patches you might provide.\n\nCurrently, our CI runs the scripts here:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build\nWhich should work for you as well.\n\nAbout the failure you are seeing, I would defer to someone else with more in-depth information about the ops.\n", "@gunan  We will be glad, if any kind of assistance in maintaining the zSystem VM in the CI by our team can change your decision. \nPlease let us know.\n", "@aselle  We have compiled TensorFlow with -fno-strict-aliasing flag and executed tests however we have observed same failures as before.\nWe have also checked if our architecture has alignment restrictions for uint8,  uint16 and/or uint32's by adding static_asserts as suggested. However, we didn't find such restrictions ( sizeof Rep16 and Rep32 are 12 bytes)\n", "Probably the next thing  I would try is debugging further. So the next step is to look at the failing tests in more detail. You can compile in debug mode both on your architecture and a supported architecture and use a debugger to step through the statements until you find a mismatch.\n", "@aselle Thanks for your comments. \nWe have built TensorFlow using '-dbg' flag on s390x architecture. \nWe are trying to debug the failing test-case using gdb.\n\nIs there any documentation available which we can refer for debugging? \n", "@aselle @girving  \nWe are currently debugging  `//tensorflow/python:string_to_hash_bucket_op_test` on s390x as well as x86 architecture.\nWe could see that the mismatch of values is happening in `def create_op()`from `ops.py` .\nWe could observe that the  op.outputs is populated after  `_add_op(self, op)` is being called:\n\n```\nret = Operation(node_def, self, inputs=inputs, output_types=dtypes,\n                    control_inputs=control_inputs, input_types=input_types,\n                    original_op=self._default_original_op, op_def=op_def)\n    if compute_shapes:\n      set_shapes_for_outputs(ret)\n-------------> **here,  ret.outputs[0].eval() gives NotFoundError**\n    self._add_op(ret)\n-------------> **here,  p ret.outputs[0].eval() gives array([3, 5, 7]) ( expected output is array[4,2,8])**\n\n    self._record_op_seen_by_control_dependencies(ret)\n```\n\nWe tried debugging function _`add_op()` , however couldn't find where the `op.outputs` is getting populated.\nAlso, we could see`self.lock` being used in this function. Is some other thread populating these values?\n", "@Nayana-ibm I don't know offhand, and my next step at this point would just be adding print statements until I did know.  I'm not sure what you mean by `self.lock`: Python graph construction is not thread safe, and is typically single threaded.\n", "@girving We are using pdb to debug test case and tried to print the op.outputs[0].eval() . The mismatch we could observe after _add_op(self, op) is being called as below:\n\n```\nret = Operation(node_def, self, inputs=inputs, output_types=dtypes,\n                    control_inputs=control_inputs, input_types=input_types,\n                    original_op=self._default_original_op, op_def=op_def)\n    if compute_shapes:\n      set_shapes_for_outputs(ret)\n-------------> **here,  ret.outputs[0].eval() gives NotFoundError**\n    self._add_op(ret)\n-------------> **here,  p ret.outputs[0].eval() gives array([3, 5, 7]) ( expected output is array[4,2,8])**\n```\n\nAbout `self.lock`, this is used in the definition of _add_op function( filename: python/framework/ops.py):\n\n```\n\nwith self._lock:\n      # pylint: disable=protected-access\n      if op._id in self._nodes_by_id:\n        raise ValueError(\"cannot add an op with id %d as it already \"\n                         \"exists in the graph\" % op._id)\n      if op.name in self._nodes_by_name:\n        raise ValueError(\"cannot add op with name %s as that name \"\n                         \"is already used\" % op.name)\n      self._nodes_by_id[op._id] = op\n      self._nodes_by_name[op.name] = op\nself._version = max(self._version, op._id)\n```\n\nIn self.lock is defined in def **init**(self) from Graph object as (filename: python/framework/ops.py):\n\n```\nself._lock = threading.Lock()\nself._nodes_by_id = dict() # GUARDED_BY(self._lock)\n```\n", "Ah, maybe it is thread safe! :)  In any case, I would be surprised if the problem was at the Python level, but I don't think I'll be able to help much in debugging this remotely since I can't tinker.\n", "@mrry Can you please have a look at this issue and share your views?", "@Nayana-ibm I agree with @girving: The problem almost certainly isn't in Python (unless there's an endianness bug in the Python runtime itself), but is more likely to be in either the OpKernel for the op that is failing or its C++ shape function. I'd suggest using gdb to step through these functions and see where they go wrong.", "\r\nThis test is now passing on big endian system. Fixed through PR #6686 \r\n", "@girving @gunan \r\nTo extend my post on CI:\r\nWe would like to know that how TensorFlow CI can be extended to support z system. \r\nPlease let us know what support will be required from IBM side. We are aware on the H/w support. \r\nWe can provide s390x machines on which we will setup Jenkins slave node to build TensorFlow. \r\nThe present Jenkins master will manage this node and post the s390x build results to GitHub.\r\n\r\nPlease let us know your comments.", "@Nayana-ibm As far as support goes, I cannot really promise we can fully support ourselves as we do not have big endian machines. So we cannot give first class support for this platform.\r\n\r\nBut I can help setup our jenkins to run all TF tests on your machines periodically. When there are failures, most probably we will escalate the issues to you, if they seem to be specific to your machines, and we will be available to consult resolving issues. You will retain ownership of these machines. I will help you setup all our dependencies on them, and as long as your machine is healthy and up, our jenkins master will schedule tests on your machine.\r\n\r\nHow does this sound?\r\n", "@gunan \r\nThis sounds good to us.\r\n\r\nAs per my understanding,  we will provide you a Jenkins slave node which will be added under your master. Correct? Also, wanted to know how the build results will be shared with us? through GitHub issue? Please let me know. ", "Sorry for the delay, my inbox is going crazy these days.\r\nYes, you will have the machine maintained by you, and I will provide you instructions to setup the machine for tensorflow builds. The build results will be visible through ci.tensorflow.org. You will have access to all build logs through ci.tensorflow.org.\r\nI can even set you up with access to start and stop the specific build for s390x.", "@gunan \r\nThanks for your support.\r\nCould you please let me know what's the recommended H/W configuration required? and \r\n S/W requirements (if any). \r\n", "Sorry for the delay, looks like I wrote up the response but forgot to send.\r\nFor H/W requirements, completely up to you. Obviously, the beefier the machine is, the quicker code is tested. But I wont block code submissions on this so we can tolerate some delays here. \r\n\r\nS/W requirements, you will need all TF dependencies on your machine installed. We usually run all our tests in a docker container, but in this case I dont think we need docker.\r\n\r\nOur jenkins will simply execute:\r\n```\r\ngit checkout <commit>\r\n<set some environment variables>\r\nyes \"\" | ./configure\r\nbazel test tensorflow/...\r\n```\r\n", "Thank you for your inputs.\r\nWe are setting up machine with all TF dependencies.\r\n\r\nI could see Tensorflow supports Ubuntu 14.04 ( Ref: https://www.tensorflow.org/install/install_linux)\r\nhowever we are planning to build the TensorFlow on RHEL and Ubuntu on s390x platform. \r\n", "I just verified that with bazel I can build on CentOS 7 without any issues.\r\nI expect a similar experience with RHEL.\r\n\r\nHowever, we do not have official RedHat support, so we will possibly redirect Redhat specific build issues to you in this build.", "@gunan \r\nWe have procured the s390x vms with Ubuntu 16.04 distribution. \r\nAlso, installed the dependencies like bazel, python-setuptools etc. \r\nPlease let me know the procedure to add those vm's under TensorFlow CI .", "Great!\r\nCould you email me? my github username @ companyname is the address\r\nWe can continue the conversation there.", "@gunan \r\nWill you please email me on nthorat@us.ibm.com.", "Closing this issue as it was originally opened for test case failure.\r\n\r\nAlso, we have completed Tensorflow CI for s390x. \r\n@gunan Thank you for your help and support. \r\n\r\n"]}, {"number": 3526, "title": "Build fails on Ubuntu 16.04 LTS, CUDA Toolkit 8.0, cuDNN 5.0.5, and Bazel 0.3.0-jdk7", "body": "Hi Everyone,\n\nI've downgraded my gcc to 5.3.0 by building from source in order to install CUDA Toolkit 8.0 with cuDNN 5.0.5. I also installed OpenCL freeglut3 and mesa libraries via apt-get. I then built Bazel from source using the installer script. Next, I installed the TensorFlow and Google Cloud Platform Python dependencies. I then cloned the tensorflow GitHub repository and modified the CROSSTOOL file variable cxx_builtin_include_directory to include the gcc location for 5.3.0. I then ran ./configure with default settings and tried to build with Bazel, but it always fails with an error like this, which appears to be a gcc issue:\n\nWARNING: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/protobuf/WORKSPACE:1: Workspace name in /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/protobuf/WORKSPACE (@__main__) does not match the name given in the repository's definition (@protobuf); this will cause a build error in future versions.\n\nERROR: /root/.cache/bazel/_bazel_root/fbc06f9baef46cade6e35d9e4137e37c/external/zlib_archive/BUILD:7:1: undeclared inclusion(s) in rule '@zlib_archive//:zlib'\n\nThis rule is missing dependency declarations for the following files included by 'external/zlib_archive/zlib-1.2.8/inftrees.c':\n  '/usr/local/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed/limits.h'\n  '/usr/local/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include-fixed/syslimits.h'\n  '/usr/local/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/stddef.h'\n  '/usr/local/lib/gcc/x86_64-unknown-linux-gnu/5.3.0/include/stdarg.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n\nIf I change the gcc version to 4.8 (installed via apt-get) in ./configure and revert CROSSTOOL I get many warnings:\n  INFO: ... warning: variable 'parsed_colon' set but not used\n\nThis warning is followed by an error:\nERROR: /opt/tensorflow/tensorflow/core/kernels/BUILD:1527:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc':\n  '/usr/local/cuda-8.0/include/cuda_runtime.h'\n  '/usr/local/cuda-8.0/include/host_config.h'\n  '/usr/local/cuda-8.0/include/builtin_types.h'\n  '/usr/local/cuda-8.0/include/device_types.h'\n  '/usr/local/cuda-8.0/include/host_defines.h'\n  '/usr/local/cuda-8.0/include/driver_types.h'\n  ...\n\nThis time, it appears to be an issue with CUDA Toolkit 8.0. Everything seems to work flawlessly up until building tensorflow from source.\n\nThanks,\n\nAdam\n", "comments": ["Try the one-earlier release of Bazel 0.2.3 or whatever it is.  0.3 never worked for me.  Rolling back twice worked for me on Ubuntu 16 and 14, with same exact CUD\\* versions.  Make sure bazel 0.3 is uninstalled, obviously.\n", "I worked around similar issue by adding `cxx_builtin_include_directory`\nhttps://github.com/tensorflow/tensorflow/issues/3431\n", "@adam-erickson please update if the above comments don't help.\n", "It looks like the solution of @yaroslavvb should work for the install with gcc 4.8.\n\nSince I'm not running Pascal architecture GPUs, but rather a node with four GeForce GTX Titan X GPUs, I ended up installing the latest CUDA 367.35 display drivers from ppa (the display drivers included with CUDA Toolkit 7.5 cause `nvidia-smi` to freeze on Ubuntu 16.04), CUDA Toolkit 7.5 from Ubuntu 16.04 LTS package management, and cuDNN 5.0.5 from the Nvidia site. I then built and ran the samples from source. One function appears to error in tests, but maybe that's because the samples are intended for Ubuntu 15.04 with cuDNN 4. I'm happily back to only the standard gcc now. TensorFlow is functioning well with the standard Python distribution. Here was my full process, after removing previous installations:\n\n**Recommended: Install OpenCL libraries**\nUpdate list:\n    `apt-get update`\nInstall OpenCL libraries:\n    `apt-get install mesa-common-dev freeglut3-dev`\n    `apt-get install libxmu-dev libxi-dev`\n\n**Install CUDA Toolkit 7.5 and 367.xx display driver from Ubuntu 16.04 apt-get**\nInstall Python dependencies:\n    `apt-get install python-pip python-dev`\nRemove existing CUDA installation:\n    `apt-get purge nvidia-*`\nInstall CUDA display driver 367.35:\n    `add-apt-repository ppa:graphics-drivers/ppa`\n    `apt-get update`\n    `apt-get install nvidia-367`\n    `reboot`\nInstall CUDA Toolkit 7.5\n    `apt-get install nvidia-cuda-toolkit`\n    `apt-get install nvidia-nsight`\n    `apt-get install nvidia-profiler`\n    `apt-get install libcupti-dev zliblg-dev`\nLink files:\n    `mkdir /usr/local/cuda`\n    `ln -s /usr/lib/x86_64-linux-gnu/ lib64`\n    `ln -s /usr/include/ include`\n    `ln -s /usr/bin/ bin`\n    `ln -s /usr/lib/x86_64-linux-gnu/ nvvm`\n    `mkdir -p extras/CUPTI`\n    `cd extras/CUPTI`\n    `ln -s /usr/lib/x86_64-linux-gnu/ lib64`\n    `ln -s /usr/include/ include`\n\n**Install cuDNN 5.0.5**\n    `cd /opt`\n    Download cuDNN 5.0.5 from: [(https://developer.nvidia.com/rdp/cudnn-download)]\n    `tar xvf cudnn-8.0-linux-x64-v5.0-ga.tar`\n    Add to `~/.bashrc`: `export LD_LIBRARY_PATH=/opt/cuda:$LD_LIBRARY_PATH`\nCopy cudnn files to the default CUDA directories and set permissions:\n    `cp cuda/include/cudnn.h /usr/local/cuda/include/`\n    `cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/`\n    `chmod a+r /usr/local/cuda/include/cudnn.h`\n    `chmod a+r /usr/local/cuda/lib64/libcudnn*`\nDownload and run the CUDA Toolkit 7.5 samples:\n    Get the full run file here: [https://developer.nvidia.com/cuda-toolkit]\nExtract to path:\n    `mkdir /opt/cuda/cudatoolkit`\n    `sh cuda_7.5.xxx_linux_64.run -extract=/opt/cuda/cudatoolkit`\n    `cd /opt/cuda/cudatoolkit`\nInstall only the samples to /opt/cuda/samples:\n        `sh cuda-samples-linux-7.5.xx-xxxx.run`\n    `cd ..`\n    `rm -rf cudatoolkit`\nRun the Device Query tool:\n    `cd samples/1_Utilities/deviceQuery`\n    `make`\n    `./deviceQuery`\nRun the bandwidth test:\n    `cd /opt/cuda/samples/1_Utilities/bandwidthTest`\n    `make`\n    `./bandwidthTest`\nRun the n-body sample with nvprof:\n    `cd ../..`\n    `cd 5_Simulations/nbody`\n    `make`\n    `nvprof ./nbody -benchmark -numdevices=4`\n    `nvprof --print-gpu-trace ./nbody -benchmark -numdevices=4`\n\n**Install TensorFlow from binary for CUDA Toolkit 7.5**\n-Set new variable and install:\n    `cd /opt`\n    `export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl`\n    `pip install --upgrade $TF_BINARY_URL`\n", "Good to hear it! I'm closing the issue but please reopen if there is something that needs to be addressed.\n"]}, {"number": 3525, "title": "core dump when initializing gpu", "body": "I have the same problem with [#2620](https://github.com/tensorflow/tensorflow/issues/2620), like this:\n\n``` shell\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: Tesla K20c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.7055\npciBusID 0000:02:00.0\nTotal memory: 4.69GiB\nFree memory: 4.54GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x9568510\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: Tesla K20c\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.7055\npciBusID 0000:04:00.0\nTotal memory: 4.69GiB\nFree memory: 4.61GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x99b29a0\nSegmentation fault (core dumped)\n```\n\nIn fact, gpu 2 is busy:\n\n``` shell\n+------------------------------------------------------+                       \n| NVIDIA-SMI 352.79     Driver Version: 352.79         |                       \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K20c          Off  | 0000:02:00.0     Off |                    0 |\n| 30%   42C    P0    48W / 225W |     80MiB /  4799MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K20c          Off  | 0000:04:00.0     Off |                    0 |\n| 30%   24C    P8    15W / 225W |     14MiB /  4799MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K20c          Off  | 0000:83:00.0     Off |                    0 |\n| 46%   60C    P0   111W / 225W |   4768MiB /  4799MiB |     99%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K20c          Off  | 0000:84:00.0     Off |                    0 |\n| 40%   53C    P0   100W / 225W |    648MiB /  4799MiB |     97%      Default |\n+-------------------------------+----------------------+----------------------+\n```\n\n I only want to use gpu 1, but tensorflow want to init all gpus.\n", "comments": ["Do you think this is the same as #152? \n", "@michaelisard I am not sure, maybe it's the same problem. \nIn my context, all memory of gpu 2 has been used, when I run tensorflow program, specifically,  the calling of tf.Session.run(), it try to check all gpus on my machine, but the fact is that 4768MiB /  4799MiB  memory of gpu 2 has been used, the memory is not enough to run other's thread. \n\nIn the #152, the output of nvidia-smi says that there is enough memory.\n", "@michaelisard  By the way,  CUDA_VISIBLE_DEVICES can solve my problem\n", "Great! I am closing the issue but please reopen if there's something else to look at.\n"]}, {"number": 3524, "title": "Undefined symbols for architecture x86_64: _deflate (zlib)", "body": "Hi!\n\nI was trying to reproduce one of my previous work on tensorflow on IOS when I stumble upon this error:\n\n```\nUndefined symbols for architecture x86_64:\n  \"_deflate\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::Deflate(int) in libtensorflow-core.a(zlib_outputbuffer.o)\n  \"_deflateEnd\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::Close() in libtensorflow-core.a(zlib_outputbuffer.o)\n  \"_deflateInit2_\", referenced from:\n      tensorflow::io::ZlibOutputBuffer::ZlibOutputBuffer(tensorflow::WritableFile*, int, int, tensorflow::io::ZlibCompressionOptions const&) in libtensorflow-core.a(zlib_outputbuffer.o)\n  \"_inflate\", referenced from:\n      tensorflow::io::ZlibInputBuffer::Inflate() in libtensorflow-core.a(zlib_inputbuffer.o)\n  \"_inflateEnd\", referenced from:\n      tensorflow::io::ZlibInputBuffer::~ZlibInputBuffer() in libtensorflow-core.a(zlib_inputbuffer.o)\n      tensorflow::io::ZlibInputBuffer::~ZlibInputBuffer() in libtensorflow-core.a(zlib_inputbuffer.o)\n  \"_inflateInit2_\", referenced from:\n      tensorflow::io::ZlibInputBuffer::ZlibInputBuffer(tensorflow::RandomAccessFile*, unsigned long, unsigned long, tensorflow::io::ZlibCompressionOptions const&) in libtensorflow-core.a(zlib_inputbuffer.o)\n```\n\nStep i did:\n- I cloned tensorflow at 10am 27/07/2016, paris timezone\n- I ran gen_files_list.sh\n- I compile tensorflow on IOS using the compile_all_ios.sh script on my project: \u274c\n- I git reset --hard HEAD\n- I compile tensorflow on IOS using the compile_all_ios.sh script on my project: \u274c\n\nIt works on the simple IOS examples.\nDo you have any hint on this one ?\n\nOther info:\n- Mac OSX v 10.11.5\n- Tensorflow on master branch\n", "comments": ["I suspect the makefile doesn't properly add -lz. @petewarden would know.\n", "Some more information:\nIn the `proto_text_cc_files.txt`, i've compilled tensorflow **with** and **without** those lines:\n\n```\ntensorflow/core/lib/io/zlib_outputbuffer.cc\ntensorflow/core/lib/io/zlib_inputbuffer.cc\n```\n\nNothing changed.\n", "I believe I've fixed this now, with the addition of zlib to the linking stage of the OS X part of the iOS build. Closing for now, but please reopen if you're still hitting issues.\n", "I have the same issue with the latest code, my env is mac os 10.12, Xcode 8.3\r\n\r\n@petewarden ,would you offer any help? thx\r\n\r\n`[Undefined symbols for architecture x86_64:\r\n  \"_deflate\", referenced from:\r\n      tensorflow::io::ZlibOutputBuffer::Deflate(int) in zlib_outputbuffer.o\r\n  \"_deflateEnd\", referenced from:\r\n      tensorflow::io::ZlibOutputBuffer::Close() in zlib_outputbuffer.o\r\n  \"_deflateInit2_\", referenced from:\r\n      tensorflow::io::ZlibOutputBuffer::Init() in zlib_outputbuffer.o\r\n  \"_inflate\", referenced from:\r\n      tensorflow::io::ZlibInputStream::Inflate() in zlib_inputstream.o\r\n  \"_inflateEnd\", referenced from:\r\n      tensorflow::io::ZlibInputStream::~ZlibInputStream() in zlib_inputstream.o\r\n  \"_inflateInit2_\", referenced from:\r\n      tensorflow::io::ZlibInputStream::InitZlibBuffer() in zlib_inputstream.o\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [/Users/bruce/Downloads/tensorflow-master/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\n+ '[' 2 -ne 0 ']'\r\n+ echo 'armv7 compilation failed.'\r\narmv7 compilation failed.\r\n+ exit 1](url)`"]}, {"number": 3523, "title": "Do you really guys calculate the gates values in LSTM?", "body": "Hello guys, I was checking at the code in the classes BasicLSTMCell  as well as LSTMCell and I don't see you use formulas to get the values of the gates (output, input and forget) is this procedure you guys use really valid (since gates have their own calculation)?\n\nin the _linear method (which you use) I just see matmul, concat and some functions from variable_scope.py However if you did it in another way, I would like to know it.\n", "comments": ["I think this question is better suited to [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) unless you determine that there's a specific bug that needs to be addressed.\n"]}, {"number": 3522, "title": "fix for BM_UNARY test", "body": "here is a fix for the test. Can you kindly review @rryan ?\n", "comments": ["Can one of the admins verify this patch?\n", "The code LGTM in its current state. I'm not sure if I have permission to verify the patch for Jenkins but here goes:\n\n@tensorflow-jenkins test this please\n", "thanks again! the tests now run on the gpu on the mac! I also fixed an issue with the function names of the benchmark... now it shows them:\n\n```\nBenchmark                                                   Time(ns) Iterations\n-------------------------------------------------------------------------------\nBM_cpu_Floor_DT_FLOAT/4096                                     26842      27131  610.4MB/s 152.6M items/s\nBM_cpu_Floor_DT_FLOAT/32768                                    95100       7409  1378.3MB/s 344.6M items/s\nBM_cpu_Floor_DT_FLOAT/262144                                  259694       2739  4037.7MB/s 1009.4M items/s\nBM_cpu_Floor_DT_FLOAT/1048576                                 847080        700  4951.5MB/s 1237.9M items/s\n...\n```\n", "> thanks again! the tests now run on the gpu on the mac! I also fixed an issue with the function names of the benchmark... now it shows them:\n\nThanks!\n", "Looks like @tensorflow-jenkins doesn't like me :) @vrv could you trigger a build please?\n", "Thanks @rryan and @kashif! @tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3521, "title": "Clarify docs for tensorflow/python/ops/rnn_cell.py", "body": "What the docs and code refer to as a \"cell\" corresponds to what the literature\ncalls a layer of cells/units. This clarifies that e.g. RNNCell is an array of\ncells/units.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Sorry, used my personal email address the first time around.  Fixed.\n"]}, {"number": 3520, "title": "verify_tensor_all_finite does not handle list of tensors of different shape", "body": "`tf.verify_tensor_all_finite` works on lists of tensors that have the same shape.\n\nbut it does not seem to work on lists of tensors that have different shapes.\n### Environment info\n\nOperating System: Mac OSX 10.10.5\n\nInstalled version of CUDA and cuDNN: None\n\nIf installed from binary pip package, provide: 0.9.0rc0\n### Steps to reproduce\n\n``` Python\nimport tensorflow as tf\n\nsess = tf.Session()\n\nx0 = tf.Variable(tf.random_normal([1], dtype=tf.float32))\nx1 = tf.Variable(tf.random_normal([1], dtype=tf.float32))\nx2 = tf.Variable(tf.random_normal([3], dtype=tf.float32))\n\nsess.run(tf.initialize_all_variables())\n\n# works\nassert_ops = [tf.verify_tensor_all_finite(x1, msg='')]\nwith tf.control_dependencies(assert_ops):\n    y = tf.mul(x0, tf.mul(x1, tf.reduce_sum(x2)))\nprint(y.eval(session=sess))\n\n# works\nassert_ops = [tf.verify_tensor_all_finite(x1, msg=''),\n              tf.verify_tensor_all_finite(x2, msg='')]\nwith tf.control_dependencies(assert_ops):\n    y = tf.mul(x0, tf.mul(x1, tf.reduce_sum(x2)))\nprint(y.eval(session=sess))\n\n# works (x0 and x1 have same shape)\nassert_ops = [tf.verify_tensor_all_finite([x0, x1], msg='')]\nwith tf.control_dependencies(assert_ops):\n    y = tf.mul(x0, tf.mul(x1, tf.reduce_sum(x2)))\nprint(y.eval(session=sess))\n\n# fails (x0 and x2 have different shapes)\nassert_ops = [tf.verify_tensor_all_finite([x0, x2], msg='')]\nwith tf.control_dependencies(assert_ops):\n    y = tf.mul(x0, tf.mul(x1, tf.reduce_sum(x2)))\nprint(y.eval(session=sess))\n```\n", "comments": ["`tf.verify_tensor_all_finite()` actually expects a _single_ tensor. If you happen to pass in a list of identically-sized N-dimensional tensors, TensorFlow will pack them into a single N+1-dimensional tensor and pass that to the op.\n", "oh i see. ok, makes sense. thanks @mrry. \n"]}, {"number": 3519, "title": "Suggestions for simplifying directory structure", "body": "(version/machine agnostic)\n\nI just want to say that after using TF for a while, I still have to fire too many brain cells to remember which directory certain files are in.  \n\nThe shortest way I can say this is that I suggest placing **all** instructional/non-source files in the same directory, _probably at the top_ of tensorflow so  visitors can easily find them.\n\nRight now there are:\n- `tensorflow[org]/models` (separate repo containing pre-trained models)\n- `tensorflow[org]/tensorflow/tensorflow/examples/`\n  -`tensorflow[org]/tensorflow/tensorflow/models`\n- `tensorflow[org]/tensorflow/tensorflow/tutorials` (these are Tensorflow website tutorials)\n- `tensorflow[org]/tensorflow/tensorflow/examples/how_tos`\n\nHopefully I am not the only one that sometimes finds this confusing, probably because of the same words being used repeatedly to describe different things.\n\nSuggestion 1 is to place the tensorflow/[examples & models] directories into the root of `tensorflow[org]/tensorflow` and renaming that directory to examples_tutorials, to make where to look obvious for those not digging through the source (which is probably a majority of visitors).  I think that single, simple change would simplify this directory structure quite a bit.\n\nI have a couple of additional suggestions for renaming the tensorflow/tensorflow/models directory (note not the pre-trained repo).\n1. Rename the separate tensorflow/models **repo** (that contains code pertaining to pre-trained models) to `pre-trained-models` or `pre-trained-weights` and rename the `tensorflow/examples/models` **directory** to something that conveys the fact that it contains research paper models and not pre-trained ones.\n2. Rename `models/embedding` to `word2vec` or something else more descriptive\n\nHappy to submit a PR or two if the team wants to make these changes.\n", "comments": ["@martinwicke would you like to comment or reassign?\n", "This is a good thing to do, but it isn't particularly high on our list of priorities. Note that there are not only pre-trained models in the models repo, so I don't think it should be renamed. \n\nI agree with your suggestion to have a single top-level examples directory. Honestly, I would just have a tensorflow/examples directory, for instance like this:\n\ntensorflow/examples/mnist\ntensorflow/examples/word2vec (although embedding is actually more descriptive)\n...\n\nI don't see a lot of value distinguishing between tutorials and howtos and we'll likely remove that distinction on the website soon.\n\nThis is a perfectly valid feature request, and mostly what would have to happen is to move the code around and fix the links in the documentation and the build files. A PR would be greatly appreciated. \n", "Thanks for the feedback @JohnAllen -- we are reorganizing sample models, and converging on the [Official Models](https://github.com/tensorflow/models/tree/master/official) as the place to house reference implementations. Closing this, as that effort is underway."]}, {"number": 3518, "title": "Update version string to 0.10.0rc0", "body": "", "comments": ["LGTM\n"]}, {"number": 3517, "title": "Error when running distributed MNIST example", "body": "Hi I am new to tensorflow and distributed tensorflow. I was trying to run an example from #2726 \n\nRight now I am using tensorflow 0.9 on a cluster of Raspberry Pi3, using slurm to manage the nodes in the cluster, the script is below:\n# !/bin/bash\n# SBATCH -N 4\n# SBATCH --nodelist=piw[25-28]\n\nnode1=piw25\nnode2=piw26\nnode3=piw27\nnode4=piw28\n# On node1:\n\nsrun -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=ps --task_index=0 &\n# On node2:\n\nsrun -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=worker --task_index=0 &\n# On node3:\n\nsrun -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=worker --task_index=1 &\n# On node4:\n\nsrun -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=worker --task_index=2\nwait\n\nAnd this is the error message:\n\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, piw27:2223, localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, piw27:2223, piw28:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, piw27:2223, piw28:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, localhost:2223, piw28:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nTraceback (most recent call last):\n  File \"mnist_yetanother.py\", line 351, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"mnist_yetanother.py\", line 310, in main\n    with sv.prepare_or_wait_for_session(server.target, config=None) as sess:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 684, in prepare_or_wait_for_session\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 176, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: /job:worker/replica:0/task:0/cpu:0 unknown device.\n     [[Node: truncated_normal_2_S5 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device_incarnation=-8305312558883263444, tensor_name=\"edge_62_truncated_normal_2\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]]\nsrun: error: piw28: task 0: Exited with exit code 1\nE0726 15:08:58.966743325   10814 tcp_client_posix.c:173]     failed to connect to 'ipv4:192.168.50.38:2223': socket error: connection refused\nE tensorflow/core/distributed_runtime/master.cc:202] Master init: Unavailable: \nE0726 15:09:04.384147901   10819 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:09:09.463843371   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:09:18.410973793   10820 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:09:29.903416857   10814 tcp_client_posix.c:173]     failed to connect to 'ipv4:192.168.50.38:2223': socket error: connection refused\nE tensorflow/core/distributed_runtime/master.cc:202] Master init: Unavailable: \nE0726 15:10:00.977546704   10814 tcp_client_posix.c:173]     failed to connect to 'ipv4:192.168.50.38:2223': socket error: connection refused\nE0726 15:10:47.956032631   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:11:48.090654174   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:13:21.260355773   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:15:21.261626939   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:17:21.263101653   10819 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:19:21.264292261   10814 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:21:21.265185108   10820 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\nE0726 15:23:21.265939820   10801 tcp_client_posix.c:191]     failed to connect to 'ipv4:192.168.50.38:2223': timeout occurred\n\nI killed the program after a while. I'm not sure why I would get \"[Node: truncated_normal_2_S5 = _Recv[client_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device_incarnation=-8305312558883263444, tensor_name=\"edge_62_truncated_normal_2\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]()]\" such error message. Searching online does not help too much...\n\nAny help or advice will be greatly appreciated!\n", "comments": ["I think the relevant part of the message is 'Error: /job:worker/replica:0/task:0/cpu:0 unknown device.' Could you include mnist_yetanother.py as well to make sure you are setting up the [cluster spec](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/distributed/index.md) correctly?\n", "Hi @michaelisard thank you for your reply, I used exactly the same code as in #2726, so the setup of cluster spec remains the same. Although I did try to hard code the assignment of the node into the code, still the same error.\n", "Looking at your log output (and slightly unjumbling it), it appears that the ClusterSpec is appropriately configured:\n\n```\n# Presumably worker 2:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, piw27:2223, localhost:2223}\n# Presumably the PS:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, piw27:2223, piw28:2223}\n# [...]\n# Presumably worker 0:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {localhost:2223, piw27:2223, piw28:2223}\n# Presumably worker 1:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {piw25:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {piw26:2223, localhost:2223, piw28:2223}\n```\n\nHowever, the error you are seeing appears to be a result of misconfiguration: one worker is making a request to the process that it _believes_ is worker 0, but the receiving process does not have the appropriate device (and so presumbly is actually a _different_ worker).\n\nHow is your DNS configured for the cluster? Is it possible that one of the hosts has a different mapping for `piw26` than the others (or than the cluster manager)?\n", "Hi @mrry the cluster is set up to use /etc/hosts for hostname resolution within the cluster. We switched to use the IP addresses of the nodes and we ran into the same error. I'm skeptical if the error message is caused by \n\n> tensorflow.python.framework.errors.InvalidArgumentError: /job:worker/replica:0/task:0/cpu:0 unknown device.\n> [[Node: truncated_normal_2_S5 = _Recvclient_terminated=false, recv_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device_incarnation=-8305312558883263444, tensor_name=\"edge_62_truncated_normal_2\", tensor_type=DT_FLOAT, _device=\"/job:ps/replica:0/task:0/cpu:0\"]]\n\nSomething related to truncated_normal as the input argument?\n\nThank you for your reply!\n", "Do you have the ability to recompile from source? I can think of a small patch that would make it easier to diagnose the problem.\n\n(I don't think it's related to the particular use of `truncated_normal`, and it still seems like an address misconfiguration to me, although using IP addresses rules out DNS misconfiguration.)\n", "@mrry inspired us to check if the tasks are running on the designated nodes. We now changed the way of assigning the nodes, and now it's working! thanks a lot!!!\n\nThe script is shown below:\n#!/bin/bash\n\n#SBATCH -N 4\n#SBATCH --nodelist=piw[25-28]\n\nnode1=piw25\nnode2=piw26\nnode3=piw27\nnode4=piw28\n\n#On node1:\nsrun --nodelist=piw25 -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=ps --task_index=0 &\n#On node2:\nsrun --nodelist=piw26 -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=worker --task_index=0 &\n#On node3:\nsrun --nodelist=piw27 -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=worker --task_index=1 &\n#On node4:\nsrun --nodelist=piw28 -N 1 -n 1 python mnist_yetanother.py \\\n     --ps_hosts=$node1:2223 \\\n     --worker_hosts=$node2:2223,$node3:2223,$node4:2223 \\\n     --job_name=worker --task_index=2\nwait\n", "Glad to hear it! So - just for my understanding - was the problem that Slurm was assigning some tasks to the same nodes? (or assigning them in a non-deterministic order?)\n", "I don't think this is caused by slurm assigning tasks to the same nodes. As long as the tasks are specified to each node specifically they can run correctly. I have run into the situation where some tasks assigned to the same node, I will try to post that error message later, different from what I post here.\n", "When I accidentally tried to assign several tasks to the same node, I got the following error message:\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {192.168.50.35:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {192.168.50.36:2223, 192.168.50.37:2223, localhost:2223}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2223\nE0727 11:38:35.762428930   22857 tcp_server_posix.c:284]     bind addr=[::]:2223: Address already in use\nsrun: error: piw25: task 0: Segmentation fault\nE0727 11:38:35.762761220   22857 server_chttp2.c:119]        No address added out of total 1 resolved\nE0727 11:38:37.724595086   21952 tcp_server_posix.c:284]     bind addr=[::]:2223: Address already in use\nsrun: error: piw27: task 0: Segmentation fault\nE0727 11:38:37.725087528   21952 server_chttp2.c:119]        No address added out of total 1 resolved\nE0727 11:38:37.822318646   23426 tcp_server_posix.c:284]     bind addr=[::]:2223: Address already in use\nE0727 11:38:37.822415468   23426 server_chttp2.c:119]        No address added out of total 1 resolved\nsrun: error: piw26: task 0: Segmentation fault\nslurmstepd: error: **\\* STEP 626.1 CANCELLED AT 2016-07-27T11:40:22 **\\* on piw28\nsrun: got SIGCONT\nslurmstepd: error: **\\* JOB 626 CANCELLED AT 2016-07-27T11:40:22 **\\* on piw25\nsrun: Job step aborted: Waiting up to 2 seconds for job step to finish.\n\nSo it told me the address that I tried to access is already in use.\n"]}, {"number": 3516, "title": "Fix an error message.", "body": "Minimum tensor rank should be `dim + 1` instead of `dim`.\n\nExample:\n\n``` python\ntf.argmin(x, 1)  # x is a 1-d vector.\n```\n\nbefore:\n\n> InvalidArgumentError: Minimum tensor rank: 1 but got: 1\n\nafter:\n\n> InvalidArgumentError: Minimum tensor rank: 2 but got: 1\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3515, "title": "Language Modeling: Test Sentence Probability", "body": "I have trained and saved an RNN model using the tutorial given [here in the Tensorflow docs](https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html). I want to evaluate this model by finding **log probability of each test sentence**. I would really appreciate if someone can help me out with this.\nThanks!\n", "comments": ["Hi, this is a question more appropriate to StackOverflow.\n"]}, {"number": 3514, "title": "Explain how to make it work with ipython.", "body": "It is not obvious that although ipython is available in the\ntensorflow environment, it does not know about tensorflow until\nipython gets install into the tensorflow environment that was\njust created. Same goes for other packages that have already been\ninstalled but not into the tensorflow environment. Fixes #3321 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Just signed the CLA\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Thanks @johann-petrak !\n"]}, {"number": 3513, "title": "Added useful ops to makefile build", "body": "These are common ops needed by some of our users, so adding them to the mobile build.\n", "comments": []}, {"number": 3512, "title": "Branch 128492931", "body": "", "comments": ["@tensorflow-jenkins test this please\n"]}, {"number": 3511, "title": "Branch 128485842", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n"]}, {"number": 3510, "title": "`tf.reduce_min/max/mean` returns uninitialized value on tensors with zero-size dimension", "body": "I find that `tf.reduce_min/max/mean` returns uninitialized value (`nan` for example) on tensors with a zero-size dimension. This would cause problems, for example, when computing loss in an object detection problem, where an input image may contain no object. Please consider defining its behaviour on such inputs, e.g. outputing zeros, raising errors, etc.\n### Environment info\n- Operating System: Ubuntu 14.04 LTS\n- CUDA 7.5, cuDNN 4.0\n- TF version: 0.9.0rc0, pip version (gpu, python 2.7)\n### Steps to reproduce\n\n```\nimport tensorflow as tf\nwith tf.Session():\n  print(tf.reduce_max(tf.constant(-1.0, shape=[0, 10])).eval())\n```\n\nOutput:\n\n```\n-3.40282e+38\n```\n\nAlso works for `tf.reduce_min`, `tf.reduce_mean`, but not for `tf.reduce_sum`.\n", "comments": ["This is the expected behavior. When a reduction is performed over a tensor with zero elements and the result does not have zero elements, the result is filled with the reducer's identity function. For max that is 'lowest' for the type in question, which is what you should be seeing. (When I try this I see -3.4e+38 for max, and +3.4e+38 for min since min's identity is highest.)\n\nPlease reopen if you aren't seeing this behavior.\n", "@michaelisard I see. Thanks for explaining!\n"]}, {"number": 3509, "title": "Check failed,  cuCtxSetCurrent(context). training cifar10 multi-gpu example on 2 gpus.", "body": "I'm having trouble getting the multiple GPU example to work. Here is the full output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                            \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                             \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                                                                \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                                                               \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                                                               \n>> Downloading cifar-10-binary.tar.gz 100.0%                                                                                                                                            \nSuccessfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\n```\n### Environment info\n\nOperating System: CentOS release 6.7 (Final). Using a docker image available here: [gideonitemd/hal-tf](https://hub.docker.com/r/gideonitemd/hal-tf/).\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\ncudnn/7.0/lib64/libcudnn.so\ncudnn/7.0/lib64/libcudnn.so.7.0.64  \ncudnn/7.0/lib64/libcudnn.so.7.0\ncudnn/7.0/lib64/libcudnn_static.a  \n\n/usr/lib/libcuda.so \n/usr/lib/libcuda.so.1 \n/usr/lib/libcuda.so.352.39\n\n(should be libcuda v7.5)\n\nRunning `python -c \"import tensorflow; print(tensorflow.__version__)\"` yields 0.8.0\n### Steps to reproduce\n\nRun this (the output of the `docker_run_gpu.sh` script):\n\n```\ndocker run -it -v /usr/lib64/libcuda.so:/usr/lib64/libcuda.so -v /usr/lib64/libcuda.so.1:/usr/lib64/libcuda.so.1 -v /usr/lib64/libcuda.so.352.39:/usr/lib64/libcuda.so.352.39 -v /lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko:/lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko -v /lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin,:/lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin, --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm --env LD_LIBRARY_PATH=/cbio/shared/software/cudnn/7.0/lib64:/usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib:/usr/local/cuda-7.5/targets/x86_64-linux/lib/:/opt/mpich2/gcc/eth/lib:/opt/gnu/gcc/4.8.1/lib64:/opt/gnu/gcc/4.8.1/lib:/opt/gnu/gmp/lib:/opt/gnu/mpc/lib:/opt/gnu/mpfr/lib:/usr/lib64/ --env CUDA_VISIBLE_DEVICES=0,1 -v /cbio/ski/fuchs/projects/tissue-microarray-resnet/:/mnt/data -v /cbio/ski/fuchs/home/dresdnerg/software/tensorflow/tensorflow/models/image/cifar10:/mnt/code -it gideonitemd/tmrn python cifar10_multi_gpu_train.py --num_gpus 2\n```\n### What have you tried?\n1. Messing with `CUDA_VISIBLE_DEVICES`: empty, `=0`, `=0,1`. All failed.\n2. Looking at other issues with the same error, none address this specific problem since it works fine on a single GPU.\n3. `nvidia-smi` shows no other jobs accessing the GPU\n4. `docker ps` shows no zombie images\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Do any of the steps [here](http://stackoverflow.com/questions/34866455/unable-to-get-cuda-to-work-in-tensorflow) help? (StackOverflow is a better venue for this type of question.)\n", "Not really, but [this](http://stackoverflow.com/questions/35805595/tensorflow-failed-to-run-on-k20m-and-k40m-gpus) did!\n\nThe StackOverflow thread says that this is a bug in TF but in fact I simply did not have to all of the GPUs on the machine. It works fine if I do.\n\nMore generally, I suppose the issue is error messages. Would have saved me a lot of time if TF let me know that it doesn't have permissions to access a GPU but obviously I don't know enough about the internal workings of the framework to know how feasible this is at this point in time.\n"]}, {"number": 3508, "title": "Training on a GTX 1080 does not work, produces random labels", "body": "Hi! \nI have come across a very very strange issue. Namely, training on an NVIDIA GTX 1080 does not work at all, and judging from the error rate, the predicted labels are completely random.\n\nI have 2 almost identical systems (see below), and while on the System with the GTX 960 the training runs perfectly fine, on the system with the GTX 1080, the training simply doesn't work.\n\nTo test this, I ran\nthe following code: \n\n`python -m tensorflow.models.image.mnist.convolutional`\n\nOn the System 1 (GTX 960), i get to an error rate below 4% within 2-3 batches, and at the end, it's below 1%:\n\n```\nStep 0 (epoch 0.00), 7.6 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 13.0 ms\nMinibatch loss: 3.296, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.3%\nStep 200 (epoch 0.23), 13.1 ms\nMinibatch loss: 3.459, learning rate: 0.010000\nMinibatch error: 12.5%\nValidation error: 3.9%\n...\nStep 8500 (epoch 9.89), 13.0 ms\nMinibatch loss: 1.604, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.9%\nTest error: 0.8%\n```\n\nOn the GTX 1080 system, the performance simply _never_ improves! Error rate is steady at around 90%.\n\n```\nStep 8400 (epoch 9.77), 5.5 ms\nMinibatch loss: 3.881, learning rate: 0.006302\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 8500 (epoch 9.89), 5.5 ms\nMinibatch loss: 3.877, learning rate: 0.006302\nMinibatch error: 87.5%\nValidation error: 88.7%\nTest error: 89.7%\n```\n\nI tested this with TensorFlow 0.9, from the release PIP package for Python 3.5 with GPU support.\nI also tested this with TensorFlow master from a week ago (fc9162975e52978d3af38549b570cc3cc5f0ab66), compiled to a PIP package on one machine, installed on both machines.\n\nHere are the full system Specs, but the difference between them is only the GPU (1080 vs 960) and the Driver (367.35 vs 361.42)\n\nSystem 1\n- OS: Ubuntu 16.04.1 LTS\n- Kernel: 4.4.0-31-generic\n- NVIDIA Driver Version: 361.42 \n- CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\n- RAM: 12 GB Ram\n- GPU: NVIDIA GTX 960, 4GB VRAM (edition: MSI GTX 960 Gaming 4G)\n\nSystem 2:\n- OS: Ubuntu 16.04.1 LTS\n- Kernel: 4.4.0-31-generic\n- NVIDIA Driver Version: 367.35\n- CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\n- RAM: 12 GB Ram\n- GPU: NVIDIA GTX 1080, 8GB VRAM\n\nMy LD_LIBRARY_PATH on both machines is:\n`:/usr/local/cuda/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64/`\n\nMy Cuda version is 7.5, cudnn is 4.0.7 on BOTH machines. \n\nOutput of `ls -l /usr/local/cuda/lib64` on the Machine with the GTX 960 and GTX 1080\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx960-txt\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx1080-txt\n\nDoes anyone know what could cause this and how to fix this?\n", "comments": ["Ooops, double-issue. Please disregard.\n"]}, {"number": 3507, "title": "Training on a GTX 1080 does not work, produces random labels", "body": "Hi! \nI have come across a very very strange issue. Namely, training on an NVIDIA GTX 1080 does not work at all, and judging from the error rate, the predicted labels are completely random.\n\nI have 2 almost identical systems (see below), and while on the System with the GTX 960 the training runs perfectly fine, on the system with the GTX 1080, the training simply doesn't work.\n\nTo test this, I ran\nthe following code: \n\n`python -m tensorflow.models.image.mnist.convolutional`\n\nOn the System 1 (GTX 960), i get to an error rate below 4% within 2-3 batches, and at the end, it's below 1%:\n\n```\nStep 0 (epoch 0.00), 7.6 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 13.0 ms\nMinibatch loss: 3.296, learning rate: 0.010000\nMinibatch error: 4.7%\nValidation error: 7.3%\nStep 200 (epoch 0.23), 13.1 ms\nMinibatch loss: 3.459, learning rate: 0.010000\nMinibatch error: 12.5%\nValidation error: 3.9%\n...\nStep 8500 (epoch 9.89), 13.0 ms\nMinibatch loss: 1.604, learning rate: 0.006302\nMinibatch error: 1.6%\nValidation error: 0.9%\nTest error: 0.8%\n```\n\nOn the GTX 1080 system, the performance simply _never_ improves! Error rate is steady at around 90%.\n\n```\nStep 8400 (epoch 9.77), 5.5 ms\nMinibatch loss: 3.881, learning rate: 0.006302\nMinibatch error: 85.9%\nValidation error: 88.7%\nStep 8500 (epoch 9.89), 5.5 ms\nMinibatch loss: 3.877, learning rate: 0.006302\nMinibatch error: 87.5%\nValidation error: 88.7%\nTest error: 89.7%\n```\n\nI tested this with TensorFlow 0.9, from the release PIP package for Python 3.5 with GPU support.\nI also tested this with TensorFlow master from a week ago (fc9162975e52978d3af38549b570cc3cc5f0ab66), compiled to a PIP package on one machine, installed on both machines.\n\nHere are the full system Specs, but the difference between them is only the GPU (1080 vs 960) and the Driver (367.35 vs 361.42)\n\nSystem 1\n- OS: Ubuntu 16.04.1 LTS\n- Kernel: 4.4.0-31-generic\n- NVIDIA Driver Version: 361.42 \n- CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\n- RAM: 12 GB Ram\n- GPU: NVIDIA GTX 960, 4GB VRAM (edition: MSI GTX 960 Gaming 4G)\n\nSystem 2:\n- OS: Ubuntu 16.04.1 LTS\n- Kernel: 4.4.0-31-generic\n- NVIDIA Driver Version: 367.35\n- CPU: Intel(R) Core(TM) i7 CPU         930  @ 2.80GHz\n- RAM: 12 GB Ram\n- GPU: NVIDIA GTX 1080, 8GB VRAM\n\nMy LD_LIBRARY_PATH on both machines is:\n`:/usr/local/cuda/lib64:/usr/local/cuda-7.5/extras/CUPTI/lib64/`\n\nMy Cuda version is 7.5, cudnn is 4.0.7 on BOTH machines. \n\nOutput of `ls -l /usr/local/cuda/lib64` on the Machine with the GTX 960 and GTX 1080\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx960-txt\nhttps://gist.github.com/akors/30f5fbe3994e3ac40a4adbb6f76eb756#file-cudalibs-gtx1080-txt\n\nDoes anyone know what could cause this and how to fix this?\n", "comments": ["Judging from my own model, it seems that the labels returned are simply always Zero.\n\nIs there a simple way to confirm this with the mnist model?\n\nps.: I have run the [mnist_with_summaries.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py) code, and here everything seems fine. What is the difference between those two?\n", "People had similar problems with GTX 1080 (ie, https://github.com/tensorflow/tensorflow/issues/3068), which were fixed by building TensorFlow with CUDA 8.0 instead of default 7.5 - https://github.com/tensorflow/tensorflow/issues/3052\n", "@akors would you let me know if CUDA 8 helps?\n", "Yes, it seems to have fixed the issue. Thank you for your support, and sorry that I didn't find the two threads myself.\n", "I'm having the exact same issues with the GTX 1080 and CUDA 7.5, but I am unable to successfully compile Tensorflow with CUDA 8.0. Is anybody able to share their compile Tensorflow whl file that works with CUDA 8.0?\n", "@ashleyjsands I have one compiled from the master branch (I think it's labeled0.10rc0 or something), for Ubuntu 16.04.1. what is your system?\n\nIf it's incompatible to my whql, try these instructions for compilation: https://github.com/tensorflow/tensorflow/issues/2053#issuecomment-214891013\n", "@ashleyjsands \nHere's my version compiled for CUDA 8.0: https://dl.dropboxusercontent.com/u/1414175/tensorflow_gpu_cuda8-0.9.0-4fb28d6-py3-none-any.whl\n\nYou have to rename (or symlink) it to `tensorflow-0.9.0-py3-none-any.whl` to install.\n", "Thanks @akors. Yes, I'm using Ubuntuy 16.04, but I am using Python 2. I will give Python 3 a try and along with your whl file. Thanks for sending it through.\n\nI tried your instructions for compilation previously, but I got stuck in the part when it came to editting the CROSSTOOL file as I apt-get install-ed gcc-4.9 on my machine and I couldn't figure out the correct paths to set for the cxx_builtin_include_directory values.\n", "Glad I could help.\n\n> I tried your instructions for compilation previously, but I got stuck in the part when it came to editting the CROSSTOOL file as I apt-get install-ed gcc-4.9 on my machine and I couldn't figure out the correct paths to set for the cxx_builtin_include_directory values.\n\nFirst, for CUDA 8, you need GCC 5.3.1, which is not available in the Repos. You have to compile that manually. Second, here's my patch to the CROSSTOOL file:\n\n``` diff\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL b/third_party/gpus/crosstool/CROSSTOOL\nindex 8db81a9..d026738 100644\n--- a/third_party/gpus/crosstool/CROSSTOOL\n+++ b/third_party/gpus/crosstool/CROSSTOOL\n@@ -50,6 +50,7 @@ toolchain {\n   # Use \"-std=c++11\" for nvcc. For consistency, force both the host compiler\n   # and the device compiler to use \"-std=c++11\".\n   cxx_flag: \"-std=c++11\"\n+  cxx_flag: \"-D_FORCE_INLINES\"\n   linker_flag: \"-lstdc++\"\n   linker_flag: \"-B/usr/bin/\"\n\n@@ -57,8 +58,10 @@ toolchain {\n   # used by gcc. That works because bazel currently doesn't track files at\n   # absolute locations and has no remote execution, yet. However, this will need\n   # to be fixed, maybe with auto-detection?\n-  cxx_builtin_include_directory: \"/usr/lib/gcc/\"\n-  cxx_builtin_include_directory: \"/usr/local/include\"\n+  cxx_builtin_include_directory: \"/opt/gcc-5.3/lib/gcc/\"\n+  cxx_builtin_include_directory: \"/opt/gcc-5.3/local/include\"\n+  cxx_builtin_include_directory: \"/opt/gcc-5.3/include\"\n+  cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include/\"\n   cxx_builtin_include_directory: \"/usr/include\"\n   tool_path { name: \"gcov\" path: \"/usr/bin/gcov\" }\n\n```\n"]}, {"number": 3506, "title": "functional ops are not exposed in documentation", "body": "It appears that functional ops are now fully documented, given /tensorflow/g3doc/api_docs/python/functional_ops.md, but they are not exposed in the main documentation (index.md does not link to functional_ops.md)\n", "comments": ["I take it back. I see that they are linked, but for some reason in the API browser they don't show up in the left side navigator.\n", "I also can't find TensorArray in the docs.\n", "We now have spiffy new docs. Feel free to open a new issue if you have issues with the new docs on our devsite."]}, {"number": 3505, "title": "Enable fully_connected to uniquify the scope name", "body": "The implementation of `tf.contrib.layers.fully_connected` uses `variable_op_scope` to handle the name scope of the variables, the problem is that the name scope is only _uniquified_ if `scope` is `None`, that is, if you dont pass a custom name, by default it will be `\"fully_connected\"`. \n\nHowever, in the library I am building I create tons of shorcut methods like `relu_layer`, `sigmoid_layer`, etc, which are implemented using `fully_connected` plus their corresponding activation function. I'd like these names to be uniquified automatically so I don't get these kind of errors:\n\n> ValueError: Variable relu_layer/weights already exists, disallowed. Did you mean to set reuse=True in VarScope?\n\nIn the mean time, could you give me a hint of how to uniquify a variable name scope myself? Is there a function that does this already?\n\nThanks!\n", "comments": ["[StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) is a better venue for this kind of usage question. Does adding your own variable_op_scope outside the call to fully_connected help?\n", "@michaelisard It works but I don't think its a proper solution, the names and TensorBoard graph become highly redundant, for example `relu_layer_2/fully_connected/Relu` and `relu_layer_2/fully_connected/weights`, with a small fix you could remove the `fully_connected` part.\n\nJust change\n\n```\nwith variable_scope.variable_op_scope([inputs],\n                                    scope,\n                                    'fully_connected',\n                                    reuse=reuse) as sc:\n```\n\nto\n\n```\nscope = scope if scope else `fully_connected`\nwith variable_scope.variable_op_scope([inputs],\n                                    None,\n                                    scope,\n                                    reuse=reuse) as sc:\n```\n\non\nhttps://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/contrib/layers/python/layers/layers.py#L786\n\nI could create a pull request if you like.\n", "You could also just define the input parameters scope as `scope='fully_connected'` on the function definition an eliminate the line `scope = scope if scope else 'fully_connected'` of my solution\n", "@sguada maybe you can comment?\n", "Using automatic naming for variable_scopes is a bad idea, if you add a new layer in between all the variables change their name, and create problems restoring and reusing models.  For instance you cannot reuse a layer unless it has a explicit scope.\n\nThere is already some partial functions that assign different activation functions to fully_connected \nhttps://github.com/tensorflow/tensorflow/blob/73ced9d797056c7e67a06ed2098dd809d85ec44a/tensorflow/contrib/layers/python/layers/layers.py#L1337\n\nI could consider adding a default_parameter='fully_connected' to the layers, but I would encourage you to use explicit names for all your layers. \n", "@sguada Thing is, in the same way `fully_connected` lets your users get away with not naming every single layer but getting a sensible default, I too would like the users of my library to have that flexibility. That automatic behavior is very helpful when your just learning and want to get things done quickly.\n\nThe `default_parameter='fully_connected'` idea is excellent, would allow both automatic naming + unique identification when reuse is required.\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 3504, "title": "Inception v3 model crashes with out of memory with batch size of 128 on a GPU with 12 GB memory..", "body": "When I run the inception v3 network, with a _batch size of 32_ , from [here](https://github.com/tensorflow/models/tree/master/inception) on an nVidia GPU with 12 GB memory, I get the below messages during initialization about raising pool_size_limit. The initialization takes a long time. But it eventually manages to run. Is there a way to speed this up ? The pool limit starts from 100 and keeps increasing until it reaches 2997, is there a way to increase the pool limit ? Perhaps, some sort of an environment variable ?\n\nIf I try running with batch_size of 128, it crashes with out of memory. The log of which is attached to this report. Is there a way to fix this ?\n[tensorflow-inception-v3-bs128.txt](https://github.com/tensorflow/tensorflow/files/383534/tensorflow-inception-v3-bs128.txt)\n### Environment info\n\nOperating System:\nStock gpu-dev Docker image on CentOS 7\n\nInstalled version of CUDA and cuDNN: \nroot@d4239a28fc92:~# ls /usr/local/cuda-7.5/lib64/libcud*\n/usr/local/cuda-7.5/lib64/libcuda.so         /usr/local/cuda-7.5/lib64/libcudart.so\n/usr/local/cuda-7.5/lib64/libcuda.so.1       /usr/local/cuda-7.5/lib64/libcudart.so.7.5\n/usr/local/cuda-7.5/lib64/libcuda.so.352.93  /usr/local/cuda-7.5/lib64/libcudart.so.7.5.18\n/usr/local/cuda-7.5/lib64/libcudadevrt.a     /usr/local/cuda-7.5/lib64/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.8.0\n### Steps to reproduce\n\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=32 --max_steps=100\n### What have you tried?\n1. Running with a smaller batch size of 32 instead of 128 to get the model to run successfully.\n2. Pass the command -m 240000000000 to nvidia-docker.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=2169 evicted_count=1000 eviction_rate=0.461042 and unsatisfied allocation rate=0.722842\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=4230 evicted_count=3000 eviction_rate=0.70922 and unsatisfied allocation rate=0.68447\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 146 to 160\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2019 evicted_count=2000 eviction_rate=0.990589 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=2028 evicted_count=2000 eviction_rate=0.986193 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3560 evicted_count=2000 eviction_rate=0.561798 and unsatisfied allocation rate=0.605911\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 449 to 493\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1065 evicted_count=1000 eviction_rate=0.938967 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3857 get requests, put_count=3333 evicted_count=1000 eviction_rate=0.30003 and unsatisfied allocation rate=0.422349\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1158 to 1273\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 65569 get requests, put_count=65760 evicted_count=1000 eviction_rate=0.0152068 and unsatisfied allocation rate=0.0161052\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 2725 to 2997\n2016-07-26 13:27:52.039502: step 0, loss = 13.02 (2.4 examples/sec; 13.434 sec/batch)\n2016-07-26 13:28:21.119108: step 10, loss = 14.07 (25.9 examples/sec; 1.234 sec/batch)\n2016-07-26 13:28:33.393687: step 20, loss = 14.80 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:28:45.642208: step 30, loss = 14.68 (26.3 examples/sec; 1.217 sec/batch)\n2016-07-26 13:28:57.914613: step 40, loss = 13.67 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:29:10.188971: step 50, loss = 13.41 (26.1 examples/sec; 1.226 sec/batch)\n2016-07-26 13:29:22.507990: step 60, loss = 13.20 (25.9 examples/sec; 1.235 sec/batch)\n2016-07-26 13:29:34.767687: step 70, loss = 13.15 (26.2 examples/sec; 1.223 sec/batch)\n2016-07-26 13:29:47.051145: step 80, loss = 13.08 (26.0 examples/sec; 1.229 sec/batch)\n2016-07-26 13:29:59.302955: step 90, loss = 13.09 (26.1 examples/sec; 1.228 sec/batch)\n", "comments": ["The solution is to lower the batch size. The allocator is working as expected ie growing slowly until it reaches a limit. I did some searching doesn't seem that it can be changed easily here is the code in question.\n\nhttps://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/core/common_runtime/gpu/process_state.cc#L162\n", "@zheng-xq does it make sense to make the initial pool size configurable?\n", "This is a bit weird. We shouldn't use pool-allocator under the normal path. It should use BFC allocator instead, which is quite a bit more efficient when it comes to fragmentation. \n\nThe code for that decision is: \n\n  if (!HasGPUDevice() || !FLAGS_brain_mem_reg_cuda_dma) {\n    return GetCPUAllocator(numa_node);\n  }\n\nCould you confirm that TensorFlow detects any GPU in this case? Please upload the entire log to \"http://pastebin.com/\" and paste the link here. \n\nAlso it is a good idea to post your \"nvidia-smi\" result inside your docker container. \n", "Attached the output of the run and the nvidia-smi that you requested from inside the nvidia-docker container. Note that in the first case, with batch size 32, it is actually running the network successfully (I even verified, using nvidia-smi, that the targetted GPU is being utilized.)\n\nOne funny observation is that the only time I see the bfc_allocator being called is when the run failed with out of memory, the log of which I have attached to my first post that is.\n\nAll the successful runs are with pool_allocator, I don't know if this information is useful, but just an observation. \n\nBetween the runs except batch size and number of gpus, which is specified via the command-line to the python script, I am changing nothing else. I am using the stock scripts.\n\n[slurm-3424.txt](https://github.com/tensorflow/tensorflow/files/385891/slurm-3424.txt)\n[nvidia-smi-sl270d-m40.txt](https://github.com/tensorflow/tensorflow/files/385892/nvidia-smi-sl270d-m40.txt)\n", "Closing due to inactivity. Feel free to open a new issue if you still have a problem."]}, {"number": 3503, "title": "Installing tensorflow on windows through Docker", "body": "Hi,\n\nI've successfully installed docker toolbox and created a container but when I enter \n\ndocker run -it b.gcr.io/tensorflow/tensorflow\n\nI get \"Unable to find image 'docker run -it b.gcr.io/tensorflow/tensorflow' loally\"\n\nI have read on other threads that this might be due to space allocation so this container already has 20gb.\n\nWhat do I do? I've got a Windows 10 Pro.\n\nThanks a lot!\n", "comments": ["Because of a severe lack of Windows machine to test this on we can't directly support Windows (yet). Maybe someone here has an idea. You can also try StackOverflow. \n", "I don't believe this is a TensorFlow issue. Please comment to reopen if it is, and if so, let me know what we can change to improve this.\n"]}, {"number": 3502, "title": "Streaming_mean along specific dimension", "body": "Currently the streaming_mean function computes the mean for all tensor values. It would be interesting to have the possibility to specify the axis along with we want to compute the mean. \n", "comments": ["Marking as contributions welcome since it is in contrib not core TensorFlow.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 3501, "title": "build fail with nvcc_options?", "body": "I build tensorflow in virtualenv , it says :\n    gcc: error: unrecognized command line option '-nvcc_options=relaxed-constexpr'\n    gcc: error: unrecognized command line option '-nvcc_options=ftz=true'\n\n gcc is 4.8.2\n cuda is 7.0\n cudnn is 4.0.7 \nmore detail:\n\n```\nERROR: /u01/qianming/tensorflow/tensorflow/core/kernels/BUILD:1575:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops_gpu' failed: gcc failed: error executing command\n(cd /home/hongpengfei.lhpf/.cache/bazel/_bazel_hongpengfei.lhpf/072f5a261c0f2d9a7bd6ffe5c05ae7b2/execroot/tensorflow && \\\nexec env - \\\nLD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda-7.5/lib64::/usr/lib/toolchains/lib:/usr/local/lib:/usr/local/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib \\\n       PATH=/u01/qianming/tensor_env/bin:/home/hongpengfei.lhpf/.usr/local/bin/:/u01/mysql/bin:/usr/local/cuda-7.5/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/X11R6/bin:/opt/dell/srvadmin/bin:/home/hongpengfei.lhpf/bin:/home/mysql/bin:/usr/local/toolchains/bin:/usr/local/cuda/bin:/home/hongpengfei.lhpf/bin \\\n/home/hongpengfei.lhpf/.usr/local/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/training_ops_gpu/tensorflow/core/kernels/training_ops_gpu.cu.d '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/training_ops_gpu/tensorflow/core/kernels/training_ops_gpu.cu.o' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/protobuf -iquote bazel-out/local_linux-opt/genfiles/external/protobuf -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/highwayhash -iquote bazel-out/local_linux-opt/genfiles/external/highwayhash -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem external/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/external/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/highwayhash -isystem bazel-out/local_linux-opt/genfiles/external/highwayhash -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-d02e6a705c30 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-d02e6a705c30 -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -x cuda '-DGOOGLE_CUDA=1' '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/core/kernels/training_ops_gpu.cu.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/training_ops_gpu/tensorflow/core/kernels/training_ops_gpu.cu.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\ngcc: error: unrecognized command line option '-nvcc_options=relaxed-constexpr'\ngcc: error: unrecognized command line option '-nvcc_options=ftz=true'\n```\n", "comments": ["What is the OS? (It's helpful if you fill out the issue template you see when you click on 'new issue'!)\n", "it's centOS 6.5 \n", "@martinwicke this probably needs more info to debug but I'm not sure what specifically to ask for.\n", "How did you build it? With GPU support, I assume? Any other options, or just `--config=cuda -c opt`?\n", "I use `bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer`\nto build tensorflow with GPU support.\nit seems like problem of compiler.\n\n```\ngcc (GCC) 4.8.2\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Feb_16_22:59:02_CST_2015\nCuda compilation tools, release 7.0, V7.0.27\n```\n", "@justdark Hello, I had exactly the same error when accidentally change the gcc tool path in CROSSTOOL https://github.com/tensorflow/tensorflow/blob/v0.9.0/third_party/gpus/crosstool/CROSSTOOL#L49 to my own gcc. make sure  `clang/bin/crosstool_wrapper_driver_is_not_gcc` and change the path in crosstool_wrapper_driver_is_not_gcc file if necessary-- it works for me (centos 6.7, tensorflow v0.9.0)\n", "@wyli thanks a lot.\n", "Looks like the issue was resolved.\r\nClosing the issue."]}, {"number": 3500, "title": "Replaced _logger.warn with _logger.warning", "body": "Fixes #3085.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins Test this please\n"]}, {"number": 3499, "title": "Placeholder names are inconsistent when re-entering a scope", "body": "```\n    with tf.Graph().as_default() as g:\n        for name in [\"a\",\"b\"]:\n            with tf.variable_scope(\"fc\"):\n                x = tf.placeholder(tf.float32, shape=[1], name=name)\n                print(x)\n```\n\ngives\n\n```\nTensor(\"fc/a:0\", shape=(1,), dtype=float32)\nTensor(\"fc_1/b:0\", shape=(1,), dtype=float32)\n```\n\nThe variable \"b\" unexpectedly has the name \"fc_1/b:0\" rather than \"fc/b:0\". This makes it difficult to refer to placeholders by name in my program.\n", "comments": ["```\nwith tf.Graph().as_default() as g:\n    with tf.variable_scope(\"fc\"):\n        for name in [\"a\",\"b\"]:\n            x = tf.placeholder(tf.float32, shape=[1], name=name)\n            print(x)\n```\n\nThe variable scope should be defined outside the for loop, otherwise you are defining two variable scopes and since they are not marked to be reused tensorflow automatically appends the second one with \"_1\"\n", "This only happens with placeholders, not with variables. For variables, entering the same scope twice results in variables being created in the same variable scope.\n", "Well it would seem variable scope is just that variable scope not placeholder scope?\n", "Does [name_scope](https://www.tensorflow.org/versions/r0.9/api_docs/python/framework.html#Graph.name_scope) do what you need?\n\n([StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) is a better venue for this kind of question.)\n", "Automatically closing due to lack of recent activity. Please reopen when you have additional information.\n", "Exactly the same thing happens with `name_scope` instead of `variable_scope`. \n", "Your comment about variables created is not true:\n\n``` python\nimport tensorflow as tf \n\nwith tf.Session() as sess:\n        for name in [\"a\",\"b\"]:\n            with tf.variable_scope(\"fc\"):\n                xx = tf.Variable([1,2], name=name+\"v\")\n        print(xx.name)\n                x = tf.placeholder(tf.float32, shape=[1], name=name)\n                print(x)\n```\n\nyields\n\n```\nfc/av:0\nTensor(\"fc/a:0\", shape=(1,), dtype=float32)\nfc_1/bv:0\nTensor(\"fc_1/b:0\", shape=(1,), dtype=float32)\n```\n\nname_scope on the outside is what you want. If you want to switch scopes and return to that scope you can (see documentation of name_scope). e.g. this does what you want...\n\n``` python\nimport tensorflow as tf\n\nwith tf.name_scope(\"fc\") as scope:\n   with tf.Session() as sess:\n        for name in [\"a\",\"b\"]:\n            with tf.name_scope(scope):\n                xx = tf.Variable([1,2], name=name+\"v\")\n                print(xx.name)\n                x = tf.placeholder(tf.float32, shape=[1], name=name)\n                print(x)\n```\n\n(notice how we capture the scope on the outside)\n", "in case anyone else is wondering the same thing; `tf.name_scope(name + '/')` will force `name` to be re-used. `scope` is just `name + '/'`\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "there is inconsistent between placeholder and variables. \r\n\r\nWhen add '/' to placeholder, it works. \r\n\r\nBut the variables repeated '/' to '//' ...."]}]