[{"number": 17881, "title": "TensortRT Invalid data type: 'int32' when converting TF Object Detection graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes see gist (change line 6 to object detection frozen inference graph path): https://gist.github.com/louisquinn/0c6729a32e87e899ece317de84d02acc\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.7\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.1\r\n- **GPU model and memory**: GTX 1080 8GB\r\n- **Exact command to reproduce**: See gist (change line 6 to object detection frozen inference graph path): https://gist.github.com/louisquinn/0c6729a32e87e899ece317de84d02acc\r\n\r\n### Describe the problem\r\nGetting the following error when calling `trt.create_inference_graph` (see line 18 of gist):\r\n`tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2503] Non-OK-status: ConvertDType(tf_dtype, &dtype) status: Invalid argument: Unsupported data type int32`\r\n\r\nIs int32 not supported by TensorRT, not yet implemented in the Tensorflow wrap, or am I doing something incorrect? This happens with all pre-trained TF Object Detection models. ", "comments": ["@louisquinn at the time our error messages were not very user friendly. Can you tell which network are you trying? it is possible that it is not fully supported yet and error message could be misleading.", "Ran into the same error message with custom SSD model.", "@samikama \r\nYou can reproduce this error using any of the **Object Detection** models from here:\r\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\r\n\r\nI have not attempted TensorRT with the **Mask** models yet. ", "@samikama @petewarden \r\nHi guys is there any update on this? Were you able to reproduce the error?", "@louisquinn you can try commenting out trt_shfn.cc line 52 under tensorflow/contrib/tensorrt/shape_fn and see if that works for you. Mine works fine after commenting out that line.", "@chiachunfu \r\nThanks for the tip! Ill try that today. Did you notice improvements in inference speed?", "Sorry I couldn't get to that. I will try to get to it tomorrow. Meanwhile can you try with master to see if error message is more sensible. You can also try to increase minimum_segment_size to some large number. In general object detection network support will improve with TensorRT4.0 since 3.0.4 is missing some operators.", "@samikama Okay great I will try your suggestions and update here. ", "@louisquinn I went through your test code with ssd_inception_v2_coco model from object detection zoo. I should have read the issue earlier :) Yes int32 ops are not available natively in TensorRT. It will be possible to convert object detection networks and others through plugin mechanism in near future with new TensorRT releases. Sorry for the inconvenience. ", "Closing issue. INT32 nodes are not supported in TensorRT until release 4.0", "Okay great, thanks @samikama for the update. "]}, {"number": 17880, "title": "How to force Tensorflow app to use Android framework's NNAPI?", "body": "### Describe the problem\r\nI am using x86 (intel platform) android device to check if tensorflow apk is calling Android's NNAPI. It seems its not calling the APIs.\r\nHow can i force tensorflow apk to forcefully use Android framework's NNAPI?\r\n\r\nAndroid studio project location: <src>/tensorflow/contrib/lite/java/demo/app/\r\nAndroid version: 8.1", "comments": ["I have a PR for this, see https://github.com/tensorflow/tensorflow/pull/16065", "This seems to be resolved with another PR see:\r\nhttps://github.com/tensorflow/tensorflow/blob/049dfd5e070cfa84c82eea71c6c746a70cba4a3f/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java#L207\r\n\r\nPlease close if this is done to your satisfaction. Thanks!", "I'll close this for now. Let us know if this still doesn't work for you.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This is solved..."]}, {"number": 17879, "title": "r1.3 freeze_graph.py issue", "body": "1.\r\npython freeze_graph.py \\\r\n--input_graph=./graph/model.graph.pbtxt \\\r\n--input_checkpoint=**./checkpoint/model.ckpt-100** \\\r\n--output_graph=./graph/model.graph.pb \\\r\n--output_node_names=out\r\n\r\n2.\r\npython freeze_graph.py \\\r\n--input_graph=./graph/model.graph.pbtxt \\\r\n--input_checkpoint=**./checkpoint//model.ckpt-100** \\\r\n--output_graph=./graph/model.graph.pb \\\r\n--output_node_names=out\r\n\r\nHere, it success when I using No.1, but No2., I get the error:\"Input checkpoint './checkpoint//model.ckpt-100' doesn't exist!\"\r\nThe different is the path with \"/\" or \"//\", I don't know if it is a bug in r1.3.\r\nplease~~~", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17878, "title": "Updated README.md", "body": "Edited the word 'lets you' to 'enables you to', which I think is better for this sentence. The word 'enable' here gives the readers a feeling that the architecture makes some awesome things possible, while 'let' is more like to give some permissions to the users.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I already signed the CLA!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17877, "title": "tf.manip.roll silently ignores negative axes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **TensorFlow installed from (source or binary)**: unknown\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.InteractiveSession()\r\nprint(tf.manip.roll(tf.range(5), -1, axis=0).eval())\r\n# [1 2 3 4 0]\r\nprint(tf.manip.roll(tf.range(5), -1, axis=-1).eval())\r\n# [0 1 2 3 4]\r\n```\r\n\r\n### Describe the problem\r\n\r\n`axis=-1` and `axis=0` should be equivalent, if `tf.manip.roll()` works like `numpy.roll()` and other TensorFlow/NumPy functions that accept negative axes. However, instead negative axes are silently ignored. At the very least, TensorFlow should have raised an informative error.", "comments": ["@keveman Can you answer this question?", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "#18409 PR adds support for negative axis to tf.manip.roll"]}, {"number": 17876, "title": "Custom optimiser based on pre-trained model", "body": "Hello. This might be a support ticket or a feature request.\r\n\r\nI'm trying to make an optimiser based on output of a pretrained model. Running a model requires an open session in tensorflow but there is nothing like it inside OpKernel where the existing model should be called. \r\n\r\nHow to correctly call a model inside an opkernel? Is it possible to access already compiled functionality  of this kind? \r\n\r\nThis feature would be very handy when it comes to complex loss functions. I'm able to write it on my own and wrap it in a PR when my questions are resolved. ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: Not yet\r\nOS Platform and Distribution: OSX 10.13\r\nTensorFlow installed from: Source\r\nTensorFlow version: 1.5\r\nBazel version: 0.11.0\r\nCUDA/cuDNN version: 9.0 / 7.0\r\nGPU model and memory: 2 * GTX 1070, 8gb\r\nExact command to reproduce: N/A", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17875, "title": "[Intel MKL] Enabling more operator fusions", "body": "Support for Conv+ReLU fusion; Enabling Shape op as a part of MKL layout propagation\r\n\r\n1) This commit adds support for Conv+ReLU and BatchNorm+ReLU fusion and corresponding graph unit\r\ntests. 2) This PR also adds support for Shape op in MKL layout propagation. 3) It also updates\r\nMklToTf conversion pass unit tests because of that change.", "comments": ["Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "So sorry this fell through the cracks.\r\n\r\n@tatianashp could you take a look?", "Nagging Reviewer @tatianashp: It has been 15 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @tatianashp: It has been 36 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "@nhasabni Sorry it fell through the cracks again. Very large PRs are hard to review. Could you please resolve conflicts and split the changes into a few smaller PRs. I'll review then.", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @benoitsteiner: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "There has been no activity on this PR for a while.\r\n@nhasabni Could you please either resolve conflicts or close the PR?\r\n/cc @agramesh1 ", "Nagging Assignee @benoitsteiner: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing due to lack of recent activity. If this PR is still relevant please resolve conflicts and re-open."]}, {"number": 17874, "title": "Revert \"Fix dataset resampling bug introduced by a bug in datasets itself. fixes #16606 \"", "body": "Reverts tensorflow/tensorflow#17858\r\n\r\nBreaks sanity checks.", "comments": []}, {"number": 17873, "title": "I am unable to install the latest tensorflow version from pip. It always show me the following error.. ", "body": "Please go to Stack Overflow for help and support:\r\ntensorflow-1.0.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\r\n\r\nI am running python3.6 64 bit on windows 10 ", "comments": ["Hey,\r\n\r\nI ran into this problem on windows 10 as well. I tried it on 3.5 and it worked. Hopefully this solves it for you.", "Thanks, \r\nIt worked for me.\r\n"]}, {"number": 17872, "title": "Branch 189819449", "body": "", "comments": []}, {"number": 17871, "title": "Branch 189819449", "body": "", "comments": []}, {"number": 17870, "title": "Fix windows GPU build scripts.", "body": "PiperOrigin-RevId: 188629017", "comments": []}, {"number": 17869, "title": "Unable to run inference on mobilenet", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI want to run mobilenet inference on one image. I have converted the JPEG image to binary. The binary file size is 602112 bytes [3 x 224 x 224 x sizeof(float32)]\r\n\r\nI downloaded the mobilenet model from: \r\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md\r\n\r\nAnd tried running inference on it as follows:\r\n\r\n# init\r\nprotobuf = 'mobilenet_v1_0.25_128_frozen.pb'\r\nimg_fname = 'ILSVRC2012_val_00000001.bin'\r\niname = 'prefix/input:0'\r\noname = 'prefix/MobilenetV1/Predictions/Softmax:0'\r\n\r\n# read graph definition\r\ngfile = tf.gfile.GFile(protobuf, \"rb\")\r\ngraph_def = tf.GraphDef()\r\ngraph_def.ParseFromString(gfile.read())\r\n_ = tf.import_graph_def(graph_def, name='prefix')\r\n\r\n# load image\r\nimage_data = tf.gfile.GFile(img_fname,'rb').read()\r\n\r\n# run inference\r\nwith tf.Session() as sess:\r\n output_tensor = sess.graph.get_tensor_by_name(oname)\r\n output = sess.run(output_tensor, {iname:image_data})\r\n\r\n data_fname, _ = oname.split(':0')\r\n data_fname = data_fname.replace(\"/\", \".\") + \".bin\"\r\n output.tofile(data_fname)\r\n print(data_fname, 'saved')\r\n\r\nIt doesn't seem to work. It's not crashing but printing a lot of numbers like:\r\n\\x86Bo\\x13QB\\x05\\xf6\\x89B\\xeb\\x01qB\\x9bb\\x80B\\xe7\\xa7VBA\\xb3\\x80B\\xe3c\\x85Bs\\x03kB\\xd7\\xe5\\x7fBs;{B\\x8be\\x8eB[+jB/\\x08{B\\xff6RB\\x87\\xe3&B\\x15\\x1f\\xb3A\\xc7\\xfdQB\\xf3\\xe6UB\\x0b\\xa8\\nB\\xab\\xc5GB\\xef\\x12JB'", "comments": ["What output were you expecting?\r\n\r\nFrom:\r\n```python\r\nprint(output_tensor.shape)\r\nprint(output_tensor.dtype)\r\n```\r\n\r\nyou can tell that the shape of the output tensor is [None, 1001] and its dtype is float32, which means that `output` contains, for each image in the batch of input images, a vector of 1001 elements. This vector is representing the probability distribution of the classification.\r\n\r\nThe contents of the saved file are the serialized numpy array of that `[<# input images>, 1001]` shaped tensor.\r\n\r\nI'm going to go ahead and close this issue since this is not a bug or feature request. For support like this I'd recommend asking on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). There is also a larger community that reads questions there.\r\n\r\nHope that helps.\r\nFeel free to re-open if I've misunderstood.\r\n\r\nThanks."]}, {"number": 17868, "title": "Test cases fail on AWS DeepLens device for Tensorflow v1.4", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.4.0-0-gd752244\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\nTEST_TMPDIR=$BAZEL_OUTPUT_BASE bazel test --build_tests_only --config=mkl -c opt --verbose_failures --incompatible_load_argument_is_label=false --test_verbose_timeout_warnings //tensorflow/python/...\r\n\r\nconfiguration output (\".tf_configure.bazelrc\" file):\r\n\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --define PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nbuild --define PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --force_python=py2\r\nbuild --host_force_python=py2\r\nbuild --python_path=\"/usr/bin/python\"\r\ntest --force_python=py2\r\ntest --host_force_python=py2\r\ntest --define PYTHON_BIN_PATH=\"/usr/bin/python\"\r\ntest --define PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nrun --define PYTHON_BIN_PATH=\"/usr/bin/python\"\r\nrun --define PYTHON_LIB_PATH=\"/usr/local/lib/python2.7/dist-packages\"\r\nbuild --define with_jemalloc=true\r\nbuild:gcp --define with_gcp_support=true\r\nbuild:hdfs --define with_hdfs_support=true\r\nbuild:s3 --define with_s3_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild:gdr --define with_gdr_support=true\r\nbuild:verbs --define with_verbs_support=true\r\nbuild --action_env TF_NEED_OPENCL=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_MKL_ROOT=$BUILD_DIR/tmp/intel/mklml_lnx_2018.0.20170425\"\r\nbuild --action_env TF_DOWNLOAD_MK=0\r\nbuild:opt --cxxopt=-march=native --copt=-march=native\r\nbuild:mkl --define using_mkl=true\r\nbuild:mkl -c opt\r\nbuild:mkl --copt=\"-DEIGEN_USE_VML\"\r\nbuild:monolithic --define framework_shared_object=false\r\nbuild --define framework_shared_object=true\r\n\r\nMKL related configuration:\r\n\r\nbuild --action_env TF_MKL_ROOT=$BUILD_DIR/tmp/intel/mklml_lnx_2018.0.20170425\"\r\nbuild --action_env TF_DOWNLOAD_MKL=0\r\n\r\n### Describe the problem\r\nWe build Tensorflow v1.4 and install it in a AWS DeepLens device. The device is equipped with a Intel Atom Processor E3930. Then we run the unit test cases using the above command. 19 out of 456 tensorflow test cases fail on this platform. \r\n\r\nThe errors of each failing tests are shown below. Detailed logs can be found in the attachment.\r\n\r\nkernel_tests:conv_ops_test: assert error: numbers match but shapes of the result do not match.\r\nnn_fused_batchnorm_test.py: AssertionError: 0.1277604103088379 not less than 0.01\r\nlayers_normalization_test.py: assert error: numbers don't match but scaled.\r\ncheckpoint_utils_test.py: checkpoint too large: 28077 > 28000\r\nitem_test.py: AssertionError: Lists differ: ['Const', 'Const_1', 'add'] != ['Const_1', 'Const', 'add']\r\nspecial_math_ops_test.py: shape mismatch for sum.\r\ntimeline_test.py: cpu usage not maximum ?\r\nanalyzer_cli_test.py: testEvalExpression\r\ncurses_ui_test.py: ui output format mismatch First differing element 2:'array([[ 1.,  1.,  1.,  1.,  1.],' 'array([[1., 1., 1., 1., 1.],'\r\nsession_debug_file_test.py: mutliple failures\r\nsession_debug_grpc_test: timeout 315 seconds\r\nstepper_test\r\ntensor_format_test: string mismatch\r\ntensor_test: AssertionError: '[0 ..., 9]' not found in 'tf.Tensor([0 ... 9], shape=(10,), dtype=int32)'\r\nfeature_column_test: AssertionError: OpError not raised\r\nmodel_analyzer_test:\r\nrun_metadata_test: AssertionError: u'DMT' != 'MatMul'\r\nprint_selective_registration_header_test: String mismatch\r\nconvolutional_recurrent_test: tolerance\r\n\r\n### Source code / logs\r\nSee attached:\r\n[fail.zip](https://github.com/tensorflow/tensorflow/files/1831319/fail.zip)\r\n", "comments": ["Nagging Assignee @tatatodd: It has been 186 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17867, "title": "Tensorflow Projector - nearest points in original space", "body": "In [Tensorflow Projector](http://projector.tensorflow.org), the cosine and euclidean distance, and the ranking based on that seems to be incorrect when the number of dimensions for the points is 50 or more.  It is fine when the number of dimensions is 49 or less.  Here's some code to test\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.metrics.pairwise import cosine_distances, euclidean_distances\r\n\r\nnpoints,ndims = 4,49\r\nnclose = npoints-1\r\nfunc = cosine_distances\r\n\r\nnp.random.seed(seed=123456)\r\nembed = np.random.uniform(size=npoints*ndims).reshape((npoints,ndims))\r\nlabels = np.array(['pt%03d'%n for n in range(1,npoints+1)])\r\n\r\nnp.savetxt('embed-%d-%d.tsv'%(npoints,ndims), embed, delimiter='\\t', fmt='%.6f')\r\nnp.savetxt('labels-%d.tsv'%(npoints), labels, fmt='%s')\r\n\r\ndist = func(embed)\r\nind = np.argsort(dist)[:,1:nclose+1]\r\n\r\nnames = labels[ind]\r\ndist = dist[np.arange(labels.shape[0])[:,None],ind]\r\n\r\nnp.concatenate((labels[:,None],\r\n                np.dstack((names,np.around(dist,4))).reshape((dist.shape[0],-1))), axis=1)\r\n```\r\n\r\nThe output of the above code is shown below.  For `pt001`, the closest is `pt004` with cosine distance 0.1695, etc.\r\n```\r\narray([['pt001', 'pt004', '0.1695', 'pt003', '0.2716', 'pt002', '0.2897'],\r\n       ['pt002', 'pt003', '0.2345', 'pt004', '0.2365', 'pt001', '0.2897'],\r\n       ['pt003', 'pt004', '0.2099', 'pt002', '0.2345', 'pt001', '0.2716'],\r\n       ['pt004', 'pt001', '0.1695', 'pt003', '0.2099', 'pt002', '0.2365']],\r\n      dtype='<U32')\r\n```\r\nThe above code also generates files which can be uploaded to the projector website for confirmation.  The output there is:\r\n```\r\nNearest points in the original space:\r\npt004 0.169\r\npt003 0.272\r\npt002 0.290\r\n```\r\nNow change ndims to 50 in the above code and re-run.  The output is\r\n```\r\narray([['pt001', 'pt002', '0.1675', 'pt004', '0.1968', 'pt003', '0.2571'],\r\n       ['pt002', 'pt001', '0.1675', 'pt004', '0.2444', 'pt003', '0.246'],\r\n       ['pt003', 'pt004', '0.1943', 'pt002', '0.246', 'pt001', '0.2571'],\r\n       ['pt004', 'pt003', '0.1943', 'pt001', '0.1968', 'pt002', '0.2444']],\r\n      dtype='<U32')\r\n```\r\nOnce the new files are loaded, the output from the website is:\r\n```\r\nNearest points in the original space: \r\npt002 1.040\r\npt004 1.304\r\npt003 1.538\r\n```\r\nThe website does report the number of dimensions correctly at the top (first 49 and then 50).  Also, `Spherize Data` was turned on and off that makes a very slight difference.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Please note that the issue refers to https://projector.tensorflow.org.\r\n\r\nHave I written custom code: __No__\r\nOS Platform and Distribution:  __Chrome on Windows__\r\nTensorFlow installed from: __Not applicable__\r\nTensorFlow version: __Not applicable__\r\nBazel version: __Not applicable__\r\nCUDA/cuDNN version: __Not applicable__\r\nGPU model and memory: __Not applicable__\r\nExact command to reproduce: __Included in the initial message__", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@dsmilkov Could you take a look at this project issue?", "Hi @ironv \r\n\r\nIt's probably due to the fact that as soon as the data points are >=50 dims, we are doing approximate PCA / T-SNE / NearestNeighbors computations by randomly projecting the data to 50 random directions.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this issue since for dim>50, we do approximate PCA/T-SNE and show an icon on the UI that this is approximate.", "1. UI is stating that data is projected to 200 dimensions. However, I see approximate calculation for 100 dimensions.\r\n2. So \"PCA is approximate\" warning is intended to warn about all the results? It's on the opposite side of the page and the wording \"Nearest point in the **original space**\" is misleading.\r\nCould you please have another look into this?", "Is there any option to download the nearest neighbors as csv file. ", "> Closing this issue since for dim>50, we do approximate PCA/T-SNE and show an icon on the UI that this is approximate.\r\n\r\n@dsmilkov ,Could you please give a link to the source code that calculates euclidean distance?\r\n"]}, {"number": 17866, "title": "Branch 189799697", "body": "", "comments": []}, {"number": 17865, "title": "Disable one more flaky test on mac.", "body": "", "comments": []}, {"number": 17864, "title": "Revert \"Adds missing protobuf dep to tf.contrib.data ops. (#17840)\"", "body": "This reverts commit 36ec749ec79c2313924666a1c5324620e493d0c4.", "comments": ["Updated this after discussion with Allen, now it keeps the lib_proto_parsing dep but protects it with if_static().  PTAL, and thanks in advance!", "I'm facing `undefined symbol error for _dataset_ops.so` when I run my compiled tensorflow\r\nboth adding it the dependency and protecting it with if static is not showing in source code\r\ndespite it was released after\r\n\r\nI'm not facing that with the pre-built tensorflow, so, how did this commit slipped from `r1.7`?\r\n\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v1.7.0"]}, {"number": 17863, "title": "Support a single Tensor in StagingArea.put()", "body": "This PR is to fix #13288 as a follow-up of #17862 to support a single Tensor in StagingArea.put().\r\nAs described in the above issue, considering the following two pieces of codes:\r\n**Snippet 1**\r\n> import tensorflow as tf\r\n> from tensorflow.contrib import staging\r\n> staging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))\r\n\r\n**Snippet 2**\r\n> import tensorflow as tf\r\n> from tensorflow.contrib import staging\r\n> staging.StagingArea(dtypes=[tf.int32]).put((tf.constant(1),))\r\n\r\nThe Snippet 1 won't work while the Snippet 2 works. It obviously currently can only support a tuple or a list of tensors instead of single tensor.\r\n\r\nThis PR is to support a single Tensor in StagingArea.put().", "comments": ["//tensorflow/python/kernel_tests:stage_op_test seems to be failing. Can you take a look? ", "nvm might not be related to your PR. Let me check on tests at head."]}, {"number": 17862, "title": "Update the doc of StagingArea.put() to a tuple or list of Tensors", "body": "This PR is to fix #13288.\r\n\r\nAs described in the above issue, considering the following two pieces of codes:\r\n**Snippet 1**\r\n> import tensorflow as tf\r\n> from tensorflow.contrib import staging\r\n> staging.StagingArea(dtypes=[tf.int32]).put(tf.constant(1))\r\n\r\n**Snippet 2**\r\n> import tensorflow as tf\r\n> from tensorflow.contrib import staging\r\n> staging.StagingArea(dtypes=[tf.int32]).put((tf.constant(1),))\r\n\r\nThe **Snippet 1** won't work while the **Snippet 2** works. It obviously currently can only support a tuple or a list of tensors instead of single tensor. \r\n\r\nThus this PR is firstly to fix the doc of StagingArea.put(). Next step, I'll try to continue working on how to support a single Tensor in a StagingArea.put().", "comments": ["Closing as covered in #17863"]}, {"number": 17861, "title": "RNN no learning when operation as node as opposed to fed as calculated values", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: na\r\n- **GCC/Compiler version (if compiling from source)**: na\r\n- **CUDA/cuDNN version**: na\r\n- **GPU model and memory**: na\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a RNN with variable sequence lengths. To get the last non-zero output i.e. the relevant output for each sample I have a function `last.relevant()` (see below). I define a node for this operation: `rel_output = self.last_relevant(outputs, seq_lengths)`. The odd thing is, I have to evaluate this node, save the results of it in a variable and feed it to the next operation which has a placeholder for the results of the `rel_output` node. Actually, I would expect the exactly same behaviour when I use the `rel_output` node to define a new node. But when I do, the training gets stuck after the second iteration and all gradients go to zero.\r\n\r\nThis is really hard to track as no exceptions are raised, just the training getting stuck. I spent a couple of hours finding that bug.\r\n\r\n### Source code / logs\r\n\r\n    def last_relevant(self, output, seq_length):\r\n        # the RNN returns outputs for every input unit, but we are just\r\n        # interested in the last one that is not zero\r\n        # author: Danijar Hafner\r\n        # (https://danijar.com/variable-sequence-lengths-in-tensorflow/)\r\n        batch_size = tf.shape(output)[0]\r\n        max_length = tf.shape(output)[1]\r\n        out_size = int(output.get_shape()[2])\r\n        index = tf.range(0, batch_size) * max_length + (seq_length - 1)\r\n        flat = tf.reshape(output, [-1, out_size])\r\n        relevant = tf.gather(flat, index)\r\n        return relevant\r\n\r\n#### Training working fine\r\n        rel_output = self.last_relevant(outputs, seq_lengths)\r\n        last_nonzero_output = tf.placeholder(\"float\", [n_samples, self.n_hidden])\r\n        pred = tf.nn.softmax(tf.tanh(tf.matmul(last_nonzero_output, self.weights['out']) + self.biases['out']))\r\n\r\n        ro = session.run(rel_output, feed_dict={x: data})\r\n        _ = session.run([optimizer], feed_dict={last_nonzero_output: ro, y: labels_oh})\r\n\r\n#### Training getting stuck\r\n        rel_output = self.last_relevant(outputs, seq_lengths)        \r\n        pred = tf.nn.softmax(tf.tanh(tf.matmul(rel_output, self.weights['out']) + self.biases['out']))\r\n\r\n       _ = session.run([optimizer], feed_dict={x: data, y: labels_oh})", "comments": ["Try doing a reshape on the rel_output?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "@rohan100jain In what way? Currently it has the shape (n_samples, n_hidden_units) where n_hidden_units are the hidden units of the RNN. AFAIK that's the way it is supposed to be for multiplying the weights in the next step."]}, {"number": 17860, "title": "Failed assert in the TF native code kills JVM", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7, Java 1.8.0_161 x64\r\n- **TensorFlow installed from (source or binary)**: binaries downloaded from Maven repo\r\n- **TensorFlow version (use command below)**: both org.tensorflow:tensorflow:1.4.0 and org.tensorflow:tensorflow:1.5.0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI am running some experiments calling TensorFlow from Java.\r\nAnd in case my code hits some assertion in the TF native code the JVM terminates immediately.\r\n\r\nThis I believe closes the door for running TensorFlow in any kind of Java server environment.\r\n\r\nInstead the internal assertions should be propagated to JNI layer and thrown as normal Java exceptions (`IllegalStateException`, `AssertionError` and the likes).\r\n\r\n### Source code / logs\r\n\r\nHere are two cases of the assertions I ran into yesterday with TF 1.4.0\r\n\r\n```\r\n2018-03-19 17:56:00.364932: F .\\tensorflow/core/lib/core/refcount.h:82] Check failed: ref_.load() >= 1 (0 vs. 1)\r\n\r\n2018-03-19 18:30:22.390146: F .\\tensorflow/core/framework/tensor_shape.h:130] Check failed: static_cast<uint32>(dt) < 256u (374 vs. 256)\r\n```\r\n\r\nThe errors above are reproducible more or less consistently when using the same input `Tensor<Float>` in three different `Graph`'s sequentially.\r\nIt it probably not the right way to do that. But the issue in general still stands - assertions in the native code must not kill the hosting JVM.\r\n\r\nI've been trying to reproduce the problem in an isolated minimal test project. But hasn't been able to do that so far. I believe however that this is not strictly necessary to illustrate the core problem.\r\nIt should be possible to reproduce the JVM death by adding some \"always fail\" assertion to the TF native code.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "It is stated in the description that I used pre-compiled binaries (from public Maven repository).\r\nAnd Bazel version is required \"if compiling from source\". You should probably adjust your \"butler\" rules.\r\nI've added \"N/A\" for both Bazel and GCC though.", "@gunan can you please take a look or redirect? Thanks", "I am OOO this month, please redirect.", "@yifeif can you please take a look?\r\nThanks\r\n", "Hi @sabi0!We are checking to see if you still need help in this issue , Have you tried latest stable version TF 2.6  yet? Please create a new issue if the issue is replicating in newer version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17858, "title": "Fix dataset resampling bug introduced by a bug in datasets itself. fixes #16606 ", "body": "Fixes github issue #16606.\r\n\r\nThe core issue is that in the case of certain random Tensors, the\r\nfollowing two lines aren't the same:\r\n\r\n```\r\nrand_0s_and_1s_ds = ...\r\ngather_ds = rand_0s_and_1s_ds.map(lambda i: tf.gather([0, 1], i))\r\ntup_ds = tf.data.Dataset.zip(gather_ds, rand_0s_and_1s_ds)\r\n\r\n```\r\n\r\n```\r\nrand_0s_and_1s_ds = ...\r\ntup_ds = rand_0s_and_1s_ds.map(lambda i: (tf.gather([0, 1], i), i))\r\n```\r\n\r\nNote that this does NOT fix the underlying issue of drawing multiple\r\nsampes from the underlying distribution.\r\n\r\nTested:\r\nWith the new test, `bazel test :resample_test` fails before and succeeds\r\nafter.", "comments": ["This PR has broken the sanity checks, there are pylint errors on this.\r\nPlease avoid merging any PRs without completed tests.\r\n\r\nI will revert this PR asap. This blocks progress on many other PRs."]}, {"number": 17857, "title": "improve fp16 tftrt prediction", "body": "  delay fp32 to fp16 conversion to reduce accumulated rounding error", "comments": ["Hope that we have not freeze the code for 1.7 yet. Otherwise I'll do a PR to master @aaroey ", "I'll wait for @gunan's opinion.", "If this can be reviewed in the next 2 hours I can still get it into 1.7", "Ping @gunan to see if now is still a good time.", "Hi @jjsjann123,\r\n\r\nAs a followup, would you please add separate unit tests for convert_nodes.cc and convert_graph.cc? These files are becoming enormous (especially convert_nodes.cc) and it will be very hard to maintain and debug without a test.\r\n\r\nThanks.", "@aaroey Sure. Will do as follow up PRs to master.", "@jjsjann123 That would be great, thanks!", "it that tools for convert float 32 model to float 16 model?"]}, {"number": 17856, "title": "second order differential of fused_batch_norm makes a large number of nodes.", "body": "I found that the second order differential of tf.nn.fused_batch_norm has much larger number of nodes than that of tf.nn.batch_normalization. \r\n\r\nI wrote a sample program which has 7 batch normalization layers. Using tf.nn.fused_batch_norm, it has about 22,000 nodes. On the other hand, using tf.nn.batch_normalization, it has only 1700 nodes (I counted with tensorboard). Is this a reasonable behavior?\r\n\r\n### Source Code\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nuse_fused_batch_norm = False\r\n\r\nwith tf.name_scope(\"network\"):\r\n    featmap = tf.constant(np.random.rand(3,3,32,32), dtype=tf.float32)\r\n\r\n    for i in range(7):\r\n        with tf.name_scope(\"layer{}\".format(i)):\r\n            if use_fused_batch_norm:\r\n                beta = tf.Variable(np.zeros(3), name=\"beta\", dtype=np.float32)\r\n                gamma = tf.Variable(np.ones(3), name=\"gamma\", dtype=np.float32)\r\n                featmap, _, _ = tf.nn.fused_batch_norm(featmap, gamma, beta, data_format=\"NCHW\")\r\n\r\n            else:\r\n                beta = tf.Variable(np.zeros([1,3,1,1]), name=\"beta\", dtype=np.float32)\r\n                gamma = tf.Variable(np.ones([1,3,1,1]), name=\"gamma\", dtype=np.float32)\r\n                mu, sigma = tf.nn.moments(featmap, axes=[0,2,3], keep_dims=True)\r\n\r\n                featmap = tf.nn.batch_normalization(featmap, mu, sigma, beta, gamma, variance_epsilon=1e-07)\r\n\r\n    y = tf.reduce_mean(featmap, axis=[1,2,3])\r\n\r\nW = tf.trainable_variables()\r\n\r\nwith tf.name_scope(\"rop\"):\r\n    temporary = tf.ones_like(y)\r\n    grad      = tf.gradients(y, W, grad_ys=temporary)\r\n    r         = [tf.ones_like(t) for t in W]\r\n    ggrad     = tf.gradients(grad, temporary, grad_ys=r)[0]\r\n\r\n\r\nwith tf.Session() as sess:\r\n    summary_writer = tf.summary.FileWriter(\"log\",\r\n                                           sess.graph)\r\n\r\n```\r\n\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n('v1.5.0-0-g37aa430d84', '1.5.0')\r\n- **Python version**: \r\n2.7.12\r\n- **CUDA/cuDNN version**:\r\ncuda9 / cudnn 7.0.5\r\n- **GPU model and memory**:\r\nGTX 1080, 8GB\r\n- **Exact command to reproduce**:\r\npython main.py\r\n- **Bazel version** \r\nN/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "FusedBatchNormGradGrad uses gradients funcion to calculate gradients in  [nn_grad.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_grad.py#L936).\r\n\r\n```\r\n  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\r\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial)\r\n```\r\n\r\nThen derivative of (grad_y) with respect to x is calculated. I think this makes a large number of nodes of the above network. The network size become much smaller, when we use stop_gradients in the above snippet like\r\n\r\n```\r\n  grad_grad_y, grad_x, grad_scale = gradients_impl.gradients(\r\n      [grad_x, grad_scale, grad_offset], [grad_y, x, scale], grad_initial, stop_gradients=[grad_y, x, scale])\r\n```\r\n\r\nCan we use stop_gradients?", "@ebrevdo @andydavis1 could you comment about whether this is WAI?", "This is fine. Grappler prunes many of them at runtime.", "@ebrevdo thank you for reply. \r\n\r\nDo you mean that the graph is shrinked at sess.run, so the temporary big graph is not important?\r\n\r\nWhen I use second order differential of fused_batch_norm in a big graph like Resnet, memory is exhausted at tf.gradients.\r\nSo my program can't reach sess.run. \r\n\r\nIf we can optimize graph at graph construction time, I think we should do it.\r\n", "The graph is rewritten at session.run, yes. That doesn't help you though.\nThousands of ops seems a bit excessive.  Can you identify which forward op\nexactly causes the gradient graph to blow up?\n\nOn Tue, Apr 3, 2018, 7:09 PM laket <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> thank you for reply.\n>\n> Do you mean that the graph is shrinked at sess.run, so the temporary big\n> graph is not important?\n>\n> When I use second order differential of fused_batch_norm in a big graph\n> like Resnet, memory is exhausted at tf.gradients.\n> So my program can't reach sess.run.\n>\n> If we can optimize graph at graph construction time, I think we should do\n> it.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-378457131>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim8w6KmgEQo0HcrK0ytJvNdIgr5euks5tlCtGgaJpZM4SxqqD>\n> .\n>\n", "As described in my second [post](https://github.com/tensorflow/tensorflow/issues/17856#issuecomment-375599868) , \r\nprocess in FusedBatchNormGradGrad cause the gradient graph to blow up.\r\n\r\nWhen I use stop_gradient in FusedBatchNormGradGrad or \r\ntf.nn.batch_normalization instead of fused_batch_norm, the graph is much shrinked. \r\nSo I think fused_batch_norm cause the gradient to blow up.", "I confirmed that using stop_gradient as described above makes wrong output.\r\n\r\nI'm sure the implementation of FusedBatchNormGradGrad is inefficient, \r\nbut I don't have any good idea to solve this problem yet. \r\n"]}, {"number": 17855, "title": "why training and validation loss are always zero during the training phase of deep neural network?", "body": "I'm new to machine learning and I'm trying to learn more about it. I have been facing many problems doing my project as DEEP NEURAL NETWORK Classifier (classes 0,1). \r\nI'm using windows 8.1, 4GB ram, python 3.6.4(pip installation), tensorflow cpu version v1.4 (pip installation).\r\nAll other packages required for this code also installed through pip installation and have been imported.\r\nDuring training, Train loss and validation loss are always zero. I don't know why?\r\nIf anyone know the problem, please let me know asap!! My project deadline is very near!!\r\n\r\nThings I've tried:\r\n1. different values for hidden_units, no. of hidden layers, batch_size, learning_rate \r\n2. Equally distributed my sample inputs according to the class labels. \r\n3. I gave the same data to LDA (Linear Discriminant Analysis) and it's able to classify at the accuracy of 99.6%\r\n4. I gave the same data to bernoulliRBM and it classifies all samples as class '0'\r\n\r\n# here is my CODE\r\n\r\n    def build_neural_network(hidden_units_1=8, hidden_units_2=16):\r\n   \r\n        tf.reset_default_graph()\r\n        inputs = tf.placeholder(tf.float64, shape=[None, train_x.shape[1]])\r\n        labels = tf.placeholder(tf.float64, shape=[None, 1])\r\n        learning_rate = tf.placeholder(tf.float64)\r\n        is_training=tf.Variable(True,dtype=tf.bool)\r\n        initializer = tf.contrib.layers.xavier_initializer()\r\n        fc = tf.layers.dense(inputs, hidden_units_1, activation=None,kernel_initializer=initializer)\r\n        fc=tf.layers.batch_normalization(fc, training=is_training)\r\n        fc=tf.nn.relu(fc)\r\n        fc = tf.layers.dense(fc, hidden_units_2, activation=None,kernel_initializer=initializer)\r\n        fc=tf.layers.batch_normalization(fc, training=is_training)\r\n        fc=tf.nn.relu(fc)\r\n        logits = tf.layers.dense(fc, 1, activation=None)\r\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\n        cost = tf.reduce_mean(cross_entropy)\r\n       \r\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n             optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n        \r\n        predicted = tf.nn.softmax(logits)\r\n        correct_pred = tf.equal(tf.round(predicted), labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\r\n\r\n        export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\r\n                    'cost', 'optimizer', 'predicted', 'accuracy']\r\n        Graph = namedtuple('Graph', export_nodes)\r\n        local_dict = locals()\r\n        graph = Graph(*[local_dict[each] for each in export_nodes])\r\n        \r\n        return graph\r\n   \r\n    model = build_neural_network()\r\n\r\n    epochs = 10\r\n    train_collect = 10\r\n    train_print=train_collect*2\r\n    learning_rate_value = 0.01 \r\n    batch_size=100\r\n    x_collect = []\r\n    train_loss_collect = []\r\n    train_acc_collect = []\r\n    valid_loss_collect = []\r\n    valid_acc_collect = []\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        saver.restore(sess, \"./save.ckpt\")\r\n        iteration=0\r\n        for e in range(epochs):\r\n              for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\r\n                    iteration+=1\r\n                    feed = {model.inputs: train_x,\r\n                                model.labels: train_y,\r\n                                model.learning_rate: learning_rate_value,\r\n                                model.is_training:True,\r\n                                }\r\n                    train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], \r\n                            feed_dict=feed)\r\n                    \r\n                    if iteration % train_collect == 0:\r\n                        x_collect.append(e)\r\n                        train_loss_collect.append(train_loss)\r\n                        train_acc_collect.append(train_acc)\r\n                    \r\n                        if iteration % train_print==0:\r\n                            print(\"Epoch: {}/{}\".format(e + 1, epochs),\r\n                            \"Train Loss: {:.4f}\".format(train_loss),\r\n                            \"Train Acc: {:.4f}\".format(train_acc))\r\n                        \r\n                        feed = {model.inputs: valid_x,\r\n                                    model.labels: valid_y,\r\n                                    model.is_training:False\r\n                                    }\r\n                        val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\r\n                        valid_loss_collect.append(val_loss)\r\n                        valid_acc_collect.append(val_acc)\r\n                                       \r\n                        if iteration % train_print==0:\r\n                            print(\"Epoch: {}/{}\".format(e + 1, epochs),\r\n                           \"Validation Loss: {:.4f}\".format(val_loss),\r\n                           \"Validation Acc: {:.4f}\".format(val_acc))\r\n                \r\n\r\n    saver.save(sess, \"./save.ckpt\")\r\n\r\n#here is the OUTPUT of training:\r\n      \r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n    \r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 1/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 1/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     Epoch: 2/10 Validation Loss: 0.0000 Validation Acc: 0.4949\r\n     Epoch: 2/10 Train Loss: 0.0000 Train Acc: 0.5013\r\n     ...\r\n(I'm getting the same result as above till epoch 10/10)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I've updated the information.\r\nCan anyone please explain", "@vkmenon I have used ReLU bro. First I've put None but after batch_normalization, I've used tf.nn.relu. So, it should work.\r\n      \r\n       fc = tf.layers.dense(inputs, hidden_units_1, activation=None ,kernel_initializer=initializer)\r\n       fc=tf.layers.batch_normalization(fc, training=is_training)\r\n       fc=tf.nn.relu(fc)", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17854, "title": "tf.estimator.RunConfig return worker is not a valid task_type in the cluster_spec job", "body": "### System information\r\n- os/ubuntu1604/x86_64\r\n- **Exact command to reproduce**: config = tf.estimator.RunConfig()\r\n- docker image: tensorflow/tensorflow:1.4.0-gpu\r\n\r\n### Describe the problem\r\ntf.estimator.RunConfig return worker is not a valid task_type in the cluster_spec job\r\n\r\n### Source code / logs\r\n   os.environ['TF_CONFIG'] = json.dumps({\r\n    ##'cluster': cluster,\r\n    'cluster': {\r\n        \"chief\" : chief_node,\r\n        \"ps_hosts\": ps_hosts,\r\n        \"worker_hosts\": worker_hosts\r\n    },\r\n    'task' : {\r\n        'type' : FLAGS.job_name,\r\n        'index': FLAGS.task_index,\r\n    }\r\n})\r\nconfig = tf.estimator.RunConfig()\r\n\r\n-------LOG---------\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/run_config.py\", line 464, in __init__\r\n    self._init_distributed_setting_from_environment_var(tf_config)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/run_config.py\", line 480, in _init_distributed_setting_from_environment_var\r\n    self._cluster_spec, task_env, TaskType.CHIEF)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/run_config.py\", line 188, in _validate_task_type_and_task_id\r\n    'variable.' % (task_type, cluster_spec))\r\nValueError: worker is not a valid task_type in the cluster_spec:\r\nClusterSpec({'chief': ['10.0.0.5:2223'], 'ps_hosts': ['10.0.0.5:2222'], 'worker_hosts': ['10.0.0.6:2222', '10.0.0.4:2222']})\r\n", "comments": ["stupid mistake on cluster spec naming"]}, {"number": 17853, "title": "cannot decode FixedLenFeature with decode_raw", "body": "I try to parse data from tfrecord file with Dataset, and when I try to parse image with decode_raw, it throws an error.  I use tf1.6, the code is as following:\r\n\r\n```\r\ndef _parse_tfrecords_func(record):\r\n    \"\"\"\r\n    :param record:\r\n    :return:\r\n    \"\"\"\r\n    features = {\"img\": tf.FixedLenFeature([],tf.string),\r\n                \"label\": tf.FixedLenFeature((), tf.int64, default_value=0),\r\n                \"width\": tf.FixedLenFeature((), tf.int64, default_value=0),\r\n                \"height\": tf.FixedLenFeature((), tf.int64, default_value=0),\r\n                \"channel\": tf.FixedLenFeature((), tf.int64, default_value=0)}\r\n    parsed_features = tf.parse_single_example(record, features)\r\n    for keys in parsed_features:\r\n        print(keys)\r\n\r\n    print(type(features['img']))\r\n    print(dir(features['img']))\r\n    img = tf.decode_raw(features['img'], tf.uint8)  # TODO: fix the error\r\n    img_reshape = tf.reshape(img, (parsed_features['width'], parsed_features['height'], parsed_features['channel']))\r\n    return img_reshape, parsed_features['label']\r\n\r\ndef dataset_tfrecords():\r\n    \"\"\"\r\n    Dataset\u8bfbtfrecords\r\n    :return:\r\n    \"\"\"\r\n    tfrecords_files = ['tfrecords_example']\r\n    dataset = tf.data.TFRecordDataset(tfrecords_files)\r\n    dataset = dataset.map(_parse_tfrecords_func)\r\n    dataset.repeat()\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n    next_elem = iterator.get_next()\r\n    sess = tf.Session()\r\n    sess.run(iterator.initializer)\r\n\r\n    for i in range(1):\r\n        next_elem = sess.run(next_elem)\r\n        print(type(next_elem))\r\n```\r\nlabel, width, height, channel are parsed succeed, however, decode image with decode_raw causes an error, error information is as following:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1036, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 235, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 214, in constant\r\n    value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 433, in make_tensor_proto\r\nimg\r\nchannel\r\nheight\r\nlabel\r\nwidth\r\n<class 'tensorflow.python.ops.parsing_ops.FixedLenFeature'>\r\n['__add__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_fields', '_make', '_replace', '_source', '_tf_api_names', 'count', 'default_value', 'dtype', 'index', 'shape']\r\n    _AssertCompatible(values, dtype)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 344, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected string, got tf.string of type 'DType' instead.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"test_dataset.py\", line 99, in <module>\r\n    dataset_tfrecords()\r\n  File \"test_dataset.py\", line 75, in dataset_tfrecords\r\n    dataset = dataset.map(_parse_tfrecords_func)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 790, in map\r\n    return MapDataset(self, map_func)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1597, in __init__\r\n    self._map_func.add_to_graph(ops.get_default_graph())\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 486, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 321, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\function.py\", line 338, in _create_definition_if_needed_impl\r\n    outputs = self._func(*inputs)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1562, in tf_map_func\r\n    ret = map_func(nested_args)\r\n  File \"test_dataset.py\", line 64, in _parse_tfrecords_func\r\n    img = tf.decode_raw(features['img'], tf.uint8)  # TODO: fix the error\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_parsing_ops.py\", line 214, in decode_raw\r\n    little_endian=little_endian, name=name)\r\n  File \"D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 519, in _apply_op_helper\r\n    repr(values), type(values).__name__))\r\nTypeError: Expected string passed to parameter 'bytes' of op 'DecodeRaw', got FixedLenFeature(shape=[], dtype=tf.string, default_value=None) of type 'FixedLenFeature' instead.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I think I have made a mistake in this function, just ignore this issue, thanks for attention."]}, {"number": 17852, "title": "TF1.6/1.7 PS/Worker Distributed Run Failed with \"UnavailableError: OS Error\" when jobs are not running on current machine", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: official 1.6.0 release binary, or build from master branch (with latest commit: 47407ccb99a61fd5115130020ff8ef5ef9272433)\r\n- **TensorFlow version (use command below)**: 1.6.0 official release or master\r\n- **Python version**: python 3.5 or python 2.7\r\n- **Bazel version (if compiling from source)**:   0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**:  Tesla K80, 12206MiB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.6.0-rc1-1503-g47407cc', '1.6.0')\r\n\r\n### Describe the problem\r\n\r\n#### The expected behavior\r\n\r\nThe below source code utilized ps/worker mode to do some training, for usage: we need to run\r\n\r\n> python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'\r\n>  \r\n>   python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'worker'\r\n\r\nrespectively on \"ps job\" machine and \"worker job\" machine. \r\n\r\nIf we run the script firstly on ps, normally, it will wait for worker machine ready, before going furthur, the log is as below:\r\n\r\n> \r\n>  2018-03-20 05:49:40.410488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:14416}\r\n> 2018-03-20 05:49:40.410614: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.4:14417}\r\n> 2018-03-20 05:49:40.418149: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416\r\n> ps 0, create done queue\r\n> ps 0, running\r\n> 2018-03-20 05:49:50.430531: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:00.430728: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:10.430943: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:20.431080: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:30.431351: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> ^C2018-03-20 05:50:40.434895: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:50:50.435104: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n> 2018-03-20 05:51:00.435244: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n\r\nThen we run the script on worker machine, the two machines communicated and coordinated to get things done. \r\n \r\n#### The problem\r\n It works pretty well on tf1.5/1.4 or earlier version, but on latest 1.6.0 release version (and i also tried to build from master source code), it failed for sometimes. I did some investigation and testing, here are the symptoms:\r\n\r\n- If the specified ps/worker-hosts are having the same ip as current machine running the scripts (e.g. ps/worker are running different ports of current machine), everything is just fine, they works. \r\n\r\n- If the specified ps/worker-hosts are having the same ip (we call it A-IP), but different with current machine, even though current machine can ping successfully the  A-IP, but will failed. The error log after starting ps task (with **python mnist_replica.py --data_dir /tmp/tensorflow/mnist/input_data --task_index 0 --ps_hosts '10.0.1.5:14416' --worker_hosts '10.0.1.4:14417' --job_name 'ps'**):\r\n\r\n> 2018-03-20 05:57:29.228323: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:14416}\r\n> 2018-03-20 05:57:29.228478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.1.4:14417}\r\n> 2018-03-20 05:57:29.229155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:14416\r\n> ps 0, create done queue\r\n> ps 0, running\r\n> I0320 05:57:29.309552659    3803 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525449.309441854\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:29.309786369    3803 subchannel.cc:484]          Retry in 998 milliseconds\r\n> I0320 05:57:30.307312499    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:30.308555551    3804 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525450.308464247\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:30.308750759    3804 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:31.307171978    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:31.308303225    3802 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525451.308214021\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:31.308338927    3802 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:32.307163816    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:32.308250261    3801 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525452.308164957\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:32.308284662    3801 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:33.307136307    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> I0320 05:57:33.308314356    3806 subchannel.cc:677]          Connect failed: {\"created\":\"@1521525453.308215652\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\n> I0320 05:57:33.308375658    3806 subchannel.cc:484]          Retry in 999 milliseconds\r\n> I0320 05:57:34.307172752    3796 subchannel.cc:437]          Failed to connect to channel, retrying\r\n> 2018-03-20 05:57:34.308793: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\r\n> Traceback (most recent call last):\r\n>   File \"mnist_replica.py\", line 304, in <module>\r\n>     main(args)\r\n>   File \"mnist_replica.py\", line 102, in main\r\n>     sess.run(queue.dequeue())\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run\r\n>     run_metadata_ptr)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n>     options, run_metadata)\r\n>   File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\n- If the specified ps/worker-hosts are having different ips (in the same LAN, can ping successfully each other), errors on starting on ps worker is similar with the second situation.  \r\n\r\n- The exception happens in MasterSession initilization ( i guess there needs some communication via grpc there)\r\n\r\n#### My personal thinking\r\n\r\nTo be honest, i am wondering whether the gRPC upgrade (that was [introduced](https://github.com/tensorflow/tensorflow/commit/cb498995bf3499d3dd4a6edad407590af12ac3bd) since v1.6rc0 ) did the trick, but since I am pretty new to this component, **besides i am not sure whether somebody else have the similar issues (while I think people using tf1.6 and master will suffer from this on distribute run).**\r\n\r\nThat would be great if any experts can share some insights or thoughts. Thanks in advance!!!\r\n\r\n### Source code / logs\r\n\r\nsource code: \r\n\r\n`from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport math\r\nimport sys\r\nimport tempfile\r\nimport time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nIMAGE_PIXELS = 28\r\n\r\ndef create_done_queue(ps_task_index, worker_count):\r\n    \"\"\"Queue used to signal death for i'th ps shard. Intended to have \r\n    all workers enqueue an item onto it to signal doneness.\"\"\"\r\n\r\n    with tf.device(\"/job:ps/task:%d/cpu:0\" % (ps_task_index)):\r\n        return tf.FIFOQueue(worker_count, tf.int32, shared_name=\"done_queue\" + str(ps_task_index))\r\n\r\n\r\ndef create_done_queues(ps_count, worker_count):\r\n    return [create_done_queue(ps_task_index, worker_count) for ps_task_index in range(ps_count)]\r\n\r\n\r\ndef main(args):\r\n    mnist = input_data.read_data_sets(args.input_training_data_path, one_hot=True)\r\n    if args.download_only:\r\n        sys.exit(0)\r\n\r\n    if args.job_name is None or args.job_name == \"\":\r\n        raise ValueError(\"Must specify an explicit `job_name`\")\r\n    if args.task_index is None or args.task_index == \"\":\r\n        raise ValueError(\"Must specify an explicit `task_index`\")\r\n\r\n    print(\"job name = %s\" % args.job_name)\r\n    print(\"task index = %d\" % args.task_index)\r\n\r\n    # Construct the cluster and start the server\r\n    ps_spec = args.ps_hosts.split(\",\")\r\n    worker_spec = args.worker_hosts.split(\",\")\r\n\r\n    # Get the number of workers.\r\n    num_workers = len(worker_spec)\r\n    num_pss = len(ps_spec)\r\n\r\n    cluster = tf.train.ClusterSpec({\r\n        \"ps\": ps_spec,\r\n        \"worker\": worker_spec})\r\n\r\n    if not args.existing_servers:\r\n        # Not using existing servers. Create an in-process server.\r\n        server = tf.train.Server(\r\n            cluster, job_name=args.job_name, task_index=args.task_index, protocol=args.protocol)\r\n        if args.job_name == \"ps\":\r\n            config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\r\n            sess = tf.Session(server.target, config=config)\r\n\r\n            print(\"ps %d, create done queue\" % args.task_index)\r\n            queue = create_done_queue(args.task_index, num_workers)\r\n\r\n            print(\"ps %d, running\" % args.task_index)\r\n            for i in range(num_workers):\r\n                sess.run(queue.dequeue())\r\n                print(\"ps %d received worker %d done\" % (args.task_index, i))\r\n\r\n            print(\"all workers are done, ps %d: exit\" % (args.task_index))\r\n            sys.exit()\r\n\r\n    is_chief = (args.task_index == 0)\r\n    if args.num_gpus > 0:\r\n        # Avoid gpu allocation conflict: now allocate task_num -> #gpu\r\n        # for each worker in the corresponding machine\r\n        gpu = (args.task_index % args.num_gpus)\r\n        worker_device = \"/job:worker/task:%d/gpu:%d\" % (args.task_index, gpu)\r\n    elif args.num_gpus == 0:\r\n        # Just allocate the CPU to worker server\r\n        cpu = 0\r\n        worker_device = \"/job:worker/task:%d/cpu:%d\" % (args.task_index, cpu)\r\n\r\n    print(\"worker %d, worker_device=%s\" % (args.task_index, worker_device))\r\n    print(\"worker %d, create done queue\" % args.task_index)\r\n    queues = create_done_queues(num_pss, num_workers)\r\n    print(\"worker %d, done queue created\" % args.task_index)\r\n\r\n    # The device setter will automatically place Variables ops on separate\r\n    # parameter servers (ps). The non-Variable ops will be placed on the workers.\r\n    # The ps use CPU and workers use corresponding GPU\r\n\r\n    with tf.device(\r\n            tf.train.replica_device_setter(\r\n                worker_device=worker_device,\r\n                ps_device=\"/job:ps/cpu:0\",\r\n                cluster=cluster)):\r\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n\r\n        # Variables of the hidden layer\r\n        hid_w = tf.Variable(\r\n            tf.truncated_normal(\r\n                [IMAGE_PIXELS * IMAGE_PIXELS, args.hidden_units],\r\n                stddev=1.0 / IMAGE_PIXELS),\r\n            name=\"hid_w\")\r\n        hid_b = tf.Variable(tf.zeros([args.hidden_units]), name=\"hid_b\")\r\n\r\n        # Variables of the softmax layer\r\n        sm_w = tf.Variable(\r\n            tf.truncated_normal(\r\n                [args.hidden_units, 10],\r\n                stddev=1.0 / math.sqrt(args.hidden_units)),\r\n            name=\"sm_w\")\r\n        sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\r\n\r\n        # Ops: located on the worker specified with args.task_index\r\n        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\r\n        y_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\n        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\r\n        hid = tf.nn.relu(hid_lin)\r\n\r\n        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\r\n        cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\r\n\r\n        opt = tf.train.AdamOptimizer(args.learning_rate)\r\n\r\n        if args.sync_replicas:\r\n            if args.replicas_to_aggregate is None:\r\n                replicas_to_aggregate = num_workers\r\n            else:\r\n                replicas_to_aggregate = args.replicas_to_aggregate\r\n\r\n            opt = tf.train.SyncReplicasOptimizer(\r\n                opt,\r\n                replicas_to_aggregate=replicas_to_aggregate,\r\n                total_num_replicas=num_workers,\r\n                name=\"mnist_sync_replicas\")\r\n\r\n        train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n\r\n        if args.sync_replicas:\r\n            local_init_op = opt.local_step_init_op\r\n            if is_chief:\r\n                local_init_op = opt.chief_init_op\r\n\r\n            ready_for_local_init_op = opt.ready_for_local_init_op\r\n\r\n            # Initial token and chief queue runners required by the sync_replicas mode\r\n            chief_queue_runner = opt.get_chief_queue_runner()\r\n            sync_init_op = opt.get_init_tokens_op()\r\n\r\n        init_op = tf.global_variables_initializer()\r\n        train_dir = tempfile.mkdtemp()\r\n\r\n        enq_ops = []\r\n        for q in queues:\r\n            qop = q.enqueue(1)\r\n            enq_ops.append(qop)\r\n    if args.sync_replicas:\r\n        sv = tf.train.Supervisor(\r\n            is_chief=is_chief,\r\n            logdir=train_dir,\r\n            init_op=init_op,\r\n            local_init_op=local_init_op,\r\n            ready_for_local_init_op=ready_for_local_init_op,\r\n            recovery_wait_secs=1,\r\n            global_step=global_step)\r\n    else:\r\n        sv = tf.train.Supervisor(\r\n            is_chief=is_chief,\r\n            logdir=train_dir,\r\n            init_op=init_op,\r\n            recovery_wait_secs=1,\r\n            global_step=global_step)\r\n\r\n    sess_config = tf.ConfigProto(\r\n        allow_soft_placement=True,\r\n        log_device_placement=False,\r\n        device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % args.task_index])\r\n    if args.infer_shapes == True:\r\n        sess_config.graph_options.infer_shapes = args.infer_shapes\r\n\r\n    # The chief worker (task_index==0) session will prepare the session,\r\n    # while the remaining workers will wait for the preparation to complete.\r\n    if is_chief:\r\n        print(\"Worker %d: Initializing session...\" % args.task_index)\r\n    else:\r\n        print(\"Worker %d: Waiting for session to be initialized...\" %\r\n              args.task_index)\r\n\r\n\t\t\t  \r\n    if args.existing_servers:\r\n        server_grpc_url = \"grpc://\" + worker_spec[args.task_index]\r\n        print(\"Using existing server at: %s\" % server_grpc_url)\r\n\r\n        sess = sv.prepare_or_wait_for_session(server_grpc_url,\r\n                                              config=sess_config)\r\n    else:\r\n        sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)\r\n\r\n    print(\"Worker %d: Session initialization complete.\" % args.task_index)\r\n\r\n    if args.sync_replicas and is_chief:\r\n        # Chief worker will start the chief queue runner and call the init op.\r\n        sess.run(sync_init_op)\r\n        sv.start_queue_runners(sess, [chief_queue_runner])\r\n\r\n    # Perform training\r\n    time_begin = time.time()\r\n    print(\"Training begins @ %f\" % time_begin)\r\n\r\n    local_step = 0\r\n    while True:\r\n        # Training feed\r\n        batch_xs, batch_ys = mnist.train.next_batch(args.batch_size)\r\n        train_feed = {x: batch_xs, y_: batch_ys}\r\n\r\n        _, step = sess.run([train_step, global_step], feed_dict=train_feed)\r\n        local_step += 1\r\n\r\n        now = time.time()\r\n        print(\"%f: Worker %d: training step %d done (global step: %d)\" %\r\n              (now, args.task_index, local_step, step))\r\n\r\n        if step >= args.train_steps:\r\n            break\r\n\r\n    time_end = time.time()\r\n    print(\"Training ends @ %f\" % time_end)\r\n    training_time = time_end - time_begin\r\n    print(\"Training elapsed time: %f s\" % training_time)\r\n\r\n    # Validation feed\r\n    val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\r\n    val_xent = sess.run(cross_entropy, feed_dict=val_feed)\r\n    print(\"After %d training step(s), validation cross entropy = %g\" %\r\n          (args.train_steps, val_xent))\r\n\r\n    for op in enq_ops:\r\n        sess.run(op)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--input-training-data-path\", default=\"/tmp/mnist-data\")\r\n    parser.add_argument(\"--input_training_data_path\", default=\"/tmp/mnist-data\")\r\n    parser.add_argument(\"--download_only\", type=bool, default=False)\r\n    parser.add_argument(\"--task-index\", type=int)\r\n    parser.add_argument(\"--task_index\", type=int)\r\n    parser.add_argument(\"--num_gpus\", type=int, default=1)\r\n    parser.add_argument(\"--replicas_to_aggregate\", type=int)\r\n    parser.add_argument(\"--hidden_units\", type=int, default=100)\r\n    parser.add_argument(\"--train_steps\", type=int, default=200)\r\n    parser.add_argument(\"--batch_size\", type=int, default=100)\r\n    parser.add_argument(\"--learning_rate\", type=float, default=0.01)\r\n    parser.add_argument(\"--sync_replicas\", type=bool, default=False)\r\n    parser.add_argument(\"--existing_servers\", type=bool, default=False)\r\n    parser.add_argument(\"--ps-hosts\", default=\"localhost:2222\")\r\n    parser.add_argument(\"--ps_hosts\", default=\"localhost:2222\")\r\n    parser.add_argument(\"--worker-hosts\", default=\"localhost:2223,localhost:2224\")\r\n    parser.add_argument(\"--worker_hosts\", default=\"localhost:2223,localhost:2224\")\r\n    parser.add_argument(\"--job-name\")\r\n    parser.add_argument(\"--job_name\")\r\n    parser.add_argument(\"--protocol\", default=\"grpc\")\r\n    parser.add_argument(\"--infer_shapes\", type=bool, default=False)\r\n\r\n    (args, unknown) = parser.parse_known_args()\r\n    main(args)`\r\n", "comments": ["Update: I revert the grpc upgrade change locally, and i can start the ps task successfully now. \r\n\r\nI believe for users who are suffering this, this might be considered as a workaround. \r\n\r\nI would like to heard some feedbacks from you guys as experts.  :) ", "A different user is reporting something similar on SO: https://stackoverflow.com/questions/49403392/unavailableerror-os-error-while-training-on-gc-ml-engine", "Hi, guy from @rhaertel80 SO link here,\r\nI have since lowered the version to 1.5. But now I'm experiencing a different problem similar to [this one](https://github.com/GoogleCloudPlatform/cloudml-samples/issues/175) where I have posted some more details. \r\nThis might be completely unrelated, though", "@simpeng which gRPC commit are you going back to? \r\n\r\nThis looks like the PS is simply not up when you first try to connect. The resulting error should be handled, the most surprising thing here is that a gRPC downgrade helps resolve this. Is this repeatable?", "@martinwicke , yes it's repeatable, I did twice ( one on my local machine for debugging, the other one is on another build machine). \r\n\r\nMy testing case is: \r\n\r\n- Start the PS job first, normally it keep waiting for response from worker, util worker get started. \r\n\r\n> 2018-03-20 05:49:50.430531: I tensorflow/core/distributed_runtime/master.cc:221] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n\r\n- With 1.6, it will try to \"connect to the channel\" and retry few times, then throw exception:\r\n\r\n> {\"created\":\"@1521525453.308215652\",\"description\":\"Failed to connect to remote host: OS Error\",\"errno\":111,\"file\":\"external/grpc/src/core/lib/iomgr/tcp_client_posix.cc\",\"file_line\":198,\"os_error\":\"Connection refused\",\"syscall\":\"connect\",\"target_address\":\"ipv4:10.0.1.4:14417\"}\r\nI0320 05:57:33.308375658 3806 subchannel.cc:484] Retry in 999 milliseconds\r\nI0320 05:57:34.307172752 3796 subchannel.cc:437] Failed to connect to channel, retrying\r\n2018-03-20 05:57:34.308793: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\r\nTraceback (most recent call last):\r\nFile \"mnist_replica.py\", line 304, in \r\nmain(args)\r\nFile \"mnist_replica.py\", line 102, in main\r\nsess.run(queue.dequeue())\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 905, in run\r\nrun_metadata_ptr)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\nfeed_dict_tensor, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\noptions, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\nThe change I reverted is the [gRPC commit](https://github.com/tensorflow/tensorflow/commit/cb498995bf3499d3dd4a6edad407590af12ac3bd).\r\n", "I am struggling with the same error with Tensorflow 1.6.The same error has been reported elsewhere with Tensorflow 1.6\r\nhttps://github.com/yahoo/TensorFlowOnSpark/issues/245\r\n\r\nIt seems to be an OOM error. \r\n\r\nAny suggested workarounds ? How can I change the grpc version if I am using the gcloud ml-engine.\r\nI believe they come pre-packaged in the runtime environment.\r\n\r\nAny help will be appreciated. \r\n\r\n", "I can confirm that I get an OOM error with a different signature, if I run the same config on a single gcloud ml-engine instance(with no GPUS)\r\n`The replica master 0 ran out-of-memory and exited with a non-zero status of 247.`", "The issue repro on tf1.7 release, and the workaround(revert grpc change) still work till now. \r\n\r\nI am quite curious anybody else who use distributed tf, don't really have the exact same repro? (to be clear, the initial issue has nothing to do with OOM).", "Nagging Assignee @jart: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "@simpeng I had the same error. Could you tell me which version of grpc you downgrade to and what version of tensorflow are you using? thank you", "@simpeng Also, could you show a little on how to downgrade gprc? I mannually use pip uninstall grpcio and pip install grpcio==1.0.0, but it seems still reporting same error. I wonder if this needs to revert grpc somewhere else in tensorflow?", "@tanndx17  I reverted  the gRPC commit: https://github.com/tensorflow/tensorflow/commit/cb498995bf3499d3dd4a6edad407590af12ac3bd, and build from source. \r\n\r\nAFAIK, since tf1.8, the issue is gone. ", "No, the issue is not gone ! Even with 1.8 the issue persists for me.", "@g-hrafiq what is you issue?  I still have \"UnavailableError: OS Error\" here. My running command is\r\n\r\nOne server 1 (172.19.120.41), I run:\r\nCUDA_VISIBLE_DEVICES=0 python3 mnist_2.py \\\r\n  --ps_hosts=localhost:2222 \\\r\n  --worker_hosts=172.19.120.39:2223 \\\r\n  --job_name=ps --task_index=0\r\n\r\nOne server 2 (172.19.120.39), I run:\r\nCUDA_VISIBLE_DEVICES=0 python3 mnist_2.py \\\r\n  --ps_hosts=172.19.120.41:2222 \\\r\n  --worker_hosts=localhost:2223 \\\r\n  --job_name=worker -task_index=0\r\n\r\nThen I still  got the issue here. Just FYI, I installed tensorflow from pip", "Today I ran an object detection model retrain via CMLE using runtime 1.8 and below config.yml file, training ran fine for 11000 steps for first time then errored out \"UnavailableError: OS Error\". I restarted the job later, then it ran till 60000 until error.\r\n\r\n```\r\ntrainingInput:\r\n  runtimeVersion: \"1.8\"\r\n  scaleTier: CUSTOM\r\n  masterType: standard_gpu\r\n  workerCount: 5\r\n  workerType: standard_gpu\r\n  parameterServerCount: 3\r\n  parameterServerType: standard\r\n```", "I am getting this same error. seems to be a lot of tickets for this issue - has no one solved it yet?", "As someone mentioned above, this got solved for me by using a larger memory instance, I changed from \r\n\r\n \"masterType\": \"complex_model_m\",\r\nto \r\n \"masterType\": \"large_model\",", "This has been troubling me for a while. I found out that the problem is that GRPC uses the native \"**epoll**\" polling engine for communication. Changing this to a portable polling engine solved this issue for me. The way to do is to set the environment variable, \"**GRPC_POLL_STRATEGY=poll**\" before running the tensorflow programs. This solved this issue for me. For reference, see, https://github.com/grpc/grpc/blob/master/doc/environment_variables.md. ", "@nerdyalbin thanks for the sharing! Was the default polling engine changed for recently GRPC? Or maybe epoll behaving differently (compared with before) caused the issues coming? ", "> This has been troubling me for a while. I found out that the problem is that GRPC uses the native \"**epoll**\" polling engine for communication. Changing this to a portable polling engine solved this issue for me. The way to do is to set the environment variable, \"**GRPC_POLL_STRATEGY=poll**\" before running the tensorflow programs. This solved this issue for me. For reference, see, https://github.com/grpc/grpc/blob/master/doc/environment_variables.md.\r\n\r\nsame problem, it works! ^-^", "> This has been troubling me for a while. I found out that the problem is that GRPC uses the native \"**epoll**\" polling engine for communication. Changing this to a portable polling engine solved this issue for me. The way to do is to set the environment variable, \"**GRPC_POLL_STRATEGY=poll**\" before running the tensorflow programs. This solved this issue for me. For reference, see, https://github.com/grpc/grpc/blob/master/doc/environment_variables.md.\r\n\r\ncould you tell how to set the environment variable? thanks very much", "> > This has been troubling me for a while. I found out that the problem is that GRPC uses the native \"**epoll**\" polling engine for communication. Changing this to a portable polling engine solved this issue for me. The way to do is to set the environment variable, \"**GRPC_POLL_STRATEGY=poll**\" before running the tensorflow programs. This solved this issue for me. For reference, see, https://github.com/grpc/grpc/blob/master/doc/environment_variables.md.\r\n> \r\n> could you tell how to set the environment variable? thanks very much\r\n\r\nimport os\r\nos.environ['GRPC_POLL_STRATEGY'] = \"poll\"", "> > > This has been troubling me for a while. I found out that the problem is that GRPC uses the native \"**epoll**\" polling engine for communication. Changing this to a portable polling engine solved this issue for me. The way to do is to set the environment variable, \"**GRPC_POLL_STRATEGY=poll**\" before running the tensorflow programs. This solved this issue for me. For reference, see, https://github.com/grpc/grpc/blob/master/doc/environment_variables.md.\r\n> > \r\n> > \r\n> > could you tell how to set the environment variable? thanks very much\r\n> \r\n> import os\r\n> os.environ['GRPC_POLL_STRATEGY'] = \"poll\"\r\n\r\nit works! thank for you help!"]}, {"number": 17850, "title": "Official/nightly builds for arm32 and arm64 on Linux", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux (debian:stretch) arm32v7 and arm64v7\r\n- **TensorFlow installed from (source or binary)**:  n/a\r\n- **TensorFlow version (use command below)**:  1.5.0 - 1.7.0\r\n- **Python version**:  n/a\r\n- **Bazel version (if compiling from source)**:  n/a\r\n- **GCC/Compiler version (if compiling from source)**:  n/a\r\n- **CUDA/cuDNN version**:  n/a\r\n- **GPU model and memory**:  n/a\r\n- **Exact command to reproduce**:  n/a\r\n\r\n### Describe the problem\r\nThere has been significant community effort to build Tensorflow from source for ARM (e.g. https://github.com/samjabrahams/tensorflow-on-raspberry-pi and https://github.com/lhelontra/tensorflow-on-arm).  It would be great to get these as part of the official/nightly builds.  Python wheel/libtensorflow.so/libtensorflow_framework.so binaries.\r\n\r\n### Source code / logs\r\nn/a\r\n", "comments": ["/CC @gunan, any plans for this?", "I think we have a nightly build for raspberry pi at the moment, but for arm, not sure if we do anything separate. \r\n@petewarden to comment on these plans.", "Those nightly builds are here:\r\n\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero/\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-pi-zero-python3/\r\n\r\nHopefully the pi builds will work on my DE1-SoC's Cyclone V. :)", "Thanks @gunan and @johndeppe.  It would be great to get nightly builds of libtensorflow.so and libtensorflow_framework.so that are cross-compiled ala the raspberry pi builds.  I'm really looking to get these libs extend support for Tensorflow on ARM in other languages (e.g. C#).  Ideally, the nightly libtensorflow build would produce bits for cpu-armhf and cpu-aarm64 to start with.\r\n\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-libtensorflow/", "@gunan if you need build infrastructure for arm64 (64-bit only) please let me know, we have equipment in the Works on Arm cluster and I'd love to see Tensorflow supported on these platforms.\r\n\r\nhttp://github.com/worksonarm/cluster/issues/new", "What I ran into getting an arm64 build of libtensorflow.so https://github.com/tensorflow/tensorflow/issues/16795", "Hi guys,\r\nMaybe I'm wrong but this nightly build is no working since more than a month because has run out of space http://ci.tensorflow.org/view/Nightly/job/nightly-pi/\r\nI've seen the same issue since 22-abr-2018 ==> http://ci.tensorflow.org/view/Nightly/job/nightly-pi/254/console \r\nPlease, could someone check this?\r\nThanks you,", "ci.tensorflow.org is deprecated.\r\nWe have a replacement ready, the only remaining piece is we need to expose the binaries built by the new system.", "I wish it would be soon, I loves to have my docker image as updated as possible https://hub.docker.com/r/elswork/rpi-tensorflow/", "Are we there yet? :grin:", "It seems http://ci.tensorflow.org/view/Nightly/job/nightly-pi/ is working again, thanks you so much.", "Since 2018/06/07 build works for Rpi where disabled. No official news about that.\r\nNow http://ci.tensorflow.org/job/nightly-pi/ is working again, I feel anxious to view the output.\r\nhttp://ci.tensorflow.org/job/nightly-pi/302/console\r\nToo many mysteries and few information for my poor heart.", "OMG!!! :astonished:\r\nI've been watching build process in real time, it was more interesting than latest Google I/O:\r\n\r\n[13,134 / 13,135] Linking tensorflow/python/_pywrap_tensorflow_internal.so; 50s local\r\nINFO: Elapsed time: 1078.358s, Critical Path: 156.88s\r\nINFO: 4344 processes, local.\r\nINFO: Build completed successfully, 4457 total actions\r\nINFO: Build completed successfully, 4457 total actions\r\nFinal outputs will go to output-artifacts\r\nTue Jun 12 12:49:36 UTC 2018 : === Preparing sources in dir: /tmp/tmp.Nw0ujz9Yyt\r\nTue Jun 12 12:49:37 UTC 2018 : === Building wheel\r\nwarning: no files found matching '*.dll' under directory '*'\r\n...\r\nwarning: no files found matching '*' under directory 'tensorflow/include/unsupported'\r\nTue Jun 12 12:49:51 UTC 2018 : === Output wheel file is in: /workspace/output-artifacts\r\nOutput can be found here:\r\noutput-artifacts\r\noutput-artifacts/tensorflow-1.9.0rc0-cp27-none-linux_armv7l.whl\r\noutput-artifacts/benchmark_model\r\noutput-artifacts/libtensorflow_framework.so\r\noutput-artifacts/libtensorflow.so\r\nArchiving artifacts\r\nFinished: SUCCESS  :sparkles:", "The builds are ready, but we are still exploring ways to create public links for these.", "@gunan - any update on the ways to create public links for these artifacts? thank you.", "Sorry, still nothing from the downstream teams.\r\nI will probably manually copy them to gcs.", "Hi @gunan - where do the downstream teams do their work? I am happy to engage them there.", "They are a part of google's new cloud build tools. I think all of their work is done internally at google though.", "Looks like the features I need were pushed, but I missed the updates there.\r\nI am trying to generate the new build configs and update README now.", "This fixes the armv7 builds, but if I read the diff in ae3cefb correctly, arm64 (64-bit armv8) is not being pushed.", "I am not sure if we have any armv8 builds set up.\r\n@petewarden to see if we have anything for arm v8.", "@petewarden would it make more sense to start a new issue specific to 64-bit Arm (arm64) and close this one which has primarily been for 32-bit Arm?  I find it difficult to determine whether this is a modest-sized task for the build team or a big ask.", "@gunan commit 15a83e5 added the .whl files for using with python, but what about the `libtensorflow.so`/`libtensorflow_framework.so` that can be used on other language bindings, are they available somewhere? \r\n\r\nI'm trying to use Tensorflow with NodeJS on a raspberry pi and I have to cross-compile the libtensorflow.so to be able to use it. But I'm having issues doing that like shown on #24104.\r\n\r\nLike @jessebenson said, making those binaries available will help a lot for adding support for ARM boards on other languages bindings like NodeJS, Golang, .Net Core, etc. ", "We don't currently have the resources to support building library binaries for arm linux unfortunately, so closing this one."]}]