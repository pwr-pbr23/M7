[{"number": 33135, "title": "How to replace Keras' gradients() function with GradientTape in TF2.0?", "body": "With the \"old\" Keras library I created heatmaps for my CNNs using the `keras.backend.gradients()` function, like this:\r\n\r\n    # load model and image, then predict the class this image belongs to\r\n    model = load_model(os.path.join(model_folder, \"custom_model.h5\"))\r\n    image = image.load_img(image_path)\r\n    img_tensor = image.img_to_array(image)\r\n    img_tensor = np.expand_dims(img_tensor, axis=0)\r\n    img_tensor = preprocess_input(img_tensor)\r\n\r\n    preds = model.predict(img_tensor)\r\n    model_prediction = model.output[:, np.argmax(preds[0])]\r\n\r\n    # Calculate pooled grads for heatmap\r\n    conv_layer = model.get_layer(\"block5_conv3\")  # last conv. layer\r\n    grads = K.gradients(model_prediction, conv_layer.output)[0]\r\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\r\n\r\n    # Get values of pooled grads and model conv. layer output as Numpy arrays\r\n    input_layer = model.get_layer(\"model_input\")\r\n    iterate = K.function([input_layer], [pooled_grads, conv_layer.output[0]])\r\n    pooled_grads_value, conv_layer_output_value = iterate([img_tensor])\r\n\r\n    # Continue with heatmap generation ...\r\n\r\nNow I switched to TF2.0 and it's built-in Keras implementation. Everything works fine, however, using that code I get the following error when calling `K.gradients()`:\r\n\r\n    tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n\r\nI did some research and tried to understand how I can make use of `GradientTape`, but unfortunately I don't know much about TF nor TF2.0 - I always worked with Keras. Can you guys guide me how I can make this gradient calculation work again with my setup?", "comments": ["Hi, please have a look at [this](https://www.tensorflow.org/guide/eager#computing_gradients) doc. The use of GradientTape is described there. \r\nMaybe these lines will already solve your problem:\r\n```\r\nw = tf.Variable([[1.0]])\r\nwith tf.GradientTape() as tape:\r\n  loss = w * w\r\n\r\ngrad = tape.gradient(loss, w)\r\nprint(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)\r\n```", "> Hi, please have a look at [this](https://www.tensorflow.org/guide/eager#computing_gradients) doc. The use of GradientTape is described there.\r\n> Maybe these lines will already solve your problem:\r\n> \r\n> ```\r\n> w = tf.Variable([[1.0]])\r\n> with tf.GradientTape() as tape:\r\n>   loss = w * w\r\n> \r\n> grad = tape.gradient(loss, w)\r\n> print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)\r\n> ```\r\n\r\nSorry, but I can't see how these iines will solve my issue. That's my problem - I guess the `GradientTape` can help me, but I don't **how exactly** I need to use it in my case to achieve the same result as before with Keras' `backend.gradients()` function, as shown in my original post.", "Have you tried disabling the eager mode `tf.compat.v1.disable_eager_execution()`?", "> Have you tried disabling the eager mode `tf.compat.v1.disable_eager_execution()`?\r\n\r\nYes, I did so and that worked.\r\nHowever, that is my plan B. I would rather stick to TF2 eager execution if possible.", "Can you please take a look at this [issue](https://stackoverflow.com/questions/56478454/in-tensorflow-2-0-with-eager-execution-how-to-compute-the-gradients-of-a-networ) and let me know if it helps. Thanks!", "> Can you please take a look at this [issue](https://stackoverflow.com/questions/56478454/in-tensorflow-2-0-with-eager-execution-how-to-compute-the-gradients-of-a-networ) and let me know if it helps. Thanks!\r\n\r\nThat thread might help indeed, thanks. However, I was not able to fully solve my problem yet. I tried Fantasty's answer from the StackOverflow thread, i.e. copied the `watch_layer()` function from his solution over to my program, then updated my code (as shown above) accordingly:\r\n\r\n    # Full code for model generation and data loading & processing see orig. posting above\r\n    model_prediction = model.output[:, np.argmax(preds[0])]\r\n\r\n    with tf.GradientTape() as gtape:\r\n        watch_layer(conv_layer, gtape)\r\n        preds = model.predict(image_data)\r\n        model_prediction = model.output[:, np.argmax(preds[0])]\r\n        grads = gtape.gradient(model_prediction, conv_layer.result)\r\n\r\nBut now the call `preds = model.predict(image_data)` does lead to the following error:\r\n\r\n    LookupError: No gradient defined for operation 'IteratorGetNext' (op type: IteratorGetNext)\r\n\r\nIf I remove the `with tf.GradientTape() as gtape` statement and call `predict()` without it, then that call works fine agan (even though then obviously my original problem comes again).", "Could you try out the version linked by @gowthamkpr above?\r\nYou seem to call `model.predict(input)` however in the stackoverflow answer they use `model(input)`. Additionaly they only use the prediction inside the `with tf.GradientTape() as tape`. Please try to stick to the stackoverflow version.\r\nMaybe this will already solve your problem.", "> Could you try out the version linked by @gowthamkpr above?\r\n> You seem to call `model.predict(input)` however in the stackoverflow answer they use `model(input)`. Additionaly they only use the prediction inside the `with tf.GradientTape() as tape`. Please try to stick to the stackoverflow version.\r\n> Maybe this will already solve your problem.\r\n\r\nI **did** try to predict from within the `with` block, please see my last posting. I moved the call to `.predict()` into that block. Moreover, when I just call `model(image_data)`, as in the StackOverflow version, then the last line\r\n\r\n    grads = gtape.gradient(model_prediction, conv_layer.result)\r\n\r\nleads to `grads` equals `None`, which also does not solve my issue.", "In order to help you it would be required to see the whole file you are working on. Either you upload it on google colab and we can try to sovle your problem, or you ask your question on stackoverflow.", "> In order to help you it would be required to see the whole file you are working on. Either you upload it on google colab and we can try to sovle your problem, or you ask your question on stackoverflow.\r\n\r\nAbsolutely, please find full source code of a minimal working example here:\r\n[https://gist.github.com/haimat/10a53ad9675f8f5ac1290f06c3e4f973](https://gist.github.com/haimat/10a53ad9675f8f5ac1290f06c3e4f973)", "I was able to solve this issue [as described here](https://stackoverflow.com/questions/58322147/how-to-generate-cnn-heatmaps-using-built-in-keras-in-tf2-0-tf-keras).", "Thanks! I was having this exact same issue. \ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\udd96\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a\ud83d\ude0e", "Hi! I tried to follow the example linked by @haimat but I'm still getting a problem:\r\n\r\n```\r\nimage = preprocessing.image.load_img(IMG_PATH, target_size=(150, 150))\r\nimg_tensor = preprocessing.image.img_to_array(image)\r\nimg_tensor = np.expand_dims(img_tensor, axis=0)\r\nimg_tensor = vgg16.preprocess_input(img_tensor)\r\n\r\nconv_layer = model.get_layer(\"block3_conv1\")\r\nheatmap_model = models.Model([model.inputs], [conv_layer.output, model.output])\r\n\r\nwith tf.GradientTape() as gtape:\r\n    conv_output, predictions = heatmap_model(img_tensor)\r\n    loss = predictions[:, np.argmax(predictions[0])]\r\n    grads = gtape.gradient(loss, conv_output)\r\n```\r\n\r\nWhen trying to calculate the loss I got\r\n\r\n```\r\nInvalidArgumentError: slice index 5406 of dimension 1 out of bounds. [Op:StridedSlice] name: strided_slice/\r\n```\r\n\r\n", "see this: https://keras.io/examples/vision/integrated_gradients/"]}, {"number": 33134, "title": "[TF 2.0 API Docs] tf.nn.softmax", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/softmax\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nYes\r\n\r\n### Correct links\r\n\r\nyes\r\n\r\n### Parameters defined\r\n\r\nNo. \r\nPer the code, the first argument \"logits\" can be of any type that can be passed to \"convert_to_tensor()\", not just a tensor. Therefore the documentation can be modified to include \"Tensor objects, numpy arrays, Python lists, and Python scalars\".\r\n\r\n### Returns defined\r\nyes\r\n\r\n### Raises listed and defined\r\nyes\r\n\r\n### Submit a pull request?\r\nyes\r\n", "comments": ["@prabindh ,\r\nAs the PR associated with the issue is merged, can we proceed for the closure ?Thanks!"]}, {"number": 33133, "title": "[TF2.0] tf.data.Dataset.from_generator raises `InvalidArgumentError` when using flow_from_directory", "body": "Hi,\r\nI have an issue when creating tf.data.Dataset with  tf.keras.preprocessing.image.ImageDataGenerator() and flow_from_directory. Seems like the arguments are not passed in the right format. Or maybe I missed something...\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: GeForce GTX 1050Ti Max-Q\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nflowers = tf.keras.utils.get_file(\r\n    'flower_photos',\r\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n    untar=True)\r\n\r\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator()\r\n\r\nBATCH_SIZE = 32\r\nIMG_DIM = 224\r\nNB_CLASSES = 5\r\n\r\n\r\nds = tf.data.Dataset.from_generator(\r\n    img_gen.flow_from_directory, args=[flowers,(IMG_DIM, IMG_DIM),'rgb','categorical',BATCH_SIZE,False], \r\n    output_types=(tf.float32, tf.float32), \r\n    output_shapes = ([BATCH_SIZE,IMG_DIM,IMG_DIM,3],[BATCH_SIZE,NB_CLASSES]))\r\n\r\nit = iter(ds)\r\nbatch = next(it)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-7-5cabd7ffcaf2> in <module>\r\n----> 1 batch = next(it)\r\n\r\n~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)\r\n    620 \r\n    621   def __next__(self):  # For Python 3 compatibility\r\n--> 622     return self.next()\r\n    623 \r\n    624   def _next_internal(self):\r\n\r\n~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)\r\n    664     \"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\r\n    665     try:\r\n--> 666       return self._next_internal()\r\n    667     except errors.OutOfRangeError:\r\n    668       raise StopIteration\r\n\r\n~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)\r\n    649             self._iterator_resource,\r\n    650             output_types=self._flat_output_types,\r\n--> 651             output_shapes=self._flat_output_shapes)\r\n    652 \r\n    653       try:\r\n\r\n~/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py in iterator_get_next_sync(iterator, output_types, output_shapes, name)\r\n   2671       else:\r\n   2672         message = e.message\r\n-> 2673       _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n   2674   # Add nodes to the TensorFlow graph.\r\n   2675   if not isinstance(output_types, (list, tuple)):\r\n\r\n~/anaconda3/envs/classifier/lib/python3.7/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: ValueError: ('Invalid color mode:', b'rgb', '; expected \"rgb\", \"rgba\", or \"grayscale\".')\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 464, in get_iterator\r\n    return self._iterators[iterator_id]\r\n\r\nKeyError: 0\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 221, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 585, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 466, in get_iterator\r\n    iterator = iter(self._generator(*self._args.pop(iterator_id)))\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py\", line 540, in flow_from_directory\r\n    interpolation=interpolation\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/keras_preprocessing/image/directory_iterator.py\", line 93, in __init__\r\n    interpolation)\r\n\r\n  File \"/home/dhassault/anaconda3/envs/classifier/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py\", line 176, in set_processing_attrs\r\n    '; expected \"rgb\", \"rgba\", or \"grayscale\".')\r\n\r\nValueError: ('Invalid color mode:', b'rgb', '; expected \"rgb\", \"rgba\", or \"grayscale\".')\r\n\r\n\r\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]\r\n```\r\n\r\nAnd especially:\r\n```\r\nValueError: ('Invalid color mode:', b'rgb', '; expected \"rgb\", \"rgba\", or \"grayscale\".')\r\n```", "comments": ["Hi, for me this did it:\r\n```\r\nimport tensorflow as tf\r\n\r\nflowers = tf.keras.utils.get_file(\r\n    'flower_photos',\r\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n    untar=True)\r\nBATCH_SIZE = 32\r\nIMG_DIM = 224\r\nNB_CLASSES = 5\r\n\r\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator()\r\ngen = img_gen.flow_from_directory(flowers,(IMG_DIM, IMG_DIM),'rgb',class_mode='categorical',batch_size=BATCH_SIZE,shuffle=False)\r\n\r\nds = tf.data.Dataset.from_generator(lambda: gen, \r\n    output_types=(tf.float32, tf.float32), \r\n    output_shapes = ([BATCH_SIZE,IMG_DIM,IMG_DIM,3],[BATCH_SIZE,NB_CLASSES]))\r\n\r\nit = iter(ds)\r\nbatch = next(it)\r\nprint(batch)\r\n```", "Thanks!! It works fine :) Is it an expected behavior? By reading https://www.tensorflow.org/guide/data#consuming_python_generators I felt what I was doing was logic.", "@dhassault, Looks like issue is resolved.Can you please let us know if you are happy to close if no issue persists. Thanks!", "ds = tf.data.Dataset.from_generator(\r\n    #lambda: gen,                   # This seems to work\r\n    img_gen.flow_from_directory, args=[flowers],     #This doesn't work.\r\n    output_types=(tf.float32, tf.float32), \r\n    output_shapes=([32,256,256,3], [32,5])\r\n)\r\n\r\nAnyone know why? According to code sample and doc, it should work for both."]}, {"number": 33132, "title": "fully_connect cannot  be quantilized", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):1.14.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10/7.0\r\n- GPU model and memory:32G\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ntflite_convert  the pb ,it said fully_connect  lacking min/max\r\n\r\n**Describe the expected behavior**\r\nit should be passed\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\ntflite_convert --output_file landmark.tflite --graph_def_file landmark.pb --inference_type QUANTIZED_UINT8 --input_arrays image_batch --input_shapes 1,112,112,3 --output_arrays pfld_inference/output --std_dev_values 255 --mean_values 0\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nArray pfld_inference/fc/Tensordot, which is an input to the Reshape operator producing the output array pfld_inference/output, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\nFatal Python error: Aborted\r\n", "comments": ["before the fully_connect is a concat", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33132\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33132\">No</a>\n"]}, {"number": 33130, "title": "Cannot find NvInfer.h with TensorRT install", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:1.12.0\r\n- Python version:3.5\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.15\r\n- GCC/Compiler version (if compiling from source):GCC\r\n- CUDA/cuDNN version:9.0\r\n- GPU model and memory:7.1.4\r\n\r\n\r\n\r\n**Describe the problem**\r\n  When I try to compile Tensorflow I met this issue. To solve this problem I first try to modify the TensorRT install path in perform ./configure, but I find path is not changed, for example:\r\n           default path:  /usr/lib/x86_64-linux-gnu\r\n           I set path is: /usr/local/TensorRT\r\n This is useless, I don't know why,\r\nSo, I second try to copy NvInfer.h to /usr/lib/x86_64-linux-gnu, but this problem stall exist, and I find NvInfer.h is exist in /usr/lib/x86_64-linux-gnu, why cody can't find this file, I feel confused.\r\n \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl\", line 171\r\n                _trt_lib_version(repository_ctx, trt_install_path)\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl\", line 81, in _trt_lib_version\r\n                _find_trt_header_dir(repository_ctx, trt_install_path)\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl\", line 67, in _find_trt_header_dir\r\n                auto_configure_fail((\"Cannot find NvInfer.h with Ten...))\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/gpus/cuda_configure.bzl\", line 317, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find NvInfer.h with TensorRT install path /usr/lib/x86_64-linux-gnu\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'build_defs.bzl': no such package '@local_config_tensorrt//': Traceback (most recent call last):\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl\", line 171\r\n                _trt_lib_version(repository_ctx, trt_install_path)\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl\", line 81, in _trt_lib_version\r\n                _find_trt_header_dir(repository_ctx, trt_install_path)\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/tensorrt/tensorrt_configure.bzl\", line 67, in _find_trt_header_dir\r\n                auto_configure_fail((\"Cannot find NvInfer.h with Ten...))\r\n        File \"/install_bar/tensorflow-1.12.0/third_party/gpus/cuda_configure.bzl\", line 317, in auto_configure_fail\r\n                fail((\"\\n%sCuda Configuration Error:%...)))\r\n\r\nCuda Configuration Error: Cannot find NvInfer.h with TensorRT install path /usr/lib/x86_64-linux-gnu\r\n", "comments": ["@Julius-ZCJ ,\r\nCan you please provide `./configure `output ?thanks!", "@oanush  Here is my ./configure set:\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.5/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with Apache Ignite support? [Y/n]: n\r\nNo Apache Ignite support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:\r\n\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:\r\n\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 1.3\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.0]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=gdr            # Build with GDR support.\r\n        --config=verbs          # Build with libverbs support.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\nConfiguration finished", "Thanks for your report. Can you please attach a complete log of ./configure and bazel build starting from a fresh environment on the `master` branch? I'm afraid we don't provide support for building older versions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33130\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33130\">No</a>\n", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 33129, "title": "[TF2.0] How to get the intermediate layers output of pretrained model?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 5.3.5-arch1-1-ARCH\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 2.0.0\r\n- **Python version**: 3.7.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThis is exactly the same issue as https://github.com/tensorflow/models/issues/5236\r\nwhich has just become more of a real issue with TF 2.0.\r\n\r\nIn v1, I do this:\r\n\r\n```python\r\noutputs = list(map(lambda tname: tf.get_default_graph().get_tensor_by_name(tname), [\r\n    'DCNN/block3_pool/MaxPool:0',\r\n    'DCNN/block4_pool/MaxPool:0',\r\n    'DCNN/block5_pool/MaxPool:0'\r\n]))\r\n\r\nwith tf.Session() as sess:\r\n    val_outputs = sess.run(outputs)\r\n```\r\n\r\nWhat is the recommended way to do this with **TF 2.0 python API** now that graphs and sessions have been eliminated? Especially **without** access to the code that builds the model, which has been the greatest use case of get_tensor_by_name()? Is there any solution **other than using tf.compat.v1**?\r\n\r\nExample:\r\n```python\r\ndcnn = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\r\n# How to access vgg16/block3_pool/MaxPool:0 in TF 2.0 without the knowledge of the model source code?\r\n```\r\n\r\nIs there any upgrade instruction? Or plans to accommodate such use cases?\r\nThanks!", "comments": ["I meet the same problem. Is there any instruction ?", "@aleozlx, Just to verify, did you go through [this link](https://www.tensorflow.org/tutorials/images/transfer_learning). Thanks!", "Try this\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow_core.python.keras.applications.mobilenet_v2 import MobileNetV2\r\nfrom tensorflow_core.python.keras.layers.convolutional import Conv2D\r\nfrom tensorflow_core.python.keras.models import Model\r\nimport numpy as np\r\nfrom tensorflow_core.python.ops.init_ops_v2 import he_normal\r\n\r\nIMG_SIZE = image_size = 224\r\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\r\n\r\nbase_model = MobileNetV2(input_shape=IMG_SHAPE,\r\n                         include_top=False,\r\n                         weights='imagenet')\r\nbase_model.trainable = False\r\nbase_model.summary()\r\nlayer_name = \"block_13_expand_relu\"\r\nlayer_output = base_model.get_layer(layer_name).output\r\nx = Conv2D(16,\r\n           kernel_size=[3, 3],\r\n           strides=[1, 1],\r\n           padding='same',\r\n           kernel_initializer=he_normal(),\r\n           activation=tf.nn.relu,\r\n           name=\"conv3\",\r\n           kernel_regularizer=tf.keras.regularizers.l2(0.01))(layer_output)\r\n\r\nmodel = Model(base_model.input, outputs=[layer_output, x])\r\n\r\ninput_data = np.random.rand(1, *IMG_SHAPE)\r\nresult = model.predict(input_data)\r\nprint(result[0].shape)\r\nprint(result[1].shape)\r\n```\r\n", "Not sure if I misunderstand the question. I think this [Stack Overflow thread](https://stackoverflow.com/questions/48966281/get-intermediate-output-from-keras-tensorflow-during-prediction) covered your question.\r\nThe only difference is that in Keras, \r\n`earlyPredictor = Model(dcnn.inputs,dcnn.get_layer(theNameYouWant).outputs)` \r\nwill become the following in TF 2.0 (note it's **output** instead of **outputs**)\r\n`earlyPredictor = tf.keras.Model(dcnn.inputs,dcnn.get_layer(theNameYouWant).output)`.", "@gadagashwini It is unclear how the link provided answers to the question. That does not seem to help.\r\n\r\nThe rest of the answers are acceptable and do provide a feasible direction with .get_layer(). Thanks!\r\n\r\nHowever, at least 80% of our codes are custom and therefore I want to wait a little more to see if there is a non-Keras answer cuz we don't much care for wrapping everything up into Keras layers and models and all that OOP goodness.\r\n\r\nAnd the ultimate doubt that I still have is whether the DAG paradigm is so far completed eliminated from the v2 API (except for compat.v1 ofc)? An official/definitive answer would be of great help because there was value to that. If such is the case, I'll just adapt. Thanks again!", "@aleozlx This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "The question seems sufficiently addressed at this point. Thank you all!", "Hi @aleozlx . Please see if my recent answer is maybe close to the non-keras way you were looking for.\r\n\r\nhttps://stackoverflow.com/questions/59493222/access-output-of-intermediate-layers-in-tensor-flow-2-0-in-eager-mode/60945216#60945216", "Seems like this is a little old, but just in case, if you want to do this similar to what you had before, you could do this.\r\n\r\n```python\r\ndcnn = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\r\n\r\nnetwork_function = tf.function(lambda inputs: dcnn(inputs),\r\n                               [tf.TensorSpec(tf.TensorShape([None, 224, 224, 3]))])\r\n\r\noutputs = list(map(lambda tname: network_function.get_concrete_function().graph.get_tensor_by_name(tname), [\r\n    'vgg16/block3_pool/MaxPool:0',\r\n    'vgg16/block4_pool/MaxPool:0',\r\n    'vgg16/block5_pool/MaxPool:0'\r\n]))\r\n```", "> Seems like this is a little old, but just in case, if you want to do this similar to what you had before, you could do this.\r\n> \r\n> ```python\r\n> dcnn = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\r\n> \r\n> network_function = tf.function(lambda inputs: dcnn(inputs),\r\n>                                [tf.TensorSpec(tf.TensorShape([None, 224, 224, 3]))])\r\n> \r\n> outputs = list(map(lambda tname: network_function.get_concrete_function().graph.get_tensor_by_name(tname), [\r\n>     'vgg16/block3_pool/MaxPool:0',\r\n>     'vgg16/block4_pool/MaxPool:0',\r\n>     'vgg16/block5_pool/MaxPool:0'\r\n> ]))\r\n> ```\r\n\r\nhow do i predict  when get the outputs ?\r\n```import tensorflow as tf\r\n    model = train_model()\r\n    func = tf.function(model)\r\n    print(model.input)\r\n    tensor_specs1 = tf.TensorSpec.from_tensor(model.input)\r\n\r\n    call = func.get_concrete_function(tensor_specs1)\r\n    graph = call.graph\r\n\r\n    tensor_names = [n for n in graph.as_graph_def().node]\r\n    for name in tensor_names:\r\n        print(name)\r\n\r\n    outputs = graph.get_tensor_by_name('functional_1/output/Softmax:0')\r\n\r\n    pred_model = tf.keras.models.Model(model.input,outputs)\r\n    results = pred_model(tensor_specs1) \r\n```\r\nbut raise an exception:\r\n``` return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"/Users/anaconda/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3786, in _as_graph_element_locked\r\n    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\nValueError: Tensor Tensor(\"functional_1/output/BiasAdd:0\", shape=(?, 10), dtype=float32) is not an element of this graph\r\n```"]}, {"number": 33128, "title": "How to using batch norm with different T in every batch which input is  [B, T, D] shape tensor ", "body": "```\r\nTraceback (most recent call last):\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\r\n    return fn(*args)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]\r\n         [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n         [[RemoteCall]]\r\n         [[IteratorGetNext]]\r\n         [[gradients/speaker_res_net_raw_model_1/model/resnet/resblock-4/bnb_branch2a/FusedBatchNorm_grad/FusedBatchNormGrad/_8463]]\r\n  (1) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]\r\n         [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n         [[RemoteCall]]\r\n         [[IteratorGetNext]]\r\n0 successful operations.\r\n3 derived errors ignored.\r\n```\r\n\r\n```\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp-data/zhanghui/delta/./delta/main.py\", line 111, in <module>\r\n    app.run(main)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/tmp-data/zhanghui/delta/./delta/main.py\", line 79, in main\r\n    solver.train_and_eval()\r\n  File \"/tmp-data/zhanghui/delta/delta/utils/solver/estimator_solver.py\", line 445, in train_and_eval\r\n    self.train_and_eval_one_epoch(nn, train_spec, eval_spec)\r\n  File \"/tmp-data/zhanghui/delta/delta/utils/solver/estimator_solver.py\", line 391, in train_and_eval_one_epoch\r\n    nn, train_spec, eval_spec)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\r\n    return executor.run()\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\r\n    return self.run_local()\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\r\n    saving_listeners=saving_listeners)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 367, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1156, in _train_model\r\n    return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1219, in _train_model_distributed\r\n    self._config._train_distribute, input_fn, hooks, saving_listeners)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1329, in _actual_train_model_distributed\r\n    saving_listeners)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1484, in _train_with_estimator_spec\r\n    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 754, in run\r\n    run_metadata=run_metadata)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1252, in run\r\n    run_metadata=run_metadata)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1353, in run\r\n   raise six.reraise(*original_exc_info)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1338, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1411, in run\r\n    run_metadata=run_metadata)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\", line 1169, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/tmp-data/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]\r\n         [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n         [[RemoteCall]]\r\n         [[IteratorGetNext]]\r\n         [[gradients/speaker_res_net_raw_model_1/model/resnet/resblock-4/bnb_branch2a/FusedBatchNorm_grad/FusedBatchNormGrad/_8463]]\r\n  (1) Invalid argument: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [241,23,1], [batch]: [260,23,1]\r\n         [[{{node MultiDeviceIteratorGetNextFromShard}}]]\r\n         [[RemoteCall]]\r\n         [[IteratorGetNext]]\r\n0 successful operations.\r\n```", "comments": ["@zh794390558, Please provide the standalone code to reproduce the reported issue. Thanks!", "@zh794390558, Any update on reproducible code. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 33127, "title": "TensorflowLite Upsampling2D does not change output shape when resize_input_tensor is called", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (code can be found here https://github.com/benjamintanweihao/YOLOv3)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, but probably all\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVIDIA GTX 1070 8GB\r\n\r\n**Describe the current behavior**\r\nUpsampling 2D set to twice the size of the input always outputs the size set at convert time, even when resize inputs are called.\r\n\r\n**Describe the expected behavior**\r\nUpsampling2D outputs size times the size of the new input\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow.keras as keras\r\nfrom tensorflow.lite.python import lite, interpreter\r\n\r\n#define model that uses upsampling 2D and depends on it's output changing with the input size\r\ndef DemoModel():\r\n  inputs = keras.layers.Input(shape=[None,None, 3], name=\"input_1\")\r\n  a = keras.layers.MaxPooling2D((2,2))(inputs)\r\n  b = keras.layers.UpSampling2D(size=(2,2))(a)\r\n  c = keras.layers.Concatenate()([b, inputs])\r\n  return keras.models.Model(inputs=[inputs], outputs=[a, c])\r\n\r\n#convert to tflite\r\nmodel = DemoModel()\r\nmodel.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\r\nmodel.save(\"testModel.h5\")\r\n\r\nconverter = lite.TFLiteConverter.from_keras_model_file(\"testModel.h5\", input_shapes={'input_1': [1,32,64,3]})\r\ntflite_model=converter.convert()\r\n\r\n\r\n#run the tflite model\r\n\r\ninter = interpreter.Interpreter(model_content=tflite_model)\r\nprint(inter.get_input_details())\r\ninter.resize_tensor_input(1, [1,16,32,3])\r\ninter.allocate_tensors()\r\nprint(inter.get_input_details())\r\n\r\n\r\n```", "comments": ["Is there a workaround for this or any fix in any of the 2.x releases?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "The above code run on tf 2.4.0 gives the following runtime error:\r\n\r\nRuntimeError: Attempting to resize a fixed-size tensor.\r\n\r\nI guess that this probably indicates that tensorflow lite doesn't support changing the input size at runtime then?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33127\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33127\">No</a>\n"]}, {"number": 33126, "title": "Remove the \"nomac\" exclusion for FromTensorsTest", "body": "`FromTensorsTest` could run on mac now, so this PR removes the `nomac` exclusion.", "comments": ["@jsimsa This PR follows the discussion [here](https://github.com/tensorflow/tensorflow/pull/31380#issuecomment-531934326)."]}, {"number": 33125, "title": "[TF2.0] Keras layers with custom tensors as variables", "body": "(I've asked this question on [StackOverflow](https://stackoverflow.com/questions/58275253/tensorflow-2-0-keras-layers-with-custom-tensors-as-variables), but after looking closer into the source code, I'm not sure if a clean solution exits yet, so re-posting it here.)\r\n\r\nIn TF 1.x, it was possible to build layers with custom variables. Here's an example:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef make_custom_getter(custom_variables):\r\n    def custom_getter(getter, name, **kwargs):\r\n        if name in custom_variables:\r\n            variable = custom_variables[name]\r\n        else:\r\n            variable = getter(name, **kwargs)\r\n        return variable\r\n    return custom_getter\r\n\r\n# Make a custom getter for the dense layer variables.\r\n# Note: custom variables can result from arbitrary computation;\r\n#       for the sake of this example, we make them just constant tensors.\r\ncustom_variables = {\r\n    \"model/dense/kernel\": tf.constant(\r\n        np.random.rand(784, 64), name=\"custom_kernel\", dtype=tf.float32),\r\n    \"model/dense/bias\": tf.constant(\r\n        np.random.rand(64), name=\"custom_bias\", dtype=tf.float32),\r\n}\r\ncustom_getter = make_custom_getter(custom_variables)\r\n\r\n# Compute hiddens using a dense layer with custom variables.\r\nx = tf.random.normal(shape=(1, 784), name=\"inputs\")\r\nwith tf.variable_scope(\"model\", custom_getter=custom_getter):\r\n    Layer = tf.layers.Dense(64)\r\n    hiddens = Layer(x)\r\n\r\nprint(Layer.variables)\r\n```\r\nThe printed variables of the constructed dense layer will be custom tensors we specified in the `custom_variables` dict:\r\n```python\r\n[<tf.Tensor 'custom_kernel:0' shape=(784, 64) dtype=float32>, <tf.Tensor 'custom_bias:0' shape=(64,) dtype=float32>]\r\n```\r\nThis allows us to create layers/models that use provided tensors in `custom_variables` directly as their weights, so that we could further differentiate the output of the layers/models with respect to any tensors that `custom_variables` may depend on (particularly useful for implementing functionality in [modulating sub-nets](https://distill.pub/2018/feature-wise-transformations/), [parameter generation](https://arxiv.org/abs/1705.10301), [meta-learning](https://github.com/deepmind/learning-to-learn), etc.).\r\n\r\nVariable scopes used to make it easy to nest all off graph-building inside scopes with custom getters and build models that used the provided tensors as their parameters. Since sessions and variable scopes are no longer advisable in TF 2.0 (and all of that low-level stuff is moved to `tf.compat.v1`), what would be **the best practice** to implement the above using Keras and TF 2.0?\r\n\r\n**Note:** It looks like `tf.keras.layers.Layer` supports custom getters, but I'm running into issues passing arbitrary tensors instead of variables (e.g., the getter [gets wrapped internally](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L412-L416) which I'm not sure how affects non-variable tensors). So, not sure the use-case I outlined above is properly supported. What's the best way to go about it?", "comments": ["@alshedivat Go through this [documentation](https://www.tensorflow.org/guide/effective_tf2) and try ti convert your code and let me know if you are facing any challenges. Thanks!", "@gowthamkpr, yes, I've gone through the Effective TF2 docs, but building layers/models on top of custom tensors as variables seems to be outside the scope of what it covers.\r\n\r\nMy code above doesn't need any changes until the line `with tf.variable_scope` (it all works in the eager mode and all that). Starting that line, I'm not sure how the `custom_getter` is supposed to be passed to `tf.keras.layers.Dense` so that it internally uses the specified tensors without variable scopes. Any ideas?", "@fchollet, what's be the best practice here?", "@alshedivat It is answered in stack overflow right? Is that answer not satisfactory?", "@gowthamkpr, the stackoverflow answer provides a hacky solution customized for the Dense layer. Following that approach, I'd need to write a whole library of such lambda layers for each layer type in Keras. Using `tf.variable_scope` in TF 1.x doesn't require that.\r\n\r\nI'd like to see if there is a nice way to implement a Layer or Model wrapper in TF 2.0 that can take care of building the model graph on top of the provided custom tensors used as weights. If there is no clean way to do it now, I'd be happy to help upgrade Layer/Model/Wrapper API that allows this. Thanks.", "@alshedivat Rightnow, we don't have a clean way to do this.", "I am closing this issue for now. If there are any updates, will keep you posted. Thanks!", "Just FYI, I have implemented [context manager](https://github.com/alshedivat/meta-blocks/blob/master/meta_blocks/adaptation/maml_utils.py#L21-L93) that allows building arbitrary Keras models with custom tensors as variables. It works well, and the only downside is that I have to monkey patch/unpatch internal `tensorflow.python.keras.engine.base_layer_utils.make_variable`.\r\n\r\n@gowthamkpr, as a feature request, is it possible to expose some public API for customizing variable getters in TF2, so that my solution does not have to tweak Keras internals?"]}, {"number": 33124, "title": "Can a tensorflow-lite model accept a variable-shaped input?", "body": "When converting a saved frozen graph to tensorflow lite I received the following error: `ValueError: None is only supported in the 1st dimension. Tensor 'input' has invalid shape '[1, None, None]'.`\r\n\r\nI'm expecting a variable-sized input that can range from e.g. 10x7 to 100x100, but this seems to be actively prevented in the conversion to tensorflow lite.\r\n\r\nIt seems to be possible to fix the input size and reshape the graph to accommodate the input (e.g. https://stackoverflow.com/a/55732431) but if I had to do it for every call the overhead would probably kill any gain made by switching to tensorfow lite.\r\n\r\nIs there a way around that, or as now this functionality is missing from tf-lite?\r\n\r\nI'm using tensorflow 1.14 - I'd be happy to switch to tensorflow 2.0 but at least by looking at the source code it seems to have the same problem.", "comments": ["Can you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/20798) and let me know if it helps. Thanks!", "Thanks for getting back to me!\r\nIf I understand it correctly, the workaround advocates specifying a static input size. \r\nAs the shape of my inputs is variable this would break the network.\r\n\r\n[the content of the arrays is categorical, so I wouldn't be able to resize it as if they were images]\r\n ", "According to this issue this feature is being developed. \r\nhttps://github.com/tensorflow/tensorflow/issues/29590\r\n\r\nIs this still the case? [the issue is few months old]\r\nIf so we could merge the two threads.", "Yes. It is being tracked there. And the developers are actively working on it. \r\n\r\nI am going to close this issue as we already have a thread open for this issue and Having multiple threads doesn't help the cause. Thanks!"]}, {"number": 33123, "title": "Fix the issue for importing googletest.h", "body": "This PR removes `#include \"testing/base/public/googletest.h\"` in the `input_generator_test` and `util_test` so that these test can also run on the local environment. Otherwise, there will be an error below:\r\n\r\n```\r\nERROR: /home/abc/tensorflow/tensorflow/lite/testing/kernel_test/BUILD:33:1: C++ compilation of rule '//tensorflow/lite/testing/kernel_test:util_test' failed (Exit 1)\r\ntensorflow/lite/testing/kernel_test/util_test.cc:21:10: fatal error: testing/base/public/googletest.h: No such file or directory\r\n #include \"testing/base/public/googletest.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/lite/testing/kernel_test:util_test failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n``` \r\n\r\nFixes #26671 ", "comments": ["> Please don't reformat files which are not related to the change\r\n\r\n@mihaimaruseac The format changes are removed now.", "As this issue has been fixed by @nairb774 via (https://github.com/tensorflow/tensorflow/commit/1c88fff548acb048f2fe64fd2564e885759def12, https://github.com/tensorflow/tensorflow/commit/c9c0e70d0b1d6d4fd618c5ebdebd859dbe59848f,  https://github.com/tensorflow/tensorflow/commit/db5816707ab0fc4ae6d3959a313d67dd871a8d9d), I will close this PR. Thanks for your fix, @nairb774 !", "Hey @feihugis, sorry about the conflict. I didn't know this was out there - I'm new to the project. I stumbled onto the build problem because I am trying to improve the state of the build/test health. There seem to be a number of things that are excluded from testing coverage which don't seem to need to be. I'm hoping that I can work through the broken code/issues and get more of this under CI so we have less of this broken code floating around.\r\n\r\nSorry :(", "No worries, @nairb774! Your fixes are great. "]}, {"number": 33122, "title": "Roll of absl breaks TFLite builds (absl::MakeTaggedSeedSeq)", "body": "Build Top of master with \r\n\r\n./tensorflow/lite/tools/make/download_dependencies.sh                                                                                                                                                                  \r\n./tensorflow/lite/tools/make/build_lib.sh    \r\n\r\n\r\nFails with:\r\n\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.cc: In function \u2018int main()\u2019:                                                                                                                  \u2502\u00b7\u00b7\u00b7\u00b7\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.cc:23:25: error: \u2018MakeTaggedSeedSeq\u2019 is not a member of \u2018absl\u2019                                                                                 \u2502\u00b7\u00b7\u00b7\u00b7\r\n   auto seed_seq = absl::MakeTaggedSeedSeq(\"TEST_GENERATOR\", std::cerr);                                                                                                                                                      \u2502\u00b7\u00b7\u00b7\u00b7\r\n                         ^~~~~~~~~~~~~~~~~                              \r\n\r\nThis has been fixed in top of master absl by deleting the file. \r\n\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: master\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n", "comments": ["I have this problem as well on debian stretch.\r\n\r\n```\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.cc: In function \u2018int main()\u2019:\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.cc:23:19: error: \u2018MakeTaggedSeedSeq\u2019 is not a member of \u2018absl\u2019\r\n   auto seed_seq = absl::MakeTaggedSeedSeq(\"TEST_GENERATOR\", std::cerr);\r\n                   ^~~~\r\narm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/tools/make/downloads/absl/absl/random/internal/seed_salting_sequence_generator.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/seed_salting_sequence_generator.o\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/pool_urbg.cc: In static member function \u2018static void absl::lts_2019_08_08::random_internal::RandenPool<T>::Fill(absl::lts_2019_08_08::Span<T>) [with T = long long unsigned int]\u2019:\r\ntensorflow/lite/tools/make/downloads/absl/absl/random/internal/pool_urbg.cc:241:6: note: parameter passing for argument of type \u2018absl::lts_2019_08_08::Span<long long unsigned int>\u2019 will change in GCC 7.1\r\n void RandenPool<T>::Fill(absl::Span<result_type> data) {\r\n      ^~~~~~~~~~~~~\r\ntensorflow/lite/tools/make/Makefile:251: recipe for target '/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.o' failed\r\nmake: *** [/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/tools/make/downloads/absl/absl/random/internal/named_generator.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\nmake: Leaving directory '/home/pi/tensorflow'\r\n```\r\nIs a work-around available?", "@multiverse-tf has a bit more experience with the Raspberry Pi build pipeline and can offer an update.", "This is not rpi specific. All of TFlite is affected. \r\n\r\nAlso the software engineer in me cringes when I see a name space like absl::lts_2019_08_08:: Maybe we can add a git sha in the name space next. \ud83d\ude1c\r\n", "Indeed. This specific absl version is defined in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl and was introduced in https://github.com/tensorflow/tensorflow/commit/7c2f3e2f88523dd016f36484646c7e5a14c2da38#diff-455a4c7f8e22d7c514e8c2caa27506c5\r\n\r\nA rollback of the above commit is underway.", "@multiverse-tf thanks for the rollback. Is there any way you can setup a TFLite CI builder or something so such a rollout doesn't rollin in the first place :) ", "resolved now", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33122\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33122\">No</a>\n"]}, {"number": 33121, "title": "[INTEL MKL] support for {int8,int8} convolutions and fusions", "body": "Support for {int8,int8} convolutions and its fusions", "comments": ["@penpornk I have made the changes. Please take a look."]}, {"number": 33120, "title": "[Cherrypick TF1.15] Fix performance slowdown", "body": "Original:\r\n\r\nAutomated rollback of commit db7e43192d405973c6c50f6e60e831a198bb4a49\r\n\r\nPiperOrigin-RevId: 267164406", "comments": []}, {"number": 33119, "title": "Corrected grammatical errors and added hyphens", "body": "Nice reading those beautiful codes", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33119) for more info**.\n\n<!-- need_sender_cla -->", "@lamberta do we need these changes ?", "Thanks, but I think the README copy is pretty set now. But please feel free to fix tensorflow.org docs in the tensorflow/docs repo \ud83d\ude00https://github.com/tensorflow/docs/tree/master/site/en ", "@lamberta thank you "]}, {"number": 33118, "title": "corrected the grammatical errors", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33118) for more info**.\n\n<!-- need_sender_cla -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac @chanshah "]}, {"number": 33117, "title": "Object Detection API Training fails with: Cuda call failed with an illegal memory access on TF1.15rc3", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using legacy train.py script from Object Detection API\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):  v1.15.0-rc2-8-g776451109a 1.15.0-rc3\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): \"https://github.com/bazelbuild/bazel/releases/download/0.24.1/bazel-0.24.1-installer-linux-x86_64.sh\"\r\n- GCC/Compiler version (if compiling from source): GCC 6.5.0\r\n- CUDA/cuDNN version: CUDA \"10.0\", cuDNN \"7-7.6.4.38-1.cuda10.0\"\r\n- GPU model and memory: \"NVIDIA Tesla K80, 11441MiB\"\r\n\r\n**Describe the current behavior**\r\nThe training from this source compiled binary starts successfully, but dies after some iterations.\r\n\r\n**Describe the expected behavior**\r\nThe training commences and finishes normally.\r\n\r\n**Code to reproduce the issue**\r\nCompile the current r1.15 release candidate and use the train.py script from the OD API to train a model.\r\n\r\n**Other info / logs**\r\nThe training works with an official pip release of tensorflow 1.14 package, but fails with the compiled new version. Here is the log output of the run: Also start up seems to be incredibly slow (almost 60 seconds for the first iteration)\r\n[tf_err_log.txt](https://github.com/tensorflow/tensorflow/files/3698811/tf_err_log.txt)\r\n\r\n", "comments": ["@Atharex Can you please provide a github gist for us to reproduce the issue? Thanks!", "Sorry for the late response, had a bad case of the flu this week.\r\n\r\nNot sure how to do this on gist, but I can give you the Dockerfile to build the same image as I'm using and the log output of the OD API train script.\r\n\r\n[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/3724535/Dockerfile.txt) (the nvidia files you would have to put in manually)\r\n[tf_err_log.txt](https://github.com/tensorflow/tensorflow/files/3724537/tf_err_log.txt)\r\n\r\nI'm using the [SSD Mobilenet v2 config](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config) model for the training (works normally on official TF1.14 pip installation) \r\n\r\nAnd the training script is invoked like this:\r\n`python3 /tensorflow/train.py --logtostderr --train_dir=/training_dataset/ --pipeline_config_path=/training_dataset/model.config`", "Still happening with the official 1.15.0 release :/ ", "@gowthamkpr were you able to reproduce the issue? Do you need more info?", "@Atharex I was not able to reproduce the issue. Will look into it further and will get back to you. If you can implement this using google colab and share a github gist, it would be helpful. thanks!", "Is it possible to install a custom built tensorflow package onto google colab? \r\nSadly I'm not familiar with gists :/ What should I put in there, besides my build steps? ", "@Atharex you can try following the steps from this [tutorial](https://medium.com/@moshe.livne/training-tensorflow-for-free-pet-object-detection-api-sample-trained-on-google-collab-c2e65f4a9949) and just test it on 1.15rc3 and let me know if its still an issue. Thanks!", "Closing this issue as it has been inactive for almost a week. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 33116, "title": "[INTEL MKL] Support for s8s8 convolutions and fusions", "body": "Support for {int8,int8} convolutions and its fusions", "comments": []}, {"number": 33115, "title": "[INTEL MKL] scatter optimizations.", "body": "Parallelizing scatter update op.", "comments": ["@penpornk made all the changes requested except defaulting to Serial Execute for google internal runs. Will wait for your comments for that.", "Hi @penpornk Thanks for the response. I have wrapped the code around ifdef\r\n", "@penpornk gentle reminder to review the latest change", "Hi @penpornk made the changes. Kindly review.", "HI @penpornk Just checking on the status of this PR", "Seems auto-merge is not happening but the changes are now committed so we can close this. Thank you for the PR."]}, {"number": 33114, "title": "define _USE_MATH_DEFINES in hlo_evaluator.cc", "body": "MSVC didn't define `M_PI` by default.\r\n\r\ndefine `_USE_MATH_DEFINES` to use `M_PI`\r\n\r\nhttps://docs.microsoft.com/en-us/cpp/c-runtime-library/math-constants\r\n\r\nThis PR fixed the `hlo_evaluator.cc` build break.", "comments": ["PLATFORM_WINDOWS already added to stream_executor\\platform\\platform.h on Aug 28, 2019 but not in v2.0.0.\r\n\r\nThis PR will fix the problem of the file `tensorflow/compiler/xla/service/hlo_evaluator.cc` didn't define `M_PI`.", "> PLATFORM_WINDOWS already added to stream_executor\\platform\\platform.h on Aug 28, 2019 but not in v2.0.0.\r\n> \r\n> This PR will fix the problem of the file `tensorflow/compiler/xla/service/hlo_evaluator.cc` didn't define `M_PI`.\r\n\r\nCan you please change the PR description to say this?", "> Can you please change the PR description to say this?\r\n\r\nThe PR description should also include _why_ you're defining _USE_MATH_DEFINES in hlo_evaluator.cc."]}, {"number": 33113, "title": "Warning on why compute devices aren't being used ", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs of now, if \"tf.debugging.set_log_device_placement(True)\" is set, there is useful feedback on which device is being used for computation. \r\n\r\nIt would be helpful however, if feedback was given as to why a TPU, GPU, XLA_GPU or CPU wasn't used. Especially if a lower-power device was chosen instead (GPU instead of TPU, or CPU used instead of GPU for instance). \r\n\r\nInformation and hints about CUDA versions, incompatibility and other setup flaws would greatly speed up debugging and getting things going.\r\n\r\n**Will this change the current api? How?**\r\nNo, it will simply add information to the device placement log.\r\n\r\n**Who will benefit with this feature?**\r\nAny users trying to set up their TPU, GPU or cluster.\r\n", "comments": ["@victorflorintsev ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 33112, "title": "Link broken", "body": "The link on the following line is broken: https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/csv.ipynb?short_path=443a7a7#L360", "comments": ["Seems to be working for me.", "Its working for me too. ", "Closing this issue as its not an issue. Please add additional comments and we can open this issue again. Thanks!"]}, {"number": 33111, "title": "Use correct casts to get right dimensions on s390x", "body": "The `testConcatAxisType` test in `concat_op_test` is failing on an assertion for s390x (Ref. Issue#[28114](https://github.com/tensorflow/tensorflow/issues/28114)):\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/87c06bad5aec2f21ec96b7253de551e8/execroot/org_tensorflow/bazel-out/s390x-opt/bin/tensorflow/python/kernel_tests/concat_op_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/concat_op_test.py\", line 621, in testConcatAxisType\r\n    self.assertEqual([2, 6], c.get_shape().as_list())\r\nAssertionError: Lists differ: [2, 6] != [4, 3]\r\n```\r\nIt turned out that the cast operation was not correct for big-endian architecture and not able to draw right dimension value when the Axis is of int64 type. Updating the cast resolved the issue.", "comments": ["@jaingaurav Kindly review the PR when you get chance.\r\n@gbaned I wish to merge these code changes in `r1.15` branch too. Is there any way available to achieve this or I have to raise another PR?", "@jaingaurav Thanks for reviewing the code changes. I have incorporated your review comments. As you have suggested, will create a separate PR for `1.15` branch.", "Created PR#[33417](https://github.com/tensorflow/tensorflow/pull/33417) for merging the code changes on 1.15 branch."]}, {"number": 33110, "title": "Weight decay in adam optimizer", "body": "- TensorFlow version: 1.14 / 2.*\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCould we please get weight decay (L2 penalty) in Adam optimizer? PyTorch has it and it is as easy as adding a `weight_decay` float argument when initializing the optimizer.\r\nRectified adam would be great too! \r\n\r\n**Who will benefit with this feature?**\r\nEveryone that want to use L2 weight decay. \r\n", "comments": ["@JoaoLages Can you please fill in the issue template thats given [here](https://github.com/tensorflow/tensorflow/issues/new/choose) which will help us expedite the process. Thanks!", "@gowthamkpr I've updated the issue description, idk if this is what you were looking for", "@JoaoLages Thanks for the update.", "Please use this [optimizer](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/weight_decay_optimizers.py)", "@tanzhenyu can you provide an example of how to use it?", "@JoaoLages it should be trivial. just like any other optimizer"]}, {"number": 33109, "title": "Update word2vec_basic.py", "body": "Is there any time that `(data_index - span) % len(data)` is different than `(data_index + len(data) - span) % len(data)`? I couldn't find a reason to add the divisor len(data) here.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33109) for more info**.\n\n<!-- need_sender_cla -->", "@MaverickMeerkat thank you for your contribution, please sign CLA.", "You're welcome. I signed.", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33109) for more info**.\n\n<!-- ok -->"]}, {"number": 33108, "title": "Tensorflow 2.0 - bazel build from source fails (can't exec cc1plus)", "body": "**System information**\r\nSystem: Gentoo\r\nTensorflow 2.0, 1.14\r\nPython 3.6\r\nInstalled from source\r\nBazel 0.26.1\r\nGCC 8.3.0, 7.4.0 (both fails, both have c++ installed - /usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0/cc1plus)\r\nCUDA 10.1\r\nCuDNN 7.6.4\r\n\r\n\r\n```\r\n# bazel build --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\n...\r\n\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (278 packages loaded, 21924 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /w/pip/.cache/bazel/_bazel_pip/667008e4969ed3eba4f1f491903cb254/external/com_google_protobuf/BUILD:106:1: C++ compilation of rule '@com_google_protobuf//:protobuf_lite' failed (Exit 1)\r\ngcc: error trying to exec 'cc1plus': execvp: No such file or directory\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n\r\n```\r\n\r\n\r\ng++ is installed on system...\r\n\r\n```\r\n # g++ --print-prog-name=cc1plus\r\n/usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0/cc1plus\r\n```\r\n\r\nPlease help, I get the same error for every other release.", "comments": ["Most likely `cc1plus` is not on a path that is visible in `PATH` envvar that Bazel uses. To check, compare the output of `which cc1plus` with the output that you'd get from running `bazel run :test_path` on a new project with\r\n\r\n```bazel\r\nsh_binary(name=\"test_path\", srcs=[\"test_path.sh\"])\r\n```\r\n\r\nand\r\n\r\n```bash\r\n#!/bin/bash\r\necho $PATH\r\n```", "Thanks! I've added /usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0 to the PATH, but now I'm getting errors for header files, should I set some environment variable or set it during ./configuration? \r\n\r\n```\r\nERROR: /w/pip/.cache/bazel/_bazel_pip/667008e4969ed3eba4f1f491903cb254/external/com_googlesource_code_re2/BUILD:26:1: C++ compilation of rule '@com_googlesource_code_re2//:re2' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /w/pip/.cache/bazel/_bazel_pip/667008e4969ed3eba4f1f491903cb254/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/lib64/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8 \\\r\n    PATH=/usr/lib/llvm/8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/opt/cuda/bin:/opt/cuda/libnvvp:/usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0:/usr/lib64/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8/ \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/com_googlesource_code_re2/_objs/re2/bitstate.pic.d '-frandom-seed=bazel-out/host/bin/external/com_googlesource_code_re2/_objs/re2/bitstate.pic.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 -pthread -c external/com_googlesource_code_re2/re2/bitstate.cc -o bazel-out/host/bin/external/com_googlesource_code_re2/_objs/re2/bitstate.pic.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nexternal/com_googlesource_code_re2/re2/bitstate.cc:20:10: fatal error: stddef.h: No such file or directory\r\n #include <stddef.h>\r\n          ^~~~~~~~~~\r\ncompilation terminated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "I think it's better to set everything up during `./configure`. Otherwise you have to set up all kind of env vars for binaries, include paths, library paths, etc. In that case, this is not a supported usecase and would be better to continue that approach on StackOverflow as it is all a configuration issue.", "I couldn't set it during configure, I've tried to set it through CPATH or CPLUS_INCLUDE_PATH but to no success. ", "Unfortunately, you'll have to ask for help on StackOverflow, as we cannot support all build configurations.", "I could build Tensorflow on Gentoo, isn't this problem with bazel or some build scripts? Could your post what are your environmental variables set for building TF from source? ", "I've tried to pass path through CPATH, C_INCLUDE_PATH, CPLUS_INCLUDE_PATH, LD_LIBRARY_PATH, TF_CUDA_PATHS - it doesn't work. Build fails because header files can not be found, although they are all present in /usr/lib64/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8. Also gcc is compiled to include this path:\r\n```\r\n\r\nsing built-in specs.\r\nCOLLECT_GCC=gcc\r\nCOLLECT_LTO_WRAPPER=/usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0/lto-wrapper\r\nTarget: x86_64-pc-linux-gnu\r\nConfigured with: /tmp/portage/sys-devel/gcc-8.3.0-r1/work/gcc-8.3.0/configure --host=x86_64-pc-linux-gnu --build=x86_64-pc-linux-gnu --prefix=/usr --bindir=/usr/x86_64-pc-linux-gnu/gcc-bin/8.3.0 --includedir=/usr/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include --datadir=/usr/share/gcc-data/x86_64-pc-linux-gnu/8.3.0 --mandir=/usr/share/gcc-data/x86_64-pc-linux-gnu/8.3.0/man --infodir=/usr/share/gcc-data/x86_64-pc-linux-gnu/8.3.0/info **--with-gxx-include-dir=/usr/lib/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8** --with-python-dir=/share/gcc-data/x86_64-pc-linux-gnu/8.3.0/python --enable-languages=c,c++,fortran --enable-obsolete --enable-secureplt --disable-werror --with-system-zlib --enable-nls --without-included-gettext --enable-checking=release --with-bugurl=https://bugs.gentoo.org/ --with-pkgversion='Gentoo Hardened 8.3.0-r1 p1.1' --enable-esp --enable-libstdcxx-time --disable-libstdcxx-pch --enable-shared --enable-threads=posix --enable-__cxa_atexit --enable-clocale=gnu --disable-multilib --with-multilib-list=m64 --disable-altivec --disable-fixed-point --enable-targets=all --enable-libgomp --disable-libmudflap --disable-libssp --disable-libmpx --disable-systemtap --enable-vtable-verify --enable-lto --without-isl --enable-default-pie --enable-default-ssp\r\nThread model: posix\r\ngcc version 8.3.0 (Gentoo Hardened 8.3.0-r1 p1.1)\r\n```\r\nI've tried passing -I /usr/lib64/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8 as optimization flag to gcc - still doesn't work. It looks like the wrapper doesn't know how to handle Gentoo toolchain properly. \r\n\r\n```\r\nERROR: /w/pip/.cache/bazel/_bazel_pip/667008e4969ed3eba4f1f491903cb254/external/zlib_archive/BUILD.bazel:5:1: C++ compilation of rule '@zlib_archive//:zlib' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /w/pip/.cache/bazel/_bazel_pip/667008e4969ed3eba4f1f491903cb254/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/lib64/gcc/x86_64-pc-linux-gnu/8.3.0/include/g++-v8 \\\r\n    PATH=/usr/lib/llvm/8/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/bin:/opt/cuda/bin:/opt/cuda/libnvvp:/usr/libexec/gcc/x86_64-pc-linux-gnu/8.3.0 \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.d '-frandom-seed=bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.o' -iquote external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -isystem external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -Wno-shift-negative-value -DZ_HAVE_UNISTD_H -c external/zlib_archive/crc32.c -o bazel-out/host/bin/external/zlib_archive/_objs/zlib/crc32.o)\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nIn file included from external/zlib_archive/zlib.h:34,\r\n                 from external/zlib_archive/zutil.h:22,\r\n                 from external/zlib_archive/crc32.c:31:\r\nexternal/zlib_archive/zconf.h:247:14: fatal error: stddef.h: No such file or directory\r\n```\r\n\r\nI ran out of ideas how can I compile Tensorflow. I guess something goes awry because wrapper calls directly for cc1plus, instead of using gcc or g++ binary?", "Adding @perfinion in case he or SIG Build can help", "Please help Allmighty Devs", "For Gentoo.org Linux\r\n\r\nAlso see (Failed to emerge dev-util/bazel-0.27.2):\r\nhttps://forums.gentoo.org/viewtopic-t-1104292.html\r\n\r\ndev-util/bazel-0.27.2::gentoo that sci-libs/tensorflow-2.1.0_rc0::gentoo requires is not AMD64 stable as per:\r\nhttps://packages.gentoo.org/packages/dev-util/bazel\r\n\r\nBug report around Bazel:\r\nhttps://bugs.gentoo.org/buglist.cgi?quicksearch=bazel", "This has been patched in Gentoo Linux on top of Bazel v1.2.0 here:\r\nhttps://gitweb.gentoo.org/repo/gentoo.git/tree/dev-util/bazel/files/1.2.0-grpc-gettid.patch\r\n\r\nBackported to Bazel v0.29.1 here:\r\nhttps://github.com/paulbors/gentoo-localrepo/blob/master/dev-util/bazel/files/0.29.1-grpc-gettid.patch\r\n\r\nIf you're not running Gentoo, a similar patch can be ported over to your distro if one hasn't been created already.", "Whoops, sorry I didn't see this issue earlier. I maintain the Gentoo TF package too.\r\nIf you're on gentoo, you should just `emerge sci-libs/tensorflow`, is there some specific reason you're trying to build manually not just using the one in portage? The package for it should handle all the right versions of bazel and cuda and everything else. If you need other features that are not in the package let me know and I might be able to add them :)\r\n\r\nAs for actually building yourself. You shouldnt need to do anything strange with the linker flags or anything like that. If you're doing a cpu build everything will just work with the defaults from the `./configure` script. If you're doing a cuda build, you'll need to set the gcc path to the full one:\r\n\r\n`export GCC_HOST_COMPILER_PATH=\"/usr/x86_64-pc-linux-gnu/gcc-bin/8.3.0/x86_64-pc-linux-gnu-gcc\"` before `./configure` then it'll work. (cuda isnt compatible with gcc9 yet so you've gotta use gcc8). `/usr/bin/gcc` is a link to that longer gcc path and nvcc has issues with it, so pass that long one and everything works fine. (this should only apply for cuda builds, non-cuda are much easier)", "Read above comments and please update Gentoo's ebuild to use latest stable version of Bazel v1.2.0+ as the \"right version of Bazel\" is buggy.\r\n\r\nSee this ebuild for a localrepo on how to patch the older buggy \"right version of Bazel\" to have Tensorflow install on Gentoo:\r\nhttps://github.com/paulbors/gentoo-localrepo/tree/master/dev-util/bazel\r\n\r\nBazel bug has been patched in Gentoo Linux on top of Bazel v1.2.0 as per below (above localrepo ebuild backports the patch to the \"right version of Bazel\" so that your ebuild can install on my localhost):\r\nhttps://gitweb.gentoo.org/repo/gentoo.git/tree/dev-util/bazel/files/1.2.0-grpc-gettid.patch", "To clarify, Gentoo builds from source via the ebuld file (no need to go through your manual steps as per your latest instrustions). After all you gave us an ebuild ;)\r\n\r\nThe problem is a dependency of Tensorflow on an older buggy version of Bazel (which is no longer the right bersion at older v0.29.1 as that's super old).\r\n\r\nHere's the direct link on how to patch the bug reported in this ticket in Bazel v0.29.1:\r\nhttps://github.com/paulbors/gentoo-localrepo/blob/master/dev-util/bazel/files/0.29.1-grpc-gettid.patch\r\n\r\nHowever, Tensorflow should move on and catch up with its build system and use the latest stable version of Bazel.", "Hi @lutel! , we are  checking to see if you are still looking for assistance in this issue.\r\nCould  you try with [tested configurations ](https://www.tensorflow.org/install/source#tested_build_configurations)again ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33108\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33108\">No</a>\n"]}, {"number": 33107, "title": "Compilation error - pybind11", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- Centos 6.9\r\n- N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source):  0.25.2\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10/7\r\n- GPU model and memory: v100\r\n\r\n\r\n\r\n**Describe the problem**\r\nCompilation fails around tensorflow/python\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure \r\nbazel build --jobs=16 --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nNeed to compile for RHEL6 and Centos6 as precompiled packages need glibc > 2.12\r\nHave successfully compiled most previous versions with a little massaging. Attached is the log from where the error occurs.\r\n[tens2.0err.txt](https://github.com/tensorflow/tensorflow/files/3696937/tens2.0err.txt)\r\n\r\n\r\n", "comments": ["@berniekirby, Please include the ./configure output. Thanks!", "@berniekirby, Is this still an issue.", "Commenting out the first include in tensorflow/python/framework/op_def_registry.cc\r\nThus:\r\n\r\n```\r\n/* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n\r\nLicensed under the Apache License, Version 2.0 (the \"License\");\r\nyou may not use this file except in compliance with the License.\r\nYou may obtain a copy of the License at\r\n\r\n    http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nUnless required by applicable law or agreed to in writing, software\r\ndistributed under the License is distributed on an \"AS IS\" BASIS,\r\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\nSee the License for the specific language governing permissions and\r\nlimitations under the License.\r\n==============================================================================*/\r\n\r\n//#include \"include/pybind11/pybind11.h\"\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/op_def.pb.h\"\r\n#include \"tensorflow/core/framework/op_def_util.h\"\r\n#include \"tensorflow/python/lib/core/pybind11_status.h\"\r\n```\r\n\r\nResolves the issue.\r\npybind11.h must also be included somewhere else via the other includes.", "@berniekirby, Glad it resolved. \r\nAre you happy to close this issue. Thanks!", "Assuming the \"fix\" is verified and I don't need to do that in the next git download of course you can close it. Surprised others have not seen this issue.", "Closing this issue now, Feel free to reopen if the issue still persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33107\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33107\">No</a>\n"]}, {"number": 33106, "title": "Resolve undeclared inclusion due to symlinks in nvcc path", "body": "If there is a sym-link in the path that is used for the GCC compiler used by `nvcc` then Bazel resolves it while `configure.py` doesn't.  This results in issues with paths not matching, and errors about undeclared inclusions for header files in that GCC install.\r\n\r\nThis commit makes `configure.py` resolve any sym-links in the path so that it matches Bazel's behaviour.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F33106) for more info**.\n\n<!-- need_sender_cla -->", "@owainkenwayucl thank you for your contribution, please sign CLA.", "Sorry, I'm travelling and since I wrote this while doing my job at UCL, I'll need to get them to sign the agreement.\r\n\r\nIt's just one line though...", "No CLA means we cannot review and cannot approve", "This has been superceded by another patch, so closing PR."]}, {"number": 33105, "title": "tf.data.Dataset.interleave does not seem to respect num_parallel_calls argument", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): *Yes*\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *Linux 5.3.1-arch1-1-ARCH GNU/Linux*\r\n- TensorFlow installed from (source or binary): *binary*\r\n- TensorFlow version (use command below): *v2.0.0-rc2-26-g64c3d38 2.0.0*\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nWhile using `tf.data.Dataset.interleave` on a Dataset of 9 elements, with the following arguments:\r\n- cycle_length = 2\r\n- block_length = 1\r\n- num_parallel_calls = 2\r\n\r\n6 threads seems to be launched concurrently.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect the function to be called concurrently by pair:\r\n```\r\n(call 1, call 2)\r\n(call 3, call 4)\r\n(call 5, call 6)\r\n(call 7, call 8)\r\n(call 9)\r\n```\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport time\r\nimport timeit\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.data.ops import dataset_ops\r\nfrom tensorflow_core.python.data.ops.readers import _create_or_validate_filenames_dataset, \\\r\n    _create_dataset_reader\r\nfrom tensorflow.python.framework import tensor_spec\r\n\r\n# Clone of TFRecordDataset to add some customization (see creator_fn)\r\nclass MyTFRecordDataset(dataset_ops.DatasetV2):\r\n    def _inputs(self):\r\n        return self._impl._inputs()  # pylint: disable=protected-access\r\n\r\n    @property\r\n    def element_spec(self):\r\n        return tensor_spec.TensorSpec([], tf.dtypes.string)\r\n\r\n    def __init__(self, filenames, compression_type=None, buffer_size=None,\r\n                 num_parallel_reads=None):\r\n        filenames = _create_or_validate_filenames_dataset(filenames)\r\n\r\n        self._filenames = filenames\r\n        self._compression_type = compression_type\r\n        self._buffer_size = buffer_size\r\n        self._num_parallel_reads = num_parallel_reads\r\n\r\n        def creator_fn(filename):\r\n            tf.print(\"Creator_fn\", filename.numpy())\r\n            start = time.perf_counter()\r\n            time.sleep(0.5)\r\n            result = tf.constant([str(x) for x in range(7)])\r\n            tf.print(\"Reading time\", time.perf_counter() - start)\r\n            return result\r\n\r\n        self._impl = _create_dataset_reader(\r\n            lambda x: tf.data.Dataset.from_tensor_slices(tf.py_function(creator_fn, [x], tf.string)),\r\n            filenames, num_parallel_reads)\r\n        variant_tensor = self._impl._variant_tensor  # pylint: disable=protected-access\r\n        super(MyTFRecordDataset, self).__init__(variant_tensor)\r\n\r\ndef dataset_interleave_ds():\r\n    ds = tf.data.Dataset.from_tensor_slices(\r\n        [str(x) for x in range(9)]\r\n    ).interleave(\r\n        MyTFRecordDataset,\r\n        cycle_length=2,\r\n        block_length=1,\r\n        num_parallel_calls=2\r\n    )\r\n\r\n    return ds\r\n\r\ndef main():\r\n    ds = dataset_interleave_ds()\r\n\r\n    def iterate():\r\n        tf.print(\"Iterating\")\r\n        for i, s in ds.enumerate():\r\n            tf.print(\"Iteration\", i, s.shape)\r\n            time.sleep(0.0)\r\n\r\n    tf.print(timeit.timeit(iterate, number=1))\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\n\r\nWith `num_parallel_calls` set to `None` I\u00a0obtain the expected behavior:\r\n```\r\nIterating\r\nCreator_fn b'0'\r\nReading time 0.5008154709994415\r\nIteration 0 TensorShape([])\r\nCreator_fn b'1'\r\nReading time 0.5007261740011018\r\nIteration 1 TensorShape([])\r\n```\r\nTwo files loaded, iterate over them\r\n```\r\nIteration 2 TensorShape([])\r\nIteration 3 TensorShape([])\r\nIteration 4 TensorShape([])\r\nIteration 5 TensorShape([])\r\nIteration 6 TensorShape([])\r\nIteration 7 TensorShape([])\r\nIteration 8 TensorShape([])\r\nIteration 9 TensorShape([])\r\nIteration 10 TensorShape([])\r\nIteration 11 TensorShape([])\r\nIteration 12 TensorShape([])\r\nIteration 13 TensorShape([])\r\n```\r\nEnd of sequences. Open new files:\r\n```\r\nCreator_fn b'2'\r\nReading time 0.500802348000434\r\nIteration 14 TensorShape([])\r\nCreator_fn b'3'\r\nReading time 0.5007076660003804\r\nIteration 15 TensorShape([])\r\nIteration 16 TensorShape([])\r\nIteration 17 TensorShape([])\r\nIteration 18 TensorShape([])\r\nIteration 19 TensorShape([])\r\nIteration 20 TensorShape([])\r\nIteration 21 TensorShape([])\r\nIteration 22 TensorShape([])\r\nIteration 23 TensorShape([])\r\nIteration 24 TensorShape([])\r\nIteration 25 TensorShape([])\r\nIteration 26 TensorShape([])\r\nIteration 27 TensorShape([])\r\n```\r\nAnd so on:\r\n```\r\nCreator_fn b'4'\r\nReading time 0.5007231710005726\r\nIteration 28 TensorShape([])\r\nCreator_fn b'5'\r\nReading time 0.5007872259993746\r\n[...]\r\n4.591206809000141\r\n```\r\nTotal time is ~4.5 seconds, according the 0.5s sleep executed 9 times.\r\n\r\nWith `num_parallel_calls` set to `2` I expect to divide my execution time by 2, each pair of \"files\" being read in parallel.\r\n\r\n```\r\nIterating\r\nCreator_fn b'0'\r\nCreator_fn b'1'\r\nCreator_fn b'3'\r\nCreator_fn b'5'\r\nCreator_fn b'4'\r\nCreator_fn b'2'\r\nReading time 0.5008060520012805\r\nReading time 0.5010762649999378\r\nReading time 0.501729752000756\r\nReading time 0.5024584279999544\r\nReading time 0.503098459001194\r\nReading time 0.5040528110002924\r\n```\r\n6 Files opened in parallel...\r\n```\r\nIteration 0 TensorShape([])\r\n[...]\r\nIteration 14 TensorShape([])\r\nCreator_fn b'7'\r\nCreator_fn b'6'\r\nIteration 15 TensorShape([])\r\n[...]\r\nIteration 28 TensorShape([])\r\nCreator_fn b'8'\r\nIteration 29 TensorShape([])\r\n[...]\r\nIteration 41 TensorShape([])\r\nReading time 0.5002393860013399\r\nReading time 0.5006544690004375\r\nIteration 42 TensorShape([])\r\nIteration 43 TensorShape([])\r\nIteration 44 TensorShape([])\r\nIteration 45 TensorShape([])\r\nIteration 46 TensorShape([])\r\nReading time 0.5014183660005074\r\nIteration 47 TensorShape([])\r\n[...]\r\nIteration 62 TensorShape([])\r\n1.047351510000226\r\n```\r\nExecution time divided by 4!\r\n\r\nBy the way, what is the meaning of `num_parallel_calls` set to 1. 1 parallel call is 2 threads or it should be sequential?", "comments": ["Could reproduce the issue with Tensorflow Version 2.0. Here is the [Gist](https://colab.sandbox.google.com/gist/rmothukuru/4c2086c7e770d6a51d83983208ed88f8/33105.ipynb). Thanks!", "Parallel interleave with `num_parallel_calls=X` also ends up using 2*X background threads to prefetch small amount of data for future cycle elements ahead time. The primary motivation for this is to overlap opening files in remote storage systems (e.g. GCS) with useful computation.\r\n\r\nIn other words, as the size of the file you interleave over increases, the speed up you would see from using `num_parallel_calls=X` would approach X in the limit.\r\n\r\n`num_paralle_calls=1` is different from `num_parallel_calls=None`. The former will use a kernel that decouples (and thus parallelizes the computation of) the producer and consumer (and also performs the prefetching described above), while the latter will use a kernel the performs the interleave synchronously using the same thread for the producer and consumer.\r\n\r\nLet me know if you have any additional questions. If not, please close the issue.", "OK. The observed behavior is probably linked to the prefetching mechanism.\r\nThanks for the clarification about \"`1` vs. `None`\".", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33105\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33105\">No</a>\n"]}]