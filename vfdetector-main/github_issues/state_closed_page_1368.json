[{"number": 12032, "title": "Save and restore feature request", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/v5.1\r\n- **GPU model and memory**: GeForce GTX TITAN X / 12205MiB\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n I am sorry to bother you all, here this is not a bug but, in my view, a feature request. \r\n\r\nI have trained a model and initialized a Saver instance by defining\r\n\r\n\r\n<!-- language: python -->\r\n\r\n    value_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='global/old_scope')\r\n    value_list.extend(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='global/actor_critic'))\r\n    saver = tf.train.Saver(value_list, max_to_keep=100)\r\n\r\n    with tf.Session(config=tf_configs) as sess:\r\n        coord = tf.train.Coordinator()\r\n        if load_model:\r\n            print('Loading Model...')\r\n            ckpt = tf.train.get_checkpoint_state(model_path)\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n        else:\r\n            sess.run(tf.global_variables_initializer())\r\n\r\nAnd later in a new sub-scope, I added a new layer, with the same `saver` defined above, I trained the model, however, I found that weights of the new layer were not saved.\r\n\r\nHere is my network\r\n\r\n<!-- language: python -->\r\n\r\n    with tf.variable_scope(scope):\r\n        with tf.variable_scope('old_scope'):\r\n            self.inputs = tf.placeholder(shape=[None, 80, 80, 1], dtype=tf.float32)\r\n            self.conv_1 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.inputs, num_outputs=32,\r\n                                      kernel_size=[8, 8], stride=4, padding='SAME')\r\n            self.conv_2 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.conv_1, num_outputs=64,\r\n                                      kernel_size=[4, 4], stride=2, padding='SAME')\r\n            self.conv_3 = slim.conv2d(activation_fn=tf.nn.relu, inputs=self.conv_2, num_outputs=64,\r\n                                      kernel_size=[3, 3], stride=1, padding='SAME')\r\n            self.fc = slim.fully_connected(slim.flatten(self.conv_3), 512, activation_fn=tf.nn.elu)\r\n\r\n        with tf.variable_scope('added_layer'):\r\n            self.fc_1 = slim.fully_connected(self.fc, 512, activation_fn=tf.nn.elu)\r\n\r\n        with tf.variable_scope('actor_critic'):\r\n            # Output layers for policy and value estimations\r\n            self.policy = slim.fully_connected(self.fc_1,\r\n                                             cfg.ACTION_DIM,\r\n                                             activation_fn=tf.nn.softmax, \r\n                                             biases_initializer=None)\r\n            self.value = slim.fully_connected(self.fc_1,\r\n                                              1,\r\n                                              activation_fn=None,\r\n                                              biases_initializer=None)\r\n\r\nAnd I found that the [`var_list`][1] defines values to be restored and saved. But in my case, there is no checkpoint data of the new layer in the checkpoint file. \r\n\r\nSince before adding the new layer, I have trained the model and save the checkpoint data, and then after adding the new layer, I wanna train the network.\r\n\r\nAnd I can define a new instance of Saver to save the model\r\n\r\n`new_saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=GLOBAL_SCOPE))`\r\n\r\nHowever, I think it is not an elegant way to do so.\r\n\r\nAnd can you add a feature to restore some values specified by users and also save some specified values when saving?\r\n\r\nAnd in fact, it is a question asked by me on [SO](https://stackoverflow.com/questions/45502149/tensorflow-save-and-restore-model-after-adding-one-layer)\r\n\r\n  [1]: https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Closing out this feature request, as the [SavedModel](https://www.tensorflow.org/guide/saved_model) format in TensorFlow 2.0 should meet your needs. Thank you! \ud83d\ude0a "]}, {"number": 12031, "title": "Fix typos", "body": "This PR fixes some typos: `attempst`, `lenght`, and `acessed`.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12030, "title": "print out 'nan' for simple linear regression model ", "body": "```\r\nroot@1cf079dc9729:~/src# python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.2.0-5-g435cdfc', '1.2.1')\r\n```\r\nThe source code is very simple and it is from: https://www.tensorflow.org/get_started/get_started\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Model parameters\r\nW = tf.Variable([.3], dtype=tf.float32)\r\nb = tf.Variable([-.3], dtype=tf.float32)\r\n# Model input and output\r\nx = tf.placeholder(tf.float32)\r\nlinear_model = W * x + b\r\ny = tf.placeholder(tf.float32)\r\n# loss\r\nloss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\r\n# optimizer\r\noptimizer = tf.train.GradientDescentOptimizer(0.01)\r\ntrain = optimizer.minimize(loss)\r\n# training data\r\nx_train = [1,2,3,4,35]\r\ny_train = [0,-1,-2,-3,-34]\r\n# training loop\r\ninit = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init) # reset values to wrong\r\nfor i in range(1000):\r\n  sess.run(train, {x:x_train, y:y_train})\r\n\r\n#x_test = [5, 100, 345] #[5,100,345,-99.66]\r\n#y_test = [-4, -99, -344] #[-4,-99,-344,-101.66]\r\n# evaluate training accuracy\r\ncurr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:x_train, y:y_train})\r\nprint(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\r\n```\r\nthe output:\r\n```\r\nW: [ nan] b: [ nan] loss: nan\r\n```\r\nif i delete the last element for training data:\r\n```\r\nx_train = [1,2,3,4]\r\ny_train = [0,-1,-2,-3]\r\n```\r\nthe output is right:\r\n```\r\nW: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\r\n```", "comments": ["x=35, y=-34 is too large or small? the target function is y=1-x, right?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12029, "title": "Add int64_t typemap for swig", "body": "```Session.list_devices()```  gives  wrong memory bytes, and after the call, swig complains a memory leak: \r\n```\r\n# on Win7, 1.3.0RC1\r\n>>> s.list_devices()\r\n[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 69731600)]\r\n>>> 1\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\n1\r\n\r\n# on Linux, 1.3.0RC2\r\n>>> s.list_devices()\r\n[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 94080344021520), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 94080320350144)]\r\n>>> 1\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\nswig/python detected a memory leak of type 'int64_t *', no destructor found.\r\n1\r\n\r\n```\r\nThe reason is the CAPI ```TF_DeviceListMemoryBytes``` returns value of type 'int64_t', for which swig can not generate a correct wrapper:\r\n```\r\nTF_CAPI_EXPORT extern int64_t TF_DeviceListMemoryBytes(\r\n    const TF_DeviceList* list, int index, TF_Status*);\r\n```\r\n```\r\nSWIGINTERN PyObject *_wrap_TF_DeviceListMemoryBytes(PyObject *SWIGUNUSEDPARM(self), PyObject *args) {\r\n  //...\r\n  result = TF_DeviceListMemoryBytes((TF_DeviceList const *)arg1,arg2,arg3);\r\n  // problem  here\r\n  resultobj = SWIG_NewPointerObj((new int64_t(static_cast< const int64_t& >(result))), }\r\n\r\n //...\r\n}\r\n```\r\nThis wrapper causes the python side code read the address rather than the value of the int64_t variable (which is why memory bytes is wrong). And since there is no destructor for int64_t, swig complains about it.\r\n\r\nThere is another CAPI ```TF_Dim``` that has the same problem.\r\n\r\nThe problem is solved by adding a ```int64_t```  typemap for swig, then the wrapper will be like this:\r\n```\r\n    result = TF_DeviceListMemoryBytes((TF_DeviceList const *)arg1,arg2,arg3);\r\n    //...\r\n    resultobj = PyLong_FromLongLong(result); \r\n```", "comments": ["Can one of the admins verify this patch?", "@jart Friendly ping for review. Thanks!", "@skye take a quick look?", "Jenkins, test this please.", "Transient failures, trying again.\r\n\r\nJenkins, test this please.", "Transient failure on Windows, trying again.\r\n\r\nJenkins, test this please."]}, {"number": 12028, "title": "Tensorboard fails to add summaries without any warning", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 12027, "title": "Fix build issue in c++14 for condition_variable", "body": "This fix fixes the issue raised in #12017 where `condition_variable_any` has to be used in c++14.\r\n\r\nThis fix defines ConditionVariableForMutex as `std::condition_variable_any` in c++14 and `std::condition_variable` otherwise.\r\n\r\nThe fix is verified with\r\n```\r\nbazel build -s --config=opt --cxxopt=-std=c++14 ...\r\n```\r\n\r\nThis fix fixes #12017.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman and @vrv to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins Test this please", "@tensorflow-jenkins test this please", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (27/25 (108%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext7.iad.github.net ([192.30.252.198]:36921 helo=github-smtp2a-ext-cp1-prd.iad.github.net)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.89)\n\t(envelope-from <noreply@github.com>)\n\tid 1deqzR-0001hV-Vg\n\tfor mazecreator@mazecreator.com; Mon, 07 Aug 2017 17:59:44 -0500\nDate: Mon, 07 Aug 2017 16:10:07 -0700\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1502147407;\n\tbh=2aZ/WyRSvWnUPS4jcNY7PWof7l7vVce+6y5FIIdMiCQ=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=NKzYnkkfMaovDTeH5AQ+zdovYzsCNosLfgN2ol76RuhIUEFwpYeKr7l8VVmin3AKS\n\t KxP0WJEcAzQHfP+RMIlN/Rq79I5vspz9J3evXalbNcEallRedMt6bB9VhspS71QnRU\n\t 4wDZM1TBCcMnRDvPGtA4RDKhJ/Nm4OgMX/kmpzMM=\nFrom: Rasmus Munk Larsen <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12027/c320804718@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12027@github.com>\nReferences: <tensorflow/tensorflow/pull/12027@github.com>\nSubject: Re: [tensorflow/tensorflow] Fix build issue in c++14 for\n condition_variable (#12027)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5988f34f91a7d_c3b3fccd759bc38387836\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: rmlarsen\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a04b3501957714e94b7e26ae0172464ceec1d0c88f92cf0000000115a0b54f92a169ce0ec6a72f@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoDEka7ONoea0l5kGet6XwPWeIeeOks5sV5lPgaJpZM4OtQ1m>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n\n----==_mimepart_5988f34f91a7d_c3b3fccd759bc38387836\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@tensorflow-jenkins test this please\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12027#issuecomment-320804718\n----==_mimepart_5988f34f91a7d_c3b3fccd759bc38387836\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><a href=\"https://github.com/tensorflow-jenkins\" class=\"user-mention\">@tensorflow-jenkins</a> test this please</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12027#issuecomment-320804718\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoMkQEDJ54Kk3Jyfi5Arxb_wxLAFuks5sV5lPgaJpZM4OtQ1m\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoIl0GiKMZsiKYpS29s91_QR3o2Itks5sV5lPgaJpZM4OtQ1m.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12027#issuecomment-320804718\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@rmlarsen in #12027: @tensorflow-jenkins test this please\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12027#issuecomment-320804718\"}}}</script>\n----==_mimepart_5988f34f91a7d_c3b3fccd759bc38387836--\n"]}, {"number": 12026, "title": "Add loss to summary during estimator.train()", "body": "This would be useful for visualizing the losses during training phase. ", "comments": ["@terrytangyuan, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ispirmustafa, @xiejw and @tensorflower-gardener to be potential reviewers.", "@ispirmustafa Thanks! I am going to close this one since we can do this via a run hook. "]}, {"number": 12025, "title": "Parallelize bottleneck creation in retraining script", "body": "Hi\r\n\r\nIt might be possible to parallelize the creation of the bottleneck files in the Inception retraining script. Currently it can only do a few images per second and if you have 100s of thousands of images, it takes forever. I could do it myself also and create a pull request, if that's fine.\r\n\r\nThanks", "comments": ["If you have an improved version, you should just go ahead with a pull request (I'm guessing it's under files in tensorflow/models repo)"]}, {"number": 12024, "title": "tf.contrib.distributions.percentile returning AttributeError", "body": "Hello,\r\n\r\nI am trying to utilize tf.contrib.distributiions.percentile as described in:\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/distributions/percentile \r\n\r\n- OS Platform and Distribution :  Ubuntu 14.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version : 1.2.0\r\n- Python version: 3.4.5\r\n\r\nMy invocation is as follows:\r\npctl_val = tf.contrib.distributions.percentile(grad_values, pctl)\r\n\r\nHere is the error message that I get:\r\nAttributeError: 'module' object has no attribute 'percentile'\r\n\r\nI understand that the percentile feature was not available in 1.1.0 and it is now available in 1.2.0. Still, why do I get the error?", "comments": ["With Tensorflow version 1.2.0, I am not getting this error. Could you please check if your Tensorflow version has been upgraded correctly. \r\nFeel free to reopen this issue if this problem persists."]}, {"number": 12023, "title": "Add SavedModel support to tfcompile", "body": "This might be a little bit of a hacky way to add support but it appears to work. Long term it would be better to make SavedModel a first class citizen.", "comments": ["Can one of the admins verify this patch?", "@tatatodd Friendly ping for review", "Sorry for the delay.  FYI I'll be out of the office until after Labor Day 9/5.  I hope that you're not in a rush to get this in.\r\n\r\nI'd taken a look previously, and as you say, the approach isn't super-clean.  So it'll take some time on the review.", "@tatatodd feel free to outline a better approach", "@elibixby there seems to be some merge conflicts. Can you address those so this is ready for review again.", "@sb2nov Committed a merge resolving conflicts", "Jenkins, test this please.", "Sorry for the delay.  I fear the approach in this PR will be an overall maintenance burden.\r\n\r\n@suharshs and I have discussed making SavedModel more of a first-class citizen in general, for all of our tools.  He plans to add a C++ library call to perform freeze_graph functionality.  Once that support is in, I'll update tfcompile to take SavedModel as input, and hook up a call to his C++ library to the rest of the compilation pipeline.  The advantage of this approach is that we won't need a pre-processing python script, and we won't need a separate tf_saved_model_library skylark rule.\r\n\r\n@elibixby what do you think?  ", "> I'll update tfcompile to take SavedModel as input, and hook up a call to his C++ library to the rest of the compilation pipeline. The advantage of this approach is that we won't need a pre-processing python script, and we won't need a separate tf_saved_model_library skylark rule.\r\n\r\nYes, please.", "SGTM. This was just meant to be a stopgap to show people how to do it if support was a long way off. If this is on the horizon I'm happy closing this."]}, {"number": 12022, "title": "Error exporting TensorForestEstimator model for serving", "body": "### Problem\r\nI am trying to host a TensorForestEstimator model on Google Cloud's ML Engine. Everything works right, but at the very end the model fails to export with stack trace:\r\n\r\n```\r\nTraceback (most recent call last):\r\n[...]\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 502, in train_and_evaluate\r\n  export_results = self._maybe_export(eval_result)\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 597, in _maybe_export\r\n  eval_result=eval_result))\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/export_strategy.py\", line 87, in export\r\n  return self.export_fn(estimator, export_path, **kwargs)\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 412, in export_fn\r\n  checkpoint_path=checkpoint_path)\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1280, in export_savedmodel\r\n  actual_default_output_alternative_key)\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 252, in build_all_signature_defs\r\n  for input_key, inputs in input_alternatives.items()\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 254, in <dictcomp>\r\n  in output_alternatives.items()}\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py\", line 119, in build_standardized_signature_def\r\n  input_tensors, output_tensors)\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py\", line 146, in predict_signature_def\r\n  signature_constants.PREDICT_METHOD_NAME)\r\nFile \"/root/.local/lib/python2.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py\", line 45, in build_signature_def\r\n  signature_def.outputs[item].CopyFrom(outputs[item])\r\nTypeError: None has type NoneType, but expected one of: bytes, unicode\r\n```\r\n\r\nBased on that trace, I'm thinking the error is in the make_export_strategy function with `default_output_alternative_key=None`. So what I did is set `default_output_alternative_key='default'` but then got the error:\r\n\r\n```\r\nValueError: Requested default_output_alternative: default, but available output_alternatives are: [None]\r\n```\r\n\r\nSo this shows that there are no output alternatives and my model is single-headed. Here is the code:\r\n\r\n```\r\ndef serving_input_fn():\r\n    feature_placeholders = {\r\n    column['name']: tf.placeholder(dtype=column['dtype'], shape=[None])\r\n    for column in columns_list if column['derived'] == 'N' and column['column_role'] != 'label'\r\n    }\r\n\r\n    features = {\r\n        key: tf.expand_dims(tensor, -1)\r\n        for key, tensor in feature_placeholders.items()\r\n    }\r\n\r\n    return InputFnOps(\r\n        features=features,\r\n        labels=None,\r\n        default_inputs=feature_placeholders\r\n    )\r\n\r\ndef get_experiment_fn(args):\r\n    def _experiment(run_config, hparams):\r\n        return Experiment(\r\n            estimator=TensorForestEstimator(\r\n                params=ForestHParams(\r\n                    num_trees=args.num_trees,\r\n                    max_nodes=10000,\r\n                    min_split_samples=2,\r\n                    num_features=7,\r\n                    num_classes=args.num_projections,\r\n                    regression=True\r\n                ),\r\n                model_dir=args.job_dir,\r\n                graph_builder_class=RandomForestGraphs,\r\n                config=run_config,\r\n                report_feature_importances=True,\r\n            ),\r\n            train_input_fn=get_input_fn(\r\n                project_name=args.project,\r\n                data_location=args.train_data,\r\n                dataset_size=args.train_size,\r\n                batch_size=args.train_batch_size\r\n            ),\r\n            train_steps=args.train_steps,\r\n            eval_input_fn=get_input_fn(\r\n                project_name=args.project,\r\n                data_location=args.eval_data,\r\n                dataset_size=args.eval_size,\r\n                batch_size=args.eval_batch_size\r\n            ),\r\n            eval_steps=args.eval_steps,\r\n            eval_metrics=get_eval_metrics(),\r\n            export_strategies=[\r\n                make_export_strategy(\r\n                    serving_input_fn,\r\n                    default_output_alternative_key=None,\r\n                    exports_to_keep=1\r\n                )\r\n            ]\r\n        )\r\n    return _experiment\r\n\r\n\r\ndef main():\r\n    args = get_arg_parser().parse_args()\r\n\r\n    learn_runner.run(\r\n        experiment_fn=get_experiment_fn(args),\r\n        run_config=RunConfig(model_dir=args.job_dir),\r\n        hparams=HParams(**args.__dict__)\r\n    )\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nThis seems like a bug, but I could be wrong.\r\n\r\n### System information\r\n```\r\n== cat /etc/issue ===============================================\r\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.4\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin16.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin mbmagenic12 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar  3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.1.0.post1)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n", "comments": ["We ran into this issue as well. I don't think your problem is related to `default_output_alternative_key` being `None` rather, it looks like `TensorForestEstimator` currently populates `model_ops.predictions` only if a `keys_column` is provided[1]. Without `model_ops.predictions` populated, `default_outputs` in `saved_model_export` will lack a default key[2]. As a result, this will make the model end up with no heads and eventually the export will fail. \r\n\r\nTLDR: Populating `keys_column` in `TensorForestEstimator` should fix your problem. That being said, it's definitely a bug if an optional argument is really required for the model to function properly. So I would leave this issue open.\r\n\r\nAs a side: I did not really understand why requiring `keys_name`[3] to be part of the incoming`feature_columns` list (after all, it's not necessary a feature) and then pop it?\r\n\r\n[1] \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/client/random_forest.py#L235\r\n[2] \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/utils/saved_model_export_utils.py#L217\r\n[3] \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/client/random_forest.py#L157\r\n", "Should also mention that the workaround (populating `key_column`) only helps with `tensorflow 1.3`.", "It looks like upgrading to `tensorflow 1.3` fixed the issue. I did not have to even populate `keys_name`, the graph was saved as normal after training/evaluation.\r\n\r\nHowever, I need a fix for `tensorflow 1.2` since I am using Google Cloud ML Engine and 1.2 is the highest version supported.", "@caisq, are you able to take a look?", "@justinshapiro: Are you able to resolve it now? Any temporary wayout?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Assignee @skye: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I'm assuming this is resolved now. Please comment or reopen if I'm mistaken."]}, {"number": 12021, "title": "Disable codeowners due to noisiness.", "body": "", "comments": ["This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from o3.sgmail.github.com ([192.254.112.98]:44973)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES128-GCM-SHA256:128)\n\t(Exim 4.89)\n\t(envelope-from <bounces+848413-e6d9-mazecreator=mazecreator.com@sgmail.github.com>)\n\tid 1ddNhv-0004ZI-LY\n\tfor mazecreator@mazecreator.com; Thu, 03 Aug 2017 16:31:34 -0500\nDKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=github.com; \n\th=from:reply-to:to:cc:in-reply-to:references:subject:mime-version:content-type:content-transfer-encoding:list-id:list-archive:list-post:list-unsubscribe; \n\ts=s20150108; bh=N6A8IaLAOEM9eUfsPiiTl1MNE34=; b=lWdPQhCnWcrrrdcK\n\tLiHUu4c52sKbklXtXFJIHBqPGdypan3UGlDVvCINkz0RtvMtqCoaTJwPLXrFAnSO\n\t9Gaj+OBoQ7nsHiExnGHua2/rI/VGNWprKKrJ/7lf/2OO4Fjwndth7LNmd2kDHGOx\n\tEVDpv6nA1yH5FDnY1uJGU8dgZs4=\nReceived: by filter1089p1mdw1.sendgrid.net with SMTP id filter1089p1mdw1-8700-5983989A-24\n        2017-08-03 21:41:46.401708476 +0000 UTC\nReceived: from github-smtp2b-ext-cp1-prd.iad.github.net (github-smtp2b-ext-cp1-prd.iad.github.net [192.30.253.17])\n\tby ismtpd0026p1mdw1.sendgrid.net (SG) with ESMTP id bp0vA2KyQ56t1AAVnK_W6A\n\tfor <mazecreator@mazecreator.com>; Thu, 03 Aug 2017 21:41:46.399 +0000 (UTC)\nDate: Thu, 03 Aug 2017 21:41:46 +0000 (UTC)\nFrom: Martin Wicke <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/pull/12021/review/54231242@github.com>\nIn-Reply-To: <tensorflow/tensorflow/pull/12021@github.com>\nReferences: <tensorflow/tensorflow/pull/12021@github.com>\nSubject: Re: [tensorflow/tensorflow] Disable codeowners due to noisiness.\n (#12021)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5983989a130a4_7f053fa04197fc2c2904b4\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: martinwicke\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a042bc9fe58ab6a353194e689cb5df3d95601f903392cf00000001159b5a9a92a169ce0ec5c9c8@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoOp-95QsW0MIBF10e2axiGH3AmNMks5sUj6agaJpZM4OtBdJ>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-SG-EID: BJ4qYjf5a3yL0lCrdDNghY4YYR+k1a+cluU6wEX1Jqy2HzE2PmbFJj88lDtIg6ffEpzgAvgXGReJd1\n 3Z9GKbSAVrIJTN7fnQcctOV4+sCfr8ns418y+AYkr+G1kWuq468NeDiY/ODeeG2Of+zSwEXbbUavTm\n 9zAaMMBaedZ0p4m+NtxqTjQwedNVuIh5mBKFZpbCNes94GfG+M55FjmI3jzukb8VQjeeOZKXJUqa7c\n 0=\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n----==_mimepart_5983989a130a4_7f053fa04197fc2c2904b4\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nmartinwicke approved this pull request.\n\n\n\n\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/pull/12021#pullrequestreview-54231242\n----==_mimepart_5983989a130a4_7f053fa04197fc2c2904b4\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p><b>@martinwicke</b> approved this pull request.</p>\n\n\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/pull/12021#pullrequestreview-54231242\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoEjsJhPuz8VfL8d03RKKrbuvSdfFks5sUj6agaJpZM4OtBdJ\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoB8Bt5iWk57e4FKAV8xWbqDWpWQzks5sUj6agaJpZM4OtBdJ.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/pull/12021#pullrequestreview-54231242\"></link>\n  <meta itemprop=\"name\" content=\"View Pull Request\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Pull Request on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@martinwicke approved #12021\"}],\"action\":{\"name\":\"View Pull Request\",\"url\":\"https://github.com/tensorflow/tensorflow/pull/12021#pullrequestreview-54231242\"}}}</script>\n----==_mimepart_5983989a130a4_7f053fa04197fc2c2904b4--\n", "@aselle fyi"]}, {"number": 12020, "title": "Branch 164170971", "body": "", "comments": []}, {"number": 12018, "title": "Missing tensorflow/core/debug/debug_service.grpc.pb.h header file", "body": "Hi,\r\n\r\ndebug_io_utils.h includes a header file tensorflow/core/debug/debug_service.grpc.pb.h, which is missing from the repo. This causes build on Linux failed. \r\n\r\n`// TODO(cais): Support grpc:// debug URLs in open source once Python grpc\r\n//   genrule becomes available. See b/23796275.\r\n\r\n#ifndef PLATFORM_WINDOWS\r\n#include \"tensorflow/core/debug/debug_service.grpc.pb.h\"\r\n`\r\nAnyone see this issue?\r\n\r\nThanks,\r\nRLE", "comments": ["@caisq I think this change may relates to the problem: https://github.com/tensorflow/tensorflow/commit/41803db36d4f4a3239bd81e5d460eb0e6e2eea88\r\n\r\nThat changes #if defined(PLATFORM_GOOGLE) to #ifndef PLATFORM_WINDOWS, which essentially allows it to #include \"tensorflow/core/debug/debug_service.grpc.pb.h\" on Linux but that file does not exist on the master branch.\r\n\r\nCould you help taking a look?", "`debug_service_grpc.pb.h` is an automatically generated file from this bazel BUILD rule:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/debug/BUILD#L45\r\n\r\nSo there should be no problem building TF is you are using bazel. What build tool are you using?", "I used Linux Continuous Integration Build steps from this link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md", "So I see that you are running cmake build on Linux. This is our nightly build:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-cmake-cpu/\r\n\r\nIt seems to be okay. You can look at the log to see what commands / configuration it uses:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-cmake-cpu/368/consoleFull", "On Ubuntu 16 with anaconda 2.7, when running \r\n\r\ncmake -DCMAKE_BUILD_TYPE=Release ../tensorflow/contrib/cmake\r\nmake -j6 all\r\n\r\nI get the error: fatal error: tensorflow/core/debug/debug_service.grpc.pb.h: No such file or directory\r\n\r\nAny ideas how to resolve this?", "Encountered the similar issue. The \r\n```\r\ntensorflow/core/debug/debug_service.pb.h\r\n```\r\nwas generated but not\r\n```\r\ntensorflow/core/debug/debug_service.grpc.pb.h\r\n```\r\nso the make fails.\r\n\r\nI assume `*.grpc.pb.h` might be generated with grpc plugins though I am not familiar with cmake so not sure how to fix it.", "Confirmed on macOS 10.12.4 using TensorFlow 1.3.0 and CMake 3.7.2.", "Created a PR #13145 that should fix the cmake issue on Linux."]}, {"number": 12017, "title": "stream_executor/platform/mutex.h doesn't compile under C++14", "body": "Please go to Stack Overflow for help and support:\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X with clang/llvm 5.0\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n\r\n### Describe the problem\r\nThere's an ifdef in mutex.h that uses shared_timed_mutex when compiled as C++14 and up. The file doesn't compile because C++14 requires using condition_variable_any rather than condition_variable with that kind of mutex.\r\n\r\n### Source code / logs\r\n```\r\nIn file included from external/org_tensorflow/tensorflow/stream_executor/platform/mutex.h:25:\r\nexternal/org_tensorflow/tensorflow/stream_executor/platform/default/mutex.h:86:26: error: no matching member function for call to 'wait_for'\r\n  std::cv_status s = cv->wait_for(*mu, std::chrono::milliseconds(ms));\r\n                     ~~~~^~~~~~~~\r\nexternal/org_chromium_clang_mac/include/c++/v1/__mutex_base:404:21: note: candidate function not viable: no known conversion from 'perftools::gputools::mutex_lock' to 'unique_lock<std::__1::mutex> &' for 1st argument\r\ncondition_variable::wait_for(unique_lock<mutex>& __lk,\r\n                    ^\r\nexternal/org_chromium_clang_mac/include/c++/v1/__mutex_base:426:21: note: candidate function template not viable: requires 3 arguments, but 2 were provided\r\ncondition_variable::wait_for(unique_lock<mutex>& __lk,\r\n```", "comments": ["It's possible we don't guarantee TF to build with C++14. Not sure whether this is a bug or a feature request.  @martinwicke ?", "We're not guaranteeing it. But we'd welcome a PR to fix it, if possible without breaking other things. ", "Added a PR #12027 for the fix in c++14."]}, {"number": 12016, "title": "A single value placeholder cause an GPU memory warning.", "body": "I train my model on GTX 980 with 4 GB memory. The tensorflow version is 1.1.0. Python is 3.6.0. \r\n\r\nThe example code is on this GitHub link (https://github.com/dennybritz/cnn-text-classification-tf). Since the code is kind of long, I do not want to copy them. IF someone can run it, I will appreciate it.\r\n\r\nThe problem is when I increase the epoch, It cause a memory error on my GPU. But, when delete the variable placeholder \"self.dropout_keep_prob\", there would be no error message. I do not know why, my code running on CPU is totally fine. \r\n\r\nBelow is the warning message:\r\n2017-08-03 15:38:52.537616: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.84GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.", "comments": []}, {"number": 12015, "title": "Missing file pywrap_tensorflow_internal", "body": "\r\nPlease close this issue. This issue was due to trying to import tensorflow from the source directory", "comments": []}, {"number": 12014, "title": "Testing sanity.", "body": "", "comments": []}, {"number": 12013, "title": "Make layout optimizer_test manual due to Grappler being off in the r1.3 branch.", "body": "", "comments": ["Can one of the admins verify this patch?", "@tfboyd, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @keveman to be potential reviewers.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 12012, "title": "Make plugin_data optional instead of repeated", "body": "This patches tensorflow/tensorflow#11952 into master. Here is that PR's description:\r\n\r\nEvery summary op writes data for a single plugin to process. Hence, each\r\nSummaryMetadata proto should have a single PluginData optional field\r\n(instead of a repeated one). This removes much complexity from\r\nTensorBoard logic that loops over the plugin data. It also simplifies\r\nthe SQL schema - it can now enforce a one-to-one relationship between\r\nsummary op and plugin.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "I am canceling this PR and will make these changes internally instead."]}, {"number": 12011, "title": "Quantized graph not running with commit:bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b", "body": "OS: Ubuntu 16.04 64bits\r\n Android Version: 7.1 (Nougat)\r\n NDK Version: android-ndk-r12b\r\n\r\ncommit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b\r\nAuthor: Alan Yee <alyee@ucsd.edu>\r\nDate:   Mon Jul 24 22:46:38 2017 -0700\r\n\r\n\r\n\r\nLOG:\r\n\r\n```\r\nnative : benchmark_model.cc:405 Input layers: [Variable]\r\nnative : benchmark_model.cc:406 Input shapes: [1,227,227,3]\r\nnative : benchmark_model.cc:407 Input types: [float]\r\nnative : benchmark_model.cc:408 Output layers: [prob]\r\nnative : benchmark_model.cc:409 Num runs: [50]\r\nnative : benchmark_model.cc:410 Inter-run delay (seconds): [-1.0]\r\nnative : benchmark_model.cc:411 Num threads: [16]\r\nnative : benchmark_model.cc:412 Benchmark name: []\r\nnative : benchmark_model.cc:413 Output prefix: []\r\nnative : benchmark_model.cc:414 Show sizes: [0]\r\nnative : benchmark_model.cc:415 Warmup runs: [2]\r\nnative : benchmark_model.cc:54 Loading TensorFlow.\r\nnative : benchmark_model.cc:61 Got config, 0 devices\r\ncan't determine number of CPU cores: assuming 4\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseAnd\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseAnd\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseAnd\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseAnd\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseAnd\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseAnd\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseAnd\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseXor\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseXor\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseXor\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseXor\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseXor\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseXor\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseXor\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: Invert\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: Invert\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: Invert\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: Invert\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: Invert\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"Invert\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: Invert\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT16 } } }') for unknown op: BitwiseOr\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_UINT8 } } }') for unknown op: BitwiseOr\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT64 } } }') for unknown op: BitwiseOr\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT32 } } }') for unknown op: BitwiseOr\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT16 } } }') for unknown op: BitwiseOr\r\nnative : op_kernel.cc:1142 OpKernel ('op: \"BitwiseOr\" device_type: \"CPU\" constraint { name: \"T\" allowed_values { list { type: DT_INT8 } } }') for unknown op: BitwiseOr\r\nnative : benchmark_model.cc:74 Could not create TensorFlow Session: Not found: Op type not registered 'RoundToSteps' in binary running on localhost. Make sure the Op and Kernel are registered in the binary running in this process.\r\n\r\n```\r\n\r\nEarlier this error was not getting reported.\r\n\r\nThanks", "comments": ["Did you confirm this works just before the commit, and fails just after?", "I was using 1.2 version. later I pulled to that commit id, I am not sure from which commit id  this error is happening.\r\nthanks", "Can you post a small reproducible case?", "Quantize the inception v3/alexnet and try running benchmark app.\r\n\r\n", "@cwhipkey Can you see what's going on with the RoundToSteps Op?", "This is an old bug in the quantize_graph.py script. We used to have a RoundToSteps op, but it was removed but the script still inserts it. The quantize_graph.py script is now obsolete though, since it's been replaced by the Graph Transform Tool's quantize_nodes rule:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/#quantize_nodes\r\n\r\nIt is confusing though, and I need to officially deprecate quantize_graph.py, and add a logging message point people to the right tool to use.", "@petewarden \r\nany quick fix to get rid of this error?\r\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Since the recommended fix is to use the more up to date script, closing this one for now. I'm also hoping we'll have some improved documentation on quantization soon."]}, {"number": 12010, "title": "[OpenCL] Uses Eigen implementation of asinh, acosh and atanh", "body": "- [Eigen] Bumps to the version that supports asinh, acosh and atanh\r\n- std:: -> numext\r\n- simplifies SYCL kernel registration", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "The code fails to compile with the following error:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/../include/c++/v1/limits:546:64: error: no member named 'max_digits10' in 'std::__1::numeric_limits<Eigen::half>'\r\n    static _LIBCPP_CONSTEXPR const int  max_digits10 = __base::max_digits10;\r\n                                                       ~~~~~~~~^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:91:22: note: in instantiation of template class 'std::__1::numeric_limits<const Eigen::half>' requested here\r\n    IsInteger = std::numeric_limits<T>::is_integer,\r\n                     ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/NumTraits.h:150:41: note: in instantiation of template class 'Eigen::GenericNumTraits<const Eigen::half>' requested here\r\ntemplate<typename T> struct NumTraits : GenericNumTraits<T>\r\n                                        ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/functors/UnaryFunctors.h:787:38: note: in instantiation of template class 'Eigen::NumTraits<const Eigen::half>' requested here\r\ntemplate<typename Scalar,bool iscpx=(NumTraits<Scalar>::IsComplex!=0) > struct scalar_sign_op;\r\n                                     ^\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorBase.h:89:60: note: in instantiation of default argument for 'scalar_sign_op<const Eigen::half>' required here\r\n    EIGEN_STRONG_INLINE const TensorCwiseUnaryOp<internal::scalar_sign_op<Scalar>, const Derived>\r\n                                                           ^~~~~~~~~~~~~~~~~~~~~~\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMap.h:27:112: note: in instantiation of template class 'Eigen::TensorBase<Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer>, 0>' requested here\r\ntemplate<typename PlainObjectType, int Options_, template <class> class MakePointer_> class TensorMap : public TensorBase<TensorMap<PlainObjectType, Options_, MakePointer_> >\r\n                                                                                                               ^\r\ntensorflow/examples/adding_an_op/zero_out_op_kernel_2.cc:66:31: note: in instantiation of template class 'Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 1, 1, long>, 16, MakePointer>' requested here\r\n    auto input = input_tensor.flat<T>();\r\n                              ^\r\ntensorflow/examples/adding_an_op/zero_out_op_kernel_2.cc:61:12: note: in instantiation of member function 'ZeroOutOp<Eigen::half>::Compute' requested here\r\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n           ^\r\ntensorflow/examples/adding_an_op/zero_out_op_kernel_2.cc:114:27: note: in instantiation of member function 'ZeroOutOp<Eigen::half>::ZeroOutOp' requested here\r\nTF_CALL_REAL_NUMBER_TYPES(REGISTER_KERNEL);", "Interestingly that fails to compile `scalar_sign_op` - that we did not touch. I am going to investigate more.\r\n\r\nThis Eigen commit seems to be responsible for the compilation error: https://bitbucket.org/eigen/eigen/commits/a9b7a79bf45462dd37df8fe173eeb2c7be594e03?at=default\r\n\r\nRelevant to: https://github.com/tensorflow/tensorflow/issues/9697", "Further investigation points to: https://bitbucket.org/eigen/eigen/src/8d1ccfd9c5a0bf1a662bd0fe35858f4e527d2bac/Eigen/src/Core/arch/CUDA/Half.h?at=default&fileviewer=file-view-default#Half.h-516\r\n```//static const int max_digits10 = ;``` Mac is missing the definition for it - hence the compilation error.\r\n@ggael / @benoitsteiner why is that commented out? Could we simply set `max_digits10` to `5`? (Based on [IEEE 754](http://half.sourceforge.net/structstd_1_1numeric__limits_3_01half__float_1_1half_01_4.html#a1561594738198528e5b3c227a8ba9337))\r\n", "@benoitsteiner any feedback for Luke?", "@benoitsteiner ping", "I added max_digits10: https://bitbucket.org/eigen/eigen/commits/da11a2b1da3f/", "Thanks @ggael ! I will bump Eigen to the tip and give it a sping :)", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "Apparently stuck somewhere. Jenkins, test this please.", "Here's the android error:\r\n\r\n```\r\n20:36:21 In file included from tensorflow/core/kernels/cwise_op_log.cc:16:\r\n20:36:21 In file included from ./tensorflow/core/kernels/cwise_ops_common.h:27:\r\n20:36:21 ./tensorflow/core/kernels/cwise_ops.h:50:12: error: no member named 'asinh' in namespace 'Eigen::numext'; did you mean simply 'asinh'?\r\n20:36:21     return numext::asinh(a);\r\n20:36:21            ^~~~~~~~~~~~~\r\n20:36:21            asinh\r\n20:36:21 external/androidndk/ndk/platforms/android-24/arch-x86_64/usr/include/math.h:224:8: note: 'asinh' declared here\r\n20:36:21 double  asinh(double);\r\n20:36:21         ^\r\n20:36:21 In file included from tensorflow/core/kernels/cwise_op_log.cc:16:\r\n20:36:21 In file included from ./tensorflow/core/kernels/cwise_ops_common.h:27:\r\n20:36:21 ./tensorflow/core/kernels/cwise_ops.h:62:12: error: no member named 'acosh' in namespace 'Eigen::numext'; did you mean simply 'acosh'?\r\n20:36:21     return numext::acosh(a);\r\n20:36:21            ^~~~~~~~~~~~~\r\n20:36:21            acosh\r\n20:36:21 external/androidndk/ndk/platforms/android-24/arch-x86_64/usr/include/math.h:223:8: note: 'acosh' declared here\r\n20:36:21 double  acosh(double);\r\n20:36:21         ^\r\n20:36:21 In file included from tensorflow/core/kernels/cwise_op_log.cc:16:\r\n20:36:21 In file included from ./tensorflow/core/kernels/cwise_ops_common.h:27:\r\n20:36:21 ./tensorflow/core/kernels/cwise_ops.h:74:12: error: no member named 'atanh' in namespace 'Eigen::numext'; did you mean simply 'atanh'?\r\n20:36:21     return numext::atanh(a);\r\n20:36:21            ^~~~~~~~~~~~~\r\n20:36:21            atanh\r\n20:36:21 external/androidndk/ndk/platforms/android-24/arch-x86_64/usr/include/math.h:225:8: note: 'atanh' declared here\r\n20:36:21 double  atanh(double);\r\n20:36:21         ^\r\n```", " Eigen implementation of asinh, acosh and atanh in Eigen can be found in  `Eigen/src/Core/MathFunctions.h` .\r\nTo Solve the android error take the following steps:\r\nFirstly, all of the hyperbolic functions will be enabled if the `EIGEN_HAS_CXX11_MATH` flag is 1. So if you want to use eigen hyperbolic math, either you need to define your own macro flag for android and  change the #ifdef macros at lines 1412, 1381, and 1443 in `Eigen/src/Core/MathFunctions.h` to \r\n`#if EIGEN_HAS_CXX11_MATH || [YOUR_FLAG_FOR_ANDROID]`.\r\nSecondly, in the `Eigen/Core` file at lines 74-78 you need to add `EIGEN_USING_STD_MATH(FUNC)` for your own platform. See the `#elif` added below:\r\n```\r\n#if defined(__CUDA_ARCH__) && defined(__NVCC__)\r\n  #define EIGEN_USING_STD_MATH(FUNC) using ::FUNC;\r\n#elif defined([YOUR_ANDROID_PLATFORM]) \r\n #define EIGEN_USING_STD_MATH(FUNC) using ::FUNC;\r\n#else\r\n  #define EIGEN_USING_STD_MATH(FUNC) using std::FUNC;\r\n#endif\r\n```", "@benoitsteiner / @rmlarsen / @drpngx It seems like additional development for android is required - and at present we are not able to test that target. If that is OK with you I am tempted to do something like: \r\n```\r\n#ifndef __ANDROID__\r\n    return numext::atanh(a);\r\n#else\r\n    return std::atanh(a);\r\n#endif  // __ANDROID__\r\n```\r\nWhat do you think? ( see ecbff0e)", "Using #ifdef EIGEN_HAS_CXX11_MATH would make the code much more portable since atanh, asinh nd acosth are available in Eigen iff EIGEN_HAS_CXX11_MATH is defined", "@benoitsteiner done in 8e42815", "Jenkins, test this please.", "Previous run had tests timeout.\r\n\r\nJenkins, test this please.", "Windows fails seems to be unrelated.\r\n\r\n@benoitsteiner / @rmlarsen / @drpngx it seems that changes suggested by @mehdi-goli on Eigen side are needed. As mentioned earlier we are not able of testing this target at present.\r\nHow should we proceed with this?", "Jenkins, test this please.", "Jenkins, test this please.", "The Py3 pip failure is unrelated to this."]}, {"number": 12008, "title": "Building 1.2.1 with bazel 0.5.2 fails with \"Genrules without outputs don't make sense\"", "body": "Hello,\r\n\r\nI'm trying to build Tensorflow on a Debian Stretch system usng the distribution provided CUDA packages. I'm using tensorflow 1.2.1 from git clone --recursive and bazel 0.5.2.\r\n\r\nThe build fails with \"Genrules without outputs don't make sense\" error.\r\n```\r\nERROR: /home/gandalf/.cache/bazel/_bazel_gandalf/db00e87a820fe7f78d45fd73738fa4bd/external/local_config_cuda/cuda/BUILD:170:12: in outs attribute of genrule rule @local_config_cuda//cuda:cuda-include: Genrules without outputs don't make sense.\r\n```\r\nWhen looking at the file referenced in the error message I can see the following lines (starting from :170):\r\n\r\n```\r\ngenrule(\r\n    name = \"cuda-include\",\r\n    outs = [\r\n    ],\r\n    cmd = \"\"\"\r\n    \"\"\",\r\n)\r\n\r\ngenrule(\r\n    name = \"cuda-nvvm\",\r\n    outs = [\r\n    ],\r\n    cmd = \"\"\"\r\n    \"\"\",\r\n)\r\n\r\ngenrule(\r\n    name = \"cuda-extras\",\r\n    outs = [\r\n    ],\r\n    cmd = \"\"\"\r\n    \"\"\",\r\n)\r\n\r\ngenrule(\r\n    name = \"cuda-lib\",\r\n    outs = [\r\n      \"lib/libcuda.so\",\r\n      \"lib/libcudart.so.8.0\",\r\n      \"lib/libcudart_static.a\",\r\n      \"lib/libcublas.so.8.0\",\r\n      \"lib/libcusolver.so.8.0\",\r\n      \"lib/libcurand.so.8.0\",\r\n      \"lib/libcufft.so.8.0\",\r\n      \"lib/libcudnn.so.6\",\r\n      \"lib/libcupti.so.8.0\",    ],\r\n    cmd = \"\"\"\r\nln -s /usr/lib/x86_64-linux-gnu/nvidia/current/libcuda.so.375.82 $(@D)/lib/libcuda.so && ln -s /usr/lib/x86_64-linux-gnu/libcudart.so.8.0.44 $(@D)/lib/libcudart.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcudart_static.a $(@D)/lib/libcudart_static.a && ln -s /usr/lib/x86_64-linux-gnu/libcublas.so.8.0.45 $(@D)/lib/libcublas.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcusolver.so.8.0.44 $(@D)/lib/libcusolver.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcurand.so.8.0.44 $(@D)/lib/libcurand.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcufft.so.8.0.44 $(@D)/lib/libcufft.so.8.0 && ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21 $(@D)/lib/libcudnn.so.6 && ln -s /usr/lib/x86_64-linux-gnu/libcupti.so.8.0.44 $(@D)/lib/libcupti.so.8.0    \"\"\",\r\n)\r\n\r\ngenrule(\r\n    name = \"cudnn-include\",\r\n    outs = [\r\n      \"include/cudnn.h\",    ],\r\n    cmd = \"\"\"\r\nln -s /usr/include/cudnn.h $(@D)/cudnn.h    \"\"\",\r\n)\r\n```\r\nSo indeed, there's genrules section with empty output but I have absolutely no idea if it's a problem or not and how I'm supposed to fix it.\r\n\r\nThanks in advance,\r\n\r\nBest regards, Adam.\r\n\r\nHere is the complete build log:\r\n```\r\ndebian/configure-expect.sh 2.7 \"-mavx -mavx2 -mfma -msse4.1 -msse4.2\" /usr/lib/x86_64-linux-gnu/\r\nspawn ./configure\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python2.7\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n/usr/lib/python2.7/dist-packages/\r\nDo you wish to build TensorFlow with MKL support? [y/N] N\r\nNo MKL support will be enabled for TensorFlow\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: -mavx -mavx2 -mfma -msse4.1 -msse4.2\r\nDo you wish to use jemalloc as the malloc implementation? [Y/n] Y\r\njemalloc enabled\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] Y\r\nGoogle Cloud Platform support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] Y\r\nHadoop File System support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with the XLA just-in-time compiler (experimental)? [y/N] N\r\nNo XLA JIT support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with VERBS support? [y/N] Y\r\nVERBS support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with OpenCL support? [y/N] N\r\nNo OpenCL support will be enabled for TensorFlow\r\nDo you wish to build TensorFlow with CUDA support? [y/N] Y\r\nCUDA support will be enabled for TensorFlow\r\nDo you want to use clang as CUDA compiler? [y/N] N\r\nnvcc will be used as CUDA compiler\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \r\nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc\r\nPlease specify the cuDNN version you want to use. [Leave empty to use system default]: \r\nPlease specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/lib/x86_64-linux-gnu/]: \r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size.\r\n3.0,3.5,3.7,5.0,5.2,5.3,6.0,6.1\r\n[Default is: \"3.5,5.2\"]: Extracting Bazel installation...\r\n.........\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\nConfiguration finished\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n____Loading package: tensorflow/tools/pip_package\r\n____Loading package: @local_config_cuda//crosstool\r\n____Loading package: @bazel_tools//tools/jdk\r\n____Loading package: @local_jdk//\r\n____Loading package: @bazel_tools//tools/cpp\r\n____Loading package: @local_config_cc//\r\n____Loading complete.  Analyzing...\r\n____Loading package: tensorflow/contrib/graph_editor\r\n____Loading package: tensorflow/contrib/tensor_forest\r\n____Loading package: tensorflow/python/saved_model\r\n____Loading package: tensorflow/contrib/slim/python/slim/data\r\n____Loading package: tensorflow/contrib/signal\r\n____Loading package: @six_archive//\r\n____Downloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 379,293 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,652,655 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/re2/archive/b94b7cd42e9f02673cd748c1ac1d16db4052514c.tar.gz: 177,937 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 553,707 bytes\r\n____Downloading http://mirror.bazel.build/github.com/nanopb/nanopb/archive/1251fa1065afc0d62f635e0f63fec8276e14e13c.tar.gz: 80,756 bytes\r\n____Loading package: @com_googlesource_code_re2//\r\n____Downloading http://mirror.bazel.build/github.com/google/gemmlowp/archive/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz: 675,655 bytes\r\n____Downloading http://mirror.bazel.build/github.com/nanopb/nanopb/archive/1251fa1065afc0d62f635e0f63fec8276e14e13c.tar.gz: 150,238 bytes\r\n____Downloading http://mirror.bazel.build/github.com/glennrp/libpng/archive/v1.2.53.zip: 103,443 bytes\r\n____Loading package: @org_pythonhosted_markdown//\r\n____Downloading http://mirror.bazel.build/github.com/nanopb/nanopb/archive/1251fa1065afc0d62f635e0f63fec8276e14e13c.tar.gz: 214,048 bytes\r\n____Downloading http://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz: 393,473 bytes\r\n____Downloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 556,541 bytes\r\n____Loading package: @nanopb_git//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,943,035 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 556,541 bytes\r\n____Downloading http://mirror.bazel.build/pypi.python.org/packages/b7/7f/44d3cfe5a12ba002b253f6985a4477edfa66da53787a2a838a40f6415263/Werkzeug-0.11.10.tar.gz: 889,455 bytes\r\n____Loading package: @zlib_archive//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,168,497 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/highwayhash/archive/dfcb97ca4fe9277bf9dc1802dd979b071896453b.tar.gz: 121,218 bytes\r\n____Loading package: @org_pocoo_werkzeug//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,416,647 bytes\r\n____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 384,964 bytes\r\n____Loading package: @fft2d//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,610,913 bytes\r\n____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 688,416 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/farmhash/archive/92e897b282426729f4724d91a637596c7e2fe28f.zip: 88,003 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 411,906 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,800,925 bytes\r\n____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 912,460 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/farmhash/archive/92e897b282426729f4724d91a637596c7e2fe28f.zip: 156,067 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 536,690 bytes\r\n____Loading package: @libxsmm_archive//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,946,979 bytes\r\n____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 1,177,310 bytes\r\n____Downloading http://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz: 166,595 bytes\r\n____Loading package: @nccl_archive//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 4,098,705 bytes\r\n____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 1,561,904 bytes\r\n____Downloading http://mirror.bazel.build/github.com/jemalloc/jemalloc/archive/4.4.0.tar.gz: 312,649 bytes\r\n____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 295,629 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 4,214,981 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,361,965 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/farmhash/archive/92e897b282426729f4724d91a637596c7e2fe28f.zip: 418,397 bytes\r\n____Downloading http://mirror.bazel.build/curl.haxx.se/download/curl-7.49.1.tar.gz: 1,068,439 bytes\r\n____Downloading http://mirror.bazel.build/bitbucket.org/eigen/eigen/get/f3a22f35b044.tar.gz: 2,164,554 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 1,079,468 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,597,353 bytes\r\n____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 712,521 bytes\r\n____Loading package: @eigen_archive//\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,747,661 bytes\r\n____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 868,501 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 1,900,805 bytes\r\n____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 1,024,481 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,053,949 bytes\r\n____Downloading http://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz: 1,180,461 bytes\r\n____Loading package: @jpeg//\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,075,220 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 307,790 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,337,549 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,238,290 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 676,470 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,470,841 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,415,540 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,026,716 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,604,133 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,572,938 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,423,756 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,751,605 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 2,772,876 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 1,928,564 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 2,967,141 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 3,038,042 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,583,680 bytes\r\n____Downloading http://mirror.bazel.build/github.com/grpc/grpc/archive/d7ff4ff40071d2b486a052183e3e9f9382afb745.tar.gz: 3,239,397 bytes\r\n____Loading package: @curl//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,414,628 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 3,781,074 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/boringssl/archive/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105.tar.gz: 4,302,898 bytes\r\n____Loading package: @grpc//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 926,641 bytes\r\n____Loading package: @boringssl//\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 2,126,269 bytes\r\n____Downloading http://mirror.bazel.build/github.com/google/protobuf/archive/2b7430d96aeff2bb624c8d52182ff5e4b9f7f18a.tar.gz: 3,469,115 bytes\r\n____Loading package: @protobuf//\r\n____Loading package: @local_config_cuda//cuda\r\n____Loading package: tools/defaults\r\n____Loading package: tensorflow/tools/graph_transforms\r\n____Loading package: tensorflow/contrib\r\n____Loading package: tensorflow/cc\r\nERROR: /home/gandalf/.cache/bazel/_bazel_gandalf/db00e87a820fe7f78d45fd73738fa4bd/external/local_config_cuda/cuda/BUILD:170:12: in outs attribute of genrule rule @local_config_cuda//cuda:cuda-include: Genrules without outputs don't make sense.\r\n____Downloading http://mirror.bazel.build/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz: 1,153,203 bytes\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n____Elapsed time: 10.746s\r\ndebian/rules:29: recipe for target 'build-py2-2.7' failed\r\n```", "comments": ["@ebrevdo could you please take a look.", "I'm away on vacation.\n\nOn Aug 3, 2017 7:51 PM, \"shivaniag\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> could you please take a look.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12008#issuecomment-320162542>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim69AzUzcItNhgIhrEyxBgCp-c4hIks5sUrF6gaJpZM4OspvZ>\n> .\n>\n", "@ebrevdo thank you for letting us know. @av8ramit could you take a look.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Is this still an issue on the latest version?"]}, {"number": 12007, "title": "Data type support for seq2seq attention mechanisms", "body": "Got an exception like the following when I was trying to use tf.float16 in my seq2seq model with Bahdanau Attention to allocate less memory on GPU:\r\n\r\n`ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: 'Tensor(\"decoder_1/BahdanauAttention/mul:0\", shape=(?, ?, 2048), dtype=float16)'`\r\n\r\nInserted `dtype` argument to attention mechanisms that is used in Dense layer of both query layer and memory layer. Default is `tf.float32`", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "Added the e-mail address I've used to commit to GitHub account and to my Google account.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@ebrevdo could you take a look, please?", "Jenkins, test this please.", "Stuck in the master somewhere. Jenkins, test this please.", "@ceteke ping", "@sb2nov pong\r\n\r\nSorry, I did not have enough time to look at the comments. I'm on it..", "@ebrevdo Can you check my latest changes?", "LGTM; conditioned on passing tests.\r\n", "BTW i see you're using float16s.  let me know if you run into any issues with any of the herlpers/decoders with float16.", "Jenkins, test this please.", "There are failures in the Windows build that look related to this PR, including (but not limited to):\r\n```\r\n17:34:30   File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py\", line 195, in __init__\r\n17:34:30     score_mask_value = self._memory_layer.dtype.as_numpy_dtype(-np.inf)\r\n17:34:30 AttributeError: 'str' object has no attribute 'as_numpy_dtype'\r\n```", "@ebrevdo Yes, a silly mistake I've made :) Fixed that problem.", "Jenkins, test this please."]}, {"number": 12006, "title": "Data type support for seq2seq attention mechanisms", "body": "Got an exception like the following when I was trying to use tf.float16 in my seq2seq model with Bahdanau Attention to allocate less memory on GPU:\r\n\r\n`ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: 'Tensor(\"decoder_1/BahdanauAttention/mul:0\", shape=(?, ?, 2048), dtype=float16)'`\r\n\r\nInserted `dtype` argument to attention mechanisms that is used in Dense layer of both query layer and memory layer. Default is `tf.float32`", "comments": ["Can one of the admins verify this patch?", "Sorry about the mistake \ud83d\ude04 "]}, {"number": 12005, "title": "Warning when running rnn_commons.select_last_activations (TensorFlow 1.2.1)", "body": "I see the following warning:\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.", "comments": ["If you look at the source code where the warning originates, you'll see it triggers when the implied dense tensor has more than 10M elements.  For performance reasons you may want to avoid triggering this conversion if possible.\r\n\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12004, "title": "Where does \"unknown error\" message come from ?", "body": "Hi,\r\n\r\nWhile doing my training of a faster-rcnn model, I get an \"unknown error\" message but the training seems to work fine.\r\n\r\nI just want to know where in Tensorflow's source code there is a \"print('unknown error')\" ? I don't seem to find that anywhere.\r\n\r\nThanks you guys,", "comments": []}, {"number": 12003, "title": "top_k doesn't order lower index first if two elements are equals", "body": "# Bug\r\nIf two elements are equal, ```top_k``` is not ordering them by index value.\r\n\r\n# Reproducing the error\r\nHere is the command I am running\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n sess.run(tf.nn.top_k(tf.convert_to_tensor([0, 0, 0, 0]), k=3))\r\n```\r\n\r\nAnd here is the output for Tensorflow 1.3.0-rc0\r\n```\r\nTopKV2(values=array([0, 0, 0], dtype=int32), indices=array([1, 3, 0], dtype=int32))\r\n```\r\ncompared to version 1.2.1\r\n```\r\nTopKV2(values=array([0, 0, 0]), indices=array([0, 1, 2]))\r\n```\r\n\r\n# Setup\r\nRunning ```tf_env_collect.sh```:\r\n```\r\n\r\n== cat /etc/issue ===============================================\r\nLinux 0187adfde6c8 4.4.0-47-generic #68-Ubuntu SMP Wed Oct 26 19:39:52 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 0187adfde6c8 4.4.0-47-generic #68-Ubuntu SMP Wed Oct 26 19:39:52 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.3.0rc0)\r\ntensorflow-tensorboard (0.1.2)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0-rc0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 90: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n", "comments": ["I was able to reproduce in version 1.3.0-rc0 and rc1, confirmed it changed from version 1.2  Opening internal bug.\r\n\r\nThanks for the detailed report with simple repro example.", "Stable sort was not part of the API guarantee, we changed it for the increased performance of unstable sort.", "The documentation claims sort stability:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/top_k\r\n\r\n", "@poxvoculi thanks for pointing that out. We'll have to revert to stable_sort.", "I believe this should be fixed now, or in the very next push.", "Great thanks!"]}, {"number": 12002, "title": "tf.nn.sparse_softmax_cross_entropy_with_logits() seems to return bad values !", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: from pip\r\n- **TensorFlow version (use command below)**: 'v1.2.0-5-g435cdfc', '1.2.1'\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: cudNN v8.0\r\n- **GPU model and memory**: 2* NVidia GeForce 1080Ti (11Go each)\r\n- **Exact command to reproduce**: Following code\r\n\r\n### Describe the problem\r\nIt seems that `tf.nn.sparse_softmax_cross_entropy_with_logits() ` and `tf.nn.softmax_cross_entropy_with_logits()` are returning bad values. According to this [stackOverflow post](https://stackoverflow.com/questions/36078411/tensorflow-are-my-logits-in-the-right-format-for-cross-entropy-function/36086477#36086477), `tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)` is almost equivalent to `-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1))`.\r\nBut when I'm using the provided optimized function, I don't get the same results. It appears that its come from the log function when logits is equal to 0. But I've read that `tf.nn.sparse_softmax_cross_entropy_with_logits()` handle that case, and that's the case, cause I would have some *Nan* output. But instead I have huge numbers, so I (naturally) thought that to avoid *log(0)* a small constant must have been added to the problematic numbers. So I tried to reproduce this tip (with 1e-10) and I don't still have the same result. So I tried to read the code [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_ops.py) to understand what's going on. But I can't find in the repo the gen_nn_ops module to understand why this function returns such a \"strange\" result. I will be pleased to contribute to understand what happens (and to correct it, if needed of course !).\r\nHere is a really simple piece of code to reproduce the \"error\" (if it's indeed one). And we have the same behavior with the sparse version of the function (with proper labels).\r\nThanks, \r\n### Source code / logs\r\n```\r\ngraph = tf.Graph()\r\n\r\nfeatures = np.array([[6.83324017e-02, 4.55211316e-01,-1.41892820e-01, 6.41751984e-01, -5.45895865e-01, 5.38657679e-01, 1.93379897e-01, 1.60154529e-01, 1.57859872e-02, 1.36758294e-02, 4.40859703e+00, 4.96067050e+03, -5.95230431e+01, 2.29624126e+00, 4.02069655e+00], [8.82284599e-01, 6.42900440e-01,-4.27639642e-02, 1.83567706e-01, 7.52404702e-01, -6.32605771e-01, 5.40391531e-01, 5.84584613e-01,-7.15044264e-03,-8.23328268e-02, 6.29273115e+00,-4.32369561e+01, 7.07259958e+00,-1.02810233e+00,-7.04034886e-01], [5.48660773e-01, 8.08794529e-01,-5.96924524e-02,-7.26052964e-01,-2.70772000e-02, 6.87105464e-01, 5.68913359e-01, 4.76252594e-01, 4.14203699e-02,-5.79935485e-03, 9.40232256e+00,-2.01665599e+04, 1.34500232e+01,-2.24989629e-01, 2.52753983e-01], [7.46613308e-01, 8.23272733e-01,-1.04753678e-01, 7.87653516e-01, 5.33736860e-01, 3.07777360e-01, 8.51814816e-01, 7.29870149e-01,-5.67521706e-03, 2.37203887e-02, 6.33280960e+00, 4.08845288e+05, 4.48007235e+01, 5.33139458e-02, 2.37384134e-02], [4.47498908e-01, 1.49080014e-01,-9.07106172e-03,-2.67174181e-01,-5.21700457e-01, 8.10213916e-01, 9.18038857e-01, 8.36740457e-01,-7.64173908e-03,-1.18870530e-02, 6.18394833e+00, 7.37307204e+01,-5.58432681e+01, 3.83996968e-01, 9.18497562e-01], [4.71607629e-01, 1.31179570e-01,-4.56846546e-02,-9.27597302e-01,-3.63639607e-01,-8.56123912e-02, 3.32925650e-01, 2.86999292e-01,-1.37396795e-01,-2.39745171e-01, 6.28318531e+00,-9.03421275e+04,-9.83543039e+03,-1.09839821e+00, 1.05041514e+00], [4.71613040e-01, 1.31166299e-01,-4.56797268e-02,-9.27775404e-01,-3.64117510e-01,-8.15551274e-02, 3.32854008e-01, 2.86979856e-01,-1.36950051e-01,-2.39623484e-01, 6.28318531e+00,-2.85787226e+05, 1.02588457e+05,-1.09795489e+00, 1.05020120e+00], [1.72510574e-01, 3.40244123e-02,-1.78258372e-01,-1.78623912e-01, 9.82406854e-01,-5.45001987e-02, 6.49133952e-01, 4.58514334e-01,-1.05587941e-01,-1.50382361e-01, 6.56445597e+00,-7.39915259e+01,-3.39043636e+01, 8.32312454e-01, 1.66266815e+00]])\r\n\r\nlabels = np.array([[ 1., 0.], [ 0., 1.], [ 0., 1.], [ 1., 0.], [ 0., 1.], [ 0., 1.], [ 0., 1.], [ 1., 0.]])\r\n\r\ntotalLoss = 0\r\ntotalTest = 0\r\n\r\nwith graph.as_default():\r\n    x = tf.placeholder(\"float\", [None, 15], name = \"x\")\r\n    y = tf.placeholder(\"int64\", [None, 2], name = \"y\")\r\n\r\n    h1 = tf.Variable(tf.truncated_normal([15, 100], stddev = 0.1), name = \"h1\") \r\n    out = tf.Variable(tf.truncated_normal([100, 2], stddev = 0.1), name = \"out\")\r\n    b1 =  tf.Variable(tf.truncated_normal([100], stddev = 0.1), name = \"b1\")\r\n    bout = tf.Variable(tf.truncated_normal([2], stddev = 0.1), name = \"bout\")\r\n\r\n\r\n    def model(x):\r\n        layer_1 = tf.add(tf.matmul(x, h1), b1)\r\n        layer_1 = tf.nn.relu(layer_1)\r\n\r\n        out_layer = tf.matmul(layer_1, out) + bout\r\n        return out_layer\r\n\r\n    logits = model(x)\r\n    \r\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = y))\r\n    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\r\n\r\n    with tf.Session(graph = graph) as session:\r\n        for i in xrange(10):\r\n            tf.global_variables_initializer().run()\r\n            \r\n            _ = session.run(optimizer, feed_dict = {x : features, y : labels})\r\n            l = session.run(loss, feed_dict = {x : features, y : labels})\r\n            test = session.run(tf.reduce_mean(-tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits) + 1e-10), 1)), feed_dict = {x : features})\r\n            totalLoss += l \r\n            totalTest += test\r\n\r\nprint(\"mathematical : \", totalTest *1. /10)\r\nprint(\"sparse_softmax_cross_entropy : \", totalLoss *1. /10)\r\n```\r\n", "comments": ["`tf.log(tf.nn.softmax(logits) + 1e-10)` is known to be unstable.\r\nIf I replace it by `tf.nn.log_softmax(logits)` I can see consistent results.", "Could you please put some of the consistent results you have please ? Because I don't have the same results at all ... Here are the results when I replace `tf.log(tf.nn.softmax(logits) + 1e-10)` by `tf.nn.log_softmax(logits)` on 3 different runs :  \r\n#### run 1\r\nmathematical :  9671.87484341\r\nsparse_softmax_cross_entropy :  1208.98436916\r\n\r\n#### run 2\r\nmathematical :  30372.4364977\r\nsparse_softmax_cross_entropy :  3796.55453706\r\n\r\n#### run 3\r\nmathematical :  9671.87484341\r\nsparse_softmax_cross_entropy :  1208.98436916 \r\n\r\nAny Idea why ? A numerical stability issue ?", "```\r\nmathematical :  1244.12398391\r\nsparse_softmax_cross_entropy :  1244.12398391\r\n\r\nmathematical :  2525.72263083\r\nsparse_softmax_cross_entropy :  2525.72263083\r\n\r\nmathematical :  4146.75835724\r\nsparse_softmax_cross_entropy :  4146.75835724\r\n\r\nmathematical :  1013.51154659\r\nsparse_softmax_cross_entropy :  1013.51154659\r\n\r\nmathematical :  2610.52162684\r\nsparse_softmax_cross_entropy :  2610.52162684\r\n```\r\nUsing TF 1.3.0rc1, btw.", "So here is the source of the difference between your and my results ! So it seems that between our two versions of TF, the way of coding these 2 functions have changed ! \r\nWouldn't it nice to warn v 1.2 users that a huge difference exists ? (And maybe an error so ...)", "I didn't mean that TF version is the problem. It was just an FYI.\r\nIn fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  mazecreator@gmail.com\n    Domain mazecreator.com has exceeded the max emails per hour (25/25 (100%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\nReceived: from github-smtp2-ext6.iad.github.net ([192.30.252.197]:52170 helo=github-smtp2b-ext-cp1-prd.iad.github.net)\n\tby server2.lowesthostingrates.com with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.89)\n\t(envelope-from <noreply@github.com>)\n\tid 1ddkcH-0000Hn-Ds\n\tfor mazecreator@mazecreator.com; Fri, 04 Aug 2017 16:59:15 -0500\nDate: Fri, 04 Aug 2017 15:09:31 -0700\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1501884571;\n\tbh=SypO/JhO5lnky6YYSSkgVxcutkL9roA2yInEbZlWXqQ=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=U4v3RW4uRTyff9y8OCHZPTyEY0RiSmfwM9UD1sPt5tHGxf9L2GMQBHcVaMRPp5ikD\n\t QbDUx1htRZdj4hW06ileJ4Pq0SkxwJQ1wq12EjHe4tLIdZovi1/VYEFo+/rgBQ52Vg\n\t c5E1DOhnVWqiCW8w9zZMQ4rfCmbxwdgVR3xhLEFE=\nFrom: Yuxin Wu <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/12002/320363688@github.com>\nIn-Reply-To: <tensorflow/tensorflow/issues/12002@github.com>\nReferences: <tensorflow/tensorflow/issues/12002@github.com>\nSubject: Re: [tensorflow/tensorflow]\n tf.nn.sparse_softmax_cross_entropy_with_logits() seems to return bad values !\n (#12002)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: ppwwyyxx\nX-GitHub-Recipient: Mazecreator\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0118f3a070872b7681c27cc414163f618de8744dfc4b54d692cf00000001159cb29b92a169ce0ec3ee17@reply.github.com>,\n <https://github.com/notifications/unsubscribe/ARjzoAa4s9Afp3Xz0VcL31zSaAYJ7AQ-ks5sU5abgaJpZM4OsgT5>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: mazecreator@mazecreator.com\nX-Spam-Status: No, score=\nX-Spam-Score:\nX-Spam-Bar:\nX-Ham-Report:\nX-Spam-Flag: NO\n\n\n----==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\nI didn't mean that TF version is the problem. It was just an FYI.\nIn fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/12002#issuecomment-320363688\n----==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n<p>I didn't mean that TF version is the problem. It was just an FYI.<br>\nIn fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.</p>\n\n<p style=\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&mdash;<br />You are receiving this because you are subscribed to this thread.<br />Reply to this email directly, <a href=\"https://github.com/tensorflow/tensorflow/issues/12002#issuecomment-320363688\">view it on GitHub</a>, or <a href=\"https://github.com/notifications/unsubscribe-auth/ARjzoOv9XkmzvloD_t0StSlpiMJpwtLMks5sU5abgaJpZM4OsgT5\">mute the thread</a>.<img alt=\"\" height=\"1\" src=\"https://github.com/notifications/beacon/ARjzoCKP9tmNdQ-sgniF4uhFHvkjvyjXks5sU5abgaJpZM4OsgT5.gif\" width=\"1\" /></p>\n<div itemscope itemtype=\"http://schema.org/EmailMessage\">\n<div itemprop=\"action\" itemscope itemtype=\"http://schema.org/ViewAction\">\n  <link itemprop=\"url\" href=\"https://github.com/tensorflow/tensorflow/issues/12002#issuecomment-320363688\"></link>\n  <meta itemprop=\"name\" content=\"View Issue\"></meta>\n</div>\n<meta itemprop=\"description\" content=\"View this Issue on GitHub\"></meta>\n</div>\n\n<script type=\"application/json\" data-scope=\"inboxmarkup\">{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorflow/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@ppwwyyxx in #12002: I didn't mean that TF version is the problem. It was just an FYI.\\r\\nIn fact the code (with log_softmax) still produces correct result here with TF 1.2.0rc0.\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/tensorflow/tensorflow/issues/12002#issuecomment-320363688\"}}}</script>\n----==_mimepart_5984f09ba80f_562f3fa08329dc3c3211aa--\n", "I reproduced the same code on 4 different computers (2 MAC, 2 Ubuntu) with GPU compatibility and CPU based on the same tensorflow version  'v1.2.0-5-g435cdfc', '1.2.1' and I still have this big difference ... I would just like to understand what's going on. With exactly the same code I don't have the same output's behavior ...", "Hi, I've just installed tensorflow with the following command : `sudo pip install tensorflow`. I ran some example codes to get how it works. Once I understood how it works I tried on my own dataset and it turns out that computing hand made cross entropy (with `log_softmax()`) does not match with the output of `softmax_cross_entropy_with_logits()`. My dataset is strictly different than the one proposed in the code. When I run the source code of this issue, I have the same kind of results than bdenoun's. Results don't match. \r\nThanks for your help ! ", "Hi, after cmontassier's comment, I reinstalled tf from pip and I still have the same issue. And same thing for the other 4 computers... so I really think it comes from this particular version !", "Thank you @bdenoun, for reaching out!\r\n\r\n@ebrevdo, are you able to take a look?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "sparse_softmax_cross_entropy_with_logits [subtracts the max of the logits](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/kernels/sparse_xent_op.h#L201) before calculating the cross entropy, for numerical stability.  could that be the reason you're getting different results?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 12000, "title": "error C2678: binary '<': no operator found which takes a left-hand operand of type IndicesRowIterator", "body": "If you build a debug version of the current tensorflow version 1.3.0 on Windows  with CMAKE  following error is occurred:\r\n\r\n(ClCompile target) ->\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Tools\\MSVC\\14.10.25017\\include\\xutility(978): error C2678: binary '<': no operator found which\r\ntakes a left-hand operand of type 'tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling sou\r\nrce file C:\\Development\\dev\\test_projects\\deeplearning\\tensorflow\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc) [C:\\Development\\de\r\nv\\test_projects\\deeplearning\\tensorflow_build64\\tf_core_kernels.vcxproj]\r\n\r\nThe reason is **IndicesRowIterator::operator<()** is missing.\r\nAdding these lines \r\n\r\n  **bool operator<( const IndicesRowIterator& other ) const {\r\n     QCHECK_LT( iter_, other.iter_ );\r\n     return ( row_idx_ < other.row_idx_ );\r\n  }**\r\n\r\nsolves the problem.\r\n\r\n###System information\r\ntensorflow 1.3.0\r\nWindows 10\r\nVisualStudio 2017\r\nCMake 3.8.1\r\n\r\n\r\n\r\n", "comments": ["@gunan could you please take a look.", "The proposal looks good to me.\r\nCould you send a pull request?", "Sorry I'm not able pulling a request.", "@rasitsimsek  I am having the same issue, how did you fixed this error?", "A PR #13553 is pending review.", "Hi cuevas1208, I had added \r\n\r\n` \r\nbool operator<( const IndicesRowIterator& other ) const\r\n{\r\n     QCHECK_LT( iter_, other.iter_ );\r\n     return ( row_idx_ < other.row_idx_ );\r\n }\r\n`\r\n\r\nat **tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc** . This code fragment had solved the problem. ", "Looks like this issue has been addressed in #14404. As #14404 has already been merged, I think this issue could be closed.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@rasitsimsek @gunan \r\nI am trying to build TensorFlow pip package on Windows 10 and i am getting similar error. I have tried both 1.4.1 and 1.4.0 versions. Not sure if its a related bug or my own fault. Please help. Thanks in advance.\r\n\r\nError message\r\n(ClCompile target) ->\r\n         C:\\Program Files (x86)\\Microsoft VisualStudio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417):\r\nerror C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file C:\\tmp\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n \r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417):\r\nerror C2100: illegal indirection (compiling source file C:\\tmp\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc)\r\n\r\nCommand used:\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\amd64\\MSBuild.exe\" /p:Configuration=Release /m:4 /p:Platform=x64 /p:PreferredToolArchitecture=x64 tf_python_build_pip_package.vcxproj\r\n\r\nPlatform:\r\nWindows 10\r\nVisual studio 2017 community edition\r\nCmake 3.10.1\r\nswig 3.0.12\r\nPython 3.5.4", "Looking at the comment by @yongtang the fixes are merged after the 1.4 branch. Please try again with master branch.", "Hi @vksbhandary. I see the same problem on master branch as you do. This issue seems to be different than reported one. Did you find a way how to solve this? Thanks.\r\n`algorithm(2417): error C2678: binary '*': no operator found w\r\nhich takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator'`\r\n\r\n\r\n", "Hi @vldbnc. I see the same problem and is quite different from this issue: binary '<': no operator found.\r\n\r\nIt got following:\r\n1>sparse_column_iterable.cc\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion)\r\n1>D:\\...\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(54): note: could be 'const __int64 &tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator::operator *(void)'\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): note: while trying to match the argument list '(const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator)'\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2439): note: see reference to function template instantiation '_FwdIt std::_Lower_bound_unchecked<_Iter,_Ty,_Fn>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n1>        with\r\n1>        [\r\n1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Iter=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Ty=tensorflow::int64,\r\n1>            _Fn=std::less<void>,\r\n1>            _Pr=std::less<void>\r\n1>        ]\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2447): note: see reference to function template instantiation '_FwdIt std::lower_bound<_FwdIt,_Ty,std::less<void>>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled\r\n1>        with\r\n1>        [\r\n1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Ty=tensorflow::int64,\r\n1>            _Pr=std::less<void>\r\n1>        ]\r\n1>D:\\....\\tensorflow\\tensorflow\\contrib\\boosted_trees\\lib\\utils\\sparse_column_iterable.cc(119): note: see reference to function template instantiation '_FwdIt std::lower_bound<tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,tensorflow::int64>(_FwdIt,_FwdIt,const _Ty &)' being compiled\r\n1>        with\r\n1>        [\r\n1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,\r\n1>            _Ty=tensorflow::int64\r\n1>        ]\r\n1>C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Tools\\MSVC\\14.12.25827\\include\\algorithm(2417): error C2100: illegal indirection\r\n1>Done building project \"tf_core_kernels.vcxproj\" -- FAILED.\r\n\r\n\r\n\r\n###System information\r\n tensorflow 1.5.0RC\r\n Windows 10\r\n VisualStudio Prof. 2017\r\n CMake 3.10.1\r\n  "]}]