[{"number": 32250, "title": "tf.saved_model.save() broken for my subclassed model in tf-2.0.0rc0", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.02 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf2.0.0rc0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nn [4]: tf.version.GIT_VERSION                                                           \r\nOut[4]: 'v2.0.0-beta1-5101-gc75bb66'\r\n\r\nIn [5]: tf.version.VERSION                                                               \r\nOut[5]: '2.0.0-rc0'\r\n\r\n**Describe the current behavior**\r\n\r\nI have a subclassed model that I have been saving and deploying since tf-2.0.0b0 and after upgrading to rc0, it errored out with this message.\r\n\r\n[save.py:136] Skipping full serialization of object <__main__.MyModel object at 0x7f76815add30> because an error occurred while tracing layer functions. Error message: in converted code:\r\n\r\nTypeError: tf__call() takes from 2 to 3 positional arguments but 4 were given\r\n\r\nThis is how the call method is defined in my class.  Can someone tell me what went wrong, or what I need to change to make this work?\r\n\r\nThanks, David\r\n\r\nclass MyModel(Model):\r\n\r\n    def __init__(self, n_layers, h_dim, b_dim,\r\n                       activation_fn,\r\n                       kernel_init,\r\n                       batch_norm):\r\n        super().__init__()\r\n        ... skipping ...\r\n\r\n    @tf.function\r\n    def call(self, inputs, training=False):\r\n        y = inputs[0]\r\n        h = inputs[1]\r\n        n_var = inputs[2]\r\n        x = layers.concatenate([y, h])\r\n        if self.n_layers >= 1: x = self.d_hidden_1(x,training=training)\r\n        if self.n_layers >= 2: x = self.d_hidden_2(x,training=training)\r\n        if self.n_layers >= 3: x = self.d_hidden_3(x,training=training)\r\n        if self.n_layers >= 4: x = self.d_hidden_4(x,training=training)\r\n        if self.n_layers >= 5: x = self.d_hidden_5(x,training=training)\r\n        # scale output by 1/n_var\r\n        #return self.d_out(x) / n_var\r\n        return self.d_out(x,training=training)\r\n\r\n**Describe the expected behavior**\r\nIt should just work\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nSee above\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@DavidKWH Can you please assemble a minimal reproducible code snippet to reflect this issue? Thanks!", "This has been resolved in the discuss forum.  It has been reproduced and a fixed has been proposed by Kathy.  Please see \r\n\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/wlwLvWnD4BI\r\n\r\n", "This issue has been resolved in the nightly and cherrypicked to the 2.0 branch. Please let us know if it is ok to close the issue. \r\n\r\nThanks!", "Closing issue, please feel free to reopen if you continue to see issues. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32250\">No</a>\n"]}, {"number": 32249, "title": "Is tensorflow lite malloc free?", "body": "I read that TFL Micro is malloc free, but is Tensorflow Lite also malloc free? I wondering if its safe to use inside an audio thread?\r\n\r\n", "comments": ["TensorFlow lite for fixed shape inference minimizes mallocs needed when running repeated inferences. In cases of fixed shapes after AllocateTensors() there should be no more mallocs. However, malloc is thread safe,  so you can just try and make sure it works well for your use case.\r\n"]}, {"number": 32248, "title": "Performance regression in sparse_dense_matmul", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n1.14.0\r\n- Python version:\r\n3.7\r\n\r\n**Describe the current behavior**\r\n\r\nPerformance of `sparse_dense_matmul` is significantly slower compared to 1.12\r\n \r\n**Describe the expected behavior**\r\n\r\nPerformance does not degrade between versions. If there is a regression, it should be included in the release notes.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef create_sparse_tensor(batch_size, num_features, num_samples):\r\n  total_samples = num_samples * batch_size\r\n  batch_indices = np.sort(np.random.randint(0, batch_size, total_samples))\r\n  feature_indices = np.random.randint(0, num_features, total_samples)\r\n  indices = np.stack([batch_indices, feature_indices], axis=1)\r\n  values = np.random.random(total_samples).astype(np.float32)\r\n  return tf.SparseTensorValue(indices=indices, values=values,\r\n                              dense_shape=[batch_size, num_features])\r\n\r\n\r\ndef bench(batch_size=32, num_bits=18, num_samples=1000, num_runs=2000):\r\n  num_features = 1 << num_bits\r\n  experiment = (\"batch_size_%d_num_bits_%d\" % (batch_size, num_bits))\r\n  sparse_input = tf.sparse_placeholder(dtype=tf.float32,\r\n                                       shape=(batch_size, num_features),\r\n                                       name=\"sparse_input_\" + experiment)\r\n  dense_weight = tf.get_variable(name=\"dense_weight_\" + experiment,\r\n                                 shape=(num_features, 50),\r\n                                 dtype=tf.float32)\r\n\r\n  output_sparse = tf.sparse_tensor_dense_matmul(sparse_input, dense_weight)\r\n\r\n  with tf.Session() as sess:\r\n    print(experiment)\r\n    sparse_input_values = [\r\n      create_sparse_tensor(batch_size, num_features, num_samples)\r\n      for n in range(num_runs)\r\n    ]\r\n\r\n    sess.run(tf.initializers.global_variables())\r\n\r\n    start = time.time()\r\n    for n in range(num_runs):\r\n      sess.run(output_sparse, feed_dict={sparse_input: sparse_input_values[n]})\r\n    elapsed = time.time() - start\r\n    print(\"Avg time taken for sparse_matmul: %.4f\" % (elapsed / num_runs))\r\n\r\nif __name__ == \"__main__\":\r\n  tf.logging.set_verbosity(tf.logging.ERROR)\r\n  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n  print(\"--------------------\")\r\n  print(tf.__version__)\r\n  print(\"--------------------\")\r\n  for b in [4, 16, 32]:\r\n    for n in [18, 22]:\r\n      bench(batch_size=b, num_bits=n)\r\n  print(\"--------------------\")\r\n```\r\n**Other info / logs**\r\n```\r\n--------------------\r\n1.14.0\r\n--------------------\r\nbatch_size_4_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0015\r\nbatch_size_4_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0020\r\nbatch_size_16_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0049\r\nbatch_size_16_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0072\r\nbatch_size_32_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0100\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0117\r\n--------------------\r\n--------------------\r\n1.12.0\r\n--------------------\r\nbatch_size_4_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0009\r\nbatch_size_4_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0012\r\nbatch_size_16_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0027\r\nbatch_size_16_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0035\r\nbatch_size_32_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0048\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0064\r\n--------------------\r\n```", "comments": ["Note: The performance seems to be fixed in 1.15:\r\n\r\n```\r\n--------------------\r\n1.15.0-dev20190821\r\n--------------------\r\nbatch_size_4_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0010\r\nbatch_size_4_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0011\r\nbatch_size_16_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0022\r\nbatch_size_16_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0032\r\nbatch_size_32_num_bits_18\r\nAvg time taken for sparse_matmul: 0.0041\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0060\r\n--------------------\r\n```\r\n\r\nWould it be possible to cherry pick whatever fixed this issue into a 1.14 branch?", "A little bisecting on nightly releases tells me the issue was fixed in 1.15 between july 01 and july 03\r\n\r\n```\r\n--------------------\r\n1.15.0-dev20190821\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0044 (GOOD)\r\n--------------------\r\n--------------------\r\n1.15.0-dev20190626\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0090 (BAD)\r\n--------------------\r\n--------------------\r\n1.15.0-dev20190726\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0046 (GOOD)\r\n--------------------\r\n--------------------\r\n1.15.0-dev20190710\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0049 (GOOD)\r\n--------------------\r\n--------------------\r\n1.15.0-dev20190703\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0047 (GOOD)\r\n--------------------\r\n--------------------\r\n1.15.0-dev20190629\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0087 (BAD)\r\n--------------------\r\n--------------------\r\n1.15.0-dev20190701\r\n--------------------\r\nbatch_size_32_num_bits_22\r\nAvg time taken for sparse_matmul: 0.0090 (BAD)\r\n--------------------\r\n```", "I have tried on colab with TF version 1.12,1.14,nightly versions,1.15.0-dev20190626 and was able to reproduce the performance issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/d64e18e060adb3159d862ad0cbc23ec7/untitled159.ipynb).Thanks!", "Briac and pavan (@bqm and @pavanky) on our team have found the commit that fixes the slow down through git bisection.\r\nhttps://github.com/tensorflow/tensorflow/commit/e4a0ae3ed04d4d7f8c0381cb8a2d187f86411de8#diff-455a4c7f8e22d7c514e8c2caa27506c5 . Our current plan is to build TF 1.14 and cherrypick the above commit.  \r\n\r\nHere are a couple of ideas that we could consider to prevent future regressions.\r\n1. I assume TensorFlow has some performance testing before releases.  We could consider adding `sparse_tensor_dense_matmul` into the performance testing suite. This is a commonly used op when working with sparse workload.\r\n2. Would it make sense to reduce the frequency of Eigen dependency updates?  E.g. only depend on released versions of Eigen. \r\n\r\nThanks for helping to look into this. ", "I will take a look at this issue, unfortunately we do a lot of contributions to Eigen recently, and follow it's head pretty close. /cc @rmlarsen \r\n\r\nCan you try to build Tensorflow from sources with XSMM support (https://github.com/tensorflow/tensorflow/blob/82bf3e6c20782727d683beae4be046335b6f3297/tensorflow/core/kernels/BUILD#L82-L90), as far as I know internally it's a LARGE performance improvement for apps that rely on sparse matmuls.", "@yzhuang I'm surprised that this commit fixed any regressions, because I know that it actually introduced pretty large regression to TensorChipping that is used a lot in sparse matmul, and the fix was submitted with https://github.com/tensorflow/tensorflow/commit/79e6d267c299d2051ba35ce014379f3d120efce6#diff-455a4c7f8e22d7c514e8c2caa27506c5", "@ezhulenev You can look at the repro script, and start with the nightly builds. We definitely see a performance improvement between the nightly from July 01 and July 03 so @bqm started a git bisect to find the actual commit.\r\n\r\nMay be we are testing at different levels of sparsity ?", "@ezhulenev Thanks for the note!\r\n\r\nLIBXSMM 1.14 has been incorporated into recent TF (master). TF can now also use Bfloat16 of LIBXSMM's SpMDM (sparse_dense_matmul).", "@pavanky \r\nIs this still an issue", "This seems to no longer be an issue in 1.15.2"]}, {"number": 32247, "title": "Release Notes for TensorFlow 2.0.0", "body": "Updating the release doc with the 2.0.0 release notes.", "comments": ["You mean, removed freeze_graph tool?\n", "I think so.\n\n>\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32247) for more info**.\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F32247) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 32246, "title": "tf.train.SingularMonitoredSession() Error:tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile", "body": "hi,\r\nI'm new with tensorflow, I'm facing an issue while executing a [project](https://github.com/hcmlab/vadnet/blob/master/train/code/main.py) on window 10.\r\n\r\nUsing :\r\npython-3.5.4\r\ntensorboard-gpu=1.14.0\r\ncuda-10\r\n\r\nERROR:\r\ntensorflow.python.framework.errors_impl.NotFoundError: Failed to create a NewWriteableFile: C:\\Users\\MY-PC\\Desktop\\tools\\vadnet\\train\\nets\\vad_Model[48000,48000,24000,512,None]_Conv7[2,True,False,4,2]_SceAdam[1e-04,0.9,0.999,1e-08]\\ckpt\\model.ckpt-0_temp_df1af0f684d24212bc1ebde45848fe3f/part-00000-of-00001.data-00000-of-00001.tempstate18107421248045726006 : The system cannot find the path specified.\r\n\r\npath is correct and directory is created \"nets\\vad_Model[48000,48000,24000,512,None]_Conv7[2,True,False,4,2]_SceAdam[1e-04,0.9,0.999,1e-08]\\ckpt\" \r\nby script but \r\n\"part-00000-of-00001.data-00000-of-00001.tempstate18107421248045726006\" \r\nfile not creating.\r\n\r\nI read somewhere, need to use an absolute path. I had tried that also but still facing the same error.\r\nthank you!\r\n\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "hi ravikyram,\r\nit's itself a complete project, thats why i gave the link.", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nPlease post the issue in [that repo](https://github.com/hcmlab/vadnet/issues) or in stackoverflow. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32246\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32246\">No</a>\n"]}, {"number": 32245, "title": "When GlobalJitLevel is on, disable the Grappler memory opt.", "body": "This commit disables the Grappler memory optimizer when the XLA JIT is detected on.\r\n\r\nThe (current) XLA clustering can result in loss of concurrency between kernel compute and memory copies. As such, it usually loses the concurrency needed to hide the latencies of the inserted swap-ins and swap-outs and incurs great performance overhead.\r\n\r\nYou may find more details about the performance degradation in this document:\r\nhttps://docs.google.com/document/d/1q1UPN2CRRNBoUXM0zORT-cTG9m7ctQRB2xfUPhaK1Ek/edit?usp=sharing\r\n\r\n@sanjoy, please also help to take a look.\r\n", "comments": ["One question:\r\n\r\nAccording to `GetGlobalJitLevelForGraph()`, there are two ways to enable the XLA JIT; one is through the `ConfigProto` and the other is through the environment variable.\r\n\r\nThis PR can only check the ConfigProto and cannot check the env var flags as they only exist in the XLA world.\r\n\r\nDo you think this is an issue? Any suggestions to address it?", "maybe call getenv and parse the env variable?", "> This PR can only check the ConfigProto and cannot check the env var flags as they only exist in the XLA world.\r\n\r\nI think we should just expose `GetGlobalJitLevelForGraph` to to TensorFlow by moving it to a header/cc outside compiler/xla.\r\n\r\n@rmlarsen WDYT?", "> > This PR can only check the ConfigProto and cannot check the env var flags as they only exist in the XLA world.\r\n> \r\n> I think we should just expose `GetGlobalJitLevelForGraph` to to TensorFlow by moving it to a header/cc outside compiler/xla.\r\n> \r\n> @rmlarsen WDYT?\r\n\r\nIt is more than that, as the `GetGlobalJitLevelForGraph` depends on compiler/jit/flags.cc, etc. which do not exist in a Tensorflow build without enabling XLA.\r\n\r\nIf we want to move the function, we will have to move several xla flag related files with it. Considering that, below are two options we have.\r\n\r\n1. Just check the proto config without checking the env variables. This may make sense as the proto config should be the way for clients to enable XLA. Env variables are more for testing? Let me know if this is not the case though.\r\n2. Move the xla flags related files into TF.\r\n\r\nPersonally, I am slightly leaning towards option 1 but please give some guidance. Thanks.\r\n ", "> maybe call getenv and parse the env variable?\r\n\r\nIMHO, re-implement the parsing logic in TF may be ugly because there will then be two separate places to be maintained for future env variable change.", "> This may make sense as the proto config should be the way for clients to enable XLA. Env variables are more for testing? Let me know if this is not the case though.\r\n\r\nEnv vars are less elegant but they're super convenient when trying out XLA (\"no need to make code changes, just flip this flag\") so I'd like to make this work with env vars if possible.\r\n\r\nAnother possibility is to do some form of dependency injection via a registry -- allow the XLA JIT to register a callback that lets TF query whether the XLA global jit is enabled.  This will be more code but should be cleaner overall.", "> Another possibility is to do some form of dependency injection via a registry -- allow the XLA JIT to register a callback that lets TF query whether the XLA global jit is enabled. This will be more code but should be cleaner overall.\r\n\r\nGood suggestion. Let's proceed with this route.\r\n", "I implemented a config proxy in the Tensorflow core per our previous discussion. Please help to take a look when you have a moment. Thanks.\r\n", "@trentlo can you please resolve conflicts ?", "> @trentlo can you please resolve conflicts ?\r\n\r\nYes, done.", "@sanjoy \r\n\r\nThanks for your time guiding me through this PR. I feel that reviewing this PR ends up being a tedious process. ", "@rthadur \r\n\r\nI noticed that the `Ubuntu CC` test may be stuck. Will that block the merging?\r\n", "> Will that block the merging?\r\n\r\nI'm taking care of the merge.  Sorry for the delay, I'm a bit busier than usual.", "> I'm taking care of the merge. Sorry for the delay, I'm a bit busier than usual.\r\n\r\nGot it. No worries and take time then. Thanks.\r\n", "@trentlo @goldiegadde I am not sure if this PR catch the r2.0 cherry-pick cycle. It does seem to fix a performance regression running the official ResNet50 model w/ XLA 8-GPU. \r\n\r\nI observed significant performance boost (~3900 image/s to ~7500 image/s) comparing 2.0 nightly packages in 2019-09-19 (wo/ this PR) and 2019-09-20 (w/ this PR).\r\n\r\nThe benchmark I am running:\r\n\r\n```\r\npython3 official/vision/image_classification/resnet_imagenet_main.py \\\r\n    --ds=default \\\r\n    --bs=2048 \\\r\n    --dtype=fp16 \\\r\n    --enable_xla \\\r\n    --ng=8 \\\r\n    --use_tensor_lr \\\r\n    --pgtc=2 \\\r\n    --gt_mode=gpu_private \\\r\n    --datasets_num_private_threads=48\r\n```\r\n\r\nPing @tfboyd to confirm. Several other CLs on the same day are suspicious, but my hunch is this one.\r\n\r\nEDIT: my guess is wrong. Turns out 59436c19a00ad06e1ef9a228f74bb4d007614752 that reverts 53bdcf55ce180f609797746992987ce89c3543bb improves the performance greatly.", "I will cherry-pick this on my own r2.0 branch and report the numbers tomorrow.", "@byronyi FYI as below.\r\n\r\nIndeed, cherry-pick the commits and measure the performance difference is the most direct way to verify.\r\n\r\nA quick way to verify is to use the following flags. It dumps a bunch of logged files into `./graph_dump`.\r\n\r\n`TF_DUMP_GRAPH_PREFIX=\"./graph_dump\" TF_XLA_FLAGS=\"--tf_xla_clustering_debug=true\" python3 ...`\r\n\r\nThen you can use `grep -r GpuToHost ./graph_dump` to see whether any copy ops are inserted by Grappler. If you see them, likely they can cause great performance penalty. These ops should disappear when XLA is ON if this PR is there.\r\n", "I am seeing the same bounce back in the nightly tests.  XLA stated they were not focused on multi-GPU tests as their focus was XLA on by default for single GPU so I stopped bothering them with the results.  The drop to ~3,900 occurred around 03-AUG in my testing.  I see the bounce back to 8,500 with 2.0.0-dev20190922.  This is still not back to the ~9K we saw before the drop.  I doubt this will get cherry-picked into TF 2.0 as XLA is still experimental and RC2 is likely to be the last RC needed before release.    \r\n\r\n\r\n", "@tfboyd It turns out that 53bdcf55ce180f609797746992987ce89c3543bb was the commit that causes bad performance in the nightly package, not the issue this PR fixes. This CL was reverted in 59436c19a00ad06e1ef9a228f74bb4d007614752, so the performance went back to ~7,600 in our environment.\r\n\r\nSo there must be some other changes that cause performance degradation in r2.0. Do you mind to point to me a snapshot with the expected performance, i.e. before dropping to ~3,900 around 03-AUG?", "@byronyi   Here is the info I have that may be of the most use.  It would be nice to get back to the 9K, but it is also possible too many other changes have occurred. I only run on nightly and when nightly works.\r\n\r\nCommand for the test you would need a copy of imagenet.\r\n\r\n```bash\r\n# Flags\r\n--batch_size=2048 --data_dir=/data/imagenet --dtype=fp16 --enable_eager --enable_xla --log_steps=10 --model_dir=/workspace/benchmarks/perfzero/workspace/output/2019-08-02-12-50-22-843844/benchmark_xla_8_gpu_fp16 --num_gpus=8 --noreport_accuracy_metrics --skip_eval --train_steps=110\r\n\r\n# [PerfZero](https://github.com/tensorflow/benchmarks/tree/master/perfzero) command.  I suggest using perfzero when possible as it helps ensure your\r\n# args are the same and a common docker is used for repeat-ability.  Not perfect but lightweight\r\n# and improving.  \r\npython3 /workspace/benchmarks/perfzero/lib/benchmark.py --benchmark_methods=official.benchmark.keras_imagenet_benchmark.Resnet50KerasBenchmarkReal.benchmark_xla_8_gpu_fp16 --data_downloads=gs://tf-perf-imagenet-uswest1/tensorflow/imagenet --python_path=models --git_repos=\"https://github.com/tensorflow/models.git;master;6b586a910d74a44f57da4d2335c79a20dc2803ab\"\r\n````\r\n\r\n**GOOD**\r\n07/31\r\n8,723.9 to 9,000 images/sec\r\ntensorflow hash:3e0ad8a004\r\nmodel garden hash: 13e7c85d521d7bb7cba0bf7d743366f7708b9df7\r\n\r\n\r\n**BAD**\r\n08/03\r\nWe had a break in the builds for a few days and a big change to the default code path I believe.  Many problems\r\n2,764.3 to 2,900 images/sec.\r\ntensorflow hash: 3205135\r\nmodel garden: 6b586a910d74a44f57da4d2335c79a20dc2803ab\r\n\r\n**Current**\r\n09/26\r\n8,300 to 8,500 images/sec\r\ntensorflow hash: 1ae01a0\r\nmodel garden: 6f1e3b38d80f131e21f0721196df7cfc5ced2b74\r\n\r\nIt might be hard to bisect as I think the model garden code changed significantly between the two dates with the [following lines ](https://github.com/tensorflow/models/blob/master/official/vision/image_classification/resnet_imagenet_main.py#L168)and we also moved the code around recently so running the test now vs back in August there is a different directory:\r\n\r\n```python\r\n   # TODO(b/138957587): Remove when force_v2_in_keras_compile is on longer\r\n    # a valid arg for this model. Also remove as a valid flag.\r\n    if flags_obj.force_v2_in_keras_compile is not None:\r\n      model.compile(\r\n          loss='sparse_categorical_crossentropy',\r\n          optimizer=optimizer,\r\n          metrics=(['sparse_categorical_accuracy']\r\n                   if flags_obj.report_accuracy_metrics else None),\r\n          run_eagerly=flags_obj.run_eagerly,\r\n          experimental_run_tf_function=flags_obj.force_v2_in_keras_compile)\r\n    else:\r\n      model.compile(\r\n          loss='sparse_categorical_crossentropy',\r\n          optimizer=optimizer,\r\n          metrics=(['sparse_categorical_accuracy']\r\n                   if flags_obj.report_accuracy_metrics else None),\r\n          run_eagerly=flags_obj.run_eagerly)\r\n```\r\n\r\n@sganeshb has taken over testing.  I am not involved in TensorFlow performance going forward.", "Thanks Toby @tfboyd! That\u2019s much appreciated.", "For the curious: 1d139edd6eab8f26bdde6f31f3e3709fe3cd6ed0 seems to get the performance of r1.15 back to 7600+. However after 1 single day another regression 53bdcf5 degrades the performance to <4,000 again.", "@byronyi   Wanted you to know I read your findings and nice work tracking it down.  While not good, your work shows the pain of finding old issues, they often end up layered.  I would not want you to think your work bisecting was not looked at.  It was a rough situation.  The XLA team is/was focused only on single GPU performance so they could make XLA on by default for that scenario.  They seem to be looking more holistically now.  \r\n\r\nI am not sure there is anything we can do about it at this point other than move forward with TensorFlow 2.1.  FYI TensorFlow 2.0 100% has the XLA regression for multi-GPU.", "Thanks Toby! I admit it could be difficult, but I work hard with my colleagues trying to fix these regressions for our internal 2.0 fork. \r\n\r\nPersonally I have been looking forward to it for too long. Just can't wait there and see our users have to choose between 2.0 w/ new features and old 1.x wo/ perf regressions. We'd really like to have our cake and eat it :)", "@byronyi   Not irrelevant at all.  I have said before if you can get say 10K examples/sec on 8xV100s with FP16 + XLA but you need to scale to 32 GPUs (making up a number) to do that with FP32 then why are you focused on mulit-node and wasting money.  Figure out FP16.  There are exceptions for sure.  This is a simple mental example.  The other point being if you scale up 1 gpu and 8 GPUs (whatever your single node is) then you are scaling up everything else.  \r\n\r\n", "I also agree.  It is not cool this exists in 1.x.  It came down the XLA team not wanting to focus on mulit-GPU and we stopped TF 1.x testing maybe too early.  I do not recall the final data but I stopped nightly perf tests on TF 1.x a couple months ago it was just too much for one person and there was a lack of enthusiasm to resolve issues.  :-(  Your job/role sounds really cool.  I wish we had these groups when I started.  I did a lot with multi-node and desperately wanted places to test.  All I had was AWS and K80s...yup a long time ago.  I eventually just gave up as it was pointless as the P100s came out and I did not have access.  A very long story not approved for the public.  I was ahead of schedule and burned out when the time came.\r\n\r\nSuper exciting.  ", "@anj-s I saw 53bdcf55ce180f609797746992987ce89c3543bb submitted to master again. I assume the performance regression discussed above has been fixed?\r\n\r\nEDIT: I checked that the real bug is fixed by cc9938e. Thanks!", "@tfboyd The aforementioned regression seems to be caused by ef9f0e8f2f204c45462544cb0f14b9d33061de29, identified by bisecting between 08-01 nightly and 08-04 nightly. This commit is also present in both r1.15 and r2.0. \r\n\r\nI am testing with model garden version [127b158](https://github.com/tensorflow/models/tree/127b158a4c75626602f92e75582b124d142eb64b). The benchmark I am running is:\r\n\r\n```\r\npython3 official/vision/image_classification/resnet_imagenet_main.py \\\r\n  --dd=/path/to/imagenet \\\r\n  --ds=default \\\r\n  --bs=2048 \\\r\n  --dtype=fp16 \\\r\n  --enable_xla \\\r\n  --ng=8 \\\r\n  --use_tensor_lr \\\r\n  --pgtc=2 \\\r\n  --gt_mode=gpu_private \\\r\n  --datasets_num_private_threads=48 \\\r\n  --fp16_implementation=graph_rewrite\r\n```\r\n\r\nWith this CL the performance drops from ~7300 image/s to ~2000 image/s, tested on a 8x V100-PCIE machine (balanced bus topology) using NCCL and MirroredStrategy. \r\n\r\nThe nightly 1.x TF packages are built using the following setup:\r\n\r\nOS: Debian 9.9\r\nGCC: 6.3\r\nPython: 3.5.3\r\nBazel: 0.26.1\r\nCUDA: 10.0\r\ncuDNN: 7.6.2.24\r\nTensorRT: 5.1.5\r\n\r\n```\r\nbazel build \\\r\n    --action_env=CUDA_TOOLKIT_PATH=\"/usr/local/cuda\" \\\r\n    --action_env=GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\" \\\r\n    --action_env=PYTHON_BIN_PATH=\"/usr/bin/python3\" \\\r\n    --action_env=PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\" \\\r\n    --action_env=TF_CONFIGURE_IOS=\"0\" \\\r\n    --action_env=TF_CUDA_COMPUTE_CAPABILITIES=\"6.1,7.0,7.5\" \\\r\n    --config=cuda \\\r\n    --config=numa \\\r\n    --config=tensorrt \\\r\n    --copt=-Wno-sign-compare \\\r\n    --copt=-march=ivybridge \\\r\n    --define=with_default_optimizations=true \\\r\n    --define=with_xla_support=true \\\r\n    --host_copt=-march=ivybridge \\\r\n    --python_path=\"/usr/bin/python3\" \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nI will test by reverting it on the release branches. In the meanwhile, I'd like to ping @rachellim and see if she got any quick fixes for cherry-picking a fix instead of reverting the violating commit.\r\n\r\nUPDATE: reverting the following commits improves 8x V100 performance from ~1,600 to ~7,700 image/s on latest r2.0 branch snapshot 64c3d382cadf7bbe8e7e99884bede8284ff67f56:\r\n\r\n* 7107f907aa\r\n* 6208021e3d\r\n* ef9f0e8f2f\r\n\r\nPing @goldiegadde; is it still possible to revert these commits with 2.0? It will be very important for professional users to reproduce the expected multi-GPU ResNet50 performance numbers.\r\n\r\nI build the 2.0 package using the following options:\r\n\r\n```\r\nbazel build \\\r\n    --action_env=CUDA_TOOLKIT_PATH=\"/usr/local/cuda\" \\\r\n    --action_env=GCC_HOST_COMPILER_PATH=\"/usr/bin/gcc\" \\\r\n    --action_env=PYTHON_BIN_PATH=\"/usr/bin/python3\" \\\r\n    --action_env=PYTHON_LIB_PATH=\"/usr/local/lib/python3.5/dist-packages\" \\\r\n    --action_env=TF_CONFIGURE_IOS=\"0\" \\\r\n    --action_env=TF_CUDA_COMPUTE_CAPABILITIES=\"6.1,7.0,7.5\" \\\r\n    --config=cuda \\\r\n    --config=numa \\\r\n    --config=tensorrt \\\r\n    --copt=-Wno-sign-compare \\\r\n    --copt=-march=broadwell \\\r\n    --define=tf_api_version=2 \\\r\n    --define=with_default_optimizations=true \\\r\n    --define=with_xla_support=true \\\r\n    --host_copt=-march=broadwell \\\r\n    --python_path=\"/usr/bin/python3\" \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\n```", "Does https://github.com/tensorflow/tensorflow/commit/96a407a9186082045e368a680cc3e8af15d85d00 fix the performance issue? If so, maybe we can cherrypick it into the release? ", "@rachellim Assuming it does fix the issue cleanly as a cherry-pick.  It will not get into TF 2.0 as we are at the end after many weeks.  XLA is an experimental feature and doing a cherry-pick for 1.15 (same story with 2.0) may not seem risky is likely not justified form a risk reward stand point.  All of that negativity said, you can 100% petition the release owner.  I am an XLA fan for additional context. ", "@rachellim Cherry-Picking 96a407a9186082045e368a680cc3e8af15d85d00 along with 6d8f05acd72df61e5f4e5b4c72837b7caed3e942 does seem to fix the performance regression. Ping @jsimsa; any possibility we get these fixes into 2.0.1?\r\n\r\nBtw, for r1.15 cherry-picking 96a407a9186082045e368a680cc3e8af15d85d00 only seems fine. I will test the performance and see how it goes. Ping @goldiegadde; at least we can still cherry-pick for r1.15, right?", "<s>We'll cherry pick this into r1.15. As for 2.x, I'll let @goldiegadde comment further.</s>\r\n\r\nEdit: Actually, I'll defer to the release owner @goldiegadde as to how to handle this for both 1.15 and 2.x, since this PR involves pretty significant code change (including behavior changes to datasets with distribution strategy)."]}, {"number": 32244, "title": "Please modify cuda-bin in cuda_configure.bzl to copy just needed files instead of directory", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 30\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch \r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 7.4.1\r\n- CUDA/cuDNN version: 10.1 / 7.6\r\n- GPU model and memory: Quadro M2200\r\n\r\nIf you don't have CUDA installed in `/usr/local/cuda`, but in the \"normal RPM way\" (so that binaries end up in `/usr/bin`, libraries in `/usr/lib64` etc), the following `cuda-bin` rule\r\n\r\n```\r\n# in third_party/gpus/cuda_configure.bzl\r\ncopy_rules.append(make_copy_dir_rule(\r\n    repository_ctx,\r\n    name = \"cuda-bin\",\r\n    src_dir = cuda_config.cuda_toolkit_path + \"/bin\",\r\n    out_dir = \"cuda/bin\",\r\n))\r\n```\r\n\r\nwill end up trying to copy all the binaries in `/usr/bin`, which on some systems (where these commands are not other-readable) may lead to errors like this one:\r\n\r\n```\r\nERROR: /home/key/.cache/bazel/_bazel_key/8de93c72df6c3fec3e289f10fadff72b/external/local_config_cuda/cuda/BUILD:1399:1: Executing genrule @local_config_cuda//cuda:cuda-bin failed (Exit 1)\r\ncp: cannot open '/usr/bin/./sudoedit' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./locate' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./sudo' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./sudoreplay' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./chsh' for reading: Permission denied\r\ncp: cannot open '/usr/bin/./chfn' for reading: Permission denied\r\n\r\n```\r\n\r\nEvidently these files should in fact not be copied ... looking for what binaries really are wanted, I found this:\r\n\r\n```\r\n# in nccl/build_defs.bzl.tpl\r\n_device_link = rule(\r\n    implementation = _device_link_impl,\r\n    attrs = {\r\n        \"deps\": attr.label_list(),\r\n        \"out\": attr.output(mandatory = True),\r\n        \"gpu_archs\": attr.string_list(),\r\n        \"nvlink_args\": attr.string_list(),\r\n        \"_nvlink\": attr.label(\r\n            default = Label(\"@local_config_cuda//cuda:cuda/bin/nvlink\"),\r\n            allow_single_file = True,\r\n            executable = True,\r\n            cfg = \"host\",\r\n        ),\r\n        \"_fatbinary\": attr.label(\r\n            default = Label(\"@local_config_cuda//cuda:cuda/bin/fatbinary\"),\r\n            allow_single_file = True,\r\n            executable = True,\r\n            cfg = \"host\",\r\n        ),\r\n        \"_bin2c\": attr.label(\r\n            default = Label(\"@local_config_cuda//cuda:cuda/bin/bin2c\"),\r\n            allow_single_file = True,\r\n            executable = True,\r\n            cfg = \"host\",\r\n        ),\r\n        \"_link_stub\": attr.label(\r\n            default = Label(\"@local_config_cuda//cuda:cuda/bin/crt/link.stub\"),\r\n            allow_single_file = True,\r\n        ),\r\n    },\r\n```\r\n\r\nIn fact when I hard-code these 4 files, modifying the `cuda-bin` rule like this\r\n\r\n```\r\ncopy_rules.append(make_copy_files_rule(\r\n         repository_ctx,\r\n         name = \"cuda-bin\",\r\n         srcs = [\r\n            cuda_config.cuda_toolkit_path + \"/bin/\" + \"crt/link.stub\",\r\n            cuda_config.cuda_toolkit_path + \"/bin/\" + \"nvlink\",\r\n            cuda_config.cuda_toolkit_path + \"/bin/\" + \"fatbinary\",\r\n            cuda_config.cuda_toolkit_path + \"/bin/\" + \"bin2c\"\r\n        ],\r\n        outs = [\r\n            \"cuda/bin/\" + \"crt/link.stub\",\r\n            \"cuda/bin/\" + \"nvlink\",\r\n            \"cuda/bin/\" + \"fatbinary\",\r\n            \"cuda/bin/\" + \"bin2c\"\r\n        ],\r\n     ))\r\n```\r\n\r\nall works great.\r\n\r\nWould you be interested in adapting the build rule accordingly? Not only would it not fail, it'd be also cleaner not to try to copy files like `sudo` :-)\r\n\r\nI'd be happy to submit a PR but would probably need some guidance (assuming you don't want the file names hard coded as I'm doing for my workaround). Thanks!\r\n\r\n\r\n", "comments": ["Your suggestion looks like a good idea -- would you mind preparing a PR with the same changes as worked for you?\r\n\r\nCreating a PR would initiate more tests for your change, which would help catch any potential problems of changing the list of binaries.", "Great, I will do that!", "Here it is: https://github.com/tensorflow/tensorflow/pull/32838\r\n\r\nthanks!", "Hi! @skeydan ! Could you close this issue then?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "closing as #32838 landed a while ago", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32244\">No</a>\n", "thanks!"]}, {"number": 32243, "title": "SpatialDropout2D", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 1.11\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nSpatialDropout2D is often used in CNNs (https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout2D).  At inference time, the layer should be treated just like a regular dropout layer where it just scales the coefficients by the dropout term. However, when I take a Keras model with SpatialDropout2D and saved it as a TF protobuf, TensorFlow converts SpatialDropout2D into a set of primitive operators to do the random dropout instead-- even for a TensorFlow serving graph where the model has been frozen and optimized for inference.  \r\n\r\n**Will this change the current api? How?**\r\n\r\nI'd like to see the SpatialDropout layers behave the same as the regular Dropout layers during inference.  So the TensorFlow Serving protobuf should not include SpatialDropout2D (or any set of random/slice ops to reconstruct it), but instead should just scale the convolutional filters by the dropout rate as explained in the original paper (https://arxiv.org/abs/1411.4280).\r\n\r\n**Who will benefit with this feature?**\r\n\r\nIf the SpatialDropout layers behave the same as the regular Dropout layers, it will reduce the number of operations during inference and speed up inference on these graphs.\r\n\r\n**Any Other info.**\r\n", "comments": ["This sounds like it might be an implementation bug, rather than a feature request.\r\n\r\n@karmel @fchollet, could you take a look?", "@tonyreina We see that you are using old version of tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions.Please open a new issue in case you face any errors, we will get you the right help .Hence moving this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32243\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32243\">No</a>\n"]}, {"number": 32242, "title": "[TF2.0] Checkpoint doesn't store non-TF objects", "body": "Hello all,\r\n\r\nI found that `tf.train.Checkpoint` doesn't save non-TF objects and tf.Tensors. The MWE below shows the case:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass A(tf.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.scalar = 1.\r\n        self.variable = tf.Variable(1.)\r\n        self.tensor = tf.convert_to_tensor(1.)\r\n        self.list = [tf.convert_to_tensor(10.0), 20.0, tf.Variable(0.0)]\r\n\r\n    def __str__(self):\r\n        return f\"scalar={self.scalar}, variable={self.variable.numpy()}, tensor={self.tensor}, list={self.list}\"\r\n\r\n\r\na_module = A()\r\ncheckpoint = tf.train.Checkpoint(a=a_module)\r\nmanager = tf.train.CheckpointManager(checkpoint, '/tmp/example', max_to_keep=3)\r\nmanager.save()\r\n\r\nprint(f\"1. {a_module}\")\r\n\r\n#### modify\r\n\r\na_module.scalar = -100.\r\na_module.variable.assign(123.)\r\na_module.tensor = tf.convert_to_tensor(-12.)\r\na_module.list = [3., 3.]\r\n\r\nprint(f\"2. {a_module}\")\r\n\r\n#### restore\r\ncheckpoint.restore(manager.latest_checkpoint)\r\n\r\nprint(f\"3. {a_module}\")\r\n```\r\n\r\nOutput:\r\n```python\r\n1. scalar=1.0, variable=1.0, tensor=1.0, list=ListWrapper([<tf.Tensor: id=9, shape=(), dtype=float32, numpy=10.0>, 20.0, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>])\r\n2. scalar=-100.0, variable=123.0, tensor=-12.0, list=ListWrapper([3.0, 3.0])\r\n3. scalar=-100.0, variable=1.0, tensor=-12.0, list=ListWrapper([3.0, 3.0])\r\n```\r\n\r\nIn the output you can see that the checkpointer restores only the variable and completely ignores other fields of the A object.\r\n\r\nI expected that at least pythonic objects, numpy and TF tensors should be stored by the checkpointer on the disk. Is that a normal behavior?\r\n\r\nIf the answer is **yes**: I think that this feature should be implemented, otherwise it is very confusing and not straightforward why the checkpoint saves some objects and others not.\r\n**no**: this is a bug, must be fixed.\r\n\r\n**System information**\r\n- OS Platform and Distribution: `macOS 10.14.6`\r\n- TensorFlow version: `v1.12.1-10419-g5138353309 2.0.0-dev20190905`\r\n- Python version: `3.6.8`\r\n", "comments": ["I have tried on colab with TF version 2.0.0-dev20190903, 2.0.0-rc0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/e4d3e264f71cd7372f17020f0db42c76/untitled160.ipynb).Thanks!", "Hello @ravikyram , @jvishnuvardhan , @rchao! Any updates on the topic?", "Hello @ravikyram , @jvishnuvardhan , @rchao again! Does anyone have looked at this issue?", "@rchao could you PTAL?", "Reassigning to @k-w-w to take a look or triage. Thanks.", "Hi @awav,\r\n\r\nThis is intended. Tensorflow checkpoints only save and restore the variables/other TensorFlow objects with state. Constants tensors and python objects won't be included in the Checkpoint, and this is expected because some constants (e.g. lookup tables) can get exceedingly large. \r\n\r\nThere is an experimental API that lets you save arbitrary python values to the Checkpoint. Check it out here: https://www.tensorflow.org/api_docs/python/tf/train/experimental/PythonState\r\n\r\nFeel free to submit a PR with clarifications to docstring of [tf.train.Checkpoint](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32242\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32242\">No</a>\n", "Hello @k-w-w ,\r\n\r\nOkay, what's about this line: `self.list = [tf.convert_to_tensor(10.0), 20.0, tf.Variable(0.0)]`, the list has a variable, but it is not stored in the checkpoint.\r\n\r\n> Constants tensors and python objects won't be included in the Checkpoint, and this is expected because some constants (e.g. lookup tables) can get exceedingly large.\r\nVariables can be exceedingly large as well. The literals can and should be stored in a checkpoint. \r\n\r\nWriting a serializer for `int`, `float` and `str` every time when I need to store them is a bit weird.\r\n\r\nCan we keep an issue open?\r\n\r\n", "When the values were restored, the list no longer contained any variables. Checkpoint.restore only restores the values of the tensorflow objects, but will not modify the objects or datastructures containing those objects. In other words, it will not rewrite the elements in the list.\r\n\r\nWhat would be the use case for storing python ints/floats/strings? \r\n\r\nedit: Just noticed that you had written a response in the quoted text.\r\n\r\nYes, variables can get large as well. I guess a better reason that constants aren't saved out is that they aren't expected to change when training. Checkpoints are used to save/restore different states of the model.", "@awav,\r\nAny updates regarding this? Please take a look at @k-w-w's comment above and let us know if this is still an issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32242\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32242\">No</a>\n"]}, {"number": 32241, "title": "tf.keras Sequence model looses final output when saved as frozen pb graph?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes and No. Both cases fail.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux / Google Collaboratory runtime\r\n- TensorFlow installed from (source or binary): No : Google Collab default runtime\r\n- TensorFlow version (use command below): Google Collab default runtime / Tensorflow  1.14.0\r\ntf.keras 2.2.4-tf\r\n- Python version: Google Collab default runtime: 3.6.8 (default, Jan 14 2019, 11:02:34) \r\n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\r\n\r\n**Describe the current behavior**\r\n\r\nI am able to successfully build a model train, evaluate and run prediticions on a Sequence model using NasNetMobile. I can save my model using model.save in tf.keras and have a well formed h5 file with a graph, weights and inputs and outputs as expected and verified in Netron.\r\n\r\nI wish to convert this model to pb. Using various techniques I will describe below, I load the model using tf.keras, and introspecting the model summary and model inputs and outputs. All is as to be expected (save a slight difference in output tensor name).\r\n\r\nSaving the PB does not error, and creates a valid PB file but it lacks the final graph output, and tools like tf_coreml and Netron complain of missing output.\r\n\r\n**Describe the expected behavior**\r\n\r\nLoading a save h5 file and saving the resulting Tensorflow pb results in a well formed PB that can be used for inference.\r\n\r\n**Code to reproduce the issue**\r\n\r\n**Model definition:**\r\n\r\n```\r\n\r\nbase_model = tf.keras.applications.NASNetMobile(input_shape=(IMG_SIZE, IMG_SIZE, 3),\r\n                                               include_top=False,\r\n                                               weights='imagenet')\r\nbase_model.trainable = False\r\n\r\noutput = tf.keras.layers.Dense(num_labels, activation = 'sigmoid', name=\"output\")\r\nmodel = tf.keras.Sequential([\r\n  base_model,\r\n  tf.keras.layers.GlobalAveragePooling2D(),\r\n  output])\r\nmodel.summary()\r\n```\r\n\r\nWhich prints:\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nNASNet (Model)               (None, 7, 7, 1056)        4269716   \r\n_________________________________________________________________\r\nglobal_average_pooling2d (Gl (None, 1056)              0         \r\n_________________________________________________________________\r\noutput (Dense)               (None, 229)               242053    \r\n=================================================================\r\nTotal params: 4,511,769\r\nTrainable params: 242,053\r\nNon-trainable params: 4,269,716\r\n_________________________________________________________________\r\n```\r\n\r\n**Saving the model:**\r\n\r\n```\r\n#verify inputs and outputs\r\nprint(model.input.op.name)\r\nprint(model.output.op.name) \r\ntf.keras.models.save_model(model, model_path, overwrite=True,save_format=\"h5\")\r\n```\r\n\r\nor alternatively;\r\n```\r\n#verify inputs and outputs\r\nprint(model.input.op.name)\r\nprint(model.output.op.name) \r\n model.save(model_path)\r\n```\r\n\r\nBoth mechanisms print the following inputs and output tensor names:\r\n\r\n```\r\nNASNet_input\r\noutput/Identity\r\n```\r\n\r\n**Loading said model in a different ipynb:**\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n# unclear if strictly necessary but git issues imply it may be defensive to add these lines\r\ntf.reset_default_graph()\r\nK.clear_session()\r\nK.set_learning_phase(0)\r\n\r\nrestored_model = tf.keras.models.load_model(model_path, compile=True)\r\nprint(restored_model.inputs)\r\nprint(restored_model.outputs)\r\n\r\nrestored_model.summary()\r\n```\r\n\r\nWhich prints:\r\n\r\n```\r\n[<tf.Tensor 'NASNet_input:0' shape=(?, 224, 224, 3) dtype=float32>]\r\n[<tf.Tensor 'output/Sigmoid:0' shape=(?, 229) dtype=float32>]\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nNASNet (Model)               (None, 7, 7, 1056)        4269716   \r\n_________________________________________________________________\r\nglobal_average_pooling2d (Gl (None, 1056)              0         \r\n_________________________________________________________________\r\noutput (Dense)               (None, 229)               242053    \r\n=================================================================\r\nTotal params: 4,511,769\r\nTrainable params: 242,053\r\nNon-trainable params: 4,269,716\r\n```\r\n\r\nNotice the output tensor has changed its name / no longer identity / placeholder?\r\n\r\n**Saving to PB**\r\n\r\nThe simplest incantation which does not do any graph cleaning / optimization\r\n\r\n```\r\nimport datetime\r\n\r\noutput_model_name = model_name + \".pb\"\r\noutput_model_path = \"/tmp/\" + output_model_name\r\nsave_dir = \"./tmp_{:%Y-%m-%d_%H%M%S}\".format(datetime.datetime.now())\r\ntf.saved_model.simple_save(K.get_session(),\r\n                           save_dir,\r\n                           inputs={\"input\": restored_model.inputs[0]},\r\n                           outputs={\"output\": restored_model.outputs[0]})\r\n\r\nfreeze_graph.freeze_graph(None,\r\n                          None,\r\n                          None,\r\n                          None,\r\n                          restored_model.outputs[0].op.name,\r\n                          None,\r\n                          None,\r\n                           output_model_path,\r\n                          False,\r\n                          \"\",\r\n                          input_saved_model_dir=save_dir)\r\n```\r\n\r\nResults in a PB that is missing the output layer.\r\n\r\n**Other PB saving methods ive tried:**\r\n\r\nAll result with a PB missing output:\r\n```\r\nwith tf.keras.backend.get_session() as session:\r\n  graph = session.graph\r\n  input_graph_def = graph.as_graph_def()\r\n  \r\n  # For demonstration purpose we show the first 15 ops the TF model\r\nwith graph.as_default() as g:\r\n    tf.import_graph_def(input_graph_def, name='')\r\n    \r\n    ops = g.get_operations()\r\n    for op in ops[0:15]:\r\n        print('op name: {}, op type: \"{}\"'.format(op.name, op.type));\r\n    for op in ops[::-1][0:15]:\r\n        print('op name: {}, op type: \"{}\"'.format(op.name, op.type));\r\n\r\n  \r\n\r\ninput_node_names = ['NASNet_input']\r\noutput_node_names = ['output/Sigmoid']\r\ngdef = strip_unused_lib.strip_unused(\r\n      input_graph_def = input_graph_def,\r\n      input_node_names = input_node_names,\r\n      output_node_names = output_node_names,\r\n      placeholder_type_enum = dtypes.float32.as_datatype_enum)\r\n  \r\nwith gfile.GFile(output_model_path, \"wb\") as f:\r\n    f.write(gdef.SerializeToString())\r\n```\r\n\r\nAnd :\r\n\r\n```\r\n# from tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n\r\n\r\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\r\n    \"\"\"\r\n    Freezes the state of a session into a pruned computation graph.\r\n\r\n    Creates a new computation graph where variable nodes are replaced by\r\n    constants taking their current value in the session. The new graph will be\r\n    pruned so subgraphs that are not necessary to compute the requested\r\n    outputs are removed.\r\n    @param session The TensorFlow session to be frozen.\r\n    @param keep_var_names A list of variable names that should not be frozen,\r\n                          or None to freeze all the variables in the graph.\r\n    @param output_names Names of the relevant graph outputs.\r\n    @param clear_devices Remove the device directives from the graph for better portability.\r\n    @return The frozen graph definition.\r\n    \"\"\"\r\n    from tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n    graph = session.graph\r\n    with graph.as_default():\r\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\r\n        output_names = output_names or []\r\n        output_names += [v.op.name for v in tf.global_variables()]\r\n        # Graph -> GraphDef ProtoBuf\r\n        input_graph_def = graph.as_graph_def()\r\n        if clear_devices:\r\n            for node in input_graph_def.node:\r\n                node.device = \"\"\r\n        frozen_graph = convert_variables_to_constants(session, input_graph_def,\r\n                                                      output_names, freeze_var_names)\r\n        return frozen_graph\r\n\r\n\r\n\r\n      \r\nfrozen_graph = freeze_session(K.get_session(),\r\n                              output_names=[out.op.name for out in restored_model.outputs], \r\n                             clear_devices=True)\r\n\r\ntf.train.write_graph(frozen_graph, \"/tmp\", model_name+\".pb\", as_text=False)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nHere is a link the my H5 file:\r\n\r\nhttps://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-04_21_57_48.h5\r\n\r\n\r\nHere is a link to a resulting PB file ive tried to make, which lacks output:\r\n\r\nhttps://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-04_21_57_48.h5.pb\r\n\r\nThank you very much!\r\n", "comments": ["Ok, looking further into this, it appears this bug was fixed in TF 2.0:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/28668\r\n\r\nSwitching Google Collab to use TF 2.0 now prints the `output/Identity` upon loading the model:\r\n\r\n```\r\ntry:\r\n  # %tensorflow_version only exists in Colab.\r\n  %tensorflow_version 2.x\r\nexcept Exception:\r\n  pass\r\n\r\n\r\n# import tensorflow.compat.v1 as tf\r\n# tf.disable_v2_behavior()\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras.backend as K\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\nrestored_model = tf.keras.models.load_model(model_path, compile=True)\r\nprint(restored_model.input.op.name)\r\nprint(restored_model.output.op.name)\r\nrestored_model.summary()\r\n```\r\n\r\nprints\r\n\r\n```\r\nTensorFlow 2.x selected.\r\nWARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\r\nNASNet_input\r\noutput/Identity\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nNASNet (Model)               (None, 7, 7, 1056)        4269716   \r\n_________________________________________________________________\r\nglobal_average_pooling2d (Gl (None, 1056)              0         \r\n_________________________________________________________________\r\noutput (Dense)               (None, 229)               242053    \r\n=================================================================\r\nTotal params: 4,511,769\r\nTrainable params: 242,053\r\nNon-trainable params: 4,269,716\r\n```\r\n\r\nHowever, its unclear if I can export a protocol buffer from TF 2.0 directly in Python anymore? The above code all relies of sessions which are gone, as well as other functions. Perusing the documentation doesn't reveal anything obvious but im new to TF 2.0 changes.\r\n\r\nThank you in advance.\r\n ", "Using the new `tf.saved_model.save` function lets me save a pb file but, sadly, the PB file appears malformed or not appropriate for inference (Netron complains its very large - and once its loaded it appears very different than my h5). ", "Ok, so, [freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tools/freeze_graph.py) from 2.0 branch does not appear to work either, even if I adjust tensor names from operation names, I consistently get output not in graph. Im using TF 2.0.0rc0, using the freeze_graph.py, and using 'output/Identity' as my output tensor name:\r\n\r\nMy freeze graph call:\r\n\r\n`freeze_graph.py freeze_graph.py --input_saved_model_dir=\"Path/to/saved/model\" --output_node_names=\"output/identity\" --output_graph=\"/Path/To/Output.pb\"`\r\n\r\nWhich results in\r\n\r\n```\r\n2019-09-05 17:14:15.542361: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-05 17:14:15.551903: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd5b49f48e0 executing computations on platform Host. Devices:\r\n2019-09-05 17:14:15.551919: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nW0905 17:14:17.094927 4566570432 deprecation.py:323] From freeze_graph.py:163: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nI0905 17:14:19.887789 4566570432 saver.py:1284] Restoring parameters from ./CinemaNet_Saved_Model/1/variables/variables\r\nW0905 17:14:23.509812 4566570432 deprecation.py:323] From freeze_graph.py:235: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0905 17:14:23.509926 4566570432 deprecation.py:323] From /Library/Python/2.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:275: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 493, in <module>\r\n    run_main()\r\n  File \"freeze_graph.py\", line 489, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Library/Python/2.7/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/Library/Python/2.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"freeze_graph.py\", line 488, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"freeze_graph.py\", line 380, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"freeze_graph.py\", line 363, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"freeze_graph.py\", line 235, in freeze_graph_with_def_protos\r\n    variable_names_blacklist=variable_names_blacklist)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 275, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 195, in extract_sub_graph\r\n    _assert_nodes_are_present(name_to_node, dest_nodes)\r\n  File \"/Library/Python/2.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 150, in _assert_nodes_are_present\r\n    assert d in name_to_node, \"%s is not in graph\" % d\r\nAssertionError: output/Identity is not in graph\r\n```", "Any help? \r\n\r\nIs freeze_graph.py officially supported in TF 2.0?\r\n\r\nHow do I get this TF 1.14 TF Keras Sequence model ACTUALLY EXPORTED WITH OUTPUT NODES?\r\n\r\nThis seems so trivial and yet, it isn't working. I must be missing something. Id really appreciate some help.", "@vade Yes freeze_graph.py is officially supported by tensorflow 2.0. You can find it [here](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/tools/freeze_graph.py)", "Well, Ive tried it, and it doesn't work - as far as I can tell evoking the graph API for whatever reason, on this model removes the last output node. :( \r\n\r\n`python3 freeze_graph.py freeze_graph.py --input_saved_model_dir=Cinemanet-2019-09-06_13_59_49_Saved_Model_TF2 --output_node_names=cinemanet_output/Identity --output_graph=./CinemaNet.pb`\r\n\r\n```\r\n2.0.0-rc0\r\n2019-09-06 13:37:57.762585: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-06 13:37:57.782132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe12823eec0 executing computations on platform Host. Devices:\r\n2019-09-06 13:37:57.782153: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:From freeze_graph.py:163: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nW0906 13:37:59.396905 4518606272 deprecation.py:323] From freeze_graph.py:163: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nINFO:tensorflow:Restoring parameters from Cinemanet-2019-09-06_13_59_49_Saved_Model_TF2/variables/variables\r\nI0906 13:38:02.095875 4518606272 saver.py:1284] Restoring parameters from Cinemanet-2019-09-06_13_59_49_Saved_Model_TF2/variables/variables\r\nWARNING:tensorflow:From freeze_graph.py:235: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nW0906 13:38:06.049952 4518606272 deprecation.py:323] From freeze_graph.py:235: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:275: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nW0906 13:38:06.050114 4518606272 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py:275: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 493, in <module>\r\n    run_main()\r\n  File \"freeze_graph.py\", line 489, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"freeze_graph.py\", line 488, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"freeze_graph.py\", line 380, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"freeze_graph.py\", line 363, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"freeze_graph.py\", line 235, in freeze_graph_with_def_protos\r\n    variable_names_blacklist=variable_names_blacklist)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 275, in convert_variables_to_constants\r\n    inference_graph = extract_sub_graph(input_graph_def, output_node_names)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 195, in extract_sub_graph\r\n    _assert_nodes_are_present(name_to_node, dest_nodes)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/graph_util_impl.py\", line 150, in _assert_nodes_are_present\r\n    assert d in name_to_node, \"%s is not in graph\" % d\r\nAssertionError: cinemanet_output/Identity is not in graph\r\n```", "@vade  I don't have access to your h5 file and .pb file. Also can you please reproduce the issue in colab and share the github gist with me. Thanks!", "Hi, Apologies: Here is a my working google collaboration doc:\r\n\r\nhttps://colab.research.google.com/drive/1-Mb8Smap__h9Ld2l_fuLuQn0wCB4WSs_\r\n\r\nAnd here are some model files that may be pertinent:\r\n\r\nThe H5 has the output as expected and defined in the Sequence model created in tf.keras.\r\n\r\nThe output PB *does not* have an output at all.\r\n\r\nHowever, I can find the named tensor (that should be the output) of the model and use the resulting PB file.\r\n\r\nThank you\r\n\r\nhttps://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53.json\r\nhttps://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53-weights.h5\r\nhttps://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53.h5\r\nhttps://storage.cloud.google.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53.pb\r\n\r\n", "Netron images of the h5 with output and the pb without for reference:\r\n\r\n<img width=\"1136\" alt=\"Screen Shot 2019-09-18 at 2 08 41 PM\" src=\"https://user-images.githubusercontent.com/65011/65173932-e0af4380-da1d-11e9-8296-8893dd1570f1.png\">\r\n<img width=\"1136\" alt=\"Screen Shot 2019-09-18 at 2 08 09 PM\" src=\"https://user-images.githubusercontent.com/65011/65173933-e0af4380-da1d-11e9-84d9-3b92d4e255b4.png\">\r\n", "@vade I am unable to find the google collaboration doc. \r\nHeres what I get:\r\n![image](https://user-images.githubusercontent.com/47574994/65193710-e8c4ae80-da30-11e9-9f3e-9efa3719b1af.png)\r\n", "@gowthamkpr - strange - I was able to open this collab link on Chrome and it worked (I was not logged into my account):\r\n\r\nhttps://colab.research.google.com/drive/1-Mb8Smap__h9Ld2l_fuLuQn0wCB4WSs_\r\n\r\nAnd also verified the GCS storage links worked (I also share these links with collaborators and they are able to view it, and the GCS bucket and contents are public)\r\n\r\n<img width=\"1360\" alt=\"image\" src=\"https://user-images.githubusercontent.com/65011/65198743-44536400-da52-11e9-8d48-684065626aba.png\">\r\n\r\nTry these URLS - im not sure why they are different in the GCS console. \r\n\r\nhttps://storage.googleapis.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53.json\r\n\r\nhttps://storage.googleapis.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53-weights.h5\r\n\r\nhttps://storage.googleapis.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53.h5\r\n\r\nhttps://storage.googleapis.com/synopsis_cinemanet/Synopsis_Models/Cinemanet-2019-09-17_15_28_53.pb\r\n\r\nSorry for the back and forth!", "@vade Thank you I had an issue with my browser. When I am running the second step, I am unable to access the project.\r\n\r\n![image](https://user-images.githubusercontent.com/47574994/65647669-2fc41e00-dfb4-11e9-9479-c3a3e97ec89a.png)\r\n", "@gowthamkpr `freeze_graph.py` is discontinued now, right? I am referring to this release note: https://github.com/tensorflow/tensorflow/releases/tag/v2.0.0/. ", "@sayakpaul Yes, freeze_graph.py is discontinued in Tensorflow 2.0. I think that is the cause of error @vade ", "Closing this issue as it has been inactive for more than 15 days. Please add additional comments and we can open the issue again. Thanks!", "Hi I'm trying to dumb frozen .pb graph from .hdf5 file but I my .pb have difference output with .h5 file \r\n### input and output name of .hdf5 file\r\n```bash\r\nAll input nodes: [<tf.Tensor 'input_1:0' shape=(?, 256, 320, 3) dtype=float32>]\r\nAll output nodes: [<tf.Tensor 'fcn17/truediv:0' shape=(?, ?, ?, 4) dtype=float32>]\r\n```\r\n### and output value of hdf5 file like this: \r\n```bash\r\narray([[[[0.95520484, 0.02454368, 0.01151993, 0.0087315 ],\r\n         [0.96743697, 0.01915975, 0.00736554, 0.00603788],\r\n         [0.97429854, 0.01614887, 0.00556678, 0.00398591],\r\n         ...,\r\n         [0.96292394, 0.01786121, 0.00704672, 0.01216816],\r\n         [0.95855474, 0.02005052, 0.00870537, 0.0126894 ],\r\n         [0.9483133 , 0.02560406, 0.01250436, 0.01357825]],\r\n         ...,\r\n         [0.6439966 , 0.33929765, 0.01019213, 0.00651362],\r\n         [0.64723253, 0.3306629 , 0.01337334, 0.00873124],\r\n         [0.649275  , 0.3211698 , 0.01852187, 0.0110333 ]]]],\r\n      dtype=float32)\r\n```\r\n###  this is output of .pb file ( 'fcn17/truediv:0' ) : \r\n```bash\r\n[[[0.24999999 0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.24999999 0.24999999]\r\n  ...\r\n  [0.25       0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.25       0.25      ]\r\n  [0.25       0.25       0.25       0.25      ]]]\r\n```\r\nand i see my output of hdf5 file difference pb file\r\n### this is my code i try to dumb freeze graph from hdf5 file \r\n```bash\r\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\r\n    \"\"\"\r\n    Freezes the state of a session into a pruned computation graph.\r\n\r\n    Creates a new computation graph where variable nodes are replaced by\r\n    constants taking their current value in the session. The new graph will be\r\n    pruned so subgraphs that are not necessary to compute the requested\r\n    outputs are removed.\r\n    @param session The TensorFlow session to be frozen.\r\n    @param keep_var_names A list of variable names that should not be frozen,\r\n                          or None to freeze all the variables in the graph.\r\n    @param output_names Names of the relevant graph outputs.\r\n    @param clear_devices Remove the device directives from the graph for better portability.\r\n    @return The frozen graph definition.\r\n    \"\"\"\r\n    graph = session.graph\r\n    with graph.as_default():\r\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\r\n        output_names = output_names or []\r\n        output_names += [v.op.name for v in tf.global_variables()]\r\n        input_graph_def = graph.as_graph_def()\r\n        if clear_devices:\r\n            for node in input_graph_def.node:\r\n                node.device = \"\"\r\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\r\n            session, input_graph_def, output_names, freeze_var_names)\r\n        return frozen_graph\r\n\r\ndef freeze_graph_keras(net, model_dir):\r\n    \"\"\"Extract the sub graph defined by the output nodes and convert \r\n    all its variables into constant \r\n    Args:\r\n        model_dir: the root folder containing the checkpoint state file\r\n        output_node_names: a string, containing all the output node's names, \r\n                            comma separated\r\n    \"\"\"\r\n\r\n    # The export path contains the name and the version of the model\r\n    tf.keras.backend.set_learning_phase(0)  # Ignore dropout at inference\r\n    file_name = os.path.basename(model_dir).replace('.hdf5', '.pb')\r\n    model_dir = os.path.dirname(model_dir)\r\n    print(os.path.join(model_dir, file_name))\r\n    with tf.keras.backend.get_session() as sess:\r\n        tf.initialize_all_variables().run()\r\n        frozen_graph = freeze_session(K.get_session(),\r\n                                      output_names=[out.op.name for out in net.outputs])\r\n        tf.train.write_graph(frozen_graph, model_dir,\r\n                             file_name, as_text=False)\r\n    print('All input nodes:', net.inputs)\r\n    print('All output nodes:', net.outputs)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--model_dir\", type=str,\r\n                        default=\"path.hdf5\", help=\"Model folder to export\")\r\n\r\n    args = parser.parse_args()                    \r\n\r\n    model = build_my_model((256,320,3), num_classes=4,\r\n                    lr_init=1e-3, lr_decay=5e-4)\r\n\r\n    freeze_graph_keras(model, args.model_dir)\r\n```\r\nI'm using tensorflow version 1.14.0 and keras 2.2.4"]}, {"number": 32240, "title": "TPU has unsupported tensorflow op \"LogUniformCandidateSampler\" using XLA", "body": "Using NCE loss for an NLP model and ran into an issue where XLA doesn't compile using TPU.\r\n```\r\nNo registered 'LogUniformCandidateSampler' OpKernel for XLA_CPU_JIT devices compatible with node node LogUniformCandidateSampler\r\n```\r\n\r\n**System information**\r\n- Using Google's cloud TPU with Tensorflow 1.14\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/3579968/tf_env.txt)\r\n\r\n**Describe the current behavior**\r\nI get an error when running the code on TPU because XLA doesn't support candidate samplers.\r\n\r\nimage:\r\n![Screen Shot 2019-09-05 at 8 29 54 AM](https://user-images.githubusercontent.com/44978436/64356249-60b5c200-cfb7-11e9-9a09-9f9a35057f6e.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect the model to be able to run and calculate loss on TPU using XLA\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nfrom tensorflow.python.compiler.xla import xla\r\n\r\ndef my_model(features, labels):\r\n  nce_weights = tf.get_variable(\r\n    name=\"nce_weights\",\r\n    shape=[10, 4],\r\n    initializer=tf.random_uniform_initializer(-1.0, 1.0)\r\n  )\r\n  nce_biases = tf.get_variable(\r\n    name=\"nce_biases\",\r\n    shape=[10],\r\n    initializer=tf.zeros_initializer()\r\n  )\r\n\r\n  num_sampled = 1\r\n  num_classes = 10\r\n\r\n  sampler_func = tf.random.log_uniform_candidate_sampler\r\n  sampled_values = sampler_func(\r\n    true_classes=tf.cast(labels, tf.dtypes.int64),\r\n    num_sampled=num_sampled,\r\n    range_max=num_classes,\r\n    num_true=1,\r\n    unique=True,\r\n    seed=None,\r\n  )\r\n\r\n  loss = tf.reduce_mean(\r\n    tf.nn.nce_loss(\r\n      weights=nce_weights,\r\n      biases=nce_biases,\r\n      labels=labels,\r\n      inputs=features,\r\n      num_sampled=num_sampled,\r\n      num_classes=num_classes,\r\n      sampled_values=sampled_values,\r\n    )\r\n  )\r\n\r\n  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\r\n  return loss, optimizer\r\n\r\nsess = tf.Session()\r\nfeatures = tf.constant(1.2, shape=[1, 4], dtype=tf.float32)\r\nlabels = tf.constant(1, shape=[1, 1], dtype=tf.float32)\r\n\r\n[y] = xla.compile(my_model, inputs=[features, labels])\r\n\r\nsess.run(tf.global_variables_initializer())\r\nsess.run(y)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0905 08:26:12.392647 140303472117184 deprecation_wrapper.py:119] From issue.py:45: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-09-05 08:26:12.529939: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-09-05 08:26:12.908472: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\r\n2019-09-05 08:26:12.908636: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557f84258f10 executing computations on platform Host. Devices:\r\n2019-09-05 08:26:12.908650: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nW0905 08:26:13.021475 140303472117184 deprecation_wrapper.py:119] From issue.py:6: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\r\n\r\nW0905 08:26:13.398072 140303472117184 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0905 08:26:13.421444 140303472117184 deprecation_wrapper.py:119] From issue.py:42: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\r\n\r\nW0905 08:26:13.709804 140303472117184 deprecation_wrapper.py:119] From issue.py:51: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-09-05 08:26:14.238822: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-09-05 08:26:14.645355: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at xla_ops.cc:343 : Invalid argument: Detected unsupported operations when trying to compile graph cluster_18305510630026920806[] on XLA_CPU_JIT: LogUniformCandidateSampler (No registered 'LogUniformCandidateSampler' OpKernel for XLA_CPU_JIT devices compatible with node {{node LogUniformCandidateSampler}}\r\n\t.  Registered:  device='CPU'\r\n){{node LogUniformCandidateSampler}}\r\n\tThis error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\nTraceback (most recent call last):\r\n  File \"issue.py\", line 52, in <module>\r\n    sess.run(y)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 950, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph cluster_18305510630026920806[] on XLA_CPU_JIT: LogUniformCandidateSampler (No registered 'LogUniformCandidateSampler' OpKernel for XLA_CPU_JIT devices compatible with node node LogUniformCandidateSampler (defined at issue.py:27) \r\n\t.  Registered:  device='CPU'\r\n)node LogUniformCandidateSampler (defined at issue.py:27) \r\n\tThis error might be occurring with the use of xla.compile. If it is not necessary that every Op be compiled with XLA, an alternative is to use auto_jit with OptimizerOptions.global_jit_level = ON_2 or the environment variable TF_XLA_FLAGS=\"tf_xla_auto_jit=2\" which will attempt to use xla to compile as much of the graph as the compiler is able to.\r\n\t [[cluster]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node LogUniformCandidateSampler:\r\n Cast (defined at issue.py:22)\r\n\r\nInput Source operations connected to node LogUniformCandidateSampler:\r\n Cast (defined at issue.py:22)\r\n```\r\n\r\n", "comments": ["I have tried on colab with TF version 1.14, Nightly versions and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/0108c56cf87abb2f2fd74b39884d878f/untitled161.ipynb).Thanks!", "@jvishnuvardhan I am also willing to contribute to the tensorflow source code if need be", "@elyasmehtabuddin Please feel free to contribute by raising PR's [here](https://github.com/tensorflow/tensorflow/pulls). Thanks!", "Unfortunately this is not a supported op that you can use on TPUs. The list of compatible base op for TPUs is available at https://cloud.google.com/tpu/docs/tensorflow-ops. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32240\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32240\">No</a>\n", "@frankchn Are there plans to support this op in the future for XLA (not TPU)?", "I've filed a feature request for this internally, but I am not sure it would get on our roadmap in the short term. "]}, {"number": 32239, "title": "ModelCheckpoint can't use \"val_acc\" even if fit() function receives validation_data argument", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0-rc0-gpu\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: NVidia Tesla P100 (16 gb)\r\n\r\n**Describe the current behavior**\r\nRecently I switched from TF1 wrapped with native Keras to TF2 using the built-in `tf.keras` implementation. In my code I train my model like so:\r\n\r\n    checkpoint = ModelCheckpoint(auto_save_path, monitor=\"val_acc\", save_best_only=True)\r\n    callbacks.append(checkpoint)\r\n\r\n    keras_model.fit_generator(training_img_generator,\r\n        steps_per_epoch=100, epochs=20, validation_steps=100,\r\n        validation_data=validation_img_generator, callbacks=callbacks)\r\n\r\nHowever, with that code I do get the following warning message:\r\n\r\n    WARNING:tensorflow:Can save best model only with val_acc available, skipping.\r\n\r\nA full working example can be found as [Github Gist here](https://gist.github.com/haimat/65e9dddc9cd8004ea8870853f84d7b02).\r\n\r\n**Describe the expected behavior**\r\nI don't understand this error, because I do provide the `validation_data` argument to the `fit_generator()` function. Also, I used exactly the same code with Keras on top of TF1, which worked fine there. This warning came now after switching to TF2 and built-in Keras API.", "comments": ["@haimat Can you  please provide the github gist of this issue. Thanks!", "@gowthamkpr I have created a minimal working example as [Github Gist here](https://gist.github.com/haimat/65e9dddc9cd8004ea8870853f84d7b02).", "@haimat I'm facing the same issue as you when using ModelCheckpoint in 2.0-rc0. As a work-around, I used tensorflow-gpu==2.0.0-beta1 instead and changed 'val_acc' to 'val_accuracy' and the warning disappeared.\r\n\r\nBy the way, if you're using CSV logger, please also note that column names 'acc'  and 'val_acc' has been changed to 'accuracy' and 'val_accuracy' respectively.\r\n\r\nHope this helps.", "Any chance this will be fixed for the next RC or the final 2.0 release?", "> Any chance this will be fixed for the next RC or the final 2.0 release?\r\n\r\nSo unfortunately this has not been fixed in RC1.\r\nCan you at least tell me if there are plans to fix this any time soon, for final 2.0 release?\r\nIf not, is there some kind of workaround (instead of going back to beta1)?", "> is there some kind of workaround\r\n\r\nLooking into this issue, in the meantime compiling with \"accuracy\" and using \"val_accuracy\" in ModelCheckpoint should work", "Ok, so it seems with rc2 if you use \"val_accuracy\" it works fine.\r\nI dom't know whether this is the final solution for TF2 or just a workaround, but fine for me for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32239\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32239\">No</a>\n", "@haimat Sorry for the late reply, in TF2.0 it should use whatever you compiled with, i.e. `model.compile(..., metrics=['acc'])` will log as \"val_acc\", otherwise it will log as \"val_accuracy\" "]}, {"number": 32238, "title": "Accept list and tuple in get_layer.", "body": "This PR adds the possibility of calling `tf.keras.engine.network.Network.get_layer` with multiple indices or multiple names, returning a list of layers. If only one index or name is used, the method returns a single layer, so it does not break the current API. ", "comments": ["Thanks for the feedback; should I make a PR with only the unit tests that I wrote that are valid for the existing API?"]}, {"number": 32237, "title": "iOS framework build of tf_lite has missing _TFLGpuDelegateCreate object", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- TF Version: b50852ccac3d9e90af0568391ace90fc1da440e1 (master on Sep. 5 2019)\r\n- build type: built with `sh tensorflow/lite/tools/make/build_ios_universal_lib.sh`\r\n- trying to compile for iOS on Xcode 10.2\r\n\r\n\r\n**Describe the problem**\r\n\r\n- Add TensorFlow_lite.framework to target, link and add header search path as no module header avaiable\r\n- Following error on build: `\"Undefined symbols for architecture arm64:\r\n  \"_TFLGpuDelegateCreate\", referenced from:\"` in `.mm` class calling `TFLGpuDelegateCreate(&options)`", "comments": ["Hi, sorry for the late reply.\r\nCurrently, the GPU delegate should be part of the nightly builds of `TensorFlowLiteSwift` frameworks distributed via CocoaPods. See https://www.tensorflow.org/lite/guide/ios#cocoapods_developers for more details.\r\nWould that resolve your issue?", "I'm closing the issue for now. Feel free to open a new issue if you're still having problems.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32237\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32237\">No</a>\n", "ok thanks, i guess it works, haven't tried as current hack is working;)"]}, {"number": 32236, "title": "boolean_mask does not accept a Tensor as axis", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): unknown 1.14.0\r\n- Python version: 3.6.8 (Anaconda)\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\n[`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) does not accept a scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) object as axis parameter.\r\n\r\n**Describe the expected behavior**\r\n\r\nAs per the docs, [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) should accept a scalar  [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) object as axis parameter.\r\n\r\n> * `axis`: A 0-D int Tensor representing the axis in `tensor` to mask from. By default, axis is 0 which will mask from the first dimension. Otherwise K + axis <= N.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe following snippet:\r\n\r\n```py\r\nimport tensorflow as tf\r\ntf.boolean_mask([1, 2, 3], [True, False, True], axis=tf.constant(0, dtype=tf.int32))\r\n```\r\n\r\nCauses the exception:\r\n\r\n```none\r\nTypeError: slice indices must be integers or None or have an __index__ method\r\n```\r\n\r\nFor comparison, the equivalent operation with [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) works correctly:\r\n\r\n```py\r\nimport tensorflow as tf\r\nwith tf.Session() as sess:\r\n    print(sess.run(tf.gather([1, 2, 3], [0, 1], axis=tf.constant(0, dtype=tf.int32))))\r\n    # [1 2]\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nFull traceback:\r\n\r\n```none\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-4beb5ed72842> in <module>\r\n      1 import tensorflow as tf\r\n----> 2 tf.boolean_mask([1, 2, 3], [True, False, True], axis=tf.constant(0, dtype=tf.int32))\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py in boolean_mask(tensor, mask, name, axis)\r\n   1369           \" are None.  E.g. shape=[None] is ok, but shape=None is not.\")\r\n   1370     axis = 0 if axis is None else axis\r\n-> 1371     shape_tensor[axis:axis + ndims_mask].assert_is_compatible_with(shape_mask)\r\n   1372\r\n   1373     leading_size = gen_math_ops.prod(shape(tensor)[axis:axis + ndims_mask], [0])\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py in __getitem__(self, key)\r\n    861     if self._dims is not None:\r\n    862       if isinstance(key, slice):\r\n--> 863         return TensorShape(self._dims[key])\r\n    864       else:\r\n    865         if self._v2_behavior:\r\n\r\nTypeError: slice indices must be integers or None or have an __index__ method\r\n```", "comments": ["Issue replicating for the TF version-1.14, kindly find the [gist](https://colab.sandbox.google.com/gist/oanush/8f7790e5d292c7d84482bd3c9629d2e0/32236.ipynb) of colab.Thanks!", "@javidcf I tried with latest TF and the issue should have been fixed. I will close this issue but feel free to reopen if issue persists.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32236\">No</a>\n", "@yongtang Thanks for having a look. Unfortunately, this still happens in graph mode, e.g. with `tf.function`:\r\n\r\n```python\r\n@tf.function\r\ndef f():\r\n    return tf.boolean_mask([1, 2, 3], [True, False, True],\r\n                           axis=tf.constant(0, dtype=tf.int32))\r\nf()\r\n# TypeError: slice indices must be integers or None or have an __index__ method\r\n```\r\n\r\nTested in TensorFlow 2.2.0-rc4.", "@javidcf  Added a PR for the fix.", "@javidcf Update:  Added a PR #39159 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32236\">No</a>\n"]}, {"number": 32235, "title": "tf.scatter_nd sums updates if indices are duplicated", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0rc0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\ntf.scatter_nd sums update values if indices are present multiple times. I use this function for moving pixels in an image. Some pixels are present twice and this gives wrong pixel values in scatter.\r\n\r\ntf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)\r\ntf.Tensor([ 0 11  0 10  9  0  0 24], shape=(8,), dtype=int32)\r\ntf.Tensor([ 0 11  0 10  9  0  0 36], shape=(8,), dtype=int32)\r\n**Describe the expected behavior**\r\nI expect this behavior:\r\n\r\ntf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)\r\ntf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)\r\ntf.Tensor([ 0 11  0 10  9  0  0 12], shape=(8,), dtype=int32)\r\n\r\nOtherwise I need to find unique indices and delete multiples. And why it is adding? Maybe other options are possible then too.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nindices1 = tf.constant([[4], [3], [1], [7]])\r\nupdates1 = tf.constant([9, 10, 11, 12])\r\n\r\nindices2 = tf.constant([[4], [3], [1], [7], [7]])\r\nupdates2 = tf.constant([9, 10, 11, 12, 12])\r\n\r\nindices3 = tf.constant([[4], [3], [1], [7], [7], [7]])\r\nupdates3 = tf.constant([9, 10, 11, 12, 12, 12])\r\n\r\nshape = tf.constant([8])\r\n\r\nscatter1 = tf.scatter_nd(indices1, updates1, shape)\r\nscatter2 = tf.scatter_nd(indices2, updates2, shape)\r\nscatter3 = tf.scatter_nd(indices3, updates3, shape)\r\n\r\nprint(scatter1)\r\nprint(scatter2)\r\nprint(scatter3)\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@RichardArendsen71 ,\r\nThe present functionality is for summation only as per the [link](https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/scatter_nd?hl=en) `If indices contains duplicates, then their updates are accumulated (summed).` \r\nCan you please let us know the use cases where the other operations can be applied,so that we can consider it as a feature request?Thanks!", "Thanks for the answer. I missed that part. Actually I wanted to use:\r\n\r\n`image_rotate=tf.keras.preprocessing.image.apply_affine_transform(\r\n            image, theta=-angle*180/math.pi, fill_mode='constant', cval=0.0, order=1)`\r\n\r\nI works in eager execution, but with `@tf.function` it returns an error:\r\n\r\n> NotImplementedError: in converted code:\r\n    relative to /home/face:\r\n\r\n    projects/tensorflow2/facenettf2/crop_compare.py:27 crop_eyes_fixed1  *\r\n        image_rotate=tf.keras.preprocessing.image.apply_affine_transform(\r\n    .virtualenvs/tensorflow2/lib/python3.6/site-packages/keras_preprocessing/image/affine_transformations.py:285 apply_affine_transform  *\r\n        theta = np.deg2rad(theta)\r\n    .virtualenvs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:736 __array__\r\n        \" array.\".format(self.name))\r\n\r\n    NotImplementedError: Cannot convert a symbolic Tensor (truediv_4:0) to a numpy array.\r\n\r\nIf this can be fixed I am also happy. For a sort term solution I tried to write my own rotation code and needed the function tf.scatter_nd, to set the pixel values of the rotated images.\r\n\r\nSo, main reason. I want to rotate an image at an given angle. I found [https://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/image/rot90?hl=en](url) , but this is only 90 degrees.\r\n\r\nWhat is the best way to go? I really want to use tensorflow 2.0.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32235\">No</a>\n", "When will this function be available for tensorflow 2.0? https://www.tensorflow.org/api_docs/python/tf/contrib/image/transform", "Unfortunately TF 2.0 won't have this functionality. However we have added many other functions to ```tf.image``` module. See https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image#functions\r\n\r\nYou may use ```transform``` function from tensorflow addons package if necessary.\r\nFor this you have to install ```tensorflow-addons``` separately.\r\nSee https://github.com/tensorflow/addons#maintainers\r\nThanks!", "I tried addons before, but that did not work with the latest TF 2.0 version. Now, I tried it again and it works, also with AutoGraph. So, I can do now what I needed. Thanks."]}, {"number": 32234, "title": "TF-2.0: Keras model save load memory leakage", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): tensorflow-gpu==2.0.0-beta1\r\n- Python version: Python 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.0.130/ cuDNN 7.6\r\n- GPU model and memory: GeForce GTX 1050 Ti, 4036 MiB\r\n\r\n**Describe the current behavior**\r\nDuring saving and loading model I face memory leakage. Eventually it crashes with `OSError: [Errno 12] Cannot allocate memory`. During real training it happens quite fast.\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect that memory should be cleaned.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tqdm import tqdm\r\nfrom memory_profiler import profile\r\n\r\ndata_array = np.random.random_sample((1, 1024))\r\ntf_array = tf.constant(data_array, dtype=tf.float32)\r\n\r\ninput = tf.keras.Input((1, 1024))\r\nhidden_layer = tf.keras.layers.Dense(1024)(input)\r\noutput = tf.keras.layers.Dense(1)(hidden_layer)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\n\r\npred = model([tf_array])\r\nprint(pred)\r\n\r\n\r\n@profile\r\ndef func():\r\n    export_path = \"temp_export\"\r\n    tf.saved_model.save(model, export_path)\r\n    imported = tf.saved_model.load(export_path)\r\n\r\n\r\nfor i in tqdm(range(1000000), total=1000000):\r\n    func()\r\n```\r\n\r\n**Other info / logs**\r\nProfiler logs (2 steps)\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2019-09-05 15-14-15](https://user-images.githubusercontent.com/54936869/64340897-de95bf80-cfef-11e9-8069-4a8c532cd059.png)\r\n\r\n", "comments": ["@julyrashchenko Can you please provide the github gist of the issue as I am unable to reproduce it on my side. Thanks!", "@gowthamkpr \r\nI managed to reproduce it on Google colab, here is the [gist](https://colab.research.google.com/drive/1If-E-UR23-ytIIIYctXScoJOMxZp42Xc). Is it okay? Also I attached output of my code running. You can see, that at the end there is memory leakage of about 4 Gb.\r\n![image](https://user-images.githubusercontent.com/54936869/64475250-01a7a700-d189-11e9-978f-a225a1d4db1e.png)\r\n\r\n", "@julyrashchenko Can you try `!pip install -q tf-nightly-gpu-2.0-preview` and let us know. With `tf-nightly` my output looks little different. ~100 MB per 100 iterations doesn't look bad i guess. @aaroey will know better. You could also delete those `imported` models. Thanks!\r\n\r\n```\r\n0 : free memory 11185.9296875 Mb\r\n100 : free memory 11073.01171875 Mb\r\n200 : free memory 10961.89453125 Mb\r\n300 : free memory 10849.40625 Mb\r\n400 : free memory 10738.34765625 Mb\r\n500 : free memory 10624.68359375 Mb\r\n600 : free memory 10512.73828125 Mb\r\n700 : free memory 10398.15234375 Mb\r\n800 : free memory 10285.1875 Mb\r\n900 : free memory 10172.0859375 Mb\r\n1000 : free memory 10057.1875 Mb\r\n1100 : free memory 9946.0859375 Mb\r\n1200 : free memory 9834.51171875 Mb\r\n1300 : free memory 9723.73046875 Mb\r\n```", "Yes, I also see that with such settings (tf-nightly-gpu-2.0-preview) memory leak decreased to about 100 Mb per 100 iterations. Usage `del imported` didn't changed situation (~100 Mb per 100 steps, too). But I  think it is still a huge problem because leak increases with complicating the model. E.g., when I add a couple of dense layers, leak is about 200 Mb per 100 steps.\r\n![\u0421\u043d\u0438\u043c\u043e\u043a \u044d\u043a\u0440\u0430\u043d\u0430 \u043e\u0442 2019-09-30 14-18-32](https://user-images.githubusercontent.com/54936869/65874031-6ec7ea00-e38d-11e9-867b-d6e7b3dec7d1.png)\r\n", "Let me add to this thread what I saw w.r.t. tensorflow 2.0 memory leak in LoadSavedModel.\r\n\r\n-  System info\r\n\r\n```\r\nmodel name      : Intel(R) Xeon(R) CPU           X5670  @ 2.93GHz\r\nLSB Version:    :core-4.1-amd64:core-4.1-noarch\r\nDistributor ID: CentOS\r\nDescription:    CentOS Linux release 7.4.1708 (Core)\r\nRelease:        7.4.1708\r\ngcc version: (crosstool-NG 1.24.0) 8.3.0\r\nPython version: Python 3.7.3\r\nBazel version (if compiling from source): bazel 0.26.1\r\nNO GPU used nor configed\r\n```\r\n\r\n- Tensorflow versions\r\nI detected this memory leak in both release `2.0.0` and `r2.1` branch.\r\n\r\n- Background \r\nI trained a tensorflow model in python and then loaded this model in C++ to do inference, then I detected memory leak using `-fsanitize leak`.\r\n\r\nTo reproduce this memory leak, any arbitrary simple TF model can be used. \r\n\r\n- Step 1: Save an arbitrary trained model in Python\r\nPython code that I used to save a TF model to a directory.\r\n```\r\n    evl = model.evaluate(x_train, y_train)\r\n    tf.saved_model.save(model, out_dir)\r\n```\r\n , and under output directory \r\n```\r\nls out_dir\r\nsaved_model.pb assets/ variables/\r\n```\r\n\r\n- Step2: main.cpp in C++\r\nBelow is the C++ main.cpp which simply loads saved model from a directory, check status and then exit.  Please replace TRAINED_MODEL_PATH with your own model path before compile.\r\n```\r\n#include <tensorflow/cc/saved_model/loader.h>         // SavedModelBundle\r\n#include <tensorflow/cc/saved_model/tag_constants.h>  // kSavedModelTagServe\r\n#include <tensorflow/core/framework/tensor.h>         // Tensor\r\n#include <tensorflow/core/platform/init_main.h>       // Init_Main\r\n#include <iostream>\r\n#include <stdexcept>\r\n\r\nint main(int argc, char** argv)\r\n{\r\n    tensorflow::SavedModelBundle bundle;\r\n    tensorflow::SessionOptions sessionOptions;\r\n    tensorflow::RunOptions runOptions;\r\n    tensorflow::Status load_graph_status = LoadSavedModel(\r\n        sessionOptions, runOptions, \"TRAINED_MODEL_PATH\",\r\n        {tensorflow::kSavedModelTagServe}, &bundle);\r\n    // Check load was successful\r\n    if (!load_graph_status.ok()) {\r\n        std::cout << \"Error!\" << std::endl;\r\n        return -1;\r\n    }\r\n    return 0;\r\n}\r\n```\r\nBash cmd to compile `main.cpp` with ASAN, USAN, and leak detection.\r\n```\r\ng++ -fsanitize=address -fsanitize=leak -fsanitize=undefined main.cpp \\\r\n    -Ipath_to_tensorflow_include \\\r\n    -ltensorflow_cc -ltensorflow_framework -lrt \\\r\n    -Lpath_to_tensorflow_lib \\\r\n    -o a.out\r\n```\r\nIf you have installed `libtensorflow` and configed using `pkg_config`, you may alternative use:\r\n```\r\ng++ -fsanitize=address -fsanitize=leak -fsanitize=undefined main.cpp $(pkg-config --cflags tensorflow) $(pkg-config --libs tensorflow) -o a.out\r\n```\r\n\r\nBash cmd to run compiled executable:\r\n```\r\nexport LSAN_OPTIONS=reports_objects=1:verbosity=0:detect_leaks=1\r\n./a.out\r\n```\r\n\r\nThe above compiled executable run to complete successfully, however, memory leak detected. See error log below:\r\n```\r\n2019-11-10 14:37:49.126448: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel   from: /home/UNIXHOME/yli/repo/ccsqv/models/tf_multinom_2019_10_28\r\n2019-11-10 14:37:49.131680: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph   with tags { serve }\r\n2019-11-10 14:37:49.131734: I tensorflow/cc/saved_model/loader.cc:264] Reading SavedModel  debug info (if present) from: /home/UNIXHOME/yli/repo/ccsqv/models/tf_multinom_2019_10_28\r\n2019-11-10 14:37:49.131881: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU  supports instructions that this TensorFlow binary was not compiled to use: SSE3 SSE4.1     SSE4.2\r\n2019-11-10 14:37:49.205028: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU  Frequency: 2925790000 Hz\r\n2019-11-10 14:37:49.214318: I tensorflow/compiler/xla/service/service.cc:168] XLA service  0x614000006c40 initialized for platform Host (this does not guarantee that XLA will be     used). Devices:\r\n2019-11-10 14:37:49.214379: I tensorflow/compiler/xla/service/service.cc:176]              StreamExecutor device (0): Host, Default Version\r\n2019-11-10 14:37:49.261663: I tensorflow/cc/saved_model/loader.cc:203] Restoring           SavedModel bundle.\r\n2019-11-10 14:37:49.365354: I tensorflow/cc/saved_model/loader.cc:152] Running             initialization op on SavedModel bundle at path: /home/UNIXHOME/yli/repo/ccsqv/models/      tf_multinom_2019_10_28\r\n2019-11-10 14:37:49.386885: I tensorflow/cc/saved_model/loader.cc:333] SavedModel load for tags { serve }; Status: success: OK. Took 260467 microseconds.\r\n\r\n=================================================================\r\n==14480==ERROR: LeakSanitizer: detected memory leaks\r\n\r\nDirect leak of 24 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7f0928f47a69 in operator new(unsigned long) /build/gcc-tc-passfinal-targ2targ//src/gcc/libsanitizer/asan/asan_new_delete.cc:90\r\n    #1 0x7f09212aca67 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233ea67)\r\n    #2 0x7f092129e88a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::    allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233088a)\r\n    #3 0x7f091e611fb8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::             allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_framework.so.2+0xef3fb8)\r\n    #4 0x7f0927491ae2 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/    libtensorflow_cc.so.2+0x8523ae2)\r\n    #5 0x7f091e6921af in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_framework.  so.2+0xf741af)\r\n    #6 0x7f091dee5d15 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::                 char_traits<char>, std::allocator<char> > const&, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::        __cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >  >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::SavedModelBundle*) (/home/UNIXHOME/yli/venv/tf-r2. 1-cpu/lib/libtensorflow_framework.so.2+0x7c7d15)\r\n    #7 0x405a32 in main (/home/UNIXHOME/yli/repo/ccsqv/a.out+0x405a32)\r\n    #8 0x7f091c361c04 in __libc_start_main (/lib64/libc.so.6+0x21c04)\r\n\r\nIndirect leak of 4096 byte(s) in 1 object(s) allocated from:\r\n    #0 0x7f0928f47a69 in operator new(unsigned long) /build/gcc-tc-passfinal-targ2targ//src/gcc/libsanitizer/asan/asan_new_delete.cc:90\r\n    #1 0x7f09212ac3a8 in void std::vector<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar>  >, std::allocator<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> > > >::                  _M_realloc_insert<tensorflow::kernel_factory::OpKernelRegistrar*>(__gnu_cxx::__normal_iterator<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::          default_delete<tensorflow::kernel_factory::OpKernelRegistrar> >*, std::vector<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::OpKernelRegistrar> >, std::allocator<std::unique_ptr<tensorflow::kernel_factory::OpKernelRegistrar, std::default_delete<tensorflow::kernel_factory::         OpKernelRegistrar> > > > >, tensorflow::kernel_factory::OpKernelRegistrar*&&) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233e3a8)\r\n    #2 0x7f09212acc6e in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233ec6e)\r\n    #3 0x7f092129e88a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::    allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233088a)\r\n    #4 0x7f091e611fb8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::             allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_framework.so.2+0xef3fb8)\r\n    #5 0x7f0927491ae2 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/    libtensorflow_cc.so.2+0x8523ae2)\r\n    #6 0x7f091e6921af in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_framework.  so.2+0xf741af)                                                                                                                                                                   #7 0x7f091dee5d15 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::                 char_traits<char>, std::allocator<char> > const&, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::        __cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >  >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::SavedModelBundle*) (/home/UNIXHOME/yli/venv/tf-r2. 1-cpu/lib/libtensorflow_framework.so.2+0x7c7d15)\r\n    #8 0x405a32 in main (/home/UNIXHOME/yli/repo/ccsqv/a.out+0x405a32)\r\n    #9 0x7f091c361c04 in __libc_start_main (/lib64/libc.so.6+0x21c04)\r\n\r\nIndirect leak of 376 byte(s) in 376 object(s) allocated from:\r\n    #0 0x7f0928f47a69 in operator new(unsigned long) /build/gcc-tc-passfinal-targ2targ//src/gcc/libsanitizer/asan/asan_new_delete.cc:90\r\n    #1 0x7f09212acb47 in tensorflow::RegisterXlaDeviceKernels(char const*, char const*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233eb47)\r\n    #2 0x7f092129e88a in tensorflow::XlaCpuDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::    allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_cc.so.2+0x233088a)\r\n    #3 0x7f091e611fb8 in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::             allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_framework.so.2+0xef3fb8)\r\n    #4 0x7f0927491ae2 in tensorflow::DirectSessionFactory::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/    libtensorflow_cc.so.2+0x8523ae2)\r\n    #5 0x7f091e6921af in tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (/home/UNIXHOME/yli/venv/tf-r2.1-cpu/lib/libtensorflow_framework.  so.2+0xf741af)\r\n    #6 0x7f091dee5d15 in tensorflow::LoadSavedModel(tensorflow::SessionOptions const&, tensorflow::RunOptions const&, std::__cxx11::basic_string<char, std::                 char_traits<char>, std::allocator<char> > const&, std::unordered_set<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::        __cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >  >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, tensorflow::SavedModelBundle*) (/home/UNIXHOME/yli/venv/tf-r2. 1-cpu/lib/libtensorflow_framework.so.2+0x7c7d15)\r\n    #7 0x405a32 in main (/home/UNIXHOME/yli/repo/ccsqv/a.out+0x405a32)\r\n    #8 0x7f091c361c04 in __libc_start_main (/lib64/libc.so.6+0x21c04)\r\n\r\nSUMMARY: AddressSanitizer: 4496 byte(s) leaked in 378 allocation(s).\r\n```\r\nThe above leak was seen in both release `v2.0.0` and branch `r2.1`. \r\n\r\nThis may be able to explain memory leak found in python LoadSavedModel.\r\n\r\nTo prevent further memory leakage, creating a nightly build with ASAN, USAN, LeakSanitizer and cover the core APIs may be a good idea to detect them early.\r\n\r\n- I dugged a little bit. Function `XlaCpuDeviceFactory::CreateDevices` calls function `RegisterXlaDeviceKernels`, which calls `new` to allocate memory for a few objects, but may not been deleted in the end.\r\n-- https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/compiler/jit/xla_cpu_device.cc#L67 \r\n-- https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/compiler/jit/xla_device.cc#L525\r\n\r\nWould you please take a look at this issue? Thank you!", "To avoid running into memory leakage caused by XLA support, we recompiled tensorflow library without XLA support e.g., `build:xla --define with_xla_support=false`, and confirmed that Leak sanitizer check runs OK.\r\n\r\nThis was our work-around to this bug, not a FIX to this bug.", "I don't think this is the root cause.  Since we have `static XlaDeviceOpRegistrations* registrations = ...` the allocation should happen only once and not cause a gradual memory leak.\r\n\r\nMoreover, we need the `XlaDeviceOpRegistrations` to be alive for the entire process lifetime.", "Thanks for the reply and pointing out that `static XlaDeviceOpRegistrations* registrations = ` looks fine.\r\n\r\nPlease let me know once the root cause is found and fixed. Thanks!", "@julyrashchenko I've tried the model in the example with tf-nightly, but don't see a huge increase of the memory usage. Maybe the model is not large enough? Can you try tf-nightly for your real training job? We have fixed several memory leaking issues recently, and this problem may be potentially fixed already.", "@julyrashchenko I just ran your code and I cannot reproduce the issue. I compared results from my earlier run (posted above) with the current run with `tf-nightly-gpu`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/997e4ce4c826fc15521f6c4eec4234e9/untitled670.ipynb) is the gist for your reference. \r\n\r\nEarlier, it used to leak ~100MB per 100 iterations. Currently it is way lower (~2 MB to 4 MB) per 100 iterations.\r\n\r\nI am closing this issue as it was resolved in the `tf-nightly`. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32234\">No</a>\n", "> @julyrashchenko I've tried the model in the example with tf-nightly, but don't see a huge increase of the memory usage. Maybe the model is not large enough? Can you try tf-nightly for your real training job? We have fixed several memory leaking issues recently, and this problem may be potentially fixed already.\r\n\r\n> @julyrashchenko I just your code and I cannot reproduce the issue. I compared results from my earlier run (posted above) with the current run with `tf-nightly-gpu`. [Here](https://colab.sandbox.google.com/gist/jvishnuvardhan/997e4ce4c826fc15521f6c4eec4234e9/untitled670.ipynb) is the gist for your reference.\r\n> \r\n> Earlier, it used to leak ~100MB per 100 iterations. Currently it is way lower (~2 MB to 4 MB) per 100 iterations.\r\n> \r\n> I am closing this issue as it was resolved in the `tf-nightly`. Please feel free to reopen if the issue persists again. Thanks!\r\n\r\n@yhliang2018 @jvishnuvardhan Yes, I confirm that memory leak dissapeared with such setting when running on Google colab. But unfortunately I still suffer from it when running the same code on my machine, although much less with `tf-nightly` than before (about 100 Mb per 100 iterations now). Maybe, someone also encounters this problem? Or there is something I could not have taken into account?", "@julyrashchenko What is the TF version you installed? If you have installed latest `tf-nightly` as in my google colab, you should be noticing similar performance. How much leak you are noticing now on your local machine? Thanks!", "@jvishnuvardhan I installed with `pip install tf-nightly-gpu`, `tf.__version__` shows `2.1.0-dev20191126`.  Besides I tested it on one other machine and got the same results. With this code, memory leakage varies up to 100 Mb per 100 iterations. I was waiting till the end, but memory was never freed, it just decreased to zero, and then the program stopped. I attach the logs from my terminal.\r\n\r\n```\r\n$ python3 test_mem_leak.py\r\n2019-11-27 10:09:39.775448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\ntfversion =  2.1.0-dev20191126\r\n2019-11-27 10:09:40.578410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2019-11-27 10:09:40.619368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.619875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2019-11-27 10:09:40.619893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-11-27 10:09:40.666372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-11-27 10:09:40.695804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-11-27 10:09:40.703159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-11-27 10:09:40.754736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-11-27 10:09:40.763086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-11-27 10:09:40.849905: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-27 10:09:40.850212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.851218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.852249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-11-27 10:09:40.852961: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-11-27 10:09:40.896006: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2808000000 Hz\r\n2019-11-27 10:09:40.896977: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x344f4a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2019-11-27 10:09:40.897046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2019-11-27 10:09:40.976757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.977051: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x411bee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2019-11-27 10:09:40.977068: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n2019-11-27 10:09:40.977163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.977366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2019-11-27 10:09:40.977384: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2019-11-27 10:09:40.977395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2019-11-27 10:09:40.977403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2019-11-27 10:09:40.977412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2019-11-27 10:09:40.977421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2019-11-27 10:09:40.977428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2019-11-27 10:09:40.977435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2019-11-27 10:09:40.977469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.977677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:40.977859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-11-27 10:09:41.359293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-11-27 10:09:41.359323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-11-27 10:09:41.359330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-11-27 10:09:41.359455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:41.359711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-11-27 10:09:41.359913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2019-11-27 10:09:41.562306: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n0 : free memory 9856.7734375 Mb\r\n100 : free memory 9757.01171875 Mb\r\n200 : free memory 9667.04296875 Mb\r\n300 : free memory 9580.7421875 Mb\r\n400 : free memory 9500.4921875 Mb\r\n500 : free memory 9397.984375 Mb\r\n600 : free memory 9156.78515625 Mb\r\n700 : free memory 9087.34765625 Mb\r\n800 : free memory 9027.28125 Mb\r\n900 : free memory 9001.51171875 Mb\r\n1000 : free memory 8924.78125 Mb\r\n1100 : free memory 8836.68359375 Mb\r\n1200 : free memory 8741.44140625 Mb\r\n1300 : free memory 8663.7421875 Mb\r\n1400 : free memory 8587.09375 Mb\r\n1500 : free memory 8504.03125 Mb\r\n```", "@julyrashchenko Not sure what is the root-cause. In colab it is working well as I mentioned in my last comment. Thanks!", "I am also getting a memory leak in my setup, which does active learning. It required lots of saving and loading. I am running my processes inside dockers and after a while dockers shut down my processes because they acquire memory and never release it.\r\nI am running 2.0 and will try ```tf-nightly-gpu```.", "I've managed to find a workaround for this issue which satisfied me. This is to save a model with `h5` format. With this approach there is no memory leak even with tf-2.0, and besides there are other benefits:\r\n1. models are saved and loaded much faster (in my case ~4 times faster);\r\n2. loaded models take much less memory (10 times less);\r\n3. prediction also goes faster (~5 times).\r\n\r\nAnd it didn't require many modifications to apply this approach,  essentially I had to implement method `get_config` for my custom layers.\r\n\r\nAlso if you need immediate memory deallocation, you can use `tf.keras.backend.clear_session()` and `gc.collect()`, they work fine now.\r\n\r\nHere is modified code from my [gist](https://colab.research.google.com/drive/1If-E-UR23-ytIIIYctXScoJOMxZp42Xc):\r\n```\r\nimport tensorflow as tf\r\nimport psutil\r\nimport gc\r\n\r\ninput = tf.keras.Input((1, 1024))\r\ndense1 = tf.keras.layers.Dense(1024)(input)\r\ndense2 = tf.keras.layers.Dense(1024)(dense1)\r\ndense2 = tf.keras.layers.BatchNormalization()(dense2)\r\ndense2 = tf.keras.layers.LeakyReLU()(dense2)\r\noutput = tf.keras.layers.Dense(1)(dense2)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\n\r\ndef func():\r\n  export_path = \"temp_export.h5\"\r\n  model.save(export_path)\r\n  tf.keras.models.load_model(export_path)\r\n  tf.keras.backend.clear_session()\r\n\r\nfor i in range(1000000):\r\n    func()\r\n    if i % 100 == 0:\r\n        print(i, \": free memory\", psutil.virtual_memory().available / (1024.0 ** 2), \"Mb\") \r\n    gc.collect()\r\n```", "@julyrashchenko Thanks for the update.\r\nAnother important thing as mentioned by @k-w-w in another [post](https://github.com/tensorflow/tensorflow/issues/28923#issuecomment-521027273)\r\n\r\n> The best practices can be summed up to:\r\nIf you are planning to load the model back into Keras, use the Keras APIs for saving and loading. If you're not planning on using the Keras APIs to further train the model, then tf.saved_model.save and tf.saved_model.load is sufficient. \r\n\r\nI am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32234\">No</a>\n", "Similar issue #40171 ", "Hi, the fix with `.h5` doesn't do the trick for me, i still get memory leaks with both `save` and `save_weights` on tensorflow==2.7.0:\r\n\r\n```\r\nimport gc\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\nfrom tqdm import tqdm\r\n\r\ninput = tf.keras.Input((1, 1024))\r\ndense1 = tf.keras.layers.Dense(1024)(input)\r\ndense2 = tf.keras.layers.Dense(1024)(dense1)\r\ndense2 = tf.keras.layers.BatchNormalization()(dense2)\r\ndense2 = tf.keras.layers.LeakyReLU()(dense2)\r\noutput = tf.keras.layers.Dense(1)(dense2)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\n\r\n\r\ndef func():\r\n    export_path = \"temp_export.h5\"\r\n    model.save(export_path)\r\n    tf.keras.backend.clear_session()\r\n    gc.collect()\r\n\r\n\r\nfirst_memory_usage = psutil.virtual_memory().available\r\nprogress_bar = tqdm()\r\nstep = 0\r\nwhile True:\r\n    func()\r\n    progress_bar.set_description(\r\n        f\"Already lost {(first_memory_usage - psutil.virtual_memory().available) / (1024 ** 2):.3f} MB from tf.keras.Model.save\"\r\n    )\r\n    step += 1\r\n    progress_bar.update(step)\r\n```\r\n```\r\nimport gc\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\nfrom tqdm import tqdm\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    input = tf.keras.Input((1, 1024))\r\n    dense1 = tf.keras.layers.Dense(1024)(input)\r\n    dense2 = tf.keras.layers.Dense(1024)(dense1)\r\n    dense2 = tf.keras.layers.BatchNormalization()(dense2)\r\n    dense2 = tf.keras.layers.LeakyReLU()(dense2)\r\n    output = tf.keras.layers.Dense(1)(dense2)\r\n    model = tf.keras.Model(inputs=[input], outputs=[output])\r\n\r\n    first_memory_usage = psutil.virtual_memory().available\r\n    progress_bar = tqdm()\r\n    step = 0\r\n    while True:\r\n        model.save_weights(\"test.h5\")\r\n\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n        progress_bar.set_description(\r\n            f\"Already lost {(first_memory_usage - psutil.virtual_memory().available) / (1024 ** 2):.3f} MB from tf.keras.Model.save_weights\"\r\n        )\r\n        step += 1\r\n        progress_bar.update(step)\r\n           \r\n```\r\n", "@mayerjTNG As the closed issue was an old issue, Please open a new issue with the above standalone code to reproduce the issue. Thanks!"]}, {"number": 32233, "title": "Why tfrecord only support Int64, Float, Bytes, not support Int32 or Double\uff1f", "body": "for example\uff0c i only need Int32 \uff0c it seems to need more space to store the Int64 data\uff1f also i need Double but using Float will cut off the precision of data.", "comments": ["These are proto types not Python types, which means a) integers are encoded as varints which means small values will not use 8 bytes (see https://developers.google.com/protocol-buffers/docs/encoding) b) the `float` type can represent both 32-bit and 64-bit values (see https://developers.google.com/protocol-buffers/docs/proto).", "> b) the `float` type can represent both 32-bit and 64-bit values (see https://developers.google.com/protocol-buffers/docs/proto).\r\n\r\nI don't think that's true - protobuf have a `double` type, and the `float` type maps to a 32-bit in any language that have different types:\r\n\r\n![image](https://user-images.githubusercontent.com/56919/111143236-d70c8a80-8585-11eb-9146-e6da70e91d1d.png)\r\n"]}, {"number": 32232, "title": "Distributed Training is not working with input signature", "body": "Distributed training is not working when I am passing input signature in tf.function\r\n\r\n\r\n```\r\n@tf.function(input_signature=input_signature)\r\ndef _distributed_train_step(self, inputs, targets):\r\n        def step_fn(inp, tar):\r\n            with tf.GradientTape() as tape:\r\n                logits = self(inp, training=True)\r\n                cross_entropy = self.get_loss(tar, logits)\r\n                loss = tf.reduce_sum(cross_entropy) * (1.0 / self.batch_size)\r\n\r\n            with tf.name_scope(\"gradients\"):\r\n                gradients = tape.gradient(loss, self.trainable_variables)\r\n                if self.grad_clip:\r\n                    gradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))\r\n                                 for grad in gradients]\r\n                self.optimizer.apply_gradients(list(zip(gradients, self.trainable_variables)))\r\n            return cross_entropy\r\n\r\n        per_example_losses = self.mirrored_strategy.experimental_run_v2(\r\n            step_fn, args=(inputs, targets))\r\n\r\n        mean_loss = self.mirrored_strategy.reduce(\r\n            tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\r\n\r\n        mean_loss = mean_loss / self.batch_size\r\n        perplexity = self.get_perplexity(mean_loss)\r\n        step = self.optimizer.iterations\r\n        self._log_model_summary_data(self.train_writer,\r\n                                     step,\r\n                                     mean_loss,\r\n                                     perplexity)\r\n\r\n        return step, mean_loss, perplexity\r\n```\r\nPlease have a look at traceback.\r\n\r\n`Traceback (most recent call last):\r\n  File \"train_gpt2.py\", line 86, in <module>\r\n    train()\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 764, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"train_gpt2.py\", line 59, in train\r\n    model.fit([dist_train_dataset, dist_test_dataset])\r\n  File \"/home/abhay/edge_gpt2/model_gpt.py\", line 359, in fit\r\n    step, train_loss, perplexity = self.distributed_train_step(inputs, targets)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1821, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2104, in _maybe_define_function\r\n    *args, **kwargs)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1650, in canonicalize_function_inputs\r\n    self._flat_input_signature)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1710, in _convert_inputs_to_signature\r\n    format_error_message(inputs, input_signature))\r\nValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:\r\n  inputs: (\r\n    PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor`\r\n`ValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:`", "comments": ["@akanyaani ,\r\nThank you for reporting the issue.\r\nCan you share a simple and standalone code to reproduce the issue?\r\nAlso mention the TF version being used.", "This is because the model is passed as the first argument in the decorated function (which isn't convertible to a tensor), I don't think there's currently a way to specify a tensorspec for a decorated function which take a tf model. This is inconvenient, as currently experimental_run_v2 calls must be inside a decorated wrapper function.\r\n\r\n@akanyaani Does, something like this work?\r\n\r\n```\r\ndef distributed_wrapper(self, inputs, targets):\r\n\r\n    @tf.function(input_signature=input_signature)\r\n    def _distributed_train_step(inputs, targets):\r\n            def step_fn(inp, tar):\r\n                with tf.GradientTape() as tape:\r\n                    logits = self(inp, training=True)\r\n                    cross_entropy = self.get_loss(tar, logits)\r\n                    loss = tf.reduce_sum(cross_entropy) * (1.0 / self.batch_size)\r\n\r\n                with tf.name_scope(\"gradients\"):\r\n                    gradients = tape.gradient(loss, self.trainable_variables)\r\n                    if self.grad_clip:\r\n                        gradients = [(tf.clip_by_value(grad, -self.clip_value, self.clip_value))\r\n                                    for grad in gradients]\r\n                    self.optimizer.apply_gradients(list(zip(gradients, self.trainable_variables)))\r\n                return cross_entropy\r\n\r\n            per_example_losses = self.mirrored_strategy.experimental_run_v2(\r\n                step_fn, args=(inputs, targets))\r\n\r\n            mean_loss = self.mirrored_strategy.reduce(\r\n                tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\r\n\r\n            mean_loss = mean_loss / self.batch_size\r\n            perplexity = self.get_perplexity(mean_loss)\r\n            step = self.optimizer.iterations\r\n            self._log_model_summary_data(self.train_writer,\r\n                                        step,\r\n                                        mean_loss,\r\n                                        perplexity)\r\n\r\n            return step, mean_loss, perplexity\r\n    step, mean_loss, perplexity =  _distributed_train_step(inputs, targets)\r\nreturn step, mean_loss, perplexity  \r\n```\r\nIf any of the arguments called from the wrapper's scope other than the model are python primitives, it seems like this induces retracing and thus poor performance.", "Hi,\r\n@mjlbach As you suggested i will try but using the same method on single GPU it is working fine.\r\n\r\nYou can replicate the issues using this repo code.\r\n[Repo Link gpt-2-tensorflow2.0](https://github.com/akanyaani/gpt-2-tensorflow2.0)", "Hi, @mjlbach I tried what you suggested but I got the same error.\r\n\r\n ```\r\ncould not be loaded or symbol could not be found.\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0910 15:28:20.957778 139744540489472 cross_device_ops.py:779] Efficient allreduce is not supported for 1 IndexedSlices\r\nTraceback (most recent call last):\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1704, in _convert_inputs_to_signature\r\n    value, dtype_hint=spec.dtype)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1184, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1242, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1296, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 286, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 227, in constant\r\n    allow_broadcast=True)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 235, in _constant_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\", line 96, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\nValueError: Attempt to convert a value (PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: <tf.Tensor: id=282, shape=(16, 512), dtype=int32, numpy=\r\narray([[    3,   663,   289, ...,     0,     0,     0],\r\n       [    3,   524,   276, ...,     0,     0,     0],\r\n       [    3, 15756,   987, ...,     0,     0,     0],\r\n       ...,\r\n       [    3,  3164,  4571, ...,     0,     0,     0],\r\n       [    3,  1089,   289, ...,     0,     0,     0],\r\n       [    3,   798,  4138, ...,     0,     0,     0]], dtype=int32)>,\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: <tf.Tensor: id=285, shape=(16, 512), dtype=int32, numpy=\r\narray([[    3,   663,   987, ...,     0,     0,     0],\r\n       [    3,   175, 16233, ...,     0,     0,     0],\r\n       [    3,   698,    53, ...,     0,     0,     0],\r\n       ...,\r\n       [    3,    83,   632, ...,     0,     0,     0],\r\n       [    3, 13453,   168, ...,     0,     0,     0],\r\n       [    3,   421,    53, ...,     0,     0,     0]], dtype=int32)>\r\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_gpt2.py\", line 98, in <module>\r\n    train()\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 764, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"train_gpt2.py\", line 69, in train\r\n    model.fit([dist_train_dataset, dist_test_dataset])\r\n  File \"/home/abhay/edge_gpt2/model_gpt.py\", line 362, in fit\r\n    step, train_loss, perplexity = self.distributed_train_step(inputs, targets)\r\n  File \"/home/abhay/edge_gpt2/model_gpt.py\", line 283, in distributed_train_step\r\n    return _distributed_train_step(inputs, targets)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 439, in __call__\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1821, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2104, in _maybe_define_function\r\n    *args, **kwargs)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1650, in canonicalize_function_inputs\r\n    self._flat_input_signature)\r\n  File \"/home/abhay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1710, in _convert_inputs_to_signature\r\n    format_error_message(inputs, input_signature))\r\nValueError: When input_signature is provided, all inputs to the Python function must be convertible to tensors:\r\n  inputs: (\r\n    PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(\r\n[[    3   663   289 ...     0     0     0]\r\n [    3   524   276 ...     0     0     0]\r\n [    3 15756   987 ...     0     0     0]\r\n ...\r\n [    3  3164  4571 ...     0     0     0]\r\n [    3  1089   289 ...     0     0     0]\r\n [    3   798  4138 ...     0     0     0]], shape=(16, 512), dtype=int32),\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(\r\n[[    3   663   987 ...     0     0     0]\r\n [    3   175 16233 ...     0     0     0]\r\n [    3   698    53 ...     0     0     0]\r\n ...\r\n [    3    83   632 ...     0     0     0]\r\n [    3 13453   168 ...     0     0     0]\r\n [    3   421    53 ...     0     0     0]], shape=(16, 512), dtype=int32)\r\n},\r\n    PerReplica:{\r\n  0 /job:localhost/replica:0/task:0/device:GPU:0: tf.Tensor(\r\n[[  663   289   168 ...     0     0     0]\r\n [  524   276    30 ...     0     0     0]\r\n [15756   987    24 ...     0     0     0]\r\n ...\r\n [ 3164  4571    24 ...     0     0     0]\r\n [ 1089   289   396 ...     0     0     0]\r\n [  798  4138   295 ...     0     0     0]], shape=(16, 512), dtype=int32),\r\n  1 /job:localhost/replica:0/task:0/device:GPU:1: tf.Tensor(\r\n[[  663   987   142 ...     0     0     0]\r\n [  175 16233  2043 ...     0     0     0]\r\n [  698    53    45 ...     0     0     0]\r\n ...\r\n [   83   632  1739 ...     0     0     0]\r\n [13453   168  5897 ...     0     0     0]\r\n [  421    53    45 ...     0     0     0]], shape=(16, 512), dtype=int32)\r\n})\r\n  input_signature: (\r\n    TensorSpec(shape=(None, None), dtype=tf.int32, name=None),\r\n    TensorSpec(shape=(None, None), dtype=tf.int32, name=None))\r\n```", "Hi - thank you for reporting, this is a known issue that we plan to fix. (https://github.com/tensorflow/tensorflow/issues/29911) \r\n\r\nMeanwhile, I listed some workarounds in https://github.com/tensorflow/tensorflow/issues/29911#issuecomment-505688141 that can be useful to you ? \r\n", "Hi, @guptapriya I am able to train without input signature.\r\nIt works when you remove the signature part from tf.function\r\n\r\n```\r\n@tf.function(input_signature=input_signature)\r\n\r\nTo\r\n\r\n@tf.function\r\n```", "This issue has now been fixed and an example has been provided in #29911 for how to use the `element_spec` property of distributed datasets/iterators to specify the `input_signature`. Please reopen this issue as needed. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32232\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32232\">No</a>\n"]}, {"number": 32231, "title": "Bug in categorical_crossentropy loss (label smoothing)", "body": "Possible bug found in categorical_crossentropy's label smoothing argument in [line](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/losses.py#L940-L941) and for TF2 its in [line](https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/losses.py#L960-L961)\r\n\r\nThe axis specified in num_class is 1 which works fine for a  Rank 2 tensor but fails for higher rank tensors.\r\n\r\ndef _smooth_labels():\r\n    num_classes = math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)   #  -1 instead of 1 \r\n    return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)\r\n\r\n  y_true = smart_cond.smart_cond(label_smoothing,\r\n                                 _smooth_labels, lambda: y_true)\r\n  return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)", "comments": ["@praveenjune17 Sorry for the delay in response. Could you provide a simple standalone code to reproduce the issue? Also, provide platform details. Thanks!", "@jvishnuvardhan Raised this [PR](https://github.com/tensorflow/tensorflow/pull/32274) and added a Colab notebook which explains the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32231\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32231\">No</a>\n"]}, {"number": 32230, "title": "tf 2.0.0-rc0 tutorial colab (hub_with_keras) fails when batching images for simple model training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): google colab\r\n- TensorFlow version (use command below): 2.0.0-rc0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nOriginal bug report: https://github.com/tensorflow/hub/issues/359\r\n\r\nUsing tf 2.0.0-rc0 and constructing a simple Keras model fails to train. The issue seems to be due to batching in the tf.keras.preprocessing.image.ImageDataGenerrator().flow_from_dictionary(). \r\nMaybe some default parameters were changed.\r\nIf this is the case, the tutorial colab has to be adapted:\r\nhttps://www.tensorflow.org/beta/tutorials/images/hub_with_keras\r\n\r\n**Code to reproduce the issue**\r\nOriginal official colab: https://www.tensorflow.org/beta/tutorials/images/hub_with_keras\r\nIssue first reported based on this colab: https://colab.research.google.com/gist/MokkeMeguru/788255c93d4a4d8d4485098a7ba5e9f5/hub_with_keras.ipynb\r\nSimplified colab reproducing the error with a simple model:\r\nhttps://colab.sandbox.google.com/drive/1YI0XnFOgt9OEUzpvlAP4Tcb1W9NkPMdN#scrollTo=OGNpmn43C0O6\r\n\r\n", "comments": ["Resolution: instead of \".fit()\", one should have used \".fit_generator()\"."]}, {"number": 32229, "title": "why tensorflow 1.14 does not have contrib module?", "body": "```\r\n    hparams = tf.contrib.training.HParams(\r\nAttributeError: module 'tensorflow' has no attribute 'contrib'\r\n(env) [16:29:19] fagangjin:tf2_tacotron2_chinese git:(master*) $ python\r\nPython 3.5.2 (default, Nov 12 2018, 13:43:14) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nt>>> tf.__version__\r\n'1.14.0'\r\n>>> \r\n\r\n```\r\n\r\nDoes this normal?", "comments": ["@jinfagang \r\nTF 1.14 has contrib module.Please, find the [gist ](https://colab.sandbox.google.com/gist/ravikyram/f3cc2d7dca34311af82f3e9cb77b2eff/untitled155.ipynb) here.Thanks!", "It exists, see following test:\r\n\r\n```console\r\n(4) mihaimaruseac@ankh:/tmp/gh/4$ pip install -q tensorflow==1.14 && python -c \"import tensorflow as tf; print(tf.contrib.training)\"\r\nWARNING:tensorflow:\r\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\r\nFor more information, please see:\r\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n  * https://github.com/tensorflow/addons\r\n  * https://github.com/tensorflow/io (for I/O related ops)\r\nIf you depend on functionality not listed there, please file an issue.\r\n\r\n<module 'tensorflow.contrib.training' from '/tmp/gh/4/local/lib/python2.7/site-packages/tensorflow/contrib/training/__init__.pyc'>\r\n```\r\n\r\nAre you using a pip installed tensorflow or one built from source? Did you alter the code?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32229\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32229\">No</a>\n"]}, {"number": 32228, "title": "Build error C2589 on x64-Windows-static", "body": "Hi Developers,\r\n\r\nI recently tried to build tensorflow using vcpkg, and I have encountered some compilation errors.\r\nAfter two days of research this issue, I gave up. And I am not familiar with the bazel compiler.\r\nSo can someone help me?\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 x64 build 1903**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version: **v1.14.0**\r\n- Python version: **3.7.3**\r\n- Installed using virtualenv? pip? conda?: **vcpkg**\r\n- Bazel version (if compiling from source): **0.25.2**\r\n- GCC/Compiler version (if compiling from source): **Visual Studio 2017 ver 15.9.13**\r\n- CUDA/cuDNN version:  **N/A**\r\n- GPU model and memory: **CPU model, 32GB Memory**\r\n\r\n\r\n\r\n**Describe the problem**\r\nFirst build, error like:\r\n`.\\tensorflow/stream_executor/rng.h(66): error C2589: 'constant': illegal token on right side of '::'`\r\nTried to build **again**, the build was **successful**. I don't know why. \r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Clone vcpkg from `https://github.com/microsoft/vcpkg.git`.\r\n2. Build vcpkg with command `.\\bootstrap.bat`.\r\n3. Install tensorflow with command `.\\vcpkg.exe install tensorflow-cc:x64-windows-static`.\r\n\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n[build-x64-windows-static-err.log](https://github.com/tensorflow/tensorflow/files/3577942/build-x64-windows-static-err.log)\r\n[build-x64-windows-static-out.log](https://github.com/tensorflow/tensorflow/files/3577943/build-x64-windows-static-out.log)\r\n[config-x64-windows-static-err.log](https://github.com/tensorflow/tensorflow/files/3577944/config-x64-windows-static-err.log)\r\n[config-x64-windows-static-out.log](https://github.com/tensorflow/tensorflow/files/3577945/config-x64-windows-static-out.log)\r\n\r\n\r\nThanks.", "comments": ["Related: [#7995](https://github.com/microsoft/vcpkg/issues/7995).", "@vitong I think is looking into this.", "@ JackBoosY \r\nIs this still an issue.", "After updating the version, there are some errors in the configuration:\r\n```\r\nERROR: Unrecognized option: --experimental_repo_remote_exec\r\n```\r\n waiting for the solution to try to reproduce this issue.", "I will reopen this issue if this issue still happened.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32228\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32228\">No</a>\n"]}, {"number": 32227, "title": "`tf.strings.regex_replace` cannot add special characters on rewrite pattern", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.4\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0b1 (CPU version)\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\nGoogle's re2 library seems to be unable to handle rewrite patterns with characters like `\\n`, `\\t`, etc.\r\n\r\n**Describe the expected behavior**\r\nThe expected output of the code below should be `<tf.Tensor: id=4, shape=(), dtype=string, numpy=b'hello\\tworld'>`\r\n\r\n**Code to reproduce the issue**\r\n```\r\n>>> s = \"hello world\"\r\n>>> tf.strings.regex_replace(s, r\" \", r\"\\t\")\r\nexternal/com_googlesource_code_re2/re2/re2.cc:925: invalid rewrite pattern: \\t\r\n<tf.Tensor: id=4, shape=(), dtype=string, numpy=b'helloworld'>\r\n```", "comments": ["@danielwatson6 ,\r\nPlease be informed that Its the existing functionality explained in [link](https://github.com/google/re2/wiki/Syntax).Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32227\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32227\">No</a>\n"]}, {"number": 32226, "title": "Support for complex valued SVD gradient backprop", "body": "Implemented the correct backprop formular for complex valued SVD.\r\n\r\nrelated issue: https://github.com/tensorflow/tensorflow/issues/13641.\r\nSee the references https://re-ra.xyz/misc/complexsvd.pdf and also SVD section in https://giggleliu.github.io/2019/04/02/einsumbp.html.\r\n\r\nThe numerical results of this new implementation is correct as tested by hand. \r\nThe tf test part is a bit subtle as I am not familiar with tf inner test API and bazel stuff. So currently, I only add complex128 dtype to the same test. And local bazel test passed. \r\nBut I am actually not very satisfied with grad svd test here. There is gauge fixing in it and the object function seems not always gauge invariant. \r\nWant other's suggestion on test part, whether it is ok just keeping the test like this. If not, some more discussions and help on redesigning the test are needed.", "comments": []}, {"number": 32225, "title": "fixed integer overflow and add warning", "body": "The current code can lead to integer overflow and weird results\r\n\r\nstatic int64 GetDirectConvCost f-n can lead to integer overflow and weird results \r\n issue: #32045\r\n\r\n\r\n", "comments": []}, {"number": 32224, "title": "Deeplab V3 TFLite starter model has wrong input details shape dtype", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nCurrent Behavior:\r\nWhen running the starter model found [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite) and printing out the input details, the dtype in the shape is labeled as \"int32\" but the model expects \"float32\"\r\ninput details = [{'name': 'sub_7',\r\n  'index': 183,\r\n  'shape': array([  1, 257, 257,   3], dtype=int32),\r\n  'dtype': numpy.float32,\r\n  'quantization': (0.0, 0)}]\r\n\r\nWhen feeding an image of dtype int32:\r\nValueError: Cannot set tensor: Got tensor of type INT32 but expected type FLOAT32 for input 183, name: sub_7 \r\n\r\nTLDR: dtype and shape dtype are inconsistent. The dtype in the input details shape should be float32.", "comments": ["The int32 dtype (`'shape': array([ 1, 257, 257, 3], dtype=int32)`) you referred to is data type for 'shape', not for the data type of the input tensor. And you can see `'dtype': numpy.float32`", "@freedomtan My bad! Got confused, thanks for the explanation :+1: "]}, {"number": 32223, "title": "[ROCm] Test updates to fix the ROCm Nightly CSB", "body": "Recent changes have added new tests to the set of testcases that are being run as part of the ROCm Nightly CSB. These tests are failing because\r\n\r\n1. They test features (CSR Matrix ops) that were also recently added and are currently not supported on the ROCm platform.  We need to skip them on the ROCm platform, until support is added for them.  This PR adds no_rocm tags on those tests.\r\n\r\n2. A recently added subtest (`//tensorflow/python/eager:forwardprop_test_gpu.testVariableHVPNoFuntion`) started failing on the ROCm platform. We are investigating the cause of the failure, but in the meantime we will skip the subtest on the ROCm platform to ensure the NIghtly CSB passes again.\r\n\r\n------------------------------------------------------\r\n\r\n@whchung @chsigg \r\n\r\n", "comments": []}, {"number": 32222, "title": "AttributeError: module 'tensorflow' has no attribute 'indices'", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14 (colab)\r\n- Are you willing to contribute it (Yes/No): not sure how\r\n\r\n**Describe the feature and the current behavior/state.**\r\nnp.indices is useful to quickly get coordinates for each element in a shape, tensorflow doesn't have this so user must import numpy for this\r\n\r\n**Will this change the current api? How?**\r\nsure, `coordinates = tf.indices(x.shape[1:-1], dtype=DTYPE)` would allow one to get coordinates for a tensor. Ideally it would have a `normalize=True` kwarg where user can get normalized coordinates for coordinate convolution https://arxiv.org/abs/1807.03247\r\n\r\n**Who will benefit with this feature?**\r\nanyone who uses convolution, and wants code to run faster (i.e. by not importing numpy)\r\n\r\n**Any Other info.**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ncoordinates_np = np.indices((28, 28))  # works\r\ncoordinates_tf = tf.indices((28, 28))  # crashes\r\n```\r\n\r\n> AttributeError: module 'tensorflow' has no attribute 'indices'\r\n\r\n[https://docs.scipy.org/doc/numpy/reference/generated/numpy.indices.html](np.indices docs)", "comments": ["Up for grabs?", "Sure\r\n", "```\r\n# https://github.com/titu1994/keras-coordconv/blob/master/coord.py\r\nimport tensorflow as tf\r\n\r\nfrom tools import normalize, log\r\n\r\nK, L, B = tf.keras, tf.keras.layers, tf.keras.backend\r\n\r\n\r\ndef Coordinator(shape):\r\n    log(\"build coordinator with shape:\", shape, debug=True, color='green')\r\n    if len(shape) is 2:\r\n        return ConcatCoords2D()\r\n    elif len(shape) is 3:\r\n        return ConcatCoords3D()\r\n    elif len(shape) is 4:\r\n        return ConcatCoords4D()\r\n    else:\r\n        raise Exception(f\"{shape} not supported by coordinator\")\r\n\r\n\r\nclass ConcatCoords2D(L.Layer):\r\n    def __init__(self):\r\n        super(ConcatCoords2D, self).__init__()\r\n        self.handler = ConcatCoords3D()\r\n        self.built = True\r\n\r\n    @tf.function\r\n    def call(self, x):\r\n        x = tf.expand_dims(x, -1)\r\n        return self.handler(x)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        output_shape = list(input_shape)\r\n        output_shape.append(1)\r\n        output_shape[-1] = output_shape[-1] + 1\r\n        return tuple(output_shape)\r\n\r\n\r\nclass ConcatCoords3D(L.Layer):\r\n    def __init__(self):\r\n        super(ConcatCoords3D, self).__init__()\r\n        self.built = True\r\n\r\n    @tf.function\r\n    def call(self, x):\r\n        shape = tf.shape(x)\r\n        coords = tf.range(shape[1])\r\n        coords = tf.expand_dims(coords, 0)\r\n        coords = tf.expand_dims(coords, -1)\r\n        coords = tf.tile(coords, [shape[0], 1, 1])\r\n        coords = tf.cast(coords, tf.float32)\r\n        coords = normalize(coords)\r\n        return tf.concat([x, coords], -1)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        output_shape = list(input_shape)\r\n        output_shape[-1] = output_shape[-1] + 1\r\n        return tuple(output_shape)\r\n\r\n\r\nclass ConcatCoords4D(L.Layer):\r\n    def __init__(self):\r\n        super(ConcatCoords4D, self).__init__()\r\n\r\n    def build(self, shape):\r\n        h = tf.range(shape[1], dtype=tf.float32)\r\n        w = tf.range(shape[2], dtype=tf.float32)\r\n        h, w = normalize(h), normalize(w)\r\n        hw = tf.stack(tf.meshgrid(h, w, indexing='ij'), axis=-1)\r\n        hw = tf.expand_dims(hw, 0)\r\n        self.hw = tf.tile(hw, [shape[0], 1, 1, 1])\r\n        super().build(shape)\r\n\r\n    @tf.function\r\n    def call(self, x):\r\n        return tf.concat([x, self.hw], -1)\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        output_shape = list(input_shape)\r\n        output_shape[-1] = output_shape[-1] + 2\r\n        return tuple(output_shape)\r\n```", "Hi all! I'm writing regarding the CoordConv layers in native Tensorflow API. I saw this ISSUE as a possible related to my question :) Is CoordConv considered a feature in a future release? I am using a nice implementation from the following repo: [https://github.com/mvoelk/keras_layers](https://github.com/mvoelk/keras_layers) . However, it would nice to see CoordConv as a native Layer in Tensorflow :)", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 32221, "title": "AttributeError: 'Tensor' object has no attribute ['order', 'degree', 'rank']", "body": "**System information**\r\n- TensorFlow version (you are using): 1.14\r\n- Are you willing to contribute it (Yes/No): Yes. `return len(self.shape) - 1`\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nfrom wikipedia:\r\n> The order (also degree or rank) of a tensor is thus the sum of the orders of its arguments plus the order of the resulting tensor. This is also the dimensionality of the array of numbers needed to represent the tensor with respect to a specific basis, or equivalently, the number of indices needed to label each component in that array.\r\n\r\nI assumed I could do x.rank ; x.order, x.degree to get this value for a tensor. This is useful when we build a layer we need to do different functions for different order of tensors (e.g. image vs sequence data) \r\n\r\n**Will this change the current api? How?**\r\nUser can quickly access the rank of a tensor without having to recalculate it for each tensor. Tensors by definition have an order, so this change allows TF tensors to have the normal attributes a tensor would have in linear algebra discussion\r\n\r\n**Who will benefit with this feature?**\r\nUsers making custom layers often need to do boilerplate: `x_rank = len(x.shape) - 1 or such, and  might do it wrong. \r\n\r\nI made some neural interfaces to resize/reshape data to a common code, but convolution is a lot more effective if you concat coords to the tensor (https://arxiv.org/abs/1807.03247); logic often differs for different tensor ranks, so rather than users rewrite the 'len(x.shape) - N' 123456789 times, maybe we can just add this attribute to the Tensor class?   \r\n\r\n**Any Other info.**\r\ntf.keras.layers.Input mutates input shape to add batch size. I'm not sure how to handle the [mandatory] shape mutation in keras, I personally detest that, but it's not going away, guess we can just increment the rank, or not, you guys r smart, what do you think, is a batch of tensors a 1-higher-rank tensor? cc @alextp @fchollet\r\n\r\nNote: TFP lib has a \"batch_shape\" tuple, not integer, not sure if that causes issues, they have a 'param_shape' or such function to help with it \r\n\r\nexample with image data sort of shape\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('tf:', tf.__version__)\r\nprint('np:', np.__version__)\r\n\r\nSHAPE_AFTER_KERAS_MUTATES_IT = (1, 123, 123, 4)\r\ny = tf.random.normal(SHAPE_AFTER_KERAS_MUTATES_IT)\r\n\r\nSHAPE_WITHOUT_BATCH_OR_CHANNELS = y.shape[1:-1]\r\ncoords_np = np.indices(SHAPE_WITHOUT_BATCH_OR_CHANNELS)   # no tf.indices ? :(\r\ncoords = tf.convert_to_tensor(coords_np, dtype=tf.float32)\r\n\r\nif y.rank is 3:  # <--- this fails and we have to manually compute rank\r\n     coords = tf.transpose(coords, [1, 2, 0])\r\n     coords = tf.expand_dims(coords, 0)\r\n     coords.shape\r\n\r\ny_with_coords = tf.concat([y, coords], -1)\r\n```", "comments": ["This is by design, use len(tensor.shape) instead, or tf.rank(tensor), etc."]}]